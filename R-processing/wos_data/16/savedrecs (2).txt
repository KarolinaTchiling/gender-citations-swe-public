FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Cha, J
   Eid, M
   El Saddik, A
AF Cha, Jongeun
   Eid, Mohamad
   El Saddik, Abdulmotaleb
TI Touchable 3D Video System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; Haptic surface properties; haptic rendering
   algorithm; video representation
ID REPRESENTATION; HAPTICS; DISPLAY
AB Multimedia technologies are reaching the limits of providing audio-visual media that viewers consume passively. An important factor, which will ultimately enhance the user's experience in terms of impressiveness and immersion, is interaction. Among daily life interactions, haptic interaction plays a prominent role in enhancing the quality of experience of users, and in promoting physical and emotional development. Therefore, a critical step in multimedia research is expected to bring the sense of touch, or haptics, into multimedia systems and applications. This article proposes a touchable 3D video system where viewers can actively touch a video scene through a force-feedback device, and presents the underlying technologies in three functional components: (1) contents generation, (2) contents transmission, and (3) viewing and interaction. First of all, we introduce a depth image-based haptic representation (DIBHR) method that adds haptic and heightmap images, in addition to the traditional depth image-based representation (DIBR), to encode the haptic surface properties of the video media. In this representation, the haptic image contains the stiffness, static friction, and dynamic friction, whereas the heightmap image contains roughness of the video contents. Based on this representation method, we discuss how to generate synthetic and natural (real) video media through a 3D modeling tool and a depth camera, respectively. Next, we introduce a transmission mechanism based on the MPEG-4 framework where new MPEG-4 BIFS nodes are designed to describe the haptic scene. Finally, a haptic rendering algorithm to compute the interaction force between the scene and the viewer is described. As a result, the performance of the haptic rendering algorithm is evaluated in terms of computational time and smooth contact force. It operates marginally within a 1 kHz update rate that is required to provide stable interaction force and provide smoother contact force with the depth image that has high frequency geometrical noise using a median filter.
C1 [Cha, Jongeun; Eid, Mohamad; El Saddik, Abdulmotaleb] Univ Ottawa, Multimedia Commun Res Lab, Sch Informat Technol & Engn, Ottawa, ON K1N 6N5, Canada.
C3 University of Ottawa
RP Cha, J (corresponding author), Univ Ottawa, Multimedia Commun Res Lab, Sch Informat Technol & Engn, 800 King Edward, Ottawa, ON K1N 6N5, Canada.
RI /D-4159-2009
OI /0000-0002-7690-8547
CR Alatan AA, 2007, IEEE T CIRC SYST VID, V17, P1587, DOI 10.1109/TCSVT.2007.909974
   Avila RS, 1996, IEEE VISUAL, P197, DOI 10.1109/VISUAL.1996.568108
   BUKOWSKA M, 2001, W DINK HALF CENTURY
   CHA J, 2004, P 5 PAC RIM C MULT A, P482
   Cha J, 2008, LECT NOTES COMPUT SC, V5024, P640
   Cha J, 2007, WORLD HAPTICS 2007: SECOND JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P274
   Cha J, 2006, IEEE T CONSUM ELECTR, V52, P477, DOI 10.1109/TCE.2006.1649668
   CHORIANOPOULOS K, 2007, ACM COMPUT ENTERTAIN, V5, P2
   CONTI F, 2005, P JOINT EUROHAPTICS
   Eid M., 2007, International Journal of Advanced Media and Communication, V1, P71, DOI 10.1504/IJAMC.2007.013918
   Eid M, 2008, LECT NOTES COMPUT SC, V5024, P857, DOI 10.1007/978-3-540-69057-3_108
   El Saddik A, 2007, IEEE INSTRU MEAS MAG, V10, P10, DOI 10.1109/MIM.2007.339540
   FEHN C, 2003, JTC1SC29WG11 ISOIEC
   Gao Z., 2005, COMPUT AIDED DESIGN, V2, P263
   Gaw D, 2006, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2006, PROCEEDINGS, P287
   GIBSON JJ, 1962, PSYCHOL REV, V69, P477, DOI 10.1037/h0046962
   Hale KS, 2004, IEEE COMPUT GRAPH, V24, P33, DOI 10.1109/MCG.2004.1274059
   Ho CH, 1999, PRESENCE-VIRTUAL AUG, V8, P477, DOI 10.1162/105474699566413
   IGNATENKO A, 2003, P GRAPH, P169
   Ikits M, 2003, IEEE VISUALIZATION 2003, PROCEEDINGS, P263, DOI 10.1109/VISUAL.2003.1250381
   KAUFF P, 2001, P PCS 01, P429
   Kim L, 2004, IEEE COMPUT GRAPH, V24, P66, DOI 10.1109/MCG.2004.1274064
   Kim SM, 2006, IEICE T INF SYST, VE89D, P37, DOI 10.1093/ietisy/e89-d.1.37
   Lawrence DA, 2000, IEEE VISUAL, P131
   Lee MH, 1999, MECHATRONICS, V9, P1, DOI 10.1016/S0957-4158(98)00045-2
   Levkovich-Maslyuk L, 2004, IEEE T CIRC SYST VID, V14, P1032, DOI 10.1109/TCSVT.2004.830676
   Lucchese L., 2001, PINSA-A (Proceedings of the Indian National Science Academy) Part A (Physical Sciences), V67, P207
   Magnenat-Thalmann N, 2006, IEEE MULTIMEDIA, V13, P6, DOI 10.1109/MMUL.2006.56
   Matusik W, 2004, ACM T GRAPHIC, V23, P814, DOI 10.1145/1015706.1015805
   McNeely WA, 1999, COMP GRAPH, P401, DOI 10.1145/311535.311600
   Melder N, 2004, 12TH INTERNATIONAL SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P234, DOI 10.1109/HAPTIC.2004.1287201
   MICHELITSCH G, 2002, P EUROHAPTICS C
   O'Modhrain S., 2003, European Conference on Interactive Television: From Viewers to Actors?, P41
   REACHIN AB, 2003, REACHIN API 3 2 PROG
   Reiner M, 2004, IEEE T CIRC SYST VID, V14, P392, DOI 10.1109/TCSVT.2004.823399
   RIVA G, 2003, BEING THERE CONCEPTS, pCH2
   Ruspini D. C., 1997, ANN C COMP GRAPH INT, P345, DOI DOI 10.1145/258734.258878
   SALISBURY JK, 1997, P ASME S HAPT INT VI, P61
   Sallnas E.-L., 2000, ACM Transactions on Computer-Human Interaction, V7, P461, DOI 10.1145/365058.365086
   *SENS TECHN INC, 2005, OPENHAPTICS TOOLK VE
   *SENSEGRAPHICS AB, 2006, H3D API MAN VERS 1 5
   Smolic A, 2005, P IEEE, V93, P98, DOI 10.1109/JPROC.2004.839608
   Thompson T. V.  II, 1997, Proceedings of the ASME Dynamic Systems and Control Division, P37
   THOMPSON TV, 1999, P S HAPT INT VIRT EN, P89
   Walker S., 2003, ACM SIGGRAPH, V2003, P83, DOI DOI 10.1145/641480.641499
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   YAMAGUCHI T, 2006, P EUROHAPTICS C
   ZILLES CB, 1995, IROS '95 - 1995 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS: HUMAN ROBOT INTERACTION AND COOPERATIVE ROBOTS, PROCEEDINGS, VOL 3, P146, DOI 10.1109/IROS.1995.525876
NR 48
TC 25
Z9 28
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2009
VL 5
IS 4
AR 29
DI 10.1145/1596990.1596993
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 529WO
UT WOS:000272549900003
DA 2024-07-18
ER

PT J
AU Erdmann, M
   Nakayama, K
   Hara, T
   Nishio, S
AF Erdmann, Maike
   Nakayama, Kotaro
   Hara, Takahiro
   Nishio, Shojiro
TI Improving the Extraction of Bilingual Terminology from Wikipedia
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Bilingual dictionary; Wikipedia mining;
   link analysis
AB Research on the automatic construction of bilingual dictionaries has achieved impressive results. Bilingual dictionaries are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well.
   In this article, we want to further pursue the idea of using Wikipedia as a corpus for bilingual terminology extraction. We propose a method that extracts term-translation pairs from different types of Wikipedia link information. After that, an SVM classifier trained on the features of manually labeled training data determines the correctness of unseen term-translation pairs.
C1 [Erdmann, Maike; Hara, Takahiro; Nishio, Shojiro] Osaka Univ, Suita, Osaka 565, Japan.
   [Nakayama, Kotaro] Univ Tokyo, Tokyo 1138654, Japan.
C3 Osaka University; University of Tokyo
RP Erdmann, M (corresponding author), Osaka Univ, Suita, Osaka 565, Japan.
EM Erdmann.maike@ist.osaka-u.ac.jp
FU Microsoft;  [21300032];  [20500093]
FX This research was supported in part by Grant-in-Aid on Priority Areas
   (21300032, 20500093), and by the Microsoft Research IJARC CORE project.
CR [Anonymous], P ANN M ASS COMP LIN
   [Anonymous], P C EUR CHAPT ASS CO
   [Anonymous], P INT C LANG RES EV
   [Anonymous], FEATURE EXTRACTION F
   [Anonymous], P ANN C INT SOC INET
   [Anonymous], P INT C DAT SYST ADV
   [Anonymous], P C COMP LING CL
   [Anonymous], P C EUR CHAPT ASS CO
   [Anonymous], P MACH TRANSL SUMM X
   [Anonymous], P WORKSH CROSS LANG
   [Anonymous], WORK NOT CROSS LANG
   [Anonymous], COMPUT LING
   [Anonymous], J INFORM PROCESS
   [Anonymous], NATURAL LANGUAGE PRO
   [Anonymous], 2011, ACM T INTEL SYST TEC
   [Anonymous], MCNEMARS TEST CORREL
   [Anonymous], 2006, P EACL WORKSH NEW TE
   [Anonymous], P 2 ACM INT C WEB SE
   [Anonymous], P ANN INT ACM SIGIR
   [Anonymous], P 46 ANN M ASS COMP
   Brown P.F., 1993, COMPUT LINGUIST, V19, P2
   Brown PeterF., 1990, Computational Linguistics, V16, P2
   Och FJ, 2003, COMPUT LINGUIST, V29, pc, DOI 10.1162/089120103321337421
   Resnick P., 2003, Computational Linguistics, V29, P3
NR 24
TC 27
Z9 27
U1 3
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2009
VL 5
IS 4
AR 31
DI 10.1145/1596990.1596995
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 529WO
UT WOS:000272549900005
DA 2024-07-18
ER

PT J
AU Sauer, D
   Yang, YH
AF Sauer, Danielle
   Yang, Yee-Hong
TI Music-Driven Character Animation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Experimentation; Human Factors; Character animation; motion
   synthesis; music analysis; primitive movements
ID TIME BEAT TRACKING; MOTION SYNTHESIS; AUDIO SIGNALS
AB Music-driven character animation extracts musical features from a song and uses them to create an animation. This article presents a system that builds a new animation directly from musical attributes, rather than simply synchronizing it to the music like similar systems. Using a simple script that identifies the movements involved in the performance and their timing, the user can easily control the animation of characters. Another unique feature of the system is its ability to incorporate multiple characters into the same animation, both with synchronized and unsynchronized movements. A system that integrates Celtic dance movements is developed in this article. An evaluation of the results shows that the majority of animations are found to be appealing to viewers and that altering the music can change the attractiveness of the final result.
C1 [Sauer, Danielle; Yang, Yee-Hong] Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
C3 University of Alberta
RP Sauer, D (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
EM yang@cs.ualberta.ca
FU NSERC; AutoDesk
FX This research was supported by NSERC and AutoDesk.
CR Alankus G, 2005, COMPUT ANIMAT VIRT W, V16, P259, DOI 10.1002/cav.99
   Bello JP, 2005, IEEE T SPEECH AUDI P, V13, P1035, DOI 10.1109/TSA.2005.851998
   Cardle M, 2002, 20TH EUROGRAPHICS UK CONFERENCE, PROCEEDINGS, P38, DOI 10.1109/EGUK.2002.1011270
   Dixon S, 2003, PROC SPIE, V5021, P122, DOI 10.1117/12.476314
   DUNNE C, 1996, CELTIC FEET
   Fod A, 2002, AUTON ROBOT, V12, P39, DOI 10.1023/A:1013254724861
   Goto M., 1994, Proceedings ACM Multimedia '94, P365, DOI 10.1145/192593.192700
   Goto M, 1999, SPEECH COMMUN, V27, P311, DOI 10.1016/S0167-6393(98)00076-4
   Goto M, 2001, J NEW MUSIC RES, V30, P159, DOI 10.1076/jnmr.30.2.159.7114
   Kim TH, 2003, ACM T GRAPHIC, V22, P392, DOI 10.1145/882262.882283
   Scheirer ED, 1998, J ACOUST SOC AM, V103, P588, DOI 10.1121/1.421129
   Shiratori T, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P857, DOI 10.1109/AFGR.2004.1301641
   SHIRATORI T, 2006, EUROGRAPHICS, V25, P3
   TAYLOR R, 2005, P NEW INT MUS EXPR, P220
   TZANETAKIS G, 2001, P WSES INT C AC MUS
   Woch A, 2004, MOTOR CONTROL, V8, P547, DOI 10.1123/mcj.8.4.547
NR 16
TC 10
Z9 10
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2009
VL 5
IS 4
AR 27
DI 10.1145/1596990.1596991
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA 529WO
UT WOS:000272549900001
DA 2024-07-18
ER

PT J
AU Cattelan, RG
   Teixeira, C
   Goularte, R
   Pimentel, MDGC
AF Cattelan, Renan G.
   Teixeira, Cesar
   Goularte, Rudinei
   C Pimentel, Maria Da Graca
TI Watch-and-Comment as a Paradigm toward Ubiquitous Interactive Video
   Editing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Experimentation; Annotation; interactive digital video;
   P2P collaboration; Ginga-NCL
AB The literature reports research efforts allowing the editing of interactive TV multimedia documents by end-users. In this article we propose complementary contributions relative to end-user generated interactive video, video tagging, and collaboration. In earlier work we proposed the watch-and-comment (WaC) paradigm as the seamless capture of an individual's comments so that corresponding annotated interactive videos be automatically generated. As a proof of concept, we implemented a prototype application, the WACTOOL, that supports the capture of digital ink and voice comments over individual frames and segments of the video, producing a declarative document that specifies both: different media stream structure and synchronization.
   In this article, we extend the WaC paradigm in two ways. First, user-video interactions are associated with edit commands and digital ink operations. Second, focusing on collaboration and distribution issues, we employ annotations as simple containers for context information by using them as tags in order to organize, store and distribute information in a P2P-based multimedia capture platform. We highlight the design principles of the watch-and-comment paradigm, and demonstrate related results including the current version of the WACTOOL and its architecture. We also illustrate how an interactive video produced by the WACTOOL can be rendered in an interactive video environment, the Ginga-NCL player, and include results from a preliminary evaluation.
C1 [Cattelan, Renan G.; Goularte, Rudinei; C Pimentel, Maria Da Graca] Univ Sao Paulo, BR-05508 Sao Paulo, Brazil.
   [Teixeira, Cesar] Univ Fed Sao Carlos, BR-13560 Sao Carlos, SP, Brazil.
C3 Universidade de Sao Paulo; Universidade Federal de Sao Carlos
RP Cattelan, RG (corresponding author), Univ Sao Paulo, BR-05508 Sao Paulo, Brazil.
RI Pimentel, Maria G C/D-2875-2009; Teixeira, César/A-3477-2012; Goularte,
   Rudinei/E-2441-2011
OI Pimentel, Maria G C/0000-0001-8264-5811; Teixeira,
   César/0000-0001-9396-1211; Cattelan, Renan/0000-0001-9993-8469;
   Goularte, Rudinei/0000-0003-1531-1576
FU FINEP; FAPESP [03/13930-4]; CAPES; CNPq
FX The authors thank the following organizations for their support: FINEP,
   FAPESP, CAPES, and CNPq. R. G. Cattelan is a PhD candidate supported by
   FAPESP (03/13930-4).
CR Abowd G. D., 2002, IEEE Pervasive Computing, V1, P48, DOI 10.1109/MPRV.2002.993144
   BELMONT V, 2006, CNET REV
   BULTERMAN DCA, 2003, P 2003 ACM S DOC ENG, P32
   BULTERMAN DCA, 2006, P 14 ANN ACM INT C M, P651
   BUTKUS A, 2007, P EUR INT TEL C, P226
   CATTELAN R, 2008, P ACM S APPL COMP SA, P1246
   Cattelan RG, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P1246
   CESAR P, 2007, P EUR C INT TEL, P11
   CESAR P, 2006, P ACM S DOC ENG, P176
   CESAR P, 2006, P ACM S DOC ENG DOCE, P186, DOI DOI 10.1145/1166160.1166209
   Cesar P, 2008, LECT NOTES COMPUT SC, V5066, P168, DOI 10.1007/978-3-540-69478-6_22
   COPPENS T, 2004, P EUR INT TEL C
   COSTA RMR, 2006, P ACM S DOC ENG, P165
   DENENDE NV, 2007, P EUR INT TEL C, P185
   Geerts D., 2006, P NORDICHI 2006, P461, DOI DOI 10.1145/1182475.1182537
   GIRGENSOHN A, 2000, P UIST 00, P81
   Goularte Rudinei., 2004, Proceedings o f the 2004 ACM symposium on Document engineering, Milwaukee, Wisconsin, USA, P84, DOI DOI 10.1145/1030397.1030414
   GUIMARAES RL, 2008, P EUR INT TEL C, P61
   Harboe G., 2008, ACM COMPUTERS ENTERT, V6, P1, DOI DOI 10.1145/1350843.1350851
   HEMMERYCKXDELEE.B, 2007, P EUR INT TEL C, P1
   HUA XS, 2006, P 1 ACM INT WORKSH H, P65
   HUA XS, 2004, P 12 ACM INT C MULT, P172
   Kindberg T., 2005, CHI 05, P1545, DOI [DOI 10.1145/1056808.1056962, 10.1145/1056808.1056962]
   Kirk D, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P61, DOI 10.1145/1240624.1240634
   LIU KY, 2005, P 13 ACM INT C MULT, P61
   Luyten K., 2006, CHL 06 EXTENDED ABST, P1049, DOI DOI 10.1145/1125451.1125651
   Madhwacharyula CL, 2006, ACM T MULTIM COMPUT, V2, P358, DOI 10.1145/1201730.1201736
   NIELSEN J, 1992, P SIGCHI C HUM FACT, P373, DOI DOI 10.1145/142750.142834
   PIMENTEL M, 2008, P EUR INT TEL C, P72
   Pimentel MDC, 2007, IEEE PERVAS COMPUT, V6, P93
   Pimentel MG, 2007, IEEE INT SYM MULTIM, P207, DOI 10.1109/ISM.Workshops.2007.43
   Ramos G., 2003, Proceedings of the UIST '03 Symposium on User Interface Software and Technology, P105, DOI DOI 10.1145/964696.964708
   Regan T., 2004, P NORDICHI 2004, P141
   Ross AS, 2005, NAT CLIN PRACT GASTR, V2, P281, DOI 10.1038/ncpgasthep0195
   SADKA AH, 2004, P WORKSH IM AN MULT
   Shamma DA., 2007, P INT WORKSHOP WORKS, P275, DOI [10.1145/1290082.1290120, DOI 10.1145/1290082.1290120]
   SHAW R, 2006, P 1 ACM INT WORKSH H, P89
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Tsekleves E, 2007, IEEE INT SYM MULTIM, P201, DOI 10.1109/ISM.Workshops.2007.42
   Ullmer B., 1998, PROC ACM SIGGRAPH 98, P379, DOI DOI 10.1145/280814.280940
   WEISER M, 1991, SCI AM, V265, P94, DOI 10.1038/scientificamerican0991-94
   Weisz JD, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P877
   WRIGHT C, 1991, CONTROL INSTRUM, V23, P55
   Zigelbaum J., 2007, Proc. TEI '07, P43, DOI [10.1145/1226969.1226978, DOI 10.1145/1226969.1226978]
NR 44
TC 26
Z9 34
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 4
AR 28
DI 10.1145/1412196.1412201
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 376AP
UT WOS:000261155300005
DA 2024-07-18
ER

PT J
AU Snoek, CGM
   Worring, M
   Hauptmann, AG
AF Snoek, Cees G. M.
   Worring, Marcel
   Hauptmann, Alexander G.
TI Learning rich semantics from news video archives by style analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; management; performance; benchmark evaluation; multimedia
   understanding; multimodal detectors; news video indexing; semantic
   classification; statistical pattern recognition; style analysis
ID RETRIEVAL; RECOGNITION; EXTRACTION
AB We propose a generic and robust framework for news video indexing which we founded on a broadcast news production model. We identify within this model four production phases, each providing useful metadata for annotation. In contrast to semiautomatic indexing approaches which exploit this information at production time, we adhere to an automatic data-driven approach. To that end, we analyze a digital news video using a separate set of multimodal detectors for each production phase. By combining the resulting production-derived features into a statistical classifier ensemble, the framework facilitates robust classification of several rich semantic concepts in news video; rich meaning that concepts share many similarities in their production process. Experiments on an archive of 120 hours of news video from the 2003 TRECVID benchmark show that a combined analysis of production phases yields the best results. In addition, we demonstrate that the accuracy of the proposed style analysis framework for classification of several rich semantic concepts is state-of-the-art.
C1 Univ Amsterdam, Inst Informat, NL-1098 SJ Amsterdam, Netherlands.
   Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
C3 University of Amsterdam; Carnegie Mellon University
RP Snoek, CGM (corresponding author), Univ Amsterdam, Inst Informat, Kruislaan 403, NL-1098 SJ Amsterdam, Netherlands.
EM cgmsnoek@science.uva.nl
RI Worring, Marcel/JRW-7059-2023
OI Worring, Marcel/0000-0003-4097-4136; Snoek, Cees/0000-0001-9092-1556
CR Adams B, 2002, IEEE T MULTIMEDIA, V4, P472, DOI 10.1109/TMM.2002.802016
   Adams WH, 2003, EURASIP J APPL SIG P, V2003, P170, DOI 10.1155/S1110865703211173
   AMIR A, 2003, P TRECVID WORKSH NIS
   [Anonymous], 1983, INTRO MODERN INFORM
   BAAN J, 2001, NIST SPECIAL PUBLICA, V500
   BARRY B, 2003, P IEEE INT C MULT EX
   BOGGS J, 2000, ART WATCHING FILMS
   Bordwell D., 1997, FILM ART INTRO
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Davis M, 2003, IEEE MULTIMEDIA, V10, P54, DOI 10.1109/MMUL.2003.1195161
   Dey AK, 2001, PERS UBIQUIT COMPUT, V5, P4, DOI 10.1007/s007790170019
   Dourish P, 2004, PERS UBIQUIT COMPUT, V8, P19, DOI 10.1007/s00779-003-0253-8
   Gauvain JL, 2002, SPEECH COMMUN, V37, P89, DOI 10.1016/S0167-6393(01)00061-9
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   HAUPTMANN A, 2003, NIST SPECIAL PUBLICA
   Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819
   Nack F, 2004, MULTIMED TOOLS APPL, V22, P263, DOI 10.1023/B:MTAP.0000017031.26875.f7
   Naphade MR, 2002, IEEE T CIRC SYST VID, V12, P40
   Platt JC, 2000, ADV NEUR IN, P61
   QUENOT G, 2002, NIST SPECIAL PUBLICA, V500
   Sato T, 1999, MULTIMEDIA SYST, V7, P385, DOI 10.1007/s005300050140
   SCHAEFER P, 1990, EARTH ISL J, V5, P2
   Schneiderman H, 2004, INT J COMPUT VISION, V56, P151, DOI 10.1023/B:VISI.0000011202.85607.00
   SMEATON A, 2003, NIST SPECIAL PUBLICA
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SMITH J, 1998, P IEEE CVPR 98 WORKS
   SNOEK C, 2004, P IEEE INT C MULT EX
   Snoek CGM, 2005, IEEE T MULTIMEDIA, V7, P638, DOI 10.1109/TMM.2005.850966
   Snoek CGM, 2005, MULTIMED TOOLS APPL, V25, P5, DOI 10.1023/B:MTAP.0000046380.27575.a5
   Sundaram H, 2002, IEEE T MULTIMEDIA, V4, P482, DOI 10.1109/TMM.2002.802017
   Truong BT, 2005, MULTIMED TOOLS APPL, V26, P277, DOI 10.1007/s11042-005-0892-z
   TSENG B, 2002, P IEEE INT C IM PROC, V2, P535
   Vapnik V.N., 2000, The Nature of Statistical Learning Theory, DOI DOI 10.1007/978-1-4757-3264-1_1
   Vendrig J, 2002, IEEE T MULTIMEDIA, V4, P492, DOI 10.1109/TMM.2002.802021
   Wactlar HD, 1999, COMPUTER, V32, P66, DOI 10.1109/2.745722
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Xie LX, 2004, PATTERN RECOGN LETT, V25, P767, DOI 10.1016/j.patrec.2004.01.005
   Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4
NR 39
TC 18
Z9 21
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2006
VL 2
IS 2
BP 91
EP 108
DI 10.1145/1142020.1142021
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IX
UT WOS:000250871300001
DA 2024-07-18
ER

PT J
AU Srinivas, K
   Bhandari, AK
AF Srinivas, Kankanala
   Bhandari, Ashish Kumar
TI Context-Based Novel Histogram Bin Stretching Algorithm for Automatic
   Contrast Enhancement
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Histogram bin stretching; contrast enhancement; spatial contextual
   information; discrete cosine transform
ID PLATEAU LIMIT; EQUALIZATION; DIFFERENCE; SCHEME
AB This article presents CHBS, a novel context-based histogram bin stretching method that enhances the contrast by increasing the range of gray levels and randomness among the gray levels. It comprises image spatial contextual information and discrete cosine transform (DCT). It constitutes the global enhancement with the context-based histogram bin stretching and local details with the DCT. First, it uses the spatial similarities among surrounding pixels to generate random numbers. Unlike the other methods, the similarity map is generated based on the neighboring pixels' mutual relationship. Intensity values are distributed among the available dynamic range to generate a global contrast-enhanced image. Second, the DCT is further applied to the previous contrast-enhanced image to adjust its local details automatically. Several experiments are conducted on the different levels of contrast degraded images. Both subjective and objective assessment outcomes validate that the projected approach is better or comparable with several state-of-the-art approaches in terms of brightness preservation, richer details, and natural appearance.
C1 [Srinivas, Kankanala] VIT AP Univ, Sch Elect Engn, Amaravati, Andhra Pradesh, India.
   [Bhandari, Ashish Kumar] Natl Inst Technol Patna, Dept Elect & Commun Engn, Patna 800005, Bihar, India.
C3 VIT-AP University; National Institute of Technology (NIT System);
   National Institute of Technology Patna
RP Bhandari, AK (corresponding author), Natl Inst Technol Patna, Dept Elect & Commun Engn, Patna 800005, Bihar, India.
EM srinivas483@gmail.com; bhandari.iiitj@gmail.com
RI Bhandari, Ashish Kumar/AAA-9991-2019; KANKANALA, SRINIVAS/AAU-1515-2021
OI Bhandari, Ashish Kumar/0000-0001-9842-8125; KANKANALA,
   SRINIVAS/0000-0002-0426-4850
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Aquino-Morínigo PB, 2017, SIGNAL IMAGE VIDEO P, V11, P857, DOI 10.1007/s11760-016-1032-0
   Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   Berkhin P, 2005, INTERNET MATH, V2, P73, DOI 10.1080/15427951.2005.10129098
   Bhandari AK, 2020, IEEE T FUZZY SYST, V28, P2009, DOI 10.1109/TFUZZ.2019.2930028
   Celik T, 2016, IEEE T IMAGE PROCESS, V25, P4719, DOI 10.1109/TIP.2016.2599103
   Celik T, 2014, IEEE T IMAGE PROCESS, V23, P5298, DOI 10.1109/TIP.2014.2364537
   Celik T, 2012, PATTERN RECOGN, V45, P3810, DOI 10.1016/j.patcog.2012.03.019
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chang YC, 2010, IEEE T CONSUM ELECTR, V56, P737, DOI 10.1109/TCE.2010.5505995
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1301, DOI 10.1109/TCE.2003.1261233
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1310, DOI 10.1109/TCE.2003.1261234
   Coltuc D, 2006, IEEE T IMAGE PROCESS, V15, P1143, DOI 10.1109/TIP.2005.864170
   Gonzalez R.C., 2018, Digital Image Processing
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Hao SJ, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498341
   Hasler D, 2003, P SOC PHOTO-OPT INS, V5007, P87, DOI 10.1117/12.477378
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Kandhway P, 2019, IET IMAGE PROCESS, V13, P1658, DOI 10.1049/iet-ipr.2019.0111
   Kim M, 2008, IEEE T CONSUM ELECTR, V54, P1389, DOI 10.1109/TCE.2008.4637632
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Koh CC, 2006, PROC SPIE, V6057, DOI 10.1117/12.642569
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Majumder A, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1278387.1278391
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Ooi CH, 2010, IEEE T CONSUM ELECTR, V56, P2552, DOI 10.1109/TCE.2010.5681140
   Ooi CH, 2009, IEEE T CONSUM ELECTR, V55, P2072, DOI 10.1109/TCE.2009.5373771
   Parihar AS, 2017, IEEE T IMAGE PROCESS, V26, P1810, DOI 10.1109/TIP.2017.2665975
   Paul A, 2018, IET IMAGE PROCESS, V12, P1617, DOI 10.1049/iet-ipr.2017.1088
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Sim KS, 2007, PATTERN RECOGN LETT, V28, P1209, DOI 10.1016/j.patrec.2007.02.003
   Singh K, 2016, J MOD OPTIC, V63, P1444, DOI 10.1080/09500340.2016.1154194
   Singh K, 2015, OPTIK, V126, P2619, DOI 10.1016/j.ijleo.2015.06.060
   Singh K, 2014, PATTERN RECOGN LETT, V36, P10, DOI 10.1016/j.patrec.2013.08.024
   Srinivas K, 2021, IEEE T IMAGE PROCESS, V30, P5391, DOI 10.1109/TIP.2021.3083448
   Srinivas K, 2020, IEEE T CIRC SYST VID, V30, P4663, DOI 10.1109/TCSVT.2019.2960861
   Sun CC, 2005, IEEE T CONSUM ELECTR, V51, P1300
   Wang Q, 2007, IEEE T CONSUM ELECTR, V53, P757, DOI 10.1109/TCE.2007.381756
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen LY, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183518
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 47
TC 1
Z9 1
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 204
DI 10.1145/3597303
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200027
DA 2024-07-18
ER

PT J
AU Zhu, DD
   Shao, X
   Zhou, QQ
   Min, XK
   Zhai, G
   Yang, XK
AF Zhu, Dandan
   Shao, Xuan
   Zhou, Qiangqiang
   Min, Xiongkuo
   Zhai, Guangtao
   Yang, Xiaokang
TI A Novel Lightweight Audio-visual Saliency Model for Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Lightweight model; deep canonical correlation analysis; audio-visual
   saliency prediction; feature fusion; sound source localization
ID ATTENTION; NETWORK; FRAMEWORK
AB Audio information has not been considered an important factor in visual attention models regardless of many psychological studies that have shown the importance of audio information in the human visual perception system. Since existing visual attention models only utilize visual information, their performance is limited but also requires high-computational complexity due to the limited information available. To overcome these problems, we propose a lightweight audio-visual saliency (LAVS) model for video sequences. To the best of our knowledge, this article is the first trial to utilize audio cues for an efficient deep-learning model for the video saliency estimation. First, spatial-temporal visual features are extracted by the lightweight receptive field block (RFB) with the bidirectional ConvLSTM units. Then, audio features are extracted by using an improved lightweight environment sound classification model. Subsequently, deep canonical correlation analysis (DCCA) aims at capturing the correspondence between audio and spatial-temporal visual features, thus obtaining a spatial-temporal auditory saliency. Lastly, the spatial-temporal visual and auditory saliency are fused to obtain the audio-visual saliency map. Extensive comparative experiments and ablation studies validate the performance of the LAVS model in terms of effectiveness and complexity.
C1 [Zhu, Dandan] East China Normal Univ, Inst AI Educ Shanghai, 3663 Zhongshan North Rd, Shanghai 200333, Peoples R China.
   [Zhu, Dandan; Yang, Xiaokang] Shanghai Jiao Tong Univ, Key Lab Artificial Intelligence, Minist Educ, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Shao, Xuan] Donghua Univ, Sch Comp Sci & Technol, 2999 Renmin North Rd, Shanghai 201620, Peoples R China.
   [Zhou, Qiangqiang] Jiangxi Normal Univ, Sch Software, 99 Ziyang Ave, Nanchang 330022, Jiangxi, Peoples R China.
   [Min, Xiongkuo; Zhai, Guangtao] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
C3 East China Normal University; Shanghai Jiao Tong University; Donghua
   University; Jiangxi Normal University; Shanghai Jiao Tong University
RP Zhou, QQ (corresponding author), Jiangxi Normal Univ, Sch Software, 99 Ziyang Ave, Nanchang 330022, Jiangxi, Peoples R China.
EM ddzhu@dhu.edu.cn; 1810553@tongji.edu.cn; zqqcsu@gmail.com;
   minxiongkuo@sjtu.edu.cn; zhaiguangtao@sjtu.edu.cn; xkyang@sjtu.edu.cn
RI Zhou, Qiangqiang/AAF-9803-2019; Zhai, Guangtao/X-5949-2019
OI Zhai, Guangtao/0000-0001-8165-9322
FU Fundamental Research Funds for the Central Universities; foundation of
   Key Laboratory of Artificial Intelligence, Ministry of Education, P.R.
   China; National Natural Science Foundation of China [62001289]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities and the foundation of Key Laboratory of
   Artificial Intelligence, Ministry of Education, P.R. China and the
   National Natural Science Foundation of China under Grant 62001289.
CR Andrew G., 2013, ICML, P1247
   [Anonymous], 2013, 2013 47th Annual Conference on Information Sciences and Systems (CISS)
   Aytar Y, 2016, ADV NEUR IN, V29
   Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Boccignone G., 2018, P EUROPEAN C COMPUTE
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chen YX, 2014, IEEE T CIRC SYST VID, V24, P1992, DOI 10.1109/TCSVT.2014.2329380
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cheng ZY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P655, DOI 10.1145/3077136.3080772
   Cheng ZY, 2016, ACM T INFORM SYST, V34, DOI 10.1145/2846092
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Coutrot A, 2016, SPRINGER SER COG NEU, V10, P291, DOI 10.1007/978-1-4939-3435-5_16
   Coutrot A, 2015, EUR SIGNAL PR CONF, P1531, DOI 10.1109/EUSIPCO.2015.7362640
   Coutrot A, 2014, IEEE IMAGE PROC, P1100, DOI 10.1109/ICIP.2014.7025219
   Coutrot A, 2014, J VISION, V14, DOI 10.1167/14.8.5
   Coutrot A, 2012, J EYE MOVEMENT RES, V5
   Dang T, 2018, IEEE INT CONF ROBOT, P2526, DOI 10.1109/ICRA.2018.8460992
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Fan HH, 2019, AAAI CONF ARTIF INTE, P8263
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Hadizadeh H, 2014, IEEE T IMAGE PROCESS, V23, P19, DOI 10.1109/TIP.2013.2282897
   He S, 2019, PROC CVPR IEEE, P10198, DOI 10.1109/CVPR.2019.01045
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Koutras P, 2015, SIGNAL PROCESS-IMAGE, V38, P15, DOI 10.1016/j.image.2015.08.004
   Kovács G, 2019, ADV ROBOTICS, V33, P520, DOI 10.1080/01691864.2019.1602564
   Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Kummerer M., 2014, ARXIV14111045
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Linardos P., 2019, P BRIT MACH VIS C, P1
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Majd M, 2019, APPL INTELL, V49, P2515, DOI 10.1007/s10489-018-1395-8
   Marat Sophie, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P1784
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Min XK, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996463
   Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Pan Junting, 2017, P IEEE C COMPUTER VI
   Rahman T., 2021, NeurIPS, P9774
   Ratajczak Remi, 2016, P 1 INT C APPL SYST
   Rebuffi S.-A., 2020, P IEEE CVF C COMP VI, P8836, DOI [10.1109/CVPR42600.2020.00886, DOI 10.1109/CVPR42600.2020.00886]
   Ruesch J, 2008, IEEE INT CONF ROBOT, P962, DOI 10.1109/ROBOT.2008.4543329
   Schauerte B, 2011, IEEE INT C INT ROBOT, P1173, DOI 10.1109/IROS.2011.6048857
   Schorkhuber C., 2010, P 7 SOUND MUS COMP C, P3
   Shao Y, 2009, INT CONF ACOUST SPEE, P4625, DOI 10.1109/ICASSP.2009.4960661
   Sharma J, 2020, INTERSPEECH, P1186, DOI 10.21437/Interspeech.2020-1303
   Sidaty N, 2017, NEUROCOMPUTING, V259, P94, DOI 10.1016/j.neucom.2016.08.130
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Song GH, 2013, J EYE MOVEMENT RES, V6
   Tsiami A, 2020, PROC CVPR IEEE, P4765, DOI 10.1109/CVPR42600.2020.00482
   Tsiami A, 2019, SIGNAL PROCESS-IMAGE, V76, P186, DOI 10.1016/j.image.2019.05.001
   Van der Burg E, 2008, J EXP PSYCHOL HUMAN, V34, P1053, DOI 10.1037/0096-1523.34.5.1053
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Y., 2018, P INT C LEARN REPR
   Yamada K, 2011, LECT NOTES COMPUT SC, V7087, P277, DOI 10.1007/978-3-642-25367-6_25
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yao SY, 2021, IEEE IMAGE PROC, P1604, DOI 10.1109/ICIP42928.2021.9506089
   Yuan X, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P1028, DOI 10.1109/ROBIO.2018.8664750
   Zeng DH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3387164
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhu DY, 2021, PETROL SCI, V18, P479, DOI 10.1007/s12182-020-00535-w
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 80
TC 4
Z9 4
U1 4
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2023
VL 19
IS 4
AR 147
DI 10.1145/3576857
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FX8
UT WOS:001011937600009
DA 2024-07-18
ER

PT J
AU Fu, ZL
   Xie, HT
   Fang, SC
   Wang, YX
   Xing, MT
   Zhang, YD
AF Fu, Zilong
   Xie, Hongtao
   Fang, Shancheng
   Wang, Yuxin
   Xing, Mengting
   Zhang, Yongdong
TI Learning Pixel Affinity Pyramid for Arbitrary-Shaped Text Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Scene text detection; pixel affinity; deep learning; instance
   segmentation
AB Arbitrary-shaped text detection in natural images is a challenging task due to the complexity of the background and the diversity of text properties. The difficulty lies in two aspects: accurate separation of adjacent texts and sufficient text feature representation. To handle these problems, we consider text detection as instance segmentation and propose a novel text detection framework, which jointly learns semantic segmentation and a pixel affinity pyramid in a unified fully convolutional network. Specifically, the pixel affinity pyramid is proposed to encode multi-scale instance affiliation relationships of pixels, which is not only robust to varying shapes of text but also provides an accurate boundary description for separating closely located texts. In the inference phase, a simple but effective post-processing is presented to reconstruct text instances from the semantic segmentation results under the guidance of the learned pixel affinity pyramid, achieving good accuracy and efficiency. Furthermore, to enhance the representation of text features in the neural network, two modules - the Region Enhancement Module (REM) and Attentional Fusion Module (AFM) - are proposed. The REM models the semantic correlations of regional features to enhance the features from the text area, which effectively suppresses false-positive detection. The AFM adaptively fuses multi-scale textual information through an attention mechanism to obtain abundant text semantic features, which benefits multi-sized text detection. Extensive ablation experiments are conducted demonstrating the effectiveness of the REM and AFM. Evaluation results on standard benchmarks, including Total-Text, ICDAR2015, SCUT-CTW1500, and MSRA-TD500, show that our method surpasses most existing text detectors and achieves state-of-the-art performance, denoting its superior capability in detecting arbitrary-shaped texts.
C1 [Fu, Zilong; Xie, Hongtao; Fang, Shancheng; Wang, Yuxin; Xing, Mengting; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, POB 1212, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xie, HT (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, POB 1212, Hefei 230026, Anhui, Peoples R China.
EM JeromeF@mail.ustc.edu.cn; htxie@ustc.edu.cn; fangshancheng@iie.ac.cn;
   wangyx58@mail.ustc.edu.cn; metingx@mail.ustc.edu.cn; zhyd73@ustc.edu.cn
OI Wang, Yuxin/0000-0002-0228-6220
FU National Nature Science Foundation of China [62121002, 62022076,
   U1936210]; Fundamental Research Funds for the Central Universities
   [WK3480000011]; Youth Innovation Promotion Association Chinese Academy
   of Sciences [Y2021122]
FX This work is supported by the National Nature Science Foundation of
   China (grant nos. 62121002, 62022076, and U1936210), the Fundamental
   Research Funds for the Central Universities (grant no. WK3480000011),
   and the Youth Innovation Promotion Association Chinese Academy of
   Sciences (grant no. Y2021122).
CR Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Deng Dan, 2018, AAAI CONF ARTIF INTE
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Fang SC, 2021, PROC CVPR IEEE, P7094, DOI 10.1109/CVPR46437.2021.00702
   Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao NY, 2019, IEEE I CONF COMP VIS, P642, DOI 10.1109/ICCV.2019.00073
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang WL, 2013, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2013.157
   Huang ZD, 2019, IEEE WINT CONF APPL, P764, DOI 10.1109/WACV.2019.00086
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kang C, 2017, AAAI CONF ARTIF INTE, P4103
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Li ZJ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3362988
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Liu YD, 2018, LECT NOTES COMPUT SC, V11207, P708, DOI 10.1007/978-3-030-01219-9_42
   Liu ZY, 2019, ACM T ALGORITHMS, V15, DOI 10.1145/3264434
   Liu ZC, 2018, Arxiv, DOI arXiv:1805.08365
   Liu ZC, 2019, PROC CVPR IEEE, P7261, DOI 10.1109/CVPR.2019.00744
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mei T, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071402
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   Wang FF, 2018, PROC CVPR IEEE, P1381, DOI 10.1109/CVPR.2018.00150
   Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43
   Wang PF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1277, DOI 10.1145/3343031.3350988
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y., 2020, IEEE T MULTIMEDIA
   Wang YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14174, DOI 10.1109/ICCV48922.2021.01393
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue CH, 2019, Arxiv, DOI arXiv:1901.02596
   Yao C, 2016, Arxiv, DOI arXiv:1606.09002
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye JX, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207049
   Yuliang L, 2017, Arxiv, DOI arXiv:1712.02170
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang L, 2020, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR42600.2020.00378
   Zhang SY, 2019, Arxiv, DOI arXiv:1905.11634
   Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhou YZ, 2017, PROC CVPR IEEE, P4961, DOI 10.1109/CVPR.2017.527
NR 65
TC 1
Z9 1
U1 3
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
DI 10.1145/3524617
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800004
DA 2024-07-18
ER

PT J
AU Gao, HH
   Dai, BB
   Miao, HK
   Yang, XX
   Barroso, RJD
   Walayat, H
AF Gao, Honghao
   Dai, Baobin
   Miao, Huaikou
   Yang, Xiaoxian
   Barroso, Ramon J. Duran
   Walayat, Hussain
TI A Novel GAPG Approach to Automatic Property Generation for Formal
   Verification: The GAN Perspective
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Model checking; verification property; generative adversarial network
   (GAN); automatic property generation; computational tree logic;
   correctness and reliability
ID ADVERSARIAL NETWORKS
AB Formal methods have been widely used to support software testing to guarantee correctness and reliability. For example, model checking technology attempts to ensure that the verification property of a specific formal model is satisfactory for discovering bugs or abnormal behavior from the perspective of temporal logic. However, because automatic approaches are lacking, a software developer/tester must manually specify verification properties. A generative adversarial network (GAN) learns features from input training data and outputs new data with similar or coincident features. GANs have been successfully used in the image processing and text processing fields and achieved interesting and automatic results. Inspired by the power of GANs, in this article, we propose a GAN-based automatic property generation (GAPG) approach to generate verification properties supporting model checking. First, the verification properties in the form of computational tree logic (CTL) are encoded and used as input to the GAN. Second, we introduce regular expressions as grammar rules to check the correctness of the generated properties. These rules work to detect and filter meaningless properties that occur because the GAN learning process is uncontrollable and may generate unsuitable properties in real applications. Third, the learning network is further trained by using labeled information associated with the input properties. These are intended to guide the training process to generate additional new properties, particularly those that map to corresponding formal models. Finally, a series of comprehensive experiments demonstrate that the proposed GAPG method can obtain new verification properties from two aspects: (1) using only CTL formulas and (2) using CTL formulas combined with Kripke structures.
C1 [Gao, Honghao; Dai, Baobin; Miao, Huaikou] Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
   [Yang, Xiaoxian] Shanghai Polytech Univ, Sch Comp & Informat Engn, Shanghai 201209, Peoples R China.
   [Barroso, Ramon J. Duran] Univ Valladolid, Fac Telecommun Engn, Valladolid 47002, Spain.
   [Walayat, Hussain] Univ Technol Sydney, Fac Engn & IT, Sydney, NSW 2007, Australia.
C3 Shanghai University; Shanghai Polytechnic University; Universidad de
   Valladolid; University of Technology Sydney
RP Yang, XX (corresponding author), Shanghai Polytech Univ, Sch Comp & Informat Engn, Shanghai 201209, Peoples R China.
EM gaohonghao@shu.edu.cn; dai_stu1995@shu.edu.cn; hkmiao@shu.edu.cn;
   xxyang@sspu.edu.cn; rduran@tel.uva.es; Walayat.Hussain@uts.edu.au
RI Gao, Honghao/AAX-4529-2020; Hussain, Walayat/HIZ-8975-2022; Durán
   Barroso, Ramón J./L-1969-2014
OI Gao, Honghao/0000-0001-6861-9684; Hussain, Walayat/0000-0003-0610-4006;
   Durán Barroso, Ramón J./0000-0003-1423-1646
FU National Natural Science Foundation of China [61902236]
FX The research was supported by the National Natural Science Foundation of
   China (No. 61902236).
CR Abate A, 2021, IEEE CONTR SYST LETT, V5, P773, DOI 10.1109/LCSYS.2020.3005328
   Akram Adeel, 2018, 2018 IEEE 4th International Conference on Computer and Communications (ICCC). Proceedings, P1483, DOI 10.1109/CompComm.2018.8780648
   [Anonymous], 2018, IEEE Standard 1709-2018 (Revision of IEEE Std 1709-2010, P1, DOI [10.1109/IEEESTD.2018.8332112, DOI 10.1109/IEEESTD.2018.8332112]
   [Anonymous], 2009, IEEE STD 1159 2009 R, pc1, DOI [DOI 10.1109/IEEESTD.2019.8796486, 10.1109/IEEESTD.2019.8637988]
   Baier C, 2008, PRINCIPLES OF MODEL CHECKING, P1
   Bird C, 2014, 36TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE COMPANION 2014), P205, DOI 10.1145/2591062.2591173
   Cazenave T, 2012, IEEE T COMP INTEL AI, V4, P68, DOI 10.1109/TCIAIG.2011.2180723
   Cui J, 2018, IEEE T RELIAB, V67, P481, DOI 10.1109/TR.2018.2806349
   Gao HH, 2018, INT J SOFTW ENG KNOW, V28, P1369, DOI 10.1142/S0218194018500390
   Gao HH, 2017, INT J SOFTW ENG KNOW, V27, P897, DOI 10.1142/S0218194017500334
   Gao HH, 2013, APPL MATH INFORM SCI, V7, P139, DOI 10.12785/amis/071L20
   Gao X, 2020, IEEE T IMAGE PROCESS, V29, P8706, DOI 10.1109/TIP.2020.3018856
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   He SY, 2019, IEEE T PLASMA SCI, V47, P1878, DOI 10.1109/TPS.2019.2904796
   Huszar F, 2015, Arxiv, DOI arXiv:1511.05101
   Khalil K, 2019, IEEE T CIRCUITS-II, V66, P1885, DOI 10.1109/TCSII.2019.2924663
   Lei XY, 2019, IEEE ACCESS, V7, P124087, DOI 10.1109/ACCESS.2019.2927169
   Li L, 2018, IEEE T SOFTWARE ENG, V44, P725, DOI 10.1109/TSE.2017.2712621
   Li T, 2018, IEEE INT CONF BIG DA, P5375, DOI 10.1109/BigData.2018.8622140
   Lv Z, 2019, IEEE ACCESS, V7, P14947, DOI 10.1109/ACCESS.2019.2892649
   Mili S, 2019, IEEE SYST J, V13, P3989, DOI 10.1109/JSYST.2019.2923818
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Nanda SP, 2019, 2019 IEEE 2ND INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT), P296, DOI [10.1109/infoct.2019.8711369, 10.1109/INFOCT.2019.8711369]
   Nardone V, 2019, IEEE SYST J, V13, P1018, DOI 10.1109/JSYST.2018.2793665
   Naseer M, 2020, DES AUT TEST EUROPE, P666, DOI 10.23919/DATE48585.2020.9116247
   Peng YQ, 2020, IEEE ACCESS, V8, P3987, DOI 10.1109/ACCESS.2019.2961767
   Qian Yichen, 2019, [Journal of Communications and Information Networks, 通信与信息网络学报], V4, P30
   Phan QS, 2015, PROCEEDINGS 10TH INTERNATIONAL CONFERENCE ON AVAILABILITY, RELIABILITY AND SECURITY ARES 2015, P100, DOI 10.1109/ARES.2015.14
   Reid S., 2013, ISO/IEC/IEEE 29119-1
   Rivieccio U, 2017, J LOGIC COMPUT, V27, P155, DOI 10.1093/logcom/exv038
   Rössig A, 2021, J GLOBAL OPTIM, V81, P109, DOI 10.1007/s10898-020-00949-1
   Sakib Kazi, 2013, VERIFICATION COMMUNI
   Segura S, 2016, IEEE T SOFTWARE ENG, V42, P805, DOI 10.1109/TSE.2016.2532875
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sultana S, 2017, IEEE ACCESS, V5, P14455, DOI 10.1109/ACCESS.2017.2728860
   Tuan YL, 2019, IEEE-ACM T AUDIO SPE, V27, P788, DOI 10.1109/TASLP.2019.2896437
   Venzke A, 2021, IEEE T SMART GRID, V12, P383, DOI 10.1109/TSG.2020.3009401
   Yang Y, 2020, IEEE ACCESS, V8, P105217, DOI 10.1109/ACCESS.2020.2993928
   Yin SH, 2018, 2018 NINTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME 2018), P852, DOI 10.1109/ITME.2018.00191
   Yu LT, 2017, AAAI CONF ARTIF INTE, P2852
   Zhang MS, 2018, IEEE INT CONF AUTOM, P132, DOI 10.1145/3238147.3238187
NR 42
TC 13
Z9 13
U1 3
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 16
DI 10.1145/3517154
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400016
DA 2024-07-18
ER

PT J
AU Sharma, P
   Bisht, I
   Sur, A
AF Sharma, Prasen
   Bisht, Ira
   Sur, Arijit
TI Wavelength-based Attributed Deep Neural Network for Underwater Image
   Restoration
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image restoration; underwater vision; enhancement; super-resolution;
   deep learning
ID ENHANCEMENT
AB Background: Underwater images, in general, suffer from low contrast and high color distortions due to the non-uniform attenuation of the light as it propagates through the water. In addition, the degree of attenuation varies with the wavelength, resulting in the asymmetric traversing of colors. Despite the prolific works for underwater image restoration (UIR) using deep learning, the above asymmetricity has not been addressed in the respective network engineering.
   Contributions: As the first novelty, this article shows that attributing the right receptive field size (context) based on the traversing range of the color channel may lead to a substantial performance gain for the task of UIR. Further, it is important to suppress the irrelevant multi-contextual features and increase the representational power of the model. Therefore, as a second novelty, we have incorporated an attentive skip mechanism to adaptively refine the learned multi-contextual features. The proposed framework, called Deep WaveNet, is optimized using the traditional pixel-wise and feature-based cost functions. An extensive set of experiments have been carried out to show the efficacy of the proposed scheme over existing best-published literature on benchmark datasets. More importantly, we have demonstrated a comprehensive validation of enhanced images across various high-level vision tasks, e.g., underwater image semantic segmentation and diver's 2D pose estimation. A sample video to exhibit our real-world performance is available at https://tinyurl.com/yzcrup9n. Also, we have open-sourced our framework at https://github.com/pksvision/Deep-WaveNet-UnderwaterImage- Restoration.
C1 [Sharma, Prasen; Bisht, Ira; Sur, Arijit] Indian Inst Technol Guwahati, Dept Comp Sci & Engn, Multimedia Lab, Gauhati, Assam, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Guwahati
RP Sharma, P (corresponding author), Indian Inst Technol Guwahati, Dept Comp Sci & Engn, Multimedia Lab, Gauhati, Assam, India.
EM kumar176101005@iitg.ac.in; ibisht@iitg.ac.in; arijit@iitg.ac.in
RI Sur, Arijit/AAB-4216-2020
OI Sur, Arijit/0000-0002-9038-8138
FU IITG Technology Innovation and Development Foundation (IITGTI); IITG
   Technology Innovation and Development Foundation (DF); Department of
   Science and Technology, India [DST/NMICPS/TIH12/IITG/2020]; Department
   of Biotechnology, Govt. of India [BT/COE/34/SP28408/2018]
FX This work is supported by IITG Technology Innovation and Development
   Foundation (IITGTI & DF), which has been set up at IIT Guwahati as a
   part of the National Mission on Interdisciplinary Cyber Physical Systems
   (NMICPS). IITGTI & DF is undertaking research, development, and training
   activities on Technologies for Under Water Exploration with financial
   assistance from the Department of Science and Technology, India, through
   grant number DST/NMICPS/TIH12/IITG/2020. The authors gratefully
   acknowledge the support provided for the present work. We also
   acknowledge the Department of Biotechnology, Govt. of India, for the
   financial support for the project BT/COE/34/SP28408/2018 (for computing
   resources).
CR Alexandridis AK, 2013, NEURAL NETWORKS, V42, P1, DOI 10.1016/j.neunet.2013.01.008
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   [Anonymous], 2022, 10 INT C LEARN UNPUB
   Bai LF, 2020, IEEE ACCESS, V8, P128973, DOI 10.1109/ACCESS.2020.3009161
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chau L., 2017, 2017 IEEE INT S CIRC, P1, DOI [10.1109/ISCAS.2017.8050994, DOI 10.1109/ISCAS.2017.8050994]
   Chen L, 2021, IEEE T CIRC SYST VID, V31, P3078, DOI 10.1109/TCSVT.2020.3035108
   Chen YZ, 2020, IEEE ACCESS, V8, P117759, DOI 10.1109/ACCESS.2020.3004141
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Divakar N, 2017, IEEE COMPUT SOC CONF, P1076, DOI 10.1109/CVPRW.2017.145
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dudhane A, 2020, IEEE SIGNAL PROC LET, V27, P675, DOI 10.1109/LSP.2020.2988590
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   Han RY, 2020, IEEE ACCESS, V8, P218838, DOI 10.1109/ACCESS.2020.3041280
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hou GJ, 2020, IEEE ACCESS, V8, P122078, DOI 10.1109/ACCESS.2020.3006359
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Islam MJ, 2020, IEEE INT CONF ROBOT, P900, DOI [10.1109/ICRA40945.2020.9197213, 10.1109/icra40945.2020.9197213]
   Islam MJ, 2020, IEEE INT C INT ROBOT, P1769, DOI 10.1109/IROS45743.2020.9340821
   Islam MJ, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P, 2015, International Conference on Learning Representations
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Li CY, 2016, INT CONF ACOUST SPEE, P1731, DOI 10.1109/ICASSP.2016.7471973
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li YJ, 2019, IEEE ACCESS, V7, P83721, DOI 10.1109/ACCESS.2019.2925209
   Liu D, 2019, IEEE T IMAGE PROCESS, V28, P4401, DOI 10.1109/TIP.2019.2908802
   Liu D, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P842
   Liu P, 2019, IEEE ACCESS, V7, P94614, DOI 10.1109/ACCESS.2019.2928976
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu XD, 2020, IEEE GEOSCI REMOTE S, V17, P1488, DOI 10.1109/LGRS.2019.2950056
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu HM, 2017, IEEE ACCESS, V5, P670, DOI 10.1109/ACCESS.2017.2648845
   Ma JX, 2018, INT J PATTERN RECOGN, V32, DOI 10.1142/S0218001418540186
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Paszke Adam, 2019, ADVANCES
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Peng YT, 2015, IEEE IMAGE PROC, P4952, DOI 10.1109/ICIP.2015.7351749
   Sharma PK, 2020, IEEE WINT CONF APPL, P2344, DOI [10.1109/wacv45572.2020.9093528, 10.1109/WACV45572.2020.9093528]
   Sharma PK, 2019, IEEE IMAGE PROC, P2796, DOI [10.1109/icip.2019.8803353, 10.1109/ICIP.2019.8803353]
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JH, 2019, IEEE ACCESS, V7, P145199, DOI 10.1109/ACCESS.2019.2945576
   Wang J, 2020, IEEE ACCESS, V8, P130719, DOI 10.1109/ACCESS.2020.3003351
   Wang L, 2020, PROC CVPR IEEE, P3773, DOI 10.1109/CVPR42600.2020.00383
   Wang N, 2017, IEEE ACCESS, V5, P18941, DOI 10.1109/ACCESS.2017.2753796
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Wang SQ, 2015, IEEE SIGNAL PROC LET, V22, P2387, DOI 10.1109/LSP.2015.2487369
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2016, PROC CVPR IEEE, P4792, DOI 10.1109/CVPR.2016.518
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wen HC, 2013, IEEE INT SYMP CIRC S, P753, DOI 10.1109/ISCAS.2013.6571956
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu J, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1
   Yang J., 2017, 2017 IEEE INT C IMAG, DOI [10.1109/ICIP.2017.8296473, DOI 10.1109/ICIP.2017.8296473]
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yang SD, 2019, IEEE ACCESS, V7, P165318, DOI 10.1109/ACCESS.2019.2953463
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yu F., 2015, ARXIV
   Yu K, 2016, Arxiv, DOI arXiv:1608.02778
   Yu QH, 2021, PROC CVPR IEEE, P1154, DOI 10.1109/CVPR46437.2021.00121
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 89
TC 26
Z9 26
U1 7
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 2
DI 10.1145/3511021
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, S
   Ben, HX
   Hao, YB
   He, XN
   Wang, M
AF Wang, Shuo
   Ben, Huixia
   Hao, Yanbin
   He, Xiangnan
   Wang, Meng
TI Boosting Hyperspectral Image Classification with Dual Hierarchical
   Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Hyperspectral image classification; hierarchical learning; pseudo label
ID INFORMATION; SUPERPIXEL; NETWORK
AB Hyperspectral image (HSI) classification aims at predicting the pixel-wise labels in an image, where there are only a few labeled pixel samples (hard labels) for training. It is a challenging task since the classification process is susceptible to over-fitting under training with limited samples. To relieve this problem, we propose a method based on dual hierarchical learning. First, we employ a connectionist hyperspectral convolution (HC) network to capture the representations of the pixels from different receptive fields. Specifically, an HC is designed to learn the correlation among adjacent pixels and is further extended to a connectionist hierarchical structure. These operations use the correlation to enhance one-pixel learning from multiple receptive fields. Second, we analyze the properties in the hyperspectral image and introduce a hierarchical pseudo label generation algorithm to enrich the supervision of the label information. Finally, we design a dual hierarchical learning strategy to help all HC layers learn from both the hard labels and the hierarchical pseudo labels. In other words, it addresses the HSI classification problem from different views. For inference, we employ two fusion strategies to find a better prediction. The experimental results on four popular HSI benchmarks, i.e., Salinas-A, IndianPines, PaviaU, and PaviaC, demonstrate the effectiveness of the proposed method. Our code is publicly available on GitHub: https://github.com/ShuoWangCS/HSI-DHL.
C1 [Wang, Shuo; Hao, Yanbin; He, Xiangnan] Univ Sci & Technol China, Sch Data Sci, Sch Informat Sci & Technol, Hefei 230601, Peoples R China.
   [Ben, Huixia; Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Sch Artificial Intelligence, Hefei, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Hefei University of Technology
RP Wang, M (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Sch Artificial Intelligence, Hefei, Peoples R China.
EM shuowang.hfut@gmail.com; huixiaben@mail.hfut.edu.cn;
   haoyanbin@hotmail.com; xiangnanhe@gmail.com; eric.mengwang@gmail.com
RI Hao, Yanbin/AAC-8050-2019; Wang, Meng/ITR-8699-2023; Yuan,
   Yi/KGL-5178-2024
OI Hao, Yanbin/0000-0002-0695-1566; 
FU National Nature Science Foundation of China (NSFC) [62101524];
   University Synergy Innovation Program of Anhui Province [GXXT-2021-008]
FX This work is supported by the National Nature Science Foundation of
   China (NSFC) under grants 62101524 and The University Synergy Innovation
   Program of Anhui Province GXXT-2021-008.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Appice A, 2019, ISPRS J PHOTOGRAMM, V147, P215, DOI 10.1016/j.isprsjprs.2018.11.023
   Appice A, 2017, PATTERN RECOGN, V63, P229, DOI 10.1016/j.patcog.2016.10.010
   Appice A, 2016, MACH LEARN, V103, P343, DOI 10.1007/s10994-016-5559-7
   Camps-Valls G, 2007, IEEE T GEOSCI REMOTE, V45, P3044, DOI 10.1109/TGRS.2007.895416
   Castelluccio M, 2017, JOINT URB REMOTE SEN
   Chang CI, 2000, IEEE T INFORM THEORY, V46, P1927, DOI 10.1109/18.857802
   Cui BE, 2020, INT J REMOTE SENS, V41, P6157, DOI 10.1080/01431161.2020.1736730
   Delle Piane C., 2019, 6 EAGE SHALE WORKSHO, V2019, P1
   Dópido I, 2013, IEEE T GEOSCI REMOTE, V51, P4032, DOI 10.1109/TGRS.2012.2228275
   Du Q, 2003, PATTERN RECOGN, V36, P1, DOI 10.1016/S0031-3203(02)00065-1
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Gao Y, 2014, IEEE T IMAGE PROCESS, V23, P2769, DOI 10.1109/TIP.2014.2319735
   Gonzalez Santiago J., 2020, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, V43, P407
   Green RO, 1998, REMOTE SENS ENVIRON, V65, P227, DOI 10.1016/S0034-4257(98)00064-9
   Hang RL, 2021, IEEE T GEOSCI REMOTE, V59, P1424, DOI 10.1109/TGRS.2020.3003341
   Harris JR, 2005, CAN J EARTH SCI, V42, P2173, DOI 10.1139/E05-064
   Haut J., 2017, P 17 INT C COMP MATH, P1063, DOI DOI 10.1109/JMASS.2020.3019669
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He MY, 2017, IEEE IMAGE PROC, P3904, DOI 10.1109/ICIP.2017.8297014
   Hecker C, 2008, IEEE T GEOSCI REMOTE, V46, P4162, DOI 10.1109/TGRS.2008.2001035
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiao LC, 2017, IEEE T GEOSCI REMOTE, V55, P5585, DOI 10.1109/TGRS.2017.2710079
   Kingma Diederik P., 2015, P 3 INT C LEARN
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kunkel B., 1988, Proceedings of the SPIE - The International Society for Optical Engineering, V868, P134, DOI 10.1117/12.943611
   Lee H, 2017, IEEE T IMAGE PROCESS, V26, P4843, DOI 10.1109/TIP.2017.2725580
   Li ST, 2018, IEEE T IMAGE PROCESS, V27, P4118, DOI 10.1109/TIP.2018.2836307
   Liang L, 2015, REMOTE SENS ENVIRON, V165, P123, DOI 10.1016/j.rse.2015.04.032
   Liang Y, 2020, IEEE GEOSCI REMOTE S, V17, P1042, DOI 10.1109/LGRS.2019.2939356
   Liu B, 2019, IEEE T GEOSCI REMOTE, V57, P2290, DOI 10.1109/TGRS.2018.2872830
   Liu B, 2018, IEEE T GEOSCI REMOTE, V56, P1909, DOI 10.1109/TGRS.2017.2769673
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo HW, 2018, Arxiv, DOI arXiv:1810.12563
   Ma L, 2010, IEEE T GEOSCI REMOTE, V48, P4099, DOI 10.1109/TGRS.2010.2055876
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Paoletti ME, 2020, J SUPERCOMPUT, V76, P8866, DOI 10.1007/s11227-020-03187-0
   Roy SK, 2021, IEEE T GEOSCI REMOTE, V59, P7831, DOI 10.1109/TGRS.2020.3043267
   Roy SK, 2020, IEEE GEOSCI REMOTE S, V17, P277, DOI 10.1109/LGRS.2019.2918719
   Shen Y, 2021, IEEE T GEOSCI REMOTE, V59, P6029, DOI 10.1109/TGRS.2020.3014286
   Tan K, 2015, ISPRS J PHOTOGRAMM, V105, P19, DOI 10.1016/j.isprsjprs.2015.03.006
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Villa A, 2011, IEEE T GEOSCI REMOTE, V49, P4865, DOI 10.1109/TGRS.2011.2153861
   Wang LG, 2014, ISPRS J PHOTOGRAMM, V97, P123, DOI 10.1016/j.isprsjprs.2014.08.016
   Zhang J, 2018, PROC CVPR IEEE, P9029, DOI 10.1109/CVPR.2018.00941
   Zhang LF, 2012, IEEE T GEOSCI REMOTE, V50, P879, DOI 10.1109/TGRS.2011.2162339
   Zheng Z, 2020, IEEE T GEOSCI REMOTE, V58, P5612, DOI 10.1109/TGRS.2020.2967821
   Zhong P, 2010, IEEE T IMAGE PROCESS, V19, P1890, DOI 10.1109/TIP.2010.2045034
   Zhong ZL, 2018, IEEE T GEOSCI REMOTE, V56, P847, DOI 10.1109/TGRS.2017.2755542
   Zhou F, 2019, IEEE J-STARS, V12, P1549, DOI 10.1109/JSTARS.2019.2910990
   Zhou YC, 2016, IEEE T CYBERNETICS, V46, P1667, DOI 10.1109/TCYB.2015.2453359
   Zhu QQ, 2022, IEEE T CYBERNETICS, V52, P11709, DOI 10.1109/TCYB.2021.3070577
   Zou L, 2020, IEEE J-STARS, V13, P659, DOI 10.1109/JSTARS.2020.2968179
NR 53
TC 1
Z9 2
U1 3
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 21
DI 10.1145/3522713
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400021
DA 2024-07-18
ER

PT J
AU Tian, Y
   Zhang, YJ
   Chen, WG
   Liu, DS
   Wang, HY
   Xu, HY
   Han, JF
   Ge, YW
AF Tian, Yan
   Zhang, Yujie
   Chen, Wei-Gang
   Liu, Dongsheng
   Wang, Huiyan
   Xu, Huayi
   Han, Jianfeng
   Ge, Yiwen
TI 3D Tooth Instance Segmentation Learning Objectness and Affinity in Point
   Cloud
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Computer vision; deep learning; objectness; instance segmentation
ID NETWORK
AB Digital dentistry has received more attention in the past decade. However, current deep learning-based methods still encounter difficult challenges. The proposal-based methods are sensitive to the localization results due to the lack of local cues, while the proposal-free methods have poor clustering outputs because of the affinity measured by the low-level characteristics, especially in situations of tightly arranged teeth. In this article, we present a novel proposal-based approach to combine objectness and pointwise knowledge in an attention mechanism for point cloud-based tooth instance segmentation, using local information to improve 3D proposal generation and measuring the importance of local points by calculating the center distance. We evaluate the performance of our approach by constructing a Shining3D tooth instance segmentation dataset. The experimental results verify that our approach gives competitive results when compared with the other available approaches.
C1 [Tian, Yan; Zhang, Yujie; Chen, Wei-Gang; Liu, Dongsheng; Wang, Huiyan] Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, 18 Xuezheng Rd, Hangzhou 310018, Peoples R China.
   [Xu, Huayi] Shining3D Tech Co Ltd, 1398 Xiangbin Rd, Hangzhou 311200, Peoples R China.
   [Han, Jianfeng] Zhejiang Xinwangzhen Tech Inc, 1186 Binan Rd, Hangzhou 310051, Peoples R China.
   [Ge, Yiwen] Univ Birmingham, Sch Comp Sci, 135 Quinto Rd, Birmingham B15 2TT, W Midlands, England.
C3 Zhejiang Gongshang University; University of Birmingham
RP Wang, HY (corresponding author), Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, 18 Xuezheng Rd, Hangzhou 310018, Peoples R China.
EM tianyan@zjgsu.edu.cn; wawade3@qq.com; wgchen_gsu@zjgsu.edu.cn;
   lds1118@zjgsu.edu.cn; WangHuiyan0102@163.com; xhyzju@qq.com;
   jeff@inteast.com; ntxfgyw@163.com
RI liu, dong/GRJ-9115-2022; Zhang, Kai/KBD-3312-2024; Wang,
   Huiyan/JXW-9178-2024; Liu, DY/JPL-4171-2023; liu,
   dongsheng/IWM-1597-2023
FU National Natural Science Foundation of China [61972351]; National Key
   Research and Development Program of China [2018YFC0824406]; Natural
   Science Foundation of Zhejiang Province [LY19F030005]; Public Welfare
   Technology Research Project of Zhejiang Province [LGF19G010002,
   LGF20G010002, LGG20F020005]; Science and Technology Program of Zhejiang
   Province (Key Research and Development Plan) [2021C01120]; Opening
   Foundation of State Key Laboratory of Virtual Reality Technology and
   System of Beihang University [VRLAB2020B15]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61972351, in part by the National Key
   Research and Development Program of China under Grant 2018YFC0824406, in
   part by the Natural Science Foundation of Zhejiang Province under Grant
   LY19F030005, in part by the Public Welfare Technology Research Project
   of Zhejiang Province under Grant LGF19G010002, LGF20G010002 and
   LGG20F020005, in part by the the Science and Technology Program of
   Zhejiang Province (Key Research and Development Plan) under Grant
   2021C01120, in part by the Opening Foundation of State Key Laboratory of
   Virtual Reality Technology and System of Beihang University under Grant
   VRLAB2020B15.
CR Chen Shaoyu., ICCV, P1368
   Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI [10.1109/iccv.2019.00987, 10.1109/ICCV.2019.00987]
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Cui ZM, 2019, PROC CVPR IEEE, P6361, DOI 10.1109/CVPR.2019.00653
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Du L, 2020, IEEE INT CONF ROBOT, P6868, DOI [10.1109/icra40945.2020.9197242, 10.1109/ICRA40945.2020.9197242]
   Engelmann F., 2020, CVPR, P9028, DOI 10.1109/CVPR42600.2020.00905
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Han L, 2020, PROC CVPR IEEE, P2937, DOI 10.1109/CVPR42600.2020.00301
   He Chenhang., CVPR, P11873
   He Tong., CVPR, P354
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Jiang L, 2020, PROC CVPR IEEE, P4866, DOI 10.1109/CVPR42600.2020.00492
   Lahoud J, 2019, IEEE I CONF COMP VIS, P9255, DOI 10.1109/ICCV.2019.00935
   Liang Zhihao., ICCV, P1256
   Liu C., 2019, Algorithms for Verifying Deep Neural Networks
   Liu Nali., 2020, SHINING3D DENT DATAS
   Liu ZJ, 2019, ADV NEUR IN, V32
   Paszke A, 2019, ADV NEUR IN, V32
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P68, DOI 10.1007/978-3-030-58589-1_5
   Pham QH, 2019, PROC CVPR IEEE, P8819, DOI 10.1109/CVPR.2019.00903
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shen Chunhua, ECCV, P564
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Tian Y, 2022, IEEE T INTELL TRANSP, V23, P165, DOI 10.1109/TITS.2020.3009000
   Tian Y, 2020, NEUROCOMPUTING, V417, P202, DOI 10.1016/j.neucom.2020.07.078
   Tian Y, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107158
   Tian Y, 2019, IEEE T INTELL TRANSP, V20, P4466, DOI 10.1109/TITS.2018.2886283
   Tian Y, 2019, J ARTIF INTELL RES, V64, P181, DOI 10.1613/jair.1.11338
   Tian Y, 2018, NEUROCOMPUTING, V318, P297, DOI 10.1016/j.neucom.2018.08.067
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Wang XL, 2019, PROC CVPR IEEE, P4091, DOI 10.1109/CVPR.2019.00422
   Wu Guangnan., 2020, ACCV, P1096
   Yang B, 2019, ADV NEUR IN, V32
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407
   Zhang Biao., CVPR, P8883
   Zhao L, 2020, AAAI CONF ARTIF INTE, V34, P12951
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 41
TC 15
Z9 17
U1 3
U2 47
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 105
DI 10.1145/3504033
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600016
DA 2024-07-18
ER

PT J
AU Natgunanathan, I
   Praitheeshan, P
   Gao, LX
   Xiang, Y
   Pan, L
AF Natgunanathan, Iynkaran
   Praitheeshan, Purathani
   Gao, Longxiang
   Xiang, Yong
   Pan, Lei
TI Blockchain-Based Audio Watermarking Technique for Multimedia Copyright
   Protection in Distribution Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Watermarking; blockchain; smart contract; copyright protection
ID SPREAD-SPECTRUM WATERMARKING; ROBUST; SCHEME; SECURE
AB Copyright protection in multimedia protection distribution is a challenging problem. To protect multimedia data, many watermarking methods have been proposed in the literature. However, most of them cannot be used effectively in a multimedia distribution network (MDN) as they are not designed to support multi-layer watermark embedding. Multi-layer watermarking mechanisms were developed to protect multimedia data across different layers in an MDN. However, in those mechanisms, we need to trust the entities in the MDN, such as regional and country distributors. To overcome this potential drawback, in this article, we propose a novel privacy protection mechanism for MDNs by combining the advantages of both blockchain and watermarking technologies. A specifically designed watermarking algorithm is used to link the copyright information with the audio file, while a novel blockchain-based smart contract mechanism is developed to enforce the proper functioning of each entity in the distribution network. Moreover, the new audio mechanism is computationally efficient. Although audio signals are used to show the effectiveness of the proposed mechanism, the proposed approach can easily be extended to other multimedia objects, such as an image. The validity of the proposed mechanism is demonstrated by our simulation results. The proposed mechanism can benefit multimedia production companies and other entities in the MDN.
C1 [Natgunanathan, Iynkaran; Praitheeshan, Purathani; Gao, Longxiang; Xiang, Yong; Pan, Lei] Deakin Univ, 221 Burwood Hwy, Burwood, Vic 3125, Australia.
C3 Deakin University
RP Natgunanathan, I (corresponding author), Deakin Univ, 221 Burwood Hwy, Burwood, Vic 3125, Australia.
EM iynkaran@deakin.edu.au; ppraithe@deakin.edu.au;
   longxiang.gao@deakin.edu.au; yxiang@deakin.edu.au; l.pan@deakin.edu.au
RI Pan, Lei/ADH-0321-2022; Pan, Lei/ITT-0556-2023; Gao,
   Longxiang/AAR-8605-2020
OI Pan, Lei/0000-0002-4691-8330; Pan, Lei/0000-0002-4691-8330; Gao,
   Longxiang/0000-0002-3026-7537
FU Australian Research Council [LP170100458]; Australian Research Council
   [LP170100458] Funding Source: Australian Research Council
FX This work was supported in part by the Australian Research Council under
   grant LP170100458.
CR Bhowmik D, 2017, INT CONF DIGIT SIG
   Chen OTC, 2008, IEEE T AUDIO SPEECH, V16, P629, DOI 10.1109/TASL.2007.913022
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Dannen C., 2017, INTRO ETHEREUM SOLID, DOI [10.1007/978-1-4842-2535-6, DOI 10.1007/978-1-4842-2535-6]
   Hu P, 2016, ELECTRON LETT, V52, P5, DOI 10.1049/el.2015.1508
   ITU-T R, 2001, METH OBJ MEAS PERC A
   Kalantari NK, 2009, IEEE T AUDIO SPEECH, V17, P1133, DOI 10.1109/TASL.2009.2019259
   Kang H, 2008, IEICE T INF SYST, VE91D, P2731, DOI 10.1093/ietisy/e91-d.11.2731
   Kim HJ, 2003, IEEE T CIRC SYST VID, V13, P885, DOI 10.1109/TCSVT.2003.815950
   Kirbiz Serap, 2006, P 2006 IEEE INT C AC, V5, P761
   Kirovski D, 2003, IEEE T SIGNAL PROCES, V51, P1020, DOI 10.1109/TSP.2003.809384
   Kishigami J, 2015, PROCEEDINGS 2015 IEEE FIFTH INTERNATIONAL CONFERENCE ON BIG DATA AND CLOUD COMPUTING BDCLOUD 2015, P187, DOI 10.1109/BDCloud.2015.60
   Ko BS, 2005, IEEE T MULTIMEDIA, V7, P212, DOI 10.1109/TMM.2005.843366
   Kushwaha SC, 2015, 2015 INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND SIGNAL PROCESSING (ICCSP), P303, DOI 10.1109/ICCSP.2015.7322893
   Lakshmi D., 2008, P 2008 IEEE REG 10 C, P1
   Liu Jianxing, 2021, MULTIMED TOOLS APPL, P1
   Liu Tianchi, 2011, 2011 7th International Conference on Networked Computing, P78
   Ma ZF, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0327-1
   Malvar HS, 2003, IEEE T SIGNAL PROCES, V51, P898, DOI 10.1109/TSP.2003.809385
   Menezes A. J., 1996, HDB APPL CRYPTOGRAPH, V1st
   Meng ZX, 2018, P INT COMP SOFTW APP, P359, DOI 10.1109/COMPSAC.2018.10258
   Natgunanathan I, 2017, IEEE-ACM T AUDIO SPE, V25, P2176, DOI 10.1109/TASLP.2017.2749001
   Natgunanathan I, 2012, IEEE T AUDIO SPEECH, V20, P2232, DOI 10.1109/TASL.2012.2199111
   Ogihara Akio, 2009, Proceedings of the 2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing. IIH-MSP 2009, P68, DOI 10.1109/IIH-MSP.2009.123
   Opuni-BoachieObourAgyekum Kwame, 2019, P INT C IM GRAPH BEI, P266
   Qureshi A, 2019, ASIAPAC SIGN INFO PR, P1606, DOI 10.1109/APSIPAASC47483.2019.9023054
   Savelyev A, 2018, COMPUT LAW SECUR REV, V34, P550, DOI 10.1016/j.clsr.2017.11.008
   Stokes S, 2019, DIGITAL COPYRIGHT: LAW AND PRACTICE, 5TH EDITION, P1
   Tresise A, 2018, AUSTR INTELLECT PROP, V28, P144
   Valizadeh Amir, 2010, IEEE T INF FOREN SEC, V6, P267
   Vishwa A, 2018, 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P1941, DOI 10.1109/SSCI.2018.8628636
   Wang BW, 2020, IEEE INT CONF MOB, P62, DOI 10.1109/MASS50613.2020.00018
   Wang HQ, 2008, APPL ACOUST, V69, P868, DOI 10.1016/j.apacoust.2007.06.001
   Wood G., 2014, Ethereum project yellow paper, V151, P1
   WU JL, 1995, IEEE T COMMUN, V43, P1857, DOI 10.1109/26.387423
   Xiang Y, 2015, IEEE-ACM T AUDIO SPE, V23, P2228, DOI 10.1109/TASLP.2015.2476755
   Xiang Y, 2014, IEEE-ACM T AUDIO SPE, V22, P1413, DOI 10.1109/TASLP.2014.2328175
   Xiang Y, 2012, IEEE T INF FOREN SEC, V7, P383, DOI 10.1109/TIFS.2011.2173678
   Xiang Y, 2011, IEEE T MULTIMEDIA, V13, P2, DOI 10.1109/TMM.2010.2080668
   Xu RZ, 2017, 2017 IEEE 13TH INTERNATIONAL SYMPOSIUM ON AUTONOMOUS DECENTRALIZED SYSTEMS (ISADS 2017), P128, DOI 10.1109/ISADS.2017.21
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang P, 2012, INT J FUZZY SYST, V14, P289
   Zhang XQ, 2013, IEEE INT WORKS INFOR, P186, DOI 10.1109/WIFS.2013.6707816
   Zhao J, 2020, LECT NOTES COMPUT SC, V12343, P201, DOI 10.1007/978-3-030-62008-0_14
NR 44
TC 8
Z9 8
U1 2
U2 32
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 86
DI 10.1145/3492803
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600020
DA 2024-07-18
ER

PT J
AU Golmaryami, M
   Taheri, R
   Pooranian, Z
   Shojafar, M
   Xiao, P
AF Golmaryami, Marjan
   Taheri, Rahim
   Pooranian, Zahra
   Shojafar, Mohammad
   Xiao, Pei
TI SETTI: A Self-supervised AdvErsarial Malware DeTection ArchiTecture in
   an IoT Environment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Internet of Things (IoT); Machine Learning (ML); malware detection;
   Self-Supervised Training (SSL); robustness; adversarial examples
AB In recent years, malware detection has become an active research topic in the area of Internet of Things (IoT) security. The principle is to exploit knowledge from large quantities of continuously generated malware. Existing algorithms practise available malware features for IoT devices and lack real-time prediction behaviours. More research is thus required on malware detection to cope with real-time misclassification of the input IoT data. Motivated by this, in this article, we propose an adversarial self-supervised architecture for detecting malware in IoT networks, SETTI, considering samples of IoT network traffic that may not be labeled. In the SETTI architecture, we design three self-supervised attack techniques, namely, Self-MDS, GSelf-MDS, and ASelf-MDS. The Self-MDS method considers the IoT input data and the adversarial sample generation in real-time. The GSelf-MDS builds a generative adversarial network model to generate adversarial samples in the self-supervised structure. Finally, ASelf-MDS utilises three well-known perturbation sample techniques to develop adversarial malware and inject it over the self-supervised architecture. Also, we apply a defence method to mitigate these attacks, namely, adversarial self-supervised training, to protect the malware detection architecture against injecting the malicious samples. To validate the attack and defence algorithms, we conduct experiments on two recent IoT datasets: IoT23 and NBIoT. Comparison of the results shows that in the IoT23 dataset, the Self-MDS method has the most damaging consequences from the attacker's point of view by reducing the accuracy rate from 98% to 74%. In the NBIoT dataset, the ASelf-MDS method is the most devastating algorithm that can plunge the accuracy rate from 98% to 77%.
C1 [Golmaryami, Marjan] ShirazUniv Technol, Comp Engn & Informat Technol Dept, Shiraz 7155713876, Iran.
   [Taheri, Rahim] Kings Coll London, Kings Commun Learning & Informat Proc Kclip Lab, London WC2R 2LS, England.
   [Pooranian, Zahra; Shojafar, Mohammad; Xiao, Pei] Univ Surrey, Inst Commun Syst ICS, 5GIC & 6GIC, Stag Hill, Guildford GU2 7XH, Surrey, England.
C3 University of London; King's College London; University of Surrey
RP Shojafar, M (corresponding author), Univ Surrey, Inst Commun Syst ICS, 5GIC & 6GIC, Stag Hill, Guildford GU2 7XH, Surrey, England.
EM m.golmaryami@sutech.ac.ir; rahim.taheri@kcl.ac.uk;
   z.pooranian@surrey.ac.uk; m.shojafar@surrey.ac.uk; p.xiao@surrey.ac.uk
RI Taheri, Rahim/AAL-9834-2020; Pooranian, Zahra/S-6654-2019; Shojafar,
   Mohammad/C-9151-2013
OI Taheri, Rahim/0000-0002-4078-3105; Pooranian, Zahra/0000-0003-3767-0377;
   Shojafar, Mohammad/0000-0003-3284-5086; Golmaryami, Seyyede
   Marjan/0000-0001-5139-1214; Xiao, Pei/0000-0002-7886-5878
CR Alizadehsani R, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3462635
   Alletto S, 2019, IEEE T INTELL TRANSP, V20, P3294, DOI 10.1109/TITS.2018.2873980
   Alqurashi S, 2017, INT CONF INTERNET, P105, DOI 10.23919/ICITST.2017.8356357
   Alwassel Humam, 2020, ADV NEURAL INF PROCE, V33
   [Anonymous], 2005, ROBOTICS SCI SYSTEMS
   Arshad S, 2018, IEEE ACCESS, V6, P4321, DOI 10.1109/ACCESS.2018.2792941
   Bazrafshan Z, 2013, 2013 5TH CONFERENCE ON INFORMATION AND KNOWLEDGE TECHNOLOGY (IKT), P113, DOI 10.1109/IKT.2013.6620049
   Bouguet, 2001, INTEL CORP, V5, P4, DOI DOI 10.1109/HPDC.2004.1323531
   Chen C, 2017, IEEE T INF FOREN SEC, V12, P914, DOI 10.1109/TIFS.2016.2621888
   Clark K, 2020, Arxiv, DOI arXiv:2003.10555
   Cruz T, 2016, IEEE T IND INFORM, V12, P2236, DOI 10.1109/TII.2016.2599841
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Dai QY, 2018, AAAI CONF ARTIF INTE, P2167
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Hjelm RD, 2019, Arxiv, DOI arXiv:1808.06670
   Dib M, 2022, ASIA CCS'22: PROCEEDINGS OF THE 2022 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P452, DOI 10.1145/3488932.3517393
   Ding M, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P913, DOI 10.1145/3269206.3271768
   Dizaji KG, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1435, DOI 10.1145/3219819.3220114
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Donahue J., 2019, Advances in neural information processing systems, P10542
   Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141
   Duan MX, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3506852
   Gharib A, 2017, LECT NOTES COMPUT SC, V10394, P184, DOI 10.1007/978-3-319-64701-2_14
   Gidaris S, 2018, Arxiv, DOI arXiv:1803.07728
   Goodfellow I., 2014, P 27 INT C NEURAL IN, P2672
   Guo JM, 2021, IEEE T NETW SCI ENG, V8, P933, DOI 10.1109/TNSE.2020.2997359
   Henaff OJ, 2020, PR MACH LEARN RES, V119
   Hendrycks D, 2019, Arxiv, DOI arXiv:1903.12261
   Hendrycks D, 2019, ADV NEUR IN, V32
   Hendrycks D, 2018, ADV NEUR IN, V31
   Hung WC, 2019, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2019.00096
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Joshi M, 2020, T ASSOC COMPUT LING, V8, P64, DOI 10.1162/tacl_a_00300
   Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181
   Kim D, 2018, IEEE WINT CONF APPL, P793, DOI 10.1109/WACV.2018.00092
   Kingma DP, 2018, 32 C NEURAL INFORM P
   Kurakin A, 2017, Arxiv, DOI arXiv:1607.02533
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Li ZC, 2020, IEEE T COMPUT SOC SY, V7, P569, DOI 10.1109/TCSS.2020.2970805
   Liu PP, 2019, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2019.00470
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Madry A, 2019, Arxiv, DOI arXiv:1706.06083
   Mahendran A, 2019, LECT NOTES COMPUT SC, V11365, P99, DOI 10.1007/978-3-030-20873-8_7
   Meidan Y, 2018, IEEE PERVAS COMPUT, V17, P12, DOI 10.1109/MPRV.2018.03367731
   Meister S, 2018, AAAI CONF ARTIF INTE, P7251
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Mugnai D, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485473
   Naval S, 2015, IEEE T INF FOREN SEC, V10, P2591, DOI 10.1109/TIFS.2015.2469253
   Nettleton DF, 2010, ARTIF INTELL REV, V33, P275, DOI 10.1007/s10462-010-9156-z
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Parmisano A., 2020, IoT-23: A Labeled Dataset with Malicious and Benign Iot Network Traffic (Version 1.0.0)
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Qin ZX, 2020, COMPUT AIDED GEOM D, V82, DOI 10.1016/j.cagd.2020.101928
   Ren Z, 2017, AAAI CONF ARTIF INTE, P1495
   Sadeghzadeh AM, 2021, IEEE T NETW SERV MAN, V18, P1962, DOI 10.1109/TNSM.2021.3052888
   Saeed A, 2021, IEEE INTERNET THINGS, V8, P1030, DOI 10.1109/JIOT.2020.3009358
   Schmidt L., 2018, Adv. Neural Inform. Process. Syst, P5014, DOI DOI 10.48550/ARXIV.1804.11285
   Song YG, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3524618
   Taheri R, 2021, IEEE T IND INFORM, V17, P8442, DOI 10.1109/TII.2020.3043458
   Taheri R, 2020, NEURAL COMPUT APPL, V32, P14781, DOI 10.1007/s00521-020-04831-9
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Yang YC, 2019, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2019.00097
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Yu LJ, 2020, PROCEEDINGS OF THE 29TH USENIX SECURITY SYMPOSIUM, P55
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Zhang HY, 2019, Arxiv, DOI arXiv:1901.08573
   Zhang J, 2021, IEEE INTERNET THINGS, V8, P7789, DOI 10.1109/JIOT.2020.3039359
   Zhang ZX, 2021, Arxiv, DOI arXiv:2105.03689
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zhu HJ, 2021, IEEE T NETW SCI ENG, V8, P984, DOI 10.1109/TNSE.2020.2996379
   Zou H, 2019, IEEE INTERNET THINGS, V6, P1238, DOI 10.1109/JIOT.2018.2868648
NR 71
TC 3
Z9 3
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 122
DI 10.1145/3536425
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000009
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Chen, M
   Xiao, WJ
   Li, M
   Hao, YX
   Hu, L
   Tao, GM
AF Chen, Min
   Xiao, Wenjing
   Li, Miao
   Hao, Yixue
   Hu, Long
   Tao, Guangming
TI A Multi-feature and Time-aware-based Stress Evaluation Mechanism for
   Mental Status Adjustment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Psychological pressure; multi-dimension feature; evaluation mechanism of
   stress; mental status adjustment
AB With the rapid economic development, the prominent social competition has led to increasing psychological pressure of people felt from each aspect of life. Driven by the Internet of Things and artificial intelligence, intelligent psychological pressure detection systems based on deep learning and wearable devices have acquired some good results in practical application. However, existing studies argue that the psychological stress state is influenced by the current environment. They put much attention on the momentary features but ignore the dynamic change process of mental status in the time dimension. Besides, the lack of research in the general laws of psychological stress makes it difficult to quantitatively evaluate the stress status, resulting in the inability to perceive the stress state of users effectively. Thus, this article proposes an evaluation mechanism of psychological stress for adjusting the mental status of users. Specifically, we design a multi-dimensional feature space and a time-aware feature encoder, which integrate various stress features and capture time characteristics of stress state change. Moreover, a novel mental state model is proposed, which uses the pressure features with time characteristics to evaluate the pressure stress level. This model also quantifies the internal relationship between pressure features. Last, we establish a practicable testbed to demonstrate how to evaluate and adjust mental state of users by the proposed evaluation mechanism of psychological stress.
C1 [Chen, Min; Xiao, Wenjing; Li, Miao; Hao, Yixue; Hu, Long] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
   [Chen, Min; Xiao, Wenjing; Li, Miao; Hao, Yixue; Hu, Long; Tao, Guangming] Opt Valley Labortory, Sport & Hlth Initiat, Wuhan, Peoples R China.
   [Tao, Guangming] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology
RP Hao, YX; Hu, L (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.; Hao, YX; Hu, L (corresponding author), Opt Valley Labortory, Sport & Hlth Initiat, Wuhan, Peoples R China.
EM minchen2020@hust.edu.cn; wenjingx@hust.edu.cn; miaoli@hust.edu.cn;
   yixuehao@hust.edu.cn; hulong@hust.edu.cn; TAO@hust.edu.cn
RI Hao, Yixue/H-8549-2017; Tao, Guangming/AAH-2617-2021; Liu,
   Zhe/KEJ-5299-2024; Xiao, Wen-Jing/D-3038-2015
OI Hao, Yixue/0000-0001-7296-2522; Tao, Guangming/0000-0002-1371-7735; 
FU Nature Science Foundation of China [61802138]; Technology Innovation
   Project of Hubei Province of China [2019AHB061]
FX This work was supported by the Nature Science Foundation of China under
   Grant 61802139. Prof. Yixue Hao's work was supported by the Nature
   Science Foundation of China under Grant 61802138. The work was also
   supported by the Technology Innovation Project of Hubei Province of
   China under Grant 2019AHB061.
CR Acemoglu D., 2002, Review of Development Economics, V6, P183, DOI DOI 10.1111/1467-9361.00149
   Ahammed K, 2020, BIOMED ENG-APP BAS C, V32, DOI 10.4015/S1016237220500118
   Akmandor AO, 2017, IEEE T MULTI-SCALE C, V3, P269, DOI 10.1109/TMSCS.2017.2703613
   Al-shargie F, 2018, MED BIOL ENG COMPUT, V56, P125, DOI 10.1007/s11517-017-1733-8
   Brouwer AM, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00224
   Brouwer AM, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/4/045008
   Cai W, 2014, IEEE INTERNET COMPUT, V18, P12, DOI 10.1109/MIC.2014.22
   Cardeña E, 2000, J TRAUMA STRESS, V13, P719, DOI 10.1023/A:1007822603186
   Chaby Lauren E, 2015, Commun Integr Biol, V8, pe1029689, DOI 10.1080/19420889.2015.1029689
   Chen M, 2018, IEEE NETWORK, V32, P8, DOI 10.1109/MNET.2018.1800110
   DAHLEM NW, 1991, J CLIN PSYCHOL, V47, P756, DOI 10.1002/1097-4679(199111)47:6<756::AID-JCLP2270470605>3.0.CO;2-L
   de Arriba-Perez F, 2019, J AMB INTEL HUM COMP, V10, P4925, DOI 10.1007/s12652-019-01188-3
   Garhammer M., 2002, Journal of Happiness Studies, V3, P217, DOI [10.1023/A:1020676100938, DOI 10.1023/A:1020676100938]
   Gisi B, 2020, INT J CARDIOL, V302, P75, DOI 10.1016/j.ijcard.2019.12.004
   Goh Eun Kyoung, 2017, Clin Nutr Res, V6, P89, DOI 10.7762/cnr.2017.6.2.89
   Graham C, 2017, J POPUL ECON, V30, P225, DOI 10.1007/s00148-016-0611-2
   Gunnar M, 2007, ANNU REV PSYCHOL, V58, P145, DOI 10.1146/annurev.psych.58.110405.085605
   Hao YX, 2021, BIG DATA COGN COMPUT, V5, DOI 10.3390/bdcc5040056
   Healey JA, 2005, IEEE T INTELL TRANSP, V6, P156, DOI 10.1109/TITS.2005.848368
   Hossain MS, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3241056
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Hossain MS, 2017, IEEE SYST J, V11, P118, DOI 10.1109/JSYST.2015.2470644
   Hwang B, 2018, TELEMED E-HEALTH, V24, P753, DOI 10.1089/tmj.2017.0250
   Jeunet C., P INT C PHYS COMP SY
   Jiang Q. J., 2001, ZHONGGUO XINGWEI YIX, V10, P36
   Jung Y, 2017, MULTIMED TOOLS APPL, V76, P11305, DOI 10.1007/s11042-016-3444-9
   Kassis M., 2020, WORKING PAPER SERIES
   Li Z., 1986, J AFFECT DISORDERS, V10, P185, DOI [10.1016/0165- 0327(86)90003-0, DOI 10.1016/0165-0327(86)90003-0]
   Masood K, 2019, IEEE ACCESS, V7, P68446, DOI 10.1109/ACCESS.2019.2917718
   McGonigal Kelly., WILLPOWER INSTINCT
   Mellor S, 2020, EMPLOY RESPONSIB RIG, V32, P1, DOI 10.1007/s10672-020-09343-1
   Mohr DC, 2017, ANNU REV CLIN PSYCHO, V13, P23, DOI 10.1146/annurev-clinpsy-032816-044949
   Oei T. I., 2019, J AFFECT DISORDERS, DOI [10.1109/URAI.2019.8768550, DOI 10.1109/URAI.2019.8768550]
   Rashid NA, 2011, PROCD SOC BEHV, V29, DOI 10.1016/j.sbspro.2011.11.339
   Rosch P J, 1999, Int J Emerg Ment Health, V1, P59
   Sharma N, 2014, APPL SOFT COMPUT, V14, P53, DOI 10.1016/j.asoc.2013.09.019
   Ursin H, 2004, PSYCHONEUROENDOCRINO, V29, P567, DOI 10.1016/S0306-4530(03)00091-X
   Wang L, 2019, IRAN J PUBLIC HEALTH, V48, P95, DOI 10.18502/ijph.v48i1.787
NR 38
TC 4
Z9 4
U1 5
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 39
DI 10.1145/3462763
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300016
DA 2024-07-18
ER

PT J
AU Yan, CG
   Meng, LX
   Li, L
   Zhang, JH
   Wang, Z
   Yin, J
   Zhang, JY
   Sun, YQ
   Zheng, BL
AF Yan, Chenggang
   Meng, Lixuan
   Li, Liang
   Zhang, Jiehua
   Wang, Zhan
   Yin, Jian
   Zhang, Jiyong
   Sun, Yaoqi
   Zheng, Bolun
TI Age-Invariant Face Recognition by Multi-Feature Fusion and Decomposition
   with Self-attention
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Age-invariant face recognition; feature fusion; feature decomposition;
   self-attention
AB Different from general face recognition, age-invariant face recognition (AIFR) aims at matching faces with a big age gap. Previous discriminative methods usually focus on decomposing facial feature into age-related and age-invariant components, which suffer from the loss of facial identity information. In this article, we propose a novel Multi-feature Fusion and Decomposition (MFD) framework for age-invariant face recognition, which learns more discriminative and robust features and reduces the intra-class variants. Specifically, we first sample multiple face images of different ages with the same identity as a face time sequence. Then, the multi-head attention is employed to capture contextual information from facial feature series, extraded by the backbone network. Next, we combine feature decomposition with fusion based on the face time sequence to ensure that the final age-independent features effectively represent the identity information of the face and have stronger robustness against the aging process. Besides, we also mitigate imbalanced age distribution in the training data by a re-weighted age loss. We experimented with the proposed MFD over the popular CACD and CACD-VS datasets, where we show that our approach improves the AIFR performance than previous state-of-the-art methods. We simultaneously show the performance of MFD on LFW dataset.
C1 [Yan, Chenggang; Zhang, Jiehua; Zhang, Jiyong; Sun, Yaoqi; Zheng, Bolun] Hangzhou Dianzi Univ, 280 Xuelin Rd, Hangzhou, Zhejiang, Peoples R China.
   [Meng, Lixuan; Yin, Jian] Shandong Univ, 180 Wenhua Western Rd, Weihai, Shandong, Peoples R China.
   [Li, Liang] Chinese Acad Sci, Inst Comp Technol, 6 Kexueyuan South Rd, Beijing, Peoples R China.
   [Wang, Zhan] Moreal Pte Ltd, 20 LORONG 35 GEYLANG,03-08, Singapore, Singapore.
C3 Hangzhou Dianzi University; Shandong University; Chinese Academy of
   Sciences; Institute of Computing Technology, CAS
RP Zhang, JH; Zheng, BL (corresponding author), Hangzhou Dianzi Univ, 280 Xuelin Rd, Hangzhou, Zhejiang, Peoples R China.; Li, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, 6 Kexueyuan South Rd, Beijing, Peoples R China.
EM cgyan@hdu.edu.cn; mlxwaited@gmail.com; liang.li@ict.ac.cn;
   jiehua.zhang@hdu.edu.cn; kingzhan@gmail.com; yinjian@sdu.edu.cn;
   jzhang@hdu.edu.cn; syq@hdu.edu.cn; blzheng@hdu.edu.cn
OI Li, Liang/0000-0002-1943-8219
FU National Key Research and Development Program of China [2020YFB1406604];
   National Natural Science Foundation of China [61931008, 61671196,
   62071415, 62001146, 61701149, 61801157, 61971268, 61901145, 61901150,
   61972123, 61771457, 61732007]; Zhejiang Province Natural Science
   Foundation of China [LR17F030006, Q19F010030]; Youth Innovation
   Promotion Association of Chinese Academy of Sciences [2020108]; 111
   Project [D17019]
FX This work was supported by the National Key Research and Development
   Program of China under Grant (2020YFB1406604), National Natural Science
   Foundation of China (61931008, 61671196, 62071415, 62001146, 61701149,
   61801157, 61971268, 61901145, 61901150, 61972123, 61771457, 61732007),
   Zhejiang Province Natural Science Foundation of China (LR17F030006,
   Q19F010030), Youth Innovation Promotion Association of Chinese Academy
   of Sciences under Grant (2020108), 111 Project, No. D17019.
CR [Anonymous], 2010, 2010 4 IEEE INT C BI, DOI DOI 10.1109/BTAS.2010.5634496
   [Anonymous], 2019, PROC INT C NEURAL IN
   [Anonymous], 2016, ACM INT C MULT
   Beichen Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8753, DOI 10.1109/CVPR42600.2020.00878
   Cao D, 2020, PROC CVPR IEEE, P5670, DOI 10.1109/CVPR42600.2020.00571
   Cao KD, 2019, ADV NEUR IN, V32
   Chang J, 2020, PROC CVPR IEEE, P5709, DOI 10.1109/CVPR42600.2020.00575
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Chen ZN, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231742
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Jincan, 2021, IEEE T CIRC SYST TEC, P1
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Duarte A, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1650, DOI 10.1145/3343031.3352587
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gong DH, 2013, IEEE I CONF COMP VIS, P2872, DOI 10.1109/ICCV.2013.357
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   Huang G. B., 2008, WORKSH FAC REAL LIF
   Khan S, 2019, PROC CVPR IEEE, P103, DOI 10.1109/CVPR.2019.00019
   Li JN, 2019, IEEE I CONF COMP VIS, P3957, DOI 10.1109/ICCV.2019.00406
   Li JP, 2020, IEEE T CYBERNETICS, V50, P3281, DOI [10.1109/TPAMI.2019.2929036, 10.1109/TCYB.2019.2904052]
   Li L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3359753
   Li L, 2012, IEEE T MULTIMEDIA, V14, P1401, DOI 10.1109/TMM.2012.2194993
   Li SQ, 2016, INT CONF CLOUD COMPU, P480, DOI 10.1109/CCIS.2016.7790306
   Li ZF, 2016, IEEE T IMAGE PROCESS, V25, P2146, DOI 10.1109/TIP.2016.2535284
   Li ZF, 2011, IEEE T INF FOREN SEC, V6, P1028, DOI 10.1109/TIFS.2011.2156787
   Liang He, 2016, 2016 ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS), P1, DOI 10.1109/ICCPS.2016.7479067
   Lin L, 2017, IEEE T PATTERN ANAL, V39, P1089, DOI 10.1109/TPAMI.2016.2567386
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu Xiaobin, 2020, P 28 ACM INT C MULT, P547
   Liu XJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P539, DOI 10.1145/3343031.3351074
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Long Xiang, 2018, P AAAI C ART INT, V32
   Long Xiang, 2018, T ASSOC COMPUT LING, V6, P173, DOI DOI 10.1162/TACL_A_00013
   Luo P, 2016, AAAI CONF ARTIF INTE, P3560
   Lv JN, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1883, DOI 10.1145/3123266.3127897
   Meng LX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3146, DOI 10.1145/3394171.3413499
   Qingqiu Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P139, DOI 10.1007/978-3-030-58520-4_9
   Ramanathan N., 2006, 2006 IEEE COMP SOC C, V1, P387
   Ren MY, 2018, PR MACH LEARN RES, V80
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Song JK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P899
   Sun Y, 2019, IEEE I CONF COMP VIS, P5348, DOI 10.1109/ICCV.2019.00545
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Teng SZ, 2021, INT J COMPUT VISION, V129, P719, DOI 10.1007/s11263-020-01402-2
   Wang H, 2019, PROC CVPR IEEE, P3522, DOI 10.1109/CVPR.2019.00364
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang W, 2016, PROC CVPR IEEE, P2378, DOI 10.1109/CVPR.2016.261
   Wang Xiao, 2020, ACM T MULTIM COMPUT, V16, P1
   Wang YT, 2018, LECT NOTES COMPUT SC, V11219, P764, DOI 10.1007/978-3-030-01267-0_45
   Wen YD, 2016, PROC CVPR IEEE, P4893, DOI 10.1109/CVPR.2016.529
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu BC, 2018, PROC CVPR IEEE, P9127, DOI 10.1109/CVPR.2018.00951
   Xu CF, 2017, NEUROCOMPUTING, V222, P62, DOI 10.1016/j.neucom.2016.10.010
   Xu T, 2020, P 2020 IEEE MTT S IN, P1
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P3014, DOI 10.1109/TMM.2020.2967645
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Yi Dong, 2014, ARXIV14117923
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhao J, 2019, AAAI CONF ARTIF INTE, P9251
   Zheng TY, 2017, IEEE COMPUT SOC CONF, P503, DOI 10.1109/CVPRW.2017.77
   ZHU H, 2018, ARXIV180402740
NR 68
TC 91
Z9 93
U1 13
U2 51
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 29
DI 10.1145/3472810
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300006
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Karagkioules, T
   Paschos, GS
   Liakopoulos, N
   Fiandrotti, A
   Tsilimantos, D
   Cagnazzo, M
AF Karagkioules, Theodoros
   Paschos, Georgios S.
   Liakopoulos, Nikolaos
   Fiandrotti, Attilio
   Tsilimantos, Dimitrios
   Cagnazzo, Marco
TI Online Learning for Adaptive Video Streaming in Mobile Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adaptive video streaming; online optimization
ID OPTIMIZATION; DASH
AB In this paper, we propose a novel algorithm for video bitrate adaptation in HTTP Adaptive Streaming (HAS), based on online learning. The proposed algorithm, named Learn2Adapt (L2A), is shown to provide a robust bitrate adaptation strategy which, unlike most of the state-of-the-art techniques, does not require parameter tuning, channel model assumptions, or application-specific adjustments. These properties make it very suitable for mobile users, who typically experience fast variations in channel characteristics. Experimental results, over real 4G traffic traces, show that L2A improves on the overall Quality of Experience (QoE) and in particular the average streaming bitrate, a result obtained independently of the channel and application scenarios.
C1 [Karagkioules, Theodoros; Fiandrotti, Attilio; Cagnazzo, Marco] Telecom Paris, 19 Pl Marguerite Perey, F-91120 Palaiseau, France.
   [Paschos, Georgios S.] Amazon Com, Boulogne, France.
   [Liakopoulos, Nikolaos; Tsilimantos, Dimitrios] Huawei Technol France, 18 Quai Point Jour, F-92100 Boulogne, France.
C3 IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Telecom
   Paris; Huawei Technologies
RP Karagkioules, T (corresponding author), Telecom Paris, 19 Pl Marguerite Perey, F-91120 Palaiseau, France.
EM theodore.karagkioules@gmail.com; Paschosg@amazon.com;
   liakopoulosnp@gmail.com; attilio.fiandrotti@telcom-paris.fr;
   dimitrios.tsilimantos@huawei.com; marco.cagnazzo@telcom-paris.fr
RI Cagnazzo, Marco/AAZ-3881-2020
OI Cagnazzo, Marco/0000-0001-6731-3755; Tsilimantos,
   Dimitrios/0000-0002-1154-499X
CR Akhshabi Saamer, 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video-NOSSDAV'13
   Altamimi S, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3397227
   [Anonymous], 2019, PR MACH LEARN RES
   [Anonymous], 2019, CISCO VISUAL NETWORK
   Belmega Elena Veronica, 2018, ONLINE CONVEX OPTIMI
   Ben Yahia M, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3280854
   Bentaleb A, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3387921
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Bentaleb A, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3219752
   Bhat D, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183516
   Bokani A, 2015, IEEE T MULTIMEDIA, V17, P2297, DOI 10.1109/TMM.2015.2494458
   Burger V, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183511
   Chen TY, 2017, IEEE T SIGNAL PROCES, V65, P6350, DOI 10.1109/TSP.2017.2750109
   Claeys Maxim, 2013, P AD LEARN AG WORKSH
   Cofano G, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092836
   Gadaleta M, 2017, IEEE T COGN COMMUN, V3, P703, DOI 10.1109/TCCN.2017.2755007
   Huang TY, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P86, DOI 10.1145/3304109.3306219
   Huang Te-Yuan, 2014, PROC ACM C SIGCOMM
   Huang TC, 2020, IEEE J SEL AREA COMM, V38, P2324, DOI 10.1109/JSAC.2020.3000363
   ISO/IEC, 2018, 23000192018 ISO IEC
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Juluri Parikshit, 2019, ACM TOMM
   Karagkioules Theo, 2020, P ACM MMSYS MMSYS 20
   Karagkioules Theodoros, 2019, ABS190511705 CORR
   Karagkioules Theodoros, 2017, P ACM NOSSDAV
   Kim S, 2019, IEEE T MULTIMEDIA, V21, P442, DOI 10.1109/TMM.2018.2856626
   Kua J, 2017, IEEE COMMUN SURV TUT, V19, P1842, DOI 10.1109/COMST.2017.2685630
   Liakopoulos N, 2019, PR MACH LEARN RES, V97
   Liakopoulos N, 2019, IEEE INFOCOM SER, P1747, DOI [10.1109/INFOCOM.2019.8737606, 10.1109/infocom.2019.8737606]
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Marcastel A, 2019, IEEE T SIGNAL PROCES, V67, P2987, DOI 10.1109/TSP.2019.2910479
   Mekki S, 2017, IEEE CONF COMPUT, P671, DOI 10.1109/INFCOMW.2017.8116457
   Neely Michael J, 2017, Online convex optimization with time-varying constraints
   Qin YY, 2020, IEEE T MOBILE COMPUT, V19, P2505, DOI 10.1109/TMC.2019.2929125
   Qin YY, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P189, DOI 10.1145/3304109.3306231
   Raca D, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P460, DOI 10.1145/3204949.3208123
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Skorin-Kapov Lea, 2018, ACM TOMM
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Tsilimantos D, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P138, DOI 10.1145/3204949.3204955
   Yousef H, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P54, DOI 10.1145/3339825.3391859
   Yue CQ, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P153, DOI 10.1145/3339825.3391867
   Zabrovskiy A, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P438, DOI 10.1145/3204949.3208140
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zinkevich Martin, 2003, INT C MACH LEARN WAS
NR 47
TC 3
Z9 3
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 2
DI 10.1145/3460819
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900002
OA Green Published
DA 2024-07-18
ER

PT J
AU Chen, CLZ
   Zhao, HM
   Yang, H
   Yu, T
   Peng, C
   Qin, H
AF Chen, Chenglizhao
   Zhao, Hongmeng
   Yang, Huan
   Yu, Teng
   Peng, Chong
   Qin, Hong
TI Full-reference Screen Content Image Quality Assessment by Fusing
   Multilevel Structure Similarity
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image quality assessment; screen content images; selective deep fusion
ID VIDEO SALIENCY DETECTION; STRATEGIES; INDEX
AB Screen content images (SCIs) usually comprise various content types with sharp edges, in which artifacts or distortions can be effectively sensed by a vanilla structure similarity measurement in a full-reference manner. Nonetheless, almost all of the current state-of-the-art (SOTA) structure similarity metrics are "locally" formulated in a single-level manner, while the true human visual system (HVS) follows the multilevel manner; such mismatch could eventually prevent these metrics from achieving reliable quality assessment. To ameliorate this issue, this article advocates a novel solution to measure structure similarity "globally" from the perspective of sparse representation. To perform multilevel quality assessment in accordance with the real HVS, the abovementioned global metric will be integrated with the conventional local ones by resorting to the newly devised selective deep fusion network. To validate its efficacy and effectiveness, we have compared our method with 12 SOTA methods over two widely used large-scale public SCI datasets, and the quantitative results indicate that our method yields significantly higher consistency with subjective quality scores than the current leading works. Both the source code and data are also publicly available to gain widespread acceptance and facilitate new advancement and validation.
C1 [Chen, Chenglizhao; Zhao, Hongmeng; Yang, Huan; Yu, Teng; Peng, Chong] Qingdao Univ, Coll Comp Sci & Technol, Qingdao, Shandong, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Stony Brook, NY 11794 USA.
C3 Qingdao University; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Yu, T; Peng, C (corresponding author), Qingdao Univ, Coll Comp Sci & Technol, Qingdao, Shandong, Peoples R China.
EM cclz123@163.com; zhm199411@163.com; cathy_huanyang@hotmail.com;
   yutenghit@foxmail.com; pchong19910@163.com; qin@cs.stonybrook.edu
FU National Natural Science Foundation of China [61802215, 61806106,
   61672077, 61532002]; Natural Science Foundation of Shandong Province
   [ZR2019BF011, ZR2019QF009]; National Science Foundation of the USA
   [IIS-1715985, IIS-1812606]
FX This research is supported in part by the National Natural Science
   Foundation of China (Grants No. 61802215, No. 61806106, No. 61672077,
   and No. 61532002), the Natural Science Foundation of Shandong Province
   (Grants No. ZR2019BF011 and No. ZR2019QF009) and the National Science
   Foundation of the USA (Grants No. IIS-1715985 and No. IIS-1812606). Teng
   Yu and Chong Peng. Code&Data are available from
   https://github.com/HongmengZhao/SR-CNN.Authors' addresses: C. Chen, H.
   Zhao, H. Yang, T. Yu (corresponding author), and C. Peng (corresponding
   author), College of Computer Science and Technology, Qingdao University;
   emails: {cclz123, zhm199411}@163.com,
   cathy_huanyang@hotmail.com,yutenghit@foxmail.com,pchong1991@163.com;H.Qi
   n, Stony Brook University; email: qin@cs.stonybrook.edu.Permission to
   make digital or hard copies of all or part of this work for personal or
   classroom use is granted without fee provided that copies are not made
   or distributed for profit or commercial advantage and that copies bear
   this notice and the full citation on the first page. Copyrights for
   components of this work owned by others than ACM must be honored.
   Abstracting with credit is permitted. To copy otherwise, or republish,
   to post on servers or to redistribute to lists, requires prior specific
   permission and/or a fee. Request permissions from
   permissions@acm.org.(c) 2021 Association for Computing Machinery.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   An, 2019, J VIS COMMUN IMAGE R, V67, P27
   [Anonymous], 2019, IEEE T IND INFORM, DOI DOI 10.1109/TII.2018.2869843
   [Anonymous], IEEE T IMAGE PROCESS
   Chen, 2016, ACM T MULTIM COMPUT, V12, P1
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P4296, DOI 10.1109/TIP.2020.2968250
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen CLZ, 2018, IEEE T MULTIMEDIA, V20, P3324, DOI 10.1109/TMM.2018.2839523
   Chen CLZ, 2018, IEEE SIGNAL PROC LET, V25, P154, DOI 10.1109/LSP.2017.2775212
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen CLZ, 2016, PATTERN RECOGN, V52, P410, DOI 10.1016/j.patcog.2015.09.033
   Chen CLZ, 2015, PATTERN RECOGN, V48, P2885, DOI 10.1016/j.patcog.2015.01.025
   Chen CLZ, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2403232
   Chen JN, 2018, IEEE SIGNAL PROC LET, V25, P1685, DOI 10.1109/LSP.2018.2871250
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Fang YM, 2017, IEEE T IMAGE PROCESS, V26, P2016, DOI 10.1109/TIP.2017.2669840
   Feng Guorui, 2019, NEUROCOMPUTING, V52, P410
   Freitas PG, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P52, DOI 10.1145/3204949.3204960
   Fu Y, 2018, IEEE T CIRC SYST VID, V28, P2428, DOI 10.1109/TCSVT.2018.2854176
   Gu K, 2018, IEEE T VIS COMPUT GR, V24, P2689, DOI 10.1109/TVCG.2017.2771284
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2015, IEEE INT SYMP CIRC S, P125, DOI 10.1109/ISCAS.2015.7168586
   Hu H, 2014, IEEE MULTIMEDIA, V21, P10, DOI 10.1109/MMUL.2014.2
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang X., 2019, ARXIV190300705
   Li YC, 2021, INT J PROD RES, V59, P1736, DOI 10.1080/00207543.2020.1724344
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Ma Z, 2014, IEEE T IMAGE PROCESS, V23, P4399, DOI 10.1109/TIP.2014.2346995
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Ni ZK, 2018, IEEE T IMAGE PROCESS, V27, P4516, DOI 10.1109/TIP.2018.2839890
   Ni ZK, 2017, IEEE T IMAGE PROCESS, V26, P4818, DOI 10.1109/TIP.2017.2718185
   Ni ZK, 2016, IEEE IMAGE PROC, P81, DOI 10.1109/ICIP.2016.7532323
   Ni ZK, 2016, IEEE SIGNAL PROC LET, V23, P1394, DOI 10.1109/LSP.2016.2599294
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Peng C, 2019, PROC CVPR IEEE, P7309, DOI 10.1109/CVPR.2019.00749
   Peng C, 2020, INFORM SCIENCES, V513, P581, DOI 10.1016/j.ins.2019.09.074
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Qian JS, 2017, ELECTRON LETT, V53, P592, DOI 10.1049/el.2017.0325
   Qin, 2021, IEEE T IMAGE PROCESS, V1, P1
   Qin Hong, 2021, IEEE T IMAGE PROCESS, V1, P1
   Simonyan K., 2014, CORR
   Wang RF, 2019, IEEE ACCESS, V7, P5285, DOI 10.1109/ACCESS.2018.2889992
   Wang SQ, 2018, IEEE COMPUT GRAPH, V38, P47, DOI 10.1109/MCG.2016.46
   Wang SQ, 2015, IEEE IMAGE PROC, P1434, DOI 10.1109/ICIP.2015.7351037
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2006, IEEE IMAGE PROC, P2945, DOI 10.1109/ICIP.2006.313136
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Xing L, 2018, SIGNAL PROCESS, V145, P233, DOI 10.1016/j.sigpro.2017.12.013
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang XD, 2013, IEEE SIGNAL PROC LET, V20, P319, DOI 10.1109/LSP.2013.2244081
   Zhang Y, 2018, IEEE T IMAGE PROCESS, V27, P5113, DOI 10.1109/TIP.2018.2851390
   Zheng LR, 2019, IEEE T MULTIMEDIA, V21, P2057, DOI 10.1109/TMM.2019.2894939
   ZhenyuWu Shuai Li, 2021, IEEE T MULTIMEDIA, V1, P1
   Zhou WJ, 2018, IEEE T IMAGE PROCESS, V27, P2086, DOI 10.1109/TIP.2018.2794207
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
   Zuo LX, 2016, IEEE IMAGE PROC, P2082, DOI 10.1109/ICIP.2016.7532725
NR 63
TC 10
Z9 10
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 94
DI 10.1145/3447393
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, XF
   Nie, XS
   Teng, JY
   Lian, L
   Yin, YL
AF Liu, Xinfang
   Nie, Xiushan
   Teng, Junya
   Lian, Li
   Yin, Yilong
TI Single-shot Semantic Matching Network for Moment Localization in Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodal retrieval; moment localization; visual comprehension; natural
   language understanding
AB Moment localization in videos using natural language refers to finding the most relevant segment from videos given a natural language query. Most of the existing methods require video segment candidates for further matching with the query, which leads to extra computational costs, and they may also not locate the relevant moments under any length evaluated. To address these issues, we present a lightweight single-shot semantic matching network (SSMN) to avoid the complex computations required to match the query and the segment candidates, and the proposed SSMN can locate moments of any length theoretically. Using the proposed SSMN, video features are first uniformly sampled to a fixed number, while the query sentence features are generated and enhanced by GloVe, long-term short memory (LSTM), and soft-attention modules. Subsequently, the video features and sentence features are fed to an enhanced cross-modal attention model to mine the semantic relationships between vision and language. Finally, a score predictor and a location predictor are designed to locate the start and stop indexes of the query moment. We evaluate the proposed method on two benchmark datasets and the experimental results demonstrate that SSMN outperforms state-of-the-art methods in both precision and efficiency.
C1 [Liu, Xinfang; Teng, Junya; Lian, Li; Yin, Yilong] Shandong Univ, Sch Software, 1500 ShunHua Rd, Jinan 250101, Peoples R China.
   [Nie, Xiushan] Shandong Jianzhu Univ, Sch Comp Sci & Technol, Fengming Rd, Jinan 250101, Peoples R China.
C3 Shandong University; Shandong Jianzhu University
RP Yin, YL (corresponding author), Shandong Univ, Sch Software, 1500 ShunHua Rd, Jinan 250101, Peoples R China.; Nie, XS (corresponding author), Shandong Jianzhu Univ, Sch Comp Sci & Technol, Fengming Rd, Jinan 250101, Peoples R China.
EM xinfangliu@qq.com; niexsh@hotmail.com; 799859083@qq.com;
   lianli@sdu.edu.cn; ylyin@sdu.edu.cn
RI lu, kai/KBB-4008-2024
FU National Natural Science Foundation of China [61876098, 61671274];
   National Key R&D Program of China [2018YFC0830100, 2018YFC0830102];
   Taishan Scholar Project of Shandong Province [tsqn202103088]; special
   funds for distinguished professors of Shandong Jianzhu University
FX This work was supported in part by the National Natural Science
   Foundation of China (61876098, 61671274), National Key R&D Program of
   China (2018YFC0830100, 2018YFC0830102), Taishan Scholar Project of
   Shandong Province (tsqn202103088) and special funds for distinguished
   professors of Shandong Jianzhu University.
CR [Anonymous], 2016, ARXIV PREPRINT ARXIV
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang XJ, 2017, IEEE T CYBERNETICS, V47, P1180, DOI 10.1109/TCYB.2016.2539546
   Chang XJ, 2017, IEEE T PATTERN ANAL, V39, P1617, DOI 10.1109/TPAMI.2016.2608901
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Ghosh S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1984
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Hahn Meera, 2019, ARXIVCSCV190409936
   He DL, 2019, AAAI CONF ARTIF INTE, P8393
   Hendricks LA, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1380
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100
   Karpathy A, 2014, ADV NEUR IN, V27
   King DB, 2015, ACS SYM SER, V1214, P1
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krishnamoorthy Niveda, 2013, P 27 AAAI C ART INT
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long Xiang, 2018, T ASSOC COMPUT LING, V6, P173, DOI DOI 10.1162/TACL_A_00013
   Mettes P, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P427, DOI 10.1145/2671188.2749404
   Miech Antoine, 2018, ARXIV180402516
   Mithun NC, 2019, INT J MULTIMED INF R, V8, P3, DOI 10.1007/s13735-018-00166-3
   Mithun NC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1856, DOI 10.1145/3240508.3240712
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Mun J, 2019, PROC CVPR IEEE, P3581, DOI 10.1109/CVPR.2019.00675
   Nie XS, 2020, IEEE T KNOWL DATA EN, V32, P1951, DOI 10.1109/TKDE.2019.2913383
   Nie XS, 2016, SCI CHINA INFORM SCI, V59, DOI 10.1007/s11432-016-5528-6
   Nie XS, 2010, INT CONF SIGN PROCES, P1837, DOI 10.1109/ICOSP.2010.5656914
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Shetty Rakshith, 2015, ARXIV PREPRINT ARXIV
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Song JK, 2020, INT J COMPUT VISION, V128, P2243, DOI 10.1007/s11263-020-01305-2
   Szegedy C., 2013, Advances in Neural Information Processing Systems, V26, P2553
   Thomason J., 2014, COLING, P1218
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhao B, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2916757
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
NR 49
TC 8
Z9 10
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 84
DI 10.1145/3441577
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400008
DA 2024-07-18
ER

PT J
AU Sahu, AK
   Sharma, S
   Puthal, D
AF Sahu, Amiya Kumar
   Sharma, Suraj
   Puthal, Deepak
TI Lightweight Multi-party Authentication and Key Agreement Protocol in
   IoT-based E-Healthcare Service
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Internet of Things; authentication; healthcare; lightweight; key
   establishment; security protocol; lattice-based cryptography;
   identity-based encryption
ID ESTABLISHMENT; NETWORKS; INTERNET; SCHEME; ROBUST
AB Internet of Things (IoT) is playing a promising role in e-healthcare applications in the recent decades; nevertheless, security is one of the crucial challenges in the current field of study. Many healthcare devices (for instance, a sensor-augmented insulin pump and heart-rate sensor) collect a user's real-time data (such as glucose level and heart rate) and send them to the cloud for proper analysis and diagnosis of the user. However, the real-time user's data are vulnerable to various authentication attacks while sending through an insecure channel. Besides that, the attacks may further open scope for many other subsequent attacks. Existing security mechanisms concentrate on two-party mutual authentication. However, an IoT-enabled healthcare application involves multiple parties such as a patient, e-healthcare test-equipment, doctors, and cloud servers that requires multi-party authentication for secure communication. Moreover, the design and implementation of a lightweight security mechanism that fits into the resource constraint IoT-enabled healthcare devices are challenging. Therefore, this article proposes a lightweight, multi-party authentication and key-establishment protocol in IoT-based e-healthcare service access network to counter the attacks in resource constraint devices. The proposed multi-party protocol has used a lattice-based cryptographic construct such as Identity-Based Encryption (IBE) to acquire security, privacy, and efficiency. The study provided all-round analysis of the scheme, such as security, power consumption, and practical usage, in the following ways. The proposed scheme is tested by a formal security tool, Scyther, to testify the security properties of the protocol. In addition, security analysis for various attacks and comparison with other existing works are provided to show the robust security characteristics. Further, an experimental evaluation of the proposed scheme using IBE cryptographic construct is provided to validate the practical usage. The power consumption of the scheme is also computed and compared with existing works to evaluate its efficiency.
C1 [Sahu, Amiya Kumar; Sharma, Suraj] Int Inst Informat Technol Bhubaneswar, Bhubaneswar 751003, Odisha, India.
   [Puthal, Deepak] Newcastle Univ, Newcastle Upon Tyne, Tyne & Wear, England.
   [Puthal, Deepak] Urban Sci Bldg,Sci Sq, Newcastle Upon Tyne NE4 5TG, Tyne & Wear, England.
C3 International Institute of Information Technology, Bhubaneswar;
   Newcastle University - UK
RP Sahu, AK (corresponding author), Int Inst Informat Technol Bhubaneswar, Bhubaneswar 751003, Odisha, India.
EM c117002@iiit-bh.ac.in; suraj@iiit-bh.ac.in; deepak.puthal@ncl.ac.uk
RI Puthal, Deepak/V-6529-2019; SAHU, AMIYA KUMAR/AAV-4840-2021
OI SAHU, AMIYA KUMAR/0000-0001-5330-0654
CR Abdalla M, 2005, LECT NOTES COMPUT SC, V3386, P65
   Arfaoui A, 2019, COMPUT NETW, V159, P23, DOI 10.1016/j.comnet.2019.04.031
   BABAI L, 1986, COMBINATORICA, V6, P1, DOI 10.1007/BF02579403
   Boneh D, 2003, SIAM J COMPUT, V32, P586, DOI 10.1137/S0097539701398521
   Canetti R, 2001, LECT NOTES COMPUT SC, V2045, P453
   Cohen Henri, 1993, ALGORITHMS LINEAR AL, P45, DOI [10.1007/978-3-662-02945-9_2, DOI 10.1007/978-3-662-02945-9_2]
   Cremers C., 2012, Operational Semantics and Verification of Security Protocols
   Cremers CJF, 2008, LECT NOTES COMPUT SC, V5123, P414
   Cremers Sjouke Mauw Cas, 2012, OPERATIONAL SEMANTIC, V1st, DOI [10.1007/978-3-540-78636-8, DOI 10.1007/978-3-540-78636-8]
   Cui ZH, 2020, IEEE T SERV COMPUT, V13, P241, DOI 10.1109/TSC.2020.2964537
   Dolev D., 1981, 22nd Annual Symposium on Foundations of Computer Science, P350, DOI 10.1109/SFCS.1981.32
   Ducas L, 2014, LECT NOTES COMPUT SC, V8874, P22, DOI 10.1007/978-3-662-45608-8_2
   Dworkin M.J, 2015, Federal Information Processing Standards, DOI [DOI 10.6028/NIST.FIPS.202, 10.6028/NIST.FIPS.202]
   Ferrag MA, 2017, IEEE COMMUN SURV TUT, V19, P3015, DOI 10.1109/COMST.2017.2718178
   Ferrag MA, 2017, SECUR COMMUN NETW, DOI 10.1155/2017/6562953
   Gama N, 2008, LECT NOTES COMPUT SC, V4965, P31
   Gupta A, 2019, COMPUT NETW, V149, P29, DOI 10.1016/j.comnet.2018.11.021
   Liu Yi-Kai, 2016, REPORT POSTQUANTUM C, DOI [10.6028/NIST.IR.8105, DOI 10.6028/NIST.IR.8105]
   Mahmood Z, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7101069
   Porambage P, 2015, IEEE ACCESS, V3, P1503, DOI 10.1109/ACCESS.2015.2474705
   Puthal D, 2019, IEEE INTERNET THINGS, V6, P1312, DOI 10.1109/JIOT.2018.2805896
   Sahu Amiya Kumar, 2017, 2017 8th International Conference on Information Technology (ICIT), P220, DOI 10.1109/ICIT.2017.21
   Shamir A., 1985, Advances in Cryptology, V84 4, P47, DOI 10.1007/3-540-39568-7_5
   Sharma S, 2018, 2018 FIFTH INTERNATIONAL CONFERENCE ON PARALLEL, DISTRIBUTED AND GRID COMPUTING (IEEE PDGC), P621, DOI 10.1109/PDGC.2018.8745799
   Sui AF, 2005, IEEE WCNC, P2088
   Tsai KL, 2016, IEEE ACCESS, V4, P6261, DOI 10.1109/ACCESS.2016.2613442
   Usman M, 2020, IEEE INTERNET THINGS, V7, P2501, DOI 10.1109/JIOT.2019.2936512
   Wazid M, 2019, FUTURE GENER COMP SY, V91, P475, DOI 10.1016/j.future.2018.09.017
   Win EK, 2017, P INT COMP SOFTW APP, P491, DOI 10.1109/COMPSAC.2017.20
   Wu F, 2018, FUTURE GENER COMP SY, V82, P727, DOI 10.1016/j.future.2017.08.042
   Yanambaka Venkata, 2019, P 2019 IEEE INT S SM, V15, P420
   Yanambaka VP, 2019, IEEE T CONSUM ELECTR, V65, P388, DOI 10.1109/TCE.2019.2926192
   Zhang YH, 2019, IEEE ACCESS, V7, P114721, DOI 10.1109/ACCESS.2019.2936123
   Zhou J, 2020, IEEE T INF FOREN SEC, V15, P420, DOI 10.1109/TIFS.2019.2923156
NR 34
TC 10
Z9 10
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 64
DI 10.1145/3398039
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100007
DA 2024-07-18
ER

PT J
AU Qi, L
   Wang, L
   Huo, J
   Shi, YH
   Gao, Y
AF Qi, Lei
   Wang, Lei
   Huo, Jing
   Shi, Yinghuan
   Gao, Yang
TI GreyReID: A Novel Two-stream Deep Framework with RGB-grey Information
   for Person Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; greyscale person images; two-stream deep
   framework
AB In this article, we observe that most false positive images (i.e., different identities with query images) in the top ranking list usually have the similar color information with the query image in person re-identification (Re-ID). Meanwhile, when we use the greyscale images generated from RGB images to conduct the person Re-ID task, some hard query images can obtain better performance compared with using RGB images. Therefore, RGB and greyscale images seem to be complementary to each other for person Re-ID. In this article, we aim to utilize both RGB and greyscale images to improve the person Re-ID performance. To this end, we propose a novel two-stream deep neural network with RGB-grey information, which can effectively fuse RGB and greyscale feature representations to enhance the generalization ability of Re-ID. First, we convert RGB images to greyscale images in each training batch. Based on these RGB and greyscale images, we train the RGB and greyscale branches, respectively. Second, to build up connections between RGB and greyscale branches, we merge the RGB and greyscale branches into a new joint branch. Finally, we concatenate the features of all three branches as the final feature representation for Re-ID. Moreover, in the training process, we adopt the joint learning scheme to simultaneously train each branch by the independent loss function, which can enhance the generalization ability of each branch. Besides, a global loss function is utilized to further fine-tune the final concatenated feature. The extensive experiments on multiple benchmark datasets fully show that the proposed method can outperform the state-of-the-art person Re-ID methods. Furthermore, using greyscale images can indeed improve the person Re-ID performance in the proposed deep framework.
C1 [Qi, Lei; Huo, Jing; Shi, Yinghuan; Gao, Yang] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
   [Wang, Lei] Univ Wollongong, Sch Comp & Informat Technol, Wollongong, NSW, Australia.
C3 Nanjing University; University of Wollongong
RP Gao, Y (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
EM qilei.cs@gmail.com; leiw@uow.edu.au; huojing@nju.edu.cn; syh@nju.edu.cn;
   gaoy@nju.edu.cn
RI Wang, Lei/D-9079-2013; GAO, Yang/HMO-8142-2023
OI Qi, Lei/0000-0001-7091-0702
FU Natural Science Foundation of China [61806092, 61673203]; Jiangsu
   Natural Science Foundation [BK20180326]; National Key Research and
   Development Program of China [2019YFC0118300, 2018AAA0100900,
   2018AAA0100905]
FX This work was supported by Natural Science Foundation of China (No.
   61806092 and 61673203), Jiangsu Natural Science Foundation (No.
   BK20180326), National Key Research and Development Program of China (No.
   2019YFC0118300), and Science and Technology Innovation 2030-"New
   Generation Artificial Intelligence" Major Project (No. 2018AAA0100900
   and 2018AAA0100905).
CR Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Dai Zuozhuo, 2018, ARXIV PREPRINT ARXIV
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2040, DOI 10.1145/3240508.3240550
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Lei Qi, 2018, MASKREID AMASK BASED
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P737, DOI 10.1145/3240508.3240585
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817
   Qi L, 2018, IEEE J-STSP, V12, P1263, DOI 10.1109/JSTSP.2018.2877475
   Radford A., 2015, ARXIV
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun Gang, 2018, IEEE C COMPUTER VISI
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Tian MQ, 2018, PROC CVPR IEEE, P5794, DOI 10.1109/CVPR.2018.00607
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Wu AC, 2020, INT J COMPUT VISION, V128, P1765, DOI 10.1007/s11263-019-01290-1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhang Q, 2019, LECT NOTES COMPUT SC, V11818, P455, DOI 10.1007/978-3-030-31456-9_50
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zheng Zhedong, 2019, CVPR
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
NR 52
TC 18
Z9 19
U1 2
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 27
DI 10.1145/3419439
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200007
DA 2024-07-18
ER

PT J
AU Yang, PH
   Kong, LH
   Qiu, MK
   Liu, X
   Chen, GH
AF Yang, Peihao
   Kong, Linghe
   Qiu, Meikang
   Liu, Xue
   Chen, Guihai
TI Compressed Imaging Reconstruction with Sparse Random Projection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Compressed sensing; random projection; cloud computing; internet of
   things; transform domain
ID SIGNAL RECONSTRUCTION; ALGORITHM
AB As the Internet of Things thrives, monitors and cameras produce tons of image data every day. To efficiently process these images, many compressed imaging frameworks are proposed. A compressed imaging framework comprises two parts, image signal measurement and reconstruction. Although a plethora of measurement devices have been designed, the development of the reconstruction is relatively lagging behind. Nowadays, most of existing reconstruction algorithms in compressed imaging are optimization problem solvers based on specific priors. The computation burdens of these optimization algorithms are enormous and the solutions are usually local optimums. Meanwhile, it is inconvenient to deploy these algorithms on cloud, which hinders the popularization of compressed imaging. In this article, we dive deep into the random projection to build reconstruction algorithms for compressed imaging. We first fully utilize the information in the measurement procedure and propose a combinatorial sparse random projection (SRP) reconstruction algorithm. Then, we generalize the SRP to a novel distributed algorithm called Cloud-SRP (CSRP), which enables efficient reconstruction on cloud. Moreover, we explore the combination of SRP with conventional optimization reconstruction algorithms and propose the Iterative-SRP (ISRP), which converges to a guaranteed fixed point. With minor modifications on the naive optimization algorithms, the ISRP yields better reconstructions. Experiments on real ghost imaging reconstruction reveal that our algorithms are effective. And simulation experiments show their advantages over the classical algorithms.
C1 [Yang, Peihao; Kong, Linghe; Chen, Guihai] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
   [Qiu, Meikang] Texas A&M Univ Commerce, Commerce, TX USA.
   [Liu, Xue] McGill Univ, Montreal, PQ, Canada.
C3 Shanghai Jiao Tong University; Texas A&M University System; Texas A&M
   University Commerce; McGill University
RP Kong, LH (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
EM yangpeihao@sjtu.edu.com; linghe.kong@sjtu.edu.com; qiumeikang@yahoo.com;
   xueliu@cs.mcgill.ca; gchen@cs.sjtu.edu.cn
OI Chen, Guihai/0000-0002-6934-1685
FU NSFC [61972253, 61672349, U190820096, 7191101302, 61672353, 61672348];
   Program for Professor of Special Appointment (Eastern Scholar) at
   Shanghai Institutions of Higher Learning
FX This work is supported in part by NSFC grants 61972253, 61672349,
   U190820096, 7191101302, 61672353, 61672348, the Program for Professor of
   Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher
   Learning. An early version of this article was presented at the IEEE
   International Conference on Smart Cloud 2019 [31].
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès E, 2006, MULTISCALE MODEL SIM, V5, P861, DOI 10.1137/05064182X
   Chan SH, 2017, IEEE T COMPUT IMAG, V3, P84, DOI 10.1109/TCI.2016.2629286
   Chang LH, 2014, PR IEEE SEN ARRAY, P405, DOI 10.1109/SAM.2014.6882428
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Duarte MF, 2005, 2005 39th Asilomar Conference on Signals, Systems and Computers, Vols 1 and 2, P1537
   Figueiredo MAT, 2006, IEEE IMAGE PROC, P2633
   Fowler JE, 2011, AUGMENT VIS REAL, V3, P31, DOI 10.1007/978-3-642-14212-3_3
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu XY, 2016, INT GEOSCI REMOTE SE, P1018, DOI 10.1109/IGARSS.2016.7729258
   Jalali S, 2019, IEEE T INFORM THEORY, V65, P8005, DOI 10.1109/TIT.2019.2940666
   Ke J, 2009, OPT COMMUN, V282, P185, DOI 10.1016/j.optcom.2008.09.083
   Kong LH, 2016, IEEE COMMUN MAG, V54, P53, DOI 10.1109/MCOM.2016.7588229
   Kutyniok G, 2016, ACM T MATH SOFTWARE, V42, DOI 10.1145/2740960
   Lee YB, 2016, NEUROIMAGE, V125, P1032, DOI 10.1016/j.neuroimage.2015.10.081
   Li P., 2014, ARXIVSTATME14082504
   Li YH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2903717
   Liu XL, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23363-w
   Liu XM, 2019, IEEE T IMAGE PROCESS, V28, P1068, DOI 10.1109/TIP.2018.2872175
   Liu XY, 2015, IEEE T PARALL DISTR, V26, P2188, DOI 10.1109/TPDS.2014.2345257
   Llull P, 2013, OPT EXPRESS, V21, P10526, DOI 10.1364/OE.21.010526
   Mostapha AMM, 2018, PROCEDIA MANUF, V22, P420, DOI 10.1016/j.promfg.2018.03.066
   Palangi H, 2016, IEEE T SIGNAL PROCES, V64, P4504, DOI 10.1109/TSP.2016.2557301
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Rani M, 2018, IEEE ACCESS, V6, P4875, DOI 10.1109/ACCESS.2018.2793851
   Taubman D. S, 2002, JPEG2000: Image Compression Fundamentals, Standards and Practice, V11
   Wang PY, 2013, FUTURE GENER COMP SY, V29, P1963, DOI 10.1016/j.future.2013.05.002
   Yang PH, 2019, 4TH IEEE INTERNATIONAL CONFERENCE ON SMART CLOUD (SMARTCLOUD 2019) / 3RD INTERNATIONAL SYMPOSIUM ON REINFORCEMENT LEARNING (ISRL 2019), P193, DOI 10.1109/SmartCloud.2019.00040
   Yang PH, 2020, IEEE T IMAGE PROCESS, V29, P6466, DOI 10.1109/TIP.2020.2989550
   Yazdanpanah A. P, 2017, ELECT IMAGING, V2017, P5, DOI [10.2352/ISSN.2470-1173.2017.13.IPAS-197, DOI 10.2352/ISSN.2470-1173.2017.13.IPAS-197]
   Yuan X, 2020, PROC CVPR IEEE, P1444, DOI 10.1109/CVPR42600.2020.00152
   Yuan X, 2016, IEEE IMAGE PROC, P2539, DOI 10.1109/ICIP.2016.7532817
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang S, 2018, MATH PROGRAM, V169, P307, DOI 10.1007/s10107-018-1236-x
   Zhang Y., 2020, P 34 AAAI C ART INT
   Zhussip M, 2019, P IEEE CVF C COMP VI
NR 39
TC 1
Z9 1
U1 2
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 26
DI 10.1145/3447431
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200006
DA 2024-07-18
ER

PT J
AU Agrawal, U
   Arora, J
   Singh, R
   Gupta, D
   Khanna, A
   Khamparia, A
AF Agrawal, Utkarsh
   Arora, Jatin
   Singh, Rahul
   Gupta, Deepak
   Khanna, Ashish
   Khamparia, Aditya
TI Hybrid Wolf-Bat Algorithm for Optimization of Connection Weights in
   Multi-layer Perceptron
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Neural network; hybrid; bat algorithm; grey wolf optimization;
   optimization; connection weights; nature inspired; MLP
ID FEEDFORWARD NEURAL-NETWORKS; CLASSIFICATION
AB In a neural network, the weights act as parameters to determine the output(s) from a set of inputs. The weights are used to find the activation values of nodes of a layer from the values of the previous layer. Finding the ideal set of these weights for training a Multi-layer Perceptron neural network such that it minimizes the classification error is a widely known optimization problem. The presented article proposes a Hybrid Wolf-Bat algorithm, a novel optimization algorithm, as a solution to solve the discussed problem. The proposed algorithm is a hybrid of two already existing nature-inspired algorithms, Grey Wolf Optimization algorithm and Bat algorithm. The novel introduced approach is tested on ten different datasets of the medical field, obtained from the UCI machine learning repository. The performance of the proposed algorithm is compared with the recently developed nature-inspired algorithms: Grey Wolf Optimization algorithm, Cuckoo Search, Bat Algorithm, and Whale Optimization Algorithm, along with the standard Back-propagation training method available in the literature. The obtained results demonstrate that the proposed method outperforms other bio-inspired algorithms in terms of both speed of convergence and accuracy.
C1 [Agrawal, Utkarsh; Arora, Jatin; Singh, Rahul; Gupta, Deepak; Khanna, Ashish] Maharaja Agrasen Inst Technol, Delhi, India.
   [Khamparia, Aditya] Lovely Profess Univ, Punjab, India.
C3 Maharaja Agrasen Institute of Technology; Lovely Professional University
RP Gupta, D (corresponding author), Maharaja Agrasen Inst Technol, Delhi, India.
EM utkrsh.agrawal@gmail.com; jatin51997@gmail.com; rahulsingh97r@gmail.com;
   deepakgupta@mait.ac.in; ashishkhanna@mait.ac.in;
   aditya.khamparia88@gmail.com
RI Gupta, Deepak/AAV-2728-2020
OI Gupta, Deepak/0000-0002-3019-7161; Khanna, Ashish/0000-0002-8418-3929
CR Aljarah I, 2018, SOFT COMPUT, V22, P1, DOI 10.1007/s00500-016-2442-1
   Dixit Abhishek, 2016, HYBRID NATURE INSPIR, DOI 10.1007%2F978-981-10-5272-9_29
   Faris H, 2019, INT J MACH LEARN CYB, V10, P2901, DOI 10.1007/s13042-018-00913-2
   Faris H, 2018, APPL INTELL, V48, P445, DOI 10.1007/s10489-017-0967-3
   Faris H, 2016, APPL INTELL, V45, P322, DOI 10.1007/s10489-016-0767-1
   Gupta D, 2019, MEASUREMENT, V143, P180, DOI 10.1016/j.measurement.2019.01.002
   Heidari AA, 2019, SOFT COMPUT, V23, P7941, DOI 10.1007/s00500-018-3424-2
   Jaddi NS, 2015, APPL SOFT COMPUT, V37, P71, DOI 10.1016/j.asoc.2015.08.002
   Krose B., 1993, J COMPUT SCI, V48, P1
   Mirjalili S, 2015, APPL INTELL, V43, P150, DOI 10.1007/s10489-014-0645-7
   Mirjalili S, 2014, ADV ENG SOFTW, V69, P46, DOI 10.1016/j.advengsoft.2013.12.007
   Nandy S., 2012, Int. J. Computer Applications, V43, P8, DOI DOI 10.5120/6401-8339
   Ojha VK, 2017, ENG APPL ARTIF INTEL, V60, P97, DOI 10.1016/j.engappai.2017.01.013
   Ojha VK, 2017, APPL SOFT COMPUT, V52, P909, DOI 10.1016/j.asoc.2016.09.035
   Ruiz LGB, 2018, EXPERT SYST APPL, V92, P380, DOI 10.1016/j.eswa.2017.09.059
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shandilya S. K., 2018, ADV NATURE INSPIRED
   Socha K, 2007, NEURAL COMPUT APPL, V16, P235, DOI 10.1007/s00521-007-0084-z
   Tiwari S.P., 2018, INT J APPL ENG RES, V6, P4282
   Valipour M, 2016, METEOROL APPL, V23, P91, DOI 10.1002/met.1533
   Villarrubia G, 2018, NEUROCOMPUTING, V272, P10, DOI 10.1016/j.neucom.2017.04.075
   Yang FB, 2018, ENERG CONVERS MANAGE, V164, P15, DOI 10.1016/j.enconman.2018.02.062
   Yang XB, 2010, ADV INFORM KNOWL PRO, P65, DOI 10.1007/978-1-84882-628-1_4
NR 23
TC 22
Z9 23
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 37
DI 10.1145/3350532
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300019
DA 2024-07-18
ER

PT J
AU Do, TT
   Hoang, T
   Tan, DKL
   Le, H
   Nguyen, T
   Cheung, NM
AF Thanh-Toan Do
   Tuan Hoang
   Dang-Khoa Le Tan
   Le, Huu
   Nguyen, Tam, V
   Cheung, Ngai-Man
TI From Selective Deep Convolutional Features to Compact Binary
   Representations for Image Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Content based image retrieval; image hashing; embedding; aggregating;
   deep convolutional features; unsupervised
ID QUANTIZATION
AB In the large-scale image retrieval task, the two most important requirements are the discriminability of image representations and the efficiency in computation and storage of representations. Regarding the former requirement, Convolutional Neural Network is proven to be a very powerful tool to extract highly discriminative local descriptors for effective image search. Additionally, to further improve the discriminative power of the descriptors, recent works adopt fine-tuned strategies. In this article, taking a different approach, we propose a novel, computationally efficient, and competitive framework. Specifically, we first propose various strategies to compute masks, namely, SIFT-masks, SUM-mask, and MAX-mask, to select a representative subset of local convolutional features and eliminate redundant features. Our in-depth analyses demonstrate that proposed masking schemes are effective to address the burstiness drawback and improve retrieval accuracy. Second, we propose to employ recent embedding and aggregating methods that can significantly boost the feature discriminability. Regarding the computation and storage efficiency, we include a hashing module to produce very compact binary image representations. Extensive experiments on six image retrieval benchmarks demonstrate that our proposed framework achieves the state-of-the-art retrieval performances.
C1 [Thanh-Toan Do] Univ Liverpool, Dept Comp Sci, Ashton St, Liverpool L69 3BX, Merseyside, England.
   [Tuan Hoang; Dang-Khoa Le Tan; Cheung, Ngai-Man] Singapore Univ Technol & Design, Singapore, Singapore.
   [Le, Huu] Queensland Univ Technol, Sci Engn Fac, Level 12,S Block,2 George St, Brisbane, Qld 4000, Australia.
   [Nguyen, Tam, V] Univ Dayton, Dept Comp Sci, 300 Coll Pk, Dayton, OH 45469 USA.
C3 University of Liverpool; Singapore University of Technology & Design;
   Queensland University of Technology (QUT); University System of Ohio;
   University of Dayton
RP Do, TT (corresponding author), Univ Liverpool, Dept Comp Sci, Ashton St, Liverpool L69 3BX, Merseyside, England.
EM thanh-toan.do@liverpool.ac.uk; nguyenanhtuan_hoang@mymail.sutd.edu.sg;
   letandang_khoa@sutd.edu.sg; huu.le@qut.edu.au;
   ngaiman_cheung@sutd.edu.sg
RI Nguyen, Tam/HSG-3007-2023; Nguyen, Tam/AAU-6504-2020
OI Nguyen, Tam/0000-0003-0236-7992; NGUYEN ANH TUAN,
   HOANG/0000-0002-1076-8043
CR [Anonymous], P ACM MM
   [Anonymous], P ACM MM
   [Anonymous], 2015, CVPR
   [Anonymous], P ICLR
   [Anonymous], P CVPR
   [Anonymous], 2016, P ECCV
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P CVPRW
   [Anonymous], P ACM MM
   [Anonymous], 2007, P CVPR
   [Anonymous], 2018, P AAAI
   [Anonymous], P ACM MM
   [Anonymous], 2015, P ICCV
   [Anonymous], P ECCV
   [Anonymous], 2014, P IEEE C COMPUTER VI
   [Anonymous], 2014, P CVPR
   [Anonymous], P CVPR
   [Anonymous], P WACV
   [Anonymous], P ECCV WORKSH
   [Anonymous], P ECCV
   [Anonymous], 2015, CORR
   [Anonymous], P CVPR
   [Anonymous], P ACM MM
   [Anonymous], P ECCV
   [Anonymous], P ECCV
   [Anonymous], P ACM MM
   [Anonymous], MATCONVNET CONVOLUTI
   [Anonymous], P CVPR
   [Anonymous], 2014, P ECCV
   [Anonymous], SIMULTANEOUS COMPRES
   [Anonymous], IEEE T IMAGE PROCESS
   [Anonymous], 2008, P ECCV
   [Anonymous], 2017, ARXIV171102512
   [Anonymous], 2012, P CVPR
   [Anonymous], P ICCV
   [Anonymous], P CVPR WORKSH
   [Anonymous], P CVPR
   [Anonymous], P CVPR
   [Anonymous], 2008, P CVPR
   [Anonymous], 2017, P ICCV
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2010, P CVPR
   [Anonymous], 2003, P ICCV
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P CVPR
   [Anonymous], P CVPR
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], P ICCV
   [Anonymous], 2014, P ECCV
   [Anonymous], P ECCV
   Arandjelovic Relja, 2016, P CVPR
   Do TT, 2018, IEEE T PATTERN ANAL, V40, P626, DOI 10.1109/TPAMI.2017.2686861
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Grauman K., 2013, Machine Learning for Computer Vision, V411, P49, DOI 10.1007/978-3-642-28661-2_3
   He K., 2017, P ICCV, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Je<prime>gou H., 2012, P ECCV, P5
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2010, INT J COMPUT VISION, V87, P316, DOI 10.1007/s11263-009-0285-2
   Krizhevsky A., 2012, Advances in Neural Information Processing Systems, V25, P1106
   Liu ZQ, 2017, IEEE T IMAGE PROCESS, V26, P3128, DOI 10.1109/TIP.2017.2660244
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Simonyan K., 2014, Very Deep Convolutional Networks for Large-Scale Image Recognition
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2018, PATTERN RECOGN, V75, P175, DOI 10.1016/j.patcog.2017.03.021
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Weiss Y., 2008, P NIPS, P1
   Xu J., 2018, P AAAI
   Zeiler MD, 2014, EUR C COMP VIS ZUR S
NR 71
TC 19
Z9 20
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 43
DI 10.1145/3314051
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IM3ZV
UT WOS:000477935400014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yu, Y
   Tang, SH
   Raposo, F
   Chen, L
AF Yu, Yi
   Tang, Suhua
   Raposo, Francisco
   Chen, Lei
TI Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music
   Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Convolutional neural networks; deep cross-modal models; correlation
   learning between audio and lyrics; cross-modal music retrieval; music
   knowledge discovery
ID CLASSIFICATION; FEATURES
AB Deep cross-modal learning has successfully demonstrated excellent performance in cross-modal multimedia retrieval, with the aim of learning joint representations between different data modalities. Unfortunately, little research focuses on cross-modal correlation learning where temporal structures of different data modalities, such as audio and lyrics, should be taken into account. Stemming from the characteristic of temporal structures of music in nature, we are motivated to learn the deep sequential correlation between audio and lyrics. In this work, we propose a deep cross-modal correlation learning architecture involving two-branch deep neural networks for audio modality and text modality (lyrics). Data in different modalities are converted to the same canonical space where intermodal canonical correlation analysis is utilized as an objective function to calculate the similarity of temporal structures. This is the first study that uses deep architectures for learning the temporal correlation between audio and lyrics. A pretrained Doc2Vec model followed by fully connected layers is used to represent lyrics. Two significant contributions are made in the audio branch, as follows: (i) We propose an end-to-end network to learn cross-modal correlation between audio and lyrics, where feature extraction and correlation learning are simultaneously performed and joint representation is learned by considering temporal structures. (ii) And, as for feature extraction, we further represent an audio signal by a short sequence of local summaries (VGG16 features) and apply a recurrent neural network to compute a compact feature that better learns the temporal structures of music audio. Experimental results, using audio to retrieve lyrics or using lyrics to retrieve audio, verify the effectiveness of the proposed deep correlation learning architectures in cross-modal music retrieval.
C1 [Yu, Yi] Natl Inst Informat, Chiyoda Ku, 2-1-2 Hitotsubashi, Tokyo 1018430, Japan.
   [Tang, Suhua] Univ Electrocommun, 1-5-1 Chofugaoka, Chofu, Tokyo 1828585, Japan.
   [Raposo, Francisco] Univ Lisbon, INESC ID Lisboa R Alves Redol 9, P-1000029 Lisbon, Portugal.
   [Chen, Lei] Hong Kong Univ Sci & Technol, Kowloon, Clear Water Bay, Hong Kong, Peoples R China.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; University of
   Electro-Communications - Japan; Universidade de Lisboa; Hong Kong
   University of Science & Technology
RP Yu, Y (corresponding author), Natl Inst Informat, Chiyoda Ku, 2-1-2 Hitotsubashi, Tokyo 1018430, Japan.
EM yiyu@nii.ac.jp; shtang@uec.ac.jp; francisco.afonso.raposo@ist.utl.pt;
   leichen@cse.ust.hk
RI Tang, Suhua/AFL-7221-2022
OI Tang, Suhua/0000-0002-5784-8411; Raposo, Francisco/0000-0003-1044-5989
CR Acar Esra, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P303, DOI 10.1007/978-3-319-04114-8_26
   Andrew G., 2013, P ICML, P1247
   [Anonymous], 2017, ABS170406761 CORR
   [Anonymous], ABS170301789 CORR
   [Anonymous], P 14 INT C MUS INF R
   [Anonymous], P WORKSH ART INT STA
   [Anonymous], 2010, P 18 ACM INT C MULTI
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], ABS170704678 CORR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], ABS161104496 CORR
   [Anonymous], ABS160705368 CORR
   [Anonymous], P 19 IEEE INT S MULT
   [Anonymous], ABS161002947 CORR
   Cao Y, 2017, AAAI CONF ARTIF INTE, P3974
   Choi Keunwoo, 2016, ARXIV160600298, DOI 10.5281/zenodo.1416254
   Costa YMG, 2017, APPL SOFT COMPUT, V52, P28, DOI 10.1016/j.asoc.2016.12.024
   Gillet O, 2007, IEEE T CIRC SYST VID, V17, P347, DOI 10.1109/TCSVT.2007.890831
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Malheiro R, 2018, IEEE T AFFECT COMPUT, V9, P240, DOI 10.1109/TAFFC.2016.2598569
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Mayer R, 2011, LECT NOTES COMPUT SC, V6731, P357, DOI 10.1007/978-3-642-21566-7_36
   McVicar Matt., 2011, P 12 INT SOC MUS INF, P783
   Mihalcea Rada., 2012, P 2012 JOINT C EMPIR, P590
   Nanni L, 2016, EXPERT SYST APPL, V45, P108, DOI 10.1016/j.eswa.2015.09.018
   Nettl B, 2000, ORIGINS OF MUSIC, P463
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Shah RR, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P607, DOI 10.1145/2647868.2654919
   Sigtia Siddharth, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6959, DOI 10.1109/ICASSP.2014.6854949
   Simonyan K., 2014, 14091556 ARXIV
   Wu XX, 2016, IEEE T MULTIMEDIA, V18, P1305, DOI 10.1109/TMM.2016.2557722
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yu Y., 2009, ACM Multimedia, P341
   Yu Y, 2019, IEEE T NEUR NET LEAR, V30, P1250, DOI 10.1109/TNNLS.2018.2856253
   Yu Y, 2013, IEEE T MULTIMEDIA, V15, P1969, DOI 10.1109/TMM.2013.2269313
   Zhong CL, 2017, LECT NOTES COMPUT SC, V10366, P169, DOI 10.1007/978-3-319-63579-8_14
NR 42
TC 62
Z9 65
U1 4
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
AR 20
DI 10.1145/3281746
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9JB
UT WOS:000459798800004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Feng, ZH
   Kittler, J
   Christmas, B
   Wu, XJ
AF Feng, Zhen-Hua
   Kittler, Josef
   Christmas, Bill
   Wu, Xiao-Jun
TI A Unified Tensor-based Active Appearance Model
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face image analysis; active appearance model; tensor algebra; missing
   training samples; cascaded regression
ID FACIAL LANDMARK LOCALIZATION; FACE ALIGNMENT; REGRESSION; AAM
AB Appearance variations result in many difficulties in face image analysis. To deal with this challenge, we present a Unified Tensor-based Active Appearance Model (UT-AAM) for jointly modelling the geometry and texture information of 2D faces. For each type of face information, namely shape and texture, we construct a unified tensor model capturing all relevant appearance variations. This contrasts with the variation-specific models of the classical tensor AAM. To achieve the unification across pose variations, a strategy for dealing with self-occluded faces is proposed to obtain consistent shape and texture representations of pose-varied faces. In addition, our UT-AAM is capable of constructing the model from an incomplete training dataset, using tensor completion methods. Last, we use an effective cascaded-regression-based method for UT-AAM fitting. With these advancements, the utility of UT-AAM in practice is considerably enhanced. As an example, we demonstrate the improvements in training facial landmark detectors through the use of UT-AAM to synthesise a large number of virtual samples. Experimental results obtained on a number of well-known face datasets demonstrate the merits of the proposed approach.
C1 [Feng, Zhen-Hua; Kittler, Josef; Christmas, Bill] Univ Surrey, 388 Stag Hill, Guildford GU2 7XH, Surrey, England.
   [Wu, Xiao-Jun] Jiangnan Univ, 1800 Lihu Ave, Wuxi 214122, Jiangsu, Peoples R China.
C3 University of Surrey; Jiangnan University
RP Feng, ZH (corresponding author), Univ Surrey, 388 Stag Hill, Guildford GU2 7XH, Surrey, England.
EM z.feng@surrey.ac.uk; j.kittler@surrey.ac.uk; w.christmas@surrey.ac.uk;
   wu_xiaojun@jiangnan.edu.cn
RI Feng, Zhenhua/T-3139-2019
OI Feng, Zhenhua/0000-0002-4485-4249
FU EPSRC Programme Grant (FACER2VM) [EP/N007743/1]; National Natural
   Science Foundation of China [61902153, 61672265, 61876072, 61602390];
   NVIDIA GPU Grant Program; EPSRC/dstl/MURI project [EP/R018456/1]
FX This work was supported in part by the EPSRC Programme Grant (FACER2VM)
   EP/N007743/1, EPSRC/dstl/MURI project EP/R018456/1, the National Natural
   Science Foundation of China (61902153, 61672265, 61876072, 61602390),
   and the NVIDIA GPU Grant Program.
CR Acar E, 2011, CHEMOMETR INTELL LAB, V106, P41, DOI 10.1016/j.chemolab.2010.08.004
   Alabort-i-Medina J, 2017, INT J COMPUT VISION, V121, P26, DOI 10.1007/s11263-016-0916-3
   Alex M, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P3
   [Anonymous], 2006, P 18 INT C NEURAL IN
   [Anonymous], 2018, ELECT J QUAL THEORY, DOI DOI 10.1155/2018/8345893
   [Anonymous], 1995, COMPUTER VISION IMAG
   Antonakos E, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2431445
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Duong CN, 2019, INT J COMPUT VISION, V127, P437, DOI 10.1007/s11263-018-1113-3
   Duong N, 2015, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2015.7299111
   Choi SI, 2018, IEEE ACCESS, V6, P13663, DOI 10.1109/ACCESS.2018.2812725
   Cootes T. F., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P227, DOI 10.1109/AFGR.2000.840639
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cootes TF, 2012, LECT NOTES COMPUT SC, V7578, P278, DOI 10.1007/978-3-642-33786-4_21
   Cristinacce D., 2006, BRIT MACH VIS C, V1, P3
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Dong XY, 2018, PROC CVPR IEEE, P360, DOI 10.1109/CVPR.2018.00045
   Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047
   Donner R, 2006, IEEE T PATTERN ANAL, V28, P1690, DOI 10.1109/TPAMI.2006.206
   Fan X, 2018, IEEE T MULTIMEDIA, V20, P567, DOI 10.1109/TMM.2017.2751143
   Fan X, 2015, IEEE T IMAGE PROCESS, V24, P1164, DOI 10.1109/TIP.2015.2390976
   Feng ZH, 2019, IEEE SIGNAL PROC LET, V26, P450, DOI 10.1109/LSP.2019.2895291
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   Feng ZH, 2017, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2017.392
   Feng ZH, 2017, IEEE COMPUT SOC CONF, P2106, DOI 10.1109/CVPRW.2017.262
   Feng ZH, 2015, IEEE T IMAGE PROCESS, V24, P3425, DOI 10.1109/TIP.2015.2446944
   Feng ZH, 2015, IEEE SIGNAL PROC LET, V22, P76, DOI 10.1109/LSP.2014.2347011
   Geng X, 2011, IEEE T SYST MAN CY B, V41, P881, DOI 10.1109/TSMCB.2010.2097588
   Giffney N, 2011, NEW MIDDLE AGES, P1
   Gonzalez-Mora J, 2007, IEEE I CONF COMP VIS, P2776
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo YA, 2016, IEEE T MULTIMEDIA, V18, P1977, DOI 10.1109/TMM.2016.2597007
   Hamsici OC, 2009, IEEE I CONF COMP VIS, P1003, DOI 10.1109/ICCV.2009.5459365
   Harshman R.A., 1970, UCLA Working Papers in Phonetics, V16, P84, DOI DOI 10.1134/S0036023613040165
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Huber P, 2015, IEEE IMAGE PROC, P1195, DOI 10.1109/ICIP.2015.7350989
   Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454
   Kittipanya-Ngam P, 2006, INT C PATT RECOG, P328
   Kittler J, 2016, LECT NOTES COMPUT SC, V9756, P185, DOI 10.1007/978-3-319-41778-3_19
   Koestinger M., 2011, IEEE INT C COMP VIS, P2144, DOI DOI 10.1109/ICCVW.2011.6130513
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Koppen P, 2018, PATTERN RECOGN, V74, P617, DOI 10.1016/j.patcog.2017.09.006
   Lee HS, 2009, IEEE T PATTERN ANAL, V31, P1102, DOI 10.1109/TPAMI.2008.286
   Lin Dahua., 2005, Proceedings of International Conference on Image Processing, V2, P386
   Liu QS, 2016, IEEE T IMAGE PROCESS, V25, P700, DOI 10.1109/TIP.2015.2502485
   Liu XM, 2009, IEEE T PATTERN ANAL, V31, P1941, DOI 10.1109/TPAMI.2008.238
   Lucey S, 2009, IMAGE VISION COMPUT, V27, P1804, DOI 10.1016/j.imavis.2009.03.002
   Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16
   Nie XS, 2017, IEEE T MULTIMEDIA, V19, P785, DOI 10.1109/TMM.2016.2629758
   Qi C, 2018, IEEE ACCESS, V6, P18795, DOI 10.1109/ACCESS.2018.2816044
   Qi N, 2016, PROC CVPR IEEE, P5916, DOI 10.1109/CVPR.2016.637
   Reale MJ, 2011, IEEE T MULTIMEDIA, V13, P474, DOI 10.1109/TMM.2011.2120600
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Romdhani S., 1999, BMVC99. Proceedings of the 10th British Machine Vision Conference, P483
   Romdhani S., 2000, EUROPEAN C COMPUTER, P799
   Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002
   Saragih J, 2007, IEEE I CONF COMP VIS, P2173
   Sclaroff S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1146, DOI 10.1109/ICCV.1998.710860
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XN, 2018, IEEE T INF FOREN SEC, V13, P2734, DOI 10.1109/TIFS.2018.2833052
   Stegmann MB, 2003, IMAGE VISION COMPUT, V21, P61, DOI 10.1016/S0262-8856(02)00126-9
   Sung JW, 2007, INT J COMPUT VISION, V75, P297, DOI 10.1007/s11263-006-0034-8
   Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Tzimiropoulos G, 2014, IEEE T INF FOREN SEC, V9, P2024, DOI 10.1109/TIFS.2014.2361018
   Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79
   Vasilescu MAO, 2002, LECT NOTES COMPUT SC, V2350, P447
   Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209
   Wang YL, 2008, PPAR RES, V2008, DOI 10.1155/2008/209629
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Wu XR, 2017, IEEE ACCESS, V5, P16649, DOI 10.1109/ACCESS.2017.2739822
   Wu YH, 2018, IMAGE VISION COMPUT, V73, P1, DOI 10.1016/j.imavis.2017.12.002
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yan JJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P392, DOI 10.1109/ICCVW.2013.126
   Zhang J, 2016, PROC CVPR IEEE, P3428, DOI 10.1109/CVPR.2016.373
   Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1
   Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 83
TC 8
Z9 8
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2019
VL 15
IS 3
SU S
SI SI
AR 86
DI 10.1145/3338841
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LR5EU
UT WOS:000535718800002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tan, M
   Yu, J
   Yu, Z
   Gao, F
   Rui, Y
   Tao, DC
AF Tan, Min
   Yu, Jun
   Yu, Zhou
   Gao, Fei
   Rui, Yong
   Tao, Dacheng
TI User-Click-Data-Based Fine-Grained Image Recognition via Weakly
   Supervised Metric Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Metric learning; fine-grained image recognition; user click data;
   convolutional neural network; weakly supervised learning
ID FEATURES; CONSTRAINTS
AB We present a novel fine-grained image recognition framework using user click data, which can bridge the semantic gap in distinguishing categories that are similar in visual. As query set in click data is usually large-scale and redundant, we first propose a click-feature-based query-merging approach to merge queries with similar semantics and construct a compact click feature. Afterward, we utilize this compact click feature and convolutional neural network (CNN)-based deep visual feature to jointly represent an image. Finally, with the combined feature, we employ the metriclearning-based template-matching scheme for efficient recognition. Considering the heavy noise in the training data, we introduce a reliability variable to characterize the image reliability, and propose a weakly-supervised metric and template leaning with smooth assumption and click prior (WMTLSC) method to jointly learn the distance metric, object templates, and image reliability. Extensive experiments are conducted on a public Clickture-Dog dataset and our newly established Clickture-Bird dataset. It is shown that the click-data-based query merging helps generating a highly compact (the dimension is reduced to 0.9%) and dense click feature for images, which greatly improves the computational efficiency. Also, introducing this click feature into CNN feature further boosts the recognition accuracy. The proposed framework performs much better than previous state-of-the-arts in fine-grained recognition tasks.
C1 [Tan, Min; Yu, Jun; Yu, Zhou; Gao, Fei] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, 1158,2nd Ave, Hangzhou 310018, Zhejiang, Peoples R China.
   [Rui, Yong] Lenovo, 6 Shang Di West Rd, Beijing 100085, Peoples R China.
   [Tao, Dacheng] Univ Sydney, Fac Engn & Informat Technol, Sydney, NSW, Australia.
C3 Hangzhou Dianzi University; Legend Holdings; Lenovo; University of
   Sydney
RP Yu, J (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, 1158,2nd Ave, Hangzhou 310018, Zhejiang, Peoples R China.
EM yujun@hdu.edu.cn
RI Tao, Dacheng/A-5449-2012
OI Tao, Dacheng/0000-0001-7225-5449; Yu, Zhou/0000-0001-8407-1137
FU National Natural Science Foundation of China [61602136, 61622205,
   61472110, 61702143, 61601158]; Australian Research Council Projects
   [FL-170100117, DP-140102164, LP-150100671]; Zhejiang Provincial Natural
   Science Foundation of China [LR15F020002]; Zhejiang Provincial Key
   Science and Technology Project Foundation [2018C01012]
FX This work was supported by National Natural Science Foundation of China
   (No. 61602136, No. 61622205, No. 61472110, No. 61702143, and No.
   61601158), Australian Research Council Projects (FL-170100117,
   DP-140102164, and LP-150100671), and in part by the Zhejiang Provincial
   Natural Science Foundation of China (No. LR15F020002) and Zhejiang
   Provincial Key Science and Technology Project Foundation (No.
   2018C01012). We would like to thank Dr. Kuiyuan Yang for helpful
   discussions in the beginning of this article.
CR [Anonymous], 2014, INF SOFTW TECHNOL
   [Anonymous], 2011, Technical Report CNS-TR-2011-001
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], MULTIMEDIA TOOLS APP
   Bai YL, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P441, DOI 10.1145/2733373.2806243
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259
   Chun IY, 2016, IEEE INT CONF MULTI
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Feng Wu, 2017, FINE GRAINED IMAGE R
   Gao SH, 2014, IEEE T IMAGE PROCESS, V23, P623, DOI 10.1109/TIP.2013.2290593
   Gavves E, 2015, IEEE I CONF COMP VIS, P2731, DOI 10.1109/ICCV.2015.313
   Hua X.-S., 2013, Proceedings of the 21st ACM International Conference on Multimedia, Oct. 21-25, ACM, P243
   Jin JQ, 2014, IEEE T INTELL TRANSP, V15, P1991, DOI 10.1109/TITS.2014.2308281
   Jin QY, 2016, ELECTR CONTACT, P1, DOI 10.1109/HOLM.2016.7779999
   Khosla A., 2011, CVPR WORKSH
   Kuang ZZ, 2018, PATTERN RECOGN, V78, P198, DOI 10.1016/j.patcog.2018.01.027
   McFee B., 2010, ICML, P775
   Meng L., 2013, INT J HYBRID INFORM, V6
   Qian Q, 2015, PROC CVPR IEEE, P3716, DOI 10.1109/CVPR.2015.7298995
   RoyChowdhury Aruni, 2015, IEEE INT C COMP VIS
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarafis I, 2015, INT J MULTIMED INF R, V4, P129, DOI 10.1007/s13735-015-0080-5
   Song Q, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P169, DOI 10.1145/2733373.2809928
   Tan M, 2012, INTELLIGENT SCI INTE
   Tan M, 2016, 8TH INTERNATIONAL CONFERENCE ON INTERNET MULTIMEDIA COMPUTING AND SERVICE (ICIMCS2016), P118, DOI 10.1145/3007669.3007730
   Tan M, 2016, IEEE T INTELL TRANSP, V17, P1415, DOI 10.1109/TITS.2015.2506182
   Tan M, 2016, NEUROCOMPUTING, V181, P96, DOI 10.1016/j.neucom.2015.04.123
   Tan M, 2014, NEUROCOMPUTING, V139, P56, DOI 10.1016/j.neucom.2013.09.054
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yang XP, 2016, IEEE T IMAGE PROCESS, V25, P4617, DOI 10.1109/TIP.2016.2593653
   Yang XP, 2015, MULTIMEDIA SYST, V21, P217, DOI 10.1007/s00530-014-0379-8
   Yang XS, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2962719
   Yang XS, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700286
   Yin Y. P., 2015, CHINESE J GEOLOGICAL, V2, P1
   Yu J, 2017, IEEE T CYBERNETICS, V47, P4014, DOI 10.1109/TCYB.2016.2591583
   Yu J, 2015, IEEE T CYBERNETICS, V45, P767, DOI 10.1109/TCYB.2014.2336697
   Yu J, 2014, IEEE T IMAGE PROCESS, V23, P2019, DOI 10.1109/TIP.2014.2311377
   Yu J, 2014, IEEE T MULTIMEDIA, V16, P159, DOI 10.1109/TMM.2013.2284755
   Zhang HW, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2978656
   Zhang L., 2016, Mathematical Problems in Engineering, V2016, P1, DOI DOI 10.1089/NAT.2016.0626
   Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212
   Zhang N, 2013, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2013.96
   Zhang Y, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2659520
   Zheng GJ, 2017, IEEE INT CON MULTI, P661, DOI 10.1109/ICME.2017.8019407
NR 45
TC 12
Z9 12
U1 7
U2 28
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
AR 70
DI 10.1145/3209666
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GT7CI
UT WOS:000444677600003
DA 2024-07-18
ER

PT J
AU Wang, AQ
   Hu, HF
   Yang, L
AF Wang, Anqi
   Hu, Haifeng
   Yang, Liang
TI Image Captioning with Affective Guiding and Selective Attention
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image captioning; visual attention; convolutional neutral network;
   affective guiding and selective attention mechanism
AB Image captioning is an increasingly important problem associated with artificial intelligence, computer vision, and natural language processing. Recent works revealed that it is possible for a machine to generate meaningful and accurate sentences for images. However, most existing methods ignore latent emotional information in an image. In this article, we propose a novel image captioning model with Affective Guiding and Selective Attention Mechanism named AG-SAM. In our method, we aim to bridge the affective gap between image captioning and the emotional response elicited by the image. First, we introduce affective components that capture higher-level concepts encoded in images into AG-SAM. Hence, our language model can be adapted to generate sentences that are more passionate and emotive. In addition, a selective gate acting on the attention mechanism controls the degree of how much visual information AG-SAM needs. Experimental results have shown that our model outperforms most existing methods, clearly reflecting an association between images and emotional components that is usually ignored in existing works.
C1 [Wang, Anqi; Hu, Haifeng; Yang, Liang] Sun Yat Sen Univ, Sch Elect & Informat Engn, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM wanganq@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn;
   yangliang5@mail2.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; Natural Science Foundation of Guangdong Province
   [2017A030311029, 2016B010109002]; Science and Technology Program of
   Guangzhou, China [201704020180]; Fundamental Research Funds for the
   Central Universities of China
FX This work was supported in part by the National Natural Science
   Foundation of China, under Grant 61673402, 61273270 and 60802069, the
   Natural Science Foundation of Guangdong Province (2017A030311029 and
   2016B010109002), and by the Science and Technology Program of Guangzhou,
   China, under Grant 201704020180, and the Fundamental Research Funds for
   the Central Universities of China.
CR [Anonymous], P C EUR CHAPT ASS CO
   [Anonymous], PROC CVPR IEEE
   [Anonymous], PROC ACM INT CONF MU
   [Anonymous], 2014, PROC EUR C COMPUT VI
   [Anonymous], 1972, EMOTION HUMAN FACE
   Bahdanau Dzmitry, 2015, P 3 INT C LEARN REPR
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Grühn D, 2008, BEHAV RES METHODS, V40, P512, DOI 10.3758/BRM.40.2.512
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Jiang YG, 2014, AAAI CONF ARTIF INTE, P73
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Lin C.-Y., 2004, ROUGE PACKAGE AUTOMA, P10
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mou WX, 2016, IEEE COMPUT SOC CONF, P1478, DOI 10.1109/CVPRW.2016.185
   Mou Wenxuan, 2015, P IEEE INT C WORKSHO, P1
   ORTONY A, 1990, PSYCHOL REV, V97, P315, DOI 10.1037/0033-295X.97.3.315
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang X, 2014, IEEE IMAGE PROC, P2423, DOI 10.1109/ICIP.2014.7025490
   Xu K., 2015, COMPUTER SCI, P2048
   Yang L, 2017, ELECTRON LETT, V53, P1471, DOI 10.1049/el.2017.2351
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
NR 32
TC 12
Z9 13
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
AR 73
DI 10.1145/3226037
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GT7CI
UT WOS:000444677600006
DA 2024-07-18
ER

PT J
AU Wen, LY
   Qi, HG
   Lyu, SW
AF Wen, Longyin
   Qi, Honggang
   Lyu, Siwei
TI Contrast Enhancement Estimation for Digital Image Forensics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Media forensics; contrast enhancement; pixel histogram
AB Inconsistency in contrast enhancement can be used to expose image forgeries. In this work, we describe a new method to estimate contrast enhancement operations from a single image. Our method takes advantage of the nature of contrast enhancement as a mapping between pixel values and the distinct characteristics it introduces to the image pixel histogram. Our method recovers the original pixel histogram and the contrast enhancement simultaneously from a single image with an iterative algorithm. Unlike previous works, our method is robust in the presence of additive noise perturbations that are used to hide the traces of contrast enhancement. Furthermore, we also develop an effective method to detect image regions undergone contrast enhancement transformations that are different from the rest of the image, and we use this method to detect composite images. We perform extensive experimental evaluations to demonstrate the efficacy and efficiency of our method.
C1 [Wen, Longyin] GE Global Res Ctr, 1 Res Circle, Niskayuna, NY 12309 USA.
   [Qi, Honggang] Univ Chinese Acad Sci, Sch Comp & Control Engn, 3 Zhong GuanCun NanYiTiao, Beijing 100049, Peoples R China.
   [Lyu, Siwei] SUNY Albany, Comp Sci Dept, 1400 Washington Ave, Albany, NY 12222 USA.
   [Lyu, Siwei] Tianjin Normal Univ, Sch Comp & Informat Engn, 393 Binshui XiDao, Tianjin, Peoples R China.
C3 General Electric; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; State University of New York (SUNY) System;
   State University of New York (SUNY) Albany; Tianjin Normal University
RP Wen, LY (corresponding author), GE Global Res Ctr, 1 Res Circle, Niskayuna, NY 12309 USA.
EM wly880815@gmail.com; hgqi@ucas.ac.cn; slyu@albany.edu
RI Lyu, Siwei/A-1140-2007
OI Lyu, Siwei/0000-0002-0992-685X
FU United States Air Force Research Laboratory (AFRL); Defense Advanced
   Research Projects Agency (DARPA) [FA8750-16-C-0166]; NSFC [61472388,
   61771341]
FX This material is based upon work supported by the United States Air
   Force Research Laboratory (AFRL) and the Defense Advanced Research
   Projects Agency (DARPA) under Contract No. FA8750-16-C-0166. The views,
   opinions and/or findings expressed are those of the author and should
   not be interpreted as representing the official views or policies of the
   Department of Defense or the U.S. Government. Honggang Qi is supported
   in part by NSFC projects 61472388 and 61771341.
CR [Anonymous], 2016, CVX MATLAB SOFTWARE
   Barni M., 2012, P ACM WORKSH MULT SE
   Bertsekas DP., 1996, Constrained Optimization and Lagrange Multiplier Methods, V1st edn
   Boyd S., 2005, Convex Optimization
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Cao G., 2010, IEEE International Conference on Image Processing, P2097, DOI DOI 10.1109/ICIP.2010.5652701
   Cao G., 2010, P ACM WORKSH MULT SE
   Cao G, 2014, IEEE T INF FOREN SEC, V9, P515, DOI 10.1109/TIFS.2014.2300937
   Dang-Nguyen Duc-Tien, 2015, P ACM MULT SYST C
   Farid H, 2001, IEEE T IMAGE PROCESS, V10, P1428, DOI 10.1109/83.951529
   Farid H, 2016, PHOTO FORENSICS, P1
   Ferrara P., 2013, P IEEE WORKSH MULT S
   Gonzalez M., 2002, Digital image processing, V2nd
   Grimaldi R., 1998, Discrete and Combinatorial Mathematics: An Applied Introduction
   Levina E, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P251, DOI 10.1109/ICCV.2001.937632
   Lin Xufeng, 2014, P SPIE, V9028
   Lyu SW, 2014, INT J COMPUT VISION, V110, P202, DOI 10.1007/s11263-013-0688-y
   Morales Jean, 2010, ADV NEURAL INFORM PR
   Stamm MC, 2010, INT CONF ACOUST SPEE, P1698, DOI 10.1109/ICASSP.2010.5495488
   Stamm MC, 2010, IEEE T INF FOREN SEC, V5, P492, DOI 10.1109/TIFS.2010.2053202
   Villani C., 2008, GRUNDLEHREN MATH WIS
   Zhang X., 2014, P IEEE C IM PROC ICI
   Zhang Xing, 2014, P IEEE C IM PROC ICI
   Zoran Daniel, 2009, P IEEE INT C COMP VI
NR 24
TC 19
Z9 19
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
AR 49
DI 10.1145/3183518
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GG9BS
UT WOS:000432996000006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Miller, K
   Al-Tamimi, AK
   Wolisz, A
AF Miller, Konstantin
   Al-Tamimi, Abdel-Karim
   Wolisz, Adam
TI QoE-Based Low-Delay Live Streaming Using Throughput Predictions
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adaptive streaming; MPEG-DASH; TCP throughput prediction
ID CONTROL-THEORETIC APPROACH; VIDEO; ADAPTATION; QUALITY; DASH
AB Recently, Hypertext Transfer Protocol (HTTP)-based adaptive streaming has become the de facto standard video streaming over the Internet. It allows clients to dynamically adapt media characteristics to the varying network conditions to ensure a high quality of experience (QoE)-that is, minimize playback interruptions while maximizing video quality at a reasonable level of quality changes. In the case of live streaming, this task becomes particularly challenging due to the latency constraints. The challenge further increases if a client uses a wireless access network, where the throughput is subject to considerable fluctuations. Consequently live streams often exhibit latencies of up to 20 to 30 seconds. In the present work, we introduce an adaptation algorithm for HTTP-based live streaming called LOLYPOP (short for low-latency prediction based adaptation), which is designed to operate with a transport latency of a few seconds. To reach this goal, LOINPOP leverages Transmission Control Protocol throughput predictions on multiple time scales, from 1 to 10 seconds, along with estimations of the relative prediction error distributions. In addition to satisfying the latency constraint, the algorithm heuristically maximizes the QoE by maximizing the average video quality as a function of the number of skipped segments and quality transitions. To select an efficient prediction method, we studied the performance of several time series prediction methods in IEEE 802.11 wireless access networks. We evaluated LOLYPOP under a large set of experimental conditions, limiting the transport latency to 3 seconds, against a state-of-the-art adaptation algorithm called FESTIVE. We observed that the average selected video representation index is by up to a factor of 3 higher than with the baseline approach. We also observed that LOLYPOP is able to reach points from a broader region in the QoE space, and thus it is better adjustable to the user profile or service provider requirements.
C1 [Miller, Konstantin; Wolisz, Adam] Fachg TKN, Sekr FT5, Einsteinufer 25, D-10587 Berlin, Germany.
   [Al-Tamimi, Abdel-Karim] Yarmouk Univ, Dept Comp Engn, Irbid 21163, Jordan.
C3 Yarmouk University
RP Miller, K (corresponding author), Fachg TKN, Sekr FT5, Einsteinufer 25, D-10587 Berlin, Germany.
EM konstantin.miller@tu-berlin.de; altamimi@yu.edu.jo;
   adam.wolisz@tu-berlin.de
OI Wolisz, Adam/0000-0002-2969-1234; Al-Tamimi,
   Abdel-Karim/0000-0003-2459-0298
FU ZIM programme of the German Federal Ministry for Economic Affairs and
   Energy (BMWi) in the context of the research project ANOPAS
FX This work was supported by the ZIM programme of the German Federal
   Ministry for Economic Affairs and Energy (BMWi) in the context of the
   research project ANOPAS.
CR Abdallah AS, 2015, IEEE ICC, P6797, DOI 10.1109/ICC.2015.7249409
   [Anonymous], 2014, P 5 ACM INT C MULT S
   [Anonymous], 2014, Cisco Visual Networking Index. Global Mobile Data Traffic Forecast Update
   [Anonymous], 2008, ITU-T Recommendation G.984.1 (2008)
   [Anonymous], 2014, P 13 ACM WORKSH HOT
   [Anonymous], 2014, CHICKEN
   Aurrecoechea C, 1998, MULTIMEDIA SYST, V6, P138, DOI 10.1007/s005300050083
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Balachandran A, 2012, PROCEEDINGS OF THE 11TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS-XI), P97
   Bethanabhotla D, 2015, IEEE T COMMUN, V63, P268, DOI 10.1109/TCOMM.2014.2378774
   Bokani A, 2013, 2013 20TH INTERNATIONAL PACKET VIDEO WORKSHOP (PV)
   Carapinha Jorge, 2010, P ITU T KAL C
   Carmel Sharon, 2002, Network Media Streaming. Patent No. US, Patent No. 6389473
   Chatfield C, 2003, ANAL TIME SERIES INT
   Chen Zhigang, 1995, P INT WORLD WID WEB
   Claeys M, 2014, IEEE COMMUN LETT, V18, P716, DOI 10.1109/LCOMM.2014.020414.132649
   ComScore, 2014, CISC VIS NETW IND GL
   Conviva, 2015, CISC VIS NETW IND GL
   Conviva, 2014, CISC VIS NETW IND GL
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   De Pessemier T, 2013, IEEE T BROADCAST, V59, P47, DOI 10.1109/TBC.2012.2220231
   El Essaili A., 2013, 2013 IEEE International Conference on Communications (ICC), P2480, DOI 10.1109/ICC.2013.6654905
   He Q, 2007, COMPUT NETW, V51, P3959, DOI 10.1016/j.comnet.2007.04.013
   Hossfeld T., 2012, P WORKSH QUAL MULT E
   Huynh-Thu Q, 2008, IEEE T BROADCAST, V54, P641, DOI 10.1109/TBC.2008.2001246
   ITU-T, 2008, VOC PERF QUAL SERV A
   ITU-T, 2014, REQ LOW LAT INT MULT
   Jarnikov D, 2011, SIGNAL PROCESS-IMAGE, V26, P378, DOI 10.1016/j.image.2011.03.003
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Johnson N., 1994, Continuous Univariate Distributions
   Kanakia H, 1995, IEEE ACM T NETWORK, V3, P671, DOI 10.1109/90.477713
   Le Boudec Jean-Yves, 2015, PERFORMANCE EVALUATI
   Le H, 2015, 2015 IEEE 26TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC), P1277, DOI 10.1109/PIMRC.2015.7343495
   Le HT, 2015, IEEE INT CONF COMM, P1771, DOI 10.1109/ICCW.2015.7247437
   Le HT, 2013, PROC INT CONF ADV, P33, DOI 10.1109/ATC.2013.6698072
   Lewcio B, 2011, INT WORK QUAL MULTIM, P43, DOI 10.1109/QoMEX.2011.6065710
   Li BC, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2505805
   Li Z., 2014, Proceedings of the 5th ACM Multimedia Systems Conference, MMSys '14, P248
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liu X, 2012, ACM SIGCOMM COMP COM, V42, P359, DOI 10.1145/2377677.2377752
   Liu Y, 2014, IEEE GLOB COMM CONF, P1164, DOI 10.1109/GLOCOM.2014.7036966
   Liu YT, 2013, IEEE INT CONF COMM, P682, DOI 10.1109/ICCW.2013.6649320
   Lohmar T., 2011, IEEE INT S WORLD WIR, P1, DOI DOI 10.1109/WOWMOM.2011.5986186
   Miller K., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P173, DOI 10.1109/PV.2012.6229732
   Miller K, 2015, IEEE T MULTIMEDIA, V17, P1309, DOI 10.1109/TMM.2015.2441002
   Miller K, 2013, 2013 20TH INTERNATIONAL PACKET VIDEO WORKSHOP (PV)
   Mirza M, 2010, IEEE ACM T NETWORK, V18, P1026, DOI 10.1109/TNET.2009.2037812
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   MPEG, 2012, 230091 MPEG DASH ISO
   Padhye J, 2000, IEEE ACM T NETWORK, V8, P133, DOI 10.1109/90.842137
   Reiter U, 2014, T-LAB SER TELECOMMUN, P55, DOI 10.1007/978-3-319-02681-7_4
   Riiser Haakon., 2012, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), V8, P1
   Sangtae Ha, 2008, Operating Systems Review, V42, P64, DOI 10.1145/1400097.1400105
   Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Singh KD, 2012, CONSUM COMM NETWORK, P127, DOI 10.1109/CCNC.2012.6181070
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Swaminathan Viswanathan, 2013, ACM T MULTIM COMPUT, V9, P1
   Sweeting Paul, 2014, RES REPORT
   Tian G., 2012, P 8 INT C EM NETW EX, P109
   Thang TC, 2014, IEEE J SEL AREA COMM, V32, P693, DOI 10.1109/JSAC.2014.140403
   Wei S, 2014, DES AUT CON, DOI 10.1145/2593069.2593204
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zhou C, 2014, IEEE T CIRC SYST VID, V24, P681, DOI 10.1109/TCSVT.2013.2290580
   Zhou C, 2012, 2012 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Zhu XQ, 2013, IEEE INT WORKSH MULT, P230, DOI 10.1109/MMSP.2013.6659293
   Zou XK, 2015, 16TH INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS (HOTMOBILE' 15), P57, DOI 10.1145/2699343.2699359
NR 70
TC 55
Z9 58
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2017
VL 13
IS 1
AR 4
DI 10.1145/2990505
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM7WJ
UT WOS:000395522700004
OA Green Published
DA 2024-07-18
ER

PT J
AU Li, YH
   Song, B
   Cao, R
   Zhang, Y
   Qin, H
AF Li, Yinghua
   Song, Bin
   Cao, Rong
   Zhang, Yue
   Qin, Hao
TI Image Encryption Based on Compressive Sensing and Scrambled Index for
   Secure Multimedia Transmission
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Compressive sensing; image encryption; multimedia security; random
   permutation; split Bregman algorithm
ID SIGNAL RECOVERY
AB With the rapid growth of multimedia message exchange and digital communication, multimedia big data has become a research hotspot in various fields. The storage and transmission of multimedia big data have high requirements for security. Images, covering the highest proportion of multimedia data, should be processed and transmitted with high security. Compressive sensing (CS) has a beneficial property for the encryption that the image can be recovered with fewer samples than conventional approaches use. In recent years, CS has been studied not only to reduce the resource requirements for signal acquisition but also to ensure the security of data. It is still an open challenge to improve security and enhance the quality of the decrypted image simultaneously using the key with small size. In this article, a CS-based encryption method is presented that associates the quantization with random measurement permutation. An enormous number of experiments have been conducted on both standard test images and face images chosen from the big database LFW. Experimental results show that our proposal has dramatic improvements on ensuring the security, enhancing the quality of the decrypted image, and raising the efficiency. Additionally, this proposal remarkably reduces storage and transmission resources. Accordingly, this encryption scheme can be applied to ensure the security of multimedia transmission.
C1 [Li, Yinghua; Song, Bin; Cao, Rong; Zhang, Yue; Qin, Hao] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
C3 Xidian University
RP Li, YH (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM liyh@stu.xidian.edu.cn; bsong@mail.xidian.edu.cn;
   rcao@stu.xidian.edu.cn; y.zhang@stu.xidian.edu.cn;
   hqin@mail.xidian.edu.cn
FU National Natural Science Foundation of China [61271173, 61372068];
   Research Fund for the Doctoral Program of Higher Education of China
   [20130203110005]; Fundamental Research Funds for the Central
   Universities [K5051301033]; 111 Project [B08038]; ISN State Key
   Laboratory
FX This work has been supported by the National Natural Science Foundation
   of China (Nos. 61271173 and 61372068), the Research Fund for the
   Doctoral Program of Higher Education of China (No. 20130203110005), the
   Fundamental Research Funds for the Central Universities (No.
   K5051301033), and the 111 Project (No. B08038), and also supported by
   the ISN State Key Laboratory.
CR Bianchi Tiziano, 2014, 2014 IEEE INT C AC S, P3992
   Cambareri V, 2015, IEEE T INF FOREN SEC, V10, P2182, DOI 10.1109/TIFS.2015.2450676
   Cambareri V, 2015, IEEE T SIGNAL PROCES, V63, P2183, DOI 10.1109/TSP.2015.2407315
   Cambareri V, 2013, IEEE INT SYMP CIRC S, P1356, DOI 10.1109/ISCAS.2013.6572106
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Candès EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Candes Emmanuel Jean, 2006, P INT C MATH, V3, P1433
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Dufaux F, 2008, IEEE T CIRC SYST VID, V18, P1168, DOI 10.1109/TCSVT.2008.928225
   Gan Lu, 2007, 2007 15 INT C DIG SI, P403
   Gao TG, 2008, CHAOS SOLITON FRACT, V38, P213, DOI 10.1016/j.chaos.2006.11.009
   Goldstein Tom, 2008, 08 UCLA CAM, V08
   Orsdemir Adem, 2008, IEEE MIL COMM C 2008, P1
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Qiu Robert, 2014, COGNITIVE NETWORKED
   Rachlin Y, 2008, ANN ALLERTON CONF, P813, DOI 10.1109/ALLERTON.2008.4797641
   Shah Reena, 2014, INNOVATION RES ELECT, V2, P1477
   Winkler T, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2545883
   Yin WT, 2013, J SCI COMPUT, V54, P684, DOI 10.1007/s10915-012-9616-5
   Yin WT, 2008, SIAM J IMAGING SCI, V1, P143, DOI 10.1137/070703983
   Zhang Leo Yu, 2014, ARXIV14116079
   Zhang Leo Yu, 2014, ABS1406 CORR
   Zhou NR, 2014, OPT LASER TECHNOL, V62, P152, DOI 10.1016/j.optlastec.2014.02.015
NR 24
TC 18
Z9 18
U1 0
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2016
VL 12
IS 4
SU S
AR 62
DI 10.1145/2903717
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8DS
UT WOS:000392736100006
DA 2024-07-18
ER

PT J
AU Wang, SG
   Zhou, A
   Lei, W
   Yu, ZW
   Hsu, CH
   Yang, FC
AF Wang, Shangguang
   Zhou, Ao
   Lei, Wei
   Yu, Zhiwen
   Hsu, Ching-Hsien
   Yang, Fangchun
TI Enhanced User Context-Aware Reputation Measurement of Multimedia Service
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimedia service; user context; reputation; feedback rating; user
   similarity
ID WEB SERVICE; TRUST; SYSTEM; QOE
AB Reputation plays an important role for users in choosing or paying for multimedia applications or services. Some efficient multimedia reputation-measurement approaches have been proposed to achieve accurate reputation measurement based on feedback ratings that users give to a multimedia service after invoking. However, the implementation of these approaches suffers from the problems of wide abuse and low utilization of user context. In this article, we study the relationship between user context and feedback ratings according to which one user often gives different feedback ratings to the same multimedia service in different user contexts. We further propose an enhanced user context-aware reputation-measurement approach for multimedia services that is accurate in two senses: (1) Each multimedia service has three reputation values with three different user context levels when its feedback ratings are sufficient and (2) the reputation of a multimedia service with different user context levels is found using user context sensitivity and user similarity when its feedback ratings are limited or not available. Experimental results based on a real-world dataset show that our approach outperforms other approaches in terms of accuracy.
C1 [Wang, Shangguang; Zhou, Ao; Yang, Fangchun] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Lei, Wei] Chinese Acad Sci, Inst Informat Engn, Beijing 10093, Peoples R China.
   [Yu, Zhiwen] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Hsu, Ching-Hsien] Foshan Univ, Sch Math & Big Data, Foshan Shi, Guangdong, Peoples R China.
   [Hsu, Ching-Hsien] Chung Hua Univ, Dept Comp Sci & Informat Engn, Hsinchu 30012, Taiwan.
C3 Beijing University of Posts & Telecommunications; Chinese Academy of
   Sciences; Northwestern Polytechnical University; Foshan University;
   Chung Hua University
RP Hsu, CH (corresponding author), Foshan Univ, Sch Math & Big Data, Foshan Shi, Guangdong, Peoples R China.; Hsu, CH (corresponding author), Chung Hua Univ, Dept Comp Sci & Informat Engn, Hsinchu 30012, Taiwan.
EM sgwang@bupt.edu.cn; aozhou@bupt.edu.cn; 13426013647@126.com;
   zhiwenyu@nwpu.edu.cn; chh@chu.edu.tw; fcyang@bupt.edu.cn
RI Zhou, Xiangfeng/KDO-8724-2024; Hsu, Ching-Hsien/AAE-6917-2020; Zhou,
   Xiaofang/C-6169-2013
OI Zhou, Xiaofang/0000-0001-6343-1455; Wang,
   Shangguang/0000-0001-7245-1298; Yu, Zhiwen/0000-0002-9905-3238
FU NSFC [61472047]; Major Project supported by National Social Science
   Foundation of China [15ZDA063]; Open Research Fund Program of Beijing
   Key Laboratory on Integration and Analysis of Large-scale Stream Data
FX This work was supported by the NSFC (No. 61472047), the Major Project
   supported by National Social Science Foundation of China (Grant No.
   15ZDA063), and the Open Research Fund Program of Beijing Key Laboratory
   on Integration and Analysis of Large-scale Stream Data.
CR Alnemr R, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON SERVICES COMPUTING, P451, DOI 10.1109/SCC.2009.55
   Atrey Pradeep K., 2008, P MULT COMP NETW C M, P1
   Bagheri E, 2009, KNOWL-BASED SYST, V22, P410, DOI 10.1016/j.knosys.2009.05.007
   Brozovsky Lukas, 2007, P C ZNAL, P1
   Caverlee James, 2008, P 8 ACM IEEE CS JOIN, P104
   Chuang SN, 2008, IEEE T SOFTWARE ENG, V34, P738, DOI 10.1109/TSE.2008.44
   Conner William, 2009, P 18 INT C WORLD WID, P891
   Dong MX, 2015, IEEE WIREL COMMUN, V22, P50, DOI 10.1109/MWC.2015.7224727
   Dong MX, 2014, IEICE T INF SYST, VE97D, P2606, DOI 10.1587/transinf.2013THP0011
   Ghaffarinejad A, 2013, FUTURE GENER COMP SY, V29, P863, DOI 10.1016/j.future.2012.03.021
   Kamvar Sepandar D., 2003, P 12 INT C WORLD WID, P640
   Kovachev D, 2014, MULTIMED TOOLS APPL, V70, P977, DOI 10.1007/s11042-012-1100-6
   Lages Alexandre G., 2007, P 9 ANN ACM INT WORK, P153
   Lee J, 2012, INT C COMMERCE BUS, P57, DOI 10.1109/CEC.2012.18
   Lee JooYoung, 2013, P IEEE ACM INT C ADV, P666
   Li W, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND ENGINEERING (CSE), P1489, DOI 10.1109/CSE.2014.277
   Liu L, 2010, IEEE INTERNET COMPUT, V14, P10, DOI 10.1109/MIC.2010.124
   Lua EK, 2011, INT C PAR DISTRIB SY, P811, DOI 10.1109/ICPADS.2011.123
   Maarouf I, 2009, IET COMMUN, V3, P846, DOI 10.1049/iet-com.2008.0324
   Malik Z, 2009, VLDB J, V18, P885, DOI 10.1007/s00778-009-0138-1
   Malik Z, 2009, IEEE INTERNET COMPUT, V13, P40, DOI 10.1109/MIC.2009.17
   Maximilien Michael E., 2001, SIGECOM EXCHANGES, V3, P24
   Maximilien Michael E., 2002, SIGMOD RECORD, V31, P36
   Modha DS, 2003, MACH LEARN, V52, P217, DOI 10.1023/A:1024016609528
   Van Broekhoven E, 2009, IEEE T FUZZY SYST, V17, P1157, DOI 10.1109/TFUZZ.2009.2023328
   Wang P, 2008, INT CONF E BUS ENG, P627, DOI 10.1109/ICEBE.2008.22
   Wang S, 2011, IET SOFTW, V5, P466, DOI 10.1049/iet-sen.2010.0077
   Wang SG, 2016, J COMPUT SYST SCI, V82, P130, DOI 10.1016/j.jcss.2015.06.009
   Wang SG, 2015, IEEE T SERV COMPUT, V8, P755, DOI 10.1109/TSC.2014.2320262
   Wang Y, 2008, IEEE INTERNET COMPUT, V12, P55, DOI 10.1109/MIC.2008.84
   Wen ST, 2012, SERV ORIENTED COMPUT, V6, P231, DOI 10.1007/s11761-012-0105-3
   Xiong L, 2004, IEEE T KNOWL DATA EN, V16, P843, DOI 10.1109/TKDE.2004.1318566
   Xu ZQ, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON WEB SERVICES, PROCEEDINGS, P249
   Yan SR, 2015, INFORM SCIENCES, V318, P51, DOI 10.1016/j.ins.2014.09.036
   Zacharia G, 2000, DECIS SUPPORT SYST, V29, P371, DOI 10.1016/S0167-9236(00)00084-1
   Zhou RF, 2007, IEEE T PARALL DISTR, V18, P460, DOI 10.1109/TPDS.2007.1015
NR 36
TC 2
Z9 2
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2016
VL 12
IS 4
SU S
AR 59
DI 10.1145/2978569
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8DS
UT WOS:000392736100003
DA 2024-07-18
ER

PT J
AU Ravi, H
   Subramanyam, AV
   Emmanuel, S
AF Ravi, Hareesh
   Subramanyam, A. V.
   Emmanuel, Sabu
TI Forensic Analysis of Linear and Nonlinear Image Filtering Using
   Quantization Noise
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Filtering classification; quantization noise; Markov process; counter
   antiforensics; HMRF; GMM
ID DOUBLE JPEG COMPRESSION; ANTI-FORENSICS; STEGANALYSIS; TRACES; MODELS
AB The availability of intelligent image editing techniques and antiforensic algorithms, make it convenient to manipulate an image and to hide the artifacts that it might have produced in the process. Real world forgeries are generally followed by the application of enhancement techniques such as filtering and/or conversion of the image format to suppress the forgery artifacts. Though several techniques evolved in the direction of detecting some of these manipulations, additional operations like recompression, nonlinear filtering, and other antiforensic methods during forgery are not deeply investigated. Toward this, we propose a robust method to detect whether a given image has undergone filtering (linear or nonlinear) based enhancement, possibly followed by format conversion after forgery. In the proposed method, JPEG quantization noise is obtained using natural image prior and quantization noise models. Transition probability features extracted from the quantization noise are used for machine learning based detection and classification. We test the effectiveness of the algorithm in classifying the class of the filter applied and the efficacy in detecting filtering in low resolution images. Experiments are performed to compare the performance of the proposed technique with state-of-the-art forensic filtering detection algorithms. It is found that the proposed technique is superior in most of the cases. Also, experiments against popular antiforensic algorithms show the counter antiforensic robustness of the proposed technique.
C1 [Ravi, Hareesh; Subramanyam, A. V.] Indraprastha Inst Informat Technol, New Delhi, India.
   [Emmanuel, Sabu] Kuwait Univ, Kuwait, Kuwait.
C3 Indraprastha Institute of Information Technology Delhi; Kuwait
   University
RP Ravi, H (corresponding author), Indraprastha Inst Informat Technol, New Delhi, India.
EM hareeshr@iiitd.ac.in; subramanyam@iiitd.ac.in; sabu@cs.ku.edu.kw
OI venkata, subramanyam/0000-0002-8873-4644; Ravi,
   Hareesh/0000-0002-3237-1899
FU Department of Electronics and Information Technology (DeitY), Government
   of India [12(4)/2014-ESD]
FX This work is supported by Department of Electronics and Information
   Technology (DeitY), Government of India, under the project "'Design and
   Development of Digital Multimedia Forgery Detection System with Project
   Number 12(4)/2014-ESD.
CR [Anonymous], 2010, J DIGITAL FORENSIC P
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], 2013, EVID BASED COMPL ALT, DOI DOI 10.1155/2013/613950
   [Anonymous], P SPIE EL IM MED FOR
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P SPIE EL IM SEC STE
   [Anonymous], 2011, P 13 INF HID C PRAG
   [Anonymous], 1995, Markov random field modeling in computer vision
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Bhatnagar G, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2542205.2542207
   Cao H, 2012, IEEE T INF FOREN SEC, V7, P992, DOI 10.1109/TIFS.2012.2185696
   Chen C., 2008, P IEEE INT C PATTERN, P1
   Chen CH, 2008, IEEE INT SYMP CIRC S, P3029, DOI 10.1109/ISCAS.2008.4542096
   Conotter V, 2013, IEEE IMAGE PROC, P4517, DOI 10.1109/ICIP.2013.6738930
   Cozzolino D, 2014, IEEE IMAGE PROC, P5297, DOI 10.1109/ICIP.2014.7026072
   Fan W., 2013, P IH MMSEC 2013 2013, P117, DOI [10.1145/2482513.2482536, DOI 10.1145/2482513.2482536]
   Fan W, 2015, IEEE T INF FOREN SEC, V10, P1076, DOI 10.1109/TIFS.2015.2398362
   Farid H, 2009, IEEE T INF FOREN SEC, V4, P154, DOI 10.1109/TIFS.2008.2012215
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Huang FJ, 2010, IEEE T INF FOREN SEC, V5, P848, DOI 10.1109/TIFS.2010.2072921
   Hui Zeng, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P2704, DOI 10.1109/ICASSP.2014.6854091
   Kang XG, 2013, IEEE T INF FOREN SEC, V8, P1456, DOI 10.1109/TIFS.2013.2273394
   Kirchner M, 2008, IEEE T INF FOREN SEC, V3, P582, DOI 10.1109/TIFS.2008.2008214
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Lathey A, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2656205
   Li S. Z., 2009, Markov random field modeling in image analysis
   Liu QZ, 2011, LECT NOTES COMPUT SC, V6676, P466, DOI 10.1007/978-3-642-21090-7_55
   Liu QZ, 2010, INFORM SCIENCES, V180, P1643, DOI 10.1016/j.ins.2010.01.001
   Pevny T, 2010, IEEE T INF FOREN SEC, V5, P215, DOI 10.1109/TIFS.2010.2045842
   Puglisi G, 2013, IEEE IMAGE PROC, P4502, DOI 10.1109/ICIP.2013.6738927
   Qiu X., 2014, P 2 ACM WORKSH INF H, P165, DOI DOI 10.1145/2600918.2600941
   Ravi H, 2014, IEEE IMAGE PROC, P5352, DOI 10.1109/ICIP.2014.7026083
   Robertson MA, 2005, IEEE T CIRC SYST VID, V15, P27, DOI 10.1109/TCSVT.2004.839995
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   ShiYue Lai, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P285, DOI 10.1007/978-3-642-24178-9_20
   Stamm MC, 2013, IEEE ACCESS, V1, P167, DOI 10.1109/ACCESS.2013.2260814
   Valenzise G., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P1949, DOI 10.1109/ICIP.2011.6115854
   Verdoliva L, 2014, IEEE INT WORKS INFOR, P149, DOI 10.1109/WIFS.2014.7084319
   Ware R., 2003, U CANTERBURY ENGLAND, V15, P2003
   Wu ZH, 2013, INT CONF ACOUST SPEE, P3043, DOI 10.1109/ICASSP.2013.6638217
   Zhang YJ, 2014, IEEE SIGNAL PROC LET, V21, P275, DOI 10.1109/LSP.2013.2295858
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 42
TC 7
Z9 7
U1 0
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2016
VL 12
IS 3
AR 39
DI 10.1145/2857069
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YO
UT WOS:000379425400005
DA 2024-07-18
ER

PT J
AU Canazza, S
   Fantozzi, C
   Pretto, N
AF Canazza, Sergio
   Fantozzi, Carlo
   Pretto, Niccolo
TI Accessing Tape Music Documents on Mobile Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Documentation; Design; audio archives; audio document preservation and
   access; mobile computing
ID AUDIO DOCUMENTS; PRESERVATION
AB The aim of this article is to present and discuss an innovative methodology aimed at accessing digitized copies of historical tape music audio documents; the methodology leverages on the multimedia and multisensory capabilities of mobile devices to provide an unprecedented level of fruition. In addition to the methodology, and stemming from it, we present an actual software application for Android tablet devices. This novel piece of software was designed and developed in a multidisciplinary team involving engineers as well asmusicians, composers, and archivists. The strongest element in our work is the fact that it follows a rigorous process and it is based on the principles of philological awareness; thus, it also takes into consideration the critical points in the musicologist's domain such as (i) the definition of preservation (i.e., master) copy, (ii) the importance of secondary information, (iii) the history of production and transmission of audio documents.
C1 [Canazza, Sergio; Fantozzi, Carlo; Pretto, Niccolo] Univ Padua, Dept Informat Engn, I-35131 Padua, PD, Italy.
C3 University of Padua
RP Canazza, S (corresponding author), Univ Padua, Dept Informat Engn, I-35131 Padua, PD, Italy.
EM fantozzi@dei.unipd.it
RI Pretto, Niccolò/AAZ-9149-2021; Canazza, Sergio/Y-8733-2019; Fantozzi,
   Carlo/N-3574-2015
OI Pretto, Niccolò/0000-0002-3742-7150; Fantozzi,
   Carlo/0000-0003-3210-4632; De Poli, Giovanni/0000-0001-8487-7093
FU Veneto Region (Italy) under program FSE [2105/1/15/1148/2013]
FX This work was supported by the Veneto Region (Italy) under program FSE,
   grant 2105/1/15/1148/2013.
CR Aditya S. K., 2014, ANDROID SQLITE ESSEN
   Adorno T., 1962, DARMSADTER BEITR NEU
   Adorno Theodor., 1947, PHILOS NEW MUSIC
   Algazi VR, 2011, IEEE SIGNAL PROC MAG, V28, P33, DOI 10.1109/MSP.2010.938756
   Anderloni F., 2014, THESIS U PADOVA
   Anderson G., 1990, GRIFFITHIANA, V38/39, P154
   [Anonymous], 2005, 03 IASATC
   [Anonymous], 1997, AUDIO ENG SOC CONVEN
   [Anonymous], 2004, 04 IASATC
   [Anonymous], 2014, 05 IASATC
   Bari A, 2001, J NEW MUSIC RES, V30, P351, DOI 10.1076/jnmr.30.4.351.7490
   Berio Luciano., 2006, Remembering the Future
   Bianconi L., 2014, THESIS U PADOVA
   Bressan F, 2013, ADV MULTIMED, V2013, DOI 10.1155/2013/276354
   Bressan F, 2013, J NEW MUSIC RES, V42, P364, DOI 10.1080/09298215.2013.840317
   Bressan F, 2013, J ELECTR COMPUT ENG, V2013, DOI 10.1155/2013/489515
   Camras Marvin., 1988, MAGNETIC RECORDING H
   Canazza S, 2001, J NEW MUSIC RES, V30, P93, DOI 10.1076/jnmr.30.1.93.7120
   Canazza S., 2011, PROC SOUND MUSIC COM, P304
   Canazza S, 2012, INT J DIGIT LIBRARIE, V12, P121, DOI 10.1007/s00799-012-0088-x
   Canazza S, 2010, SIGNAL PROCESS, V90, P977, DOI 10.1016/j.sigpro.2009.12.001
   Ceipidor U B., 2013, 2013 5 INT WORKSHOP, DOI [DOI 10.1109/NFC.2013.6482445, 10.1109/NFC.2013.6482445]
   Colanardi D., 2014, THESIS U PADOVA
   Damkliang K, 2014, ADV INTELL SYST, V250, P131, DOI 10.1007/978-81-322-1695-7_16
   Di Staso U, 2014, LECT NOTES COMPUT SC, V8518, P223, DOI 10.1007/978-3-319-07626-3_21
   Dosso D., 2014, THESIS U PADOVA
   Eimert H., 1958, DIE REIHE
   Figueiredo MJG, 2014, IEEE INT CONF INF VI, P368, DOI 10.1109/IV.2014.17
   Google, 2014, MULT LAYOUTS
   Google, 2013, ANDR COMP DEF DOC
   Hua H.-M., 2014, GERONTECHNOLOGY, V13, P207
   Kell T., 2013, P SOUND MUSIC COMPUT, P473
   Killingsworth S., 2014, WHAT MAKES HANX WRIT
   Martins G., 1994, SUMMER LOVE MAKING S
   Novati MariaMaddalena., 2012, The Studio di Fonologia: A Musical Journey, 1954-1983; Update 2008-2012
   OpenSignal, 2014, ANDR FRAGM VIS
   Orcalli A, 2001, J NEW MUSIC RES, V30, P307, DOI 10.1076/jnmr.30.4.307.7496
   Orio N, 2009, INT J DIGIT LIBRARIE, V10, P201, DOI 10.1007/s00799-010-0060-6
   Pousseur Henri, 2004, ECRITS THEORIQUES 19
   Schüller D, 2001, J AUDIO ENG SOC, V49, P618
   SCHULLER D, 1991, J AUDIO ENG SOC, V39, P1014
   Storm W., 1980, PHONOGRAPHIC B, V27, P5
   Thomas J, 2014, 2014 IEEE VIRTUAL REALITY (VR), P115, DOI 10.1109/VR.2014.6802078
   Toffler A., 1980, 3 WAVE
   Wein L, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P635, DOI 10.1145/2556288.2557270
NR 45
TC 12
Z9 12
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 20
DI 10.1145/2808200
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100010
DA 2024-07-18
ER

PT J
AU Chen, L
   Zhou, YP
   Chiu, DM
AF Chen, Liang
   Zhou, Yipeng
   Chiu, Dah Ming
TI Analysis and Detection of Fake Views in Online Video Services
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurement; Experimentation; Reliability; Fake view; online video
   service
ID ANOMALY DETECTION
AB Online video-on-demand(VoD) services invariably maintain a view count for each video they serve, and it has become an important currency for various stakeholders, from viewers, to content owners, advertizers, and the online service providers themselves. There is often significant financial incentive to use a robot (or a botnet) to artificially create fake views. How can we detect fake views? Can we detect them (and stop them) efficiently? What is the extent of fake views with current VoD service providers? These are the questions we study in this article. We develop some algorithms and show that they are quite effective for this problem.
C1 [Chen, Liang] Shenzhen Univ, Coll Informat Engn, Shenzhen 518060, Peoples R China.
   [Zhou, Yipeng] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Chiu, Dah Ming] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Shenzhen University; Shenzhen University; Chinese University of Hong
   Kong
RP Chen, L (corresponding author), Shenzhen Univ, Coll Informat Engn, Nanhai Ave 3688, Shenzhen 518060, Peoples R China.
EM lchen@szu.edu.en; ypzhou@szu.edu.cn
RI Chiu, Dah Ming/F-1885-2011
OI Zhou, Yipeng/0000-0003-1533-0865
FU open fund of Shenzhen Key Lab of Advanced Communications and Information
   Processing; Natural Science Foundation of SZU [201437]; Natural Science
   Foundation of China [61402297]; Hong Kong RGC support via GRF grant
   [14201814]
FX This work is supported by the open fund of Shenzhen Key Lab of Advanced
   Communications and Information Processing. The research was partially
   supported by the Natural Science Foundation of SZU (201437), and the
   Natural Science Foundation of China (61402297). We acknowledge the
   support of Hong Kong RGC support via GRF grant 14201814.
CR [Anonymous], 2004, P 4 ACM SIGCOMM C IN
   [Anonymous], 2005, P 5 ACM SIGCOMM C IN
   [Anonymous], 2012, P 9 USENIX S NETW SY
   Barford P, 2002, IMW 2002: PROCEEDINGS OF THE SECOND INTERNET MEASUREMENT WORKSHOP, P71, DOI 10.1145/637201.637210
   Barford P, 2001, IMW 2001: PROCEEDINGS OF THE FIRST ACM SIGCOMM INTERNET MEASUREMENT WORKSHOP, P69
   Bolton RJ, 2002, STAT SCI, V17, P235
   Chen L, 2014, IEEE IJCNN, P1, DOI 10.1109/IJCNN.2014.6889506
   Ensafi R, 2008, I C COMP SYST APPLIC, P686, DOI 10.1109/AICCSA.2008.4493603
   Gu Y., 2005, P INTERNET MEASUREME, P345, DOI 10.1145/1330107.1330148
   HAFFNER P., 2005, MINENET 05, P197, DOI DOI 10.1145/1080173.1080183
   Lall A., 2006, Performance Evaluation Review, V34, P145, DOI 10.1145/1140103.1140295
   Mahmood Zahid, 2012, YOUTUBE SLASHES 2 BI
   Mahoney Matthew V, 2003, Proceedings of the 2003 ACM symposium on Applied computing, P346
   Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882
   Scholkopf B., 1999, Making large scale svm learning practical, P41
   Sommer R, 2010, P IEEE S SECUR PRIV, P305, DOI 10.1109/SP.2010.25
   Thottan M, 2003, IEEE T SIGNAL PROCES, V51, P2191, DOI 10.1109/TSP.2003.814797
   Vapnik V.N., 1998, STAT LEARNING THEORY
   Wang HI, 2002, IEEE INFOCOM SER, P1530, DOI 10.1109/INFCOM.2002.1019404
   Yeung DS, 2007, IEEE T SYST MAN CY A, V37, P157, DOI 10.1109/TSMCA.2006.889480
   YouTube, 2014, FROZEN VIEW COUNT
NR 21
TC 10
Z9 12
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2015
VL 11
IS 2
SU S
SI SI
AR 44
DI 10.1145/2700290
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC7RV
UT WOS:000350567000004
DA 2024-07-18
ER

PT J
AU Lin, PY
AF Lin, Pei-Yu
TI Double Verification Secret Sharing Mechanism Based on Adaptive Pixel
   Pair Matching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Verification; Camouflage; adaptive pixel pair matching; secret sharing
ID GRAY-LEVEL; IMAGES; STEGANOGRAPHY
AB Verifiability is essential for the secret sharing approach, which allows the involved participants to detect cheaters during the secret retrieval process. In this article, we propose a double verification secret sharing (DVSS) mechanism that can not only prevent fraudulent participants but also satisfy the requirements of secret payload, camouflage, image fidelity and lossless revealed secret. DVSS offers double verification process to enhance the cheater detectability; experimental results reveal that the designed scheme can share larger secret capacity and retain superior image quality than the related secret sharing methods.
C1 Yuan Ze Univ, Dept Informat Commun & Innovat, Ctr Big Data & Digital Convergence, Chungli 32003, Taiwan.
C3 Yuan Ze University
RP Lin, PY (corresponding author), Yuan Ze Univ, Dept Informat Commun & Innovat, Ctr Big Data & Digital Convergence, 135 Yuan Tung Rd, Chungli 32003, Taiwan.
EM pagelin3@gmail.com
OI Lin, Pei-Yu/0000-0001-8809-1063
CR [Anonymous], 1972, PLAYBOY MAGAZINE
   Blakley G. R., 1979, P NAT COMP C AM FED, P313, DOI [10.1109/MARK.1979.8817296, DOI 10.1109/AFIPS.1979.98, DOI 10.1109/MARK.1979.8817296]
   Chan CK, 2004, PATTERN RECOGN, V37, P469, DOI 10.1016/j.patcog.2003.08.007
   Chang CC, 2008, PATTERN RECOGN, V41, P3130, DOI 10.1016/j.patcog.2008.04.006
   Chao RM, 2009, EURASIP J INF SECUR, DOI 10.1155/2009/658047
   Chen WK, 2013, J SYST SOFTWARE, V86, P581, DOI 10.1016/j.jss.2012.09.040
   Computer vision group University of Granada, 2002, COMP VIS GROUP
   Eslami Z, 2011, J SYST SOFTWARE, V84, P803, DOI 10.1016/j.jss.2011.01.002
   Hong W, 2012, IEEE T INF FOREN SEC, V7, P176, DOI 10.1109/TIFS.2011.2155062
   Hu CM, 2007, IEEE T IMAGE PROCESS, V16, P36, DOI 10.1109/TIP.2006.884916
   Lin CC, 2004, J SYST SOFTWARE, V73, P405, DOI 10.1016/S0164-1212(03)00239-5
   Lin PY, 2013, IMAGE VISION COMPUT, V31, P311, DOI 10.1016/j.imavis.2013.02.002
   Lin PY, 2009, PATTERN RECOGN, V42, P886, DOI 10.1016/j.patcog.2008.09.014
   Naor M., 1995, ADV CRYPTOLOGY EUROC
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Thien CC, 2002, COMPUT GRAPH-UK, V26, P766
   Tompa M., 1988, Journal of Cryptology, V1, P133
   Ulutas M, 2013, J SYST SOFTWARE, V86, P485, DOI 10.1016/j.jss.2012.09.027
   Ulutas M, 2011, J SYST SOFTWARE, V84, P341, DOI 10.1016/j.jss.2010.11.928
   Wu XT, 2013, J VIS COMMUN IMAGE R, V24, P552, DOI 10.1016/j.jvcir.2013.03.002
   Wu YS, 2004, PATTERN RECOGN, V37, P1377, DOI 10.1016/j.patcog.2004.01.002
   Yang CN, 2007, J SYST SOFTWARE, V80, P1070, DOI 10.1016/j.jss.2006.11.022
   Zhang XP, 2006, IEEE COMMUN LETT, V10, P781, DOI 10.1109/LCOMM.2006.060863
NR 23
TC 3
Z9 3
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2015
VL 11
IS 3
AR 36
DI 10.1145/2700291
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CB8BF
UT WOS:000349852500004
DA 2024-07-18
ER

PT J
AU Yuan, ZH
   Chen, SY
   Ghinea, G
   Muntean, GM
AF Yuan, Zhenhui
   Chen, Shengyang
   Ghinea, Gheorghita
   Muntean, Gabriel-Miro
TI User Quality of Experience of Mulsemedia Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Experimentation; Human Factors; Mulsemedia; olfaction; haptic;
   air flow; cross-modality; perception; feeling
ID SYNCHRONIZATION; PERCEPTION; MULTIMEDIA
AB User Quality of Experience (QoE) is of fundamental importance in multimedia applications and has been extensively studied for decades. However, user QoE in the context of the emerging multiple-sensorial media (mulsemedia) services, which involve different media components than the traditional multimedia applications, have not been comprehensively studied. This article presents the results of subjective tests which have investigated user perception of mulsemedia content. In particular, the impact of intensity of certain mulsemedia components including haptic and airflow on user-perceived experience are studied. Results demonstrate that by making use of mulsemedia the overall user enjoyment levels increased by up to 77%.
C1 [Yuan, Zhenhui; Chen, Shengyang; Muntean, Gabriel-Miro] Dublin City Univ, Dublin 9, Ireland.
   [Ghinea, Gheorghita] Brunel Univ, Uxbridge UB8 3PH, Middx, England.
C3 Dublin City University; Brunel University
RP Yuan, ZH (corresponding author), Dublin City Univ, Dublin 9, Ireland.
EM zhenhui.yuan@gmail.com
RI Ghinea, Gheorghita/AAG-6770-2020; Muntean, Gabriel-Miro/U-6783-2019
OI Ghinea, Gheorghita/0000-0003-2578-5580; Muntean,
   Gabriel-Miro/0000-0002-9332-4770
FU Enhancing Performance Initiative as Dublin City University Ireland;
   Enterprise Ireland; Ericsson [IP/2011/0135]; China Scholarship Council
FX The authors would like to acknowledge support from the Enhancing
   Performance Initiative as Dublin City University Ireland, Enterprise
   Ireland Innovation Partnership with Ericsson (IP/2011/0135) and the
   China Scholarship Council.
CR [Anonymous], 1978, Psychometrictheory
   [Anonymous], 1992, PRESENCE
   Apostolopoulos JG, 2012, P IEEE, V100, P974, DOI 10.1109/JPROC.2011.2182069
   Blakowski G, 1996, IEEE J SEL AREA COMM, V14, P5, DOI 10.1109/49.481691
   Bodnar A., 2014, P ACM INT C MULT INT
   Carbon CC, 2005, APPL COGNITIVE PSYCH, V19, P587, DOI 10.1002/acp.1098
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Donaldson SI, 2002, J BUS PSYCHOL, V17, P245, DOI 10.1023/A:1019637632584
   ETSI, 2009, 102643V101 ETSI TR
   Ghinea G, 2005, IEEE T MULTIMEDIA, V7, P786, DOI 10.1109/TMM.2005.850960
   Ghinea G., 2010, P INT C MAN EM DIG E, P277, DOI DOI 10.1145/1936254.1936308
   Ghinea G., 2011, MULTIPLE SENSORIAL M
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   Ghinea G, 2010, IEEE T SYST MAN CY A, V40, P657, DOI 10.1109/TSMCA.2010.2041224
   Heilig M, 1962, Patent No. [3,050,870, 3050870]
   Ishibashi Y., 2004, P 12 ANN ACM INT C M, P604
   ITu, 1998, P 911 SUBJ AUD QUAL
   Jakesch M, 2011, RES ENG DES, V22, P143, DOI 10.1007/s00163-010-0102-5
   Kahol K, 2006, ACM T MULTIM COMPUT, V2, P219, DOI 10.1145/1152149.1152153
   Kaye J.N., 2001, THESIS MIT
   Murray N, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540994
   Murray Niall., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys'13, P162, DOI [https://doi.org/10.1145/2483977.2483999, DOI 10.1145/2483977.2483999]
   Nagle HT, 1998, IEEE SPECTRUM, V35, P22, DOI 10.1109/6.715180
   NTT Com, 2006, MOV ENH INT BAS FRAG
   Steinbach E, 2012, P IEEE, V100, P937, DOI 10.1109/JPROC.2011.2182100
   Washburn D., 2003, Modelling and Simulation Magazine, V2, P3
   Yuan Z., 2014, P IEEE INT WIR COMM
NR 27
TC 50
Z9 50
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2014
VL 11
IS 1
SU S
SI SI
AR 15
DI 10.1145/2661329
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS0RG
UT WOS:000343984800007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Pazzi, RW
   Boukerche, A
AF Pazzi, Richard W.
   Boukerche, Azzedine
TI PROPANE: A Progressive Panorama Streaming Protocol to Support
   Interactive 3D Virtual Environment Exploration on Graphics-Constrained
   Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Performance; Multimedia
   communications; streaming protocol; image-based rendering; remote
   rendering and streaming; wireless networks; mobile devices
ID WALKTHROUGH
AB Image-Based Rendering (IBR) has become widely known by its relatively low requirements for generating new scenes based on a sequence of reference images. This characteristic of IBR shows a remarkable potential impact in rendering complex 3D virtual environments on graphics-constrained devices, such as head-mounted displays, set-top boxes, media streaming devices, and so on. If well exploited, IBR coupled with remote rendering would enable the exploration of complex virtual environments on these devices. However, remote rendering requires the transmission of a large volume of images. In addition, existing solutions consider limited and/or deterministic navigation schemes as a means of decreasing the volume of streamed data. This article proposes the PROgressive PANorama StrEaming protocol (PROPANE) to offer users a smoother virtual navigation experience by prestreaming the imagery data required to generate new views as the user wanders within a 3D environment. PROPANE is based on a very simple yet effective trigonometry model and uses a strafe (lateral movement) technique to minimize the delay between image updates at the client end. This article introduces the concept of key partial panoramas, namely panorama segments that cover movements in any direction by simply strafing from an appropriate key partial panorama and streaming the amount of lost pixels. Therefore, PROPANE can provide a constrained device with sufficient imagery data to cover a future user's viewpoints, thereby minimizing the impact of transmission delay and jitter. PROPANE has been implemented and compared to two baseline remote rendering schemes. The evaluation results show that the proposed technique outperforms the selected and closely related existing schemes by minimizing the response time while not limiting the user to predefined paths as opposed to previous protocols.
C1 [Pazzi, Richard W.] Univ Ontario, Inst Technol, Oshawa, ON L1H 7K4, Canada.
   [Boukerche, Azzedine] Univ Ottawa, Ottawa, ON K1N 6N5, Canada.
C3 Ontario Tech University; University of Ottawa
RP Pazzi, RW (corresponding author), Univ Ontario, Inst Technol, 2000 Simcoe St North, Oshawa, ON L1H 7K4, Canada.
EM Richard.pazzi@uoit.ca
OI Pazzi, Richard/0000-0003-4308-6265
FU Natural Sciences and Engineering Research Council of Canada (NSERC)
   Discovery Grant Program; Canada Research Chair Program; NSERC DIVA
   Strategic Research Network; MRI/ORF Research Grants
FX This work is partially sponsored by the Natural Sciences and Engineering
   Research Council of Canada (NSERC) Discovery Grant Program, Canada
   Research Chair Program, NSERC DIVA Strategic Research Network, and
   MRI/ORF Research Grants.
CR Anguelov D, 2010, COMPUTER, V43, P32, DOI 10.1109/MC.2010.170
   [Anonymous], 2009, PROC IEEE GLOB TELEC
   Bouatouch K, 2005, RR5447 INRIA
   Boukerche A., 2006, ACM Multimedia, P691, DOI DOI 10.1145/1180639.1180785
   Boukerche A, 2007, IEEE ICC, P1692, DOI 10.1109/ICC.2007.283
   Boukerche A, 2008, COMPUT COMMUN, V31, P2716, DOI 10.1016/j.comcom.2008.02.032
   Bradley D., 2005, P IEEE INT WORKSH HA
   Chi HC, 2007, IEEE J SEL AREA COMM, V25, P119, DOI 10.1109/JSAC.2007.070112
   Endo T, 1998, VSMM98: FUTUREFUSION - APPLICATION REALITIES FOR THE VIRTUAL AGE, VOLS 1 AND 2, P269
   Jiang ZD, 2006, LECT NOTES COMPUT SC, V4261, P641
   Kilner J, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P177
   Kimata H., 2012, 2012 IEEE 1st Global Conference on Consumer Electronics (GCCE 2012), P574, DOI 10.1109/GCCE.2012.6379918
   Leadbetter R., 2013, SECRETS WII U GAMEPA
   Lei Y, 2004, LECT NOTES COMPUT SC, V3252, P728
   LENGYEL ERIC., 2002, MATH 3D GAME PROGRAM
   Luo JG, 2009, IEEE T PARALL DISTR, V20, P59, DOI 10.1109/TPDS.2008.68
   Maamar HR, 2013, IEEE WIREL COMMUN, V20, P136, DOI 10.1109/MWC.2013.6549293
   Seitz S. M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P21, DOI 10.1145/237170.237196
   Seo D., 2013, MULTIMEDIA SYST, V19, P1
   Shea R, 2013, IEEE NETWORK, V27, P16, DOI 10.1109/MNET.2013.6574660
   Shum H., 2012, IMAGE BASED RENDERIN
   Tanimoto M, 2009, IEEE INT CON MULTI, P1552, DOI 10.1109/ICME.2009.5202803
   Yang L., 2002, P WORKSH VIRT ENV EG
   Zhao Q, 2013, IEEE T MULTIMEDIA, V15, P1745, DOI 10.1109/TMM.2013.2280249
NR 24
TC 4
Z9 4
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2014
VL 11
IS 1
AR 5
DI 10.1145/2602222
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AP2XQ
UT WOS:000341939800005
DA 2024-07-18
ER

PT J
AU Ademoye, OA
   Ghinea, G
AF Ademoye, Oluwakemi A.
   Ghinea, Gheorghita
TI Information Recall Task Impact in Olfaction-Enhanced Multimedia
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Experimentation; Human Factors; Olfaction; information recall;
   multimedia
AB Enhancing multimedia applications with olfactory sensations is one of the last challenges in the area. While there is evidence, both scientific and anecdotal, that olfactory cues help users in information recall tasks, there is a lack of work when the targeted information is one contained in a multimedia presentation, which is precisely the focus of this article. Accordingly, we present the results of two experimental studies. The first study measured the impact of olfactory media variation on the user's ability to perceive, synthesize, and analyze the informational content of olfactory-enhanced multimedia videos; the second study measured the impact of information content, and an information recall task in respect of user perception of the relevance, sense of reality, and acceptability of the olfactory media content, as well as the overall enjoyment of the experience. Results show that the use of olfactory media content, both pleasant and unpleasant, in multimedia displays does not significantly impact on information assimilation in a negative way. Moreover, the addition of a performance task may enhance the user's understanding of the correlation between the characteristic odor(s) and the scenario under consideration, as well as enable users to consciously learn the odors.
C1 [Ademoye, Oluwakemi A.] Swansea Metropolitan Univ, Sch Appl Comp, Swansea, W Glam, Wales.
   [Ghinea, Gheorghita] Brunel Univ, Dept Informat Syst & Comp, Uxbridge UB8 3PH, Middx, England.
C3 Swansea Metropolitan University; Brunel University
RP Ghinea, G (corresponding author), Brunel Univ, Dept Informat Syst & Comp, Kingston Lane, Uxbridge UB8 3PH, Middx, England.
EM george.ghinea@brunel.ac.uk
RI Ghinea, Gheorghita/AAG-6770-2020
OI Ghinea, Gheorghita/0000-0003-2578-5580; Ademoye,
   Kemi/0000-0001-9597-4497
CR ADEMOYE O., 2007, P 14 MULT COMP NETW, V6504
   Bodnar A, 2004, P 6 INT C MULTIMODAL, P183, DOI [10.1145/1027933.1027965, DOI 10.1145/1027933.1027965]
   Boyd-Davis S., 2006, P HUM FACT ERG SOC 2, P25
   Brewster S. A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P653
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   FOX K, 2007, SMELL REPORT HUMAN S
   Ghinea G., 1998, Proceedings ACM Multimedia 98, P49, DOI 10.1145/290747.290754
   Ghinea G, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P473, DOI 10.1145/319463.319689
   Ghinea G., 2012, ACM T MULTIMEDIA COM, V8
   Issanchou S, 2002, OLFACTION, TASTE, AND COGNITION, P211, DOI 10.1017/CBO9780511546389.020
   JUMISKO-PYYKKO S., 2007, P IS T SPIE 19 ANN S, V6507
   KAYE JN, 2001, THESIS MIT MASSACHUS
   Köster EP, 2002, OLFACTION, TASTE, AND COGNITION, P27, DOI 10.1017/CBO9780511546389.007
   Mochizuki A., 2004, ACM SIGGRAPH 2004 Sketches, P123
   Nakamoto T, 2005, SENSOR LETT, V3, P136, DOI 10.1166/sl.2005.018
   Nakamoto T, 2005, SENSOR MATER, V17, P365
   Nakamoto T, 2008, IEEE COMPUT GRAPH, V28, P75, DOI 10.1109/MCG.2008.3
   Tortell R., 2007, Virtual Reality, V11, P61, DOI 10.1007/s10055-006-0056-0
   Washburn DA, 2004, COMPUT SCI ENG, V6, P80, DOI 10.1109/MCSE.2004.66
NR 19
TC 38
Z9 39
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2013
VL 9
IS 3
AR 17
DI 10.1145/2487268.2487270
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 175GL
UT WOS:000321218800002
DA 2024-07-18
ER

PT J
AU Alsulaiman, FA
   Sakr, N
   Valdés, JJ
   El Saddik, A
AF Alsulaiman, Fawaz A.
   Sakr, Nizar
   Valdes, Julio J.
   El Saddik, Abdulmotaleb
TI Identity Verification Based on Handwritten Signatures with Haptic
   Information Using Genetic Programming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Measurement; Security; Haptics; Biometrics; Genetic
   Programming; user verification; classification
ID CLASSIFICATION; SELECTION
AB In this article, haptic-based handwritten signature verification using Genetic Programming (GP) classification is presented. A comparison of GP-based classification with classical classifiers including support vector machine, k-nearest neighbors, naive Bayes, and random forest is conducted. In addition, the use of GP in discovering small knowledge-preserving subsets of features in high-dimensional datasets of haptic-based signatures is investigated and several approaches are explored. Subsets of features extracted from GP-generated models (analytic functions) are also exploited to determine the importance and relevance of different haptic data types (e.g., force, position, torque, and orientation) in user identity verification. The results revealed that GP classifiers compare favorably with the classical methods and use a much fewer number of attributes (with simple function sets).
C1 [Alsulaiman, Fawaz A.; Sakr, Nizar; El Saddik, Abdulmotaleb] Univ Ottawa, Ottawa, ON K1N 6N5, Canada.
   [Valdes, Julio J.] Natl Res Council Canada, Inst Informat Technol, Ottawa, ON K1A 0R6, Canada.
C3 University of Ottawa; National Research Council Canada
RP Sakr, N (corresponding author), Univ Ottawa, 800 King Edward Ave, Ottawa, ON K1N 6N5, Canada.
EM nsakr@site.uottawa.ca
RI /D-4159-2009
OI /0000-0002-7690-8547
CR AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1023/A:1022689900470
   [Anonymous], 2002, INTELLIGENT DATA ANA
   [Anonymous], P IEEE S COMP INT SE
   [Anonymous], 2010, IEEE INT S HAPT AUD
   [Anonymous], 1995, 11 C UNC ART INT SAN, DOI DOI 10.1109/TGRS.2004.834800
   Bhowan U., 2011, LECT NOTES COMPUTER, V6464, P243
   Bhowan U, 2011, LECT NOTES ARTIF INT, V7106, P192, DOI 10.1007/978-3-642-25832-9_20
   Bhowan U, 2010, LECT NOTES COMPUT SC, V6021, P1, DOI 10.1007/978-3-642-12148-7_1
   Bhowan U, 2009, IEEE C EVOL COMPUTAT, P2802, DOI 10.1109/CEC.2009.4983294
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Cha J, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1596990.1596993
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Doucette J, 2008, LECT NOTES COMPUT SC, V4971, P266, DOI 10.1007/978-3-540-78671-9_23
   Draper B., 2000, VIDERE, V1, P86
   Duda R., 1973, Pattern Classification and Scene Analysis
   Eggermont Jeroen., 2004, P 2004 ACM S APPL CO, P1001
   El Saddik A, 2007, IEEE T INSTRUM MEAS, V56, P895, DOI 10.1109/TIM.2006.887174
   Espejo PG, 2010, IEEE T SYST MAN CY C, V40, P121, DOI 10.1109/TSMCC.2009.2033566
   Fan RE, 2005, J MACH LEARN RES, V6, P1889
   Ferreira C., 2006, GENE EXPRESSION PROG, V21
   Fogel DB, 2000, IEEE SPECTRUM, V37, P26, DOI 10.1109/6.819926
   Friedlander A, 2011, IEEE C EVOL COMPUTAT, P941
   Gathercole C, 1994, LECT NOTES COMPUT SC, V866, P312
   Gütlein M, 2009, 2009 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND DATA MINING, P332, DOI 10.1109/CIDM.2009.4938668
   Hall M., 2009, SIGKDD EXPLORATIONS, V11, P10, DOI [10.1145/1656274.1656278, DOI 10.1145/1656274.1656278]
   Hall M., 1999, PhD thesis
   Iglesias R., 2007, Proc. of IEEE International Workshop on Haptic, P102
   Iglesias R, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P451
   Iglesias R, 2008, 42ND ANNUAL 2008 IEEE INTERNATIONAL CARNAHAN CONFERENCE ON SECURITY TECHNOLOGY, PROCEEDINGS, P174, DOI 10.1109/CCST.2008.4751298
   Koza J. R., 1989, IJCAI-89 Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, P768
   Koza J. R., 1994, Genetic programming II: Automatic discovery of reusable programs, VII, DOI DOI 10.5555/183460
   KOZA JR, 1994, STAT COMPUT, V4, P87, DOI 10.1007/BF00175355
   Krawiec K, 2005, IEEE T SYST MAN CY B, V35, P409, DOI 10.1109/TSMCB.2005.846644
   Krawiec K., 2002, Genetic Programming and Evolvable Machines, V3, P329, DOI 10.1023/A:1020984725014
   Kubat M., 2004, P 14 INT C MACH LEAR, P179
   Kuncheva L.I., 2005, Combining Pattern Classifiers, Methods and Algorithms
   Loveard T, 2001, IEEE C EVOL COMPUTAT, P1070, DOI 10.1109/CEC.2001.934310
   Luke S., 2007, ECJ: A Java-based evolutionary computation research system, 2000-2007
   Malek B., 2006, P EUROHAPTICS INT C, P179
   Neshatian K., 2009, P 12 EUR C GEN PROGR
   Neshatian K, 2009, P 11 ANN C GEN EV CO
   Ok S., 2000, PRICAI 2000. Topics in Artificial Intelligence. 6th Pacific Rim International Conference on Artificial Intelligence. Proceedings (Lecture Notes in Artificial Intelligence Vol.1886), P435
   Orozco M., 2006, P C VIRT CONC
   Orozco M, 2008, MULTIMED TOOLS APPL, V37, P73, DOI 10.1007/s11042-007-0169-9
   Orozco M, 2006, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2006, PROCEEDINGS, P265
   Pazzani M.J., 1994, INT C MACHINE LEARNI, P217
   Sakr Nizar, 2009, Proceedings of the 2009 International Conference on Data Mining. DMIN 2009, P71
   Sakr Nizar, 2010, 2010 IEEE Haptics Symposium (Formerly known as Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems), P467, DOI 10.1109/HAPTIC.2010.5444614
   TOMEK I, 1976, IEEE T SYST MAN CYB, V6, P769, DOI 10.1109/tsmc.1976.4309452
   Valdes J. J., 2007, PROC GENET EVOL COMP, P2953
   Van Hulse J., 2007, P 24 INT C MACH LEAR, DOI [DOI 10.1145/1273496.1273614, 10.1145/1273496.1273614]
   Vapnik V., 1998, STAT LEARNING THEORY, V3
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.1093/jee/39.2.269
   Zhang MJ, 2006, PATTERN RECOGN LETT, V27, P1266, DOI 10.1016/j.patrec.2005.07.024
   Zhang MJ, 2004, LECT NOTES COMPUT SC, V3005, P369
NR 57
TC 3
Z9 3
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2013
VL 9
IS 2
AR 11
DI 10.1145/2457450.2457453
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 144MS
UT WOS:000318944400003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Park, JS
   Jain, R
AF Park, Jong-Seung
   Jain, Ramesh
TI Identification of Scene Locations from Geotagged Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Visual context; image metadata;
   geotag; stereo vision
ID STEREO; NEOGEOGRAPHY; SYSTEM
AB Due to geotagging capabilities of consumer cameras, it has become easy to capture the exact geometric location where a picture is taken. However, the location is not the whereabouts of the scene taken by the photographer but the whereabouts of the photographer himself. To determine the actual location of an object seen in a photo some sophisticated and tiresome steps are required on a special camera rig, which are generally not available in common digital cameras. This article proposes a novel method to determine the geometric location corresponding to a specific image pixel. A new technique of stereo triangulation is introduced to compute the relative depth of a pixel position. Geographical metadata embedded in images are utilized to convert relative depths to absolute coordinates. When a geographic database is available we can also infer the semantically meaningful description of a scene object from where the specified pixel is projected onto the photo. Experimental results demonstrate the effectiveness of the proposed approach in accurately identifying actual locations.
C1 [Park, Jong-Seung] Univ Incheon, Dept Comp Sci & Engn, Songdo Dong 406772, Incheon, South Korea.
   [Jain, Ramesh] Univ Calif Irvine, Irvine, CA 92697 USA.
C3 Incheon National University; University of California System; University
   of California Irvine
RP Park, JS (corresponding author), Univ Incheon, Dept Comp Sci & Engn, Songdo Dong 406772, Incheon, South Korea.
EM jong@incheon.ac.kr
FU University of Incheon International Cooperative Research
FX This work was supported by the University of Incheon International
   Cooperative Research Grant in 2010.
CR [Anonymous], 2011, I C COMM SOFTW NET
   [Anonymous], P FH SCI DAY
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   CHANG CC, 1992, CONFERENCE RECORD OF THE TWENTY-SIXTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P1037, DOI 10.1109/ACSSC.1992.269140
   Ebling MR, 2010, IEEE PERVAS COMPUT, V9, P5, DOI 10.1109/MPRV.2010.5
   Gluckman J, 2001, PROC CVPR IEEE, P111
   Goodchild MichaelF., 2000, SOCIAL SCI DIGITAL W, P163
   Hadjitheophanous S, 2010, DES AUT TEST EUROPE, P1743
   Haklay M, 2008, GEOGR COMPASS, V2, P2011, DOI 10.1111/j.1749-8198.2008.00167.x
   Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547
   Hoashi K, 2009, IEEE INT CON MULTI, P606, DOI 10.1109/ICME.2009.5202569
   Hudson-Smith A, 2009, J LOCAT BASED SERV, V3, P118, DOI 10.1080/17489720902950366
   IPTC, 2010, TECH REP
   Jain R., 2010, INT C MULTIMEDIA, P1259, DOI DOI 10.1145/1873951.1874199
   Jawed Khurram, 2009, 2009 International Conference on Computational Science and Engineering (CSE), P277, DOI 10.1109/CSE.2009.473
   JEITA, 2002, TECH REP JEITA CP 34
   JOHNSON L., 2009, THE HORIZON REPORT, P15
   Kalantidis Y, 2011, MULTIMED TOOLS APPL, V51, P555, DOI 10.1007/s11042-010-0651-7
   Kanatani K., 2008, 19th British Machine Vision Conference (BMVC), P173
   Loop C., 1999, Comput. Vision Pattern Recognit, V1, P1125, DOI [10.1109/CVPR.1999.786928, DOI 10.1109/CVPR.1999.786928]
   MWG, 2009, TECH REP
   NIMA, 1997, TECH REP TR8350 2
   Parulski KA, 2009, IMAGE PROCESS SER, P351
   Pollefeys M, 2004, LECT NOTES COMPUT SC, V3023, P509
   Ramm F., 2010, OpenStreetMap: Using and Enhancing the Free Map of the World
   Singh V.K., 2010, P INT C MULTIMEDIA, P481
   SINNOTT RW, 1984, SKY TELESCOPE, V68, P159
   Tomasi C., 1991, TECH REP CMU CS 91 1
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   VAJDA P., 2010, P SOC PHOTO-OPT INS, V7798, P27
   van der Mark W, 2006, IEEE T INTELL TRANSP, V7, P38, DOI 10.1109/TITS.2006.869625
   Viana W, 2008, IEEE INT SYM MULTIM, P310, DOI 10.1109/ISM.2008.15
   Wang JH, 2007, LECT NOTES COMPUT SC, V4577, P371
   WEIH R. C., 2009, J ARKANSAS ACAD SCI, V63, P163
   Yaegashi K, 2009, LECT NOTES COMPUT SC, V5414, P361, DOI 10.1007/978-3-540-92957-4_32
   Yuan JS, 2010, IEEE T MULTIMEDIA, V12, P705, DOI 10.1109/TMM.2010.2051868
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zitová B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 38
TC 2
Z9 2
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2013
VL 9
IS 1
AR 5
DI 10.1145/2422956.2422961
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 097EO
UT WOS:000315457000005
DA 2024-07-18
ER

PT J
AU Gopinathan, A
   Li, ZP
AF Gopinathan, Ajay
   Li, Zongpeng
TI Algorithms for Stochastic Optimization of Multicast Content Delivery
   with Network Coding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Theory; Linear programming; multicast; network coding;
   stochastic optimization
ID SERVICE LEVEL AGREEMENTS
AB The usage of network resources by content providers is commonly governed by Service-Level Agreements (SLA) between the content provider and the network service provider. Resource usage exceeding the limits specified in the SLA incurs the content provider additional charges, usually at a higher cost. Hence, the content provider's goal is to provision adequate resources in the SLA based on forecasts of future demand. We study capacity purchasing strategies when the content provider employs network coded multicast as the media delivery mechanism, with uncertainty in its future customer set explicitly taken into consideration. The latter requires the content provider to make capacity provisioning decisions based on market predictions and historical customer usage patterns. The probabilistic element suggests a stochastic optimization approach. We model this problem as a two-stage stochastic optimization problem with recourse. Such optimizations are #P-hard to solve directly, and we design two approximation algorithms for them. The first is a heuristic algorithm that exploits properties unique to network coding, so that only polynomial-time operations are needed. It performs well in general scenarios, but the gap from the optimal solution is not bounded by any constant in the worst case. This motivates our second approach, a sampling algorithm partly inspired from the work of Gupta et al. [2004a]. We employ techniques from duality theory in linear optimization to prove that the sampling algorithm provides a 3-approximation to the stochastic multicast problem. We conduct extensive simulations to illustrate the efficacy of both algorithms, and show that the performance of both is usually within 10% of the optimal solution in practice.
C1 [Gopinathan, Ajay; Li, Zongpeng] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Gopinathan, A (corresponding author), Univ Calgary, Dept Comp Sci, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
EM ajay.gopinathan@ucalgary.ca
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   BEALE EML, 1955, J ROY STAT SOC B, V17, P173
   Birge J. R., 1997, Introduction to Stochastic Programming
   Bouillet E, 2002, IEEE J SEL AREA COMM, V20, P691, DOI 10.1109/JSAC.2002.1003036
   Dantzig GB, 1955, MANAGE SCI, V1, P197, DOI 10.1287/mnsc.1.3-4.197
   DUAN Z., 2003, IEEE ACM T NETWORK, V11, P6
   Dyer M, 2006, MATH PROGRAM, V106, P423, DOI 10.1007/s10107-005-0597-0
   GUPTA A., 2009, PROCEEDINGS OF THE A
   GUPTA A., 2004, PROCEEDINGS OF THE A
   GUPTA A., 2004, PROCEEDINGS OF THE I
   HECKMANN O., 2002, PROCEEDINGS OF THE I
   IMMORLICA N., 2004, PROCEEDINGS OF THE A
   JAIN K., 2003, PROCEEDINGS OF THE A
   Kall P., 1994, STOCHASTIC PROGRAMMI
   Khalil I., 2002, Journal of Network and Systems Management, V10, P11, DOI 10.1023/A:1014449424633
   Koetter R, 2003, IEEE ACM T NETWORK, V11, P782, DOI 10.1109/TNET.2003.818197
   LI B., 2005, PROCEEDINGS OF THE I
   LI Z., 2004, PROCEEDINGS OF THE 3
   LI Z., 2007, PROCEEDINGS OF THE I
   Ma HD, 2002, ACM SIGCOMM COMP COM, V32, P31, DOI 10.1145/510726.510729
   Mitra D, 2005, IEEE ACM T NETWORK, V13, P221, DOI 10.1109/TNET.2005.845527
   Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481
   RATNAKAR N., 2005, PROCEEDINGS OF THE I
   SEN S, 1994, TELECOMMUN SYST, V3, P11, DOI 10.1007/BF02110042
   SHMOYS D. B., 2004, PROCEEDINGS OF THE I
   SLYKE R. V., 1969, SIAM J APPL MATH, V17, P638, DOI DOI 10.1137/0117061
   THIMM M., 2001, PROCEEDINGS OF THE 2
   Verma DC, 2004, P IEEE, V92, P1382, DOI 10.1109/JPROC.2004.832969
   WARDROP J., 1952, PROCEEDINGS OF THE I
NR 29
TC 0
Z9 0
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2012
VL 8
IS 4
AR 56
DI 10.1145/2379790.2379798
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 052QA
UT WOS:000312211900008
DA 2024-07-18
ER

PT J
AU Gao, XF
   Zhang, CM
   Huang, Y
   Deng, ZG
AF Gao, Xifeng
   Zhang, Caiming
   Huang, Yan
   Deng, Zhigang
TI A Robust High-Capacity Affine-Transformation- Invariant Scheme for
   Watermarking 3D Geometric Models
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; 3D model watermarking; affine transformation invariant;
   copyright protection; 3D model authentication; high capacity
ID MESH; STEGANOGRAPHY
AB In this article we propose a novel, robust, and high-capacity watermarking method for 3D meshes with arbitrary connectivities in the spatial domain based on affine invariants. Given a 3D mesh model, a watermark is embedded as affine-invariant length ratios of one diagonal segment to the residing diagonal intersected by the other one in a coplanar convex quadrilateral. In the extraction process, a watermark is recovered by combining all the watermark pieces embedded in length ratios through majority voting. Extensive experimental results demonstrate the robustness, high computational efficiency, high capacity, and affine-transformation-invariant characteristics of the proposed approach.
C1 [Gao, Xifeng; Deng, Zhigang] Univ Houston, Dept Comp Sci, Houston, TX 77004 USA.
   [Zhang, Caiming; Huang, Yan] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
C3 University of Houston System; University of Houston; Shandong University
RP Deng, ZG (corresponding author), Univ Houston, Dept Comp Sci, Houston, TX 77004 USA.
EM zdeng@cs.uh.edu
RI Zhang, Caiming/AHD-6558-2022
OI Zhang, Caiming/0000-0002-6365-6221; Deng, Zhigang/0000-0003-2571-5865;
   Deng, Zhigang/0000-0002-0452-8676
FU National Nature Science Foundation of China [61020106001, 60933008]; US
   National Science Foundation [IIS-0914965]; Direct For Computer & Info
   Scie & Enginr; Div Of Information & Intelligent Systems [0914965]
   Funding Source: National Science Foundation
FX This work is supported in part by the National Nature Science Foundation
   of China (61020106001, 60933008) and US National Science Foundation
   IIS-0914965.
CR Agarwal P, 2007, MM&SEC'07: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2007, P175
   AIGER D., 2008, P SIGG 08 C
   [Anonymous], 2010, P IEEE INT SIL INS C
   Au OKC, 2006, IEEE T VIS COMPUT GR, V12, P386, DOI 10.1109/TVCG.2006.47
   Benedens O., 2000, Information Security. Third International Workshop, ISW 2000. Proceedings (Lecture Notes in Computer Science Vol.1975), P15
   Benedens O, 2000, COMPUT GRAPH FORUM, V19, pC199, DOI 10.1111/1467-8659.00412
   Bogomjakov A, 2008, COMPUT GRAPH FORUM, V27, P637, DOI 10.1111/j.1467-8659.2008.01161.x
   Cayre F, 2003, SIGNAL PROCESS-IMAGE, V18, P309, DOI 10.1016/S0923-5965(02)00147-9
   CAYRE F, 2004, RR5223 INRIA
   Chao MW, 2009, IEEE T VIS COMPUT GR, V15, P274, DOI 10.1109/TVCG.2008.94
   CHENG S.-C., 2009, ASIAN J HLTH INF SCI, V4, P36
   Cheng SC, 2005, PATTERN RECOGN, V38, P527, DOI 10.1016/j.patcog.2004.08.016
   Cho JW, 2007, IEEE T SIGNAL PROCES, V55, P142, DOI 10.1109/TSP.2006.882111
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Efimov N.V., 1980, - Higher Geometry
   Gao XF, 2010, MM&SEC 2010: 2010 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, PROCEEDINGS, P137
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Hoppe H., 1996, Proc. ACM SIGGRAPH, P99
   Huttenlocher D. P., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P263, DOI 10.1109/CVPR.1991.139699
   Kalivas A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P676
   Konstantinides JM, 2009, IEEE T MULTIMEDIA, V11, P23, DOI 10.1109/TMM.2008.2008913
   Lee SH, 2007, DIGIT SIGNAL PROCESS, V17, P396, DOI 10.1016/j.dsp.2005.04.014
   Li L, 2004, COMPUT GRAPH-UK, V28, P981, DOI 10.1016/j.cag.2004.08.002
   Liu Y, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P43
   Luo M, 2008, IEEE IMAGE PROC, P441, DOI 10.1109/ICIP.2008.4711786
   Ohbuchi R, 1998, IEEE J SEL AREA COMM, V16, P551, DOI 10.1109/49.668977
   Ohbuchi R, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P261, DOI 10.1145/266180.266377
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   PEREZ-FREIRE L., 2006, LECT NOTES COMPUTER
   PRAUN E, 1999, P ACM SIGGRAPH 99, P325
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Uccheddu F, 2008, PROC SPIE, V6819, DOI 10.1117/12.766480
   Wagner M. G., 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P201, DOI 10.1109/GMAP.2000.838252
   Wang K, 2011, COMPUT GRAPH-UK, V35, P1, DOI 10.1016/j.cag.2010.09.010
   Wang K, 2009, IEEE IMAGE PROC, P3657, DOI 10.1109/ICIP.2009.5414248
   Wang K, 2008, IEEE T INF FOREN SEC, V3, P620, DOI 10.1109/TIFS.2008.2007229
   Wang K, 2008, IEEE T MULTIMEDIA, V10, P1513, DOI 10.1109/TMM.2008.2007350
   Wang YP, 2009, IEEE T VIS COMPUT GR, V15, P285, DOI 10.1109/TVCG.2008.101
   Yin KK, 2001, COMPUT GRAPH-UK, V25, P409, DOI 10.1016/S0097-8493(01)00065-6
   Yu ZQ, 2003, PATTERN RECOGN, V36, P2603, DOI 10.1016/S0031-3203(03)00086-4
   Zafeiriou S, 2005, IEEE T VIS COMPUT GR, V11, P596, DOI 10.1109/TVCG.2005.71
NR 43
TC 12
Z9 13
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 2
SU S
AR 34
DI 10.1145/2344436.2344440
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 011KE
UT WOS:000309162800004
DA 2024-07-18
ER

PT J
AU Jiang, W
   Cotton, C
   Chang, SF
   Ellis, D
   Loui, AC
AF Jiang, Wei
   Cotton, Courtenay
   Chang, Shih-Fu
   Ellis, Dan
   Loui, Alexander C.
TI Audio-Visual Atoms for Generic Video Concept Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Semantic concept detection; joint
   audio-visual analysis; audio-visual atom; audio-visual codebook
AB We investigate the challenging issue of joint audio-visual analysis of generic videos targeting at concept detection. We extract a novel local representation, Audio-Visual Atom (AVA), which is defined as a region track associated with regional visual features and audio onset features. We develop a hierarchical algorithm to extract visual atoms from generic videos, and locate energy onsets from the corresponding soundtrack by time-frequency analysis. Audio atoms are extracted around energy onsets. Visual and audio atoms form AVAs, based on which discriminative audio-visual codebooks are constructed for concept detection. Experiments over Kodak's consumer benchmark videos confirm the effectiveness of our approach.
C1 [Jiang, Wei; Cotton, Courtenay; Chang, Shih-Fu; Ellis, Dan] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
   [Loui, Alexander C.] Eastman Kodak Co, Kodak Res Labs, Rochester, NY 10027 USA.
C3 Columbia University; Eastman Kodak
RP Jiang, W (corresponding author), Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
EM wjiang@ee.columbia.edu
FU NSF [CNS-07-16293, CNS-07-51078]; New York State NYSTAR CATT
FX This work has been supported by a gift from Eastman Kodak, NSF grants
   CNS-07-16293 and CNS-07-51078, and the New York State NYSTAR CATT
   program. W. Jiang has also been supported as a Kodak Graduate Fellow.
CR ANEMUELLER J, 2008, P INT C COMP SCI
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 1981, P 7 INT JOINT C ART
   Barzelay Z., 2007, P IEEE C COMP VIS PA, P1
   Beal MJ, 2003, IEEE T PATTERN ANAL, V25, P828, DOI 10.1109/TPAMI.2003.1206512
   BIRCHFELD S, 2007, P ACM SIGMM WORKSH M
   CHANG S, 2007, P ACM SIGMM WORKSH I
   Chen YX, 2004, J MACH LEARN RES, V5, P913
   Chu S, 2008, INT CONF ACOUST SPEE, P1, DOI 10.1109/ICASSP.2008.4517531
   Cristani M, 2007, IEEE T MULTIMEDIA, V9, P257, DOI 10.1109/TMM.2006.886263
   DeMenthon D., 2003, P 11 ACM INT C MULTI, P508
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   GALMAR E, 2007, P ACM INT C IM VID R
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Iwano K, 2007, EURASIP J AUDIO SPEE, DOI 10.1155/2007/64506
   Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903
   JIANG W, 2010, THESIS COLUMBIA U
   Kaucic R., 1996, Proceedings ofEuropean Conference on Computer Vision, P376
   Krstulovic S., 2006, Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, V3, P496
   Loui Alexander., 2007, MIR 07, P245
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Maron O, 1998, ADV NEUR IN, V10, P570
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Ogle JP, 2007, INT CONF ACOUST SPEE, P233
   Petitcolas F., 2003, MPEG MATLAB
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   TIEU K, 2001, INT J COMPUT VISION, V56, P228
   Yanagawa A., 2006, 21920065 ADVENT COL
   Zhou HY, 2009, COMPUT VIS IMAGE UND, V113, P345, DOI 10.1016/j.cviu.2008.08.006
NR 33
TC 5
Z9 6
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2010
VL 6
IS 3
SI SI
AR 14
DI 10.1145/1823746.1823748
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 649UL
UT WOS:000281799300003
DA 2024-07-18
ER

PT J
AU Kolan, P
   Dantu, R
   Cangussu, JW
AF Kolan, Prakash
   Dantu, Ram
   Cangussu, Joao W.
TI Nuisance Level of a Voice Call
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT ACM Multimedia Conference 2007
CY SEP 24-29, 2007
CL Augsburg, GERMANY
SP ACM
DE Security; Human Factors; Multimedia communications; security; presence;
   nuisance; unwantedness; behavior; tolerance
AB In our everyday life, we communicate with many people such as family, friends, neighbors, and colleagues. We communicate with them using different communication media such as email, telephone calls, and face-to-face interactions. While email is not real-time and face-to-face communications require geographic proximity, voice and video communications are preferred over other modes of communication. However, real-time voice/video calls may create nuisance to the receiver. In this article, we describe a mathematical model for computing nuisance level of incoming voice/video calls. We computed the closeness and nuisance level using the calling patterns between the caller and the callee. To validate the nuisance model, we collected cell phone call records of real-life people at our university and computed the nuisance value for all voice calls. We validated the nuisance levels using the feedback from those real-life people. Such a nuisance model is useful for predicting unwanted voice and video sessions in an IP communication network.
C1 [Kolan, Prakash; Dantu, Ram] Univ N Texas, Denton, TX 76203 USA.
   [Cangussu, Joao W.] Univ Texas Dallas, Richardson, TX 75083 USA.
C3 University of North Texas System; University of North Texas Denton;
   University of Texas System; University of Texas Dallas
RP Kolan, P (corresponding author), Univ N Texas, Denton, TX 76203 USA.
CR [Anonymous], P 2 VOIP SEC WORKSH
   Biever C, 2004, MOVE SPAM MAKE WAY S
   CANNY J, 2006, ACM QUEUE, V4, P6
   Cohen W., 1996, P AAAI SPRING S MACH
   Dantu R., 2006, Survey of calling patterns
   DANTU R, 2004, P IEEE GLOBECOMM WOR
   DANTU R, 2005, P USENIX SRUTI STEPS
   Eagle N. N., 2005, THESIS MIT
   Graham Paul., 2002, A PLAN FOR SPAM
   GRANOVETTER MS, 1973, AM J SOCIOL, V78, P1360, DOI 10.1086/225469
   HERSHKOP S, 2004, ACM CROSSROADS
   HERSHKOP S, 2005, URL CLUSTERING CLASS
   HERSHKOP S., 2005, P 11 ACM SIGKDD INT, P98
   KHALIL A, 2006, P C COMP SUPP COLL W
   Khalil A., 2005, P INT C HUM COMP INT
   KOLAN P, 2007, COMSW WORKSH BANG IN
   Kolan P, 2007, ACM T AUTON ADAP SYS, V2, DOI 10.1145/1216895.1216897
   LUCY S, 2006, IS YOUR VOIP PHONE V
   MANBER U, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P1
   MARSDEN PV, 1984, SOC FORCES, V63, P482, DOI 10.2307/2579058
   Mitchell T. M., 1997, MACHINE LEARNING
   RIGOUTSOS I, 2004, P 1 C E MAIL ANT
   Rosenberg J., 2006, A session initiation protocol (sip) event package for conference state
   Sahami M., 1998, P LEARN TEXT CAT 199, VVolume 62, P98
   Sakkis Georgios, 2003, INFORM RETRIEVAL
   SALTON G, 1998, INTRO MODERN INFORM
   Schooler E., 2002, 3261 RFC
   SEGAL RB, 1999, P 3 INT C AUT AG
   SOONTHORNPHISAJ N, 2002, P IEEE INT C SOFTW P
NR 29
TC 11
Z9 11
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2008
VL 5
IS 1
AR 6
DI 10.1145/1404880.1404886
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 376AS
UT WOS:000261155700006
DA 2024-07-18
ER

PT J
AU Duchowski, AT
   Cötekin, A
AF Duchowski, Andrew T.
   Cotekin, Arzu
TI Foveated gaze-contingent displays for peripheral LOD management, 3D
   visualization, and stereo Imaging
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE performance; human factors; eye tracking; foveation; gaze-contingent
   displays; level-of-detail
ID EFFECTIVE STIMULUS; ACUITY; SPAN
AB Advancements in graphics hardware have allowed development of hardware-accelerated imaging displays. This article reviews techniques for real-time simulation of arbitrary visual fields over still images and video. The goal is to provide the vision sciences and perceptual graphics communities techniques for the investigation of fundamental processes of visual perception. Classic gaze-contingent displays used for these purposes are reviewed and for the first time a pixel shader is introduced for display of a high-resolution window over peripherally degraded stimulus. The pixel shader advances current state-of-the-art by allowing real-time processing of still or streamed images, obviating the need for preprocessing or storage.
C1 [Duchowski, Andrew T.] Clemson Univ, Dept Comp Sci, Clemson, SC 29634 USA.
   [Cotekin, Arzu] Univ Zurich, GIVA, Dept Geog, CH-8057 Zurich, Switzerland.
C3 Clemson University; University of Zurich
RP Duchowski, AT (corresponding author), Clemson Univ, Dept Comp Sci, 100 McAdams Hall, Clemson, SC 29634 USA.
EM duchowski@acm.org
RI Çöltekin, Arzu/ACY-8666-2022
OI Çöltekin, Arzu/0000-0002-3178-3509
CR [Anonymous], 2003, LEVEL DETAIL 3D GRAP
   [Anonymous], 1996, P 23 ANN C COMP GRAP, DOI DOI 10.1145/237170.237262
   Baudisch P, 2003, COMMUN ACM, V46, P60, DOI 10.1145/636772.636799
   Bertera JH, 2000, PERCEPT PSYCHOPHYS, V62, P576, DOI 10.3758/BF03212109
   BOHME M, 2006, P EY TRACK RES APPL, P109
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   CAMPBELL FW, 1965, NATURE, V208, P191, DOI 10.1038/208191a0
   CHANGHASNAIN CJ, 2001, SPIE OE MAGAZINE, V1, P30
   Coltekin A., 2006, THESIS HELSINKI U TE
   DORR M, 2005, P 2 S APPL PERC GRAP
   DRASCIC D, 1991, WA SPIE 2, V1457, P58
   Duchowski A., 2003, Eye Tracking Methodology: Theory and Practice
   DUCHOWSKI AT, 1995, P SOC PHOTO-OPT INS, V2411, P128, DOI 10.1117/12.207556
   Duchowski AT, 2000, IEEE T IMAGE PROCESS, V9, P1437, DOI 10.1109/83.855439
   DUCHOWSKI AT, WA SPIE, V3391, P98
   DUCHOWSKI AT, 2004, P EY TRACK RES APPL, P59
   Foley J.D., 1990, Computer graphics: Principles and practice
   FOSTER DH, 1989, VISION RES, V29, P1017, DOI 10.1016/0042-6989(89)90116-8
   FUNKHOUSER TA, 1993, P ACM SIGGRAPH INT C
   Geisler W. S., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P83, DOI 10.1145/507072.507090
   GEISLER WWS, 1998, C HUM VIS EL IM BELL
   HAHN PJ, 1997, P DAT COMPR IND WORK
   Holliman N.:., 2005, 3D display systems
   Levoy M., 1990, Computer Graphics, V24, P217, DOI 10.1145/91394.91449
   LINDE IVD, 2003, THESIS ANGLIA POLYTE
   Lipton L., 1982, FDN STEREOSCOPIC CIN
   Loschky LC, 2000, ETRA '00, P97, DOI DOI 10.1145/355017.355032
   Loschky LC, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314310
   LUEBKE D, 2000, P ACM SIGGRAPH INT C
   LUEBKE D, 1997, P ACM SIGGRAPH INT C
   MCCONKIE GW, 1975, PERCEPT PSYCHOPHYS, V17, P578, DOI 10.3758/BF03203972
   Mon-Williams M, 1998, HUM FACTORS, V40, P42, DOI 10.1518/001872098779480622
   MURPHY H, 2001, EUROGRAPHICS C MANCH
   *NAT I HLTH, 2003, AG REL MAC DEGN WHAT
   Nikolov S.G., 2004, Proceedings of the 2004 Symposium on Eye Tracking Research \\ Applications, P11, DOI [10.1145/968363, DOI 10.1145/968363]
   O'Sullivan C, 2003, ACM T GRAPHIC, V22, P527, DOI 10.1145/882262.882303
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   Park DS, 2000, PHARMACOLOGY OF CEREBRAL ISCHEMIA 2000, P105, DOI 10.1145/355017.355033
   Parkhurst Derrick., 2004, APGV'04: Proceedings of the 1st Symposium on Applied perception in graphics and visualization, P49, DOI [10.1145/1012551, DOI 10.1145/1012551]
   Parkhurst DJ, 2002, HUM FACTORS, V44, P611, DOI 10.1518/0018720024497015
   Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543
   RAYNER K, 1979, SCIENCE, V206, P468, DOI 10.1126/science.504987
   Reddy M, 2001, IEEE COMPUT GRAPH, V21, P68, DOI 10.1109/38.946633
   Reingold EM, 2003, HUM FACTORS, V45, P307, DOI 10.1518/hfes.45.2.307.27235
   Shreiner D., 2006, OPENGL PROGRAMMING G
   *TOB TECHN AB, 2003, TOB ET 17 EY TRACK P
   Ware C., 2020, INFORM VISUALIZATION
   Watson B., 1997, ACM Transactions on Computer-Human Interaction, V4, P323, DOI 10.1145/267135.267137
   WATSON B, 2004, P ACM SIGGRAPH INT C, P750
   Williams L., 1983, Computer Graphics, V17, P1, DOI 10.1145/964967.801126
   [No title captured]
NR 51
TC 60
Z9 65
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 4
AR 24
DI 10.1145/1314303.1314309
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 250QU
UT WOS:000252315900006
DA 2024-07-18
ER

PT J
AU Heck, R
   Wallick, M
   Gleicher, M
AF Heck, Rachel
   Wallick, Michael
   Gleicher, Michael
TI Virtual Videography
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE design; automated camera management; video production; attention
   modeling; computational cinematography
AB Well-produced videos provide a convenient and effective way to archive lectures. In this article, we offer a new way to create lecture videos that retains many of the advantages of well-composed recordings, without the cost and intrusion of a video production crew. We present an automated system called Virtual Videography that employs the art of videography to mimic videographer-produced videos, while unobtrusively recording lectures. The system uses the data recorded by unattended video cameras and microphones to produce a new edited video as an offline postprocess. By producing videos offline, our system can use future information when planning shot sequences and synthesizing new shots. Using simple syntactic cues gathered from the original video and a novel shot planning algorithm, the system makes cinematic decisions without any semantic understanding of the lecture.
C1 Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Heck, R (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
EM heckr@cs.wisc.edu
CR Abowd GD, 1999, IBM SYST J, V38, P508, DOI 10.1147/sj.384.0508
   [Anonymous], 9811 CRL
   [Anonymous], MSRTR200289
   [Anonymous], 1991, Film directing shot by shot: visualizing from concept to screen
   [Anonymous], P 4 INT C INT US INT
   BIANCHI M, 1998, JOINT DARP NIST SMAR
   Bianchi M., 2004, MUM 04, P117
   Bordwell David., 1998, On the History of Film Style
   BURNS B, 1998, 3 BAY AR VIS M VIS A
   Burns Ken., 2001, JAZZ FILM KEN BURNS
   *CANOPUS, 2002, CAN IMAG
   Cantine John, 1995, SHOT SHOT PRACTICAL
   CHEN M, 2001, P ACM MULT C
   Christianson DB, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P148
   CLARK RE, 1983, REV EDUC RES, V53, P445, DOI 10.3102/00346543053004445
   Clark RolandC., 1991, EDUC TECHNOL, V31, P34
   CUTLER R, 2002, P ACM MULT C
   Drucker S. M., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P139, DOI 10.1145/199404.199428
   Girgensohn A, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P464
   GIRGENSOHN A, 2000, P UIST 00, P81
   GLEICHER M, 1992, COMP GRAPH, V26, P331, DOI 10.1145/142920.134088
   GLEICHER M, 2000, P ACM MULT C
   Gleicher M.L., 2002, SMARTGRAPH 02, P9
   HE L, 1999, P ACM MULT C
   HE LW, 1996, P 23 ANN C COMP GRAP, P217
   JAMES D, 2000, P AUSTR C COMP ED, P145
   Karp P., 1993, Proceedings Graphics Interface '93, P118
   LEE DS, 2002, P ACM MULT C
   LI Y, 2001, HPL2001191 IMAGING S
   LIU T, 2003, P 2 INT C IM VID RET
   MACHNICKI E, 2002, P SPIE C MULT COMP N
   MACHRONE B, 2003, EVERY PHOTO TELLS ST
   MUKHOPADHYAY S, 1999, P ACM MULT C
   Onishi M, 2001, PROC CVPR IEEE, P131
   Onishi M, 2000, INT C PATT RECOG, P615, DOI 10.1109/ICPR.2000.902994
   Pinhanez CS, 1997, APPL ARTIF INTELL, V11, P285, DOI 10.1080/088395197118163
   ROWE L, 2003, BIBS LECT WEBCASTING
   RUI Y, 2001, P ACM MULT C
   RUSSELL DM, 2000, P 33 HAW INT C SYST
   Saund E., 1999, IMAGE MOSAICING DIAG
   SAUND E, 2003, P 16 ANN ACM S US IN, P183
   *STAGETOOLS, 2002, MOV PICT
   STEINMETZ A, 2001, P SPIE C MULT COMP N, V4312
   SYEDAMAHMOOD T, 2000, P ACM MULT C
   TEODOSIO L, 1993, P ACM MULT C
   WALLICK MN, 2005, P MIR COMP VIS COMP
   WALLICK MN, 2004, P IEEE INT C MULT EX
   Wang F, 2004, INT C PATT RECOG, P934, DOI 10.1109/ICPR.2004.1334682
   2005, GREAT MEDIA DEBATE
NR 49
TC 31
Z9 37
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 1
AR 4
DI 10.1145/1198302.1198306
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IZ
UT WOS:000250871500004
DA 2024-07-18
ER

PT J
AU Moncrieff, S
   Venkatesh, S
   West, G
AF Moncrieff, Simon
   Venkatesh, Svetha
   West, Geoff
TI Online audio background determination for complex audio environments
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; management; audio analysis; surveillance and monitoring;
   online background modelling
AB We present a method for foreground/background separation of audio using a background modelling technique. The technique models the background in an online, unsupervised, and adaptive fashion, and is designed for application to long term surveillance and monitoring problems. The background is determined using a statistical method to model the states of the audio over time. In addition, three methods are used to increase the accuracy of background modelling in complex audio environments. Such environments can cause the failure of the statistical model to accurately capture the background states. An entropy-based approach is used to unify background representations fragmented over multiple states of the statistical model. The approach successfully unifies such background states, resulting in a more robust background model. We adaptively adjust the number of states considered background according to background complexity, resulting in the more accurate classification of background models. Finally, we use an auxiliary model cache to retain potential background states in the system. This prevents the deletion of such states due to a rapid influx of observed states that can occur for highly dynamic sections of the audio signal. The separation algorithm was successfully applied to a number of audio environments representing monitoring applications.
C1 Curtin Univ Technol, Dept Comp, Perth, WA 6845, Australia.
C3 Curtin University
RP Moncrieff, S (corresponding author), Curtin Univ Technol, Dept Comp, GPO Box U1987, Perth, WA 6845, Australia.
EM simonm@cs.curtin.edu.au
OI Venkatesh, Svetha/0000-0001-8675-6631
CR [Anonymous], 1993, Discrete Time Processing of Speech Signals
   [Anonymous], 1999, Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, DOI DOI 10.3115/1034678.1034693
   [Anonymous], 2004, IM PROC 2004 ICIP 04
   [Anonymous], 1992, LECT WAVELETS
   [Anonymous], 2005, IEEE INT C MULT EXP
   [Anonymous], 2000, Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations
   Azlan M., 2005, P ICOST 2005 3 INT C
   Chen JF, 2005, IEEE INT SYMP CIRC S, P1750
   Chen JF, 2005, LECT NOTES COMPUT SC, V3468, P47, DOI 10.1385/1-59259-820-x:047
   CLARKSON B, 1998, WORKSH PERC US INT S, P47
   Clavel C., 2005, IEEE INT C MULT EXP
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Cowling M, 2003, PATTERN RECOGN LETT, V24, P2895, DOI 10.1016/S0167-8655(03)00147-8
   Cristani M, 2004, INT C PATT RECOG, P399, DOI 10.1109/ICPR.2004.1334232
   Elgammal A, 2000, EUR C COMP VIS, P751, DOI DOI 10.1007/3-540-45053-X_48
   Ellis D., 2001, P WORKSH CONS REL AC
   Foote JT, 2003, PROC SPIE, V5021, P167, DOI 10.1117/12.476302
   Gaunard P, 1998, INT CONF ACOUST SPEE, P3609, DOI 10.1109/ICASSP.1998.679661
   MONCRIEFE S, 2005, IEEE INT C MULT EXP
   MONCRIEFE S, 2006, INT C PATT REC ICPR
   RADHAKRISHNAN R, 2004, P 6 ACM SIGMM INT WO, P157
   Stäger M, 2003, SEVENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P12, DOI 10.1109/ISWC.2003.1241387
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Vacher M, 2004, Proceedings of the Second IASTED International Conference on Biomedical Engineering, P395
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Zhang T, 1999, INT CONF ACOUST SPEE, P3001, DOI 10.1109/ICASSP.1999.757472
NR 26
TC 12
Z9 13
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 2
AR 8
DI 10.1145/1230812.1230814
PG 30
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230JA
UT WOS:000250871600002
DA 2024-07-18
ER

PT J
AU Shu, Z
   Gao, L
   Yi, S
   Wu, FY
   Ding, X
   Wan, T
   Xin, SQ
AF Shu, Zhenyu
   Gao, Ling
   Yi, Shun
   Wu, Fangyu
   Ding, Xin
   Wan, Ting
   Xin, Shiqing
TI Context-Aware 3D Points of Interest Detection via Spatial Attention
   Mechanism
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D point of interest; deep learning; attention mechanism
ID SALIENCY
AB Detecting points of interest is a fundamental problem in 3D shape analysis and can be beneficial to various tasks in multimedia processing. Traditional learning-based detection methods usually rely on each vertex's geometric features to discriminate points of interest from other vertices. Observing that points of interest are related to not only geometric features on themselves but also the geometric features of surrounding vertices, we propose a novel context-aware 3D points of interest detection algorithm by adopting the spatial attention mechanism in this article. By designing a context attention module, our approach presents a novel deep neural network to simultaneously pay attention to the geometric features of vertices and their local contexts during extracting points of interest. To obtain satisfactory extraction results, our method adaptively assigns different weights to those features in a data-driven way. Extensive experimental results on SHREC 2007, SHREC 2011, and SHREC 2014 datasets show that our algorithm achieves superior performance over existing methods.
C1 [Shu, Zhenyu] NingboTech Univ, Sch Comp & Data Engn, Ningbo 315100, Zhejiang, Peoples R China.
   [Shu, Zhenyu] Ningbo Inst, Zhejiang Univ, Ningbo 315100, Zhejiang, Peoples R China.
   [Gao, Ling; Wu, Fangyu; Ding, Xin; Wan, Ting] NingboTech Univ, Sch Comp & Data Engn, Ningbo 315100, Peoples R China.
   [Yi, Shun] NingboTech Univ, Sch Comp & Data Engn, Ningbo 315100, Peoples R China.
   [Yi, Shun] Zhejiang Univ, Sch Mech Engn, Hangzhou 310027, Zhejiang, Peoples R China.
   [Yi, Shun] Shandong Univ, Sch Comp Sci & Technol, Qingdao 315100, Shandong, Peoples R China.
C3 NingboTech University; Zhejiang University; NingboTech University;
   NingboTech University; Zhejiang University; Shandong University
RP Yi, S (corresponding author), NingboTech Univ, Sch Comp & Data Engn, Ningbo 315100, Peoples R China.; Yi, S (corresponding author), Zhejiang Univ, Sch Mech Engn, Hangzhou 310027, Zhejiang, Peoples R China.; Yi, S (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 315100, Shandong, Peoples R China.
EM shuzhenyu@nit.zju.edu.cn; 1404919041@qq.com; ys331_paper@163.com;
   fangyu.wu@zju.edu.cn; XDing07@163.com; 771716519@qq.com;
   xinshiqing@163.com
RI Ding, Xin/JDD-5161-2023
OI shu, zhenyu/0000-0001-5733-6638; Xin, Shiqing/0000-0001-8452-8723; Wan,
   Ting/0009-0008-5831-4180
FU National Natural Science Foundation of China [62172356, 61872321,
   62272277]; Zhejiang Provincial Natural Science Foundation of China
   [LY22F020026]; Ningbo Major Special Projects of the "Science and
   Technology Innovation 2025 [2020Z005, 2020Z007, 2021Z012, 2019B10128]
FX This work is supported by the National Natural Science Foundation of
   China (62172356, 61872321, 62272277), Zhejiang Provincial Natural
   Science Foundation of China (LY22F020026), the Ningbo Major Special
   Projects of the "Science and Technology Innovation 2025" (2020Z005,
   2020Z007, 2021Z012, 2019B10128).
CR [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Chen XB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185525
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Creusot C, 2013, INT J COMPUT VISION, V102, P146, DOI 10.1007/s11263-012-0605-9
   Dutagaci H., 2011, Proceedings of the 4th Eurographics conference on 3D Object Retrieval, P65
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Feixas M, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462056
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Gao Z, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377876
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Giorgi Daniela, 2007, SHREC Competition, V8
   Godil A., 2011, Three-Dimensional Imaging, Interaction, and Measurement, Proceedings of the IST/SPIE Electronic Imaging, 2011, San Francisco, CA, USA, 23-27 January 2011, VVolume 7864, P275
   He Yang, 2020, CVPR, P2009
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Lau M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925927
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Nousias S, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3550073
   Pickup D., 2014, P 7 EUROGRAPHICS WOR, P10
   Pratikakis I., 2010, EUR WORKSH 3D OBJ RE, P7, DOI DOI 10.2312/3DOR/3DOR10/007-014
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shu ZY, 2022, COMPUT AIDED DESIGN, V145, DOI 10.1016/j.cad.2021.103181
   Shu ZY, 2022, IEEE T MULTIMEDIA, V24, P1637, DOI 10.1109/TMM.2021.3070977
   Shu ZY, 2020, IEEE T VIS COMPUT GR, V26, P2671, DOI 10.1109/TVCG.2019.2892076
   Shu ZY, 2019, IEEE T VIS COMPUT GR, V25, P2583, DOI 10.1109/TVCG.2018.2848628
   Sun Ziyi, 2022, ACM T MULTIM COMPUT, V18
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Teran L, 2014, LECT NOTES COMPUT SC, V8689, P159, DOI 10.1007/978-3-319-10590-1_11
   Wang CW, 2016, LECT NOTES COMPUT SC, V9772, P601, DOI 10.1007/978-3-319-42294-7_54
   Wei GS, 2021, COMPUT AIDED DESIGN, V141, DOI 10.1016/j.cad.2021.103105
   Xie J, 2017, IEEE T MULTIMEDIA, V19, P2463, DOI 10.1109/TMM.2017.2698200
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yuan J, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3394955
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
   Zhu GY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3538648
   Zou GY, 2008, COMPUT ANIMAT VIRT W, V19, P399, DOI 10.1002/cav.244
NR 44
TC 0
Z9 0
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 202
DI 10.1145/3597026
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200025
DA 2024-07-18
ER

PT J
AU Zhao, SR
   Jiang, HY
   Tao, HQ
   Zha, R
   Zhang, K
   Xu, T
   Chen, EH
AF Zhao, Sirui
   Jiang, Hongyu
   Tao, Hanqing
   Zha, Rui
   Zhang, Kun
   Xu, Tong
   Chen, Enhong
TI PEDM: A Multi-task Learning Model for Persona-aware Emoji-embedded
   Dialogue Generation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Emoji embedding; dialogue generation; multi-task learning; personalized
   conversation
ID EMOTICON USAGE
AB As a vivid and linguistic symbol, Emojis have become a prevailing medium interspersed in text-based communication (e.g., social media and chit-chat) to express emotions, attitudes, and situations. Generally speaking, a social-oriented chatbot that can generate appropriate Emoji-embedded responses would be much more competitive, making communications more fun, engaging, and human-like. However, the current Emoji-related research is still in its infancy, leading to an awkward situation of data deficiency. How to develop an Emoji-embedded dialogue system while addressing the lack of data will be interesting and meaningful for the application of future AI. To bridge this gap, we propose a multi-task learning method for persona-aware Emoji-embedded dialogue generation in this article. Specifically, as the benchmark of model training and evaluation, which includes 1.2 million Emoji-embedded tweets and 1.1 million post-response pairs, we first construct a dataset named EmojiTweet to handle the data deficiency problem. Then, a Seq2Seq-based model with multi-task learning is designed to simultaneously learn response generation and Emoji embedding from the constructed non-Emoji dialogue and Emoji-embedded monologue data. Afterward, we incorporate persona factors into our model by adopting persona fusion and personalized bias methods to deliver personalized dialogues with more accurately selected Emojis. Finally, we conduct extensive experiments, where the experimental results and evaluations demonstrate that our model has three key benefits: improved dialogue quality, higher user engagement, and not relying on large-scale Emoji-embedded dialogue data representing specific personas. EmojiTweet will be published publicly via https://mea-lab- 421.github.io/EmojiTweet/.
C1 [Zhao, Sirui] Univ Sci & Technol China, Southwest Univ Sci & Technol, Hefei, Mianyang, Peoples R China.
   [Jiang, Hongyu] Thoughtworks, Chengdu, Peoples R China.
   [Tao, Hanqing; Zha, Rui; Xu, Tong; Chen, Enhong] Univ Sci & Technol China, Hefei, Peoples R China.
   [Zhang, Kun] Hefei Univ Technol, Hefei, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Southwest University of Science & Technology - China;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Hefei University of Technology
RP Xu, T; Chen, EH (corresponding author), Univ Sci & Technol China, Hefei, Peoples R China.
EM sirui@mail.ustc.edu.cn; hongyu.jiang@thoughtworks.com;
   hqtao@mail.ustc.edu.cn; zr990210@mail.ustc.edu.cn;
   zhang1028kun@gmail.com; tongxu@ustc.edu.cn; cheneh@ustc.edu.cn
RI Zha, Rui/JCN-5806-2023
OI Zha, Rui/0000-0002-6557-123X; Tao, Hanqing/0000-0002-5004-9756; Zhang,
   Kun/0000-0002-0743-9003; Chen, Enhong/0000-0002-4835-4102; Jiang,
   Hongyu/0000-0003-1534-188X
FU National Natural Science Foundation of China [61727809, 62072423,
   62006066, U22A2094]; CAAI-Huawei MindSpore Open Fund
   [CAAIXSJLJJ-2021-007B]; USTC Research Funds of the Double First-Class
   Initiative [YD2150002009]
FX This work was supported by the National Natural Science Foundation of
   China (Grant Nos. 61727809, 62072423, 62006066), the Joint Funds of the
   National Natural Science Foundation of China (U22A2094), the CAAI-Huawei
   MindSpore Open Fund (CAAIXSJLJJ-2021-007B), and the USTC Research Funds
   of the Double First-Class Initiative (No. YD2150002009).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Barbieri F., 2018, P 12 INT WORKSH SEM, P24, DOI DOI 10.18653/V1/S18-1003
   Barbieri F, 2018, Arxiv, DOI arXiv:1805.00731
   Barbieri F, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4766
   Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Berengueres J, 2017, IEEE INT CONF BIG DA, P4321, DOI 10.1109/BigData.2017.8258461
   Cappallo SH, 2018, Figshare, DOI 10.21942/uva.5822100.v3
   Chung JY, 2014, Arxiv, DOI arXiv:1412.3555
   Clark K, 2019, Arxiv, DOI arXiv:1907.04829
   Cohn N., 2018, P 40 ANN C COGNITIVE, P1524
   Deriu J, 2021, ARTIF INTELL REV, V54, P755, DOI 10.1007/s10462-020-09866-x
   Gao JF, 2018, ACM/SIGIR PROCEEDINGS 2018, P1371, DOI 10.1145/3209978.3210183
   Gruber N, 2020, FRONT ARTIF INTELL, V3, DOI 10.3389/frai.2020.00040
   Harwit E, 2017, CHIN J COMMUN, V10, P312, DOI 10.1080/17544750.2016.1213757
   Hayati S. A., 2019, P 10 WORKSH COMP APP, P91
   Hernault H, 2008, LECT NOTES COMPUT SC, V5208, P139
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hongyu Jiang, 2020, 2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics), P505, DOI 10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00092
   Huang ML, 2020, ACM T INFORM SYST, V38, DOI 10.1145/3383123
   Jiang H., 2020, IEEE International Conference on Cyber Science and Technology Congress (CyberSciTech), P296
   Joongyum Kim, 2020, ACM Transactions on Social Computing, V3, DOI 10.1145/3373146
   Kaye LK, 2021, COMPUT HUM BEHAV, V116, DOI 10.1016/j.chb.2020.106648
   Kaye LK, 2016, COMPUT HUM BEHAV, V60, P463, DOI 10.1016/j.chb.2016.02.088
   Kottur S, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3728
   Li JW, 2016, Arxiv, DOI arXiv:1603.06155
   Li JW, 2016, Arxiv, DOI arXiv:1510.03055
   Li Jiwei, 2016, NAACL, P110
   Li W., 2018, 12 INT AAAI C WEB SO
   Liang Weibin, 2014, 2014 ASIA PACIFIC SI, P1
   Luan Y, 2017, Arxiv, DOI arXiv:1710.07388
   Lubis N, 2019, IEEE-ACM T AUDIO SPE, V27, P866, DOI 10.1109/TASLP.2019.2900910
   McCulloch G., 2018, P 1 INT WORKSHOP EMO
   Min WQ, 2020, IEEE T MULTIMEDIA, V22, P2659, DOI 10.1109/TMM.2019.2958761
   Och FJ, 2003, 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P160
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Peng H, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1063, DOI 10.1145/3178876.3186005
   Qian Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4279
   Qin Penggang, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P7185, DOI 10.1145/3503161.3551604
   Ruder S, 2017, Arxiv, DOI [arXiv:1706.05098, 10.48550/arXiv.1706.05098]
   Seargeant P., 2019, EMOJI REVOLUTION TEC
   Sennrich Rico, 2015, COMPUTER ENCE, V2015
   Serban Iulian, 2018, arXiv
   Shiha Mohammed O., 2017, International Journal of Computer and Electrical Engineering, V9, P360, DOI 10.17706/ijcee.2017.9.1.360-369
   Shum HY, 2018, FRONT INFORM TECH EL, V19, P10, DOI 10.1631/FITEE.1700826
   Song HY, 2019, Arxiv, DOI arXiv:1905.12188
   Sordoni A, 2015, Arxiv, DOI arXiv:1506.06714
   Sutskever I, 2014, ADV NEUR IN, V27
   Walker MA, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P1373
   Wall HJ, 2016, COMPUT HUM BEHAV, V62, P70, DOI 10.1016/j.chb.2016.03.040
   Wang Jianan, 2017, P 9 SIGHAN WORKSHOP, P1
   Weidong He, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P2755, DOI 10.1145/3394171.3413679
   Wen HY, 2018, Arxiv, DOI arXiv:1806.04441
   Wu CH, 2018, PROCEEDINGS OF THE 2018 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2018 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC'18 ADJUNCT), P1337, DOI 10.1145/3267305.3274181
   Xu Y, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376416
   Yan XC, 2016, Arxiv, DOI arXiv:1512.00570
   Zhang SZ, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2204
   Zhang Y, 2022, IEEE T KNOWL DATA EN, V34, P5586, DOI 10.1109/TKDE.2021.3070203
   Zhao TC, 2017, Arxiv, DOI arXiv:1703.10960
   Zheng YH, 2020, Arxiv, DOI arXiv:1901.09672
   Zhong SH, 2020, IEEE-ACM T AUDIO SPE, V28, P2211, DOI 10.1109/TASLP.2020.3003864
   Zhou L., 2019, The design and implementation of xiaoice, an empathetic social chatbot, DOI 10.1162/coli_a_00368
   Zhou WJ, 2019, Arxiv, DOI arXiv:1908.11813
NR 64
TC 2
Z9 2
U1 6
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 132
DI 10.1145/3571819
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700007
DA 2024-07-18
ER

PT J
AU Wang, L
   Li, K
   Tang, JJ
   Liang, YY
AF Wang, Li
   Li, Ke
   Tang, Jingjing
   Liang, Yuying
TI Image Super-Resolution via Lightweight Attention-Directed Feature
   Aggregation Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Super-resolution; lightweight; attention mechanism; asymmetric
   convolution; spatial information
ID ACCURATE
AB The advent of convolutional neural networks (CNNs) has brought substantial progress in image superresolution (SR) reconstruction. However, most SR methods pursue deep architectures to boost performance, and the resulting large model sizes are impractical for real-world applications. Furthermore, they insufficiently explore the internal structural information of image features, disadvantaging the restoration of fine texture details. To solve these challenges, we propose a lightweight architecture based on a CNN named attention-directed feature aggregation network (AFAN), consisting of chained stacking multi-aware attention modules (MAAMs) and a simple channel attention module (SCAM), for image SR. Specifically, in each MAAM, we construct a space-aware attention block (SAAB) and a dimension-aware attention block (DAAB) that individually yield unique three-dimensional modulation coefficients to adaptively recalibrate structural information from an asymmetric convolution residual block (ACRB). The synergistic strategy captures multiple content features that are both space-aware and dimension-aware to preserve more fine-grained details. In addition, to further enhance the accuracy and robustness of the network, SCAM is embedded in the last MAAM to highlight channels with high activated values at low computational load. Comprehensive experiments verify that our proposed network attains high qualitative accuracy while employing fewer parameters and moderate computational requirements, exceeding most state-of-the-art lightweight approaches.
C1 [Wang, Li; Tang, Jingjing] Hohai Univ, Nanjing 211100, Jiangsu, Peoples R China.
   [Li, Ke; Liang, Yuying] Nanchang Inst Technol, Qinshan Lake Hero Ave, Nanchang, Jiangxi, Peoples R China.
C3 Hohai University; Nanchang Institute Technology
RP Liang, YY (corresponding author), Nanchang Inst Technol, Qinshan Lake Hero Ave, Nanchang, Jiangxi, Peoples R China.
EM wang_and_li@hhu.edu.cn; icehhu1019@163.com
RI Li, Ke/HZM-6170-2023
OI Li, Ke/0000-0002-7873-1554; Wang, Li/0000-0003-2054-1392
FU National Natural Science Foundation of China [51979085]; Science and
   Technology Project of Jiangxi Provincial Education Department
   [GJJ212101, GJJ219310]; Nanchang Key Laboratory Construction Project
   [2020-NCZDSY-005]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 51979085), Science and Technology Project of Jiangxi
   Provincial Education Department (Grants No. GJJ212101 and No.
   GJJ219310), and Nanchang Key Laboratory Construction Project (No.
   2020-NCZDSY-005).
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Banerjee S, 2020, Arxiv, DOI [arXiv:2008.01116, 10.48550/arXiv.2008.01116, DOI 10.48550/ARXIV.2008.01116]
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen XL, 2020, LECT NOTES ARTIF INT, V12595, P233, DOI 10.1007/978-3-030-66645-3_20
   Choi JS, 2017, IEEE COMPUT SOC CONF, P1150, DOI 10.1109/CVPRW.2017.153
   Chu XX, 2021, INT C PATT RECOG, P59, DOI 10.1109/ICPR48806.2021.9413080
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gao G., 2021, arXiv
   Gunturk BK, 2003, IEEE T IMAGE PROCESS, V12, P597, DOI 10.1109/TIP.2003.811513
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   He XY, 2019, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2019.00183
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu YT, 2020, IEEE T CIRC SYST VID, V30, P3911, DOI 10.1109/TCSVT.2019.2915238
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Islam MJ, 2020, IEEE INT CONF ROBOT, P900, DOI [10.1109/ICRA40945.2020.9197213, 10.1109/icra40945.2020.9197213]
   Jiang ZQ, 2020, IEEE T BROADCAST, V66, P814, DOI 10.1109/TBC.2020.2977513
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim JH, 2020, Arxiv, DOI arXiv:1811.12043
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P1443, DOI 10.1109/TCYB.2020.2970104
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li B, 2020, IEEE T IMAGE PROCESS, V29, P8368, DOI 10.1109/TIP.2020.3014953
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Li ZZ, 2019, Arxiv, DOI arXiv:1907.05282
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu H, 2020, KNOWL-BASED SYST, V203, DOI 10.1016/j.knosys.2020.106103
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu Y., 2021, IEEE T CIRC SYST VID, P1
   Lo S. Y., 2019, P ACM MULTIMEDIA ASI, P1, DOI DOI 10.1145/3338533.3366558
   Lu T, 2021, IEEE SIGNAL PROC LET, V28, P1305, DOI 10.1109/LSP.2021.3084522
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Ouyang DQ, 2019, PATTERN RECOGN LETT, V117, P153, DOI 10.1016/j.patrec.2018.05.009
   Park Karam, 2021, IEEE T MULTIMEDIA, V2021, P1
   Rasti P, 2016, LECT NOTES COMPUT SC, V9756, P175, DOI 10.1007/978-3-319-41778-3_18
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi WZ, 2013, LECT NOTES COMPUT SC, V8151, P9, DOI 10.1007/978-3-642-40760-4_2
   Sun L, 2021, IEEE-CAA J AUTOMATIC, V8, P1271, DOI 10.1109/JAS.2021.1004009
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian CW, 2022, IEEE T SYST MAN CY-S, V52, P3718, DOI 10.1109/TSMC.2021.3069265
   Tian CW, 2020, KNOWL-BASED SYST, V205, DOI 10.1016/j.knosys.2020.106235
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wan J, 2021, IEEE T BROADCAST, V67, P372, DOI 10.1109/TBC.2020.3028356
   Wang CF, 2019, Arxiv, DOI arXiv:1904.02358
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wei DY, 2022, DIGIT SIGNAL PROCESS, V120, DOI 10.1016/j.dsp.2021.103254
   Wei Z, 2021, ARXIV
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiangxiang Chu, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P99, DOI 10.1007/978-3-030-66823-5_6
   Xuehui Wang, 2021, Computer Vision - ACCV 2020. 15th Asian Conference on Computer Vision. Lecture Notes in Computer Science (LNCS 12623), P268, DOI 10.1007/978-3-030-69532-3_17
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang DY, 2021, IEEE T GEOSCI REMOTE, V59, P5183, DOI 10.1109/TGRS.2020.3009918
   Zhang HR, 2021, IEEE J-STSP, V15, P253, DOI 10.1109/JSTSP.2020.3045282
   Zhang K, 2019, PROC CVPR IEEE, P1671, DOI 10.1109/CVPR.2019.00177
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang Y, 2022, SIGNAL IMAGE VIDEO P, V16, P155, DOI 10.1007/s11760-021-01969-4
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao H, 2020, COMPUTER VISION ECCV, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhu FY, 2019, IEEE INT CONF COMP V, P2453, DOI 10.1109/ICCVW.2019.00300
NR 68
TC 6
Z9 6
U1 4
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 60
DI 10.1145/3546076
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000010
DA 2024-07-18
ER

PT J
AU Liu, YJ
   Xiong, ZW
   Li, Y
   Lu, YN
   Tian, XM
   Zha, ZJ
AF Liu, Yajing
   Xiong, Zhiwei
   Li, Ya
   Lu, Yuning
   Tian, Xinmei
   Zha, Zheng-Jun
TI Category-Stitch Learning for Union Domain Generalization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Domain generalization; variational autoencoder; generator
AB Domain generalization aims at generalizing the network trained on multiple domains to unknown but related domains. Under the assumption that different domains share the same classes, previous works can build relationships across domains. However, in realistic scenarios, the change of domains is always followed by the change of categories, which raises a difficulty for collecting sufficient aligned categories across domains. Bearing this in mind, this article introduces union domain generalization (UDG) as a new domain generalization scenario, in which the label space varies across domains, and the categories in unknown domains belong to the union of all given domain categories. The absence of categories in given domains is the main obstacle to aligning different domain distributions and obtaining domain-invariant information. To address this problem, we propose category-stitch learning (CSL), which aims at jointly learning the domain-invariant information and completing missing categories in all domains through an improved variational autoencoder and generators. The domain-invariant information extraction and sample generation cross-promote each other to better generalizability. Additionally, we decouple category and domain information and propose explicitly regularizing the semantic information by the classification loss with transferred samples. Thus our method can breakthrough the category limit and generate samples of missing categories in each domain. Extensive experiments and visualizations are conducted on MNIST, VLCS, PACS, Office-Home, and DomainNet datasets to demonstrate the effectiveness of our proposed method.
C1 [Liu, Yajing; Xiong, Zhiwei; Lu, Yuning; Tian, Xinmei; Zha, Zheng-Jun] Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230027, Peoples R China.
   [Li, Ya] iFLYTEK Res, 666 Wangjiang West Rd, Hefei 230088, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xiong, ZW (corresponding author), Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230027, Peoples R China.
EM lyj123@mail.ustc.edu.cn; zwxiong@mail.ustc.edu.cn; yali8@iflytek.com;
   lyn0@mail.ustc.edu.cn; xinmei@mail.ustc.edu.cn; zhazj@ustc.edu.cn
RI Liu, Jinyu/JYQ-6274-2024; FENG, X/JPL-4188-2023; jing,
   wang/KCZ-2144-2024; Wang, Chao/JHT-6081-2023; wu,
   xiaokang/JUJ-4602-2023; Yang, Tian/JFB-1008-2023; zhao,
   lin/JPK-8436-2023; Wang, Minghao/JMD-0670-2023; SUN,
   YANLING/JTT-9082-2023; Jiang, Yu/JEZ-9814-2023; wang,
   wenjuan/JGD-0428-2023; cheng, chen/JHS-9462-2023; Yang,
   Yifan/JTV-1487-2023; WANG, YANG/JFA-8821-2023; LIU, HUI/JPX-8014-2023;
   huang, libo/JMB-4345-2023; Zha, Zheng-Jun/AAF-8667-2020
OI wang, wenjuan/0000-0002-4220-8817; 
FU National Key R&D Program of China [2017YFA0700800]; National Natural
   Science Foundation of China [62131003, 62021001]
FX We acknowledge funding from National Key R&D Program of China under
   Grant 2017YFA0700800, and National Natural Science Foundation of China
   under Grants 62131003 and 62021001.
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 1997, Information theory and statistics
   Balaji Y, 2018, ADV NEUR IN, V31
   Baldi P., 2012, P ICML WORKSH UNS TR, P37
   Ben-David S., 2007, NIPS, P137
   Cao ZJ, 2018, LECT NOTES COMPUT SC, V11212, P139, DOI 10.1007/978-3-030-01237-3_9
   Cao ZJ, 2019, PROC CVPR IEEE, P2980, DOI 10.1109/CVPR.2019.00310
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Choi MJ, 2010, PROC CVPR IEEE, P129, DOI 10.1109/CVPR.2010.5540221
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Doersch C, 2021, Arxiv, DOI arXiv:1606.05908
   French G., 2018, P INT C LEARNING REP
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ghifary M, 2017, IEEE T PATTERN ANAL, V39, P1414, DOI 10.1109/TPAMI.2016.2599532
   Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293
   Griffin G., 2007, CALTECH 256 OBJECT C
   Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12
   Klys J, 2018, ADV NEURAL INFORM PR, P6444
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Li YJ, 2019, PR MACH LEARN RES, V97
   Liu YJ, 2023, IEEE T MULTIMEDIA, V25, P126, DOI 10.1109/TMM.2021.3121564
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Mitsuzumi Y, 2021, PROC CVPR IEEE, P1084, DOI 10.1109/CVPR46437.2021.00114
   Muandet Krikamol, 2013, INT C MACH LEARN, P10
   Kieu M, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3418213
   Odena A, 2017, PR MACH LEARN RES, V70
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234
   Parascandolo G., 2020, LEARNING EXPLANATION
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sariyildiz MB, 2019, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2019.00227
   Shankar Shiv, 2018, P INT C LEARNING REP
   Sohn K, 2015, ADV NEUR IN, V28
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Tian Yingtao, 2018, P ICLR 2019 C BLIND
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang YX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1267, DOI 10.1145/3343031.3351004
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   Xu Z, 2014, LECT NOTES COMPUT SC, V8691, P628, DOI 10.1007/978-3-319-10578-9_41
   Yao Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1578, DOI 10.1145/3343031.3350955
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13864, DOI 10.1109/CVPR42600.2020.01388
   Zareapoor M, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3458280
   Zhao S., 2020, Advances in Neural Information Processing Systems, V33
NR 51
TC 1
Z9 1
U1 3
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 25
DI 10.1145/3524136
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400025
DA 2024-07-18
ER

PT J
AU Zhang, PY
   Dou, HZ
   Zhang, WH
   Zhao, YH
   Qin, ZQ
   Hu, DP
   Fang, Y
   Li, X
AF Zhang, Pengyi
   Dou, Huanzhang
   Zhang, Wenhu
   Zhao, Yuhan
   Qin, Zequn
   Hu, Dongping
   Fang, Yi
   Li, Xi
TI A Large-Scale Synthetic Gait Dataset Towards in-the-Wild Simulation and
   Comparison Study
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Gait recognition; synthetic dataset; in the wild scenarios; fine-grained
   attributes; Unity3D
ID RECOGNITION; PERFORMANCE
AB Gait recognition has a rapid development in recent years. However, current gait recognition focuses primarily on ideal laboratory scenes, leaving the gait in the wild unexplored. One of the main reasons is the difficulty of collecting in-the-wild gait datasets, which must ensure diversity of both intrinsic and extrinsic human gait factors. To remedy this problem, we propose to construct a large-scale gait dataset with the help of controllable computer simulation. In detail, to diversify the intrinsic factors of gait, we generate numerous characters with diverse attributes and associate them with various types of walking styles. To diversify the extrinsic factors of gait, we build a complicated scene with a dense camera layout. Then we design an automatic generation toolkit under Unity3D for simulating the walking scenarios and capturing the gait data. As a result, we obtain a dataset simulating towards the in-the-wild scenario, called VersatileGait, which has more than one million silhouette sequences of 10,000 subjects with diverse scenarios. VersatileGait possesses several nice properties, including huge dataset size, diverse pedestrian attributes, complicated camera layout, high-quality annotations, small domain gap with the real one, good scalability for new demands, and no privacy issues. By conducting a series of experiments, we first explore the effects of different factors on gait recognition. We further illustrate the effectiveness of using our dataset to pre-train models, which obtain considerable performance gain on CASIA-B, OU-MVLP, and CASIA-E. Besides, we show the great potential of the fine-grained labels other than the ID label in improving the efficiency and effectiveness of models. Our dataset and its corresponding generation toolkit are available at https://github.com/peterzpy/VersatileGait.
C1 [Zhang, Pengyi; Dou, Huanzhang; Zhang, Wenhu; Zhao, Yuhan; Qin, Zequn; Li, Xi] Zhejiang Univ, Coll Comp Sci, Rd 38 West Lake Dist, Hangzhou 310007, Zhejiang, Peoples R China.
   [Hu, Dongping; Fang, Yi] Merit Interact Co Ltd, 1500 Cangqian St Yuhang Dist, Hangzhou 310012, Zhejiang, Peoples R China.
   [Li, Xi] Zhejiang Univ, Shanghai Inst Adv Study, Shanghai Al Lab, Shanghai, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Qin, ZQ; Li, X (corresponding author), Zhejiang Univ, Coll Comp Sci, Rd 38 West Lake Dist, Hangzhou 310007, Zhejiang, Peoples R China.; Li, X (corresponding author), Zhejiang Univ, Shanghai Inst Adv Study, Shanghai Al Lab, Shanghai, Peoples R China.
EM pyzhang@zju.edu.cn; hzdou@zju.edu.cn; wen-huzhang@zju.edu.cn;
   yuhanzhao@zju.edu.cn; zequnqin@zju.edu.cn; hudp@getui.com;
   fangyi@getui.com; xilizju@zju.edu.cn
RI Qin, Zequn/HGU-7660-2022
OI Hu, Dongping/0000-0003-0159-4288; Dou, Huanzhang/0000-0002-3277-5906;
   Zhang, Pengyi/0000-0003-2637-7511; Zhao, Yuhan/0000-0002-5431-8247
FU Zhejiang Provincial Natural Science Foundation of China [LR19F020004];
   National Key Research and Development Program of China [2020AAA0107400];
   National Natural Science Foundation of China [U20A20222]; Zhejiang
   University K.P.Chao's High Technology Development Foundation
FX This work is supported in part by Zhejiang Provincial Natural Science
   Foundation of China under Grant LR19F020004, National Key Research and
   Development Program of China under Grant 2020AAA0107400, National
   Natural Science Foundation of China under Grant U20A20222, and Zhejiang
   University K.P.Chao's High Technology Development Foundation.
CR [Anonymous], 2021, Unity3d
   [Anonymous], 2023, ACM T MULTIM COMPUT, V19
   [Anonymous], 2021, CASIA E
   Ariyanto G., 2011, 2011 INT JOINT C BIO, P1, DOI DOI 10.1109/IJCB.2011.6117582
   Azzakhnini S, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152125
   Bak S, 2018, LECT NOTES COMPUT SC, V11217, P193, DOI 10.1007/978-3-030-01261-8_12
   Balazia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152124
   Bashir Khalid, 2009, P ICDP, P1
   Bastioni M., 2008, P BANGALORE COMPUTE, P1
   Bodor R, 2009, IMAGE VISION COMPUT, V27, P1194, DOI 10.1016/j.imavis.2008.11.008
   Bouchrika Imed, 2018, SURVEILLANCE ACTION, P1, DOI DOI 10.1007/978-3-319-68533-5_1
   Boulgouris Nikolaos V., 2007, P IEEE INT C IMAGE P, pI
   Chao Fan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14213, DOI 10.1109/CVPR42600.2020.01423
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   Chattopadhyay P, 2014, IEEE T INF FOREN SEC, V9, P1843, DOI 10.1109/TIFS.2014.2352114
   Connor P, 2018, COMPUT VIS IMAGE UND, V167, P1, DOI 10.1016/j.cviu.2018.01.007
   DeCann Brian, 2013, P SPIE BIOMETRIC SUR, P101
   Fabbri M, 2018, LECT NOTES COMPUT SC, V11208, P450, DOI 10.1007/978-3-030-01225-0_27
   Gafurov D., 2007, ANN NORW COMP SCI CO, P19
   Goffredo M, 2010, IEEE T SYST MAN CY B, V40, P997, DOI 10.1109/TSMCB.2009.2031091
   Gross J., 2001, Tech. Rep. CMU-RI-TR-01-18, V45, P1
   Guan'an Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P275, DOI 10.1007/978-3-030-58598-3_17
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Hofmann M, 2014, J VIS COMMUN IMAGE R, V25, P195, DOI 10.1016/j.jvcir.2013.02.006
   Hofmann M, 2011, WSCG 2011: COMMUNICATION PAPERS PROCEEDINGS, P99
   Hu YT, 2019, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR.2019.00322
   Huang XH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12889, DOI 10.1109/ICCV48922.2021.01267
   Huang Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14900, DOI 10.1109/ICCV48922.2021.01465
   Iwama H, 2012, IEEE T INF FOREN SEC, V7, P1511, DOI 10.1109/TIFS.2012.2204253
   Jia Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P519, DOI 10.1007/978-3-030-58545-7_30
   Jun K, 2020, IEEE ACCESS, V8, P19196, DOI 10.1109/ACCESS.2020.2967845
   Kusakunniran W, 2014, IEEE T IMAGE PROCESS, V23, P696, DOI 10.1109/TIP.2013.2294552
   Kusakunniran W, 2013, IEEE T INF FOREN SEC, V8, P1642, DOI 10.1109/TIFS.2013.2252342
   Liao RJ, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107069
   Lin BB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3054, DOI 10.1145/3394171.3413861
   Lin BB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14628, DOI 10.1109/ICCV48922.2021.01438
   Lin YS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3469288
   Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261
   Makihara Y, 2006, LECT NOTES COMPUT SC, V3953, P151, DOI 10.1007/11744078_12
   Makihara Y, 2017, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR.2017.718
   Mixamo, 2021, US
   Prakash Aayush, 2021, P IEEE CVF INT C COM, P16044
   Pushparani M., 2012, GLOB J COMPUT SCI TE, V12, P975
   Qi L, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419439
   Rida I, 2019, IET BIOMETRICS, V8, P14, DOI 10.1049/iet-bmt.2018.5063
   Ruder S, 2017, Arxiv, DOI [arXiv:1706.05098, 10.48550/arXiv.1706.05098]
   Saihui Hou, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P384, DOI 10.1109/TBIOM.2021.3074963
   Saihui Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P382, DOI 10.1007/978-3-030-58545-7_22
   Samangooei S, 2010, MULTIMED TOOLS APPL, V49, P195, DOI 10.1007/s11042-009-0391-8
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR.2018.00395
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Sepas-Moghaddam A, 2023, IEEE T PATTERN ANAL, V45, P264, DOI 10.1109/TPAMI.2022.3151865
   Shen L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356316
   Singh Jasvinder Pal, 2019, 2019 INT C ISSUES CH, P1
   Su Ge, 2021, AUTOPHAGY, V17, P1
   Su H, 2017, IEEE WINT CONF APPL, P530, DOI 10.1109/WACV.2017.65
   Sugandhi K, 2017, COMM COM INF SC, V721, P377, DOI 10.1007/978-981-10-5427-3_40
   Takemura Noriko, 2018, IPSJ Transactions on Computer Vision and Applications, V10, DOI 10.1186/s41074-018-0039-6
   Tan DL, 2006, INT C PATT RECOG, P1000
   Tremblay J, 2018, IEEE COMPUT SOC CONF, P1082, DOI 10.1109/CVPRW.2018.00143
   van der Ploeg T, 2014, BMC MED RES METHODOL, V14, DOI 10.1186/1471-2288-14-137
   Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492
   Wan CS, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3230633
   Wang L, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P433, DOI 10.1109/ICIP.2002.1038998
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang XP, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355397
   Wang YN, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3422, DOI 10.1145/3394171.3413815
   Xiang Li, 2021, Computer Vision - ACCV 2020. 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12624), P3, DOI 10.1007/978-3-030-69535-4_1
   Xiang Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13306, DOI 10.1109/CVPR42600.2020.01332
   Xiang SC, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102822
   Xu C., 2017, IPSJ TCVA, V9, P24, DOI DOI 10.1186/S41074-017-0035-2
   Xu C, 2021, IEEE T CIRC SYST VID, V31, P260, DOI 10.1109/TCSVT.2020.2975671
   Yu SQ, 2006, INT C PATT RECOG, P441
   Yu T, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243217
   Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223
   Zhang ZY, 2019, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR.2019.00484
   Zhao AT, 2022, IEEE T CYBERNETICS, V52, P9439, DOI 10.1109/TCYB.2021.3056104
   Zhao GY, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P529
   Zhao S., 2019, P ADV NEUR INF PROC, P7285
NR 79
TC 2
Z9 2
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 17
DI 10.1145/3517199
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400017
DA 2024-07-18
ER

PT J
AU Saxena, N
   Raman, B
AF Saxena, Nidhi
   Raman, Balasubramanian
TI Pansharpening Scheme Using Bi-dimensional Empirical Mode Decomposition
   and Neural Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Bi-dimensional empirical mode decomposition; convolutional neural
   network; multi-spectral images; pansharpening scheme
ID PAN-SHARPENING METHOD; DATA-FUSION; SPATIAL-RESOLUTION; IMAGES;
   MULTIRESOLUTION; ALGORITHM; QUALITY
AB The pansharpening is a combination of multispectral (MS) and panchromatic (PAN) images that produce a high-spatial-spectral-resolution MS images. In multiresolution analysis-based pansharpening schemes, some spatial and spectral distortions are found. It can be reduced by adding spatial detail images of the PAN image into MS images. In the convolution neural network- (CNN) based method, the lowpass filter image extracted by the CNN model when MS and PAN images are directly applied into the input. The feature values are very high and reduce the conversion efficiency. In the proposed scheme, bi-dimensional empirical mode decomposition is used to extract the spatial detail information of the PAN image to reduce the feature values of the input. This extracted PAN image information is applied to the CNN to produce the non-linear changes in the image pixels and transformed into the perfect spatial detail image. It identifies the spatial and spectral detail quantity for the proposed scheme and it also varies with the different datasets automatically of the same satellite images. Simulation results in the context of qualitative and quantitative analysis demonstrate the effectiveness of proposed scheme applied on datasets collected by different satellites.
C1 [Saxena, Nidhi] Vellore Inst Technol, Technol Informat Forecasting & Assessment Council, Vellore, TN, India.
   [Raman, Balasubramanian] Indian Inst Technol, Dept Comp Sci & Engn, Roorkee, Uttarakhand, India.
C3 Department of Science & Technology (India); Technology Information,
   Forecasting & Assessment Council (TIFAC); Vellore Institute of
   Technology (VIT); VIT Vellore; Indian Institute of Technology System
   (IIT System); Indian Institute of Technology (IIT) - Roorkee
RP Saxena, N (corresponding author), Vellore Inst Technol, Technol Informat Forecasting & Assessment Council, Vellore, TN, India.
EM nidhi.saxena@vit.ac.in; balarfcs@iitr.ac.in
RI Saxena, Nidhi/HHC-1974-2022
OI Saxena, Nidhi/0000-0003-2598-2314
CR Aiazzi B, 2006, PHOTOGRAMM ENG REM S, V72, P591, DOI 10.14358/PERS.72.5.591
   Aiazzi B, 2003, 2ND GRSS/ISPRS JOINT WORKSHOP ON REMOTE SENSING AND DATA FUSION OVER URBAN AREAS, P90, DOI 10.1109/DFUA.2003.1219964
   Aiazzi B, 2013, ISPRS J PHOTOGRAMM, V86, P65, DOI 10.1016/j.isprsjprs.2013.09.007
   Alparone L, 2008, PHOTOGRAMM ENG REM S, V74, P193, DOI 10.14358/PERS.74.2.193
   Alparone L, 2007, IEEE T GEOSCI REMOTE, V45, P3012, DOI 10.1109/TGRS.2007.904923
   Azarang A, 2019, IEEE ACCESS, V7, P35673, DOI 10.1109/ACCESS.2019.2905511
   CHAVEZ PS, 1991, PHOTOGRAMM ENG REM S, V57, P295
   Chunwei Tian, 2019, Genetic and Evolutionary Computing. Proceedings of the Twelfth International Conference on Genetic and Evolutionary Computing. Advances in Intelligent Systems and Computing (AISC 834), P563, DOI 10.1007/978-981-13-5841-8_59
   Czaja W, 2014, PROC SPIE, V9088, DOI 10.1117/12.2050405
   Dong WH, 2014, REMOTE SENS-BASEL, V6, P8446, DOI 10.3390/rs6098446
   Flandrin P, 2004, IEEE SIGNAL PROC LET, V11, P112, DOI 10.1109/LSP.2003.821662
   GILLESPIE AR, 1987, REMOTE SENS ENVIRON, V22, P343, DOI 10.1016/0034-4257(87)90088-5
   González-Audícana M, 2004, IEEE T GEOSCI REMOTE, V42, P1291, DOI 10.1109/TGRS.2004.825593
   He L, 2019, IEEE J-STARS, V12, P1188, DOI 10.1109/JSTARS.2019.2898574
   Helmy AK, 2015, EGYPT INFORM J, V16, P121, DOI 10.1016/j.eij.2015.02.003
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Joshi M, 2010, IEEE T GEOSCI REMOTE, V48, P1245, DOI 10.1109/TGRS.2009.2030323
   Kang XD, 2014, IEEE T GEOSCI REMOTE, V52, P5088, DOI 10.1109/TGRS.2013.2286827
   Kasem HM, 2018, INT C PATT RECOG, P2688, DOI 10.1109/ICPR.2018.8546080
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lee J, 2010, IEEE T GEOSCI REMOTE, V48, P155, DOI 10.1109/TGRS.2009.2028613
   Li ST, 2013, IEEE T GEOSCI REMOTE, V51, P4779, DOI 10.1109/TGRS.2012.2230332
   Li ST, 2011, IEEE T GEOSCI REMOTE, V49, P738, DOI 10.1109/TGRS.2010.2067219
   Li YP, 2020, IEEE ACCESS, V8, P60019, DOI 10.1109/ACCESS.2020.2981814
   Masi G, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8070594
   Mura MD, 2015, P IEEE, V103, P1585, DOI 10.1109/JPROC.2015.2462751
   Nunes JC, 2003, IMAGE VISION COMPUT, V21, P1019, DOI 10.1016/S0262-8856(03)00094-5
   Otazu X, 2005, IEEE T GEOSCI REMOTE, V43, P2376, DOI 10.1109/TGRS.2005.856106
   Restaino R, 2016, IEEE T IMAGE PROCESS, V25, P2882, DOI 10.1109/TIP.2016.2556944
   Saxena N, 2021, INT J REMOTE SENS, V42, P2898, DOI 10.1080/01431161.2020.1864056
   Saxena N, 2019, INT J REMOTE SENS, V40, P6098, DOI 10.1080/01431161.2019.1587203
   Saxena N, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON POWER, CONTROL, SIGNALS AND INSTRUMENTATION ENGINEERING (ICPCSI), P2801, DOI 10.1109/ICPCSI.2017.8392230
   Saxena N, 2018, IET IMAGE PROCESS, V12, P1013, DOI 10.1049/iet-ipr.2017.0961
   Saxena N, 2017, IET IMAGE PROCESS, V11, P1152, DOI 10.1049/iet-ipr.2017.0133
   Saxena Nidhi, 2021, IET IMAGE PROCESS
   Scarpa G, 2018, IEEE T GEOSCI REMOTE, V56, P5443, DOI 10.1109/TGRS.2018.2817393
   Shah VP, 2008, IEEE T GEOSCI REMOTE, V46, P1323, DOI 10.1109/TGRS.2008.916211
   Song HH, 2015, IEEE T GEOSCI REMOTE, V53, P1195, DOI 10.1109/TGRS.2014.2335818
   Stroppiana D, 2003, IEEE T GEOSCI REMOTE, V41, P907, DOI 10.1109/TGRS.2003.808898
   Sun ZL, 2005, IEEE GEOSCI REMOTE S, V2, P108, DOI 10.1109/LGRS.2005.844169
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Thomas C, 2008, IEEE T GEOSCI REMOTE, V46, P1301, DOI 10.1109/TGRS.2007.912448
   Tu TM, 2004, IEEE GEOSCI REMOTE S, V1, P309, DOI 10.1109/LGRS.2004.834804
   Vitale S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12030348
   Vivone G, 2018, IEEE T IMAGE PROCESS, V27, P3418, DOI 10.1109/TIP.2018.2819501
   Vivone G, 2015, IEEE T GEOSCI REMOTE, V53, P2565, DOI 10.1109/TGRS.2014.2361734
   Vivone G, 2014, IEEE GEOSCI REMOTE S, V11, P930, DOI 10.1109/LGRS.2013.2281996
   Wald L., 2002, Data Fusion: Definitions and Architectures: Fusion of Images of Different Spatial Resolutions
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wei YC, 2017, IEEE GEOSCI REMOTE S, V14, P1795, DOI 10.1109/LGRS.2017.2736020
   Xie Junyuan, 2012, ADV NEURAL INFORM PR, P341, DOI [DOI 10.5555/2999134.2999173, DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605]
   Yuhas R.H., 1992, Discrimination Among Semi-arid Landscape Endmembers Using the Spectral Angle Mapper (SAM) Algorithm
   Zhang LP, 2012, IEEE T SYST MAN CY B, V42, P1693, DOI 10.1109/TSMCB.2012.2198810
   Zhou WQ, 2013, IEEE GEOSCI REMOTE S, V10, P928, DOI 10.1109/LGRS.2013.2251453
   Zhu XX, 2013, IEEE T GEOSCI REMOTE, V51, P2827, DOI 10.1109/TGRS.2012.2213604
   Zhuang HF, 2016, IEEE GEOSCI REMOTE S, V13, P681, DOI 10.1109/LGRS.2016.2536058
NR 56
TC 1
Z9 1
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 108
DI 10.1145/3506709
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600019
DA 2024-07-18
ER

PT J
AU Zhang, Z
   Wang, JN
   Zhu, L
   Lu, GM
AF Zhang, Zheng
   Wang, Jianning
   Zhu, Lei
   Lu, Guangming
TI Discriminative Visual Similarity Search with Semantically
   Cycle-consistent Hashing Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Semantic-preserving hashing; cycle-consistent hashing; deep hashing
   networks; graph hashing; image retrieval
AB Deep hashing has great potential in large-scale visual similarity search due to its preferable efficiency in storage and computation. Technically, deep hashing for visual similarity search inherits the powerful representation capability of deep neural networks, and it encodes visual features into compact binary codes by preserving representative semantic visual features. Works in this field mainly focus on building the relationship between the visual and objective hash spaces, while they seldom study the triadic cross-domain semantic knowledge transfer among visual, semantic, and hashing spaces, leading to a serious semantic ignorance problem during space transformation. In this article, we propose a novel deep tripartite semantically interactive hashing framework, dubbed Semantically Cycle-consistent Hashing Networks (SCHNs), for discriminative hash code learning. Particularly, we construct a flexible semantic space and a transitive latent space, in conjunction with the visual space, to jointly deduce the privileged discriminative hash space. Specifically, a new semantic space is conceived to strengthen the flexibility and completeness of categories in the semantic feature inference phase. At the same time, a transitive latent space is formulated to explore and uncover the shared semantic interactivity embedded in visual and semantic features. Moreover, to further ensure semantic consistency across multiple spaces, we propose to build a cyclic adversarial learning module to preserve and keep their semantic concurrence during space transformation. Notably, our SCHN, for the first time, establishes the cyclic principle of deep semantic-preserving hashing by adaptive semantic parsing across different spaces in a single-modal visual similarity search. In addition, the entire learning framework is jointly optimized in an end-to-end manner. Extensive experiments performed on diverse large-scale datasets evidence the superiority of our method against other state-of-the-art deep hashing algorithms. The source codes of this article are available at https://github.com/JalinWang/SCHN.
C1 [Zhang, Zheng; Wang, Jianning; Lu, Guangming] Harbin Inst Technol, Shenzhen, Peoples R China.
   [Zhu, Lei] Shandong Normal Univ, Jinan, Peoples R China.
C3 Harbin Institute of Technology; Shandong Normal University
RP Zhu, L (corresponding author), Shandong Normal Univ, Jinan, Peoples R China.
EM darrenzz219@gmail.com; hitsz.w@outlook.com; leizhu0608@gmail.com;
   luguangm@hit.edu.cn
RI Zhu, Lei/GQQ-1130-2022; Zhang, Zheng/M-6325-2014; Zhang,
   Zhang/JAX-2097-2023; Ma, Zeyu/JGM-6358-2023
OI Zhu, Lei/0000-0002-5348-7532; Zhang, Zheng/0000-0003-1470-6998; wang,
   jianing/0000-0002-2191-5627; Zhu, Lei/0000-0002-2993-7142; Lu,
   Guang-Ming/0000-0002-6607-2264
FU National Natural Science Foundation of China [62002085, 62176077];
   Guangdong Basic and Applied Basic Research Foundation [2019A1515110475,
   2019Bl515120055]; Shenzhen Fundamental Research Fund
   [GXWD20201230155427003-20200824103320001, JCYJ20210 324132212030]
FX This work was partially supported by the National Natural Science
   Foundation of China (Grant No. 62002085 and 62176077), the Guangdong
   Basic and Applied Basic Research Foundation (Grant No. 2019A1515110475,
   2019Bl515120055), and the Shenzhen Fundamental Research Fund (Grant no.
   GXWD20201230155427003-20200824103320001, JCYJ20210 324132212030).
CR Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen YX, 2021, IEEE T CYBERNETICS, V51, P6240, DOI 10.1109/TCYB.2020.2964993
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Deng C, 2020, IEEE T NEUR NET LEAR, V31, P2189, DOI 10.1109/TNNLS.2019.2929068
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Gao LL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P723
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jain H, 2017, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2017.96
   Jiang QY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2248
   Kang WC, 2016, AAAI CONF ARTIF INTE, P1230
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li Q, 2020, INT J COMPUT VISION, V128, P2204, DOI 10.1007/s11263-020-01327-w
   Li WJ, 2016, IJCAI, P1711
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu AA, 2021, IEEE T MULTIMEDIA, V23, P4515, DOI 10.1109/TMM.2020.3043084
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu H, 2019, IEEE T PATTERN ANAL, V41, P941, DOI 10.1109/TPAMI.2018.2819978
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu Wei., 2011, P 28 INT C MACHINE L, P1
   Liu W, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3524497
   Liu XC, 2016, IEEE INT CON MULTI
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Pan YW, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P53, DOI 10.1145/2766462.2767725
   Paszke A, 2019, ADV NEUR IN, V32
   Peng YX, 2020, IEEE T MULTIMEDIA, V22, P2061, DOI 10.1109/TMM.2019.2951462
   Qiu ZF, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P225, DOI 10.1145/3077136.3080842
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Shen FM, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P595, DOI 10.1145/3077136.3080767
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shi WW, 2022, IEEE T NEUR NET LEAR, V33, P3713, DOI 10.1109/TNNLS.2021.3054386
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang ZJ, 2021, IEEE T MULTIMEDIA, V23, P1274, DOI 10.1109/TMM.2020.2995267
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Zhang HW, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P325, DOI 10.1145/2911451.2911502
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zhang Z, 2023, IEEE T KNOWL DATA EN, V35, P5091, DOI 10.1109/TKDE.2022.3144352
   Zhang Z, 2021, IEEE T NEUR NET LEAR, V32, P4514, DOI 10.1109/TNNLS.2020.3018790
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhang Z, 2019, IEEE T IMAGE PROCESS, V28, P4803, DOI 10.1109/TIP.2019.2912290
   Zhang Zheng, 2021, ACM MULTIMEDIA ASIA, P1
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 52
TC 3
Z9 3
U1 2
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 114
DI 10.1145/3532519
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000001
DA 2024-07-18
ER

PT J
AU Amirpour, H
   Pinheiro, A
   Pereira, M
   Lopes, FJP
   Ghanbari, M
AF Amirpour, Hadi
   Pinheiro, Antonio
   Pereira, Manuela
   Lopes, Fernando J. P.
   Ghanbari, Mohammad
TI Efficient Light Field Image Compression with Enhanced Random Access
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Light field; random access; image compression; HEVC; VVC
AB In light field image compression, facilitating random access to individual views plays a significant role in decoding views quickly, reducing memory footprint, and decreasing the bandwidth requirement for transmission. Highly efficient light field image compression methods mainly use inter view prediction. Therefore, they typically do not provide random access to individual views. On the other hand, methods that provide full random access usually reduce compression efficiency. To address this trade-off, a light field image encoding method that favors random access is proposed in this paper. Light field image views are grouped into independent 3 x 3 views, which are called Macro View Images (MVIs). To encode MVIs, the central view is used as a reference to compress its adjacent neighboring views using a hierarchical reference structure. To encode the central view of each MVI, the most central view along with the center of a maximum of three MVIs, are used as reference images for the disparity estimation. In addition, the proposed method allows the use of parallel processing to reduce the maximum encoding/decoding time-complexity in multi-core processors. Tile partitioning can also be used to randomly access different regions of the light field images. The simulation results show that the proposed method outperforms other state-of-the-art methods in terms of compression efficiency while providing random access to both views and regions of interest.
C1 [Amirpour, Hadi; Pinheiro, Antonio; Pereira, Manuela] Inst Telecomunicacoes, P-6201001 Covilha, Portugal.
   [Amirpour, Hadi; Pinheiro, Antonio; Pereira, Manuela] Univ Beira Interior, P-6201001 Covilha, Portugal.
   [Lopes, Fernando J. P.] Inst Telecomunicacoes, Rua Pedro Nunes, P-3030199 Coimbra, Portugal.
   [Lopes, Fernando J. P.] Polytech Inst Coimbra ISEC, Rua Pedro Nunes, P-3030199 Coimbra, Portugal.
   [Ghanbari, Mohammad] Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
C3 Universidade da Beira Interior; Institute of Telecommunications -
   Coimbra; Universidade de Coimbra; University of Essex
RP Amirpour, H (corresponding author), Inst Telecomunicacoes, P-6201001 Covilha, Portugal.; Amirpour, H (corresponding author), Univ Beira Interior, P-6201001 Covilha, Portugal.
EM hadi.amirpour@ieee.org; pinheiro@ubi.pt; mperieria@di.ubi.pt;
   flopes@isec.pt; ghan@essex.ac.uk
RI Pereira, Manuela/Q-3456-2019; Pinheiro, Antonio/B-2723-2012
OI Pereira, Manuela/0000-0002-8648-6464; Pinheiro,
   Antonio/0000-0002-5968-9901; Lopes, Fernando J. P./0000-0003-3641-0186
FU Portuguese FCT-Fundacao para a Ciencia e Tecnologia; FEDER-PT2020
   partnership agreement [PTDC/EEI-PRO/2849/2014
   -POCI-01-0145-FEDER-016693, UIDB/EEA/50008/2020, PLive X-0017-LX-20];
   Centro de Competencias em Cloud Computing [Centro-01-0145-FEDER-000019
   -C4]
FX Funded by the Portuguese FCT-Fundacao para a Ciencia e Tecnologia and
   co-funded by FEDER-PT2020 partnership agreement under the project
   PTDC/EEI-PRO/2849/2014 -POCI-01-0145-FEDER-016693, under the project
   UIDB/EEA/50008/2020, PLive X-0017-LX-20, and by operation
   Centro-01-0145-FEDER-000019 -C4 -Centro de Competencias em Cloud
   Computing.
CR Aggoun A, 2011, J DISP TECHNOL, V7, P586, DOI 10.1109/JDT.2011.2159359
   Ahmad W, 2017, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2017.8297145
   Ahmad W, 2018, IEEE DATA COMPR CONF, P396, DOI 10.1109/DCC.2018.00049
   Amirpour H, 2019, INT CONF ACOUST SPEE, P2402, DOI 10.1109/ICASSP.2019.8683215
   Amirpour H, 2019, IEEE DATA COMPR CONF, P553, DOI 10.1109/DCC.2019.00065
   Amirpour H, 2018, IEEE DATA COMPR CONF, P397, DOI 10.1109/DCC.2018.00050
   Amirpour Hadi, 2019, ABS190111396 CORR
   [Anonymous], 2009, INT C COMP PHOT
   [Anonymous], 2015, EPFL LIGHT FIELD IMA
   [Anonymous], 2015, PARAMETER VALUES HDT
   Astola P, 2018, EUR W VIS INF PROCES
   Avramelos V, 2020, MULTIMED TOOLS APPL, V79, P12847, DOI 10.1007/s11042-019-08605-x
   Bjontegaard G., 2001, Document VCEG-M33
   Cha Zhang, 2000, Proceedings DCC 2000. Data Compression Conference, P253, DOI 10.1109/DCC.2000.838165
   Chen B, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3366371
   Conti C, 2020, IEEE ACCESS, V8, P49244, DOI 10.1109/ACCESS.2020.2977767
   Conti C, 2018, IEEE T MULTIMEDIA, V20, P2905, DOI 10.1109/TMM.2018.2825882
   Dai F, 2015, IEEE IMAGE PROC, P4733, DOI 10.1109/ICIP.2015.7351705
   Dansereau DG, 2013, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2013.137
   de Carvalho MB, 2018, IEEE IMAGE PROC, P435, DOI 10.1109/ICIP.2018.8451684
   Dib E, 2019, IEEE IMAGE PROC, P3751, DOI [10.1109/icip.2019.8803756, 10.1109/ICIP.2019.8803756]
   Ebrahimi T, 2016, IEEE MULTIMEDIA, V23, P14, DOI 10.1109/MMUL.2016.64
   Elias V., 2018, J COMMUN INF SYST, V33, P92
   Georgiev T, 2013, PROC SPIE, V8667, DOI 10.1117/12.2013581
   Hou JH, 2019, IEEE T CIRC SYST VID, V29, P517, DOI 10.1109/TCSVT.2018.2802943
   Hu XJ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3395620
   Ihrke I, 2016, IEEE SIGNAL PROC MAG, V33, P59, DOI 10.1109/MSP.2016.2582220
   ISO/IEC JTC1/SC29/WG1 ISO/IEC JTC1/SC29/WG1, 2019, JTC1SC29WG1N83034 IS
   Jayaweera SS, 2021, IEEE SIGNAL PROC LET, V28, P31, DOI 10.1109/LSP.2020.3043990
   Jia CM, 2019, IEEE J EM SEL TOP C, V9, P177, DOI 10.1109/JETCAS.2018.2886642
   Jiang XR, 2017, IEEE INT CONF MULTI
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kang HH, 2008, OPT COMMUN, V281, P3640, DOI 10.1016/j.optcom.2008.03.051
   Kondermann D, 2016, IEEE COMPUT SOC CONF, P19, DOI 10.1109/CVPRW.2016.10
   Li L, 2017, IEEE DATA COMPR CONF, P131, DOI 10.1109/DCC.2017.10
   Liu DY, 2020, IEEE T MULTIMEDIA, V22, P846, DOI 10.1109/TMM.2019.2934426
   Liu D, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574674
   Magnor M, 2000, IEEE T CIRC SYST VID, V10, P338, DOI 10.1109/76.836278
   Misra K, 2013, IEEE J-STSP, V7, P969, DOI 10.1109/JSTSP.2013.2271451
   Ng R, 2005, RES REPORT, DOI DOI 10.1145/3097571
   Peng JY, 2020, IEEE T COMPUT IMAG, V6, P682, DOI 10.1109/TCI.2020.2967148
   Pereira F., 2018, Patent No. [ISO/IEC JTC 1/SC29/WG1N81022, 129181022]
   Perra C, 2016, IEEE INT CONF MULTI
   Pratapa S, 2019, COMPUT GRAPH FORUM, V38, P1, DOI 10.1111/cgf.13755
   Pratapa Srihari, 2018, ABS180506019 CORR
   Srinivasan PP, 2017, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2017.246
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Vieira A, 2015, INT CONF IMAG PROC, P494, DOI 10.1109/IPTA.2015.7367195
   Viola I, 2018, INT WORK QUAL MULTIM, P189
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Zhang J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107956
   Zhao SY, 2017, IEEE IMAGE PROC, P4562, DOI 10.1109/ICIP.2017.8297146
NR 53
TC 9
Z9 9
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 44
DI 10.1145/3471905
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400001
DA 2024-07-18
ER

PT J
AU Van Damme, S
   Vega, MT
   De Turck, F
AF Van Damme, Sam
   Vega, Maria Torres
   De Turck, Filip
TI Machine Learning Based Content-Agnostic Viewport Prediction for
   360-Degree Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Virtual Reality (VR); 360-degree video; viewport prediction;
   content-agnostic; machine learning
AB Accurate and fast estimations or predictions of the (near) future location of the users of head-mounted devices within the virtual omnidirectional environment open a plethora of opportunities in application domains such as interactive immersive gaming and tele-surgery. Therefore, the past years have seen growing attention to models for viewport prediction in 360 degrees environments. Among the approaches, content-agnostic, trajectory-based methods have the potential to provide very fast solutions, as they do not require complex analysis of the videos to provide a prediction. However, accurate trajectory-based viewport prediction is rather difficult due to the intrinsic variability in user behaviour. Furthermore, even when making use of machine learning, current approaches tend to be brute-force and heavily tailored to specific datasets with little comparison to existing benchmarks or publicly available studies. This article presents a generic, content-agnostic viewport prediction method consisting of a window-based approach combined with a preprocessing system to classify behavioural patterns in terms of user clustering and trajectory correlation. Moreover, as the state of the art does not provide a comparative analysis of different approaches, this work contributes to this. Based on the obtained results, a combined prediction model is proposed and evaluated. Our method shows a 36.8% to 53.9% improvement when compared to the static prediction baseline for a prediction horizon of 8 seconds. In addition, a 11.5% to 24.0% improvement to a brute-force machine learning prediction approach is obtained. As such, this work contributes towards the creation of more generic and structured solutions for content-agnostic viewport prediction in terms of data representation, preprocessing and modelling.
C1 [Van Damme, Sam; Vega, Maria Torres; De Turck, Filip] Univ Ghent, Dept Informat Technol, IDLab Imec, Ghent, Belgium.
C3 Ghent University
RP Van Damme, S (corresponding author), Univ Ghent, Dept Informat Technol, IDLab Imec, Ghent, Belgium.
EM sam.vandamme@ugent.be; maria.torresvega@ugent.be; filip.deturck@ugent.be
RI Vega, Maria Torres/AAL-1171-2020; Van Damme, sam/KHU-1756-2024
OI Vega, Maria Torres/0000-0002-5656-6607; Van Damme,
   Sam/0000-0001-5398-7927
FU Huawei Technologies, China; Research Foundation Flanders (FWO)
   [12W4819N]
FX This research is part of a collaborative project between Huawei and
   Ghent University, funded by Huawei Technologies, China. M. T. Vega was
   funded by the Research Foundation Flanders (FWO), grant number 12W4819N.
CR Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   [Anonymous], 2017, P 27 NOSSDAV, DOI DOI 10.1145/3083165.3083180
   Atev S, 2010, IEEE T INTELL TRANSP, V11, P647, DOI 10.1109/TITS.2010.2048101
   Aykut T, 2019, IEEE J EM SEL TOP C, V9, P231, DOI 10.1109/JETCAS.2019.2897220
   Ban Y., 2018, IEEE INT CON MULTI, P1
   Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Fanti C, 2004, ADV NEUR IN, V16, P1603
   Feng XL, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P800, DOI [10.1109/VR46266.2020.00005, 10.1109/VR46266.2020.1584727730619]
   Feng XL, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P183, DOI 10.1109/AIVR46125.2019.00038
   Heyse J, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P972, DOI [10.1109/vr.2019.8797830, 10.1109/VR.2019.8797830]
   Jiang XL, 2018, C LOCAL COMPUT NETW, P393, DOI 10.1109/LCN.2018.8638092
   Li CG, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P297, DOI 10.1109/MIPR.2019.00060
   Nasrabadi AT, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P273, DOI 10.1145/3304109.3325812
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Petrangeli S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P157, DOI 10.1109/AIVR.2018.00033
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   van der Hooft J, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3362101
   van der Hooft J, 2019, 2019 IFIP/IEEE SYMPOSIUM ON INTEGRATED NETWORK AND SERVICE MANAGEMENT (IM), P381
   Vielhaben J, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P74, DOI 10.1109/AIVR46125.2019.00020
   Wu CL, 2020, AAAI CONF ARTIF INTE, V34, P14003
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Xie L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P564, DOI 10.1145/3240508.3240556
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Xinwei Chen, 2020, IEEE Networking Letters, V2, P81, DOI 10.1109/LNET.2020.2977124
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
NR 27
TC 2
Z9 2
U1 1
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 50
DI 10.1145/3474833
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400007
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhuang, WL
   Wang, CY
   Chai, JX
   Wang, YG
   Shao, M
   Xia, SY
AF Zhuang, Wenlin
   Wang, Congyi
   Chai, Jinxiang
   Wang, Yangang
   Shao, Ming
   Xia, Siyu
TI Music2Dance: DanceNet for Music-Driven Dance Generation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D human dance; generative model; temporal convolution
AB Synthesize human motions from music (i.e., music to dance) is appealing and has attracted lots of research interests in recent years. It is challenging because of the requirement for realistic and complex human motions for dance, but more importantly, the synthesized motions should be consistent with the style, rhythm, and melody of the music. In this article, we propose a novel autoregressive generative model, DanceNet, to take the style, rhythm, and melody of music as the control signals to generate 3D dance motions with high realism and diversity. Due to the high long-term spatio-temporal complexity of dance, we propose the dilated convolution to improve the receptive field, and adopt the gated activation unit as well as separable convolution to enhance the fusion of motion features and control signals. To boost the performance of our proposed model, we capture several synchronized music-dance pairs by professional dancers and build a high-quality music-dance pair dataset. Experiments have demonstrated that the proposed method can achieve state-of-the-art results.
C1 [Zhuang, Wenlin; Wang, Yangang; Xia, Siyu] Southeast Univ, Sch Automat, Nanjing, Peoples R China.
   [Zhuang, Wenlin] Minist Educ, Key Lab Measurement & Control Complex Syst Engn, Nanjing, Peoples R China.
   [Wang, Congyi; Chai, Jinxiang] Xmov, Shanghai, Peoples R China.
   [Shao, Ming] Univ Massachusetts Dartmouth, Dartmouth, MA USA.
C3 Southeast University - China; University of Massachusetts System;
   University Massachusetts Dartmouth
RP Xia, SY (corresponding author), Southeast Univ, Sch Automat, Nanjing, Peoples R China.
EM wlzhuang@seu.edu.cn; artwang007@gmail.com; jchai@cs.tamu.edu;
   yangangwang@seu.edu.cn; mshao@umassd.edu; xia081@gmail.com
RI Wang, Yangang/IVH-8352-2023
OI Wang, Yangang/0000-0002-1325-9252
CR Adobe, 2017, AD MIX DAT
   [Anonymous], 2012, COURSERA NEURAL NETW
   Bock S, 2016, P 2016 ACM C MULTIME, P1174, DOI 10.1145/2964284.2973795
   Bock Sebastian, 2016, ISMIR, P255
   Bock Sebastian, 2012, P 15 INT C DIG AUD E
   Bowden Richard, 2000, P IEEE WORKSH HUM MO, V2000
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Cardle M, 2002, 20TH EUROGRAPHICS UK CONFERENCE, PROCEEDINGS, P38, DOI 10.1109/EGUK.2002.1011270
   Chai JX, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239459
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   Choi K, 2017, INT CONF ACOUST SPEE, P2392, DOI 10.1109/ICASSP.2017.7952585
   CMU, 2010, CARN MELL MOT CAPT D
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   Ewert S., 2011, Proc ISMIR
   Eyben F., 2010, Proceedings of the 11th International Society for Music Information Retrieval Conference, P589, DOI 10.5281/zenodo.1417131
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Ginosar S, 2019, PROC CVPR IEEE, P3492, DOI 10.1109/CVPR.2019.00361
   Glorot X., 2010, P INT C ART INT STAT, P249
   Gopalakrishnan A, 2019, PROC CVPR IEEE, P12108, DOI 10.1109/CVPR.2019.01239
   Gutierrez E.G., 2006, Tonal description of music audio signals
   Hawthorne, 2017, ARXIV PREPRINT ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Korzeniowski Filip, 2016, ARXIV PREPRINT ARXIV
   Kovar Lucas., 2008, ACM SIGGRAPH 2008 classes, page, P51
   Krebs Florian, 2013, ISMIR, P227
   Krebs Florian., 2016, Proceedings of the 17th International Society for Music Information Retrieval Conference, P129, DOI [DOI 10.5281/ZENODO.1417819, 10.5281/zenodo.1417819]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lau M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618517
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee Hsin-Ying, 2019, ADV NEURAL INFORM PR, V32
   Lee J, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1173
   Lee K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275071
   Li C, 2018, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2018.00548
   Li Y, 2002, ACM T GRAPHIC, V21, P465
   Li Z, 2017, ADVANCED ENGINEERING AND TECHNOLOGY III, P3
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Min JY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366172
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Pavlovic V, 2001, ADV NEUR IN, V13, P981
   Safonova A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239557
   SFU, 2017, SFU MOT CAPT DAT
   Tang TR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1598, DOI 10.1145/3240508.3240526
   van den Oord A, 2016, ADV NEUR IN, V29
   Xia SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766999
   Yalta Nelson, 2018, ARXIV PREPRINT ARXIV
   Yan SJ, 2019, IEEE I CONF COMP VIS, P4393, DOI 10.1109/ICCV.2019.00449
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
NR 51
TC 27
Z9 29
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 65
DI 10.1145/3485664
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Alizadehsani, R
   Sharifrazi, D
   Izadi, NH
   Joloudari, JH
   Shoeibi, A
   Gorriz, JM
   Hussain, S
   Arco, JE
   Sani, ZA
   Khozeimeh, F
   Khosravi, A
   Nahavandi, S
   Islam, SMS
   Acharya, UR
AF Alizadehsani, Roohallah
   Sharifrazi, Danial
   Izadi, Navid Hoseini
   Joloudari, Javad Hassannataj
   Shoeibi, Afshin
   Gorriz, Juan M.
   Hussain, Sadiq
   Arco, Juan E.
   Sani, Zahra Alizadeh
   Khozeimeh, Fahime
   Khosravi, Abbas
   Nahavandi, Saeid
   Islam, Sheikh Mohammed Shariful
   Acharya, U. Rajendra
TI Uncertainty-Aware Semi-Supervised Method Using Large Unlabeled and
   Limited Labeled COVID-19 Data
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Semi-supervised learning; generative adversarial networks; COVID-19;
   supervised learning; deep learning
ID CLASSIFICATION; NETWORK; CT
AB The new coronavirus has caused more than one million deaths and continues to spread rapidly. This virus targets the lungs, causing respiratory distress which can be mild or severe. The X-ray or computed tomography (CT) images of lungs can reveal whether the patient is infected with COVID-19 or not. Many researchers are trying to improve COVID-19 detection using artificial intelligence. Our motivation is to develop an auto-maticmethod that can cope with scenarios inwhich preparing labeled data is time consuming or expensive. In this article, we propose a Semi-supervised Classification using Limited Labeled Data (SCLLD) relying on Sobel edge detection and Generative Adversarial Networks (GANs) to automate the COVID-19 diagnosis. The GAN discriminator output is a probabilistic value which is used for classification in this work. The proposed system is trained using 10,000 CT scans collected from Omid Hospital, whereas a public dataset is also used for validating our system. The proposed method is compared with other state-of-the-art supervised methods such as Gaussian processes. To the best of our knowledge, this is the first time a semi-supervised method for COVID-19 detection is presented. Our system is capable of learning from a mixture of limited labeled and unlabeled datawhere supervised learners fail due to a lack of sufficient amount of labeled data. Thus, our semi-supervised training method significantly outperforms the supervised training of Convolutional Neural Network (CNN) when labeled training data is scarce. The 95% confidence intervals for our method in terms of accuracy, sensitivity, and specificity are 99.56 +/- 0.20%, 99.88 +/- 0.24%, and 99.40 +/- 0.18%, respectively, whereas intervals for the CNN (trained supervised) are 68.34 +/- 4.11%, 91.2 +/- 6.15%, and 46.40 +/- 5.21%.
C1 [Alizadehsani, Roohallah; Khozeimeh, Fahime; Khosravi, Abbas; Nahavandi, Saeid] Deakin Univ, Inst Intelligent Syst Res & Innovat IISRI, 75 Pigdons Rd, Geelong, Vic 3216, Australia.
   [Sharifrazi, Danial] Islamic Azad Univ, Sch Tech & Engn, Dept Comp Engn, Shiraz Branch, Sadra Hwy, Shiraz, Iran.
   [Izadi, Navid Hoseini] Isfahan Univ Technol, Dept Elect & Comp Engn, Esfahan 8415683111, Iran.
   [Joloudari, Javad Hassannataj] Univ Birjand, Fac Engn, Dept Comp Engn, Daneshgah & Sanati Hwy Birjand, Birjand, Iran.
   [Shoeibi, Afshin] Ferdowsi Univ Mashhad, Comp Engn Dept, Vakil Abad Hwy, Mashhad, Razavi Khorasan, Iran.
   [Shoeibi, Afshin] KN Toosi Univ Technol, Fac Elect & Comp Engn, Biomed Data Acquisit Lab, Shariati Ave, Tehran, Iran.
   [Gorriz, Juan M.; Arco, Juan E.] Univ Granada, Dept Signal Theory Networking & Commun, Av Hospicio, Granada, Spain.
   [Hussain, Sadiq] Dibrugarh Univ, Dibrugarh 786004, Assam, India.
   [Sani, Zahra Alizadeh] Iran Univ Med Sci, Rajaie Cardiovasc Med & Res Ctr, Valiasr Ave, Tehran, Iran.
   [Sani, Zahra Alizadeh] Iran Univ Med Sci, Omid Hosp, Tehran, Iran.
   [Islam, Sheikh Mohammed Shariful] Deakin Univ, Inst Phys Act & Nutr, 221 Burwood Highway, Melbourne, Vic, Australia.
   [Islam, Sheikh Mohammed Shariful] George Inst Global Hlth, Cardiovasc Div, King St, Sydney, NSW, Australia.
   [Islam, Sheikh Mohammed Shariful] Univ Sydney, Sydney Med Sch, Fisher Rd, Sydney, NSW, Australia.
   [Acharya, U. Rajendra] Ngee Ann Polytech, Dept Elect & Comp Engn, 535 Clementi Rd, Singapore, Singapore.
   [Acharya, U. Rajendra] Singapore Univ Social Sci, Dept Biomed Engn, Sch Sci & Technol, 463 Clementi Rd, Singapore, Singapore.
   [Acharya, U. Rajendra] Asia Univ, Dept Bioinformat & Med Engn, Liufeng Rd, Taichung, Taiwan.
C3 Deakin University; Islamic Azad University; Isfahan University of
   Technology; University of Birjand; Ferdowsi University Mashhad; K. N.
   Toosi University of Technology; University of Granada; Dibrugarh
   University; Iran University of Medical Sciences; Iran University of
   Medical Sciences; Deakin University; University of Sydney; George
   Institute for Global Health; University of Sydney; Singapore University
   of Social Sciences (SUSS); Asia University Taiwan
RP Alizadehsani, R (corresponding author), Deakin Univ, Inst Intelligent Syst Res & Innovat IISRI, 75 Pigdons Rd, Geelong, Vic 3216, Australia.
EM r.alizadehsani@deakin.edu.au; danial.sharifrazi@gmail.com;
   navid.hoseini1@ec.iut.ac.ir; javad.hasannataj@gmail.com;
   afshin.shoeibi@gmail.com; gorriz@ugr.es; sadiq@dibru.ac.in;
   jearco@ugr.es; d.alizadeh.sani@gmail.com; fkhozeimeh@deakin.edu.au;
   abbas.khosravi@deakin.edu.au; saeid.nahavandi@deakin.edu.au;
   shariful.islam@deakin.edu.au; rajendra_udyavara_acharya@np.edu.sg
RI Shariful Islam, Sheikh Mohammed/B-1219-2011; Gorriz, Juan
   Manuel/C-2385-2012; Joloudari, Javad Hassannataj/AAW-2357-2020;
   Alizadehsani, Roohallah/HLP-8160-2023; Acharya, Rajendra U/E-3791-2010;
   Khosravi, Abbas/AAQ-8102-2021; Hussain, Sadiq/ABA-9849-2020; Nahavandi,
   Saeid/AAE-5536-2022; Shoeibi, Afshin/AAT-3624-2020
OI Shariful Islam, Sheikh Mohammed/0000-0001-7926-9368; Joloudari, Javad
   Hassannataj/0000-0001-9374-2326; Acharya, Rajendra
   U/0000-0003-2689-8552; Shoeibi, Afshin/0000-0003-0635-6799;
   Alizadehsani, Roohallah/0000-0003-0898-5054; Hussain,
   Sadiq/0000-0002-9840-4796
FU MINECO/FEDER [RTI2018-098913-B100, CV20-45250, A-TIC-080UGR18]
FX This work was partly supported by the MINECO/FEDER under the
   RTI2018-098913-B100, CV20-45250, and A-TIC-080UGR18 projects.
CR Ai T, 2020, RADIOLOGY, V296, pE32, DOI 10.1148/radiol.2020200642
   Alizadehsani R, 2021, COMPUT BIOL MED, V128, DOI 10.1016/j.compbiomed.2020.104095
   Alizadehsani R, 2021, J MED VIROL, V93, P2307, DOI 10.1002/jmv.26699
   Alom MZ, 2020, COVID MTNET COVID 19
   Anthimopoulos M, 2016, IEEE T MED IMAGING, V35, P1207, DOI 10.1109/TMI.2016.2535865
   Apostolopoulos ID, 2020, PHYS ENG SCI MED, V43, P635, DOI 10.1007/s13246-020-00865-4
   Ardakani AA, 2021, EUR RADIOL, V31, P121, DOI 10.1007/s00330-020-07087-y
   Ardakani AA, 2020, COMPUT BIOL MED, V121, DOI 10.1016/j.compbiomed.2020.103795
   Belyaev M., 2014, ARXIV14036573
   Chen NS, 2020, LANCET, V395, P507, DOI 10.1016/S0140-6736(20)30211-7
   Dansana D, 2023, SOFT COMPUT, V27, P2635, DOI 10.1007/s00500-020-05275-y
   Depeursinge A, 2015, INVEST RADIOL, V50, P261, DOI 10.1097/RLI.0000000000000127
   Fang YC, 2020, RADIOLOGY, V296, pE115, DOI 10.1148/radiol.2020200432
   Ghahramani Z, 2004, LECT NOTES ARTIF INT, V3176, P72
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Górriz JM, 2020, NEUROCOMPUTING, V410, P237, DOI 10.1016/j.neucom.2020.05.078
   Gozes O., 2020, RAPID AI DEV CYCLE C
   Hu TC, 2015, 2015 IEEE/ACIS 14TH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE (ICIS), P259, DOI 10.1109/ICIS.2015.7166603
   Jaiswal A, 2021, J BIOMOL STRUCT DYN, V39, P5682, DOI 10.1080/07391102.2020.1788642
   Javaheri T, 2021, NPJ DIGIT MED, V4, DOI 10.1038/s41746-021-00399-3
   Khodatars M., 2020, ARXIV200701285
   Khozeimeh F., SCI REP-UK, V11, P1
   Ko H, 2020, J MED INTERNET RES, V22, DOI 10.2196/19569
   Kumar R, 2021, IEEE SENS J, V21, P16301, DOI 10.1109/JSEN.2021.3076767
   Li L., 2020, Radiology, V296, DOI 10.1148/radiol.2020200905
   Li Longze, 2018, Int J Mach Learn Comput, V8, P428, DOI 10.18178/ijmlc.2018.8.5.724
   Li M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P661, DOI 10.1145/2623330.2623612
   Liu Xiuming, 2018, ARXIV181110947
   Maghdid HS, 2021, PROC SPIE, V11734, DOI 10.1117/12.2588672
   Narin A, 2021, PATTERN ANAL APPL, V24, P1207, DOI 10.1007/s10044-021-00984-y
   Ni QQ, 2020, EUR RADIOL, V30, P6517, DOI 10.1007/s00330-020-07044-9
   Olivas E.S., 2009, Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques, DOI 10.4018/978-1-6056-6766-9&BUYLINK=TRUE
   Panwar H, 2020, CHAOS SOLITON FRACT, V140, DOI 10.1016/j.chaos.2020.110190
   Radford A., 2015, ARXIV
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Saeedi A., 2020, ARXIV200614419
   Salimans T, 2016, ADV NEUR IN, V29
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sethy PK, 2020, INT J MATH ENG MANAG, V5, P643, DOI 10.33889/IJMEMS.2020.5.4.052
   Shah V, 2021, EMERG RADIOL, V28, P497, DOI 10.1007/s10140-020-01886-y
   Sharifrazi D., 2020, CNN KCL AUTOMATICMYO, DOI [10.20944/preprints202007.0650.v1, DOI 10.20944/PREPRINTS202007.0650.V1]
   Shi WY, 2021, ANN TRANSL MED, V9, DOI 10.21037/atm-20-2464
   Shoeibi A, 2020, AUTOMATED DETECTION
   Shoeibi A, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18115780
   Song Y, 2021, IEEE ACM T COMPUT BI, V18, P2775, DOI 10.1109/TCBB.2021.3065361
   Srinivasan D, 2020, ARXIV201211840
   Wang Linda, 2020, Sci Rep, V10, P19549, DOI 10.1038/s41598-020-76550-z
   Wang S, 2021, EUR RADIOL, V31, P6096, DOI [10.1080/1064119X.2021.1966557, 10.1079/9781789246070.0001, 10.1007/s00330-021-07715-1]
   Wang SH, 2021, INFORM FUSION, V67, P208, DOI 10.1016/j.inffus.2020.10.004
   Westendorf K., 2022, bioRxiv, DOI DOI 10.1101/2021.04
   Xu XW, 2020, ENGINEERING-PRC, V6, P1122, DOI 10.1016/j.eng.2020.04.010
   Yasin R, 2020, EGYPT J RADIOL NUC M, V51, DOI 10.1186/s43055-020-00296-x
   Yu TT, 2019, CAAI T INTELL TECHNO, V4, P122, DOI 10.1049/trit.2019.0017
   Zalzala HH, 2020, NEW MICROB NEW INFEC, V38, DOI 10.1016/j.nmni.2020.100761
   Zhang JQ, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2831, DOI 10.1145/3394486.3403334
   Zhang Yu-Dong, 2020, IEEE SENS J, V2020
   Zhao J., 2020, ARXIV PREPRINT ARXIV
NR 57
TC 29
Z9 29
U1 2
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 104
DI 10.1145/3462635
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, ZD
   Zhou, WG
   Li, HQ
AF Liu, Zhandong
   Zhou, Wengang
   Li, Houqiang
TI MFECN: Multi-level Feature Enhanced Cumulative Network for Scene Text
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Scene Text Detection; Multi-level Feature; Feature Fusion; Instance
   Segmentation
AB Recently, many scene text detection algorithms have achieved impressive performance by using convolutional neural networks. However, most of them do not make full use of the context among the hierarchical multi-level features to improve the performance of scene text detection. In this article, we present an efficient multi-level features enhanced cumulative framework based on instance segmentation for scene text detection. At first, we adopt a Multi-Level Features Enhanced Cumulative (MFEC) module to capture features of cumulative enhancement of representational ability. Then, a Multi-Level Features Fusion (MFF) module is designed to fully integrate both high-level and low-level MFEC features, which can adaptively encode scene text information. To verify the effectiveness of the proposed method, we perform experiments on six public datasets (namely, CTW1500, Total-text, MSRA-TD500, ICDAR2013, ICDAR2015, and MLT2017), and make comparisons with other state-of-the-art methods. Experimental results demonstrate that the proposed Multi-Level Features Enhanced Cumulative Network (MFECN) detector can well handle scene text instances with irregular shapes (i.e., curved, oriented, and horizontal) and achieves better or comparable results.
C1 [Liu, Zhandong; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Dept Elect Engn & Informat Sci, 443 Huangshan Rd, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Dept Elect Engn & Informat Sci, 443 Huangshan Rd, Hefei 230027, Peoples R China.
EM lzd0825@mail.ustc.edu.cn; zhwg@ustc.edu.cn; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
FU Natural Science Foundation of Xinjiang Province [2020D01A73]; NSFC
   [61836011, 61662082, U1703261, 61822208, 61632019]; Youth Innovation
   Promotion Association CAS [2018497]
FX This work was supported in part to Zhandong Liu by Natural Science
   Foundation of Xinjiang Province under contract No. 2020D01A73, NSFC
   under contract Nos. 61662082 and U1703261, and in part to Dr. Wengang
   Zhou by NSFC under contract Nos. 61822208 and 61632019, and Youth
   Innovation Promotion Association CAS (No. 2018497), and in part to Dr.
   Houqiang Li by NSFC under contract No. 61836011.
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2014, IEEE T IMAGE PROCESS, DOI DOI 10.1109/TIP.2014.2353813
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen ZN, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231742
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Dai YC, 2018, INT C PATT RECOG, P3604, DOI 10.1109/ICPR.2018.8546066
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Ding, 2019, ARXIV PREPRINT ARXIV
   Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He DF, 2017, PROC CVPR IEEE, P474, DOI 10.1109/CVPR.2017.58
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   He T, 2018, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR.2018.00527
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445
   Huang S, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152129
   Huang T. S., 2016, P 24 ACM INT C MULT, P516, DOI 10.1145/2964284.2967274
   Huang ZD, 2019, IEEE WINT CONF APPL, P764, DOI 10.1109/WACV.2019.00086
   Jeong J., 2017, P BRIT MACH VIS C, DOI [10.5244/C.31.76, DOI 10.5244/C.31.76]
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Ke W, 2017, PROC CVPR IEEE, P302, DOI 10.1109/CVPR.2017.40
   Lee H, 2017, IEEE T IMAGE PROCESS, V26, P4843, DOI 10.1109/TIP.2017.2725580
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P494, DOI 10.1109/TPAMI.2019.2894422
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W., 2016, LECT NOTES COMPUT SC, P21, DOI DOI 10.1007/978-3-319-46448-0_2
   Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595
   Liu Y, 2017, COMMUN MATH BIOL NEU, DOI 10.1080/10408398.2017.1329704
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Liu ZD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356728
   Liu ZD, 2019, MULTIMED TOOLS APPL, V78, P18205, DOI 10.1007/s11042-019-7177-4
   Liu ZC, 2019, PROC CVPR IEEE, P7261, DOI 10.1109/CVPR.2019.00744
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roy AG, 2019, IEEE T MED IMAGING, V38, P540, DOI 10.1109/TMI.2018.2867261
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1847, DOI 10.1145/3240508.3240688
   Tan M., 2020, P 2020 IEEE CVF C CO, P10
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Wang FF, 2018, PROC CVPR IEEE, P1381, DOI 10.1109/CVPR.2018.00150
   Wang PF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1277, DOI 10.1145/3343031.3350988
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wolf C, 2006, INT J DOC ANAL RECOG, V8, P280, DOI 10.1007/s10032-006-0014-0
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989
   Xue CH, 2018, LECT NOTES COMPUT SC, V11220, P370, DOI 10.1007/978-3-030-01270-0_22
   Yang QP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1071
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Yin XC, 2016, IEEE T IMAGE PROCESS, V25, P2752, DOI 10.1109/TIP.2016.2554321
   Zeng XY, 2016, LECT NOTES COMPUT SC, V9911, P354, DOI 10.1007/978-3-319-46478-7_22
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang S, 2018, AAAI CONF ARTIF INTE, P2612
   Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871
   Zhao K, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1191
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YY, 2016, FRONT COMPUT SCI-CHI, V10, P19, DOI 10.1007/s11704-015-4488-0
NR 77
TC 0
Z9 0
U1 4
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 78
DI 10.1145/3440087
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400002
DA 2024-07-18
ER

PT J
AU Gupta, S
   Sharma, K
   Dinesh, DA
   Thenkanidiyoor, V
AF Gupta, Shikha
   Sharma, Krishan
   Dinesh, Dileep Aroor
   Thenkanidiyoor, Veena
TI Visual Semantic-Based Representation Learning Using Deep CNNs for Scene
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Pseudo-concept; semantic multinomial representation; subspace modeling;
   support vector machine; pseudo-concept modeling
ID IMAGE REPRESENTATION; CLASSIFICATION; KERNEL
AB In this work, we address the task of scene recognition from image data. A scene is a spatially correlated arrangement of various visual semantic contents also known as concepts, e.g., "chair," "car," "sky," etc. Representation learning using visual semantic content can be regarded as one of the most trivial ideas as it mimics the human behavior of perceiving visual information. Semantic multinomial (SMN) representation is one such representation that captures semantic information using posterior probabilities of concepts. The core part of obtaining SMN representation is the building of concept models. Therefore, it is necessary to have ground-truth (true) concept labels for every concept present in an image. Moreover, manual labeling of concepts is practically not feasible due to the large number of images in the dataset. To address this issue, we propose an approach for generating pseudo-concepts in the absence of true concept labels. We utilize the pre-trained deep CNN-based architectures where activation maps (filter responses) from convolutional layers are considered as initial cues to the pseudo-concepts. The non-significant activation maps are removed using the proposed filter-specific threshold-based approach that leads to the removal of non-prominent concepts from data. Further, we propose a grouping mechanism to group the same pseudoconcepts using subspace modeling of filter responses to achieve a non-redundant representation. Experimental studies show that generated SMN representation using pseudo-concepts achieves comparable results for scene recognition tasks on standard datasets like MIT-67 and SUN-397 even in the absence of true concept labels.
C1 [Gupta, Shikha; Sharma, Krishan; Dinesh, Dileep Aroor] Indian Inst Technol Mandi, Mandi 175001, Himachal Prades, India.
   [Thenkanidiyoor, Veena] Natl Inst Technol Goa, Ponda 403401, Goa, India.
   [Gupta, Shikha; Sharma, Krishan] Vehant Technol, B-73,Block B,Sect 57, Noida, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Mandi; National Institute of Technology (NIT System);
   National Institute of Technology Goa
RP Gupta, S (corresponding author), Indian Inst Technol Mandi, Mandi 175001, Himachal Prades, India.; Gupta, S (corresponding author), Vehant Technol, B-73,Block B,Sect 57, Noida, India.
EM shikha_g@students.iitmandi.ac.in;
   krishan_Sharma@students.iitmandi.ac.in; addileep@iitmandi.ac.in;
   veenat@iiitgoa.ac.in
CR [Anonymous], 2013, Advances in Neural Information Processing Systems (NIPS)
   [Anonymous], ACM T MULTIMEDIA COM, V17
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Barla A, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P513
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cheng XJ, 2018, PATTERN RECOGN, V74, P474, DOI 10.1016/j.patcog.2017.09.025
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dileep AD, 2014, IEEE T NEUR NET LEAR, V25, P1421, DOI 10.1109/TNNLS.2013.2293512
   Dixit M, 2015, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2015.7298916
   Donahue J, 2014, PR MACH LEARN RES, V32
   Donggeun Yoo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P71, DOI 10.1109/CVPRW.2015.7301274
   Fan R. K, 1997, SPECTRAL GRAPH THEOR
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Feng LN, 2016, IEEE T PATTERN ANAL, V38, P785, DOI 10.1109/TPAMI.2015.2469281
   Fong R, 2018, PROC CVPR IEEE, P8730, DOI 10.1109/CVPR.2018.00910
   Gao Bin-Bin, 2015, ARXIV PREPRINT ARXIV
   Golub G.H., 1989, MATRIX COMPUTATIONS
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gupta S, 2018, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS (ICPRAM 2018), P141, DOI 10.5220/0006596101410148
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Henderson JM, 2005, VIS COGN, V12, P849, DOI 10.1080/13506280444000544
   Herranz L, 2016, PROC CVPR IEEE, P571, DOI 10.1109/CVPR.2016.68
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jiang SQ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231738
   Khan SH, 2016, IEEE T IMAGE PROCESS, V25, P3372, DOI 10.1109/TIP.2016.2567076
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li LJ, 2014, INT J COMPUT VISION, V107, P20, DOI 10.1007/s11263-013-0660-x
   Li Y, 2017, INT J COMPUT VISION, V121, P344, DOI 10.1007/s11263-016-0945-y
   Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536
   Liu RS, 2018, INT J DIGIT MULTIMED, V2018, DOI [10.1155/2018/7543875, 10.1109/TMM.2018.2812605]
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Pradhan Deepak Kumar, 2017, NAT C COMP VIS PATT, P400
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Rasiwasia N, 2012, IEEE T PATTERN ANAL, V34, P902, DOI 10.1109/TPAMI.2011.175
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Seong H, 2020, IEEE ACCESS, V8, P82066, DOI 10.1109/ACCESS.2020.2989863
   Sharma K, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2701, DOI 10.1109/ICASSP.2018.8462429
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sitaula C, 2019, IEEE ACCESS, V7, P84967, DOI 10.1109/ACCESS.2019.2925002
   Sun N, 2019, IEEE T CIRC SYST VID, V29, P1715, DOI 10.1109/TCSVT.2018.2848543
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang PJ, 2017, NEUROCOMPUTING, V225, P188, DOI 10.1016/j.neucom.2016.11.023
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Thenkanidiyoor Veena, 2017, 2017 IEEE INT C SMAR, P1
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vogel J, 2004, LECT NOTES COMPUT SC, V3115, P207
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wan V, 2002, INT CONF ACOUST SPEE, P669
   Wu RB, 2015, IEEE I CONF COMP VIS, P1287, DOI 10.1109/ICCV.2015.152
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xie GS, 2017, IEEE T CIRC SYST VID, V27, P1263, DOI 10.1109/TCSVT.2015.2511543
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Yoo D., 2014, ARXIV PREPRINT ARXIV
   Yosinski J., 2015, ARXIV150606579, V2015, P12
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 65
TC 11
Z9 11
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 53
DI 10.1145/3436494
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000015
DA 2024-07-18
ER

PT J
AU Luo, XF
   Wong, F
   Hu, HF
AF Luo, Xiaofan
   Wong, Fukoeng
   Hu, Haifeng
TI FIN: Feature Integrated Network for Object Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Object detection; deep learning; multi-layer detection; feature
   integration
AB Multi-layer detection is a widely used method in the field of object detection. It extracts multiple feature maps with different resolutions from the backbone network to detect objects of different scales, which can effectively cope with the problem of object scale change in object detection. Although the multi-layer detection utilizes multiple detection layers to alleviate the burden of one single detection layer and can improve the detection accuracy to some extent, this method has two limitations. First, manually assigning anchor boxes of different sizes to different feature maps is too dependent on the human experience. Second, there is a semantic gap between each detection layer in multi-layer detection. The same detector needs to simultaneously process the detection layers with inconsistent semantic strength, which increases the optimization difficulty of the detector. In this article, we propose a feature integrated network (FIN) based on single layer detection to deal with the problems mentioned above. Different from the existing methods, we design a series of verification experiments based on the multi-layer detection model, which shows that the shallow high-resolution feature map has the potential to simultaneously and effectively detect objects of various scales. Considering that the semantic information of the shallow feature map is weak, we propose two modules to enhance the representation ability of the single detection layer. First, we propose a detection adaptation network (DANet) to extract powerful feature maps that are useful for object detection tasks. Second, we combine global context information and local detail information with a verified hourglass module (VHM) to generate a single feature map with high resolution and rich semantic information so that we can assign all anchor boxes to this detection layer. In our model, all the detection operations are concentrated on a high-resolution feature map whose semantic information and detailed information are enhanced as much as possible. Therefore, the proposed model can solve the problem of anchor assignment and inconsistent semantic strength between multiple detection layers mentioned above. A large number of experiments on the Pattern Analysis, Statistical Modelling and Computational Learning Visual Object Classes (PASCAL VOC) and Microsoft Common Objects in Context (MS COCO) datasets show that our model has good detection performance for objects of various sizes. The proposed model can achieve 81.9 mAP when the size of the input image is 300 x 300.
C1 [Luo, Xiaofan; Wong, Fukoeng; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM luoxf3@mail2.sysu.edu.cn; huangfq8@mail2.sysu.edu.cn;
   huhaif@mail.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; National Key R&D Program of China [2018YFB1601101]; Natural
   Science Foundation of Guangdong Province [2017A030311029]; Science and
   Technology Program of Guangzhou of China [201704020180]; Fundamental
   Research Funds for the Central Universities of China
FX This work was supported in part by the following: the National Natural
   Science Foundation of China under Grant 61673402, Grant 61273270, and
   Grant 60802069; the National Key R&D Program of China under Grant
   2018YFB1601101; the Natural Science Foundation of Guangdong Province
   under Grant 2017A030311029; the Science and Technology Program of
   Guangzhou of China under Grant 201704020180; and the Fundamental
   Research Funds for the Central Universities of China.
CR [Anonymous], 2018, ARXIV180406215
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Paszke A., 2019, ADV NEURAL INFORM PR, P8026, DOI DOI 10.48550/ARXIV.1912.01703
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Woo S, 2018, IEEE WINT CONF APPL, P1093, DOI 10.1109/WACV.2018.00125
   Zeng XY, 2018, IEEE T PATTERN ANAL, V40, P2109, DOI 10.1109/TPAMI.2017.2745563
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
NR 24
TC 9
Z9 9
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 48
DI 10.1145/3381086
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600009
DA 2024-07-18
ER

PT J
AU Zhou, ZL
   Wu, QMJ
   Yang, YM
   Sun, XM
AF Zhou, Zhili
   Wu, Q. M. Jonathan
   Yang, Yimin
   Sun, Xingming
TI Region-Level Visual Consistency Verification for Large-Scale
   Partial-Duplicate Image Search
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Partial-duplicate image search; copy detection; near-duplicate image
   search; image retrieval; digital forensics
ID MODEL; FEATURES
AB Most recent large-scale image search approaches build on a bag-of-visual-words model, in which local features are quantized and then efficiently matched between images. However, the limited discriminability of local features and the BOW quantization errors cause a lot of mismatches between images, which limit search accuracy. To improve the accuracy, geometric verification is popularly adopted to identify geometrically consistent local matches for image search, but it is hard to directly use these matches to distinguish partial-duplicate images from non-partial-duplicate images. To address this issue, instead of simply identifying geometrically consistent matches, we propose a region-level visual consistency verification scheme to confirm whether there are visually consistent region (VCR) pairs between images for partial-duplicate search. Specifically, after the local feature matching, the potential VCRs are constructed via mapping the regions segmented from candidate images to a query image by utilizing the properties of the matched local features. Then, the compact gradient descriptor and convolutional neural network descriptor are extracted and matched between the potential VCRs to verify their visual consistency to determine whether they are VCRs. Moreover, two fast pruning algorithms are proposed to further improve efficiency. Extensive experiments demonstrate the proposed approach achieves higher accuracy than the state of the art and provide comparable efficiency for large-scale partial-duplicate search tasks.
C1 [Zhou, Zhili; Sun, Xingming] Nanjing Univ Informat Sci & Technol, Jiangsu Engn Ctr Network Monitoring, Nanjing 210044, Peoples R China.
   [Zhou, Zhili; Sun, Xingming] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.
   [Wu, Q. M. Jonathan] Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
   [Yang, Yimin] Lakehead Univ, Dept Comp Sci, Thunder Bay, ON P7B 5E1, Canada.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; University of Windsor;
   Lakehead University
RP Zhou, ZL (corresponding author), Nanjing Univ Informat Sci & Technol, Jiangsu Engn Ctr Network Monitoring, Nanjing 210044, Peoples R China.; Zhou, ZL (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.
EM zhou_zhili@163.com; jwu@uwindsor.ca; yyang48@lakeheadu.ca;
   sunnudt@163.com
RI Yang, Yimin/A-3060-2019; Sun, Xingming/AAD-1866-2019; Yang,
   Yimin/IZE-2284-2023
OI Yang, Yimin/0000-0002-1131-2056; Yang, Yimin/0000-0002-1131-2056
FU National Key R&D Program of China [2018YFB1003205]; National Natural
   Science Foundation of China [61972205, 61602253, U1836208, U1536206,
   U1836110, 61672294]; Canada Research Chair program (CRC) Fund; AUTO21
   Network of Centers of Excellence; Priority Academic Program Development
   of Jiangsu Higher Education Institutions (PAPD) fund; Collaborative
   Innovation Center of Atmospheric Environment and Equipment Technology
   (CICAEET) fund, China; Ministry of Science and Technology (MOST) through
   Pervasive Artificial Intelligence Research (PAIR) Labs, Taiwan
   [108-2634-F-259-001]
FX This work was supported in part by the National Key R&D Program of China
   under grant 2018YFB1003205; in part by the National Natural Science
   Foundation of China under grants 61972205, 61602253, U1836208, U1536206,
   U1836110, and 61672294; in part by the Canada Research Chair program
   (CRC) Fund; in part by the AUTO21 Network of Centers of Excellence; in
   part by the Priority Academic Program Development of Jiangsu Higher
   Education Institutions (PAPD) fund; in part by the Collaborative
   Innovation Center of Atmospheric Environment and Equipment Technology
   (CICAEET) fund, China; and in part by the Ministry of Science and
   Technology (MOST) under contract 108-2634-F-259-001 through Pervasive
   Artificial Intelligence Research (PAIR) Labs, Taiwan.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2016, BIOMED RES INT
   [Anonymous], 2014, CoRR
   [Anonymous], 2004, Int. J. Comput. Vis., DOI [DOI 10.1023/B:VISI.0000029664.99615.94, 10.1023/B:VISI.0000029664.99615.94]
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Chu LY, 2013, IEEE T MULTIMEDIA, V15, P1982, DOI 10.1109/TMM.2013.2270455
   Chum O., 2004, Proc. of the Asian Conference on Computer Vision ACCV, V2, P812
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Ke Y, 2004, PROC CVPR IEEE, P506
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li XC, 2015, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR.2015.7299151
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131
   Liu Z., 2012, P ACM INT C MULTIMED, P199
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mohedano E, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P327, DOI 10.1145/2911996.2912061
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Qian X., 2015, IEEE T CIRCUITS SYST, V25, P1857
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z
   Wang S, 2015, ANNU REV CLIN PSYCHO, V11, P331, DOI 10.1146/annurev-clinpsy-032814-112828
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150
   Yang YM, 2020, IEEE T PATTERN ANAL, V42, P2912, DOI 10.1109/TPAMI.2019.2917685
   Yao JL, 2015, IEEE SIGNAL PROC LET, V22, P1404, DOI 10.1109/LSP.2014.2377795
   Zhang SL, 2014, IEEE T IMAGE PROCESS, V23, P3671, DOI 10.1109/TIP.2014.2330794
   Zhang SL, 2010, INT CONF ACOUST SPEE, P794, DOI 10.1109/ICASSP.2010.5494964
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zhao W., 2010, J NANOMATER, V2010, P5
   Zheng L, 2016, INT J COMPUT VISION, V120, P1, DOI 10.1007/s11263-016-0889-2
   Zheng LG, 2018, CMC-COMPUT MATER CON, V56, P529, DOI 10.3970/cmc.2018.03780
   Zhou W-X, 2014, ADV DIFFER EQU-NY, V2014, P16
   Zhou W, 2013, PHYSIOL REP, V1, DOI 10.1002/phy2.110
   Zhou WG, 2015, IEEE T IMAGE PROCESS, V24, P967, DOI 10.1109/TIP.2015.2389624
   Zhou Z, 2019, IEEE ACCESS, V7, P108818, DOI 10.1109/ACCESS.2019.2933228
   Zhou ZL, 2019, SOFT COMPUT, V23, P4927, DOI 10.1007/s00500-018-3151-8
   Zhou ZL, 2018, PATTERN RECOGN LETT, V109, P18, DOI 10.1016/j.patrec.2017.08.013
   Zhou ZL, 2017, IEEE T INF FOREN SEC, V12, P48, DOI 10.1109/TIFS.2016.2601065
   Zhou ZL, 2016, IEICE T INF SYST, VE99D, P1531, DOI 10.1587/transinf.2015EDP7341
NR 49
TC 29
Z9 29
U1 0
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 54
DI 10.1145/3383582
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600015
DA 2024-07-18
ER

PT J
AU Grigorev, A
   Liu, SH
   Tian, ZH
   Xiong, JX
   Rho, SM
   Feng, J
AF Grigorev, Aleksei
   Liu, Shaohui
   Tian, Zhihong
   Xiong, Jianxin
   Rho, Seungmin
   Feng, Jiang
TI Delving Deeper in Drone-Based Person Re-Id by Employing Deep Decision
   Forest and Attributes Fusion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Datasets; neural networks; re-id; attributes; drones; neural decision
   forest; CBOW; word2vec
ID INTERNET
AB Deep learning has revolutionized the field of computer vision and image processing. Its ability to extract the compact image representation has taken the person re-identification (re-id) problem to a new level. However, in most cases, researchers are focused on developing new approaches to extract more fruitful image representation and use it in the re-id task. The extra information about images is rarely taken into account because the traditional person re-id datasets usually do not have it. Nevertheless, the research in multimodal machine learning has demonstrated that the utilization of the information from different sources leads to better performance. In this work, we demonstrate how a person re-id problem can benefit from the utilization of multimodal data. We have used the UAV drone to collect and label the new person re-id dataset, which is composed of pedestrian images and its attributes. We have manually annotated this dataset with attributes, and in contrast to the recent research, we do not use the deep network to classify them. Instead, we employ the continuous bag-of-words model to extract the word embeddings from text descriptions and fuse it with features extracted from images. Then the deep neural decision forest is used for pedestrians classification. The extensive experiments on the collected dataset demonstrate the effectiveness of the proposed model.
C1 [Grigorev, Aleksei; Liu, Shaohui; Feng, Jiang] Harbin Inst Technol, Sch Comp Sci & Technol, 92 West Dazhi St, Harbin 15001, Heilongjiang, Peoples R China.
   [Grigorev, Aleksei; Liu, Shaohui; Feng, Jiang] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Tian, Zhihong] Guangzhou Univ, Cyberspace Inst Adv Technol, Guangzhou, Peoples R China.
   [Xiong, Jianxin] Beijing Inst Technol, Sch Comp, Beijing, Peoples R China.
   [Rho, Seungmin] Sejong Univ, Dept Software, Seoul, South Korea.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Guangzhou
   University; Beijing Institute of Technology; Sejong University
RP Grigorev, A (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, 92 West Dazhi St, Harbin 15001, Heilongjiang, Peoples R China.; Grigorev, A (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
EM grigorev@hit.edu.cn; shliu@hit.edu.cn; tianzhihong@gzhmedu.cn;
   xiong-jiaxin@163.com; smrho@sejong.ac.kr; fjiangl@hit.edu.cn
RI Liu, shaohui/HKE-1383-2023; Rho, Seungmin/HTP-6683-2023
CR [Anonymous], 2016, CORR
   [Anonymous], 2017, ARXIV170908325
   [Anonymous], 2015, ADV NEURAL INF PROCE
   [Anonymous], 2014, PROC EUR C COMPUT VI
   [Anonymous], 2017, ARXIV170909930
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   [Anonymous], 2017, ARXIV171110378
   [Anonymous], 2017, ARXIV171108565
   [Anonymous], 2017, arXiv
   Bengio Yann Le Cunand Yoshua, 1998, HDB BRAIN THEORY NEU, P255
   Bojanowski Piotr, 2016, ARXIV160604606V1
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chikontwe P, 2018, IEEE ACCESS, V6, P60801, DOI 10.1109/ACCESS.2018.2875783
   Ge Y.X., 2018, ARXIV181002936
   Gou MR, 2017, IEEE COMPUT SOC CONF, P1425, DOI 10.1109/CVPRW.2017.185
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsieh MR, 2017, IEEE I CONF COMP VIS, P4165, DOI 10.1109/ICCV.2017.446
   Huh Minyoung, 2016, ABS160808614 CORR
   Kingma D. P., 2014, arXiv
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leibe B., 2017, ARXIV170307737CS
   Li Shuang, 2017, IEEE C COMP VIS PATT
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li Wei, 2018, ARXIV180208122
   Liao Shengcai, 2015, PERSON REIDENTIFICAT, DOI [10.1109/CVPR.2015.7298832, DOI 10.1109/CVPR.2015.7298832]
   Lin Yutian, 2017, ARXIV170307220, DOI [10.1007/978-1-4471-6296-4_6, DOI 10.1007/978-1-4471-6296-4_6]
   Liu JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P737, DOI 10.1145/3240508.3240585
   Mikolov Tomas, 2013, P 26 INT C NEUR INF
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Qian X., 2017, ARXIV171202225
   Robicquet A, 2016, LECT NOTES COMPUT SC, V9912, P549, DOI 10.1007/978-3-319-46484-8_33
   Roy A, 2016, PROC CVPR IEEE, P5506, DOI 10.1109/CVPR.2016.594
   Taiqing Wang Shaogang, 2014, PERSON REIDENTIFICAT, DOI [10.1007/978-3-319-10593-2_45, DOI 10.1007/978-3-319-10593-2_45]
   Tan QF, 2019, IEEE INTERNET THINGS, V6, P1584, DOI 10.1109/JIOT.2018.2846624
   Tian ZH, 2019, IEEE T VEH TECHNOL, V68, P5971, DOI 10.1109/TVT.2019.2910217
   Tian ZH, 2019, INFORM SCIENCES, V491, P151, DOI 10.1016/j.ins.2019.04.011
   Tian ZH, 2019, IEEE T IND INFORM, V15, P4285, DOI 10.1109/TII.2019.2907754
   Tian ZH, 2019, FUTURE GENER COMP SY, V95, P212, DOI 10.1016/j.future.2018.12.054
   Wang Zheng, 2018, INCREMENTAL DEEP HID, DOI [10.1145/3240508.3240510, DOI 10.1145/3240508.3240510]
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Xiao Q., 2017, ARXIV171000478
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang SP, 2018, IEEE T INTELL TRANSP, V19, P187, DOI 10.1109/TITS.2017.2766093
   Zhang SP, 2017, IEEE T NEUR NET LEAR, V28, P2357, DOI 10.1109/TNNLS.2016.2586194
   Zhang SP, 2017, IEEE T CIRC SYST VID, V27, P421, DOI 10.1109/TCSVT.2016.2539860
   Zhang SP, 2015, IEEE T CIRC SYST VID, V25, P1749, DOI 10.1109/TCSVT.2015.2406194
   Zhang Xiangyu, 2017, CoRRabs/1711.08184
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Zhun, 2017, ARXIV171110295V1
   Zhu P., 2018, EUROPEAN C COMPUTER
NR 58
TC 10
Z9 11
U1 1
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 25
DI 10.1145/3360050
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300007
DA 2024-07-18
ER

PT J
AU Zhu, NJ
   Cao, J
   Shen, KW
   Chen, XS
   Zhu, SJ
AF Zhu, Nengjun
   Cao, Jian
   Shen, Kunwei
   Chen, Xiaosong
   Zhu, Siji
TI A Decision Support System with Intelligent Recommendation for
   Multi-disciplinary Medical Treatment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Decision support system; DSS; recommender system; medical oncology;
   breast cancer; MDT; medical guidelines; workflow engine; representation
   learning
ID CLINICAL-PRACTICE GUIDELINES; SUPPLIER SELECTION; PATIENT OUTCOMES; IBM
   WATSON; PERFORMANCE; MANAGEMENT; CANCER
AB Recent years have witnessed an emerging trend for improving disease treatment by forming multidisciplinary medical teams. The collaboration among specialists from multiple medical domains has been shown to be significantly helpful for designing comprehensive and reliable regimens, especially for incurable diseases. Although this kind of multi-disciplinary treatment has been increasingly adopted by healthcare providers, a new challenge has been introduced to the decision-making process-how to efficiently and effectively develop final regimens by searching for candidate treatments and considering inputs from every expert. In this article, we present a sophisticated decision support system called MdtDSS (a decision support system (DSS) for multi-disciplinary treatment (Mdt)), which is particularly developed to guide the collaborative decision-making in multi-disciplinary treatment scenarios. The system integrates a recommender system that aims to search for personalized candidates from a large-scale high-quality regimen pool and a voting system that helps collect feedback from multiple specialists without potential bias. Our decision support system optimally combines machine intelligence and human experience and helps medical practitioners make informed and accountable regimen decisions. We deployed the proposed system in a large hospital in Shanghai, China, and collected real-world data on large-scale patient cases. The evaluation shows that the proposed system achieves outstanding results in terms of high-quality multi-disciplinary treatment.
C1 [Zhu, Nengjun; Cao, Jian] Shanghai Jiao Tong Univ, Shanghai Inst Adv Commun & Data Sci, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Shen, Kunwei; Chen, Xiaosong; Zhu, Siji] Shanghai Jiao Tong Univ, Comprehens Breast Hlth Ctr, Ruijin Hosp, Sch Med, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Cao, J (corresponding author), Shanghai Jiao Tong Univ, Shanghai Inst Adv Commun & Data Sci, Dept Comp Sci & Engn, Shanghai, Peoples R China.
EM zhu_nj@sjtu.edu.cn; cao-jian@sjtu.edu.cn; kwshen@medmail.com.cn;
   chenxiaosong0156@hotmail.com; zsj_mu@yeah.net
OI Zhu, Nengjun/0000-0002-6146-9887
FU National Key Research and Development Plan [2018YFB1003800]; Cross
   Research Fund of Biomedical Engineering of Shanghai Jiaotong University
   [YG2015MS61]; Shanghai Municipal Education Commission Gaofeng Clinical
   Medicine [20172007]; Guangci Distinguished Young Scholars Training
   Program [GCQN-2017-A18, GCQN-2017-B08]
FX This work is partially supported by National Key Research and
   Development Plan (No. 2018YFB1003800), Cross Research Fund of Biomedical
   Engineering of Shanghai Jiaotong University (YG2015MS61), Shanghai
   Municipal Education Commission Gaofeng Clinical Medicine Grant Support
   (20172007), and Guangci Distinguished Young Scholars Training Program
   (GCQN-2017-A18, GCQN-2017-B08).
CR ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209
   [Anonymous], 2012, INTRO LINEAR REGRESS
   [Anonymous], 2012, P IBM CORP REDB
   [Anonymous], 2014, Clinical decisionsupport systems, DOI [10.1007/978-1-4471-4474-8_22, DOI 10.1007/978-1-4471-4474-822]
   [Anonymous], 2016, F1000RESEARCH
   Berner ES., 2007, CLIN DECISION SUPPOR, V233
   Berner ES, 2016, HEALTH INFORM SER, P1, DOI 10.1007/978-3-319-31913-1_1
   Boriah S., 2008, P 8 SIAM INT C DAT M, P243, DOI DOI 10.1137/1.9781611972788.22
   Cheeseman P., 1988, AAAI 88. Seventh National Conference on Artificial Intelligence, P607
   Chen L, 2006, IEICE T INF SYST, VE89D, P624, DOI 10.1093/ietisy/e89-d.2.624
   Chen Y, 2016, CLIN THER, V38, P688, DOI 10.1016/j.clinthera.2015.12.001
   Chesney T, 2017, DECIS SUPPORT SYST, V95, P110, DOI 10.1016/j.dss.2017.01.004
   Choi TM, 2013, DECIS SUPPORT SYST, V56, P131, DOI 10.1016/j.dss.2013.05.011
   Chozas AC, 2017, PROCEDIA COMPUT SCI, V108, P2121, DOI 10.1016/j.procs.2017.05.187
   De Fauw Jeffrey, 2016, F1000RESEARCH
   Fazi S, 2015, DECIS SUPPORT SYST, V79, P33, DOI 10.1016/j.dss.2015.08.001
   Garg AX, 2005, JAMA-J AM MED ASSOC, V293, P1223, DOI 10.1001/jama.293.10.1223
   Glancy FH, 2011, DECIS SUPPORT SYST, V50, P595, DOI 10.1016/j.dss.2010.08.010
   Gradishar WJ, 2018, J NATL COMPR CANC NE, V16, P310, DOI 10.6004/jnccn.2018.0012
   Gray RM, 2011, ENTROPY AND INFORMATION THEORY , SECOND EDITION, P395, DOI 10.1007/978-1-4419-7970-4
   Hagan Martin T., 1996, NEURAL NETW DES, V20
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Hong Z, 2013, DECIS SUPPORT SYST, V55, P67, DOI 10.1016/j.dss.2012.12.031
   Hu Liang, 2019, AAAI C ART INT AAAI
   Hunt DL, 1998, JAMA-J AM MED ASSOC, V280, P1339, DOI 10.1001/jama.280.15.1339
   Jula A, 2014, EXPERT SYST APPL, V41, P3809, DOI 10.1016/j.eswa.2013.12.017
   Kesson EM, 2012, BMJ-BRIT MED J, V344, DOI 10.1136/bmj.e2718
   Kim JI, 2015, AUTOMAT CONSTR, V58, P95, DOI 10.1016/j.autcon.2015.07.003
   KRAEMER KL, 1988, COMPUT SURV, V20, P115, DOI 10.1145/46157.46158
   Kristianto Y, 2012, DECIS SUPPORT SYST, V52, P790, DOI 10.1016/j.dss.2011.11.014
   MILLER RA, 1994, J AM MED INFORM ASSN, V1, P8, DOI 10.1136/jamia.1994.95236141
   MILNER R, 1978, J COMPUT SYST SCI, V17, P348, DOI 10.1016/0022-0000(78)90014-4
   Moja L, 2014, AM J PUBLIC HEALTH, V104, pE12, DOI 10.2105/AJPH.2014.302164
   Thong NT, 2015, EXPERT SYST APPL, V42, P3682, DOI 10.1016/j.eswa.2014.12.042
   Patel OP, 2021, IEEE T EMERG TOP COM, V9, P1031, DOI 10.1109/TETC.2019.2901272
   Ploskas N, 2015, LECT NOTES BUS INF P, V221, P26, DOI 10.1007/978-3-319-21536-5_3
   Power D., 2015, Decision Support Systems, Wiley Encyclopedia of Management
   Power D. J, 2008, HDB DECISION SUPPORT, V1, P121, DOI [DOI 10.1007/978-3-540-48713-5_7, 10.1007/978-3-540-48713-5_7]
   Powles J, 2017, HEALTH TECHNOL-GER, V7, P351, DOI 10.1007/s12553-017-0179-1
   Pradhan Sojen, 2016, P PAC AS C INF SYST, P328
   Sahebjamnia N, 2016, COMPUT IND ENG, V93, P215, DOI 10.1016/j.cie.2016.01.004
   Scott J, 2015, INT J PROD ECON, V166, P226, DOI 10.1016/j.ijpe.2014.11.008
   Taylor C, 2010, BRIT MED J, V340, DOI 10.1136/bmj.c951
   Ursavas E, 2014, DECIS SUPPORT SYST, V59, P312, DOI 10.1016/j.dss.2014.01.003
   Valluri A, 2005, DECIS SUPPORT SYST, V39, P219, DOI 10.1016/j.dss.2003.10.008
   Van Cutsem E, 2010, ANN ONCOL, V21, pv93, DOI 10.1093/annonc/mdq222
   van der Aalst WMP, 2016, BUS INFORM SYST ENG+, V58, P1, DOI 10.1007/s12599-015-0409-x
   Wang M., 2017, ARXIV171005980
   WHITLEY D, 1994, STAT COMPUT, V4, P65, DOI 10.1007/BF00175354
   Xu Y, 2017, BIOMED RES INT, V2017, DOI 10.1155/2017/1429018
   Zauderer Marjorie Glass, 2014, J CLIN ONCOL, P17653
   Zhang Y, 2017, FUTURE GENER COMP SY, V66, P30, DOI 10.1016/j.future.2015.12.001
   Zhu Nengjun, 2018, P CHIN C COMP SUPP C, P546
   Zliobaite I, 2016, STUD BIG DATA, V16, P91, DOI 10.1007/978-3-319-26989-4_4
NR 54
TC 7
Z9 7
U1 2
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 33
DI 10.1145/3352573
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300015
DA 2024-07-18
ER

PT J
AU Tasaka, S
AF Tasaka, Shuji
TI Causal Structures of Multidimensional QoE in Haptic-Audiovisual
   Communications: Bayesian Modeling
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Quality of experience; QoE; causation; correlation; SEM; CFA; Bayesian
   modeling; latent variables; haptic-audiovisual interactive
   communications; MCMC; OpenBUGS
ID EXPERIENCE; QUALITY
AB This article proposes a methodology for building and verifying plausible models that can express causation in multidimensional QoE for haptic-audiovisual interactive communications. For the modeling, we utilize subjective experimental data of five-point scores collected in a previous study where a pair of subjects carry out two kinds of interactive tasks (castanets hitting and object movement) in real space (not in virtual space). The multidimensional QoE is composed of 15 measures for the castanets hitting and 14 measures for the object movement. To reduce the dimension, we classify the QoE measures into three groups as indicators of three constructs (latent variables or factors): AVQ (AudioVisual Quality), HQ (Haptic Quality), and UXQ (User eXperience Quality). We then build two models: (1) a structural equation model in which AVQ and HQ correlated with each other give causal effects on UXQ, and (2) a confirmatory factor analysis model in which the three constructs are only correlated with each other. We refer to the former as 3C-SEM and the latter as 3C-CFA. We further introduce a CFA model with a single construct for which all QoE measures are its indicators (1C-CFA). We perform Bayesian analysis of the three models by means of Markov chain Monte Carlo simulation; in each model, the deviance information criterion is obtained for model comparison, and the posterior predictive p-value is calculated for model checking. As a result, we find that 3C-SEM is the most plausible and that HQ has a stronger causal effect on UXQ than AVQ. We also learn that the correlation between AVQ and UXQ is much higher than the direct causal effect and that the increase in the association as correlation is due to the causal effect of HQ on UXQ through the correlation of AVQ with HQ. Thus, it is suggested that improving haptic performance is more effective in enhancement of QoE than improving audiovisual performance.
C1 [Tasaka, Shuji] Nagoya Ind Sci Res Inst, Chikusa Ku, 1-13 Yotsuya Dori, Nagoya, Aichi 4640819, Japan.
RP Tasaka, S (corresponding author), Nagoya Ind Sci Res Inst, Chikusa Ku, 1-13 Yotsuya Dori, Nagoya, Aichi 4640819, Japan.
EM tasaka@nisri.jp
OI Tasaka, Shuji/0000-0002-2882-1914
FU JSPS KAKENHI [17K06454]; Grants-in-Aid for Scientific Research
   [17K06454] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI (Grant-In-Aid for Scientific
   Research of Japan Society for the Promotion of Science) grant number
   17K06454.
CR [Anonymous], 2012, IBM SPSS AMOS21 USER
   [Anonymous], 2001, Bayesian Statistical Modelling
   Bartholomew D, 2011, WILEY SER PROBAB ST, P1, DOI 10.1002/9781119970583
   Bollen K. A., 1989, STRUCTURAL EQUATIONS
   Brooks P, 2010, IEEE NETWORK, V24, P8, DOI 10.1109/MNET.2010.5430138
   Congdon P., 2014, Applied Bayesian modelling
   Hamam A., 2010, Haptic Audio-Visual Environments and Games (HAVE), 2010 IEEE International Symposium on, P1
   Hamam A, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540991
   Hamam A, 2013, MULTIMED TOOLS APPL, V67, P455, DOI 10.1007/s11042-012-0990-7
   Isomura E, 2013, 2013 IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE (CCNC), P196, DOI 10.1109/CCNC.2013.6488446
   Isomura Eiichi, 2013, IEICE T COMMUN, VJ96-B, P59
   Iwata K., 2010, Computers in Entertainment (CIE), V8, P1, DOI 10.1145/1899687.1899694
   Joreskog K. G., 1993, LISREL 8: Structural equation modeling with the SIMPLIS command language
   Kline R.B., 2016, Principles and Practice of Structural Equation Modeling, VFourth
   Laghari KUR, 2012, IEEE COMMUN MAG, V50, P58, DOI 10.1109/MCOM.2012.6178834
   Le Callet P., 2013, Qualinet White Paper on Definitions of Quality of Experience
   Levy R, 2016, CH CRC STAT SOC BEHA, P1, DOI 10.1201/9781315374604
   Loehlin JC., 2017, Latent variable models: an introduction to factor, path, and structural equation analysis', DOI [10.4324/9781315643199, DOI 10.4324/9781315643199]
   Lunn D. C. J., 2013, BUGS BOOK PRACTICAL
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Mulaik S.A., 2009, Linear causal modeling with structural equations
   OpenBUGS, 2015, OPENBUGS LIC
   Pearl J., 2009, CAUSALITY MODELS REA
   Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021
   Salisbury JK, 1997, IEEE COMPUT GRAPH, V17, P6, DOI 10.1109/MCG.1997.1626171
   Song XY, 2012, BASIC ADV BAYESIAN S
   Steinbach E, 2012, P IEEE, V100, P937, DOI 10.1109/JPROC.2011.2182100
   Tasaka Shuji, 2019, ICC 2019 - 2019 IEEE International Conference on Communications (ICC). Proceedings, DOI 10.1109/ICC.2019.8761784
   Tasaka S, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, P1880
   Tasaka S, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511202
   Tasaka S, 2017, IEEE T MULTIMEDIA, V19, P1195, DOI 10.1109/TMM.2017.2652064
   Tatematsu A., 2010, Communications Quality and Reliability (CQR), 2010 IEEE International Workshop Technical Committee on, P1, DOI [DOI 10.1109/CQR.2010.5619913, 10.1109/CQR.2010.5619913]
   Yamazaki Tatsuya, 2012, P 2012 IEEE 20 INT W
NR 33
TC 3
Z9 3
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 11
DI 10.1145/3375922
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100010
OA Bronze
DA 2024-07-18
ER

PT J
AU Li, MD
   Liu, JY
   Sun, XY
   Xiong, ZW
AF Li, Mading
   Liu, Jiaying
   Sun, Xiaoyan
   Xiong, Zhiwei
TI Image/Video Restoration via Multi planar Autoregressive Model and
   Low-Rank Optimization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image/video restoration; multiplanar autoregressive model; low-rank
   optimization; Markov random field
ID SPARSE REPRESENTATION; IMAGE INTERPOLATION; MATRIX COMPLETION; FILTER;
   ALGORITHM
AB In this article, we introduce an image/video restoration approach by utilizing the high-dimensional similarity in images/videos. After grouping similar patches from neighboring frames, we propose to build a multiplanar autoregressive (AR) model to exploit the correlation in cross-dimensional planes of the patch group, which has long been neglected by previous AR models. To further utilize the nonlocal self-similarity in images/videos, a joint multiplanar AR and low-rank based approach is proposed (MARLow) to reconstruct patch groups more effectively. Moreover, for video restoration, the temporal smoothness of the restored video is constrained by the Markov random field (MRF), where MRF encodes a priori knowledge about consistency of patches from neighboring frames. Specifically, we treat different restoration results (from different patch groups) of a certain patch as labels of an MRF, and temporal consistency among these restored patches is imposed. The proposed method is also suitable for other restoration applications such as interpolation and text removal. Extensive experimental results demonstrate that the proposed approach obtains encouraging performance comparing with state-of-the-art methods.
C1 [Li, Mading; Liu, Jiaying] Peking Univ, Wangxuan Inst Comp Technol, Zhongguancun North St 128, Beijing, Peoples R China.
   [Sun, Xiaoyan] Microsoft Res Asia, Danling St 5, Beijing, Peoples R China.
   [Xiong, Zhiwei] Univ Sci & Technol China, Dept EEIS, POB 4, Hefei 230027, Anhui, Peoples R China.
C3 Peking University; Microsoft; Microsoft Research Asia; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS
RP Liu, JY (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Zhongguancun North St 128, Beijing, Peoples R China.
EM martinli0822@pku.edu.cn; liujiaying@pku.edu.cn; xysun@microsoft.com;
   zwxiong@ustc.edu.cn
RI Liu, JY/GYJ-0138-2022
OI Liu, Jiaying/0000-0002-0468-9576
FU National Natural Science Foundation of China [61772043, 61671419];
   Beijing Natural Science Foundation [L182002, 4192025]
FX This work was supported in part by National Natural Science Foundation
   of China under contract nos. 61772043 and 61671419, and in part by the
   Beijing Natural Science Foundation under contract nos. L182002 and
   4192025.
CR Adobe Research, CONT AW FILL
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chen YL, 2014, IEEE T PATTERN ANAL, V36, P577, DOI 10.1109/TPAMI.2013.164
   Chierchia G, 2014, IEEE T IMAGE PROCESS, V23, P5531, DOI 10.1109/TIP.2014.2364141
   Dabov Kostadin, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P145
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1382, DOI 10.1109/TIP.2012.2231086
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P700, DOI 10.1109/TIP.2012.2221729
   Fadili MJ, 2009, COMPUT J, V52, P64, DOI 10.1093/comjnl/bxm055
   Goh WB, 1996, INT CONF ACOUST SPEE, P2275, DOI 10.1109/ICASSP.1996.545876
   Golbabaee M, 2012, INT CONF ACOUST SPEE, P2741, DOI 10.1109/ICASSP.2012.6288484
   He LT, 2014, IEEE T IMAGE PROCESS, V23, P5470, DOI 10.1109/TIP.2014.2362051
   Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149
   Hu W., 2015, ARXIV150902027
   Hu Y, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2903716
   Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jakhetiya V, 2017, IEEE T MULTIMEDIA, V19, P93, DOI 10.1109/TMM.2016.2609419
   Ji H, 2011, SIAM J IMAGING SCI, V4, P1122, DOI 10.1137/100817206
   Ji H, 2010, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2010.5539849
   Kokaram A., 1994, 1994 IEEE International Symposium on Circuits and Systems (Cat. No.94CH3435-5), P21, DOI 10.1109/ISCAS.1994.409091
   Li C, 2014, JOINT INT CONF SOFT, P1339, DOI 10.1109/SCIS-ISIS.2014.7044738
   Li Chao, 2015, MULTITENSOR COMPLETI
   Li MD, 2016, LECT NOTES COMPUT SC, V9911, P819, DOI 10.1007/978-3-319-46478-7_50
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Liao RJ, 2015, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2015.68
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   Liu J, 2009, IEEE I CONF COMP VIS, P2114
   Lorenzi L, 2013, IEEE T GEOSCI REMOTE, V51, P3998, DOI 10.1109/TGRS.2012.2227329
   Maggioni M, 2013, IEEE T IMAGE PROCESS, V22, P119, DOI 10.1109/TIP.2012.2210725
   Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Ono S, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2299067
   Rauhut H, 2008, IEEE T INFORM THEORY, V54, P2210, DOI 10.1109/TIT.2008.920190
   Romberg J, 2009, SIAM J IMAGING SCI, V2, P1098, DOI 10.1137/08072975X
   Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6
   Ruzic T, 2011, LECT NOTES COMPUT SC, V6915, P417, DOI 10.1007/978-3-642-23687-7_38
   Shen HF, 2014, IEEE T GEOSCI REMOTE, V52, P894, DOI 10.1109/TGRS.2013.2245509
   Sun L, 2012, INT C COMP PHOT, P1, DOI DOI 10.1109/ICCPHOT.2012.6215221
   Toh KKV, 2010, IEEE SIGNAL PROC LET, V17, P281, DOI 10.1109/LSP.2009.2038769
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Wang HY, 2017, IEEE T MULTIMEDIA, V19, P969, DOI 10.1109/TMM.2016.2638624
   Wang Hua, 2014, LOW RANK TENSOR COMP
   Zhang DB, 2012, PROC CVPR IEEE, P2192, DOI 10.1109/CVPR.2012.6247927
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang J, 2014, IEEE T CIRC SYST VID, V24, P915, DOI 10.1109/TCSVT.2014.2302380
   Zhang PX, 2014, IEEE SIGNAL PROC LET, V21, P1280, DOI 10.1109/LSP.2014.2333012
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhang ZM, 2014, PROC CVPR IEEE, P3842, DOI 10.1109/CVPR.2014.485
   Zhou D, 2012, IET IMAGE PROCESS, V6, P627, DOI 10.1049/iet-ipr.2011.0534
   Zhou MY, 2012, IEEE T IMAGE PROCESS, V21, P130, DOI 10.1109/TIP.2011.2160072
NR 56
TC 9
Z9 9
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 102
DI 10.1145/3341728
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800002
DA 2024-07-18
ER

PT J
AU Liu, ZD
   Zhou, WG
   Li, HQ
AF Liu, Zhandong
   Zhou, Wengang
   Li, Houqiang
TI AB-LSTM: Attention-based Bidirectional LSTM Model for Scene Text
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Scene text detection; bidirectional LSTM; feature fusion; attention;
   semantic segmentation
ID LOCALIZATION; RECOGNITION; COMPETITION
AB Detection of scene text in arbitrary shapes is a challenging task in the field of computer vision. Most existing scene text detection methods exploit the rectangle/quadrangular bounding box to denote the detected text, which fails to accurately fit text with arbitrary shapes, such as curved text. In addition, recent progress on scene text detection has benefited from Fully Convolutional Network. Text cues contained in multi-level convolutional features are complementary for detecting scene text objects. How to explore these multi-level features is still an open problem. To tackle the above issues, we propose an Attention-based Bidirectional Long Short-Term Memory (AB-LSTM) model for scene text detection. First, word stroke regions (WSRs) and text center blocks (TCBs) are extracted by two AB-LSTM models, respectively. Then, the union of WSRs and TCBs are used to represent text objects. To verify the effectiveness of the proposed method, we perform experiments on four public benchmarks: CTW1500, Total-text, ICDAR2013, and MSRA-TD500, and compare it with existing state-of-the-art methods. Experiment results demonstrate that the proposed method can achieve competitive results, and well handle scene text objects with arbitrary shapes (i.e., curved, oriented, and horizontal forms).
C1 [Liu, Zhandong; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Dept Elect Engn & Informat Sci, 443 Huangshan Rd, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Dept Elect Engn & Informat Sci, 443 Huangshan Rd, Hefei 230027, Peoples R China.
EM lzd0825@mail.ustc.edu.cn; zhwg@ustc.edu.cn; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
FU NSFC [61662082, U1703261, 61836011, 61822208, 61632019]; Youth
   Innovation Promotion Association CAS [2018497]
FX This work was supported in part to Dr. Houqian Li by NSFC under contract
   No. 61836011, and in part to Dr. Wengang Zhou by NSFC under contract No.
   61822208 and 61632019, and Youth Innovation Promotion Association CAS
   (No. 2018497), and in part to Zhandong Liu by NSFC under contract No.
   61662082 and U1703261.
CR [Anonymous], 2017, R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection
   [Anonymous], 2016, ACCURATE TEXT LOCALI
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bai Nong, 2016, ARXIV160609002
   Busta M, 2015, IEEE I CONF COMP VIS, P1206, DOI 10.1109/ICCV.2015.143
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He DF, 2017, PROC CVPR IEEE, P474, DOI 10.1109/CVPR.2017.58
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   He T, 2018, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR.2018.00527
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529
   Huang SY, 2017, DIS MARKERS, V2017, DOI 10.1155/2017/7962836
   Huang WL, 2013, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2013.157
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li X, 2018, Shape robust text detection with progressive scale expansion network
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Yuliang, 2017, DETECTING CURVE TEXT
   Liu ZD, 2019, MULTIMED TOOLS APPL, V78, P18205, DOI 10.1007/s11042-019-7177-4
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Long X, 2018, AAAI CONF ARTIF INTE, P7202
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Mehnert A, 1997, PATTERN RECOGN LETT, V18, P1065, DOI 10.1016/S0167-8655(97)00131-1
   Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237
   Neumann L, 2011, LECT NOTES COMPUT SC, V6494, P770, DOI 10.1007/978-3-642-19318-7_60
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shahab A, 2011, PROC INT CONF DOC, P1491, DOI 10.1109/ICDAR.2011.296
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wolf C, 2006, INT J DOC ANAL RECOG, V8, P280, DOI 10.1007/s10032-006-0014-0
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yang W, 2016, PROC CVPR IEEE, P3073, DOI 10.1109/CVPR.2016.335
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Yin XC, 2016, IEEE T IMAGE PROCESS, V25, P2752, DOI 10.1109/TIP.2016.2554321
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YY, 2016, FRONT COMPUT SCI-CHI, V10, P19, DOI 10.1007/s11704-015-4488-0
NR 54
TC 8
Z9 10
U1 1
U2 38
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 107
DI 10.1145/3356728
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800007
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Li, N
   Kwong, S
   Jiang, GY
   Zeng, HQ
AF Zhang, Yun
   Li, Na
   Kwong, Sam
   Jiang, Gangyi
   Zeng, Huanqiang
TI Statistical Early Termination and Early Skip Models for Fast Mode
   Decision in HEVC INTRA Coding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE HEVC; mode decision; intra coding; coding unit; intra angular
   prediction; early skip; early termination
ID CU SIZE DECISION; ALGORITHM; PREDICTION
AB In this article, statistical Early Termination (ET) and Early Skip (ES) models are proposed for fast Coding Unit (CU) and prediction mode decision in HEVC INTRA coding, in which three categories of ET and ES sub-algorithms are included. First, the CU ranges of the current CU are recursively predicted based on the texture and CU depth of the spatial neighboring CUs. Second, the statistical model based ET and ES schemes are proposed and applied to optimize the CU and INTRA prediction mode decision, in which the coding complexities over different decision layers are jointly minimized subject to acceptable rate-distortion degradation. Third, the mode correlations among the INTRA prediction modes are exploited to early terminate the full rate-distortion optimization in each CU decision layer. Extensive experiments are performed to evaluate the coding performance of each sub-algorithm and the overall algorithm. Experimental results reveal that the overall proposed algorithm can achieve 45.47% to 74.77%, and 58.09% on average complexity reduction, while the overall Bjontegaard delta bit rate increase and Bjontegaard delta peak signal-to-noise ratio degradation are 2.29% and -0.11 dB, respectively.
C1 [Zhang, Yun; Li, Na] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
   [Zeng, Huanqiang] Huaqiao Univ, Sch Informat Sci & Engn, Xiamen 361021, Fujian, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; City University of Hong Kong; Ningbo University; Huaqiao University
RP Zhang, Y (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM yun.zhang@siat.ac.cn; na.li1@siat.ac.cn; cssamk@cityu.edu.hk;
   jianggangyi@nbu.edu.cn; zeng0043@hqu.edu.cn
RI Zhang, Yun/V-7261-2019; Zeng, Huanqiang/U-2017-2018; jiang,
   gang/KII-8233-2024; Kwong, Sam/C-9319-2012
OI Zhang, Yun/0000-0001-9457-7801; Kwong, Sam/0000-0001-7484-7261
FU National Natural Science Foundation of China [61871372, 61672443,
   61871247]; Guangdong Natural Science Foundation for Distinguished Young
   Scholar [2016A030306022]; Shenzhen Science and Technology Development
   Project [JCYJ20170811160212033, JCYJ20180507183823045]; Shenzhen
   International Collaborative Research Project [GJHZ20170314155404913];
   RGC General Research Fund (GRF) [9042322, 9042489, CityU 11200116,
   11206317]; Guangdong International Science and Technology Cooperative
   Research Project [2018A050506063]; Key Project for Guangdong Provincial
   Science and Technology Development [2017B010110014]; Youth Innovation
   Promotion Association, Chinese Academy of Sciences [2018392]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61871372, 61672443, and 61871247, in
   part by Guangdong Natural Science Foundation for Distinguished Young
   Scholar under Grant 2016A030306022, in part by Shenzhen Science and
   Technology Development Project under Grant JCYJ20170811160212033,
   JCYJ20180507183823045, and Shenzhen International Collaborative Research
   Project under Grant GJHZ20170314155404913, in part by RGC General
   Research Fund (GRF) 9042322, 9042489 (CityU 11200116, 11206317), in part
   by Guangdong International Science and Technology Cooperative Research
   Project under Grant 2018A050506063 and Key Project for Guangdong
   Provincial Science and Technology Development under Grant
   2017B010110014, in part by Membership of Youth Innovation Promotion
   Association, Chinese Academy of Sciences, under Grant 2018392.
CR Abdelrasoul M, 2017, IET IMAGE PROCESS, V11, P888, DOI 10.1049/iet-ipr.2016.0514
   Alwani M, 2013, IEEE DATA COMPR CONF, P476, DOI 10.1109/DCC.2013.58
   [Anonymous], J NANOMATER
   Bichon M, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1733, DOI 10.1109/ICASSP.2018.8462489
   Chen F, 2018, MULTIMED TOOLS APPL, V77, P28375, DOI 10.1007/s11042-018-6011-8
   Chen J., 2016, P IEEE INT C MULTIME, P1
   Jamali M, 2019, IEEE T BROADCAST, V65, P109, DOI 10.1109/TBC.2018.2847464
   Lainema J, 2012, IEEE T CIRC SYST VID, V22, P1792, DOI 10.1109/TCSVT.2012.2221525
   Lei JJ, 2017, IEEE T BROADCAST, V63, P48, DOI 10.1109/TBC.2016.2623241
   Li Y, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3267128
   Lim K, 2015, IEEE T CIRC SYST VID, V25, P1335, DOI 10.1109/TCSVT.2014.2380194
   Liu XG, 2019, IEEE T CIRC SYST VID, V29, P144, DOI 10.1109/TCSVT.2017.2777903
   Liu XG, 2017, IEEE T CIRC SYST VID, V27, P1737, DOI 10.1109/TCSVT.2016.2556278
   Liu ZY, 2016, IEEE T IMAGE PROCESS, V25, P5088, DOI 10.1109/TIP.2016.2601264
   Luo F., 2017, P IEEE INT S CIRCUIT, P1
   Min B, 2015, IEEE T CIRC SYST VID, V25, P892, DOI 10.1109/TCSVT.2014.2363739
   Ryu S, 2018, IEEE T IMAGE PROCESS, V27, P5525, DOI 10.1109/TIP.2018.2857404
   Shang XW, 2015, IEEE IMAGE PROC, P1593, DOI 10.1109/ICIP.2015.7351069
   Shen LQ, 2018, IEEE T IMAGE PROCESS, V27, P4195, DOI 10.1109/TIP.2018.2837379
   Shen LQ, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700298
   Shen LQ, 2014, IEEE T IMAGE PROCESS, V23, P4232, DOI 10.1109/TIP.2014.2341927
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tseng CF, 2016, IET IMAGE PROCESS, V10, P215, DOI 10.1049/iet-ipr.2015.0154
   Wang LL, 2013, IEEE T CIRC SYST VID, V23, P1686, DOI 10.1109/TCSVT.2013.2255398
   Xu M, 2018, IEEE T IMAGE PROCESS, V27, P5044, DOI 10.1109/TIP.2018.2847035
   Zhang H, 2014, IEEE T CIRC SYST VID, V24, P660, DOI 10.1109/TCSVT.2013.2290578
   Zhang MM, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0237-7
   Zhang T, 2017, IEEE T CIRC SYST VID, V27, P1714, DOI 10.1109/TCSVT.2016.2556518
   Zhang Y, 2018, IEEE T CIRC SYST VID, V28, P3208, DOI 10.1109/TCSVT.2017.2747659
   Zhang Y, 2015, IEEE T IND INFORM, V11, P1492, DOI 10.1109/TII.2015.2491646
   Zhao WJ, 2015, IEEE T CIRC SYST VID, V25, P1651, DOI 10.1109/TCSVT.2015.2395751
NR 31
TC 15
Z9 15
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2019
VL 15
IS 3
AR 70
DI 10.1145/3321510
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ6YR
UT WOS:000494301200002
DA 2024-07-18
ER

PT J
AU Li, Y
   Yang, GB
   Zhu, YP
   Ding, XL
   Gong, RR
AF Li, Yue
   Yang, Gaobo
   Zhu, Yapei
   Ding, Xiangling
   Gong, Rongrong
TI Probability Model-Based Early Merge Mode Decision for Dependent Views
   Coding in 3D-HEVC
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D-HEVC; real-time applications; Merge mode; Early mode decision; Priori
   and posterior probabilities
ID MULTIVIEW; MOTION; OPTIMIZATION
AB As a 3D extension to the High Efficiency Video Coding (HEVC) standard, 3D-HEVC was developed to improve the coding efficiency of multiview videos. It inherits the prediction modes from HEVC, yet both Motion Estimation (ME) and Disparity Estimation (DE) are required for dependent views coding. This improves coding efficiency at the cost of huge computational costs. In this article, an early Merge mode decision approach is proposed for dependent texture views and dependent depth maps coding in 3D-HEVC based on priori and posterior probability models. First, the priori probability model is established by exploiting the hierarchical and interview correlations from those previously encoded blocks. Second, the posterior probability model is built by using the Coded Block Flag (CBF) of the current coding block. Finally, the joint priori and posterior probability model is adopted to early terminate the Merge mode decision for both dependent texture views and dependent depth maps coding. Experimental results show that the proposed approach saves 45.2% and 30.6% encoding time on average for dependent texture views and dependent depth maps coding while maintaining negligible loss of coding efficiency, respectively.
C1 [Li, Yue; Yang, Gaobo] Hunan Univ, Lushang Rd, Changsha 410082, Hunan, Peoples R China.
   [Zhu, Yapei] Hengyang Normal Univ, Huangbai Rd, Hengyang 421008, Peoples R China.
   [Ding, Xiangling] Jishou Univ, Renming Rd, Jishou 416000, Peoples R China.
   [Gong, Rongrong] Changsha Social Work Coll, Xiangzhang Rd, Changsha 410004, Hunan, Peoples R China.
C3 Hunan University; Hengyang Normal University; Jishou University;
   Changsha Social Work College
RP Li, Y (corresponding author), Hunan Univ, Lushang Rd, Changsha 410082, Hunan, Peoples R China.
EM yueli@hnu.edu.cn; yanggaobo@hnu.edu.cn; zyp1016@hynu.edu.cn;
   xianglingding@163.com; rongronggong@163.com
RI xiangling, Ding/T-7175-2019
OI Yang, Gaobo/0000-0003-2734-659X; ding, xiangling/0000-0002-6581-4633
FU National Key R&D Program of China [2018YFB1003205]; National Natural
   Science Foundation of China [61572183]
FX This work is supported in part by the National Key R&D Program of China
   (No. 2018YFB1003205) and National Natural Science Foundation of China
   (61572183).
CR [Anonymous], 2001, SC16Q6 ITUT
   Chen HC, 2017, ACSR ADV COMPUT, V74, P1
   Ding XL, 2019, MULTIMED TOOLS APPL, V78, P7453, DOI 10.1007/s11042-018-6504-5
   Hu Q, 2016, IEEE T MULTIMEDIA, V18, P379, DOI 10.1109/TMM.2015.2512799
   Jung SH, 2016, IEEE T CIRC SYST VID, V26, P1846, DOI 10.1109/TCSVT.2015.2473303
   Lei JJ, 2018, IEEE T CIRC SYST VID, V28, P706, DOI 10.1109/TCSVT.2016.2617332
   Li Y, 2017, IEEE T BROADCAST, V63, P535, DOI 10.1109/TBC.2017.2704423
   Li Y, 2017, IEEE T MULTIMEDIA, V19, P1431, DOI 10.1109/TMM.2017.2669863
   Li Y, 2016, J REAL-TIME IMAGE PR, V12, P575, DOI 10.1007/s11554-015-0527-1
   Li Y, 2016, IEEE T BROADCAST, V62, P700, DOI 10.1109/TBC.2016.2570018
   Muller K., 2011, JCT3VG1100
   Pan ZQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159170
   Pan ZQ, 2016, IET IMAGE PROCESS, V10, P9, DOI 10.1049/iet-ipr.2014.1018
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Pan ZQ, 2014, IEEE T BROADCAST, V60, P405, DOI 10.1109/TBC.2014.2321682
   Shen LQ, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700298
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2011, IEEE T CIRC SYST VID, V21, P837, DOI 10.1109/TCSVT.2011.2130310
   Shen LQ, 2010, SIGNAL PROCESS-IMAGE, V25, P88, DOI 10.1016/j.image.2009.11.003
   Shen LQ, 2009, IEEE T BROADCAST, V55, P761, DOI 10.1109/TBC.2009.2030453
   Song YX, 2015, J VIS COMMUN IMAGE R, V33, P60, DOI 10.1016/j.jvcir.2015.07.001
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tariq J, 2017, J VIS COMMUN IMAGE R, V44, P198, DOI 10.1016/j.jvcir.2017.01.029
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Vetro A, 2011, IEEE T BROADCAST, V57, P384, DOI 10.1109/TBC.2010.2102950
   Wang FS, 2013, SIGNAL PROCESS-IMAGE, V28, P736, DOI 10.1016/j.image.2013.05.003
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xia M, 2017, MULTIMED TOOLS APPL, V76, P8399, DOI 10.1007/s11042-016-3468-1
   Yang J., 2011, JCTVCG543
   Zeng HQ, 2014, IEEE T CIRC SYST VID, V24, P1566, DOI 10.1109/TCSVT.2014.2310143
   Zhang DY, 2017, J VIS COMMUN IMAGE R, V48, P281, DOI 10.1016/j.jvcir.2017.07.006
   Zhang JL, 2016, IEEE T CIRC SYST VID, V26, P1502, DOI 10.1109/TCSVT.2015.2461991
   Zhang N, 2014, SIGNAL PROCESS-IMAGE, V29, P951, DOI 10.1016/j.image.2014.06.003
   Zhang QY, 2017, OXID MED CELL LONGEV, V2017, DOI 10.1155/2017/7150376
   Zhang QW, 2017, J VIS COMMUN IMAGE R, V45, P170, DOI 10.1016/j.jvcir.2017.03.004
   Zhang QW, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.5.053017
   Zhang Y, 2013, IEEE T BROADCAST, V59, P390, DOI 10.1109/TBC.2013.2253033
   Zhang Y, 2012, IEEE T BROADCAST, V58, P10, DOI 10.1109/TBC.2011.2174282
   Zhao TS, 2013, IEEE T IMAGE PROCESS, V22, P1596, DOI 10.1109/TIP.2012.2235451
   Zhao WJ, 2015, IEEE T CIRC SYST VID, V25, P1651, DOI 10.1109/TCSVT.2015.2395751
NR 42
TC 11
Z9 13
U1 1
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2018
VL 14
IS 4
AR 85
DI 10.1145/3267128
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JB
UT WOS:000457139200007
DA 2024-07-18
ER

PT J
AU Afzal, H
   Aouada, D
   Mirbach, B
   Ottersten, B
AF Afzal, Hassan
   Aouada, Djamila
   Mirbach, Bruno
   Ottersten, Bjorn
TI Full 3D Reconstruction of Non-Rigidly Deforming Objects
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D reconstruction; super-resolution; non-rigid registration; point-cloud
   enhancement; 3D bilateral total variation; 3D point tracking
ID SHAPE; ANIMATION; REGISTRATION; CAPTURE; MOTION
AB In this article, we discuss enhanced full 360 degrees 3D reconstruction of dynamic scenes containing non-rigidly deforming objects using data acquired from commodity depth or 3D cameras. Several approaches for enhanced and full 3D reconstruction of non-rigid objects have been proposed in the literature. These approaches suffer from several limitations due to requirement of a template, inability to tackle large local deformations and topology changes, inability to tackle highly noisy and low-resolution data, and inability to produce online results. We target online and template-free enhancement of the quality of noisy and low-resolution full 3D reconstructions of dynamic non-rigid objects. For this purpose, we propose a view-independent recursive and dynamic multi-frame 3D super-resolution scheme for noise removal and resolution enhancement of 3D measurements. The proposed scheme tracks the position and motion of each 3D point at every timestep by making use of the current acquisition and the result of the previous iteration. The effects of system blur due to per-point tracking are subsequently tackled by introducing a novel and efficient multi-level 3D bilateral total variation regularization. These characteristics enable the proposed scheme to handle large deformations and topology changes accurately. A thorough evaluation of the proposed scheme on both real and simulated data is carried out. The results show that the proposed scheme improves upon the performance of the state-of-the-art methods and is able to accurately enhance the quality of low-resolution and highly noisy 3D reconstructions while being robust to large local deformations.
C1 [Afzal, Hassan; Aouada, Djamila; Ottersten, Bjorn] Univ Luxembourg, Interdisciplinary Ctr Secur Reliabil & Trust SnT, L-1855 Luxembourg, Luxembourg.
   [Mirbach, Bruno] IEE SA, Adv Engn Dept, L-5326 Contern, Luxembourg.
C3 University of Luxembourg
RP Aouada, D (corresponding author), Univ Luxembourg, Interdisciplinary Ctr Secur Reliabil & Trust SnT, L-1855 Luxembourg, Luxembourg.
EM hassan.afzal@uni.lu; djamila.aouada@uni.lu; bruno.mirbach@iee.lu;
   bjorn.ottersten@uni.lu
RI Ottersten, Bjorn/AAF-9147-2019; Ottersten, Bjorn/G-1005-2011
OI Aouada, Djamila/0000-0002-7576-2064; Ottersten,
   Bjorn/0000-0003-2298-6774
FU National Research Fund, Luxembourg under the CORE Project
   [C11/BM/1204105/FAVE/Ottersten]
FX The article was supported by the National Research Fund, Luxembourg
   under the CORE Project No. C11/BM/1204105/FAVE/Ottersten.
CR Afzal H, 2015, LECT NOTES COMPUT SC, V9257, P712, DOI 10.1007/978-3-319-23117-4_61
   Afzal H, 2014, 2014 2ND INTERNATIONAL CONFERENCE ON 3D VISION, VOL. 2, P7, DOI 10.1109/3DV.2014.114
   Afzal H, 2014, INT C PATT RECOG, P2459, DOI 10.1109/ICPR.2014.425
   Al Ismaeil Kassem, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P8, DOI 10.1109/CVPRW.2015.7301389
   Al Ismaeil K, 2017, IEEE T PATTERN ANAL, V39, P2045, DOI 10.1109/TPAMI.2016.2622698
   Al Ismaeil K, 2016, COMPUT VIS IMAGE UND, V147, P38, DOI 10.1016/j.cviu.2016.04.006
   Al Ismaeil K, 2013, IEEE IMAGE PROC, P660, DOI 10.1109/ICIP.2013.6738136
   Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2014, KIN
   [Anonymous], 2009, 4D REPOSITORY
   [Anonymous], 2014, ACM T GRAPHIC, DOI DOI 10.1145/2601097.2601165
   [Anonymous], 2007, Symposium on Geometry Processing
   Aouada Djamila, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P186
   Aouada D, 2014, 2014 11TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P107, DOI 10.1109/AVSS.2014.6918652
   Asus, 2010, XTION PRO LIVE
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   Berretti S, 2014, IEEE T INF FOREN SEC, V9, P1436, DOI 10.1109/TIFS.2014.2337258
   Bondi E, 2016, IEEE T INF FOREN SEC, V11, P2843, DOI 10.1109/TIFS.2016.2601059
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Cagniart C, 2010, LECT NOTES COMPUT SC, V6314, P326, DOI 10.1007/978-3-642-15561-1_24
   Carceroni RL, 2002, INT J COMPUT VISION, V49, P175, DOI 10.1023/A:1020145606604
   Chang W, 2009, COMPUT GRAPH FORUM, V28, P447, DOI 10.1111/j.1467-8659.2009.01384.x
   Cui Y, 2013, IEEE T PATTERN ANAL, V35, P1039, DOI 10.1109/TPAMI.2012.190
   Dai A., 2017, ACM Transactions on Graphics (ToG), V36, DOI DOI 10.1145/3054739
   de Aguiar E, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360697
   Dou MS, 2014, 2014 IEEE VIRTUAL REALITY (VR), P39, DOI 10.1109/VR.2014.6802048
   Dou MS, 2013, INT SYM MIX AUGMENT, P99, DOI 10.1109/ISMAR.2013.6671769
   Feng Andrew, 2014, ACM SIGGRAPH 2014 TA, DOI 10.1145/2614106.2614182
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Furukawa Y, 2010, GEOM COMPUT, V5, P193, DOI 10.1007/978-3-642-12392-4_9
   Garg R, 2013, PROC CVPR IEEE, P1272, DOI 10.1109/CVPR.2013.168
   Goldstein T., 2014, CoRR
   Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kheradmand A, 2014, IEEE T IMAGE PROCESS, V23, P5136, DOI 10.1109/TIP.2014.2362059
   Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x
   Li H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508407
   Li WS, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-222
   Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6
   Malleson C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P307, DOI 10.1109/ICCVW.2013.48
   Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Ofli F, 2013, IEEE WORK APP COMP, P53, DOI 10.1109/WACV.2013.6474999
   Olsson C, 2012, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2012.6247849
   PMD Technologies, 2012, CAMB NAN
   Rohmer E, 2013, IEEE INT C INT ROBOT, P1321, DOI 10.1109/IROS.2013.6696520
   Roth H, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.112
   Schuon S, 2009, PROC CVPR IEEE, P343, DOI 10.1109/CVPRW.2009.5206804
   Sharf A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409063
   Süssmuth J, 2008, COMPUT GRAPH FORUM, V27, P1469, DOI 10.1111/j.1467-8659.2008.01287.x
   Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531
   Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0
   Theoloalt C, 2007, IEEE T VIS COMPUT GR, V13, P663, DOI 10.1109/TVCG.2007.1006
   Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Vlasic Daniel, 2009, ACM TRANSACTIONS ON GRAPHICS, V28, P2, DOI [DOI 10.1145/1618452.1618520, 10.1145/1618452.1618520]
   Wand M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1516522.1516526
   Wand Michael, 2007, ACM International Conference Proceeding Series, V257, P49, DOI 10.2312/SGP/SGP07/049-058
   Wasenmuller Oliver, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P5
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
   Xu WP, 2015, IEEE I CONF COMP VIS, P2183, DOI 10.1109/ICCV.2015.252
   Yan Cui, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P133, DOI 10.1007/978-3-642-37484-5_12
   Ye GZ, 2012, LECT NOTES COMPUT SC, V7573, P828, DOI 10.1007/978-3-642-33709-3_59
   Yu R, 2015, IEEE I CONF COMP VIS, P918, DOI 10.1109/ICCV.2015.111
   Zeng M, 2013, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2013.26
   Zhang Q, 2014, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2014.92
   Zheng Q, 2010, COMPUT GRAPH FORUM, V29, P635, DOI 10.1111/j.1467-8659.2009.01633.x
NR 70
TC 5
Z9 5
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2018
VL 14
IS 1
SU S
AR 24
DI 10.1145/3177756
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZK
UT WOS:000433517100010
OA Green Published
DA 2024-07-18
ER

PT J
AU Huang, S
   Wang, WQ
   He, SF
   Lau, RWH
AF Huang, Shao
   Wang, Weiqiang
   He, Shengfeng
   Lau, Rynson W. H.
TI Egocentric Hand Detection Via Dynamic Region Growing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Egocentric videos; egocentric hand detection; seed region generation;
   hand region growing
ID VIDEO; GAZE
AB Egocentric videos, which mainly record the activities carried out by the users of wearable cameras, have drawn much research attention in recent years. Due to its lengthy content, a large number of ego-related applications have been developed to abstract the captured videos. As the users are accustomed to interacting with the target objects using their own hands, while their hands usually appear within their visual fields during the interaction, an egocentric hand detection step is involved in tasks like gesture recognition, action recognition, and social interaction understanding. In this work, we propose a dynamic region-growing approach for hand region detection in egocentric videos, by jointly considering hand-related motion and egocentric cues. We first determine seed regions that most likely belong to the hand, by analyzing the motion patterns across successive frames. The hand regions can then be located by extending from the seed regions, according to the scores computed for the adjacent superpixels. These scores are derived from four egocentric cues: contrast, location, position consistency, and appearance continuity. We discuss how to apply the proposed method in real-life scenarios, where multiple hands irregularly appear and disappear from the videos. Experimental results on public datasets show that the proposed method achieves superior performance compared with the state-of-the-art methods, especially in complicated scenarios.
C1 [Huang, Shao; Wang, Weiqiang] Univ Chinese Acad Sci, Sch Comp & Control Engn, Comp Vis & Multimedia Technol Lab, Beijing, Peoples R China.
   [Huang, Shao] City Univ Hong Kong, Kowloon, Hong Kong, Peoples R China.
   [He, Shengfeng] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou Higher Educ Mega Ctr, Guangzhou 510640, Guangdong, Peoples R China.
   [Lau, Rynson W. H.] City Univ Hong Kong, Dept Comp Sci, Tat Chee Ave, Kowloon, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; City University of Hong Kong; South China University of Technology;
   City University of Hong Kong
RP Wang, WQ (corresponding author), Univ Chinese Acad Sci, Sch Comp & Control Engn, Comp Vis & Multimedia Technol Lab, Beijing, Peoples R China.
EM huangshao11@mails.ucas.ac.cn; wqwang@ucas.ac.cn; hesfe@scut.edu.cn;
   Rynson.Lau@cityu.edu.hk
RI He, Shengfeng/E-5682-2016
OI He, Shengfeng/0000-0002-3802-4644
FU National Key Research and Development Program of China [2017YFB1002203];
   National Natural Science Foundation of China [61772495, 61232013,
   61702194]; Beijing Advanced Innovation Center for Imaging Technology
   [BAICIT-2016009]; SRG grant from City University of Hong [7004889]
FX The work was supported in part by the National Key Research and
   Development Program of China (No. 2017YFB1002203), by the National
   Natural Science Foundation of China (No. 61772495, 61232013, and
   61702194), by Beijing Advanced Innovation Center for Imaging Technology
   (No. BAICIT-2016009), and by a SRG grant from City University of Hong
   (No. 7004889). We would like to thank NVIDIA for the generous donation
   of Titan X Pascal GPU cards used in this work.
CR [Anonymous], ARXIV150102825
   [Anonymous], IEEE TPAMI
   [Anonymous], ARXIV161202742
   Bambach S, 2015, IEEE I CONF COMP VIS, P1949, DOI 10.1109/ICCV.2015.226
   Baraldi L, 2014, IEEE COMPUT SOC CONF, P702, DOI 10.1109/CVPRW.2014.107
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Betancourt A., 2014, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P586
   Betancourt A, 2015, IEEE T CIRC SYST VID, V25, P744, DOI 10.1109/TCSVT.2015.2409731
   Cevikalp H., 2013, Proceedings of the 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, P1
   del Molino AG, 2017, IEEE T HUM-MACH SYST, V47, P65, DOI 10.1109/THMS.2016.2623480
   Dominguez SM, 2006, IEEE T MULTIMEDIA, V8, P956, DOI 10.1109/TMM.2006.879872
   Fathi A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3281, DOI 10.1109/CVPR.2011.5995444
   Fathi A, 2012, LECT NOTES COMPUT SC, V7572, P314, DOI 10.1007/978-3-642-33718-5_23
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Genç S, 2015, SIGNAL IMAGE VIDEO P, V9, P1717, DOI 10.1007/s11760-014-0631-x
   Hayman E, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P67
   Hoshen Y, 2016, PROC CVPR IEEE, P4284, DOI 10.1109/CVPR.2016.464
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kolsch M., 2004, Computer Vision and Pattern Recognition Workshop, P158
   Kumano S, 2017, IEEE T MULTIMEDIA, V19, P107, DOI 10.1109/TMM.2016.2608002
   Kumar Jayant, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P18, DOI 10.1109/CVPRW.2015.7301344
   Lee Stefan., 2014, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P543
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Li C, 2013, IEEE I CONF COMP VIS, P2624, DOI 10.1109/ICCV.2013.326
   Li C, 2013, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2013.458
   Li Y, 2015, PROC CVPR IEEE, P287, DOI 10.1109/CVPR.2015.7298625
   Li Y, 2013, IEEE I CONF COMP VIS, P3216, DOI 10.1109/ICCV.2013.399
   Liang H, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P743, DOI 10.1145/2733373.2807972
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Pirsiavash H, 2012, PROC CVPR IEEE, P2847, DOI 10.1109/CVPR.2012.6248010
   Ren XF, 2010, PROC CVPR IEEE, P3137, DOI 10.1109/CVPR.2010.5540074
   Rogez G, 2015, PROC CVPR IEEE, P4325, DOI 10.1109/CVPR.2015.7299061
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sheikh Y, 2009, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2009.5459334
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang J, 2016, PROC CVPR IEEE, P2295, DOI 10.1109/CVPR.2016.252
   Xiong B, 2014, LECT NOTES COMPUT SC, V8693, P282, DOI 10.1007/978-3-319-10602-1_19
NR 39
TC 7
Z9 9
U1 1
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2018
VL 14
IS 1
AR 10
DI 10.1145/3152129
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW9HO
UT WOS:000425646500010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, L
AF Zhou, Liang
TI Mobile Device-to-Device Video Distribution: Theory and Application
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Performance; Smartphone; video distribution;
   device-to-device; application
ID UNDERLAYING CELLULAR NETWORKS; DISSEMINATION; OPTIMIZATION
AB As video traffic has dominated the data flow of smartphones, traditional cellular communications face substantial transmission challenges. In this work, we study mobile device-to-device (D2D) video distribution that leverages the storage and communication capacities of smartphones. In such a mobile distributed framework, D2D communication represents an opportunistic process to selectively store and transmit local videos to meet the future demand of others. The performance is measured by the service time, which denotes the elapsed period for fulfilling the demand, and the corresponding implementation of each device depends on the video's demand, availability, and size. The main contributions of this work lie in (1) considering the impact of video size in a practical mobile D2D video distribution scenario and proposing a general global estimation of the video distribution based on limited and local observations; (2) designing a purely distributed D2D video distribution scheme without the monitoring of any central controller; and (3) providing a practical implementation of the scheme, which does not need to know the video availability, user demand, and device mobility. Numerical results have demonstrated the efficiency and robustness of the proposed scheme.
C1 [Zhou, Liang] Nanjing Univ Posts & Telecommun, Minist Educ, Key Lab Broadband Wireless Commun & Sensor Networ, POB 214, Nanjing 210003, Jiangsu, Peoples R China.
   [Zhou, Liang] Nanjing Univ Posts & Telecommun, Jiangsu High Technol Res Key Lab Wireless Sensor, POB 214, Nanjing 210003, Jiangsu, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Posts & Telecommunications
RP Zhou, L (corresponding author), Nanjing Univ Posts & Telecommun, Minist Educ, Key Lab Broadband Wireless Commun & Sensor Networ, POB 214, Nanjing 210003, Jiangsu, Peoples R China.; Zhou, L (corresponding author), Nanjing Univ Posts & Telecommun, Jiangsu High Technol Res Key Lab Wireless Sensor, POB 214, Nanjing 210003, Jiangsu, Peoples R China.
EM liang.zhou@njupt.edu.cn
FU State Key Development Program of Basic Research of China [2013CB329005];
   National Natural Science Foundation of China [61571240, 61322104,
   61201165, 61271240]; Priority Academic Program Development of Jiangsu
   Higher Education Institutions; Nanjing University of Posts and
   Telecommunications Foundation [NY211032]
FX This work is partly supported by the State Key Development Program of
   Basic Research of China (2013CB329005), the National Natural Science
   Foundation of China (Grants No. 61571240, No. 61322104, No. 61201165,
   and No. 61271240), the Priority Academic Program Development of Jiangsu
   Higher Education Institutions, and Nanjing University of Posts and
   Telecommunications Foundation (Grant No. NY211032).
CR [Anonymous], 2015, CHINA MOBILE REPORT
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chen HL, 2014, IEEE COMMUN LETT, V18, P1395, DOI 10.1109/LCOMM.2014.2326852
   Ding Aaron Yi, 2013, 2013 IEEE International Conference on Sensing, Communications and Networking (SECON), P487, DOI 10.1109/SAHCN.2013.6645020
   Golrezaei N, 2014, IEEE T WIREL COMMUN, V13, P3665, DOI 10.1109/TWC.2014.2316817
   Golrezaei N, 2014, IEEE T INFORM THEORY, V60, P4286, DOI 10.1109/TIT.2014.2319312
   Ji M., 2013, Proc. of IEEE ITW, P1
   Joshua Reich and Augustin Chaintreau, 2009, P ACM C EM NETW EXP, P226, DOI [10.1145/1658939.1658950, DOI 10.1145/1658939.1658950]
   Kleinrock L., 1976, Queuing Systems, V2
   Lei L, 2014, IEEE COMMUN MAG, V52, P108, DOI 10.1109/MCOM.2014.6829952
   Li Y, 2014, IEEE T WIREL COMMUN, V13, P3596, DOI 10.1109/TWC.2014.2315807
   Li Y, 2014, IEEE T MOBILE COMPUT, V13, P1579, DOI 10.1109/TMC.2013.61
   Li YJ, 2014, IEEE T WIREL COMMUN, V13, P3978, DOI 10.1109/TWC.2014.2317703
   Lin XQ, 2014, IEEE T WIREL COMMUN, V13, P4346, DOI 10.1109/TWC.2014.2320522
   Liu Q., 2013, Neural Networks (IJCNN), The 2013 International Joint Conference on, P1, DOI DOI 10.1109/ICME.2013.6607563
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Wang Q, 2015, IEEE T VEH TECHNOL, V64, P3755, DOI 10.1109/TVT.2014.2355594
   Wang QS, 2009, FIFTEENTH ACM INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM 2009), P121
   Wang XF, 2014, IEEE COMMUN MAG, V52, P131, DOI 10.1109/MCOM.2014.6736753
   Wang Z, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2523001.2523005
   Wei LL, 2014, IEEE COMMUN MAG, V52, P90, DOI 10.1109/MCOM.2014.6829950
   Wu D, 2014, IEEE T VEH TECHNOL, V63, P2093, DOI 10.1109/TVT.2014.2311580
   Wu D, 2011, IEEE T VEH TECHNOL, V60, P2620, DOI 10.1109/TVT.2011.2153219
   Xu CQ, 2013, IEEE T VEH TECHNOL, V62, P2273, DOI 10.1109/TVT.2012.2228682
   Yu CH, 2011, IEEE T WIREL COMMUN, V10, P2752, DOI 10.1109/TWC.2011.060811.102120
   Zhao J, 2008, IEEE T VEH TECHNOL, V57, P1910, DOI 10.1109/TVT.2007.901869
   Zheng ZJ, 2014, IEEE CONF COMPUT, P219, DOI 10.1109/INFCOMW.2014.6849234
   Zhou L, 2012, IEEE T COMMUN, V60, P2017, DOI [10.1109//TCOMM.2012.051712.110165, 10.1109/TCOMM.2012.051712.110165]
   Zhou L, 2011, IEEE T MULTIMEDIA, V13, P1040, DOI 10.1109/TMM.2011.2160716
   Zhou LA, 2011, IEEE T VEH TECHNOL, V60, P692, DOI 10.1109/TVT.2010.2102782
NR 30
TC 55
Z9 57
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2016
VL 12
IS 3
AR 38
DI 10.1145/2886776
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YO
UT WOS:000379425400004
DA 2024-07-18
ER

PT J
AU Zhu, B
   Zhang, HX
   Chen, W
   Xia, F
   Maciejewski, R
AF Zhu, Biao
   Zhang, Hongxin
   Chen, Wei
   Xia, Feng
   Maciejewski, Ross
TI ShotVis: Smartphone-Based Visualization of OCR Information from Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cyber-physical interaction; data visualization; touch-based interface;
   interaction; smartphone; wearable computing
ID MOBILE; SYSTEM
AB While visualization has been widely used as a data presentation tool in both desktop and mobile devices, the rapid visualization of information from images is still underexplored. In this work, we present a smartphone image acquisition and visualization approach for text-based data. Our prototype, ShotVis, takes images of text captured from mobile devices and extracts information for visualization. First, scattered characters in the text are processed and interactively reformulated to be stored as structured data (i.e., tables of numbers, lists of words, sentences). From there, ShotVis allows users to interactively bind visual forms to the underlying data and produce visualizations of the selected forms through touch-based interactions. In this manner, ShotVis can quickly summarize text from images into word clouds, scatterplots, and various other visualizations all through a simple click of the camera. In this way, ShotVis facilitates the interactive exploration of text data captured via cameras in smartphone devices. To demonstrate our prototype, several case studies are presented along with one user study to demonstrate the effectiveness of our approach.
C1 [Zhu, Biao; Zhang, Hongxin; Chen, Wei] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
   [Xia, Feng] Dalian Univ Technol, Sch Software, Dalian, Peoples R China.
   [Maciejewski, Ross] Arizona State Univ, Tempe, AZ 85287 USA.
C3 Zhejiang University; Dalian University of Technology; Arizona State
   University; Arizona State University-Tempe
RP Zhu, B (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
EM arthurbzhu@gmail.com; zhx@cad.zju.edu.cn; chenwei@cad.zju.edu.cn;
   f.xia@ieee.org; rmacieje@asu.edu
RI Xia, Feng/Y-2859-2019; Chen, Wei/AAR-9817-2020; Zhang,
   Hongxin/T-3714-2019
OI Xia, Feng/0000-0002-8324-1859; 
FU Major Program of the National Natural Science Foundation of China
   [61232012]; National Natural Science Foundation of China [61422211];
   Fundamental Research Funds for the Central Universities; Major Program
   of the Natural Science Foundation of Zhejiang province, China
   [LZ12F02002]; US National Science Foundation [1350573]
FX This work is supported by the Major Program of the National Natural
   Science Foundation of China (under grant 61232012), the National Natural
   Science Foundation of China (under grant 61422211), the Fundamental
   Research Funds for the Central Universities, the Major Program of the
   Natural Science Foundation of Zhejiang province, China (under grant
   LZ12F02002) and the US National Science Foundation (under grant
   1350573).
CR [Anonymous], 2011, P 24 ANN ACM S US IN
   Burigat S., 2006, P 8 C HUM COMP INT M, P239
   Buttussi F, 2008, ARTIF INTELL MED, V42, P153, DOI 10.1016/j.artmed.2007.11.004
   Chittaro L, 2006, COMPUTER, V39, P40, DOI 10.1109/MC.2006.109
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Hao Jie, 2007, P IEEE INT S CONS EL, P1
   Harrower M, 2003, CARTOGR J, V40, P27, DOI 10.1179/000870403235002042
   Kandel S, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3363
   Kanjo E, 2008, PERS UBIQUIT COMPUT, V12, P599, DOI 10.1007/s00779-007-0180-1
   Kim S, 2008, INFORM VISUAL, V7, P77, DOI 10.1057/palgrave.ivs.9500168
   Lamberti F, 2007, IEEE T VIS COMPUT GR, V13, P247, DOI 10.1109/TVCG.2007.29
   Lee H, 2014, IEEE T PATTERN ANAL, V36, P833, DOI 10.1109/TPAMI.2013.166
   Lyons Richard G., 2010, Understanding Digital Signal Processing
   Mac'ias Elsa., 2011, Network Protocols and Algorithms, V3, P64
   Macias E, 2012, SENSORS-BASEL, V12, P2062, DOI 10.3390/s120202062
   Microsoft, 2014, OFF LENS ONENOTE SCA
   Miner G, 2012, PRACTICAL TEXT MINING AND STATISTICAL ANALYSIS FOR NON-STRUCTURED TEXT DATA APPLICATIONS, P1
   Munzner T., 2014, Visualization analysis and design: principles, techniques, and practice
   Oulasvirta A, 2012, PERS UBIQUIT COMPUT, V16, P105, DOI 10.1007/s00779-011-0412-2
   Razip A., 2014, P IEEE PACIFICVIS
   Roberts JC, 2014, IEEE COMPUT GRAPH, V34, P26, DOI 10.1109/MCG.2014.82
   Schneider RG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661231
   Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0
   Vasilev Andrey, 2012, NETW PROTOCOLS ALGOR, V4, P84
   Wang Yao, 2001, VIDEO PROCESSING COM
   WEISER M, 1991, SCI AM, V265, P94, DOI 10.1038/scientificamerican0991-94
   Wobbrock JO, 2007, UIST 2007: PROCEEDINGS OF THE 20TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P159
   Yoo H.Y., 2006, 2006 Asia-Pacific Symposium on Information Visualisation, V60, P143
   Zhou H., 2006, 14th Pacific Conference on Computer Graphics and Applications, P76
NR 29
TC 3
Z9 3
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 12
DI 10.1145/2808210
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100002
DA 2024-07-18
ER

PT J
AU Li, LJ
   Shamma, DA
   Kong, XN
   Jafarpour, S
   van Zwol, R
   Wang, XH
AF Li, Li-Jia
   Shamma, David A.
   Kong, Xiangnan
   Jafarpour, Sina
   van Zwol, Roelof
   Wang, Xuanhui
TI CelebrityNet: A Social Network Constructed from Large-Scale Online
   Celebrity Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Multimedia; photos; social networks
ID COMMUNITY STRUCTURE
AB Photos are an important information carrier for implicit relationships. In this article, we introduce an image based social network, called CelebrityNet, built from implicit relationships encoded in a collection of celebrity images. We analyze the social properties reflected in this image-based social network and automatically infer communities among the celebrities. We demonstrate the interesting discoveries of the CelebrityNet. We particularly compare the inferred communities with human manually labeled ones and show quantitatively that the automatically detected communities are highly aligned with that of human interpretation. Inspired by the uniqueness of visual content and tag concepts within each community of the CelebrityNet, we further demonstrate that the constructed social network can serve as a knowledge base for high-level visual recognition tasks. In particular, this social network is capable of significantly improving the performance of automatic image annotation and classification of unknown images.
C1 [Li, Li-Jia; Shamma, David A.; Kong, Xiangnan; Jafarpour, Sina; van Zwol, Roelof; Wang, Xuanhui] Yahoo Res, Sunnyvale, CA USA.
C3 Yahoo! Inc; Yahoo! Inc United States
RP Li, LJ (corresponding author), Yahoo, 701 First Ave, Sunnyvale, CA 94089 USA.
EM lijiali.vision@gmail.com; aymans@acm.org; xkong@wpi.edu;
   sjafarpour@turn.com; roelofvanzwol@mac.com; xuanhui@gmail.com
RI LU, LU/JEZ-4760-2023; l, j/JVZ-8480-2024; L, J/JEF-9564-2023
OI Li, Jia/0000-0001-5850-7013
CR [Anonymous], P ICML
   [Anonymous], 1901, B SOC VAUD SCI NAT
   [Anonymous], 2014, CORR
   [Anonymous], J ART INTELL RES
   [Anonymous], P 19 ACM INT C MULT
   [Anonymous], 2012, P 20 ACM INT C MULT, DOI DOI 10.1145/2393347.2393439
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P 18 ACM INT C MULT
   [Anonymous], 2007, PROCESSDINGS AAAI
   [Anonymous], PHYS REV LETT
   [Anonymous], P ECML PKDD
   [Anonymous], 2005, Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM'05, DOI [DOI 10.1145/1099554.1099591, 10.1145/1099554.1099591]
   [Anonymous], 2010, P ACM WORKSH SURR ME
   Bakhshi S, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P965, DOI 10.1145/2556288.2557403
   BEAUCHAMP MA, 1965, BEHAV SCI, V10, P161, DOI 10.1002/bs.3830100205
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   Cao L., 2009, P 17 ACM INT C MULTI, P125
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6
   Ding L, 2010, LECT NOTES COMPUT SC, V6314, P410, DOI 10.1007/978-3-642-15561-1_30
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   FREEMAN LC, 1977, SOCIOMETRY, V40, P35, DOI 10.2307/3033543
   Gao W, 2006, LECT NOTES COMPUT SC, V4182, P119
   González MC, 2007, PHYSICA A, V379, P307, DOI 10.1016/j.physa.2007.01.002
   Gould Peter., 1967, I BRIT GEOGRAPHERS T, V42, P53
   Kang F., 2006, CVPR, V2, P1719
   Konstas I, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P195, DOI 10.1145/1571941.1571977
   Li-Jia Li, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P121, DOI 10.1007/978-3-319-04114-8_11
   Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477
   Liu Y., 2006, AAAI, P421
   Luo Y, 2013, IEEE T IMAGE PROCESS, V22, P523, DOI 10.1109/TIP.2012.2218825
   Mahoney Michael, 2010, P 19 INT C WORLD WID, P631, DOI DOI 10.1145/1772690.1772755
   Newman MEJ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066133
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   StanleyWasserman Katherine, 1994, SOCIAL NETWORK ANAL, DOI 10.1017/CBO9780511815478
   Stone Z, 2008, PROC CVPR IEEE, P47
   Stone Z, 2010, P IEEE, V98, P1408, DOI 10.1109/JPROC.2010.2044551
   Tan P. N., 2016, INTRO DATA MINING
   Vishwanathan S., 2010, Proc. of Neural Information Processing Systems, P2361
   Wang G, 2010, LECT NOTES COMPUT SC, V6315, P169, DOI 10.1007/978-3-642-15555-0_13
   Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3
   Zha Zheng-Jun, 2008, CVPR, P1
NR 43
TC 2
Z9 2
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2015
VL 12
IS 1
AR 3
DI 10.1145/2801125
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CP9LG
UT WOS:000360215200003
DA 2024-07-18
ER

PT J
AU Huang, QH
   Chen, BS
   Wang, JD
   Mei, T
AF Huang, Qinghua
   Chen, Bisheng
   Wang, Jingdong
   Mei, Tao
TI Personalized Video Recommendation through Graph Propagation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Performance; Design; Experimentation; Video recommendation;
   graph propagation; personalized recommendation
ID OF-THE-ART; SYSTEMS
AB The rapid growth of the number of videos on the Internet provides enormous potential for users to find content of interest. However, the vast quantity of videos also turns the finding process into a difficult task. In this article, we address the problem of providing personalized video recommendation for users. Rather than only exploring the user-video bipartite graph that is formulated using click information, we first combine the clicks and queries information to build a tripartite graph. In the tripartite graph, the query nodes act as bridges to connect user nodes and video nodes. Then, to further enrich the connections between users and videos, three subgraphs between the same kinds of nodes are added to the tripartite graph by exploring content-based information (video tags and textual queries). We propose an iterative propagation algorithm over the enhanced graph to compute the preference information of each user. Experiments conducted on a dataset with 1, 369 users, 8, 765 queries, and 17, 712 videos collected from a commercial video search engine demonstrate the effectiveness of the proposed method.
C1 [Huang, Qinghua; Chen, Bisheng] S China Univ Technol, Guangzhou, Guangdong, Peoples R China.
   [Wang, Jingdong; Mei, Tao] Microsoft Res, Beijing, Peoples R China.
C3 South China University of Technology; Microsoft
RP Mei, T (corresponding author), Microsoft Res, Beijing, Peoples R China.
EM tmei@microsoft.com
RI Mei, Tao/GQZ-0596-2022; Wang, Jingdong/E-9920-2017; Huang,
   Qinghua/L-8708-2019
OI Mei, Tao/0000-0002-5990-7307; Wang, Jingdong/0000-0002-4888-4445; Huang,
   Qinghua/0000-0003-1080-6940
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   [Anonymous], 2010, P 2 INT WORKSH CONT
   [Anonymous], 2007, P 16 INT WORLD WID W, DOI DOI 10.1145/1242572.1242675
   [Anonymous], 1990, Matrices: Methods and Applications
   [Anonymous], 2007, P 6 ACM INT C IM VID
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2014, ACM T MULTIM COMPUT, V10
   Baluja S, 2008, WORLD WID WEB C, P895
   Basu C, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P714
   Burke R, 2011, AI MAG, V32, P13, DOI 10.1609/aimag.v32i3.2361
   Candillier L, 2007, LECT NOTES ARTIF INT, V4571, P548
   Cheng H, 2007, IEEE DATA MINING, P457, DOI 10.1109/ICDM.2007.8
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   Herlocker JL, 2004, ACM T INFORM SYST, V22, P5, DOI 10.1145/963770.963772
   Lee S., 2011, PROC RECSYS 11, P93, DOI [DOI 10.1145/2043932.2043952, 10.1145/2043932.2043952]
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Ozturk Gizem, 2011, Modern Approaches in Applied Intelligence. Proceedings 24th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems (IEA/AIE 2011), P406, DOI 10.1007/978-3-642-21827-9_42
   Park J, 2011, IEEE MULTIMEDIA, V18, P78, DOI 10.1109/MMUL.2010.6
   Paulson P, 2003, LECT NOTES ARTIF INT, V2873, P168
   Pazzani M. J., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P325
   Pedersen Ted., 2004, DEMONSTRATION PAPERS, P38
   Yu K., 2002, P 19 C UNC ART INT, P616
   Zhang SL, 2010, IEEE T MULTIMEDIA, V12, P510, DOI 10.1109/TMM.2010.2059634
   Zhang ZK, 2010, PHYSICA A, V389, P179, DOI 10.1016/j.physa.2009.08.036
   Zhao X., 2012, Proceedings of the 4th International Conference on Internet Multimedia Computing and Service, P161, DOI DOI 10.1145/2382336.2382382
   Zhao XJ, 2012, LECT NOTES COMPUT SC, V7131, P149
   Zhao Xiaojian, 2011, P 19 ACM INT C MULTI, P1521
NR 29
TC 18
Z9 19
U1 0
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2014
VL 10
IS 4
AR 32
DI 10.1145/2598779
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO4FY
UT WOS:000341292900002
DA 2024-07-18
ER

PT J
AU Houle, ME
   Oria, V
   Satoh, S
   Sun, J
AF Houle, Michael E.
   Oria, Vincent
   Satoh, Shin'Ichi
   Sun, Jichao
TI Annotation Propagation in Image Databases Using Similarity Graphs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Classification; feature selection; image annotation;
   iterative method; linear system; neighborhood
AB The practicality of large-scale image indexing and querying methods depends crucially upon the availability of semantic information. The manual tagging of images with semantic information is in general very labor intensive, and existing methods for automated image annotation may not always yield accurate results. The aim of this paper is to reduce to a minimum the amount of human intervention required in the semantic annotation of images, while preserving a high degree of accuracy. Ideally, only one copy of each object of interest would be labeled manually, and the labels would then be propagated automatically to all other occurrences of the objects in the database. To this end, we propose an influence propagation strategy, SW-KProp, that requires no human intervention beyond the initial labeling of a subset of the images. SW-KProp distributes semantic information within a similarity graph defined on all images in the database: each image iteratively transmits its current label information to its neighbors, and then readjusts its own label according to the combined influences of its neighbors. SW-KProp influence propagation can be efficiently performed by means of matrix computations, provided that pairwise similarities of images are available. We also propose a variant of SW-KProp which enhances the quality of the similarity graph by selecting a reduced feature set for each prelabeled image and rebuilding its neighborhood. The performances of the SW-KProp method and its variant were evaluated against several competing methods on classification tasks for three image datasets: a handwritten digit dataset, a face dataset and a web image dataset. For the digit images, SW-KProp and its variant performed consistently better than the other methods tested. For the face and web images, SW-KProp outperformed its competitors for the case when the number of prelabeled images was relatively small. The performance was seen to improve significantly when the feature selection strategy was applied.
   Categories and Subject Descriptors: H.2.4 [Database Management]: Systems-Multimedia databases
C1 [Houle, Michael E.; Satoh, Shin'Ichi] Natl Inst Informat, Tokyo 1018430, Japan.
   [Oria, Vincent; Sun, Jichao] New Jersey Inst Technol, Newark, NJ 07102 USA.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; New Jersey Institute of
   Technology
RP Houle, ME (corresponding author), Natl Inst Informat, 2-1-2 Hitotsubashi, Tokyo 1018430, Japan.
EM meh@nii.ac.jp; Oria@njit.edu; satoh@nii.ac.jp; js87@njit.edu
OI Houle, Michael/0000-0001-8486-8015
FU JSPS [24500135]; Grants-in-Aid for Scientific Research [23300041,
   24500135] Funding Source: KAKEN
FX M. E. Houle gratefully acknowledges the financial support of JSPS Grant
   24500135.
CR Ames M, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P971
   [Anonymous], 2006, P BRIT MACH VIS C BM, DOI DOI 10.5244/C.20.92
   [Anonymous], 2010, P INT C WORLD WID WE
   [Anonymous], 2001, Proceedings of the 18th International Conference on Machine Learning, ICML '01
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI [10.1145/1899412.1899418, DOI 10.1145/1899412.1899418]
   Avrachenkov Konstantin., 2008, P 31 ANN INT ACM SIG, P873
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bradski G., 2008, LEARNING OPENCV
   Chang E, 2003, IEEE T CIRC SYST VID, V13, P26, DOI 10.1109/TCSVT.2002.808079
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cusano C, 2004, PROC SPIE, V5304, P330
   Desai C, 2009, PROC INT CONF DATA, P1227, DOI 10.1109/ICDE.2009.207
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Hageman L., 2004, Applied Iterative Methods
   Hardoon DR, 2006, LECT NOTES ARTIF INT, V4093, P681
   HESTENES MR, 1952, J RES NAT BUR STAND, V49, P409, DOI 10.6028/jres.049.044
   Higham NJ, 2003, LINEAR ALGEBRA APPL, V358, P5, DOI 10.1016/S0024-3795(01)00316-0
   Houle M.E., 2011, Proceedings of the ACM International Conference on Multimedia, P1033
   Houle ME, 2005, PROC INT CONF DATA, P619
   Jeh G., 2002, PROC 8 ACM SIGKDD IN, P538
   Jeon J., 2003, P 26 ANN INT ACM SIG
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Lafferty J. D., 2003, P INT C MACH LEARN, P912, DOI DOI 10.5555/3041838.3041953
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li X., 2006, MULTIMEDIA 06, P607, DOI DOI 10.1145/1180639.1180764
   Liu J., 2006, P ACM INT WORKSHOP M, P61, DOI DOI 10.1145/1178677.1178689
   Liu W., 2010, PROC ICML, P679
   Liu W, 2012, P IEEE, V100, P2624, DOI 10.1109/JPROC.2012.2197809
   Liu WY, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P326
   Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24
   Melacci S, 2011, J MACH LEARN RES, V12, P1149
   Nov O, 2010, COMMUN ACM, V53, P128, DOI 10.1145/1785414.1785450
   Ono A, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P201, DOI 10.1109/MMCS.1996.534975
   Ozkan D., 2006, P IEEE COMP SOC C CO, V2, P1477
   Page L., 1999, 422 STANF INFOLAB ST
   Ran Li, 2010, Proceedings of the 2010 Second International Conference on Multimedia and Information Technology (MMIT 2010), P10, DOI 10.1109/MMIT.2010.34
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   SAAD Y, 1986, SIAM J SCI STAT COMP, V7, P856, DOI 10.1137/0907058
   Shi R., 2007, Proceedings of the 15th International Conference on Multimedia, P341
   Srikanth M., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P552, DOI 10.1145/1076034.1076128
   von Ahn Luis., 2004, CHI, DOI DOI 10.1145/985692.985733
   Wang CB, 2006, LECT NOTES COMPUT SC, V4035, P647, DOI 10.1145/1180639.1180774
   Xiaohong Hu, 2009, Proceedings of the 2009 International Conference on Computational Intelligence and Security (CIS 2009), P71, DOI 10.1109/CIS.2009.196
   Zhou D., 2003, Advances in Neural Information Processing Systems, V16
   Zhu JK, 2008, IEEE T MULTIMEDIA, V10, P86, DOI 10.1109/TMM.2007.911245
NR 46
TC 11
Z9 12
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2013
VL 10
IS 1
AR 7
DI 10.1145/2487736
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 280JP
UT WOS:000329025400007
DA 2024-07-18
ER

PT J
AU Turk, M
AF Turk, Matthew
TI Over Twenty Years of Eigenfaces
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Face recognition; biometrics; vision-based interaction
ID HUMAN FACES; MACHINE
AB The inaugural ACM Multimedia Conference coincided with a surge of interest in computer vision technologies for detecting and recognizing people and their activities in images and video. Face recognition was the first of these topics to broadly engage the vision and multimedia research communities. The Eigenfaces approach was, deservedly or not, the method that captured much of the initial attention, and it continues to be taught and used as a benchmark over 20 years later. This article is a brief personal view of the genesis of Eigenfaces for face recognition and its relevance to the multimedia community.
C1 Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara
RP Turk, M (corresponding author), Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
EM mturk@cs.ucsb.edu
OI Turk, Matthew/0000-0002-4198-8401
CR [Anonymous], P COMPUTER VISION PA
   [Anonymous], 1973, THESIS KYOTO U
   BURT PJ, 1988, P IEEE, V76, P1006, DOI 10.1109/5.5971
   FLEMING M, 1990, P INT JOINT C NEUR N, V2, P65
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   HARMON LD, 1981, PATTERN RECOGN, V13, P97, DOI 10.1016/0031-3203(81)90008-X
   KELLY MD, 1970, AI130 STANF ART INT
   KUHN R, 1998, P INT C SPOK LANG PR
   MIDORIKAWA H., 1988, 1 ANN INNS M, P515
   Moghaddam B, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P30, DOI 10.1109/AFGR.1998.670921
   SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519
   TURK M., 1991, THESIS MIT CAMBRIDGE
   WONG KH, 1989, P ICASSP, P1638
   YUILLE A. L., 1989, P C COMP VIS PATT RE
NR 14
TC 10
Z9 11
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 45
DI 10.1145/2490824
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700015
DA 2024-07-18
ER

PT J
AU Feng, H
   Ling, HF
   Zou, FH
   Yan, WQ
   Lu, ZD
AF Feng, Hui
   Ling, Hefei
   Zou, Fuhao
   Yan, Weiqi
   Lu, Zhengding
TI A Collusion Attack Optimization Strategy for Digital Fingerprinting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Security; Multimedia security; digital
   fingerprinting; collusion attack; optimization
AB Collusion attack is a cost-efficient attack for digital fingerprinting. In this article, we propose a novel collusion attack strategy, Iterative Optimization Collusion Attack (IOCA), which is based upon the gradient attack and the principle of informed watermark embedding. We evaluate the performance of the proposed collusion attack strategy in defeating four typical fingerprinting schemes under a well-constructed evaluation framework. The simulation results show that the proposed strategy performs more effectively than the gradient attack, and adopting no more than three fingerprinted copies can sufficiently collapse examined fingerprinting schemes. Meanwhile, the content resulted from the proposed attack still preserves high perceptual quality.
C1 [Feng, Hui; Ling, Hefei; Zou, Fuhao; Lu, Zhengding] Huazhong Univ Sci & Technol, Coll Comp Sci, Wuhan 430074, Peoples R China.
   [Yan, Weiqi] Queens Univ Belfast, Inst ECIT, Belfast BT7 1NN, Antrim, North Ireland.
C3 Huazhong University of Science & Technology; Queens University Belfast
RP Ling, HF (corresponding author), Huazhong Univ Sci & Technol, Coll Comp Sci, Wuhan 430074, Peoples R China.
EM lhefei@hotmail.com
RI feng, hui/I-8659-2018
OI feng, hui/0000-0001-6696-3094
FU NSF of China [60873226, 60803112]; Fundamental Research Funds for the
   Central Universities; Wuhan Youth Science and Technology Chenguang
   Program
FX This work is supported by the NSF of China under grant no. 60873226 and
   60803112, the Fundamental Research Funds for the Central Universities
   and Wuhan Youth Science and Technology Chenguang Program.
CR [Anonymous], EURASIP J APPL SIGNA
   [Anonymous], 2009, GROUND TRUTH DAT
   [Anonymous], EUR SIGN PROC C EUSI
   [Anonymous], 2009, USC SIPI IM DAT
   [Anonymous], P ACM INT C MULT MM
   [Anonymous], P ACM INT C MULT MM
   Boneh D, 1998, IEEE T INFORM THEORY, V44, P1897, DOI 10.1109/18.705568
   Celik MU, 2004, IEEE SIGNAL PROC LET, V11, P831, DOI 10.1109/LSP.2004.835475
   Cha BH, 2009, IEEE T INF FOREN SEC, V4, P302, DOI 10.1109/TIFS.2009.2025849
   Chen Y, 2010, IEEE T IMAGE PROCESS, V19, P1798, DOI 10.1109/TIP.2010.2045030
   Cox I., 2001, Digital Watermarking
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Ergun F, 1999, LECT NOTES COMPUT SC, V1592, P140
   Feng H, 2009, MINES 2009: FIRST INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION NETWORKING AND SECURITY, VOL 1, PROCEEDINGS, P530, DOI 10.1109/MINES.2009.17
   He S, 2005, LECT NOTES COMPUT SC, V3710, P84
   HE S., 2005, P 7 WORKSH MULT SEC, P127
   He S, 2007, INT CONF ACOUST SPEE, P161
   He S, 2007, IEEE T INF FOREN SEC, V2, P697, DOI 10.1109/TIFS.2007.908179
   He S, 2006, IEEE T INF FOREN SEC, V1, P231, DOI 10.1109/TIFS.2006.873597
   Kilian J, 1998, 1998 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY - PROCEEDINGS, P271
   Kirovski D, 2005, INT CONF ACOUST SPEE, P1037
   Kiyavash N, 2006, 2006 40TH ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-4, P1170, DOI 10.1109/CISS.2006.286642
   Kiyavash N, 2009, IEEE T INF FOREN SEC, V4, P293, DOI 10.1109/TIFS.2009.2026462
   Ling HF, 2011, LECT NOTES COMPUT SC, V6526, P224, DOI 10.1007/978-3-642-18405-5_19
   LOO P., 2001, SPIE SEC WAT MULT CO, V4314
   Miller ML, 2004, IEEE T IMAGE PROCESS, V13, P792, DOI 10.1109/TIP.2003.821551
   Staddon JN, 2001, IEEE T INFORM THEORY, V47, P1042, DOI 10.1109/18.915661
   STONE H, 1996, 96045 NEC
   Trappe W, 2003, IEEE T SIGNAL PROCES, V51, P1069, DOI 10.1109/TSP.2003.809378
   Van Trung T, 2005, DESIGN CODE CRYPTOGR, V35, P227, DOI 10.1007/s10623-005-6402-5
   Van Trung T, 2004, DESIGN CODE CRYPTOGR, V31, P125, DOI 10.1023/B:DESI.0000012440.32810.23
   Wang ZJ, 2005, IEEE T IMAGE PROCESS, V14, P804, DOI 10.1109/TIP.2005.847284
   Wang ZJ, 2003, INT CONF ACOUST SPEE, P724
   Wu M, 2004, IEEE SIGNAL PROC MAG, V21, P15
   Zhao HV, 2005, IEEE T IMAGE PROCESS, V14, P646, DOI 10.1109/TIP.2005.846035
NR 35
TC 6
Z9 7
U1 2
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 2
SU S
AR 36
DI 10.1145/2344436.2344442
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 011KE
UT WOS:000309162800006
DA 2024-07-18
ER

PT J
AU Korshunov, P
   Ooi, WT
AF Korshunov, Pavel
   Ooi, Wei Tsang
TI Video Quality for Face Detection, Recognition, and Tracking
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Measurement; Performance; Video analysis algorithm;
   video quality; blockiness; mutual information; video surveillance
AB Many distributed multimedia applications rely on video analysis algorithms for automated video and image processing. Little is known, however, about the minimum video quality required to ensure an accurate performance of these algorithms. In an attempt to understand these requirements, we focus on a set of commonly used face analysis algorithms. Using standard datasets and live videos, we conducted experiments demonstrating that the algorithms show almost no decrease in accuracy until the input video is reduced to a certain critical quality, which amounts to significantly lower bitrate compared to the quality commonly acceptable for human vision. Since computer vision percepts video differently than human vision, existing video quality metrics, designed for human perception, cannot be used to reason about the effects of video quality reduction on accuracy of video analysis algorithms. We therefore investigate two alternate video quality metrics, blockiness and mutual information, and show how they can be used to estimate the critical video qualities for face analysis algorithms.
C1 [Korshunov, Pavel; Ooi, Wei Tsang] Natl Univ Singapore, Singapore, Singapore.
C3 National University of Singapore
RP Korshunov, P (corresponding author), Natl Univ Singapore, Singapore, Singapore.
EM pavelkor@comp.nus.edu.sg
RI Ooi, Wei Tsang/AAE-7810-2019; Ooi, Wei Tsang/HLW-5142-2023
OI Ooi, Wei Tsang/0000-0001-8994-1736; Ooi, Wei Tsang/0000-0001-8994-1736
CR Bradski GR, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P214, DOI 10.1109/ACV.1998.732882
   Collins R., 2000, CMURITR0012
   Eisert P, 2000, IEEE T CIRC SYST VID, V10, P344, DOI 10.1109/76.836279
   EISERT P, 1998, P IMDSP WORKSH, P199
   Eleftheriadis A, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC396
   Girgensohn A., 2007, ACM MULTIMEDIA 07, P423
   Grother P, 2003, LECT NOTES COMPUT SC, V2688, P937
   HAKEEM A, 2005, P 13 ANN ACM INT C M, P608
   Javed O, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P649
   KIM JG, 2003, P IEEE INT C MULT EX, V3, P281
   KIM M, 2001, P INT C NETW ICN 01, P786
   KORSHUNOV P, 2005, P 13 ANN ACM INT C M, P151
   Lu JW, 2003, PATTERN RECOGN LETT, V24, P3079, DOI 10.1016/S0167-8655(03)00167-3
   MUIJS R, 2005, P 13 EUR SING PROC C
   NAIR V, 2002, P 15 VIS INT C CALG, P88
   Rangaswami R., 2004, P IEEE INT C MULT EX
   ROUSE P, 2008, P SPIE HUM VIS EL IM, V6806
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Sanchez V, 2004, INT C PATT RECOG, P799, DOI 10.1109/ICPR.2004.1334379
   Schumeyer RP, 1997, 1997 IEEE FIRST WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P531, DOI 10.1109/MMSP.1997.602689
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Smolic A, 1999, IEEE T CIRC SYST VID, V9, P255, DOI 10.1109/76.752093
   Viola P, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P747
   Wu W, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P284, DOI 10.1109/AVSS.2003.1217933
   YUAN X, P IEEE INT C ADV VID, P199
NR 25
TC 44
Z9 50
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2011
VL 7
IS 3
AR 14
DI 10.1145/2000486.2000488
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 814DB
UT WOS:000294425100002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chiu, CY
   Wang, HM
   Chen, CS
AF Chiu, Chih-Yi
   Wang, Hsin-Min
   Chen, Chu-Song
TI Fast Min-Hashing Indexing and Robust Spatio-Temporal Matching for
   Detecting Video Copies
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Performance; Content-based copy
   detection; near-duplicate; histogram pruning
ID NEAREST-NEIGHBOR; RETRIEVAL; FRAMEWORK
AB The increase in the number of video copies, both legal and illegal, has become a major problem in the multimedia and Internet era. In this article, we propose a novel method for detecting various video copies in a video sequence. To achieve fast and robust detection, the method fully integrates several components, namely the min-hashing signature to compactly represent a video sequence, a spatio-temporal matching scheme to accurately evaluate video similarity compiled from the spatial and temporal aspects, and some speedup techniques to expedite both min-hashing indexing and spatio-temporal matching. The results of experiments demonstrate that, compared to several baseline methods with different feature descriptors and matching schemes, the proposed method which combines both global and local feature descriptors yields the best performance when encountering a variety of video transformations. The method is very fast, requiring approximately 0.06 seconds to search for copies of a thirty-second video clip in a six-hour video sequence.
C1 [Wang, Hsin-Min; Chen, Chu-Song] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
C3 Academia Sinica - Taiwan
EM cychiu@mail.ncyu.edu
RI Wang, Hsin-Min/ABA-8747-2020; Chiu, Chih-Yi/AAN-2961-2020
OI Wang, Hsin-Min/0000-0003-3599-5071; Chiu, Chih-Yi/0000-0002-2859-6120
FU National Science Council of Taiwan [NSC 98-2218-E-415-003, NSC
   99-2631-H-001-020]
FX This work was supported in part by the National Science Council of
   Taiwan under Grants NSC 98-2218-E-415-003 and NSC 99-2631-H-001-020.
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], P ACMMM
   Buhler J, 2001, BIOINFORMATICS, V17, P419, DOI 10.1093/bioinformatics/17.5.419
   Chang SF, 1998, IEEE T CIRC SYST VID, V8, P602, DOI 10.1109/76.718507
   Cheung SCS, 2003, IEEE T CIRC SYST VID, V13, P59, DOI 10.1109/TCSVT.2002.808080
   Chiu CY, 2008, IEEE T CIRC SYST VID, V18, P412, DOI 10.1109/TCSVT.2008.918447
   CHIU CY, 2007, P IEEE INT S MULT IS, P10
   Cohen E., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), P489, DOI 10.1109/ICDE.2000.839448
   DAS A, 2007, P INT WORLD WID WEB
   DeMenthon D, 2006, MULTIMED TOOLS APPL, V30, P229, DOI 10.1007/s11042-006-0029-z
   Deng YN, 1998, IEEE T CIRC SYST VID, V8, P616, DOI 10.1109/76.718508
   ENNESSER F, 1995, IEEE T PATTERN ANAL, V17, P805, DOI 10.1109/34.400571
   HAMPAPUR A, 2001, P IEEE INT C MULT EX, P737
   Hoad TC, 2006, ACM T INFORM SYST, V24, P1, DOI 10.1145/1125857.1125858
   Hua XS, 2004, IEEE IMAGE PROC, P685
   Jain AK, 1999, MULTIMEDIA SYST, V7, P369, DOI 10.1007/s005300050139
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Kashino K, 2003, IEEE T MULTIMEDIA, V5, P348, DOI 10.1109/TMM.2003.813281
   Ke Y., 2004, ACM MULTIMEDIA 04, P869
   Kim C, 2005, IEEE T CIRC SYST VID, V15, P127
   Law-To J., 2006, P ACM INT C MULTIMED, P835
   Law-To Julien., 2007, P 6 ACM INT C IMAGE, P371
   Liu TM, 2003, IEEE T CIRC SYST VID, V13, P1006, DOI 10.1109/TCSVT.2003.816521
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Massoudi A, 2006, IEEE IMAGE PROC, P2297, DOI 10.1109/ICIP.2006.312834
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Ngoc P, 2008, 2008 INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS, VOLS 1-4, P2038
   Sayood K, 1996, INTRO DATA COMPRESSI
   Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215
   Shen H.T., 2005, P ACM SIGMOD INT C M, P730
   Smoliar S. W., 1994, IEEE Multimedia, V1, P62, DOI 10.1109/93.311653
   Sonka M., 2014, Image processing, analysis, and machine vision
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Willems Geert., 2008, PROCEEDING 1 ACM INT, P283
   Wu A. G., 2007, P ACM MM, P218
   Yuan J., 2004, PROC ACM MULTIMEDIA, P61, DOI DOI 10.1145/1026711.1026722
NR 36
TC 16
Z9 21
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR 10
PY 2010
VL 6
IS 2
AR 10
DI 10.1145/1671962.1671966
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 579PA
UT WOS:000276382700004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Carlsson, N
   Eager, DL
AF Carlsson, Niklas
   Eager, Derek L.
TI Server Selection in Large-Scale Video-on-Demand Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Performance analysis; modeling; video-on-demand; content
   distribution networks; server selection
ID MULTICAST; COST
AB Video on demand, particularly with user-generated content, is emerging as one of the most bandwidth-intensive applications on the Internet. Owing to content control and other issues, some video-on-demand systems attempt to prevent downloading and peer-to-peer content delivery. Instead, such systems rely on server replication, such as via third-party content distribution networks, to support video streaming (or pseudostreaming) to their clients. A major issue with such systems is the cost of the required server resources.
   By synchronizing the video streams for clients that make closely spaced requests for the same video from the same server, server costs (such as for retrieval of the video data from disk) can be amortized over multiple requests. A fundamental trade-off then arises, however, with respect to server selection. Network delivery cost is minimized by selecting the nearest server, while server cost is minimized by directing closely spaced requests for the same video to a common server.
   This article compares classes of server selection policies within the context of a simple system model. We conclude that: (i) server selection using dynamic system state information (rather than only proximities and average loads) can yield large improvements in performance, (ii) deferring server selection for a request as late as possible (i.e., until just before streaming is to begin) can yield additional large improvements, and (iii) within the class of policies using dynamic state information and deferred selection, policies using only "local" (rather than global) request information are able to achieve most of the potential performance gains.
C1 [Carlsson, Niklas; Eager, Derek L.] Univ Saskatchewan, Dept Comp Sci, Saskatoon, SK S7N 5C9, Canada.
C3 University of Saskatchewan
RP Carlsson, N (corresponding author), Univ Saskatchewan, Dept Comp Sci, Saskatoon, SK S7N 5C9, Canada.
EM carlsson@cs.usask.ca; eager@cs.usask.ca
FU Natural Sciences and Engineering Research Council of Canada
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada.
CR Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P253, DOI 10.1109/MMCS.1996.534983
   Almeida J.M., 2001, Proceedings of the 11th international workshop on Network and operating systems support for digital audio and video, P21, DOI 10.1145/378344.378348
   Almeida JM, 2004, IEEE T MULTIMEDIA, V6, P356, DOI 10.1109/TMM.2003.822796
   [Anonymous], P ACM WWW 04
   CARLSSON N, 2006, THESIS U SASKATCHEWA
   Carlsson N, 2006, PERFORM EVALUATION, V63, P864, DOI 10.1016/j.peva.2005.09.005
   Carter RL, 1997, IEEE INFOCOM SER, P1014, DOI 10.1109/INFCOM.1997.631117
   Chuang JCI, 2001, TELECOMMUN SYST, V17, P281, DOI 10.1023/A:1016695006342
   Dan A., 1994, Proceedings ACM Multimedia '94, P15, DOI 10.1145/192593.192614
   DAN A, 1995, J PARALLEL DISTR COM, V30, P168, DOI 10.1006/jpdc.1995.1135
   Dykeman H., 1986, P IEEE INT C COMM IC
   Eager D, 2000, PROC SPIE, V3969, P206
   Fahmy S, 2007, IEEE ACM T NETWORK, V15, P373, DOI 10.1109/TNET.2007.892847
   Fei ZM, 2002, IEEE J SEL AREA COMM, V20, P1399, DOI 10.1109/JSAC.2002.802069
   GUO M, 2002, P INT WORKSH NETW OP
   Jamin S., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P295, DOI 10.1109/INFCOM.2000.832199
   Jamin S, 2001, IEEE INFOCOM SER, P31, DOI 10.1109/INFCOM.2001.916684
   Johnsen FT, 2007, IEEE IPCCC, P314, DOI 10.1109/PCCC.2007.358909
   JOHNSON KL, 2006, COMPUT COM, V24, P202
   LEE G, 2006, WALL ST J ONLINE
   Phillips G, 1999, COMP COMM R, V29, P41, DOI 10.1145/316194.316205
   Qiu LL, 2001, IEEE INFOCOM SER, P1587, DOI 10.1109/INFCOM.2001.916655
   Ratnasamy S, 2002, IEEE INFOCOM SER, P1190, DOI 10.1109/INFCOM.2002.1019369
   Rost S, 2001, WEB CACHING AND CONTENT DELIVERY, P147
   TAN H, 2002, P IFIP WG 7 3 INT S, P387
   WONG JW, 1988, P IEEE, V76, P1566, DOI 10.1109/5.16350
   Zegura EW, 1996, IEEE INFOCOM SER, P594, DOI 10.1109/INFCOM.1996.493353
   Zegura EW, 2000, IEEE ACM T NETWORK, V8, P455, DOI 10.1109/90.865074
   2006, US TODAY        0816
NR 29
TC 13
Z9 13
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2010
VL 6
IS 1
AR 1
DI 10.1145/1671954.1671955
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 563VL
UT WOS:000275163200001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hlavacs, H
   Buchinger, S
AF Hlavacs, Helmut
   Buchinger, Shelley
TI Hierarchical video patching with optimal server bandwidth
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; performance; batching; server bandwidth; true
   video-on-demand; video patching
ID SCHEME; ALGORITHMS
AB Video patching is a way for transporting true video-on-demand, that is, instantaneous without any delay, from a video server to several clients. Instead of sending a unique stream to each newly arriving client, clients share as many multicast transmissions as possible, and are serviced only those parts of the video that they have missed.
   We present a novel video patching scheme using hierarchies of patches. Our scheme minimizes the bandwidth needed by the video server, and may result in the fact that clients receive several streams in parallel. We show analytically that for Poisson arrival our algorithm achieves the optimal possible server bandwidth for all schemes where clients share multicast transmissions.
   We also show, how our approach can be combined with batching. This combination requires less server bandwidth than all fixed start point periodic broadcast algorithms.
C1 [Hlavacs, Helmut; Buchinger, Shelley] Univ Vienna, Inst Distributed & Multimedia Syst, A-1080 Vienna, Austria.
C3 University of Vienna
RP Hlavacs, H (corresponding author), Univ Vienna, Inst Distributed & Multimedia Syst, Lenaug 2-8, A-1080 Vienna, Austria.
EM shelley.buchinger@univie.ac.at; helmut.hlavacs@univie.ac.at
CR AGGARWAL C, 1996, P 1996 ACM SIGMETRIC
   Aggarwal CC, 2001, IEEE T COMPUT, V50, P97, DOI 10.1109/12.908987
   Bar-Noy A, 2004, MULTIMEDIA SYST, V9, P411, DOI 10.1007/s00530-003-0114-3
   Bar-Noy A, 2004, SIAM J COMPUT, V33, P1011, DOI 10.1137/S0097539701389245
   BHATIA M, 2005, P PAR DISTR COMP NET
   Boggia G, 2005, IEEE T MULTIMEDIA, V7, P920, DOI 10.1109/TMM.2005.854383
   CAI Y, 1999, P SPIE ACM C MULT CO, P204
   Cai Y, 2007, MULTIMED TOOLS APPL, V32, P115, DOI 10.1007/s11042-006-0049-8
   Chen SQ, 2004, INT CON DISTR COMP S, P787, DOI 10.1109/ICDCS.2004.1281647
   Chien WD, 2005, IEEE T BROADCAST, V51, P360, DOI 10.1109/TBC.2005.852251
   Dan Asit, 1994, P ACM MULT, P391
   Eager D, 2001, IEEE T KNOWL DATA EN, V13, P742, DOI 10.1109/69.956098
   GAO L, 2009, P IEEE INT C MULT CO
   González S, 2004, 10TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P145, DOI 10.1109/MULMM.2004.1264979
   Guan DL, 2004, IEEE T BROADCAST, V50, P11, DOI 10.1109/TBC.2003.822982
   Guo Y, 2004, IEEE T MULTIMEDIA, V6, P387, DOI 10.1109/TMM.2003.822786
   HLAVACS H, 2002, QUOS ANAL METHOD DEL
   HU A, 2001, P IEEE INFOCOM, V1, P508
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   KAMEDA T, 2003, SURVEY VOD BROADCAST
   KONG C, 2002, P 9 INT C PAR DISTR
   Nikolaus B, 2005, IEEE T BROADCAST, V51, P354, DOI 10.1109/TBC.2005.852252
   Pâris JF, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P189, DOI 10.1145/319463.319600
   Sato K, 2005, 19TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS, VOL 1, PROCEEDINGS, P527
   Su TC, 2005, IEEE T MULTIMEDIA, V7, P972, DOI 10.1109/TMM.2005.854390
   Sun Y, 2005, LECT NOTES COMPUT SC, V3665, P190
   Tu Y.C., 2005, ACM TOMCCAP, V1, P354
   Wong Ying Wai, 2003, P 5 INT C ENT INF SY
   YANG X, 2005, P 2005 31 EUROMICRO
   YU HF, 2005, COMPUTER COMMUN, V28, P1877
   ZHAO Y, 2004, P IEEE ICASSP 2004 M
NR 31
TC 11
Z9 13
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 1
AR 8
DI 10.1145/1324287.1324295
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 264UK
UT WOS:000253315700008
DA 2024-07-18
ER

PT J
AU Zimmermann, R
   Chew, E
   Ay, SA
   Pawar, M
AF Zimmermann, Roger
   Chew, Elaine
   Ay, Sakire Arslan
   Pawar, Moses
TI Distributed musical performances: Architecture and stream management
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE design; experimentation; measurement; distributed immersive performance;
   networked musical performance; multimodal data recorder; multimedia
   storage
ID MEDIA; SYSTEM
AB An increasing number of novel applications produce a rich set of different data types that need to be managed efficiently and coherently. In this article we present our experience with designing and implementing a data management infrastructure for a distributed immersive performance ( DIP) application. The DIP project investigates a versatile framework for the capture, recording, and replay of video, audio, and MIDI ( Musical Instrument Digital Interface) streams in an interactive environment for collaborative music performance. We are focusing on two classes of data streams that are generated within this environment. The first category consists of high-resolution isochronous media streams, namely audio and video. The second class comprises MIDI data produced by electronic instruments. MIDI event sequences are alphanumeric in nature and fall into the category of the data streams that have been of interest to data management researchers in recent years.
   We present our data management architecture, which provides a repository for all DIP data. Streams of both categories need to be acquired, transmitted, stored, and replayed in real time. Data items are correlated across different streams with temporal indices. The audio and video streams are managed in our own High-performance Data Recording Architecture ( HYDRA), which integrates multistream recording and retrieval in a consistent manner. This paper reports on the practical issues and challenges that we encountered during the design, implementation and experimental phases of our prototype. We also present some analysis results and discuss future extensions for the architecture.
C1 [Zimmermann, Roger] Univ So Calif, Integrated Media Syst Ctr, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Zimmermann, R (corresponding author), Univ So Calif, Integrated Media Syst Ctr, 3737 Watt Way,Floor 3, Los Angeles, CA 90089 USA.
RI Zimmermann, Roger/D-7944-2015; Ay, Sakire Arslan/AAD-5353-2020
OI Zimmermann, Roger/0000-0002-7410-2590; Chew, Elaine/0000-0002-8342-1024
CR Blakowski G, 1996, IEEE J SEL AREA COMM, V14, P5, DOI 10.1109/49.481691
   BOLOSKY WJ, 1996, P 6 WORKSH NETW OP S
   *CCRMA, 2002, STANF U SOUNDWIRE GR
   CHAFE C, 2004, P INT S MUS AC ISMA
   CHAFE C, 2005, LOW LATENCY AUDIO NE
   Chafe Chris, 2000, P COST G 6 C DIG AUD
   Chandrasekaran S., 2003, P ACM SIGMOD INT C M
   CHEW E, 2005, P INT C SOUND MUS CO
   CHEW E, 2005, P OP WORKSH MUSICNET
   CHEW E, 2004, P OP WORKSH MUSICNET
   CHEW E, 2004, P ANN NAT ASS SCH MU
   COOPERSTOCK JR, 2001, P IEEE INT C WEB DEL
   Ghandeharizadeh S, 1997, MULTIMED TOOLS APPL, V5, P79, DOI 10.1023/A:1009690011119
   GRESHAMLANCASTE.S, 2005, AB TIME VANCOUVER MA
   Gu XY, 2005, IEEE COMMUN MAG, V43, P86, DOI 10.1109/MCOM.2005.1452835
   HSIEH JW, 1995, J PARALLEL DISTR COM, V30, P147, DOI 10.1006/jpdc.1995.1134
   *INTERNET2 MEMB M, 2004, U TEX AUST INTERNET2
   KANKI S, 1998, MELANGE TROIS
   KONSTANTAS D, 1998, P ACM S APPL COMP SA
   LAMBRINOS L, 1998, P INT C COMP COMM NE
   LAURSEN A, 1994, P ACM SIGMOD, P470
   LAZZARO J, 2001, P INT WORKSH NETW OP
   LIE A, 2003, P INT C WEB DEL MUS
   MARTIN C, 1996, MULTIMEDIA INFORMATI, pCH5
   McLeod D, 1999, IEEE SIGNAL PROC MAG, V16, P33, DOI 10.1109/79.743866
   MILLS DL, 1991, IEEE T COMMUN, V39, P1482, DOI 10.1109/26.103043
   MUNTZ R, 1997, ACM PERFORMANCE EVAL, V25, P29
   PERROT X, 2004, 2004 BERLIN PARIS NE
   *POSTGRESQL GROUP, 2003, POSTGRESQL OP SOURC
   ROBBINS S, 1995, CS9511 U TEX DIV COM
   ROSEN T, 2004, IS IT LIVE IS IT INT
   SAWCHUK AA, 2003, P ACM SIGMM 2003 WOR
   SCHOOLER E, 2001, DISTRIBUTED MUSIC FO
   Schulzrinne H., 1998, Real time streaming protocol (rtsp)
   Schulzrinne H., 1996, Rtp: a transport protocol for real-time applications
   Shahabi C, 2002, COMPUTER, V35, P56, DOI 10.1109/MC.2002.1009169
   Tobagi F. A., 1993, Proceedings ACM Multimedia 93, P393, DOI 10.1145/166266.168435
   WITHERSPOON JT, 1978, P ANN PREC TIM TIM I
   XU A, 2000, J AUDIO ENG SOC, V48, P7
   YOUNG JP, 1999, P INT COMP MUS C ICM
   Zimmermann R, 2006, MULTIMED TOOLS APPL, V28, P23, DOI 10.1007/s11042-006-5119-4
   Zimmermann R, 2005, SOFTWARE PRACT EXPER, V35, P345, DOI 10.1002/spe.639
   Zimmermann R, 2004, IEEE MULTIMEDIA, V11, P48, DOI 10.1109/MMUL.2004.1289041
   ZIMMERMANN R, 2001, P VLDB WORKSH DAT TE
   ZIMMERMANN R, 2003, P 5 INT C ENT INF SY
NR 45
TC 15
Z9 18
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 2
AR 14
DI 10.1145/1352012.1352018
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 306RG
UT WOS:000256264900006
DA 2024-07-18
ER

PT J
AU Loschky, LC
   Wolverton, GS
AF Loschky, Lester C.
   Wolverton, Gary S.
TI How late can you update gaze-contingent multiresolutional displays
   without detection?
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 49th Annual Meeting of the Human-Factors-and-Ergonomics-Society
CY SEP 26-30, 2005
CL Orlando, FL
DE design; human factors; experimentation; gaze-contingent;
   level-of-detail; multiresolution; foveation; foveated; area of interest;
   display; updates; perceptual compression; eye tracking; eye movements;
   saccades; saccadic suppression; visual perception; contrast thresholds;
   blur detection; peripheral vision; bandwidth
ID RESOLUTION; TIME; PERCEPTION; SYSTEM; IMAGE
AB This study investigated perceptual disruptions in gaze-contingent multiresolutional displays (GCMRDs) due to delays in updating the center of highest resolution after an eye movement. GCMRDs can be used to save processing resources and transmission bandwidth in many types of single-user display applications, such as virtual reality, video-telephony, simulators, and remote piloting. The current study found that image update delays as late as 60 ms after an eye movement did not significantly increase the detectability of image blur and/or motion transients due to the update. This is good news for designers of GCMRDs, since 60 ms is ample time to update many GCMRDs after an eye movement without disrupting perception. The study also found that longer eye movements led to greater blur and/or transient detection due to moving the eyes further into the low-resolution periphery, effectively reducing the image resolution at fixation prior to the update. In GCMRD applications where longer saccades are more likely (e.g., displays with relatively large distances between objects), this problem could be overcome by increasing the size of the region of highest resolution.
C1 [Loschky, Lester C.] Kansas State Univ, Dept Psychol, Manhattan, KS 66506 USA.
   [Wolverton, Gary S.] Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.
C3 Kansas State University; University of Illinois System; University of
   Illinois Urbana-Champaign
RP Loschky, LC (corresponding author), Kansas State Univ, Dept Psychol, Bluemont Hall, Manhattan, KS 66506 USA.
EM loschky@ksu.edu; wolverton@uiuc.edu
CR BURR DC, 1994, NATURE, V371, P511, DOI 10.1038/371511a0
   Draper MH, 2001, HUM FACTORS, V43, P129, DOI 10.1518/001872001775992552
   Duchowski AT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314309
   Duchowski AT, 2004, CYBERPSYCHOL BEHAV, V7, P621, DOI 10.1089/cpb.2004.7.621
   FRANK LH, 1988, HUM FACTORS, V30, P201, DOI 10.1177/001872088803000207
   Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120
   GEISLER WS, 1999, SOC INFORMATION DISP, V30, P420
   GERI GA, 1995, J OPT SOC AM A, V12, P2367, DOI 10.1364/JOSAA.12.002367
   GRUNWALD AJ, 1994, IEEE T SYST MAN CYB, V24, P120, DOI 10.1109/21.259693
   HODGSON TL, 1993, STUD VIS INFORM PROC, V4, P115
   Komogortsev Oleg., 2004, Proc. of the 12th Annual ACM Int. Conf. on Multimedia, P220, DOI [10.1145/1027527.1027577, DOI 10.1145/1027527.1027577]
   Kortum P. T., 1996, Investigative Ophthalmology and Visual Science, V37, pS297
   Loschky LC, 2005, VIS COGN, V12, P1057, DOI 10.1080/13506280444000652
   Loschky LC, 2002, J EXP PSYCHOL-APPL, V8, P99, DOI 10.1037/1076-898X.8.2.99
   Loschky LC, 2000, ETRA '00, P97, DOI DOI 10.1145/355017.355032
   LUEBKE D, 2000, CS200004 U VIRG CHAR
   McConkie GW, 2002, BEHAV RES METH INS C, V34, P481, DOI 10.3758/BF03195477
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   Park DS, 2000, PHARMACOLOGY OF CEREBRAL ISCHEMIA 2000, P105, DOI 10.1145/355017.355033
   Parkhurst DJ, 2002, HUM FACTORS, V44, P611, DOI 10.1518/0018720024497015
   Reingold E.M., 2000, READING PERCEPTUAL P, P119, DOI [DOI 10.1016/B978-008043642-5/50008-5, 10.1016/B978-008043642-5/50008-5]
   Reingold EM, 2003, HUM FACTORS, V45, P307, DOI 10.1518/hfes.45.2.307.27235
   Reingold EM, 2002, BEHAV RES METH INS C, V34, P491, DOI 10.3758/BF03195478
   Ross J, 2001, TRENDS NEUROSCI, V24, P113, DOI 10.1016/S0166-2236(00)01685-4
   Séré B, 2000, PERCEPTION, V29, P1403, DOI 10.1068/p2991
   SHIOIRI S, 1993, PERCEPT PSYCHOPHYS, V53, P305, DOI 10.3758/BF03205185
   SHIOIRI S, 1989, PERCEPTION, V18, P347, DOI 10.1068/p180347
   Thomas M., 1993, Information Display, V9, P23
   TURNER JA, 1984, P 6 INT INT IND TRAI, P75
   van Diepen PMJ, 1998, PERCEPTION, V27, P1141, DOI 10.1068/p271141
   VOLKMANN FC, 1978, VISION RES, V18, P1193, DOI 10.1016/0042-6989(78)90104-9
   Watson B., 1997, ACM Transactions on Computer-Human Interaction, V4, P323, DOI 10.1145/267135.267137
   Yang J, 2001, PICS 2001: IMAGE PROCESSING, IMAGE QUALITY, IMAGE CAPTURE, SYSTEMS CONFERENCE, PROCEEDINGS, P142
NR 33
TC 45
Z9 51
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 4
AR 25
DI 10.1145/1314303.1314310
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 250QU
UT WOS:000252315900007
OA Green Published
DA 2024-07-18
ER

PT J
AU Etsion, Y
   Tsafrir, D
   Feitelson, DG
AF Etsion, Yoav
   Tsafrir, Dan
   Feitelson, Dror G.
TI Process prioritization using output production: Scheduling for
   multimedia
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; design; performance; human factors; multimedia; resource
   management
AB Desktop operating systems such as Windows and Linux base scheduling decisions on CPU consumption; processes that consume fewer CPU cycles are prioritized, assuming that interactive processes gain from this since they spend most of their time waiting for user input. However, this doesn't work for modern multimedia applications which require significant CPU resources. We therefore suggest a new metric to identify interactive processes by explicitly measuring interactions with the user, and we use it to design and implement a process scheduler. Measurements using a variety of applications indicate that this scheduler is very effective in distinguishing between competing interactive and noninteractive processes.
C1 Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel.
C3 Hebrew University of Jerusalem
RP Etsion, Y (corresponding author), Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel.
EM etsman@cs.huji.ac.il; dants@cs.huji.ac.il; feit@cs.huji.ac.il
OI Feitelson, Dror/0000-0002-2733-7709
CR Banachowski SA, 2002, PROC SPIE, V4673, P46
   BENSON KB, 1990, ADV TELEVISION 1990
   Bovet D.P., 2001, UNDERSTANDING LINUX
   Bruno J, 1998, PROCEEDINGS OF THE USENIX 1998 ANNUAL TECHNICAL CONFERENCE, P235
   *CAN INC, 2004, EOS ELAN 7N7NE CAM
   Candea GM, 1998, PROCEEDINGS OF THE 2ND USENIX WINDOWS NT SYMPOSIUM, P157
   Childs S, 2001, SEVENTH IEEE REAL-TIME TECHNOLOGY AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P135, DOI 10.1109/RTTAS.2001.929879
   Corbet Jonathan, 2005, Linux device drivers, VThird
   Dabrowski J.R., 2001, P SIGCHI C HUMAN FAC, P317
   DALTON AB, 2003, WORKSH HOT TOP OP SY
   Duda KJ, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P261, DOI 10.1145/319344.319169
   EBRAHIMI T, 2002, MPEG4 BOOK
   ETSION Y, 2005, 200535 SCH COMP SCI
   ETSION Y, 2004, INT WORKSH NETW OP S, P110
   ETSION Y, 2003, INT C MEAS MOD COMP, P172
   EVANS S, 1993, US ANN TECHN C US AS, P205
   Feitelson DG, 1999, IEEE SOFTWARE, V16, P52, DOI 10.1109/52.754053
   FLAUTNER K, 2000, ASPLOS 9, P129
   GALLMEISTER B, 1995, POSIX 4 PROGRAMMING
   GOEL A, 2002, S OP SYST DES IMPL U, P165
   Goyal P, 1996, PROCEEDINGS OF THE SECOND SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '96), P107, DOI 10.1145/248155.238766
   Gray Kris., 2003, Microsoft DirectX 9 Programmable Graphics Pipeline
   Love R., 2005, Linux Kernel Development
   Massalin H., 1990, Computing Systems, V3, P139
   Mauro J., 2001, Solaris Internals: Core Kernel Compo- nents
   MCKUSICK MK, 1997, DESIGN IMPLEMENTATIO
   Mosberger D, 1996, PROCEEDINGS OF THE SECOND SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '96), P153, DOI 10.1145/248155.238771
   Nieh J, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P245
   NIEH J, 1993, INT WORKSH NETW OP S, P35
   NIEH J, 1997, ACM S OP SYST PRINC, P184
   PAUL B, 2000, INTRO DIR REND INFR
   Rau M. A., 1999, MASCOTS '99. Proceedings of the Seventh International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems, P252, DOI 10.1109/MASCOT.1999.805062
   SCHEIFLER RW, 1986, ACM T GRAPHIC, V5, P79, DOI 10.1145/22949.24053
   Scheirer J, 2002, INTERACT COMPUT, V14, P93, DOI 10.1016/S0953-5438(01)00059-5
   SCHEIRER J, 2002, INTERACT COMPUT, V2, P89
   Shneiderman B., 2010, DESIGNING USER INTER
   Silberschatz Abraham., 2004, OPERATING SYSTEM CON
   Solomon D.A., 2000, Inside Windows 2000, V3rd
   STEERE D, 1999, S OP SYST DES IMPL, P145
   TORVALDS L, 2003, IMPROVING INTERACTIV
   TSAFRIER D, 2001, THESIS SCH COMP SCI
   WALDSPURGER CA, 1994, S OP SYST DES IMPL U, P1
   ZHANG Y, 2001, ACM S PAR ALG ARCH, P209
   ZHENG H, 2004, S NETW SYST DES IMPL
   ZIMMERMANN R, 2003, ACM S APPL COMP, P979
NR 45
TC 12
Z9 12
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2006
VL 2
IS 4
BP 318
EP 342
DI 10.1145/1201730.1201734
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IY
UT WOS:000250871400004
DA 2024-07-18
ER

PT J
AU Arigon, AM
   Tchounikine, A
   Miquel, M
AF Arigon, Anne-Muriel
   Tchounikine, Anne
   Miquel, Maryvonne
TI Handling multiple points of view in a multimedia data warehouse
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 11th International Workshop on Multimedia Information Systems
CY SEP 19-21, 2005
CL Sorrento, ITALY
DE design; data warehouse; OLAP; multimedia; functional version; descriptor
AB Data warehouses are dedicated to collecting heterogeneous and distributed data in order to perform decision analysis. Based on multidimensional model, OLAP commercial environments such as they are currently designed in traditional applications are used to provide means for the analysis of facts that are depicted by numeric data (e.g., sales depicted by amount or quantity sold). However, in numerous fields, like in medical or bioinformatics, multimedia data are used as valuable information in the decisional process. One of the problems when integrating multimedia data as facts in a multidimensional model is to deal with dimensions built on descriptors that can be obtained by various computation modes on raw multimedia data. Taking into account these computation modes makes possible the characterization of the data by various points of view depending on the user's profile, his best-practices, his level of expertise, and so on. We propose a new multidimensional model that integrates functional dimension versions allowing the descriptors of the multidimensional data to be computed by different functions. With this approach, the user is able to obtain and choose multiple points of view on the data he analyses. This model is used to develop an OLAP application for navigation into a hypercube integrating various functional dimension versions for the calculus of descriptors in a medical use case.
C1 UCBL, LBBE UMR CNRS 5558, Lab Biometr Biol Evolut, F-69622 Villeurbanne, France.
   Inst Natl Sci Appl, LIRIS UMR CNRS 5205, Lab Informat Images Syst Informat, F-69621 Villeurbanne, France.
C3 Universite Claude Bernard Lyon 1; Institut National des Sciences
   Appliquees de Lyon - INSA Lyon
RP Arigon, AM (corresponding author), UCBL, LBBE UMR CNRS 5558, Lab Biometr Biol Evolut, 43 Blvd 11 November 1918, F-69622 Villeurbanne, France.
EM arigon@biomserv.univ-lyon1.fr; maryvonne@insa-lyon.fr;
   miquel@insa-lyon.fr
CR AGRAWAL R, 1995, MODELING MULTIDIMENS, P25
   BLASCHIKA M, 1999, LECT NOTES COMPUTER
   BLASCHKA M, 1999, P 6 DOCT CONS
   Bliujute R., 1998, P 3 INT BALT WORKSH, P27
   BODY M, 2003, P 19 INT C DAT ENG
   CABIBBO L, 1998, LECT NOTES COMPUTER, V1377
   CHAMONI P, 1999, P DAT WAR KNOWL DISC
   Chaudhuri S., 1997, SIGMOD Record, V26, P65, DOI 10.1145/248603.248616
   Chevalier P, 2001, CARDIOVASC RES, V50, P386, DOI 10.1016/S0008-6363(01)00263-2
   EDER J, 2001, LECT NOTES COMPUTER, V2114
   Golfarelli M, 2004, LECT NOTES COMPUT SC, V3289, P415
   Gyssens M, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P106
   Han J., 2012, Data Mining, P393, DOI [DOI 10.1016/B978-0-12-381479-1.00009-5, 10.1016/B978-0-12-381479-1.00009-5]
   HARINARAYAN V, 1996, P 1996 ACM SIGMOD IN, P205
   HURTADO C, 1999, P ACM 2 INT WORKSH D
   Inmon W.H., 1996, BUILDING DATA WAREHO, VSecond
   KAMP V, 1997, P INT DAT ENG APPL S
   KIMBALL R., 1996, DATA WAREHOUSE TOOLK
   LEHNER W, 1998, P INT C EXT DAT TECH
   LI C, 1996, P 5 INT C INF KNOWL
   MENDELZON AO, 2000, P 26 INT C VER LARG
   Pedersen TB, 2001, INFORM SYST, V26, P383, DOI 10.1016/S0306-4379(01)00023-0
   TIKEKAR RV, 1995, DIGITAL IMAGE STORAG
   Vassiliadis P., 1999, SIGMOD Record, V28, P64, DOI 10.1145/344816.344869
   VAUTRIN JS, 2004, ENTREPOT DONNEES MUL
   YOU J, 2001, P INT C IM PROC THES
   ZAIANE OR, 1998, P CASCON 98 M MINDS, P83
   Zhang H, 2001, PROC SPIE, V4323, P308, DOI 10.1117/12.435495
NR 28
TC 5
Z9 7
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2006
VL 2
IS 3
BP 199
EP 218
DI 10.1145/1152149.1152152
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 238IL
UT WOS:000251440700003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, KQ
   Shen, H
AF Li, Keqiu
   Shen, Hong
TI Coordinated Enroute Multimedia Object Caching in Transcoding Proxies for
   Tree Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Management; Performance; Web caching; transcoding
   proxy; multimedia object; optimization problem; Internet; tree network
ID WEB; REPLACEMENT; PLACEMENT
AB Transcoding is a promising technology that allows systems to effect a quality-versus-size tradeoff on multimedia objects. As audio and video applications have proliferated on the Internet, caching in transcoding proxies has become an important technique for improving network performance, especially in mobile networks. This article addresses the problem of coordinated enroute multimedia object caching in transcoding proxies for tree networks. We formulate this problem as an optimization problem based on our proposed model, in which multimedia object caching decisions are made on all enroute caches along the routing path by integrating both object placement and replacement policies and cache status information along the routing path of a request is used to determine the optimal locations for caching multiple versions of the same multimedia object. We propose an optimal solution using dynamic programming to compute the optimal locations. We also extend this solution to solve the same problem for several constrained cases, including constraints on the cost gain per node and on the number of versions to be placed. Our model is evaluated on different performance metrics through extensive simulation experiments. The implementation results show that our model significantly outperforms existing models that consider Web caching in transcoding proxies either on a single path or at individual nodes.
C1 [Li, Keqiu; Shen, Hong] Japan Adv Inst Sci & Technol, Grad Sch Informat Sci, Nomi, Ishikawa 9231292, Japan.
   [Shen, Hong] Univ Sci & Technol China, Dept Comp Sci & Technol, Hefei 230026, Anhui, Peoples R China.
C3 Japan Advanced Institute of Science & Technology (JAIST); Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Shen, H (corresponding author), Japan Adv Inst Sci & Technol, Grad Sch Informat Sci, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.
EM shen@jaist.ac.jp
CR Aggarwal C, 1999, IEEE T KNOWL DATA EN, V11, P94, DOI 10.1109/69.755618
   Barford P., 1998, Performance Evaluation Review, V26, P151, DOI 10.1145/277858.277897
   Bhattacharjee S, 1998, IEEE INFOCOM SER, P600, DOI 10.1109/INFCOM.1998.665080
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   Calvert KL, 1997, IEEE COMMUN MAG, V35, P160, DOI 10.1109/35.587723
   Cao P, 1997, PROCEEDINGS OF THE USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P193
   Cardellini V, 2003, WIAPP 2003: THIRD IEEE WORKSHOP ON INTERNET APPLICATIONS, PROCEEDINGS, P66, DOI 10.1109/WIAPP.2003.1210288
   Cardellini V., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P520, DOI 10.1145/354756.354861
   CHANDRA C, 1999, P USENIX 2 S INT TEC, P81
   Chang CY, 2003, IEEE T PARALL DISTR, V14, P611, DOI 10.1109/TPDS.2003.1206507
   Dahlin M. D., 1994, Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), P267
   Davison BD, 2001, IEEE INTERNET COMPUT, V5, P38, DOI 10.1109/4236.939449
   Fan L, 2000, IEEE ACM T NETWORK, V8, P281, DOI 10.1109/90.851975
   Floyd R, 1998, IEEE PERS COMMUN, V5, P47, DOI 10.1109/98.729724
   Han R, 1998, IEEE PERS COMMUN, V5, P8, DOI 10.1109/98.736473
   HUANG JL, 2004, P IEEE INFOCOM 2004
   Jia Wang, 1999, Computer Communication Review, V29, P36, DOI 10.1145/505696.505701
   Jiang AX, 2003, SECOND IEEE INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS, PROCEEDINGS, P9, DOI 10.1109/NCA.2003.1201132
   Jin S, 2001, COMPUT COMMUN, V24, P174, DOI 10.1016/S0140-3664(00)00312-1
   Korupolu MR, 2002, IEEE T KNOWL DATA EN, V14, P1317, DOI 10.1109/TKDE.2002.1047770
   Korupolu MR, 1999, PROCEEDINGS OF THE TENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P586
   Krishnan P, 2000, IEEE ACM T NETWORK, V8, P568, DOI 10.1109/90.879344
   LI K, 2005, ACM T INTERNET TECH, V5
   LI K, 2004, P 6 AS PAC WEB C APW, P772
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   PADMANABHAN VN, 2000, P ACM SIGCOMM 00 AUG, P111
   Paxson V, 1997, IEEE ACM T NETWORK, V5, P601, DOI 10.1109/90.649563
   Rodriguez P, 2000, COMPUT NETW, V33, P33, DOI 10.1016/S1389-1286(00)00086-4
   Scheuermann P, 1997, COMPUT NETWORKS ISDN, V29, P997, DOI 10.1016/S0169-7552(97)00032-9
   Shim J, 1999, IEEE T KNOWL DATA EN, V11, P549, DOI 10.1109/69.790804
   Tang XY, 2002, IEEE T COMPUT, V51, P595, DOI 10.1109/TC.2002.1009146
   Tennenhouse DL, 1997, IEEE COMMUN MAG, V35, P80, DOI 10.1109/35.568214
   Wessels D, 1998, IEEE J SEL AREA COMM, V16, P345, DOI 10.1109/49.669043
   WOLFSON O, 1991, ACM T DATABASE SYST, V16, P181, DOI 10.1145/103140.103146
   Xu JL, 2002, IEEE J SEL AREA COMM, V20, P1383, DOI 10.1109/JSAC.2002.802068
NR 35
TC 1
Z9 1
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2005
VL 1
IS 3
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DW
UT WOS:000205012400004
DA 2024-07-18
ER

PT J
AU Liu, XL
   Yu, Y
   Li, XL
   Zhao, Y
   Guo, GD
AF Liu, Xiaolong
   Yu, Yang
   Li, Xiaolong
   Zhao, Yao
   Guo, Guodong
TI TCSD: Triple Complementary Streams Detector for Comprehensive Deepfake
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deepfake; depth information; complementary information mining;
   generalization ability
AB Advancements in computer vision and deep learning have made it difficult to distinguish deepfake visual media. While existing detection frameworks have achieved significant performance on challenging deepfake datasets, these approaches consider only a single perspective. More importantly, in urban scenes, neither complex scenarios can be covered by a single view nor can the correlation between multiple datasets of information be well utilized. In this article, to mine the new view for deepfake detection and utilize the correlation of multi-view information contained in images, we propose a novel triple complementary streams detector (TCSD). First, a novel depth estimator is designed to extract depth information (DI), which has not been used in previous methods. Then, to supplement depth information for obtaining comprehensive forgery clues, we consider the incoherence between image foreground and background information (FBI) and the inconsistency between local and global information (LGI). In addition, we designed an attention-based multi-scale feature extraction (MsFE) module to extract more complementary features from DI, FBI, and LGI. Finally, two attention-based feature fusion modules are proposed to adaptively fuse information. Extensive experiment results show that the proposed approach achieves state-of-the-art performance on detecting deepfakes.
C1 [Liu, Xiaolong; Yu, Yang; Li, Xiaolong; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Liu, Xiaolong; Yu, Yang; Li, Xiaolong; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Guo, Guodong] Baidu Res, Inst Deep Learning, Beijing 100080, Peoples R China.
   [Guo, Guodong] Baidu Res, Natl Engn Lab Deep Learning Technol & Applic, Beijing 100080, Peoples R China.
   [Guo, Guodong] Univ Ubiquitous Co, IUNIUBI Res, Hangzhou 310000, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Baidu; Baidu
RP Zhao, Y (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.; Zhao, Y (corresponding author), Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
EM 18112008@bjtu.edu.cn; 18112012@bjtu.edu.cn; lixl@bjtu.edu.cn;
   yzhao@bjtu.edu.cn; guodong.guo@mail.wvu.edu
RI 于, 洋/IUN-7956-2023; Li, xiaolong/GRS-9148-2022; Guo, Guodong/M-5066-2015
OI Yu, Yang/0000-0002-5204-2929; Guo, Guodong/0000-0001-9583-0055; Zhao,
   Yao/0000-0002-8581-9554
FU National Key Research and Development Program of China [2019QY2203];
   National Natural Science Foundation of China [U1936212, 62120106009]
FX This work was supported in part by the National Key Research and
   Development Program of China (grant no. 2019QY2203) and the National
   Natural Science Foundation of China (grant nos. U1936212 and
   62120106009).
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Amerini I, 2019, IEEE INT CONF COMP V, P1205, DOI 10.1109/ICCVW.2019.00152
   [Anonymous], Facilities
   [Anonymous], DEEPF GITH
   [Anonymous], FAC GAN
   Bao JM, 2018, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2018.00702
   Bing Han, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P320, DOI 10.1109/TBIOM.2021.3065735
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Chai Lucy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P103, DOI 10.1007/978-3-030-58574-7_7
   Chen S, 2021, AAAI CONF ARTIF INTE, V35, P1081
   Chen ZK, 2021, PROC CVPR IEEE, P9010, DOI 10.1109/CVPR46437.2021.00890
   Chugh K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P439, DOI 10.1145/3394171.3413700
   Ciftci Umur Aybars, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3009287
   Cozzolino D, 2017, IH&MMSEC'17: PROCEEDINGS OF THE 2017 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P159, DOI 10.1145/3082031.3083247
   de Carvalho TJ, 2013, IEEE T INF FOREN SEC, V8, P1182, DOI 10.1109/TIFS.2013.2265677
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Fernando T, 2021, IEEE T INF FOREN SEC, V16, P1973, DOI 10.1109/TIFS.2020.3047768
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Gu QQ, 2022, AAAI CONF ARTIF INTE, P735
   Gu ZH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3473, DOI 10.1145/3474085.3475508
   Güera D, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P127
   Haliassos A, 2021, PROC CVPR IEEE, P5037, DOI 10.1109/CVPR46437.2021.00500
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He PS, 2019, IEEE IMAGE PROC, P2299, DOI [10.1109/icip.2019.8803740, 10.1109/ICIP.2019.8803740]
   Jiang LM, 2020, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR42600.2020.00296
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kim D.-K., 2022, P IEEECVF WINTER C A, P2828
   Kingma D. P., 2014, arXiv
   Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397
   Li HD, 2020, Arxiv, DOI arXiv:1808.07276
   Li HD, 2020, SIGNAL PROCESS, V174, DOI 10.1016/j.sigpro.2020.107616
   Li LZ, 2020, Arxiv, DOI arXiv:1912.13457
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   Li XD, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1864, DOI 10.1145/3394171.3414034
   Li YZ, 2019, Arxiv, DOI arXiv:1811.00656
   Li YZ, 2018, IEEE INT WORKS INFOR
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Liu HG, 2021, PROC CVPR IEEE, P772, DOI 10.1109/CVPR46437.2021.00083
   Liu SG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355394
   Lu Wei, 2021, ArXiv
   Luo YC, 2021, PROC CVPR IEEE, P16312, DOI 10.1109/CVPR46437.2021.01605
   Masi Iacopo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P667, DOI 10.1007/978-3-030-58571-6_39
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   Mittal T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2823, DOI 10.1145/3394171.3413570
   Mou LT, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2542205.2542208
   Nguyen HH, 2019, INT CONF BIOMETR THE, DOI 10.1109/btas46853.2019.9185974
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Qi H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4318, DOI 10.1145/3394171.3413707
   Rahmouni N, 2017, IEEE INT WORKS INFOR
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Ru YW, 2021, 2021 INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2021), DOI 10.1109/IJCB52358.2021.9484408
   Sabir E., 2019, INTERFACES GUI, V3, P80
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shang ZH, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107950
   Stamm Matthew C., 2016, P 4 ACM WORKSH INF H, P5
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tariq S, 2020, Arxiv, DOI arXiv:2009.07480
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Verdoliva L, 2020, Arxiv, DOI arXiv:2001.06564
   Wang CR, 2021, PROC CVPR IEEE, P14918, DOI 10.1109/CVPR46437.2021.01468
   Wang O., 2020, PROC IEEECVF C COMPU, V7, P8695
   Wei Y, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3408299
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yu Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499026
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2_6
   Zhang X, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035107
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhao TC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15003, DOI 10.1109/ICCV48922.2021.01475
   Zheng YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15024, DOI 10.1109/ICCV48922.2021.01477
   Zhu XY, 2021, PROC CVPR IEEE, P2928, DOI 10.1109/CVPR46437.2021.00295
   Zi BJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2382, DOI 10.1145/3394171.3413769
NR 72
TC 3
Z9 3
U1 5
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 213
DI 10.1145/3558004
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200036
OA Bronze
DA 2024-07-18
ER

PT J
AU Zeng, XH
   Chen, SY
   Xie, YC
   Liao, TX
AF Zeng, Xianhua
   Chen, Saiyuan
   Xie, Yicai
   Liao, Tianxing
TI 3V3D: Three-View Contextual Cross-slice Difference Three-dimensional
   Medical Image Segmentation Adversarial Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D Medical image segmentation; cross-slice difference; generative
   adversarial networks; multi-view
AB In three-dimensional (3D) medical image segmentation, it is still a great challenge to obtain the multidimensional feature information contained in voxel images using a single view for smaller segmentation targets, and the robustness of models obtained by relying solely on segmentation networks needs to be enhanced. In this article, we propose a three-view contextual cross-slice difference 3D segmentation adversarial network, in which three-view contextual cross-slice difference decoding blocks are introduced to improve the segmentation decoder's ability to perceive edge feature information. Meanwhile, dense skip connections are used to alleviate the problem that a large amount of shallow feature information is lost in encoding and insufficient information provided by a single long skip connection during image reconstruction. The adversarial network improves the performance of the segmentation network by distinguishing true or false for each patch of the predicted image. Further, the robustness of the segmentation model is improved in the form of adversarial training. We evaluate ourmodel on the publicly available brain tumor BraTS2019 dataset aswell as the ADNI1 dataset and achieve optimal results compared to recent excellent models.
C1 [Zeng, Xianhua; Chen, Saiyuan; Xie, Yicai; Liao, Tianxing] Chongqing Univ Posts & Telecommun, Coll Comp Sci & Technol, 2 Chongwen Rd, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications
RP Zeng, XH (corresponding author), Chongqing Univ Posts & Telecommun, Coll Comp Sci & Technol, 2 Chongwen Rd, Chongqing 400065, Peoples R China.
EM zengxh@cqupt.edu.cn; S200231111@stu.cqupt.edu.cn; xieyicai2015@163.com;
   d210201014@stu.cqupt.edu.cn
OI Zeng, Xianhua/0000-0001-5892-2372
FU National Natural Science Foundation of China [62076044]; Chongqing
   Talent Plan Project [cstc2022ycjh-bgzxm0160]; Postgraduate Research
   Innovation Project of Chongqing [CYS21308]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 62076044), Chongqing Talent Plan Project (Grant No.
   cstc2022ycjh-bgzxm0160), and Postgraduate Research Innovation Project of
   Chongqing (Grant No. CYS21308).
CR Arjovsky M., 2017, arXiv, DOI 10.48550/arXiv.1701.04862
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bakas S, 2019, Arxiv, DOI arXiv:1811.02629
   Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117
   Chen YZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460940
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Cirillo MD, 2021, LECT NOTES COMPUT SC, V12658, P274, DOI 10.1007/978-3-030-72084-1_25
   Ding Y, 2020, Arxiv, DOI arXiv:2012.11211
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hu HG, 2019, IEEE INT C BIOINFORM, P1194, DOI [10.1109/BIBM47256.2019.8983179, 10.1109/bibm47256.2019.8983179]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kervadec H, 2019, PR MACH LEARN RES, V102, P285
   Mondal AK, 2018, Arxiv, DOI arXiv:1810.12241
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mortazi Aliasghar, 2017, Medical Image Computing and Computer-Assisted Intervention, MICCAI 2017. 20th International Conference. Proceedings: LNCS 10434, P377, DOI 10.1007/978-3-319-66185-8_43
   Oktay O, 2018, Arxiv, DOI arXiv:1804.03999
   Petit O, 2021, LECT NOTES COMPUT SC, V12966, P267, DOI 10.1007/978-3-030-87589-3_28
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sun Y, 2019, IEEE IMAGE PROC, P1535, DOI [10.1109/icip.2019.8803073, 10.1109/ICIP.2019.8803073]
   Usman M, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-69817-y
   Wang RS, 2022, IET IMAGE PROCESS, V16, P1243, DOI 10.1049/ipr2.12419
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Xu Liming, 2023, ACM Transactions on Multimedia Computing, Communications and Applications, V19, P1
   Xue Y, 2018, NEUROINFORMATICS, V16, P383, DOI 10.1007/s12021-018-9377-x
   Yu FS, 2016, Arxiv, DOI [arXiv:1511.07122, DOI 10.48550/ARXIV.1511.07122]
   Yu Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3424116
   Alom MZ, 2018, Arxiv, DOI arXiv:1802.06955
   Zhang JP, 2021, IEEE T MED IMAGING, V40, P661, DOI 10.1109/TMI.2020.3034995
NR 32
TC 2
Z9 2
U1 4
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 192
DI 10.1145/3592614
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200015
DA 2024-07-18
ER

PT J
AU Wang, Y
   Dong, B
   Xu, K
   Piao, HY
   Ding, YF
   Yin, BC
   Yang, X
AF Wang, Yang
   Dong, Bo
   Xu, Ke
   Piao, Haiyin
   Ding, Yufei
   Yin, Baocai
   Yang, Xin
TI A Geometrical Approach to Evaluate the Adversarial Robustness of Deep
   Neural Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adversarial robustness; deep neural network (DNN); image classification
AB Deep neural networks (DNNs) are widely used for computer vision tasks. However, it has been shown that deep models are vulnerable to adversarial attacks-that is, their performances drop when imperceptible perturbations are made to the original inputs, which may further degrade the following visual tasks or introduce new problems such as data and privacy security. Hence, metrics for evaluating the robustness of deep models against adversarial attacks are desired. However, previous metrics are mainly proposed for evaluating the adversarial robustness of shallow networks on the small-scale datasets. Although the Cross Lipschitz Extreme Value for nEtwork Robustness (CLEVER) metric has been proposed for large-scale datasets (e.g., the ImageNet dataset), it is computationally expensive and its performance relies on a tractable number of samples. In this article, we propose the Adversarial Converging Time Score (ACTS), an attack-dependent metric that quantifies the adversarial robustness of a DNN on a specific input. Our key observation is that local neighborhoods on a DNN's output surface would have different shapes given different inputs. Hence, given different inputs, it requires different time for converging to an adversarial sample. Based on this geometry meaning, the ACTS measures the converging time as an adversarial robustness metric. We validate the effectiveness and generalization of the proposed ACTS metric against different adversarial attacks on the large-scale ImageNet dataset using state-of-the-art deep networks. Extensive experiments show that our ACTS metric is an efficient and effective adversarial metric over the previous CLEVER metric.
C1 [Wang, Yang; Yin, Baocai; Yang, Xin] Dalian Univ Technol, 2 Linggong Rd, Dalian, Liaoning, Peoples R China.
   [Dong, Bo] Princeton Univ, Princeton, NJ 08540 USA.
   [Xu, Ke] City Univ Hong Kong, Kowloon, 83 Tat Chee Ave, Hong Kong, Peoples R China.
   [Piao, Haiyin] Northwestern Polytech Univ, 127 West Youyi Rd, Xian 710072, Shaanxi, Peoples R China.
   [Ding, Yufei] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
C3 Dalian University of Technology; Princeton University; City University
   of Hong Kong; Northwestern Polytechnical University; University of
   California System; University of California Santa Barbara
RP Yang, X (corresponding author), Dalian Univ Technol, 2 Linggong Rd, Dalian, Liaoning, Peoples R China.
EM yangwang06@mail.dlut.edu.cn; bo.dong@princeton.edu; kkangwing@gmail.com;
   haiyinpiao@mail.nwpu.edu.cn; yufeiding@cs.ucsb.edu; ybc@dlut.edu.cn;
   xinyang@dlut.edu.cn
OI Wang, Yang/0000-0003-3369-6772; XU, Ke/0000-0001-5855-3810; Piao, Hai
   yin/0000-0002-8519-4750; Dong, Bo/0000-0001-9189-9506
FU National Key Research and Development Program of China [2022ZD0210500];
   National Natural Science Foundation of China
   [61972067/U21A20491/UI908214]; Distinguished Young Scholars Funding of
   Dalian [2022RJ01]
FX This work was supported in part by the National Key Research and
   Development Program of China (2022ZD0210500), the National Natural
   Science Foundation of China (grant 61972067/U21A20491/UI908214), and the
   Distinguished Young Scholars Funding of Dalian (no. 2022RJ01).
CR Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Athalye A, 2018, PR MACH LEARN RES, V80
   Athalye A, 2018, PR MACH LEARN RES, V80
   Bastani O, 2016, ADV NEUR IN, V29
   Brendel W., 2018, P INT C LEARNING REP
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen P.Y., 2017, P 10 ACM WORKSHOP AR
   Chen PY, 2018, AAAI CONF ARTIF INTE, P10
   Cheng Minhao, 2018, P INT C LEARNING REP
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding J., 2022, Advances in Neural Information Processing Systems
   Dong YP, 2020, PROC CVPR IEEE, P318, DOI 10.1109/CVPR42600.2020.00040
   Dong YP, 2019, PROC CVPR IEEE, P7706, DOI 10.1109/CVPR.2019.00790
   Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Engstrom Logan, 2017, arXiv
   Ferrari Claudio, 2022, ACM T MULTIM COMPUT, V19
   Gehr T, 2018, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2018.00058
   Ghosh P, 2019, AAAI CONF ARTIF INTE, P541
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ilyas Andrew, 2018, P INT C MACHINE LEAR
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Kannan H, 2018, Arxiv, DOI arXiv:1803.06373
   Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5
   Kurakin A., 2018, arXiv
   Kurakin A, 2017, Arxiv, DOI arXiv:1607.02533
   Li HF, 2020, IEEE T CYBERNETICS, V50, P4835, DOI 10.1109/TCYB.2019.2914099
   Li JG, 2020, INT CONF ACOUST SPEE, P2937, DOI [10.1109/ICASSP40776.2020.9053058, 10.1109/icassp40776.2020.9053058]
   Li YD, 2019, PR MACH LEARN RES, V97
   Liang B, 2019, Arxiv, DOI arXiv:1705.08378
   Liu XQ, 2018, LECT NOTES COMPUT SC, V11211, P381, DOI 10.1007/978-3-030-01234-2_23
   Liu Xuanqing, 2018, P INT C LEARNING REP
   Madry A, 2019, Arxiv, DOI arXiv:1706.06083
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Moosavi-Dezfooli Seyed-Mohsen, 2018, P INT C LEARNING REP
   Bhagoji AN, 2017, Arxiv, DOI arXiv:1704.02654
   Pang TY, 2019, PR MACH LEARN RES, V97
   PAPERNOT N., 2017, arXiv
   Papernot N, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P506, DOI 10.1145/3052973.3053009
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Paszke A, 2019, ADV NEUR IN, V32
   Paszke Adam, 2017, NIPS 2017 WORKSH AUT
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sinha Aman, 2017, P 5 INT C LEARNING R
   Su D, 2018, LECT NOTES COMPUT SC, V11216, P644, DOI 10.1007/978-3-030-01258-8_39
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tong C, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3381088
   Weng Tsui Wei, 2018, P INT C MACHINE LEAR
   Weng TW, 2018, Arxiv, DOI arXiv:1801.10578
   Wikipedia, 2018, JAC MATR DET
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Yang EK, 2020, IEEE T CYBERNETICS, V50, P1473, DOI 10.1109/TCYB.2018.2882908
   Zhang HY, 2019, PR MACH LEARN RES, V97
   Zhang H, 2018, ADV NEUR IN, V31
   Zhang J., 2021, ICCV, P13043
   Zico Kolter J., 2020, P 8 INT C LEARNING R
NR 58
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2023
VL 19
IS 5
SU S
AR 172
DI 10.1145/3587936
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L0SK0
UT WOS:001020437100008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lei, F
   Cao, ZQ
   Yang, YN
   Ding, YB
   Zhang, C
AF Lei, Fei
   Cao, Zhongqi
   Yang, Yuning
   Ding, Yibo
   Zhang, Cong
TI Learning the User's Deeper Preferences for Multi-modal Recommendation
   Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Graph convolutional networks; multimodal recommendation
AB Recommendation system plays an important role in the rapid development of micro-video sharing platform. Micro-video has rich modal features, such as visual, audio, and text. It is of great significance to carry out personalized recommendation by integrating multi-modal features. However, most of the current multi-modal recommendation systems can only enrich the feature representation on the item side, while it leads to poor learning of user preferences. To solve this problem, we propose a novel module named Learning the User's Deeper Preferences (LUDP), which constructs the item-item modal similarity graph and user preference graph in each modality to explore the learning of item and user representation. Specifically, we construct item-item similar modalities graph using multi-modal features, the item ID embedding is propagated and aggregated on the graph to learn the latent structural information of items; The user preference graph is constructed through the historical interaction between the user and item, on which the multi-modal features are aggregated as the user's preference for the modal. Finally, combining the two parts as auxiliary information enhances the user and item representation learned from the collaborative signals to learn deeper user preferences. Through a large number of experiments on two public datasets (TikTok, Movielens), our model is proved to be superior to the most advanced multi-modal recommendation methods.
C1 [Lei, Fei; Cao, Zhongqi; Yang, Yuning; Ding, Yibo; Zhang, Cong] Beijing Univ Technol, 100 Pingleyuan, Beijing, Peoples R China.
C3 Beijing University of Technology
RP Lei, F (corresponding author), Beijing Univ Technol, 100 Pingleyuan, Beijing, Peoples R China.
EM leifei@bjut.edu.cn; caozhongqi@emails.bjut.edu.cn;
   yangyuning@emails.bjut.edu.cn; Dingyb@emails.bjut.edu.cn;
   zc73080@emails.bjut.edu.cn
CR [Anonymous], 2016, ACM MULTIMEDIA
   [Anonymous], 2017, IEEE Transactions on Pattern Analysis and Machine Intelligence PP
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen TQ, 2012, J MACH LEARN RES, V13, P3619
   Cheng ZY, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3291060
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Feigl J, 2019, NEUROCOMPUTING, V342, P60, DOI 10.1016/j.neucom.2018.10.083
   Guo Q., MODELING HETEROGENEO
   Guo W, 2021, Arxiv, DOI arXiv:2106.00314
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He L, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P690, DOI 10.1145/3459637.3482327
   He M, 2021, LECT NOTES COMPUT SC, V12706, P225, DOI 10.1007/978-3-030-74296-6_18
   He R., 2015, VBPR VISUAL BAYESIAN
   He RN, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P507, DOI 10.1145/2872427.2883037
   He XN, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P639, DOI 10.1145/3397271.3401063
   Hershey S., 2016, CNN architectures for large-scale audio classification
   Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22
   Huang X, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3365003
   Khoshgoftaar T.M., 2009, ADV ARTIFICIAL INTEL, DOI [10.1155/2009/421425, DOI 10.1155/2009/421425]
   Kingma D, 2014, C LEARNING REPRESENT, P12
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lin XX, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P1306, DOI 10.1145/3442381.3449908
   Liu F, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P1296, DOI 10.1145/3442381.3449986
   Ma JX, 2019, PR MACH LEARN RES, V97
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Rendle S, 2012, Arxiv, DOI arXiv:1205.2618
   Rossum B. V., 2019, AUGMENTING LOD BASED
   Sun R, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1405, DOI 10.1145/3340531.3411947
   Tao ZL, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2020.102277
   van den Berg R, 2017, Arxiv, DOI arXiv:1706.02263
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Wang QF, 2023, IEEE T MULTIMEDIA, V25, P1074, DOI 10.1109/TMM.2021.3138298
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wang X, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1243, DOI 10.1145/3394486.3403177
   Wei YW, 2022, IEEE T MULTIMEDIA, V24, P2701, DOI 10.1109/TMM.2021.3088307
   Wei YW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3541, DOI 10.1145/3394171.3413556
   Wei YW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5382, DOI 10.1145/3474085.3475665
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Xu C, 2021, IEEE T IND INFORM, V17, P4197, DOI 10.1109/TII.2020.3008923
   Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890
   Zhang JH, 2021, Arxiv, DOI arXiv:2104.09036
   Zhao ZY, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3410441
NR 43
TC 1
Z9 2
U1 13
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 138
DI 10.1145/3573010
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700013
DA 2024-07-18
ER

PT J
AU Zhang, L
   Long, CJ
   Zhang, XL
   Xiao, CX
AF Zhang, Ling
   Long, Chengjiang
   Zhang, Xiaolong
   Xiao, Chunxia
TI Exploiting Residual and Illumination with GANs for Shadow Detection and
   Shadow Removal
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Shadow detection; shadow removal; residual; illumination; RI-GAN
ID OBJECT DETECTION
AB Residual image and illumination estimation have been proven to be helpful for image enhancement. In this article, we propose a general framework, called RI-GAN, that exploits residual and illumination using generative adversarial networks (GANs). The proposed framework detects and removes shadows in a coarse-to-fine fashion. At the coarse stage, we employ three generators to produce a coarse shadow-removal result, a residual image, and an inverse illumination map. We also incorporate two indirect shadow-removal images via the residual image and the inverse illumination map. With the residual image, the illumination map, and the two indirect shadow-removal images as auxiliary information, the refinement stage estimates a shadow mask to identify shadow regions in the image, and then refines the coarse shadow-removal result to the fine shadow-free image. We introduce a cross-encoding module to the refinement generator, in which the use of feature-crossing can provide additional details to promote the shadow mask and the high-quality shadow-removal result. In addition, we apply data augmentation to the discriminator to reduce the dependence between representations of the discriminator and the quality of the predicted image. Experiments for shadow detection and shadow removal demonstrate that our method outperforms state-of-the-art methods. Furthermore, RI-GAN exhibits good performance in terms of image dehazing, rain removal, and highlight removal, demonstrating the effectiveness and flexibility of the proposed framework.
C1 [Zhang, Ling; Zhang, Xiaolong] Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Hubei Key Lab Intelligent Informat Proc & Realtim, Wuhan, Hubei, Peoples R China.
   [Long, Chengjiang] JD Finance Amer Corp, Mountain View, CA USA.
   [Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan, Hubei, Peoples R China.
C3 Wuhan University of Science & Technology; Wuhan University
RP Xiao, CX (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan, Hubei, Peoples R China.
EM zhling@wust.edu.cn; cjfykx@gmail.com; xiaolong.zhang@wust.edu.cn;
   cxxiao@whu.edu.cn
RI ZHANG, XIAOLONG/IZQ-4553-2023
OI Long, Chengjiang/0000-0003-1584-7290
FU NSFC [61902286, 61972298, U1803262]
FX This work is partially supported by NSFC (No. 61902286, No. 61972298,
   No. U1803262).
CR Akashi Y, 2016, COMPUT VIS IMAGE UND, V146, P77, DOI 10.1016/j.cviu.2015.09.001
   Chen CH, 2021, PROC CVPR IEEE, P7738, DOI 10.1109/CVPR46437.2021.00765
   Chen T, 2019, PROC CVPR IEEE, P12146, DOI 10.1109/CVPR.2019.01243
   Chen ZH, 2020, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR42600.2020.00565
   Chen ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4723, DOI 10.1109/ICCV48922.2021.00470
   Cucchiara R, 2001, 2001 IEEE INTELLIGENT TRANSPORTATION SYSTEMS - PROCEEDINGS, P334, DOI 10.1109/ITSC.2001.948679
   Cun XD, 2020, AAAI CONF ARTIF INTE, V34, P10680
   Das SD, 2020, IEEE COMPUT SOC CONF, P1994, DOI 10.1109/CVPRW50498.2020.00249
   Ding B, 2019, IEEE I CONF COMP VIS, P10212, DOI 10.1109/ICCV.2019.01031
   Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18
   Fu G, 2021, PROC CVPR IEEE, P7748, DOI 10.1109/CVPR46437.2021.00766
   Fu L, 2021, PROC CVPR IEEE, P10566, DOI 10.1109/CVPR46437.2021.01043
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Goodfellow I. J., 2014, ARXIV
   Gryka M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2732407
   Guo RQ, 2011, PROC CVPR IEEE
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hu XW, 2020, IEEE T PATTERN ANAL, V42, P2795, DOI 10.1109/TPAMI.2019.2919616
   Hua G, 2018, IEEE T PATTERN ANAL, V40, P582, DOI 10.1109/TPAMI.2017.2682082
   Khan SH, 2016, IEEE T PATTERN ANAL, V38, P431, DOI 10.1109/TPAMI.2015.2462355
   Le HE, 2018, LECT NOTES COMPUT SC, V11206, P680, DOI 10.1007/978-3-030-01216-8_41
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Liu F, 2008, LECT NOTES COMPUT SC, V5305, P437
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Long CJ, 2017, PROC CVPR IEEE, P4932, DOI 10.1109/CVPR.2017.524
   Long CJ, 2015, IEEE I CONF COMP VIS, P2839, DOI 10.1109/ICCV.2015.325
   Long CJ, 2015, LECT NOTES COMPUT SC, V9003, P260, DOI 10.1007/978-3-319-16865-4_17
   Luo WH, 2020, IEEE T PATTERN ANAL, V42, P1317, DOI 10.1109/TPAMI.2019.2899570
   Mikic I, 2000, INT C PATT RECOG, P321, DOI 10.1109/ICPR.2000.905341
   Miyato T., 2018, 6 INT C LEARNING REP
   Peng L, 2020, IEEE SIGNAL PROC LET, V27, P406, DOI 10.1109/LSP.2020.2974691
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Raj NB, 2020, 2020 INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS SIGNAL PROCESSING AND NETWORKING (WISPNET), P37, DOI [10.1109/WiSPNET48689.2020.9198400, 10.1109/wispnet48689.2020.9198400]
   Ren DW, 2020, IEEE T IMAGE PROCESS, V29, P6852, DOI 10.1109/TIP.2020.2994443
   Shen HL, 2013, APPL OPTICS, V52, P4483, DOI 10.1364/AO.52.004483
   Shor Y, 2008, COMPUT GRAPH FORUM, V27, P577, DOI 10.1111/j.1467-8659.2008.01155.x
   Sidorov O, 2019, IEEE COMPUT SOC CONF, P1748, DOI 10.1109/CVPRW.2019.00225
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Nguyen V, 2017, IEEE I CONF COMP VIS, P4520, DOI 10.1109/ICCV.2017.483
   Wang H, 2020, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR42600.2020.00317
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang TY, 2020, PROC CVPR IEEE, P1877, DOI 10.1109/CVPR42600.2020.00195
   Wei JJ, 2019, COMPUT GRAPH FORUM, V38, P381, DOI 10.1111/cgf.13845
   Wu TP, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243982
   Xiao B., 2021, C COMPUTER VISION PA
   Xiao CX, 2013, COMPUT GRAPH FORUM, V32, P421, DOI 10.1111/cgf.12250
   Xiao CX, 2013, COMPUT GRAPH FORUM, V32, P207, DOI 10.1111/cgf.12198
   Yamamoto T, 2019, ITE TRANS MEDIA TECH, V7, P92, DOI 10.3169/mta.7.92
   Yang QX, 2015, IEEE T PATTERN ANAL, V37, P1304, DOI 10.1109/TPAMI.2014.2360402
   Yun-Hsuan Lin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12902, DOI 10.1109/CVPR42600.2020.01292
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang L, 2020, AAAI CONF ARTIF INTE, V34, P12829
   Zhang L, 2019, VISUAL COMPUT, V35, P1091, DOI 10.1007/s00371-019-01685-8
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P4623, DOI 10.1109/TIP.2015.2465159
   Zheng QL, 2019, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2019.00531
   Zhu L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4682, DOI 10.1109/ICCV48922.2021.00466
   Zhu L, 2018, LECT NOTES COMPUT SC, V11210, P122, DOI 10.1007/978-3-030-01231-1_8
NR 59
TC 0
Z9 0
U1 10
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 120
DI 10.1145/3571745
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300020
DA 2024-07-18
ER

PT J
AU Shi, YY
   Xu, HY
   Yuan, CF
   Li, B
   Hu, WM
   Zha, ZJ
AF Shi, Yaya
   Xu, Haiyang
   Yuan, Chunfeng
   Li, Bing
   Hu, Weiming
   Zha, Zheng-Jun
TI Learning Video-Text Aligned Representations for Video Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video captioning; video-text alignment; aligned representation
AB Video captioning requires that the model has the abilities of video understanding, video-text alignment, and text generation. Due to the semantic gap between vision and language, conducting video-text alignment is a crucial step to reduce the semantic gap, which maps the representations from the visual to the language domain. However, the existing methods often overlook this step, so the decoder has to directly take the visual representations as input, which increases the decoder's workload and limits its ability to generate semantically correct captions. In this paper, we propose a video-text alignment module with a retrieval unit and an alignment unit to learn video-text aligned representations for video captioning. Specifically, we firstly propose a retrieval unit to retrieve sentences as additional input which is used as the semantic anchor between visual scene and language description. Then, we employ an alignment unit with the input of the video and retrieved sentences to conduct the video-text alignment. The representations of two modal inputs are aligned in a shared semantic space. The obtained video-text aligned representations are used to generate semantically correct captions. Moreover, retrieved sentences provide rich semantic concepts which are helpful for generating distinctive captions. Experiments on two public benchmarks, i.e., VATEX and MSR-VTT, demonstrate that our method outperforms state-of-the-art performances by a large margin. The qualitative analysis shows that our method generates correct and distinctive captions.
C1 [Shi, Yaya; Zha, Zheng-Jun] Univ Sci & Technol China, Sch Informat Sci & Technol, 96 Jinzhai Rd, Hefei 230026, Anhui, Peoples R China.
   [Xu, Haiyang] Alibaba Grp, 969 Wenyi West Rd, Hangzhou 311121, Zhejiang, Peoples R China.
   [Yuan, Chunfeng; Li, Bing; Hu, Weiming] Chinese Acad Sci, Inst Automat, NLPR, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
   [Hu, Weiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Hu, Weiming] CAS Ctr Excellence Brain Sci & Intelligence Techn, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Alibaba Group; Chinese Academy of Sciences; Institute of
   Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Yuan, CF (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
EM shiyaya@mail.ustc.edu.cn; shuofeng.xhy@alibaba-inc.com;
   cfyuan@nlpr.ia.ac.cn; bli@nlpr.ia.ac.cn; wmhu@nlpr.ia.ac.cn;
   zhazj@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020
OI Shi, Yaya/0000-0003-0465-6712; yuan, chun feng/0000-0003-2219-4961; li,
   bing/0000-0001-6114-1411; Xu, Haiyang/0000-0001-9442-5912
FU national key R&D program of china [2020AAA0105702]; Beijing Natural
   Science Foundation [JQ21017]; Natural Science Foundation of China
   [61972397, 62036011, 62192782, 61721004, U19B2038, 61906192]; Key
   Research Program of Frontier Sciences, CAS [QYZDJ-SSW-JSC040];
   University Synergy Innovation Program of Anhui Province [GXXT-2019-025];
   Science and Technology Service Network Initiative, CAS
   [KFJ-STS-SCYD-317]; Fundamental Research Funds for the Central
   Universities [WK2100000024]
FX This work is supported by the national key R&D program of china (No.
   2020AAA0105702), Beijing Natural Science Foundation (Grant No. JQ21017),
   the Natural Science Foundation of China (Grant No. 61972397, 62036011,
   62192782, 61721004, U19B2038, 61906192), the Key Research Program of
   Frontier Sciences, CAS, Grant No. QYZDJ-SSW-JSC040, the University
   Synergy Innovation Program of Anhui Province under Grants GXXT-2019-025,
   the Science and Technology Service Network Initiative, CAS (Grant No.
   KFJ-STS-SCYD-317), and the Fundamental Research Funds for the Central
   Universities under Grant WK2100000024.
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Banerjee S., 2005, P WORKSHOP INTRINSIC, P65
   Barbu A., 2012, UAI
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Dai B, 2017, ADV NEUR IN, V30
   Das P, 2013, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2013.340
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JC, 2022, IEEE T CIRC SYST VID, V32, P880, DOI 10.1109/TCSVT.2021.3063423
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong Jiarong, 2018, BRIT MACHINE VISION, P58
   Fang H., 2021, arXiv
   Gupta Ankush, 2012, P 26 AAAI C ARTIFICI
   Guu K, 2020, PR MACH LEARN RES, V119
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He C, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3292058
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou JY, 2019, IEEE I CONF COMP VIS, P8917, DOI 10.1109/ICCV.2019.00901
   Hu YS, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P774, DOI 10.1145/3343031.3351072
   Ji WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446792
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Kuznetsova Polina, 2012, Association for Computational Linguistics
   Lewis Mike, 2020, Advances in Neural Information Processing Systems
   Lewis Patrick, 2020, ADV NEURAL INFORM PR, V33, P9459
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Liu DQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1416, DOI 10.1145/3240508.3240632
   Liu LX, 2019, IEEE I CONF COMP VIS, P4239, DOI 10.1109/ICCV.2019.00434
   Liu XH, 2018, LECT NOTES COMPUT SC, V11219, P353, DOI 10.1007/978-3-030-01267-0_21
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Luo HS, 2021, Arxiv, DOI arXiv:2104.08860
   Luo RT, 2018, PROC CVPR IEEE, P6964, DOI 10.1109/CVPR.2018.00728
   Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Portillo-Quintero JA, 2021, Arxiv, DOI arXiv:2102.12443
   Rabe Markus N., 2022, arXiv
   Radford A, 2021, PR MACH LEARN RES, V139
   Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10635, DOI 10.1109/CVPR42600.2020.01065
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan GC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P745
   Tang PJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303083
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273
   Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468
   Wu YH, 2016, Arxiv, DOI [arXiv:1609.08144, DOI 10.48550/ARXIV.1609.08144]
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang B, 2021, AAAI CONF ARTIF INTE, V35, P3119
   Yang XS, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3313873
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yogatama D, 2021, T ASSOC COMPUT LING, V9, P362, DOI 10.1162/tacl_a_00371
   Zha ZJ, 2022, IEEE T PATTERN ANAL, V44, P710, DOI 10.1109/TPAMI.2019.2909864
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zhang ZQ, 2021, PROC CVPR IEEE, P9832, DOI 10.1109/CVPR46437.2021.00971
   Zheng Q., 2020, 2020 CVPR, P13093, DOI 10.1109/CVPR42600.2020.01311
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 67
TC 4
Z9 4
U1 4
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 63
DI 10.1145/3546828
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000013
OA Bronze
DA 2024-07-18
ER

PT J
AU Liu, JZ
   Liu, F
AF Liu, Jiazhi
   Liu, Feng
TI Modified 2D-Ghost-Free Stereoscopic Display with Depth-of-Field Effects
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Backward-compatible stereoscopic display; stereoscopy; perception
AB Backward-compatible stereoscopic display, a novel display technique that can simultaneously present satisfying 3D effects to viewers with stereo glasses and clear 2D contents to viewers without, aims at helping the people who are unsuitable for watching 3D movies for a long time. In this article, we introduce two versions of backward-compatible stereoscopic display: the simpler version is far simpler than Hidden Stereo (the state-of-the-art method) while preserving competitive 2D-3D effects; the advanced version, which we call 2D-Ghost-Free Stereoscopic Display, overcomes the limitation that Hidden Stereo and the simpler version are both confined to small absolute disparity. 2D-Ghost-Free Stereoscopic Display improves tolerable disparity range by adding depth-of-field in the regions with large disparity, so that it can be applied to more scenes of 3D movies. User experiments and theoretical analysis both demonstrate the superiority of the 2D-Ghost-Free Stereoscopic Display over the state-of-the-art method and our simpler version. In addition, to make the user experiments double-blind and automatic, we developed a user study system that can automatically present 3D images and videos in NVIDIA 3D Vision 2 and collect corresponding votes of subjects on stimuli, whereas the previous researchers did not state that their user experiments were double-blind.
C1 [Liu, Jiazhi; Liu, Feng] Chinese Acad Sci, State Key Lab Informat Secur, Inst Informat Engn, Minzhuang A89, Beijing 100000, Peoples R China.
C3 Chinese Academy of Sciences
RP Liu, JZ (corresponding author), Chinese Acad Sci, State Key Lab Informat Secur, Inst Informat Engn, Minzhuang A89, Beijing 100000, Peoples R China.
EM liujiazhi@iie.ac.cn; liufeng@iie.ac.cn
RI Jiazhi, Liu/AFK-1311-2022; liu, feng/B-3050-2019
CR Didyk P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508376
   Didyk P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964991
   Didyk Piotr., 2012, HUMAN VISION ELECT I, P1
   Duchowski A.T., 2014, P ACM S APPL PERC, P39, DOI DOI 10.1145/2628257.2628259
   Fujimura Wataru, 2012, P SIGGRAPH ASIA 2012, V1, DOI [10.1145/2407707.2407708, DOI 10.1145/2407707.2407708]
   Fukiage T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073672
   Gonzalez R.C., 2018, Digital Image Processing
   Henriksen S, 2016, CURR BIOL, V26, pR500, DOI 10.1016/j.cub.2016.04.049
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156
   Ishihara S., 1987, Test for colour-blindness
   Jo Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356577
   Kellnhofer P, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073617
   Kellnhofer P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925866
   Kingdom FAA, 2012, CURR BIOL, V22, pR22, DOI 10.1016/j.cub.2011.11.048
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   LI ZP, 1994, NETWORK-COMP NEURAL, V5, P157, DOI 10.1088/0954-898X/5/2/003
   May KA, 2016, CURR BIOL, V26, P1571, DOI 10.1016/j.cub.2016.04.037
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Oskam T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024223
   Palmer S., 1999, VISION SCI PHOTONS P
   Park SG, 2014, J INF DISP, V15, P37, DOI 10.1080/15980316.2013.867906
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   RICHARDS W, 1971, J OPT SOC AM, V61, P410, DOI 10.1364/JOSA.61.000410
   Scher S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487229
   Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444
   Wadhwa N, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461966
   Wakunami K, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12954
   Wizadwongsa S, 2021, PROC CVPR IEEE, P8530, DOI 10.1109/CVPR46437.2021.00843
   Xiao L, 2018, SIGGRAPH'18: ACM SIGGRAPH 2018 TALKS, DOI 10.1145/3214745.3214769
   XiaolinWu Guangtao, 2012, P SIGGRAPH ASIA 2012, V4, DOI [10.1145/2407707.2407711, DOI 10.1145/2407707.2407711]
   Zaroff CM, 2003, INVEST OPHTH VIS SCI, V44, P891, DOI 10.1167/iovs.02-0361
   Zhang ZT, 2015, PROC CVPR IEEE, P3800, DOI 10.1109/CVPR.2015.7299004
   Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323
NR 36
TC 0
Z9 0
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
AR 47
DI 10.1145/3534964
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800021
DA 2024-07-18
ER

PT J
AU Song, YG
   Yang, XS
   Xu, CS
AF Song, Yaguang
   Yang, Xiaoshan
   Xu, Changsheng
TI Self-supervised Calorie-aware Heterogeneous Graph Networks for Food
   Recommendation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Food recommendation; recipe calories; heterogeneous graph;
   self-supervised learning
AB With the rapid development of online recipe sharing platforms, food recommendation is emerging as an important application. Although recent studies have made great progress on food recommendation, they have two shortcomings that are likely to affect the recommendation performance. (1) The relations between ingredients are not considered, which may lead to sub-optimal representations of recipes and further result in the neglect of the user's personalized ingredient combination preference. (2) Existing methods do not consider the impact of users' preferences on calories in users' food decision-making process. In this article, we propose a Self-supervised Calorie-aware Heterogeneous Graph Network (SCHGN) to model the relations between ingredients and incorporate calories of food simultaneously. Specifically, we first incorporate users, recipes, ingredients, and calories into a heterogeneous graph and explicitly present the complex relations among them with directed edges. Then, we explore the co-occurrence relation of ingredients in different recipes via self-supervised ingredient prediction. To capture users' dynamic preferences on calories of food, we learn calorie-aware user representations by hierarchical message passing and compute a comprehensive user-guided recipe representation by attention mechanism. The final food recommendation is accomplished based on the similarity between a user's calorie-aware representation and the user-guided representation of a recipe. Extensive experiment results on benchmark datasets demonstrate the effectiveness of the proposed method.
C1 [Song, Yaguang; Yang, Xiaoshan; Xu, Changsheng] Chinese Acad Sci CASIA, Inst Automat, Natl Lab Pattern Recognit, 95 Zhong Guancun East Rd, Beijing 100190, Peoples R China.
   [Song, Yaguang] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, 95 Zhong Guancun East Rd, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Song, YG (corresponding author), Chinese Acad Sci CASIA, Inst Automat, Natl Lab Pattern Recognit, 95 Zhong Guancun East Rd, Beijing 100190, Peoples R China.; Song, YG (corresponding author), Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, 95 Zhong Guancun East Rd, Beijing 100190, Peoples R China.
EM songyaguang2019@ia.ac.cn; xiaoshan.yang@nlpr.ia.ac.cn;
   csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Song, Yaguang/JOZ-6925-2023; yang,
   xiaoshan/HSE-6093-2023
OI Song, Yaguang/0000-0002-9300-8110; 
FU National Key Research and Development Program of China [2018AAA0100604];
   National Natural Science Foundation of China [61720106006, 62036012,
   62072455, 61721004, U1836220, U1705262, 61872424]; Key Research Program
   of Frontier Sciences of CAS [QYZDJ-SSW-JSC039]; Beijing Natural Science
   Foundation [L201001]
FX This work was supported by National Key Research and Development Program
   of China (No. 2018AAA0100604), National Natural Science Foundation of
   China (Nos. 61720106006, 62036012, 62072455, 61721004, U1836220,
   U1705262, 61872424), Key Research Program of Frontier Sciences of CAS
   (QYZDJ-SSW-JSC039), Beijing Natural Science Foundation (L201001).
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   Al-Nazer A, 2014, PROCEDIA COMPUT SCI, V32, P101, DOI 10.1016/j.procs.2014.05.403
   [Anonymous], 2015, P 9 ACM C REC SYST N
   Bruna J, 2014, Arxiv, DOI [arXiv:1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Chen T, 2020, PR MACH LEARN RES, V119
   Chia-Jen Lin, 2014, Advances in Knowledge Discovery and Data Mining. 18th Pacific-Asia Conference, PAKDD 2014. Proceedings: LNCS 8444, P560, DOI 10.1007/978-3-319-06605-9_46
   Defferrard M, 2017, Arxiv, DOI arXiv:1606.09375
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Hjelm RD, 2019, Arxiv, DOI arXiv:1808.06670
   Elsweiler D, 2015, DMRS, P33
   Elsweiler D, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P575, DOI 10.1145/3077136.3080826
   Freyne J, 2010, IUI 2010, P321
   Freyne J, 2010, LECT NOTES COMPUT SC, V6075, P381, DOI 10.1007/978-3-642-13470-8_36
   Gao XY, 2020, IEEE T MULTIMEDIA, V22, P1647, DOI 10.1109/TMM.2019.2945180
   Ge M., 2015, P 5 INT C DIGITAL HL, P105, DOI DOI 10.1145/2750511.2750528
   Gidaris S, 2018, Arxiv, DOI arXiv:1803.07728
   Harvey M, 2013, LECT NOTES COMPUT SC, V8214, P153, DOI 10.1007/978-3-319-02432-5_19
   Haussmann S, 2019, LECT NOTES COMPUT SC, V11779, P146, DOI 10.1007/978-3-030-30796-7_10
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He XN, 2018, ACM/SIGIR PROCEEDINGS 2018, P355, DOI 10.1145/3209978.3209981
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Huang X, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3365003
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Kabbur S, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P659
   Khoshgoftaar T.M., 2009, ADV ARTIFICIAL INTEL, DOI [10.1155/2009/421425, DOI 10.1155/2009/421425]
   Kingma Diederik P., 2014, ARXIV
   Kolesnikov A, 2019, PROC CVPR IEEE, P1920, DOI 10.1109/CVPR.2019.00202
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Hamilton WL, 2018, Arxiv, DOI [arXiv:1706.02216, 10.48550/arXiv.1706.02216, DOI 10.48550/ARXIV.1706.02216]
   Lee HH, 2020, WWW'20: COMPANION PROCEEDINGS OF THE WEB CONFERENCE 2020, P181, DOI 10.1145/3366424.3383536
   Li DY, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1719, DOI 10.1145/3394486.3403223
   Li XC, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P159, DOI 10.1145/3397271.3401080
   Ma JX, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P483, DOI 10.1145/3394486.3403091
   Marin J, 2021, IEEE T PATTERN ANAL, V43, P187, DOI 10.1109/TPAMI.2019.2927476
   Meng Lei, 2020, P MM, P3460, DOI 10.1145/3394171.3413598
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Mikolov T, 2013, Arxiv, DOI [arXiv:1301.3781, 10.48550/arXiv.1301.3781]
   Min Weiqing, 2019, IEEE T MULTIMEDIA, V2019
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Quintanilla Erik, 2020, IEEE T MULTIMEDIA, V2020
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Rendle S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168771
   Salvador A, 2021, PROC CVPR IEEE, P15470, DOI 10.1109/CVPR46437.2021.01522
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Su WJ, 2020, Arxiv, DOI arXiv:1908.08530
   Subramaniyaswamy V, 2019, J SUPERCOMPUT, V75, P3184, DOI 10.1007/s11227-018-2331-8
   Teng CY, 2012, PROCEEDINGS OF THE 3RD ANNUAL ACM WEB SCIENCE CONFERENCE, 2012, P298
   Tran TNT, 2018, J INTELL INF SYST, V50, P501, DOI 10.1007/s10844-017-0469-0
   Trattner C, 2017, Arxiv, DOI arXiv:1711.02760
   Trattner C, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P489, DOI 10.1145/3038912.3052573
   Tsukuda K, 2010, LECT NOTES COMPUT SC, V6262, P258, DOI 10.1007/978-3-642-15251-1_21
   Ueda Mayumi, 2014, P INT MULT ENG COMP, V1
   Vaswani A, 2023, Arxiv, DOI [arXiv:1706.03762, DOI 10.48550/ARXIV.1706.03762, 10.48550/arXiv.1706.03762]
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Wang WJ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3418211
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P950, DOI 10.1145/3292500.3330989
   Xin X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P931, DOI 10.1145/3397271.3401147
   Xu K., 2018, arXiv, DOI DOI 10.48550/ARXIV.1810.00826
   Yan M, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2957755
   Yang LQ, 2017, ACM T INFORM SYST, V36, DOI 10.1145/3072614
   Yang Longqi., 2015, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, P183, DOI [DOI 10.1145/2806416, 10.1145/2806416]
   Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890
   Yokoi Satoshi, 2015, IEEE INT C MULTIMEDI, P1
   Yu Chen, 2021, WSDM '21: Proceedings of the 14th ACM International Conference on Web Search and Data Mining, P544, DOI 10.1145/3437963.3441816
   Zhang SA, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3285029
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zheng Y, 2020, PROC INT CONF DATA, P133, DOI 10.1109/ICDE48307.2020.00019
   Zheng Yu, 2021, PROC INT CONF DATA
   Zhou K, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1893, DOI 10.1145/3340531.3411954
NR 75
TC 10
Z9 10
U1 6
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
DI 10.1145/3524618
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800002
OA Bronze
DA 2024-07-18
ER

PT J
AU Cao, J
   Wang, YQ
   Tao, HC
   Guo, X
AF Cao, Jie
   Wang, Youquan
   Tao, Haicheng
   Guo, Xiang
TI Sensor-based Human Activity Recognition Using Graph LSTM and Multi-task
   Classification Model
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human activity recognition; graph LSTM; pseudo-labeling; graph structure
   learning; multiple graph fusion
ID NETWORKS
AB This paper explores human activities recognition from sensor-based multi-dimensional streams. Recently, deep learning-based methods such as LSTM and CNN have achieved important progress in practical application scenarios. However, in most previous deep learning-based methods exist potential challenges such as class imbalance and multi-modal heterogeneity with time and sensor signals. To handle those problems, we propose a graph LSTM and Metric Learning model (GLML) with multiple construction graph fusion by modeling the sensor-aspect signals and the graph-aspect activities. GLML is a semi-supervised co-training architecture, which can be seen as several iteratively pseudo-labels sampling processing in the unlabeled data. Specifically, we construct three graphs to capture the different relations in each timestamp. Meanwhile, the graph attention model and attention mechanism are proposed to integrate multiple graph interactions for different sensor signals. Furthermore, to obtain a fixed representation of hidden state units and their neighboring nodes, we introduce the Graph LSTM to learn the graph-aspect relations from graph-structured constructed graphs. Notably, we propose a multi-task classification model combining loss function for classification distribution with deep metric learning to enhance the representation ability of the multi-modal sensor data. Experimental results on three public datasets demonstrate that our proposed GLML model has at least 2.44% improved in average against the state-of-the-art methods.
C1 [Cao, Jie] Hefei Univ Technol, Sch Management, 193 Tunxi Rd, Hefei 230009, Anhui, Peoples R China.
   [Wang, Youquan; Tao, Haicheng] Nanjing Univ Finance & Econ, Jiangsu Prov Key Lab E Business, 3 Wenyuan Rd, Nanjing 210046, Jiangsu, Peoples R China.
   [Guo, Xiang] Wuhan Univ, Sch Comp Sci, 16 Luojiashan, Wuhan 430072, Hubei, Peoples R China.
C3 Hefei University of Technology; Nanjing University of Finance &
   Economics; Wuhan University
RP Wang, YQ (corresponding author), Nanjing Univ Finance & Econ, Jiangsu Prov Key Lab E Business, 3 Wenyuan Rd, Nanjing 210046, Jiangsu, Peoples R China.
EM cao_jie@hfut.edu.cn; youq.wang@gmail.com; haicheng.tao@gmail.com;
   nanqiaobei@163.com
RI Cao, jie/JXR-6551-2024
FU National Natural Science Foundation of China (NSFC) [72172057, 92046026,
   71701089]; Natural Science Foundation of the Higher Education
   Institutions of Jiangsu Province, China [21KJB520034]; Fundamental
   Research on Advanced Leading Technology Project of Jiangsu Province
   [BK20192004C, BK20202011]; Jiangsu Provincial Key Research and
   Development Program [BE20200013]; National Center for International
   Joint Research on E-Business Information Processing [2013B01035]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 72172057, 92046026, 71701089, in
   part by Natural Science Foundation of the Higher Education Institutions
   of Jiangsu Province, China under Grant 21KJB520034, the Fundamental
   Research on Advanced Leading Technology Project of Jiangsu Province
   under Grant BK20192004C, BK20202011, the Jiangsu Provincial Key Research
   and Development Program under grant BE20200013, and the National Center
   for International Joint Research on E-Business Information Processing
   under Grant 2013B01035.
CR Abidine BM, 2018, PATTERN ANAL APPL, V21, P119, DOI 10.1007/s10044-016-0570-y
   Abu Alsheikh M, 2016, Arxiv, DOI arXiv:1511.04664
   Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Alharbi F, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22041373
   Colpas PA, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20092702
   Bai L, 2020, PROC ACM INTERACT MO, V4, DOI 10.1145/3397323
   Bai L, 2019, UBICOMP/ISWC'19 ADJUNCT: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2019 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P537, DOI 10.1145/3341162.3349335
   Blanke U., 2010, Wearable Computers (ISWC), 2010 International Symposium on, P1
   Bulling A, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2499621
   Chavarriaga R, 2013, PATTERN RECOGN LETT, V34, P2033, DOI 10.1016/j.patrec.2012.12.014
   Chen K., 2020, arXiv
   Chen KX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1344
   Chen KX, 2020, IEEE T NEUR NET LEAR, V31, P1747, DOI 10.1109/TNNLS.2019.2927224
   Chen LM, 2012, IEEE T SYST MAN CY C, V42, P790, DOI 10.1109/TSMCC.2012.2198883
   Davide Anguita, 2013, ESANN, V3
   Dhiman C, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441628
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Gu FQ, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3472290
   Hacohen G, 2019, PR MACH LEARN RES, V97
   Haque A, 2016, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2016.138
   Huang Y, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409332
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Jha S, 2021, INFORM SCIENCES, V575, P1, DOI 10.1016/j.ins.2021.04.062
   Kautz T, 2017, DATA MIN KNOWL DISC, V31, P1678, DOI 10.1007/s10618-017-0495-0
   Lara OD, 2013, IEEE COMMUN SURV TUT, V15, P1192, DOI 10.1109/SURV.2012.110112.00192
   Li CS, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3426238
   Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8
   Liu FZ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4981
   Liu JY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3365212
   Liu L, 2016, PATTERN RECOGN, V60, P1015, DOI 10.1016/j.patcog.2016.07.024
   Liu SZ, 2020, PROC ACM INTERACT MO, V4, DOI 10.1145/3380999
   Ma F, 2020, J. Mach. Learn. Res., V21, P1
   Ma HJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3109
   Ma XX, 2023, IEEE T KNOWL DATA EN, V35, P12012, DOI 10.1109/TKDE.2021.3118815
   NiklasWeber Aki Harma, 2017, THESIS RADBOUD U NIJ
   Noori FM, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377882
   Nweke HF, 2018, EXPERT SYST APPL, V105, P233, DOI 10.1016/j.eswa.2018.03.056
   Ordóñez FJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010115
   Qian HW, 2021, AAAI CONF ARTIF INTE, V35, P11921
   Qian HW, 2021, ARTIF INTELL, V292, DOI 10.1016/j.artint.2020.103429
   Reiss A, 2012, IEEE INT SYM WRBL CO, P108, DOI 10.1109/ISWC.2012.13
   Shang C., 2021, PROCINT C LEARN REPR, P1
   Shi WW, 2018, LECT NOTES COMPUT SC, V11209, P311, DOI 10.1007/978-3-030-01228-1_19
   Shu XB, 2019, Arxiv, DOI arXiv:1909.13245
   Stikic M, 2009, IEEE INT SYM WRBL CO, P85, DOI 10.1109/ISWC.2009.24
   Su X, 2024, IEEE T NEUR NET LEAR, V35, P4682, DOI 10.1109/TNNLS.2021.3137396
   Suh S, 2022, INT CONF PERVAS COMP, P217, DOI 10.1109/PerCom53586.2022.9762387
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang D, 2020, Arxiv, DOI arXiv:2012.07508
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang JD, 2019, PATTERN RECOGN LETT, V119, P3, DOI 10.1016/j.patrec.2018.02.010
   Wang XH, 2020, AAAI CONF ARTIF INTE, V34, P12249
   Xue HF, 2020, PROC ACM INTERACT MO, V4, DOI 10.1145/3380980
   Yang JB, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3995
   Yao SC, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P351, DOI 10.1145/3038912.3052577
   Yu Guan, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3090076
   Zeng M, 2017, IEEE INT CONF BIG DA, P522, DOI 10.1109/BigData.2017.8257967
   Zhang M., 2012, P 2 ACM SIGHIT S INT, P631, DOI [10.1145/2110363.2110433, DOI 10.1145/2110363.2110433]
   Zhang X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3111
   Zhou XZ, 2022, IEEE T AFFECT COMPUT, V13, P1605, DOI 10.1109/TAFFC.2020.3022732
NR 61
TC 7
Z9 7
U1 2
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 139
DI 10.1145/3561387
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800009
DA 2024-07-18
ER

PT J
AU Li, JF
   Liu, WF
   Zhou, YC
   Yu, J
   Tao, DP
   Xu, CS
AF Li, Jinfeng
   Liu, Weifeng
   Zhou, Yicong
   Yu, Jun
   Tao, Dapeng
   Xu, Changsheng
TI Domain-invariant Graph for Adaptive Semi-supervised Domain Adaptation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Domain adaptation; domain-invariant graph; the Nystrom method; few
   labeled source samples
ID FRAMEWORK; FEATURES; KERNEL; REGULARIZATION; MATRIX
AB Domain adaptation aims to generalize a model from a source domain to tackle tasks in a related but different target domain. Traditional domain adaptation algorithms assume that enough labeled data, which are treated as the prior knowledge are available in the source domain. However, these algorithms will be infeasible when only a few labeled data exist in the source domain, thus the performance decreases significantly. To address this challenge, we propose a Domain-invariant Graph Learning (DGL) approach for domain adaptation with only a few labeled source samples. Firstly, DGL introduces the Nystrom method to construct a plastic graph that shares similar geometric property with the target domain. Then, DGL flexibly employs the Nystrom approximation error to measure the divergence between the plastic graph and source graph to formalize the distribution mismatch from the geometric perspective. Through minimizing the approximation error, DGL learns a domain-invariant geometric graph to bridge the source and target domains. Finally, we integrate the learned domain-invariant graph with the semi-supervised learning and further propose an adaptive semi-supervised model to handle the cross-domain problems. The results of extensive experiments on popular datasets verify the superiority of DGL, especially when only a few labeled source samples are available.
C1 [Li, Jinfeng; Liu, Weifeng] Xidian Univ, China Univ Petr East China, State Key Lab Integrated Serv Networks, 66 Changjiang West Rd, Qingdao 266580, Peoples R China.
   [Zhou, Yicong] Univ Macau, Macau, Peoples R China.
   [Yu, Jun] Hangzhou Dianzi Univ, 1158 2 Dajie, Hangzhou 310018, Peoples R China.
   [Tao, Dapeng] Yunnan Univ, Kunming 650091, Yunnan, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
C3 Xidian University; China University of Petroleum; University of Macau;
   Hangzhou Dianzi University; Yunnan University; Chinese Academy of
   Sciences; Institute of Automation, CAS
RP Li, JF (corresponding author), Xidian Univ, China Univ Petr East China, State Key Lab Integrated Serv Networks, 66 Changjiang West Rd, Qingdao 266580, Peoples R China.
EM lijinfeng_stu@163.com; liuwf@upc.edu.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; liu, weifeng/B-7909-2008; Tao, Dapeng/E-8649-2013;
   li, jinfeng/GVS-5425-2022; Zhou, Yicong/A-8017-2009
OI Zhou, Yicong/0000-0002-4487-6384
FU National Natural Science Foundation of China [61671480, 61836002,
   62020106007]; Major Scientific and Technological Projects of CNPC
   [ZD2019-183-008]; Open Project Program of the National Laboratory of
   Pattern Recognition (NLPR) [202000009]
FX This work was supported by the Project supported by the National Natural
   Science Foundation of China (Grant No. 61671480, 61836002, 62020106007),
   the Major Scientific and Technological Projects of CNPC under Grant
   ZD2019-183-008, and the Open Project Program of the National Laboratory
   of Pattern Recognition (NLPR) (Grant No. 202000009).
CR Andersen M, 2012, OPTIMIZATION FOR MACHINE LEARNING, P55
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Chen S., 2016, P VIS COMM IM PROC V
   Chu WS, 2017, IEEE T PATTERN ANAL, V39, P529, DOI 10.1109/TPAMI.2016.2547397
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   Dai Wenyuan, 2007, P 24 INT C MACHINE L, P193
   Das D, 2018, LECT NOTES COMPUT SC, V11141, P342, DOI 10.1007/978-3-030-01424-7_34
   Ding ZM, 2017, IEEE T IMAGE PROCESS, V26, P660, DOI 10.1109/TIP.2016.2631887
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Gao Jing, 2008, P 14 ACM SIGKDD INT, P283, DOI [10.1145/1401890.1401928, DOI 10.1145/1401890.1401928]
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Griffin G., 2007, CALTECH 256 OBJECT C
   Hao YB, 2020, IEEE T KNOWL DATA EN, V32, P1909, DOI 10.1109/TKDE.2019.2913379
   Jiang Junguang, 2020, ACM MM, P2220
   Li M, 2015, IEEE T NEUR NET LEAR, V26, P152, DOI 10.1109/TNNLS.2014.2359798
   Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167
   Li YM, 2017, IEEE IJCNN, P1395, DOI 10.1109/IJCNN.2017.7966016
   Liu HF, 2019, IEEE T KNOWL DATA EN, V31, P799, DOI 10.1109/TKDE.2018.2843342
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183
   Long MS, 2015, IEEE T KNOWL DATA EN, V27, P1519, DOI 10.1109/TKDE.2014.2373376
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pilanci M, 2022, IEEE T KNOWL DATA EN, V34, P587, DOI 10.1109/TKDE.2020.2984212
   Taotao Jing, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P1606, DOI 10.1145/3394171.3413986
   Wang H, 2011, IEEE I CONF COMP VIS, P551, DOI 10.1109/ICCV.2011.6126287
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wang JD, 2017, IEEE DATA MINING, P1129, DOI 10.1109/ICDM.2017.150
   Wei PF, 2019, IEEE T KNOWL DATA EN, V31, P1440, DOI 10.1109/TKDE.2018.2864732
   Williams CKI, 2000, ADV NEUR IN, V12, P680
   Yan K, 2018, IEEE T CYBERNETICS, V48, P288, DOI 10.1109/TCYB.2016.2633306
   Yang XS, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700286
   Yao T, 2015, PROC CVPR IEEE, P2142, DOI 10.1109/CVPR.2015.7298826
   Yu J, 2014, IEEE T IMAGE PROCESS, V23, P2019, DOI 10.1109/TIP.2014.2311377
   Yu J, 2014, IEEE T MULTIMEDIA, V16, P159, DOI 10.1109/TMM.2013.2284755
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang K., 2008, Proc. of the 25th ICML Conference, P1232, DOI DOI 10.1145/1390156.1390311
   Zhang L, 2019, ABS190304687 CORR
   Zhang L, 2021, IEEE T CLOUD COMPUT, V9, P1117, DOI [10.1109/TCC.2019.2903254, 10.1109/vs-games.2019.8864531]
   Zhang Y, 2018, IEEE IMAGE PROC, P3753, DOI 10.1109/ICIP.2018.8451245
   Zhuang FZ, 2012, IEEE T KNOWL DATA EN, V24, P2025, DOI 10.1109/TKDE.2011.143
NR 45
TC 13
Z9 13
U1 6
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 72
DI 10.1145/3487194
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600006
DA 2024-07-18
ER

PT J
AU Biondi, N
   Pernici, F
   Bruni, M
   Mugnai, D
   Del Bimbo, A
AF Biondi, Niccolo
   Pernici, Federico
   Bruni, Matteo
   Mugnai, Daniele
   Del Bimbo, Alberto
TI CL<SUP>2</SUP>R: Compatible Lifelong Learning Representations
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; compatible learning; lifelong learning; representation
   learning; fixed classifier
ID MEMORY
AB In this article, we propose a method to partially mimic natural intelligence for the problem of lifelong learning representations that are compatible. We take the perspective of a learning agent that is interested in recognizing object instances in an open dynamic universe in a way in which any update to its internal feature representation does not render the features in the gallery unusable for visual search. We refer to this learning problem as Compatible Lifelong Learning Representations ((CLR)-R-2), as it considers compatible representation learning within the lifelong learning paradigm. We identify stationarity as the property that the feature representation is required to hold to achieve compatibility and propose a novel training procedure that encourages local and global stationarity on the learned representation. Due to stationarity, the statistical properties of the learned features do not change over time, making them interoperable with previously learned features. Extensive experiments on standard benchmark datasets show that our (CLR)-R-2 training procedure outperforms alternative baselines and state-of-the-art methods. We also provide novel metrics to specifically evaluate compatible representation learning under catastrophic forgetting in various sequential learning tasks. Code is available at https://github.com/NiccoBiondi/CompatibleLifelongRepresentation.
C1 [Biondi, Niccolo; Pernici, Federico; Bruni, Matteo; Mugnai, Daniele; Del Bimbo, Alberto] Univ Florence, Vle Morgagni 65, I-50134 Florence, Italy.
C3 University of Florence
RP Biondi, N (corresponding author), Univ Florence, Vle Morgagni 65, I-50134 Florence, Italy.
EM niccolo.biondi@unifi.it; federico.pernici@unifi.it;
   matteo.bruni@unifi.it; daniele.mugnai@unifi.it;
   alberto.delbimbo@unifi.it
OI Bruni, Matteo/0000-0003-2017-1061; Pernici,
   Federico/0000-0001-7036-6655; DEL BIMBO, ALBERTO/0000-0002-1052-8322;
   Biondi, Niccolo/0000-0003-1153-1651
CR Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150
   Barletti T, 2022, LECT NOTES COMPUT SC, V13231, P597, DOI 10.1007/978-3-031-06427-2_50
   Belouadah E, 2021, NEURAL NETWORKS, V135, P38, DOI 10.1016/j.neunet.2020.12.003
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Biondi N, 2021, Arxiv, DOI arXiv:2111.07632
   Bowen Zhao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13205, DOI 10.1109/CVPR42600.2020.01322
   Budnik M, 2021, PROC CVPR IEEE, P8224, DOI 10.1109/CVPR46437.2021.00813
   Chen K, 2019, PROC CVPR IEEE, P9860, DOI 10.1109/CVPR.2019.01010
   Chen W, 2022, IEEE T MULTIMEDIA, V24, P1844, DOI 10.1109/TMM.2021.3073279
   Chen Wei, 2020, P 31 BRIT MACHINE VI
   Chen Z., 2016, SYNTH LECT ARTIF INT, V10, P1, DOI DOI 10.2200/S00737ED1V01Y201610AIM033
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Cossu A., 2021, ARXIV
   Davari MohammadReza, 2021, P NEURIPS 2021 WORKS
   De Lange M, 2022, IEEE T PATTERN ANAL, V44, P3366, DOI 10.1109/TPAMI.2021.3057446
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   D¡az-Rodr¡guez N, 2018, Arxiv, DOI arXiv:1810.13166
   Douillard Arthur, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P86, DOI 10.1007/978-3-030-58565-5_6
   Douillard A, 2021, Arxiv, DOI arXiv:2102.06253
   ERICSSON KA, 1995, PSYCHOL REV, V102, P211, DOI 10.1037/0033-295X.102.2.211
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, COMPUT SCI, V2
   Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092
   Hu Jie, 2019, PROC IEEECVF C COMPU, P3004
   Huang G.B., 2014, LABELED FACES WILD U
   Iscen Ahmet, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P699, DOI 10.1007/978-3-030-58517-4_41
   Jung H, 2018, AAAI CONF ARTIF INTE, P3358
   Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Le Ya, 2015, CS 231N, P3
   Li Y., 2015, JMLR: Workshop and Conference Proceedings, V44, P196
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Libby A, 2021, NAT NEUROSCI, V24, P715, DOI 10.1038/s41593-021-00821-9
   LiweiWang Lunjia Hu, 2018, ADV NEURAL INFORM PR, V31
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   Masana M, 2022, Arxiv, DOI arXiv:2010.15277
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   McCloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI [10.1016/S0079-7421(08)60536-8, DOI 10.1016/S0079-7421(08)60536-8]
   Meng Qiang, 2021, P IEEECVF INT C COMP, P9939
   Meyers EM, 2018, J NEUROPHYSIOL, V120, P2260, DOI 10.1152/jn.00225.2018
   Murray JD, 2017, P NATL ACAD SCI USA, V114, P394, DOI 10.1073/pnas.1619449114
   Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012
   Paszke A, 2019, ADV NEUR IN, V32
   Pernici F, 2019, Arxiv, DOI arXiv:1902.10441
   Pernici F, 2021, INT C PATT RECOG, P6259, DOI 10.1109/ICPR48806.2021.9413299
   Pernici F, 2022, IEEE T NEUR NET LEAR, V33, P4373, DOI 10.1109/TNNLS.2021.3056762
   Pernici F, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.102983
   Pernici F, 2018, PROC CVPR IEEE, P2324, DOI 10.1109/CVPR.2018.00247
   Pernici Federico, 2019, P IEEECVF C COMPUTER
   Price WN, 2019, NAT MED, V25, P37, DOI 10.1038/s41591-018-0272-7
   Pu N., 2021, P IEEE CVF C COMP VI, P7901
   Ramanujan Vivek, 2022, P IEEECVF C COMPUTER
   RATCLIFF R, 1990, PSYCHOL REV, V97, P285, DOI 10.1037/0033-295X.97.2.285
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318
   Robins A., 1993, Proceedings 1993 The First New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert Systems, P65, DOI 10.1109/ANNES.1993.323080
   Romero A., 2014, ARXIV14126550
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salehi M, 2022, Arxiv, DOI arXiv:2110.14051
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Schwartz R, 2020, COMMUN ACM, V63, P54, DOI 10.1145/3381831
   Shen YT, 2020, PROC CVPR IEEE, P6367, DOI 10.1109/CVPR42600.2020.00640
   Sun Y, 2014, ADV NEUR IN, V27
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tolias G., 2016, P INT C LEARNING REP
   Van Noorden R, 2020, NATURE, V587, P354, DOI 10.1038/d41586-020-03187-3
   Vijayan M., 2021, COMPUTATIONAL INTELL, P156
   Wang Chien-Yi, 2020, P 31 BRIT MACHINE VI
   Wang FY, 2022, Arxiv, DOI arXiv:2204.04662
   Wu Y, 2019, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2019.00046
   Yan SP, 2021, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR46437.2021.00303
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yi D, 2014, Arxiv, DOI arXiv:1411.7923
   Yosinski J, 2014, ADV NEUR IN, V27
   Zhang N., 2007, Tech. Rep. 07-49, P7
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
NR 80
TC 0
Z9 0
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 132
DI 10.1145/3564786
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000020
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Lv, ZH
   Yu, ZC
   Xie, SX
   Alamri, A
AF Lv, Zhihan
   Yu, Zengchen
   Xie, Shuxuan
   Alamri, Atif
TI Deep Learning-based Smart Predictive Evaluation for Interactive
   Multimedia-enabled Smart Healthcare
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; smart healthcare; healthcare prediction and evaluation
   model; precision; convolutional neural network
ID ARTIFICIAL-INTELLIGENCE; DIAGNOSIS; CLASSIFICATION; RECOGNITION; WAVE
AB Two-dimensional(1) arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.
   This study aims to enhance the security for people's health, improve the medical level further, and increase the confidentiality of people's privacy information. Under the trend of wide application of deep learning algorithms, the convolutional neural network (CNN) is modified to build an interactive smart healthcare prediction and evaluation model (SHPE model) based on the deep learning model. The model is optimized and standardized for data processing. Then, the constructed model is simulated to analyze its performance. The results show that accuracy of the constructed system reaches 82.4%, which is at least 2.4% higher than other advanced CNN algorithms and 3.3% higher than other classical machine algorithms. It is proved based on comparison that the accuracy, precision, recall, and F1 of the constructed model are the highest. Further analysis on error shows that the constructed model shows the smallest error of 23.34 pixels. Therefore, it is proved that the built SHPE model shows higher prediction accuracy and smaller error while ensuring the safety performance, which provides an experimental reference for the prediction and evaluation of smart healthcare treatment in the later stage.
C1 [Lv, Zhihan] Uppsala Univ, Fac Arts, Dept Game Design, Uppsala, Sweden.
   [Yu, Zengchen; Xie, Shuxuan] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Peoples R China.
   [Alamri, Atif] King Saud Univ, Coll Comp & Informat Sci, Pervas & Mobile Comp, Riyadh 11543, Saudi Arabia.
C3 Uppsala University; Qingdao University; King Saud University
RP Lv, ZH (corresponding author), Uppsala Univ, Fac Arts, Dept Game Design, Uppsala, Sweden.
EM lvzhihan@gmail.com; 1580065638@qq.com; 473755474@qq.com; atif@ksu.edu.sa
RI Lv, Zhihan/GLR-6000-2022; Alamri, Atif/KFQ-0028-2024
OI Lv, Zhihan/0000-0003-2525-3074; Alamri, Atif/0000-0002-1887-5193
FU Deanship of Scientific Research at King Saud University, Riyadh, Saudi
   Arabia; National Natural Science Foundation of China [61902203]; Key
   Research and Development Plan Major Scientific and Technological
   Innovation Projects of ShanDong Province [2019JZZY020101]
FX This work was supported by the Deanship of Scientific Research at King
   Saud University, Riyadh, Saudi Arabia, through the Vice Deanship of
   Scientific Research Chairs: Research Chair of Pervasive and Mobile
   Computing. This work was also supported by National Natural Science
   Foundation of China (No. 61902203) and Key Research and Development Plan
   Major Scientific and Technological Innovation Projects of ShanDong
   Province (2019JZZY020101).
CR Abdel-Basset M, 2020, IEEE INTERNET THINGS, V7, P4160, DOI 10.1109/JIOT.2019.2931647
   Akay A, 2019, IEEE J BIOMED HEALTH, V23, P906, DOI 10.1109/JBHI.2019.2894713
   Akyildiz IF, 2020, IEEE ACCESS, V8, P140512, DOI 10.1109/ACCESS.2020.3012139
   Alhussein M, 2019, IEEE ACCESS, V7, P27781, DOI 10.1109/ACCESS.2019.2901672
   Chang WJ, 2019, IEEE ACCESS, V7, P44441, DOI 10.1109/ACCESS.2019.2908843
   Chen D, 2019, NPJ DIGIT MED, V2, DOI 10.1038/s41746-019-0122-0
   Chen FC, 2018, IEEE T IND ELECTRON, V65, P4392, DOI 10.1109/TIE.2017.2764844
   Deng L, 2018, IEEE SIGNAL PROC MAG, V35, P180, DOI 10.1109/MSP.2017.2762725
   Fadlullah ZM, 2018, IEEE COMMUN LETT, V22, P2479, DOI 10.1109/LCOMM.2018.2875431
   Guo Z, 2019, IEEE T RADIAT PLASMA, V3, P162, DOI [10.1109/TRPMS.2018.2890359, 10.1109/trpms.2018.2890359]
   Horry MJ, 2020, IEEE ACCESS, V8, P149808, DOI 10.1109/ACCESS.2020.3016780
   Hossain MS, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3241056
   Hossain MS, 2018, IEEE INTERNET THINGS, V5, P2399, DOI 10.1109/JIOT.2017.2772959
   Hossain MS, 2019, MULTIMEDIA SYST, V25, P565, DOI 10.1007/s00530-017-0561-x
   Hossain MS, 2017, IEEE SYST J, V11, P118, DOI 10.1109/JSYST.2015.2470644
   Huang HJ, 2019, IEEE T VEH TECHNOL, V68, P3027, DOI 10.1109/TVT.2019.2893928
   Jamshidi MB, 2020, IEEE ACCESS, V8, P109581, DOI 10.1109/ACCESS.2020.3001973
   Lakhani P, 2017, RADIOLOGY, V284, P574, DOI 10.1148/radiol.2017162326
   Latif J, 2020, IEEE ACCESS, V8, P150489, DOI 10.1109/ACCESS.2020.3016782
   Li M, 2021, IEEE ACM T COMPUT BI, V18, P575, DOI 10.1109/TCBB.2019.2919581
   Liu YL, 2019, IEEE ACCESS, V7, P22002, DOI 10.1109/ACCESS.2019.2893877
   Luo P, 2019, IEEE ACM T COMPUT BI, V16, P222, DOI 10.1109/TCBB.2017.2770120
   Rahman MA, 2020, IEEE NETWORK, V34, P98, DOI 10.1109/MNET.011.2000353
   Luong TV, 2019, IEEE WIREL COMMUN LE, V8, P1159, DOI 10.1109/LWC.2019.2909893
   Urban G, 2019, IEEE ACM T COMPUT BI, V16, P1029, DOI 10.1109/TCBB.2018.2841396
   Verma P, 2018, J PARALLEL DISTR COM, V116, P27, DOI 10.1016/j.jpdc.2017.11.018
   Wang DS, 2017, IEEE PHOTONIC TECH L, V29, P1667, DOI 10.1109/LPT.2017.2742553
   Wang Y, 2019, IEEE T SMART GRID, V10, P2593, DOI 10.1109/TSG.2018.2805723
   Wang Y, 2020, IEEE T VEH TECHNOL, V69, P5688, DOI 10.1109/TVT.2020.2981995
   Wang Y, 2020, IEEE T VEH TECHNOL, V69, P3491, DOI 10.1109/TVT.2020.2971001
   Wang Y, 2019, IEEE T VEH TECHNOL, V68, P4074, DOI 10.1109/TVT.2019.2900460
   Wu CX, 2018, IEEE ACCESS, V6, P20021, DOI 10.1109/ACCESS.2018.2823979
   Xing Y, 2019, IEEE T VEH TECHNOL, V68, P5379, DOI 10.1109/TVT.2019.2908425
   Xu YW, 2019, CLIN CANCER RES, V25, P3266, DOI 10.1158/1078-0432.CCR-18-2495
   Yang SM, 2022, IEEE T NEUR NET LEAR, V33, P2801, DOI 10.1109/TNNLS.2020.3045492
   Yang SM, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.601109
   Yang SM, 2022, IEEE T NEUR NET LEAR, V33, P4398, DOI [10.1109/TNNLS.2021.3057070, 10.4018/IJCINI.20211001.oa2]
   Yang SM, 2020, IEEE T NEUR NET LEAR, V31, P148, DOI 10.1109/TNNLS.2019.2899936
   Yang SM, 2019, IEEE T FUZZY SYST, V27, P304, DOI 10.1109/TFUZZ.2018.2856182
   Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379
   Zhang HB, 2018, IEEE INTERNET THINGS, V5, P1550, DOI 10.1109/JIOT.2018.2792423
   Zhang SS, 2019, IEEE REV BIOMED ENG, V12, P194, DOI 10.1109/RBME.2018.2864254
NR 42
TC 69
Z9 69
U1 5
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 43
DI 10.1145/3468506
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300020
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Wang, YB
   Ma, ZH
   Wei, X
   Zheng, S
   Wang, YW
   Hong, XP
AF Wang, Yabin
   Ma, Zhiheng
   Wei, Xing
   Zheng, Shuai
   Wang, Yaowei
   Hong, Xiaopeng
TI ECCNAS: Efficient Crowd Counting Neural Architecture Search
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Neural architecture search; AutoDL; continuous search space; crowd
   counting; object counting
AB Recent solutions to crowd counting problems have already achieved promising performance across various benchmarks. However, applying these approaches to real-world applications is still challenging, because they are computation intensive and lack the flexibility to meet various resource budgets. In this article, we propose an efficient crowd counting neural architecture search (ECCNAS) framework to search efficient crowd counting network structures, which can fill this research gap. A novel search from pre-trained strategy enables our cross-task NAS to explore the significantly large and flexible search space with less search time and get more proper network structures. Moreover, our well-designed search space can intrinsically provide candidate neural network structures with high performance and efficiency. In order to search network structures according to hardwares with different computational performance, we develop a novel latency cost estimation algorithm in our ECCNAS. Experiments show our searched models get an excellent trade-off between computational complexity and accuracy and have the potential to deploy in practical scenarios with various resource budgets. We reduce the computational cost, in terms of multiply-and-accumulate (MACs), by up to 96% with comparable accuracy. And we further designed experiments to validate the efficiency and the stability improvement of our proposed search from pre-trained strategy.
C1 [Wang, Yabin; Hong, Xiaopeng] Xi An Jiao Tong Univ, Sch Cyber Sci & Engn, Xian, Shaanxi, Peoples R China.
   [Ma, Zhiheng] Xi An Jiao Tong Univ, Coll Artificial Intelligence, Xian, Shaanxi, Peoples R China.
   [Wei, Xing; Zheng, Shuai] Xi An Jiao Tong Univ, Sch Software Engn, Xian, Shaanxi, Peoples R China.
   [Wang, Yaowei] Pengcheng Lab, Shenzhen, Guangdong, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University
RP Hong, XP (corresponding author), Xi An Jiao Tong Univ, Sch Cyber Sci & Engn, Xian, Shaanxi, Peoples R China.
EM iamwangyabin@stu.xjtu.edu.cn; mazhiheng@stu.xjtu.edu.cn;
   weixing@mail.xjtu.edu.cn; shuaizheng@xjtu.edu.cn; wangyw@pcl.ac.cn;
   hongxiaopeng@mail.xjtu.edu.cn
RI HONG, Xiaopeng/V-6078-2019; Ma, Zhiheng/GPW-5443-2022; Zheng,
   Shuai/JHS-8901-2023
OI HONG, Xiaopeng/0000-0002-0611-0636; Ma, Zhiheng/0000-0002-0034-2065;
   wang, Yabin/0000-0003-2931-572X
FU National Key Research and Development Project of China [2019YFB1312000];
   National Natural Science Foundation of China [62076195, 62006183,
   U20B2052, 62006182]; China Postdoctoral Science Foundation [2020M683489]
FX This work is funded by the National Key Research and Development Project
   of China under Grant No. 2019YFB1312000; the National Natural Science
   Foundation of China under Grant No. 62076195, 62006183, U20B2052, and
   62006182; and the China Postdoctoral Science Foundation under Grant No.
   2020M683489.
CR Abdou Mohamed, 2020, 2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT), P48, DOI 10.1109/ICIoT48696.2020.9089594
   Abousamra Shahira, 2021, AAAI C ART INT AAAI
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chen YK, 2019, ADV NEUR IN, V32
   Fang J., 2019, ARXIV PREPRINT ARXIV
   Fiaschi L, 2012, INT C PATT RECOG, P2685
   He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502
   He Yuhang, AAAI
   Hutter F., 2019, AUTOMATED MACHINE LE, DOI DOI 10.1007/978-3-030-05318-5
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jeong J, 2018, INT C PATT RECOG, P320, DOI 10.1109/ICPR.2018.8545816
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629
   Kumagai S., 2017, ARXIV PREPRINT ARXIV
   Lin Hui, 2021, IJCAI
   Liu CX, 2019, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2019.00017
   Liu H, 2019, PROCEEDINGS OF THE THIRD INTERNATIONAL SYMPOSIUM - EDUCATIONAL RESEARCH AND EDUCATIONAL TECHNOLOGY, 2019, P3
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Ma Zhiheng, 2020, LEARNING SCALES POIN, P220, DOI [10. 1145/3394171.3413642, DOI 10.1145/3394171.3413642]
   Ma Zhiheng, 2021, AAAI C ART INT AAAI
   Meng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3401, DOI 10.1109/CVPR.2011.5995698
   Nakamura K, 2016, INT C PATT RECOG, P277, DOI 10.1109/ICPR.2016.7899646
   Oh MH, 2020, AAAI CONF ARTIF INTE, V34, P11799
   Peng W, 2019, IEEE IMAGE PROC, P11, DOI [10.1109/icip.2019.8802919, 10.1109/ICIP.2019.8802919]
   Pham H, 2018, PR MACH LEARN RES, V80
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Sam Deepak Babu, 2019, ARXIV PREPRINT ARXIV
   Shi MJ, 2019, PROC CVPR IEEE, P7271, DOI 10.1109/CVPR.2019.00745
   Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109
   Sindagi VA, 2020, IEEE T IMAGE PROCESS, V29, P323, DOI 10.1109/TIP.2019.2928634
   Tian YK, 2020, IEEE T IMAGE PROCESS, V29, P2714, DOI 10.1109/TIP.2019.2952083
   Wang B., 2020, ADV NEURAL INFORM PR
   Wang Jingying, 2021, Advances in Computer, Communication and Computational Sciences. Proceedings of IC4S 2019. Advances in Intelligent Systems and Computing (AISC 1158), P851, DOI 10.1007/978-981-15-4409-5_76
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wu BC, 2019, PROC CVPR IEEE, P10726, DOI 10.1109/CVPR.2019.01099
   Wu XJ, 2019, INT CONF ACOUST SPEE, P2382, DOI [10.1109/ICASSP.2019.8683744, 10.1109/icassp.2019.8683744]
   Xiong HP, 2019, IEEE I CONF COMP VIS, P8361, DOI 10.1109/ICCV.2019.00845
   Xu CF, 2019, IEEE I CONF COMP VIS, P8381, DOI 10.1109/ICCV.2019.00847
   Yang JX, 2018, INT C PATT RECOG, P3244, DOI 10.1109/ICPR.2018.8545683
   Zhang AR, 2019, IEEE I CONF COMP VIS, P6787, DOI 10.1109/ICCV.2019.00689
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zheng S, 2021, COMPUT AIDED DESIGN, V132, DOI 10.1016/j.cad.2020.102984
   Zoph B, 2016, P 2016 C N AM CHAPTE
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 49
TC 18
Z9 18
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 36
DI 10.1145/3465455
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300013
DA 2024-07-18
ER

PT J
AU Tiotsop, LF
   Mizdos, T
   Barkowsky, M
   Pocta, P
   Servetti, A
   Masala, E
AF Tiotsop, Lohic Fotio
   Mizdos, Tomas
   Barkowsky, Marcus
   Pocta, Peter
   Servetti, Antonio
   Masala, Enrico
TI Mimicking Individual Media Quality Perception with Neural Network based
   Artificial Observers
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Media quality; human factors; user expectations; subjects opinions;
   neural networks
ID VIDEO QUALITY
AB The media quality assessment research community has traditionally been focusing on developing objective algorithms to predict the result of a typical subjective experiment in terms of Mean Opinion Score (MOS) value. However, the MOS, being a single value, is insufficient to model the complexity and diversity of human opinions encountered in an actual subjective experiment. In this work we propose a complementary approach for objective media quality assessment that attempts to more closely model what happens in a subjective experiment in terms of single observers and, at the same time, we perform a qualitative analysis of the proposed approach while highlighting its suitability. More precisely, we propose to model, using neural networks (NNs), the way single observers perceive media quality. Once trained, these NNs, one for each observer, are expected to mimic the corresponding observer in terms of quality perception. Then, similarly to a subjective experiment, such NNs can be used to simulate the users' single opinions, which can be later aggregated by means of different statistical indicators such as average, standard deviation, quantiles, etc. Unlike previous approaches that consider subjective experiments as a black box providing reliable ground truth data for training, the proposed approach is able to consider human factors by analyzing and weighting individual observers. Such a model may therefore implicitly account for users' expectations and tendencies, that have been shown in many studies to significantly correlate with visual quality perception. Furthermore, our proposal also introduces and investigates an index measuring how much inconsistency there would be if an observer was asked to rate many times the same stimulus. Simulation experiments conducted on several datasets demonstrate that the proposed approach can be effectively implemented in practice and thus yielding a more complete objective assessment of end users' quality of experience.
C1 [Tiotsop, Lohic Fotio; Servetti, Antonio; Masala, Enrico] Politecn Torino, Corso Duca Abruzzi 24, I-10129 Turin, Italy.
   [Mizdos, Tomas; Pocta, Peter] Univ Zilina, Univ 8215-1, SK-01026 Zilina, Slovakia.
   [Barkowsky, Marcus] Univ Appl Sci, Deggendorf Inst Technol, Dieter Gorlitz Pl 1, D-94469 Deggendorf, Germany.
C3 Polytechnic University of Turin; University of Zilina
RP Tiotsop, LF (corresponding author), Politecn Torino, Corso Duca Abruzzi 24, I-10129 Turin, Italy.
EM lohic.fotiotiotsop@polito.it; tomas.mizdos@feit.uniza.sk;
   Marcus.Barkowsky@th-deg.de; peter.pocta@feit.uniza.sk;
   antonio.servetti@polito.it; enrico.masala@polito.it
RI Pocta, Peter/A-6228-2010; Servetti, Antonio/GRS-8230-2022
OI Pocta, Peter/0000-0001-6791-1325; SERVETTI, Antonio/0000-0002-0159-5718
FU PIC4SeR
FX This work has been supported in part by PIC4SeR
   (http://pic4ser.polito.it). Some of the computational resources were
   provided by HPC@POLITO(http://www.hpc.polito.it).
CR [Anonymous], 2011, P IWSDS WORKSH PAR I, DOI DOI 10.1007/978-1-4614-1335-6_19
   [Anonymous], 2002, BT50011 ITUR REC
   Bampis CG, 2019, IEEE T CIRC SYST VID, V29, P2256, DOI 10.1109/TCSVT.2018.2868262
   Barkowsky M, 2015, IEICE T COMMUN, VE98B, P2, DOI 10.1587/transcom.E98.B.2
   Birch J, 1997, OPHTHAL PHYSL OPT, V17, P403, DOI 10.1111/j.1475-1313.1997.tb00072.x
   Brunnstrom K., 2012, EUROPEAN NETWORK QUA
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   Cisco, 2020, ANN INT REP GROWTH I
   Deldjoo Y, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3407190
   Galloso I, 2016, MULTIMED TOOLS APPL, V75, P12365, DOI 10.1007/s11042-016-3360-z
   Hossfeld T., 2016, Quality and User Experience, V1, P1, DOI [10.1007/S41233-016-0002-1, DOI 10.1007/S41233-016-0002-1, 10.1007/s41233-016-0002-1]
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   Huang QH, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2598779
   Hyder Mansoor, 2012, INT MULT TOP C EM TR, V281, DOI [10.1007/978-3-642-28962-0_20, DOI 10.1007/978-3-642-28962-0_20]
   Internet Media Group, 2019, EXT ITS4S DAT
   ITU-T, 2008, ITU T P910 SUBJECTIV
   Jalal L, 2018, IEEE T BROADCAST, V64, P552, DOI 10.1109/TBC.2018.2823914
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Janowski L, 2009, INT WORK QUAL MULTIM, P35, DOI 10.1109/QOMEX.2009.5246979
   Knoche H, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1556134.1556137
   Korhonen J, 2019, PROC CVPR IEEE, P8161, DOI 10.1109/CVPR.2019.00836
   Krasula L, 2020, IEEE T MULTIMEDIA, V22, P961, DOI 10.1109/TMM.2019.2935687
   Le Callet P, 2006, IEEE T NEURAL NETWOR, V17, P1316, DOI 10.1109/TNN.2006.879766
   Leszczuk M, 2016, MULTIMED TOOLS APPL, V75, P10745, DOI 10.1007/s11042-014-2229-2
   Li Z, 2017, IEEE DATA COMPR CONF, P52, DOI 10.1109/DCC.2017.26
   Liddell TM, 2018, J EXP SOC PSYCHOL, V79, P328, DOI 10.1016/j.jesp.2018.08.009
   Mitra K, 2015, IEEE T MOBILE COMPUT, V14, P920, DOI 10.1109/TMC.2013.155
   Mullin Jim, 2001, IHM-HCI Tutorial, P1
   Naumann AB, 2010, INTERACT COMPUT, V22, P465, DOI 10.1016/j.intcom.2010.08.005
   Netflix, 2019, VMAF VID MULT ASS FU
   Norman G, 2010, ADV HEALTH SCI EDUC, V15, P625, DOI 10.1007/s10459-010-9222-y
   Palhais J, 2012, MOBILE NETWORKS MANA, V97, P261, DOI 10.1007/978-3-642-30422-419
   Pinson M. H, 2018, TM18532 NTIA
   Rhemtulla M, 2012, PSYCHOL METHODS, V17, P354, DOI 10.1037/a0029315
   Ries M., 2007, 2007 2 INT S WIR PER, DOI [10.1109/ISWPC.2007.342629, DOI 10.1109/ISWPC.2007.342629]
   Scott MJ, 2016, IEEE T MULTIMEDIA, V18, P1796, DOI 10.1109/TMM.2016.2574623
   Seufert M, 2019, INT WORK QUAL MULTIM
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Streijl RC, 2016, MULTIMEDIA SYST, V22, P213, DOI 10.1007/s00530-014-0446-1
   Sue Stevens, 2007, Community Eye Health, V20, P52
   Tiotsop L. F., 2020, P HUM VIS EL IM C HV
   Tiotsop LF, 2020, INT CONF ACOUST SPEE, P2737, DOI [10.1109/icassp40776.2020.9053739, 10.1109/ICASSP40776.2020.9053739]
   Tiotsop LF, 2019, INT WORK QUAL MULTIM
   Varga D, 2018, IEEE INT CON MULTI
   Varga D, 2019, NEURAL PROCESS LETT, V50, P2595, DOI 10.1007/s11063-019-10036-6
   Video Qual. Experts Group Mountain View CA USA, 2010, Report on theValidation of Video Quality Models for High Definition Video Content
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei XC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4482
   Winkler S, 2008, IEEE T BROADCAST, V54, P660, DOI 10.1109/TBC.2008.2000733
   Xu L, 2016, IEEE T MULTIMEDIA, V18, P590, DOI 10.1109/TMM.2016.2525004
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Yang W, 2012, J COMPUT, V7, P161, DOI 10.4304/jcp.7.1.161-168
   You JY, 2019, IEEE IMAGE PROC, P2349, DOI [10.1109/icip.2019.8803395, 10.1109/ICIP.2019.8803395]
   Zeng Hui, 2017, ARXIV170808190V2
   Zhang Y, 2020, IEEE T NEUR NET LEAR, V31, P2716, DOI 10.1109/TNNLS.2018.2890310
   Zhu Y, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183512
NR 57
TC 5
Z9 5
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 12
DI 10.1145/3464393
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900012
DA 2024-07-18
ER

PT J
AU Lv, ZH
   Song, HB
AF Lv, Zhihan
   Song, Houbing
TI Trust Mechanism of Feedback Trust Weight in Multimedia Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Network fuzzy theory; trust mechanism; collaborative filtering
   algorithm; FTWDF; simulation; FTWDF-EEFA
ID NEURAL-NETWORK; MODEL; ALLOCATION; ALGORITHM; INTERNET
AB It is necessary to solve the inaccurate data arising from data reliability ignored by most data fusion algorithms drawing upon collaborative filtering and fuzzy network theory. Therefore, a model is constructed based on the collaborative filtering algorithm and fuzzy network theory to calculate the node trust value as the weight of weighted data fusion. First, a FTWDF ( Feedback Trust Weighted for Data Fusion) is proposed. Second, EEFA (Efficiency unequal Fuzzy clustering Algorithm) is introduced into FTWDF considering the defects of the clustering structure caused by ignoring the randomness of node energy consumption and cluster head selection in the practical application of the existing data fusion algorithm. Besides, the fuzzy logic is applied to cluster head selection and node clustering. Finally, an FTWDF-EEFA clustering algorithm is constructed for generating candidate cluster head nodes, which is verified by simulation experiments. The comparative analysis reveals that the accuracy of the FTWDF-EEFA clustering algorithm is 4.1% higher than that of the TMDF (Trust Multiple attributes Decision-making-based data Fusion) algorithm, and 8.3% higher than that of LDTS (Larger Data fusion based on node Trust evaluation in wireless Sensor networks) algorithm. It performs better in accuracy and recommendation results during the processing of ML100M dataset and NF5M dataset. Besides, the new clustering algorithm increases the survival time of nodes when analyzing the number of death nodes to prolong networks' lifespan. It improves the survival period of nodes, balances the network load, and prolongs networks' lifespan. Furthermore, the FTWDF-EEFA clustering algorithm can balance nodes' energy consumption and effectively save nodes' overall energy through analysis. Therefore, the optimized algorithm can increase the lifespan of network and improve the trust mechanism effectively. The performance of the algorithm has reached the expected effect, providing a reference for the practical application of the trust mechanism in networks.
C1 [Lv, Zhihan] Uppsala Univ, Fac Arts, Dept Game Design, SE-75105 Uppsala, Sweden.
   [Song, Houbing] Embry Riddle Aeronaut Univ, Dept Elect Engn & Comp Sci, Daytona Beach, FL 32114 USA.
C3 Uppsala University; Embry-Riddle Aeronautical University
RP Lv, ZH (corresponding author), Uppsala Univ, Fac Arts, Dept Game Design, SE-75105 Uppsala, Sweden.
EM lvzhihan@gmail.com; h.song@ieee.org
RI Song, Houbing Herbert/E-3628-2010; song, hu/JVO-3838-2024; Lyu,
   Zhihan/I-3187-2014; Lv, Zhihan/GLR-6000-2022
OI Song, Houbing Herbert/0000-0003-2631-9223; Lyu,
   Zhihan/0000-0003-2525-3074; Lv, Zhihan/0000-0003-2525-3074
FU National Natural Science Foundation of China (NSFC) [61902203]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant No. 61902203.
CR Abrazeh S, 2021, IEEE-CAA J AUTOMATIC, V8, P690, DOI 10.1109/JAS.2021.1003889
   Almogren A, 2021, IEEE INTERNET THINGS, V8, P4485, DOI 10.1109/JIOT.2020.3027440
   [Anonymous], 2016, IEEE COMPUT ARCHIT L, V4, P1, DOI [10.1109/TCC.2016.2521459, DOI 10.1109/TCC.2016.2521459]
   Arun, 2017, J COMPUT THEOR NANOS, V14, P5427, DOI DOI 10.1166/JCTN.2017.6966
   Bharanitharan K, 2013, ACM T EMBED COMPUT S, V12, DOI 10.1145/2423636.2423647
   Chen JQ, 2020, IEEE T INTELL TRANSP, V21, P135, DOI [10.1109/TITS.2018.2889746, 10.1109/ieee-iws.2019.8803964]
   Chhabra A, 2018, INT J COMMUN SYST, V31, DOI 10.1002/dac.3487
   Chiregi Matin, 2017, Journal of Service Science Research, V9, P1, DOI 10.1007/s12927-017-0001-7
   Cui YY, 2020, AEU-INT J ELECTRON C, V118, DOI 10.1016/j.aeue.2020.153134
   Deepak Gerard, 2020, International Journal of Computers and Applications, V42, P729, DOI 10.1080/1206212X.2018.1480472
   Ding ZG, 2019, INFORM SCIENCES, V486, P62, DOI 10.1016/j.ins.2019.02.028
   Granatyr J, 2019, IEEE SYS MAN CYBERN, P3840, DOI 10.1109/SMC.2019.8914641
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Guo JP, 2020, ELECTRON COMMER R A, V39, DOI 10.1016/j.elerap.2019.100828
   Hepworth J., 2020, IEEE CAA J AUTOMATIC, V2020, P1, DOI [10.1109/JAS.2020.1003545, DOI 10.1109/JAS.2020.1003545]
   Hussain S, 2019, J INTELL FUZZY SYST, V37, P2769, DOI 10.3233/JIFS-18760
   Jiang JF, 2017, IEEE COMMUN MAG, V55, P110, DOI 10.1109/MCOM.2017.1600502CM
   Lakshmi V. N., 2017, ADV COMPUTATIONAL SC, V10, P2257
   Lamrabet O, 2017, 2017 3RD INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL AND IMAGE PROCESSING (ATSIP), P61
   Liu Q, 2019, EURASIP J WIREL COMM, DOI 10.1186/s13638-019-1340-5
   Liu SX, 2021, IEEE T COMPUT SOC SY, V8, P1249, DOI 10.1109/TCSS.2020.3021179
   Liu S, 2020, COMPUT COMMUN, V151, P437, DOI 10.1016/j.comcom.2020.01.024
   Liu S, 2019, IEEE ACCESS, V7, P21343, DOI 10.1109/ACCESS.2019.2896699
   Liu XH, 2021, CLUSTER COMPUT, V24, P1901, DOI 10.1007/s10586-021-03235-1
   Liu XH, 2019, IEEE ACCESS, V7, P126913, DOI 10.1109/ACCESS.2019.2939423
   Liu XH, 2021, APPL INTELL, V51, P9015, DOI 10.1007/s10489-021-02303-8
   Magoi JS, 2020, MALAYS J LIBR INF SC, V25, P82, DOI 10.22452/mjlis.vol25no1.5
   Navia M, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010028
   Pouyanfar S, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3150226
   Prabhu B., 2017, MULTIDISCIPLINARY J, V3, P27
   Reddy VB, 2017, IEEE SENS J, V17, P3921, DOI 10.1109/JSEN.2017.2699561
   Sumalatha MS, 2021, J AMB INTEL HUM COMP, V12, P4559, DOI 10.1007/s12652-020-01834-1
   Talbi S, 2017, TELECOMMUN SYST, V65, P605, DOI 10.1007/s11235-016-0254-3
   Wang YM, 2018, EURASIP J WIREL COMM, DOI 10.1186/s13638-018-1174-6
   Yang JN, 2019, IEEE T WIREL COMMUN, V18, P1779, DOI 10.1109/TWC.2019.2897296
   Yi LZ, 2018, J PARALLEL DISTR COM, V118, P57, DOI 10.1016/j.jpdc.2017.03.005
   Yuan J, 2018, IEEE ACCESS, V6, P23626, DOI 10.1109/ACCESS.2018.2831898
   Zhang DG, 2020, IEEE ACCESS, V8, P69058, DOI 10.1109/ACCESS.2020.2986078
   Zhang DG, 2021, MOBILE NETW APPL, V26, P523, DOI 10.1007/s11036-018-1123-y
   Zhang DG, 2020, WIREL NETW, V26, P1503, DOI 10.1007/s11276-019-02216-y
   Zhang DG, 2019, IEEE ACCESS, V7, P158514, DOI 10.1109/ACCESS.2019.2950266
   Zhang DG, 2018, J NETW COMPUT APPL, V122, P37, DOI 10.1016/j.jnca.2018.07.018
   Zhang DG, 2018, EURASIP J WIREL COMM, DOI 10.1186/s13638-018-1176-4
   Zhang DG, 2018, MOBILE NETW APPL, V23, P828, DOI 10.1007/s11036-017-0878-x
   Zhang DG, 2017, SOFT COMPUT, V21, P7313, DOI 10.1007/s00500-016-2270-3
   Zhang DG, 2017, J NETW COMPUT APPL, V88, P1, DOI 10.1016/j.jnca.2017.03.025
   Zhang DG, 2015, SOFT COMPUT, V19, P1817, DOI 10.1007/s00500-014-1366-x
   Zhang DG, 2012, COMPUT MATH APPL, V64, P1044, DOI 10.1016/j.camwa.2012.03.023
   Zhang DG, 2012, APPL INTELL, V36, P75, DOI 10.1007/s10489-010-0245-0
   Zhang DG, 2021, INT J COMMUN SYST, V34, DOI 10.1002/dac.4647
   Zhang DG, 2020, AEU-INT J ELECTRON C, V126, DOI 10.1016/j.aeue.2020.153372
   Zhang DG, 2019, APPL INTELL, V49, P1866, DOI 10.1007/s10489-018-1368-y
   Zhang PY, 2020, IEEE T COMPUT SOC SY, V7, P790, DOI 10.1109/TCSS.2020.2990103
   Zhang PY, 2021, IEEE T SYST MAN CY-S, V51, P1805, DOI 10.1109/TSMC.2019.2906310
   Zhang PY, 2018, IEEE T INF FOREN SEC, V13, P2167, DOI 10.1109/TIFS.2018.2812166
   Zhang SS, 2019, PROCEDIA COMPUT SCI, V147, P473, DOI 10.1016/j.procs.2019.01.275
   Zhang T, 2021, NEUROCOMPUTING, V420, P98, DOI 10.1016/j.neucom.2020.09.042
   Zhang T, 2019, IEEE ACCESS, V7, P82571, DOI 10.1109/ACCESS.2019.2921310
NR 58
TC 87
Z9 87
U1 4
U2 64
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 140
DI 10.1145/3391296
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800024
DA 2024-07-18
ER

PT J
AU Li, Y
   Liu, GC
   Sun, YB
   Liu, QS
   Chen, SY
AF Li, Yang
   Liu, Guangcan
   Sun, Yubao
   Liu, Qingshan
   Chen, Shengyong
TI 3D Tensor Auto-encoder with Application to Video Compression
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video compression; tensor; auto-encoder; ADAM
AB Auto-encoder has been widely used to compress high-dimensional data such as the images and videos. However, the traditional auto-encoder network needs to store a large number of parameters. Namely, when the input data is of dimension n, the number of parameters in an auto-encoder is in generalO(n). In this article, we introduce a network structure called 3D Tensor Auto-Encoder (3DTAE). Unlike the traditional auto-encoder, in which a video is represented as a vector, our 3DTAE considers videos as 3D tensors to directly pass tensor objects through the network. The weights of each layer are represented by three small matrices, and thus the number of parameters in 3DTAE is just O(n(1/3)). The compact nature of 3DTAE fits well the needs of video compression. Given an ensemble of high-dimensional videos, we represent them as 3DTAE networks plus some small core tensors, and we further quantize the network parameters and the core tensors to get the final compressed data. Experimental results verify the efficiency of 3DTAE.
C1 [Li, Yang] Jiangsu Vocat Coll Informat Technol, Sch Informat Secur, Sch IoT Engn, 1 Qianou Rd, Wuxi 214153, Jiangsu, Peoples R China.
   [Liu, Guangcan; Sun, Yubao; Liu, Qingshan] Nanjing Univ Informat Sci & Technol, 219 Ningliu Rd, Nanjing 210044, Peoples R China.
   [Chen, Shengyong] Tianjin Univ Technol, Minist Educ, Key Lab Comp Vis & Syst, 319 Binshui West Rd, Tianjin 300384, Peoples R China.
C3 Jiangsu Vocational College of Information Technology; Nanjing University
   of Information Science & Technology; Tianjin University of Technology
RP Li, Y (corresponding author), Jiangsu Vocat Coll Informat Technol, Sch Informat Secur, Sch IoT Engn, 1 Qianou Rd, Wuxi 214153, Jiangsu, Peoples R China.
EM liyang_19901222@163.com; gcliu@nuist.edu.cn; ybsun@nuist.edu.cn;
   qsliu@nuist.edu.cn; csy@tjut.edu.cn
RI liu, qingqing/HHD-0360-2022; Chen, S./H-3083-2011; Liu,
   Guangcan/J-1391-2014; Liu, Qingqing/HMV-4816-2023; Chen,
   Rainie/ISS-6016-2023; Liu, Qing/GWC-9222-2022
OI Liu, Guangcan/0000-0002-9428-4387; 
FU New Generation AI Major Project of Ministry of Science and Technology of
   China [2018AAA0100601]; National Natural Science Foundation of China
   (NSFC) [62020106004, 61532009, 61672292, 61703304]; Higher Vocational
   Education Teaching Fusion Production Integration Platform Construction
   Projects of Jiangsu Province [2019(26)]; ''Qing Lan Project" Teaching
   Team in Colleges and Universities of Jiangsu Province [2017(15)]; High
   Level of Jiangsu Province Key Construction Project Fund [2017(17)];
   Jiangsu Vocational College of Information Technology [10072020028(001)]
FX This work was supported in part by the New Generation AI Major Project
   of Ministry of Science and Technology of China under Grant
   2018AAA0100601, in part by National Natural Science Foundation of China
   (NSFC) under Grant 62020106004, Grant 61532009, Grant 61672292, and
   Grant 61703304, in part by Higher Vocational Education Teaching Fusion
   Production Integration Platform Construction Projects of Jiangsu
   Province under Grant No.2019(26), in part by "Qing Lan Project" Teaching
   Team in Colleges and Universities of Jiangsu Province under Grant
   No.2017(15), in part by High Level of Jiangsu Province Key Construction
   Project Fund under Grant No.2017(17), in part by Research Project of
   Jiangsu Vocational College of Information Technology under Grant
   10072020028(001).
CR Agababov Victor, 2015, NSDI'15
   Amiri SA, 2018, MULTIMED TOOLS APPL, V77, P8677, DOI 10.1007/s11042-017-4763-1
   [Anonymous], 2018, ARXIV180511360
   [Anonymous], 230903 ISOIEC CD
   [Anonymous], 2017, NIPS
   Bader B. W., 2015, MATLAB TENSOR TOOLBO
   Bellard F, 2015, BPG IMAGE FORMAT
   Bengua Johann A., 2016, IEEE T SIG P, V99, P1
   Bjontegaard G., 2001, VCEGM33ITUTQ616 BJON
   Bourdev L., 2018, ARXIV PREPRINT ARXIV
   Chen T., 2017, P IEEE VCIP DEC, P1, DOI DOI 10.1109/VCIP.2017.8305033
   Chen ZB, 2020, IEEE T CIRC SYST VID, V30, P566, DOI 10.1109/TCSVT.2019.2892608
   Cheng ZX, 2018, PICT COD SYMP, P253, DOI 10.1109/PCS.2018.8456308
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   Dai WR, 2016, IEEE T IMAGE PROCESS, V25, P4580, DOI 10.1109/TIP.2016.2594490
   Ding C., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587736
   Du B, 2014, IEEE INT CON MULTI
   Dufaux Frederic, 2009, IEEE SIG P MAG, V26, P6
   Dumas T, 2017, INT CONF ACOUST SPEE, P1512, DOI 10.1109/ICASSP.2017.7952409
   Fang LY, 2017, J OPT SOC AM A, V34, P252, DOI 10.1364/JOSAA.34.000252
   Friedland S, 2014, IEEE T IMAGE PROCESS, V23, P4438, DOI 10.1109/TIP.2014.2348796
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Irannejad M, 2018, CIRC SYST SIGNAL PR, V37, P3537, DOI 10.1007/s00034-017-0720-5
   Jiang J, 1999, SIGNAL PROCESS-IMAGE, V14, P737, DOI 10.1016/S0923-5965(98)00041-1
   Kamisli F, 2013, IEEE T IMAGE PROCESS, V22, P3916, DOI 10.1109/TIP.2013.2264679
   Kingma D. P., 2014, arXiv
   Krizhevsky Alex, 2012, EUR S ART NEUR NETW
   Li YZ, 2018, PR MACH LEARN RES, V80
   Li Y, 2018, IEEE T CIRC SYST VID, V28, P2316, DOI 10.1109/TCSVT.2017.2727682
   Liu JY, 2019, IEEE T IMAGE PROCESS, V28, P2140, DOI 10.1109/TIP.2018.2882923
   Liu Q., 2018, ARXIV PREPRINT ARXIV
   Liu ZY, 2016, IEEE T IMAGE PROCESS, V25, P5088, DOI 10.1109/TIP.2016.2601264
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462
   Mukherjee D., 2015, SMPTE Motion Imaging Journal, V124, P44
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Song Y, 2017, MULTIMED TOOLS APPL, V76, P10083, DOI 10.1007/s11042-016-3599-4
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Theis L., 2017, ICLR
   Toderici G., 2015, ARXIV PREPRINT ARXIV
   Todeschini G, 2017, INVENTIONS-BASEL, V2, DOI 10.3390/inventions2030014
   van den Oord A, 2016, ADV NEUR IN, V29
   VN Index, 2013, CISCO VISUAL NETWORK
   Vondrick C, 2016, 30 C NEURAL INFORM P, V29
   Wallace G. K., 1991, Communications of the ACM, V34, P30, DOI 10.1145/103085.103089
   Wang QZ, 2018, MULTIMED TOOLS APPL, V77, P1715, DOI 10.1007/s11042-017-4349-y
   Wang TT, 2017, IEEE DATA COMPR CONF, P410, DOI 10.1109/DCC.2017.42
   Wu CY, 2018, LECT NOTES COMPUT SC, V11212, P425, DOI 10.1007/978-3-030-01237-3_26
   Xiph.org Foundation, 2010, XIPH ORG VID TEST ME
   Yang YM, 2018, IEEE T SYST MAN CY-S, V48, P1065, DOI 10.1109/TSMC.2016.2637279
   Zhang JE, 2020, INT REV FINANC, V20, P605, DOI 10.1111/irfi.12234
NR 52
TC 0
Z9 0
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 48
DI 10.1145/3431768
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000013
DA 2024-07-18
ER

PT J
AU Wang, T
   Ji, XJ
   Song, AG
   Madani, K
   Chohra, A
   Lu, HM
   Monero, R
AF Wang, Ting
   Ji, Xiangjun
   Song, Aiguo
   Madani, Kurosh
   Chohra, Amine
   Lu, Huimin
   Monero, Ramon
TI Output-Bounded and RBFNN-Based Position Tracking and Adaptive Force
   Control for Security Tele-Surgery
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security tele-surgery; RBFNN; bilateral position control; force control
ID IMPEDANCE CONTROL; ROBOT
AB In security e-health brain neurosurgery, one of the important processes is to move the electrocoagulation to the appropriate position in order to excavate the diseased tissue.(1) However, it has been problematic for surgeons to freely operate the electrocoagulation, as the workspace is very narrow in the brain. Due to the precision, vulnerability, and important function of brain tissues, it is essential to ensure the precision and safety of brain tissues surrounding the diseased part. The present study proposes the use of a robot-assisted tele-surgery system to accomplish the process. With the aim to achieve accuracy, an output-bounded and RBF neural network-based bilateral position control method was designed to guarantee the stability and accuracy of the operation process. For the purpose of accomplishing a minimal amount of bleeding and damage, an adaptive force control of the slave manipulator was proposed, allowing it to be appropriate to contact the susceptible vessels, nerves, and brain tissues. The stability was analyzed, and the numerical simulation results revealed the high performance of the proposed controls.
C1 [Wang, Ting] Nanjing Tech Univ, Coll Elect Engn & Control Sci, Nanjing 211816, Peoples R China.
   [Wang, Ting; Song, Aiguo] Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
   [Ji, Xiangjun] Jingling Hosp, Dept Neurosurg, 305 East Zhongshan Rd, Nanjing 210002, Peoples R China.
   [Madani, Kurosh; Chohra, Amine] Paris Est Univ, Signals Images & Intelligent Syst Lab LISSI EA 39, Lieusaint, France.
   [Lu, Huimin] Kyushu Inst Technol, Sch Engn, Kitakyushu, Fukuoka 8048550, Japan.
   [Monero, Ramon] IK4 Res Alliance, LORTEK, Arranomendi Kalea 4A, Ordizia 20240, Gipuzkoa, Spain.
C3 Nanjing Tech University; Southeast University - China; Universite
   Paris-Est-Creteil-Val-de-Marne (UPEC); Kyushu Institute of Technology
RP Wang, T (corresponding author), Nanjing Tech Univ, Coll Elect Engn & Control Sci, Nanjing 211816, Peoples R China.; Wang, T (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
RI Chohra, Amine/ABG-3479-2021
OI Chohra, Amine/0000-0002-1145-1371
FU National Natural Science Foundation of China; China Postdoctoral Science
   Foundation
FX The authors would like to thank the National Natural Science Foundation
   of China and the China Postdoctoral Science Foundation for their support
   of this research.
CR Acheson ES, 2018, MED MYCOL, V56, P129, DOI 10.1093/mmy/myx037
   [Anonymous], 1961, STABILITY LIAPUNOVS
   Bergeles C, 2014, IEEE T BIO-MED ENG, V61, P1565, DOI 10.1109/TBME.2013.2293815
   Chan M, 2014, INT J INFECT DIS, V26, P110, DOI 10.1016/j.ijid.2014.05.019
   Hassan K. K., 2002, NONLINEAR SYSTEMS
   Kane G, 2009, LECT NOTES COMPUT SC, V5761, P402, DOI 10.1007/978-3-642-04268-3_50
   Li HB, 2016, ROBOT CIM-INT MANUF, V37, P188, DOI 10.1016/j.rcim.2015.05.002
   Li L, 2019, INT J COMPUT ASS RAD, V14, P2123, DOI 10.1007/s11548-019-02032-x
   Liu Chao., 2018, IFACPapersOnLine, V51, P493
   Ren BB, 2010, IEEE T NEURAL NETWOR, V21, P1339, DOI 10.1109/TNN.2010.2047115
   Sandoval J, 2018, ROBOT AUTON SYST, V106, P95, DOI 10.1016/j.robot.2018.04.001
   Sharifi M, 2018, BIOMED SIGNAL PROCES, V45, P256, DOI 10.1016/j.bspc.2018.05.015
   Tee KP, 2009, AUTOMATICA, V45, P918, DOI 10.1016/j.automatica.2008.11.017
   Ueda H, 2017, PROC CIRP, V65, P110, DOI 10.1016/j.procir.2017.04.027
   Varma TRK, 2006, INT J MED ROBOT COMP, V2, P107, DOI 10.1002/rcs.88
   Vitiello Valentina, 2013, IEEE Rev Biomed Eng, V6, P111, DOI 10.1109/RBME.2012.2236311
   Yashiro D, 2017, IFAC PAPERSONLINE, V50, P12059, DOI 10.1016/j.ifacol.2017.08.2126
NR 17
TC 3
Z9 3
U1 1
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 61
DI 10.1145/3394920
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100004
DA 2024-07-18
ER

PT J
AU Lv, ZH
   Qiao, L
   Singh, AK
   Wang, QJ
AF Lv, Zhihan
   Qiao, Liang
   Singh, Amit Kumar
   Wang, Qingjun
TI Fine-Grained Visual Computing Based on Deep Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fine-grained; visual computing; visual attention mechanism;
   convolutional neural network; image classification
ID ANALYTICS; NETWORK
AB With increasing amounts of information, the image information received by people also increases exponentially. To perform fine-grained categorization and recognition of images and visual calculations, this study combines the Visual Geometry Group Network 16 model of convolutional neural networks and the vision attention mechanism to build amulti-level fine-grained image feature categorization model. Finally, the Tensor-Flow platform is utilized to simulate the fine-grained image classification model based on the visual attention mechanism. The results show that in terms of accuracy and required training time, the fine-grained image categorization effect of the multi-level feature categorization model constructed by this study is optimal, with an accuracy rate of 85.3% and a minimum training time of 108 s. In the similarity effect analysis, it is found that the chi-square distance between Log Gabor features and the degree of image distortion show a strong positive correlation; in addition, the validity of this measure is verified. Therefore, through the research in this study, it is found that the constructed fine-grained image categorization model has higher accuracy in image recognition categorization, shorter training time, and significantly better performance in similar feature effects, which provides an experimental reference for the visual computing of fine-grained images in the future.
C1 [Lv, Zhihan; Qiao, Liang] Qingdao Univ, Sch Data Sci & Software Engn, Qingdao 266071, Peoples R China.
   [Singh, Amit Kumar] Natl Inst Technol Patna, Dept Comp Sci & Engn, Patna 800005, Bihar, India.
   [Wang, Qingjun] Shenyang Aerosp Univ, Shenyang 110136, Peoples R China.
   [Wang, Qingjun] Nanjing Univ Aeronaut & Astronaut, Nanjing, Peoples R China.
C3 Qingdao University; National Institute of Technology (NIT System);
   National Institute of Technology Patna; Shenyang Aerospace University;
   Nanjing University of Aeronautics & Astronautics
RP Lv, ZH (corresponding author), Qingdao Univ, Sch Data Sci & Software Engn, Qingdao 266071, Peoples R China.
EM lvzhihan@gmail.com; leonqiaoove@gmail.com; amit.singh@nitp.ac.in;
   wangqingjun@sau.edu.cn
RI Lyu, Zhihan/I-3187-2014; Lv, Zhihan/GLR-6000-2022; Singh, Amit
   Kumar/D-1300-2015
OI Lyu, Zhihan/0000-0003-2525-3074; Lv, Zhihan/0000-0003-2525-3074; Singh,
   Amit Kumar/0000-0001-7359-2068; Qiao, Liang/0000-0002-8188-886X
FU National Natural Science Foundation of China [61902203]; Key Research
   and Development Plan-Major Scientific and Technological Innovation
   Projects of ShanDong Province [2019JZZY020101]
FX This work was supported by the National Natural Science Foundation of
   China (61902203) and Key Research and Development Plan-Major Scientific
   and Technological Innovation Projects of ShanDong Province
   (2019JZZY020101).
CR Abdul W, 2017, IEEE ACCESS, V5, P5531, DOI 10.1109/ACCESS.2017.2693438
   Baz I, 2019, IEEE ACCESS, V7, P76376, DOI 10.1109/ACCESS.2019.2921994
   Bi HX, 2019, IEEE T GEOSCI REMOTE, V57, P9378, DOI 10.1109/TGRS.2019.2926434
   Chen L, 2019, MULTIMED TOOLS APPL, V78, P4381, DOI 10.1007/s11042-018-5875-y
   Elgohari B., 2019, ARXIV190705919
   Fu JW, 2018, IEEE ACCESS, V6, P14510, DOI 10.1109/ACCESS.2018.2805301
   Hazarika S, 2020, IEEE T VIS COMPUT GR, V26, P34, DOI 10.1109/TVCG.2019.2934591
   Hemanth DJ, 2018, IEEE ACCESS, V6, P39487, DOI 10.1109/ACCESS.2018.2855260
   Hohman F, 2019, IEEE T VIS COMPUT GR, V25, P2674, DOI 10.1109/TVCG.2018.2843369
   Huang YQ, 2020, IEEE T IMAGE PROCESS, V29, P4013, DOI 10.1109/TIP.2020.2969330
   Kaneko A, 2020, VIS COMPUT IND BIOME, V3, DOI 10.1186/s42492-019-0040-7
   Khayat M, 2020, IEEE T VIS COMPUT GR, V26, P353, DOI 10.1109/TVCG.2019.2934264
   Liu D, 2020, IEEE T IMAGE PROCESS, V29, P3695, DOI 10.1109/TIP.2020.2964518
   Liu YH, 2019, IEEE ACCESS, V7, P98122, DOI 10.1109/ACCESS.2019.2929061
   Lv ZH, 2015, PERS UBIQUIT COMPUT, V19, P551, DOI 10.1007/s00779-015-0844-1
   Lv Z, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2645860
   Mei HH, 2020, IEEE T VIS COMPUT GR, V26, P1161, DOI 10.1109/TVCG.2019.2934800
   Min SB, 2020, IEEE T IMAGE PROCESS, V29, P4996, DOI 10.1109/TIP.2020.2977457
   Pan MY, 2020, IEEE ACCESS, V8, P32767, DOI 10.1109/ACCESS.2020.2973856
   Park D, 2018, IEEE T VIS COMPUT GR, V24, P361, DOI 10.1109/TVCG.2017.2744478
   Peng YX, 2020, IEEE T IMAGE PROCESS, V29, P2728, DOI 10.1109/TIP.2019.2952085
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Seebacher D, 2019, IEEE COMPUT GRAPH, V39, P83, DOI 10.1109/MCG.2019.2926242
   Shi C, 2019, IEEE ACCESS, V7, P113853, DOI 10.1109/ACCESS.2019.2932051
   Shu XB, 2018, IEEE T CIRC SYST VID, V28, P454, DOI 10.1109/TCSVT.2016.2607345
   Soenen T, 2019, IEEE COMMUN MAG, V57, P89, DOI 10.1109/MCOM.2019.1800810
   Strobelt H, 2019, IEEE T VIS COMPUT GR, V25, P353, DOI 10.1109/TVCG.2018.2865044
   Wang QF, 2019, IEEE ACCESS, V7, P18450, DOI 10.1109/ACCESS.2019.2896409
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Wei ZQ, 2019, IEEE ACCESS, V7, P100029, DOI 10.1109/ACCESS.2019.2929939
   Wentzel A, 2020, IEEE T VIS COMPUT GR, V26, P949, DOI 10.1109/TVCG.2019.2934546
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Yao HT, 2018, IEEE T IMAGE PROCESS, V27, P10, DOI 10.1109/TIP.2017.2751960
   Zhang Y. D., 2019, ARXIV190602365
   Zhong ZL, 2018, IEEE T GEOSCI REMOTE, V56, P847, DOI 10.1109/TGRS.2017.2755542
   Zhou XZ, 2020, IEEE T AFFECT COMPUT, V11, P542, DOI 10.1109/TAFFC.2018.2828819
   Zhou ZG, 2019, IEEE T VIS COMPUT GR, V25, P43, DOI 10.1109/TVCG.2018.2864503
   Zhu L, 2018, IEEE T GEOSCI REMOTE, V56, P5046, DOI 10.1109/TGRS.2018.2805286
NR 38
TC 27
Z9 27
U1 5
U2 62
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 16
DI 10.1145/3418215
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900016
DA 2024-07-18
ER

PT J
AU Xiao, JS
   Xu, HH
   Gao, HH
   Bian, MJ
   Li, Y
AF Xiao, Junsheng
   Xu, Huahu
   Gao, Honghao
   Bian, Minjie
   Li, Yang
TI A Weakly Supervised Semantic Segmentation Network by Aggregating Seed
   Cues: The Multi-Object Proposal Generation Perspective
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Weakly supervised semantic segmentation; image-level annotations;
   high-confidence seed map
AB Weakly supervised semantic segmentation under image-level annotations is effectiveness for real-world applications. The small and sparse discriminative regions obtained froman image classification network that are typically used as the important initial location of semantic segmentation also form the bottleneck. Although deep convolutional neural networks (DCNNs) have exhibited promising performances for single-label image classification tasks, images of the real-world usually contain multiple categories, which is still an open problem. So, the problem of obtaining high-confidence discriminative regions from multi-label classification networks remains unsolved. To solve this problem, this article proposes an innovative three-step framework within the perspective of multi-object proposal generation. First, an image is divided into candidate boxes using the object proposal method. The candidate boxes are sent to a single-classification network to obtain the discriminative regions. Second, the discriminative regions are aggregated to obtain a high-confidence seed map. Third, the seed cues grow on the feature maps of high-level semantics produced by a backbone segmentation network. Experiments are carried out on the PASCAL VOC 2012 dataset to verify the effectiveness of our approach, which is shown to outperform other baseline image segmentation methods.
C1 [Xiao, Junsheng; Gao, Honghao; Li, Yang] Shanghai Univ, Sch Comp Engn & Sci, ShangDa St 99, Shanghai 200444, Peoples R China.
   [Xu, Huahu; Bian, Minjie] Shanghai Univ, Informat Off, ShangDa St 99, Shanghai 200444, Peoples R China.
C3 Shanghai University; Shanghai University
RP Gao, HH (corresponding author), Shanghai Univ, Sch Comp Engn & Sci, ShangDa St 99, Shanghai 200444, Peoples R China.
EM xpceo@126.com; huahuxu@163.com; gaohonghao@shu.edu.cn
RI Gao, Honghao/AAX-4529-2020
OI Gao, Honghao/0000-0001-6861-9684
FU National Science Foundation of China [61902236]; CERNET Innovation
   Project [NGII20180617]; Foundation of Henan Educational Committee Grant
   [19A520005]; National Key RAMP;D Program of China [2020YFB1006003]
FX This study was supported by the National Science Foundation of China,
   under Grant N (61902236), the CERNET Innovation Project (NGII20180617),
   the Foundation of Henan Educational Committee Grant (19A520005), and the
   National Key R&D Program of China under Grant (2020YFB1006003).
CR ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Bearman Amy, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Bin Jin, 2017, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR.2017.185
   Borges VRP, 2018, IEEE ACM T COMPUT BI, V15, P257, DOI 10.1109/TCBB.2016.2615606
   Chen JT, 2021, IEEE ACM T COMPUT BI, V18, P103, DOI 10.1109/TCBB.2020.2991173
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Liang-Chieh, 2014, Comput Sci, DOI DOI 10.48550/ARXIV.1412.7062
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Ding ST, 2020, NEUROCOMPUTING, V398, P520, DOI 10.1016/j.neucom.2019.04.095
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Gao WJ, 2019, COMPUT INTELL-US, V35, P496, DOI 10.1111/coin.12202
   Gong Yunchao., 2013, Deep convolutional ranking for multilabel image annotation
   Hou QB, 2018, ADV NEUR IN, V31
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203
   Pathak D, 2015, IEEE I CONF COMP VIS, P1796, DOI 10.1109/ICCV.2015.209
   Qi XJ, 2016, LECT NOTES COMPUT SC, V9912, P90, DOI 10.1007/978-3-319-46484-8_6
   Roy A, 2017, PROC CVPR IEEE, P7282, DOI 10.1109/CVPR.2017.770
   Shen T, 2018, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2018.00148
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shimoda W, 2016, LECT NOTES COMPUT SC, V9908, P218, DOI 10.1007/978-3-319-46493-0_14
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456
   Wan SH, 2020, IEEE T MULTIMEDIA, V22, P1756, DOI 10.1109/TMM.2020.2976573
   Wan SH, 2019, FUTURE GENER COMP SY, V91, P382, DOI 10.1016/j.future.2018.08.007
   Wang X, 2018, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2018.00147
   Wei Y., 2017, P INT C COMPUTER VIS, P1354
   Wei YC, 2016, PATTERN RECOGN, V59, P234, DOI 10.1016/j.patcog.2016.01.015
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Xi YL, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115648
   Xu J, 2015, PROC CVPR IEEE, P3781, DOI 10.1109/CVPR.2015.7299002
   Yu J., 2019, IEEE T NEUR NET LEAR
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu J, 2018, IEEE T IND ELECTRON, V65, P5060, DOI 10.1109/TIE.2017.2739691
   Yu J, 2018, IEEE T INF FOREN SEC, V13, P1317, DOI 10.1109/TIFS.2017.2787986
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 47
TC 60
Z9 60
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 15
DI 10.1145/3419842
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900015
DA 2024-07-18
ER

PT J
AU Yu, Y
   Srivastava, A
   Canales, S
AF Yu, Yi
   Srivastava, Abhishek
   Canales, Simon
TI Conditional LSTM-GAN for Melody Generation from Lyrics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Lyrics-conditioned melody generation; conditional LSTM-GAN
AB Melody generation from lyrics has been a challenging research issue in the field of artificial intelligence and music, which enables us to learn and discover latent relationships between interesting lyrics and accompanying melodies. Unfortunately, the limited availability of a paired lyrics-melody dataset with alignment information has hindered the research progress. To address this problem, we create a large dataset consisting of 12,197 MIDI songs each with paired lyrics and melody alignment through leveraging different music sources where alignment relationship between syllables and music attributes is extracted. Most importantly, we propose a novel deep generative model, conditional Long Short-Term Memory (LSTM) Generative Adversarial Network for melody generation from lyrics, which contains a deep LSTM generator and a deep LSTM discriminator both conditioned on lyrics. In particular, lyrics-conditioned melody and alignment relationship between syllables of given lyrics and notes of predicted melody are generated simultaneously. Extensive experimental results have proved the effectiveness of our proposed lyrics-to-melody generative model, where plausible and tuneful sequences can be inferred from lyrics.
C1 [Yu, Yi] Natl Inst Informat, Digital Content & Media Sci Res Div, Chiyoda Ku, 2-1-2 Hitotsubashi, Tokyo 1018430, Japan.
   [Srivastava, Abhishek] Indraprastha Inst Informat Technol Delhi, Multimodal Digital Media Anal Lab, New Delhi 110020, India.
   [Canales, Simon] Ecole Polytech Fed Lausanne, Inst Genie Elect & Elect, Route Cantonale, CH-1015 Lausanne, Switzerland.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; Indraprastha Institute of
   Information Technology Delhi; Swiss Federal Institutes of Technology
   Domain; Ecole Polytechnique Federale de Lausanne
RP Yu, Y (corresponding author), Natl Inst Informat, Digital Content & Media Sci Res Div, Chiyoda Ku, 2-1-2 Hitotsubashi, Tokyo 1018430, Japan.
EM yiyu@nii.ac.jp; abhishek18124@iiitd.ac.in; simon.canales@epfl.ch
CR Anders T, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1978802.1978809
   [Anonymous], 2017, ARXIV171204371
   [Anonymous], 2015, ARXIV151104581
   [Anonymous], 2003, Proceedings of the AISB Symposium on Artificial Intelligence and Creativity in the Arts and Sciences
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Cope D., 2005, Computer Models of Musical Creativity
   Cui Lei, 2018, ARXIV180904318
   Delgado M, 2009, EXPERT SYST APPL, V36, P4574, DOI 10.1016/j.eswa.2008.05.028
   Deng Kangle, P 28 INT JOINT C ART, P2216, DOI [10.24963/ijcai.2019/307, DOI 10.24963/IJCAI.2019/307]
   Eigenfeldt A., 2010, P INT C COMP CREAT, P16
   Fang Jhih-Sheng, P 2019 C N AM CHAPT
   Fukayama Satoru, 2010, P 7 SOUND MUS COMP C, P299
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hiller Jr Lejaren A., 1958, Journal of the Audio Engineering Society, V6, P154
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jang Eric, 2016, ARXIVSTATML161101144
   Johnson DD, 2017, LECT NOTES COMPUT SC, V10198, P128, DOI 10.1007/978-3-319-55750-2_9
   Loker David, 2016, ARXIV161201058
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mogren O., 2016, CORR
   Monteith Kristine, 2012, ICCC, P87
   Patel Ankit, P 7 INT C LEARN REPR
   Ponsford D, 1999, J NEW MUSIC RES, V28, P150, DOI 10.1076/jnmr.28.2.150.3115
   Ramdas A, 2015, AAAI CONF ARTIF INTE, P3571
   Rodriguez J.D.F., 2014, ABS14020585 CORR
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Togelius Julian, 2015, 6 INT C COMP CREAT I, P204
   Wiggins GA, 2006, KNOWL-BASED SYST, V19, P449, DOI 10.1016/j.knosys.2006.04.009
   Wikipedia, SYLL
   Yu Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3281746
   Yu Yong, 2016, ARXIVCSLG160905473
   Yuan MK, 2020, IEEE T CIRC SYST VID, V30, P4258, DOI 10.1109/TCSVT.2019.2953753
NR 32
TC 46
Z9 50
U1 6
U2 37
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 35
DI 10.1145/3424116
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zeng, K
   Hu, JC
   Gong, YY
   Wattanachote, K
   Yu, RP
   Luo, XN
AF Zeng, Kun
   Hu, Jiangchuan
   Gong, Yongyi
   Wattanachote, Kanoksak
   Yu, Runpeng
   Luo, Xiaonan
TI Vertical Retargeting for Stereoscopic Images via Stereo Seam Carving
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Stereo image retargeting; occluded regions; occluding regions; coupling
   strategies; vertical retargeting
ID DEPTH; RELAXATION; VIDEO
AB Vertical retargeting for stereoscopic images using seam manipulation-based approaches has remained an open challenge over the years. Even though horizontal retargeting had attracted a huge amount of interest, its seam coupling strategies were not capable to construct valid seam pairs for vertical retargeting. In this article, we propose two seam coupling strategies for vertical retargeting, namely, real mapping and virtual mapping. Our proposed mapping strategies were implemented to address the problems of multiple assignments and missing assignments, which are able to occur in the straightforward generalization from horizontal retargeting to vertical retargeting. On the basis of our proposed method, stereo seams were allowed to lay across occluded regions and occluding regions in stereo images. We maintained the geometric consistency by removing occluded pixels and corresponding occluding pixels in both stereo images. As a result, our method guarantees valid and geometrically consistent stereo seam pairs to be found in the horizontal direction. We generate vertically retargeted stereo images by removing or adding horizontal seam pairs iteratively. We conducted experiments on a number of indoor and outdoor scenes. Experimental results demonstrated that our method overcomes the limitations of vertical retargeting and is effective in preserving the geometric consistency.
C1 [Zeng, Kun; Hu, Jiangchuan] Sun Yat Sen Univ, Sch Data & Comp Sci, Zhongshan Univ, Guangzhou Higher Educ Mega Ctr, Guangzhou 510006, Peoples R China.
   [Gong, Yongyi; Wattanachote, Kanoksak; Yu, Runpeng] Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou Higher Educ Mega Ctr, Guangzhou 510006, Peoples R China.
   [Luo, Xiaonan] Guilin Univ Elect Technol, Sch Comp Sci & Informat Secur, 1 Jinji Rd, Guilin 541004, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Guilin University of
   Electronic Technology
RP Gong, YY (corresponding author), Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou Higher Educ Mega Ctr, Guangzhou 510006, Peoples R China.
EM zengkun2@mail.sysu.edu.cn; hujch3@mail2.sysu.edu.cn;
   gongyongyi@gdufs.edu.cn; kanoksak.wattanachote@gmail.com;
   20161003430@gdufs.edu.cn; luoxn@guet.edu.cn
FU National Science Foundation of China [U1711266, 61370160, 61772149];
   Guangdong Basic and Applied Basic Research Foundation [2019A1515011078];
   Guangzhou Scientific and Technological Plan Project [201904010228]
FX The work was supported by National Science Foundation of China, Grants
   No. U1711266, No. 61370160, and No. 61772149, Guangdong Basic and
   Applied Basic Research Foundation Grant No. 2019A1515011078, and
   Guangzhou Scientific and Technological Plan Project No. 201904010228.
CR [Anonymous], 2010, 2010 3DTV C TRUE VIS
   [Anonymous], 2015, P IEEE INT C MULT EX, DOI DOI 10.1109/ICME.2015.7177529
   [Anonymous], 2007, IEEE Conference on Computer Vision and Pattern Recognition
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bohari Umaima A., 2015, INT J ELECT COMMUN S, P124
   Chai XL, 2019, IEEE ACCESS, V7, P25239, DOI 10.1109/ACCESS.2019.2896918
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cho SH, 2013, LECT NOTES COMPUT SC, V8033, P290, DOI 10.1007/978-3-642-41914-0_29
   Dekel T, 2013, IEEE T PATTERN ANAL, V35, P2513, DOI 10.1109/TPAMI.2013.46
   Fang YM, 2016, INFORM SCIENCES, V372, P347, DOI 10.1016/j.ins.2016.08.062
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Fusiello A, 2000, MACH VISION APPL, V12, P16, DOI 10.1007/s001380050120
   Gong YY, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321513
   Guthier B., 2013, IEEE IVMSP WORKSH, P1
   Guthier Benjamin, 2013, TECH REP, V13
   Nguyen HT, 2013, ETRI J, V35, P980, DOI 10.4218/etrij.13.2013.0032
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Jung SW, 2013, IEEE T IMAGE PROCESS, V22, P3892, DOI 10.1109/TIP.2013.2263150
   Junle Wang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P669, DOI 10.1109/ICASSP.2014.6853680
   Lee KY, 2012, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2012.6247657
   Lei JB, 2017, J HEALTHC ENG, V2017, P1, DOI 10.1155/2017/1053403
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   Lei JJ, 2014, J DISP TECHNOL, V10, P373, DOI 10.1109/JDT.2014.2312648
   Li B, 2014, IEEE IMAGE PROC, P2903, DOI 10.1109/ICIP.2014.7025587
   Li B, 2015, IEEE T IMAGE PROCESS, V24, P2811, DOI 10.1109/TIP.2015.2431441
   Lien KC, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P571, DOI 10.1109/3DV.2015.70
   Lin SS, 2014, IEEE T CIRC SYST VID, V24, P759, DOI 10.1109/TCSVT.2013.2291282
   Lu DW, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.2.023029
   NAKAYAMA K, 1990, VISION RES, V30, P1811, DOI 10.1016/0042-6989(90)90161-D
   Oh C, 2015, IEEE T BROADCAST, V61, P142, DOI 10.1109/TBC.2015.2402471
   Park H, 2014, IEEE T MULTIMEDIA, V16, P326, DOI 10.1109/TMM.2013.2286567
   Patel D, 2019, PATTERN RECOGN LETT, V125, P798, DOI 10.1016/j.patrec.2019.07.018
   Qi SY, 2016, IEEE T IMAGE PROCESS, V25, P2222, DOI 10.1109/TIP.2016.2528040
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Shao F, 2019, IEEE ACCESS, V7, P22541, DOI 10.1109/ACCESS.2019.2892098
   Shao F, 2017, IEEE T CYBERNETICS, V47, P4521, DOI 10.1109/TCYB.2016.2615856
   Shao F, 2017, IEEE T IMAGE PROCESS, V26, P4790, DOI 10.1109/TIP.2017.2721546
   Shao F, 2015, DISPLAYS, V39, P125, DOI 10.1016/j.displa.2015.10.001
   Shao Feng, 2015, J DISP TECHNOL, V12, P22
   Wattanachote K, 2015, IEEE T INF FOREN SEC, V10, P2477, DOI 10.1109/TIFS.2015.2464776
   Yu L, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION, CYBERNETICS AND COMPUTATIONAL SOCIAL SYSTEMS (ICCSS), P1, DOI 10.1109/ICCSS.2017.8091372
   Yue Bin, 2013, EURASIP J WIREL COMM, V2013, P116
   Zhang DY, 2017, J VIS COMMUN IMAGE R, V48, P281, DOI 10.1016/j.jvcir.2017.07.006
   Zhao Yin, 2010, IEEE SIGNAL PROCESS, V18, P19
   Zhou YZ, 2018, IEEE T IMAGE PROCESS, V27, P2301, DOI 10.1109/TIP.2017.2779272
NR 47
TC 0
Z9 0
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 125
DI 10.1145/3408295
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800009
DA 2024-07-18
ER

PT J
AU Su, YT
   Li, WH
   Nie, WZ
   Liu, AA
AF Su, Yu-Ting
   Li, Wen-Hui
   Nie, Wei-Zhi
   Liu, An-An
TI Multi-View Graph Matching for 3D Model Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D model retrieval; graph matching; unsupervised learning
ID OBJECT RETRIEVAL; RECOGNITION; CLASSIFICATION; SEARCH
AB 3D model retrieval has been widely utilized in numerous domains, such as computer-aided design, digital entertainment, and virtual reality. Recently, many graph-based methods have been proposed to address this task by using multi-viewinformation of 3D models. However, these methods are always constrained bymanyto-many graph matching for the similarity measure between pairwise models. In this article, we propose a multi-view graph matching method (MVGM) for 3D model retrieval. The proposed method can decompose the complicated multi-view graph-based similarity measure into multiple single-view graph-based similarity measures and fusion. First, we present the method for single-view graph generation, and we further propose the novel method for the similarity measure in a single-view graph by leveraging both node-wise context and model-wise context. Then, we propose multi-view fusion with diffusion, which can collaboratively integrate multiple single-view similarities w.r.t. different viewpoints and adaptively learn their weights, to compute the multi-view similarity between pairwise models. In this way, the proposed method can avoid the difficulty in the definition and computation of the traditional high-order graph. Moreover, this method is unsupervised and does not require a large-scale 3D dataset for model learning. We conduct evaluations on four popular and challenging datasets. The extensive experiments demonstrate the superiority and effectiveness of the proposed method compared against the state of the art. In particular, this unsupervised method can achieve competitive performances against the most recent supervised and deep learning method.
C1 [Su, Yu-Ting; Li, Wen-Hui; Nie, Wei-Zhi; Liu, An-An] Tianjin Univ, 92 Weijin Rd, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Li, WH; Liu, AA (corresponding author), Tianjin Univ, 92 Weijin Rd, Tianjin 300072, Peoples R China.
EM ytsu@tju.edu.cn; liwenhui@tju.edu.cn; weizhinie@tju.edu.cn;
   anan0422@gmail.com
RI Lu, Wang/JVO-0416-2024; Zeng, Yun/JFK-6190-2023; Nie,
   Weizhi/ABF-5316-2021; LI, Wenhui/JCD-9947-2023
OI nie, weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China [61572356, 61772359,
   61872267]; 2019 Tianjin New Generation Artificial Intelligence Major
   Program [19ZXZNGX00110]; 2018 Tianjin New Generation Artificial
   Intelligence Major Program [18ZXZNGX00150]; 2017 Tianjin Research
   Program of Application Foundation and Advanced Technology grant
   [17ZXRGGX00180]; Open Project Program of the State Key Lab of CAD & CG,
   Zhejiang University [A1907]; Elite Scholar Program of Tianjin University
   grant [2019XRX-0035]
FX This work was supported in part by the National Natural Science
   Foundation of China (61572356, 61772359, 61872267), a 2019 Tianjin New
   Generation Artificial Intelligence Major Program grant (19ZXZNGX00110),
   a 2018 Tianjin New Generation Artificial Intelligence Major Program
   grant (18ZXZNGX00150), a 2017 Tianjin Research Program of Application
   Foundation and Advanced Technology grant (17ZXRGGX00180), the Open
   Project Program of the State Key Lab of CAD & CG, Zhejiang University
   (grant A1907), and a Elite Scholar Program of Tianjin University grant
   (2019XRX-0035).
CR An-An Liu, 2017, IEEE Transactions on Pattern Analysis and Machine Intelligence, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Ankerst M, 1999, LECT NOTES COMPUT SC, V1651, P207
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359
   Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen ZN, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231742
   Cheng ZY, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3291060
   Daras P, 2010, INT J COMPUT VISION, V89, P229, DOI 10.1007/s11263-009-0277-2
   Donoser M, 2013, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2013.174
   Dang-Nguyen DT, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3103613
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Gao SH, 2016, IEEE T CIRC SYST VID, V26, P494, DOI 10.1109/TCSVT.2015.2389413
   Gao Y, 2014, IEEE MULTIMEDIA, V21, P52, DOI 10.1109/MMUL.2014.20
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P2269, DOI 10.1109/TIP.2011.2170081
   Gao Y, 2011, SIGNAL PROCESS-IMAGE, V26, P39, DOI 10.1016/j.image.2010.10.006
   Gao Z, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P117, DOI 10.1145/2964284.2967194
   Gao Zan, 2018, P 32 AAAI C ART INT
   Garro V, 2016, IEEE T PATTERN ANAL, V38, P1258, DOI 10.1109/TPAMI.2015.2477823
   Pedronette DCG, 2013, PATTERN RECOGN, V46, P2350, DOI 10.1016/j.patcog.2013.01.004
   Guo PF, 2018, SHOCK VIB, V2018, DOI 10.1155/2018/2784950
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P5814, DOI 10.1109/TIP.2016.2614132
   Ji RR, 2014, IEEE T IMAGE PROCESS, V23, P3099, DOI 10.1109/TIP.2014.2324291
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leibe B, 2003, PROC CVPR IEEE, P409
   Leng B, 2015, NEUROCOMPUTING, V168, P761, DOI 10.1016/j.neucom.2015.05.048
   Leng B, 2015, NEUROCOMPUTING, V151, P593, DOI 10.1016/j.neucom.2014.06.084
   Li JJ, 2019, IEEE T NEUR NET LEAR, V30, P1381, DOI 10.1109/TNNLS.2018.2868854
   Li W, 2017, MATH PROBL ENG, V2017, DOI 10.1155/2017/8945712
   Liang MJ, 2015, IEEE T CYBERNETICS, V45, P2237, DOI 10.1109/TCYB.2014.2368127
   Liu AA, 2019, IEEE T CIRC SYST VID, V29, P868, DOI 10.1109/TCSVT.2018.2810191
   Liu AA, 2018, IEEE T CYBERNETICS, V48, P916, DOI 10.1109/TCYB.2017.2664503
   Liu AA, 2016, IEEE T IMAGE PROCESS, V25, P2103, DOI 10.1109/TIP.2016.2540802
   Liu A, 2015, INFORM SCIENCES, V320, P429, DOI 10.1016/j.ins.2015.04.042
   Liu XY, 2015, ADV EDUC SCI, V11, P45
   Lu F, 2015, PROC CVPR IEEE, P168, DOI 10.1109/CVPR.2015.7298612
   Lu K, 2014, INFORM SCIENCES, V281, P703, DOI 10.1016/j.ins.2014.03.079
   Mazaheri A, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176647
   Müller H, 2001, PATTERN RECOGN LETT, V22, P593, DOI 10.1016/S0167-8655(00)00118-5
   Nie FP, 2017, IEEE T IMAGE PROCESS, V26, P5718, DOI 10.1109/TIP.2017.2746270
   Nie LQ, 2013, IEEE T MULTIMEDIA, V15, P426, DOI 10.1109/TMM.2012.2229971
   Nie WZ, 2016, J VIS COMMUN IMAGE R, V37, P40, DOI 10.1016/j.jvcir.2015.06.011
   Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386
   Papoiu ADP, 2014, J NEUROPHYSIOL, V112, P1729, DOI 10.1152/jn.00827.2013
   Polewski Przemyslaw, 2014, GEM TAG ZUR SWITZ, P1
   Rubino C, 2018, IEEE T PATTERN ANAL, V40, P1281, DOI 10.1109/TPAMI.2017.2701373
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Song Bai, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P774, DOI 10.1109/ICCV.2017.90
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su YT, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P913
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Tao DP, 2018, IEEE T CIRC SYST VID, V28, P2657, DOI 10.1109/TCSVT.2017.2726580
   Wang HL, 2015, IEEE T CIRC SYST VID, V25, P1900, DOI 10.1109/TCSVT.2015.2477939
   Webber W, 2010, ACM T INFORM SYST, V28, DOI 10.1145/1852102.1852106
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xie J, 2017, IEEE T PATTERN ANAL, V39, P1335, DOI 10.1109/TPAMI.2016.2596722
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang LR, 1996, PATTERN RECOGN, V29, P1061, DOI 10.1016/0031-3203(95)00147-6
   Ye SQ, 2017, IEEE INT WORKS MACH, DOI 10.1109/TPAMI.2017.2762295
   You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702
   Yu J, 2014, IEEE T CYBERNETICS, V44, P2431, DOI 10.1109/TCYB.2014.2307862
   Zhao S, 2015, SIGNAL PROCESS, V112, P110, DOI 10.1016/j.sigpro.2014.09.038
   Zhen Y, 2016, IEEE T CYBERNETICS, V46, P27, DOI 10.1109/TCYB.2015.2392052
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
NR 69
TC 3
Z9 3
U1 5
U2 49
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 77
DI 10.1145/3387920
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200003
DA 2024-07-18
ER

PT J
AU Wang, SF
   Hao, LF
   Ji, Q
AF Wang, Shangfei
   Hao, Longfei
   Ji, Qiang
TI Posed and Spontaneous Expression Distinction Using Latent Regression
   Bayesian Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Latent regression Bayesian network; posed and spontaneous expression
   distinction; spatial pattern; privileged information
ID EMOTION
AB Facial spatial patterns can help distinguish between posed and spontaneous expressions, but this information has not been thoroughly leveraged by current studies. We present several latent regression Bayesian networks (LRBNs) to capture the patterns existing in facial landmark points and to use those points to differentiate posed from spontaneous expressions. The visible nodes of the LRBN represent facial landmark points. Through learning, the LRBN captures the probabilistic dependencies among landmark points as well as latent variables given observations, successfully modeling the spatial patterns inherent in expressions. Current methods tend to ignore gender and expression categories, although these factors can influence spatial patterns. Therefore, we propose to incorporate this as a kind of privileged information. We construct several LRBNs to capture spatial patterns from spontaneous and posed facial expressions given expression-related factors. Facial landmark points are used during testing to classify samples as either posed or spontaneous, depending on which LRBN has the largest likelihood. We conduct experiments to showcase the superiority of the proposed approach in both modeling spatial patterns and classifying expressions as either posed or spontaneous.
C1 [Wang, Shangfei; Hao, Longfei] Univ Sci & Technol China, 443 HuangShan Rd, Hefei 230027, Anhui, Peoples R China.
   [Ji, Qiang] Rensselaer Polytech Inst, 110 8th St, Troy, NY 12180 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Rensselaer Polytechnic Institute
RP Wang, SF (corresponding author), Univ Sci & Technol China, 443 HuangShan Rd, Hefei 230027, Anhui, Peoples R China.
EM sfwang@ustc.edu.cn; hlf101@mail.ustc.edu.cn; qji@ecse.rpi.edu
OI Hao, Longfei/0000-0002-0333-4546
FU National Science Foundation of China [917418129]; Anhui Science and
   Technology Agency [1804a09020038]
FX This work has been supported by the National Science Foundation of China
   (Grant No. 917418129), and the project from Anhui Science and Technology
   Agency (1804a09020038).
CR Abdi H., 2010, Encyclopedia of research design, V1, P935
   Andre E, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2502433
   [Anonymous], P INT C ART INT STAT
   [Anonymous], 2010, P INT C MULT MM, DOI DOI 10.1145/1873951.1874056
   Bengio Yoshua, 2014, P INT C LEARN REPR C
   Bonanno GA, 1997, J ABNORM PSYCHOL, V106, P126, DOI 10.1037/0021-843X.106.1.126
   Cohn JF, 2003, ACTIVE MEDIA TECHNOLOGY, P57, DOI 10.1142/9789812704313_0005
   Dibeklioglu H, 2012, LECT NOTES COMPUT SC, V7574, P525, DOI 10.1007/978-3-642-33712-3_38
   EKMAN P, 1981, PSYCHOPHYSIOLOGY, V18, P101, DOI 10.1111/j.1469-8986.1981.tb02919.x
   Ekman P, 2003, ANN NY ACAD SCI, V1000, P205, DOI 10.1196/annals.1280.010
   EKMAN P, 1982, J NONVERBAL BEHAV, V6, P238, DOI 10.1007/BF00987191
   FUJITA BN, 1980, J NONVERBAL BEHAV, V4, P131, DOI 10.1007/BF00986815
   Gan Q, 2017, AAAI CONF ARTIF INTE, P4039
   Gregor K, 2014, PR MACH LEARN RES, V32, P1242
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2012, Neural networks: tricks of the trade, P599
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Kabir MZ, 2006, CURR APPL PHYS, V6, P393, DOI 10.1016/j.cap.2005.11.026
   Kingma D. P., 2014, arXiv
   Lithari C, 2010, BRAIN TOPOGR, V23, P27, DOI 10.1007/s10548-009-0130-5
   Littlewort GC, 2009, IMAGE VISION COMPUT, V27, P1797, DOI 10.1016/j.imavis.2008.12.010
   Mavadati M, 2016, IEEE COMPUT SOC CONF, P1452, DOI 10.1109/CVPRW.2016.182
   Mnih A, 2014, PR MACH LEARN RES, V32, P1791
   Nakajima T, 2016, INT SYMPOS COMPUT NE, P126, DOI [10.1109/CANDAR.2016.0033, 10.1109/CANDAR.2016.93]
   Namba S, 2017, CURR PSYCHOL, V36, P593, DOI 10.1007/s12144-016-9448-9
   Nie SQ, 2016, INT C PATT RECOG, P3494, DOI 10.1109/ICPR.2016.7900175
   Paul E., BBC DATASET
   Petridis S, 2013, IMAGE VISION COMPUT, V31, P186, DOI 10.1016/j.imavis.2012.08.014
   Pfister T, 2011, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2011.6126401
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Ringeval F, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3181711
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Seckington M., 2011, THESIS
   Shih-Pang Wang, 2015, 2015 IEEE Sensors. Proceedings, P1, DOI 10.1109/ICSENS.2015.7370645
   Valstar M.F., 2006, International Conference on Multimodal Interfaces, P162, DOI DOI 10.1145/1180995.1181031
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Wang SF, 2019, IEEE T IMAGE PROCESS, V28, P1428, DOI 10.1109/TIP.2018.2878339
   Wang SF, 2016, COMPUT VIS IMAGE UND, V147, P69, DOI 10.1016/j.cviu.2015.08.007
   Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716
   Yuille A.L., 2005, ADV INFORM PROCESSIN, V17, P1593
   Zhang FF, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176646
   Zhang LG, 2011, LECT NOTES COMPUT SC, V7064, P431, DOI 10.1007/978-3-642-24965-5_49
NR 43
TC 3
Z9 3
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 80
DI 10.1145/3391290
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA NO3HD
UT WOS:000569375200006
DA 2024-07-18
ER

PT J
AU Pham, S
   Heeren, P
   Schmidt, C
   Silhavy, D
   Arbanowski, S
AF Pham, Stefan
   Heeren, Patrick
   Schmidt, Calvin
   Silhavy, Daniel
   Arbanowski, Stefan
TI Evaluation of Shared Resource Allocation Using SAND for ABR Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE SAND; DANE; shared resource allocation; software defined networking;
   OpenFlow; streaming video metrics; dash.js
AB Adaptive bitrate media streaming clients adjust the quality of media content depending on the current network conditions. The shared resource allocation (SRA) feature defined in MPEG-SAND (server and network assisted DASH) allows servers to allocate bandwidth to streaming clients. This enables coordination and prioritization of clients that are connected to the same network bottleneck (e.g., to maximize the number of clients that can play back a stream fluently). In this article, we evaluate different bandwidth limitation strategies and analyze the effects on the clients. For this purpose, a testbed using multiple Raspberry Pis was created. The results show that in various scenarios, SRA improves the fairness and the QoE of streaming sessions. Solely allocating a maximum quality level to the client is not sufficient in some cases. Therefore, additional means, such as limiting bandwidth on the client or traffic shaping with software-defined networking for SRA, are evaluated.
C1 [Pham, Stefan; Heeren, Patrick; Silhavy, Daniel; Arbanowski, Stefan] Fraunhofer FOKUS, Berlin, Germany.
   [Schmidt, Calvin] TU Berlin, Berlin, Germany.
   [Heeren, Patrick] Charite Univ Med Berlin, Berlin, Germany.
C3 Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss
   Birlinghoven; Technical University of Berlin; Free University of Berlin;
   Humboldt University of Berlin; Charite Universitatsmedizin Berlin
RP Pham, S (corresponding author), Fraunhofer FOKUS, Berlin, Germany.
EM stefan.pham@fokus.fraunhofer.de; p.heeren@mailbox.org;
   calvin.sclimidt@web.de; daniel.silhavy@fokus.fraunhofer.de;
   stefan.arbanowski@fokus.fraunhofer.de
OI Pham, Stefan/0009-0003-5026-6333; Silhavy, Daniel/0009-0008-1291-2804;
   Heeren, Patrick/0000-0001-9356-2296
CR [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], 2017, 2300952017 ISO IEC
   Bentaleb A, 2017, IEEE T MULTIMEDIA, V19, P2136, DOI 10.1109/TMM.2017.2733344
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Cisco Systems, 2020, CISC ANN INT REP
   Cofano G, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P24, DOI 10.1145/2910017.2910597
   DASH Industry Forum, 2016, DASH IF POS PAP SERV
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   Github, 2019, RYU SDN FRAM
   Hossfeld T., 2018, IEEEACM T NET, V3, P1
   Kleinrouweler J.W., 2017, Proceedings of the 27th Workshop on Network and Operating Systems Support for Digital Audio and Video, P73, DOI DOI 10.1145/3083165.3083167
   Kleinrouweler JW, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P36, DOI 10.1145/2910017.2910599
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Narisetty R, 2013, 2013 SECOND GENI RESEARCH AND EDUCATIONAL EXPERIMENT WORKSHOP (GREE), P66, DOI 10.1109/GREE.2013.21
   Palma David, 2014, 2014 Third European Workshop on Software Defined Networks (EWSDN), P125, DOI 10.1109/EWSDN.2014.34
   Pfaff B., 2015, 12 USENIX S NETW SYS
   Pham S, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P165, DOI 10.1145/3304109.3306227
   Seddiki M.S., 2014, Proceedings of the Third Workshop on Hot Topics in Software Defined Networking, HotSDN'14, P207, DOI DOI 10.1145/2620728.2620766
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Thomas E., 2016, P IBC C, P1, DOI DOI 10.1049/IBC.2016.0022
   Zabrovskiy Anatoliy, 2017, P 21 C OP INN ASS FR
NR 22
TC 4
Z9 4
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 70
DI 10.1145/3388926
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FH
UT WOS:000583712600013
DA 2024-07-18
ER

PT J
AU Xia, KJ
   Yin, HS
   Jin, Y
   Qiu, S
   Zhao, HR
AF Xia, Kaijian
   Yin, Hongsheng
   Jin, Yong
   Qiu, Shi
   Zhao, Hongru
TI Cross-Domain Brain CT Image Smart Segmentation via Shared Hidden Space
   Transfer FCM Clustering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image segmentation; transfer clustering; FCM clustering; clinical
   diagnosis
ID ALGORITHM; CUTS
AB Clustering is an important issue in brain medical image segmentation. Original medical images used for clinical diagnosis are often insufficient for clustering in the current domain. As there are sufficient medical images in the related domains, transfer clustering can improve the clustering performance of the current domain by transferring knowledge across the related domains. In this article, we propose a novel shared hidden space transfer fuzzy c-means (FCM) clustering called SHST-FCM for cross-domain brain computed tomography (CT) image segmentation. SHST-FCM projects both the data samples of the source domain and target domain into the shared hidden space, such that the distributions of the two domains are as close as possible. In the learned shared subspace, the data samples of the source domain serve as the auxiliary knowledge to aid the clustering process in the target domain. Extensive experiments on brain CT medical image datasets indicate the effectiveness of the proposed method.
C1 [Xia, Kaijian] Soochow Univ, Affiliated Changshu Hosp, Suzhou, Jiangsu, Peoples R China.
   [Yin, Hongsheng] China Univ Min & Technol, Sch Informat & Control Engn, Daxue Rd, Xuzhou 221116, Jiangsu, Peoples R China.
   [Jin, Yong] Changshu Inst Technol, Sch Comp Sci & Engn, 99 South Third Ring Rd, Changshu 215500, Jiangsu, Peoples R China.
   [Qiu, Shi] Chinese Acad Sci, Xian Inst Opt & Precis Mech, 17 Informat Ave, Xian 710119, Shanxi, Peoples R China.
   [Zhao, Hongru] Soochow Univ, Dept Neurol, Affiliated Hosp 1, 899 Pinghai Rd, Suzhou 215006, Jiangsu, Peoples R China.
   [Xia, Kaijian] Changshu 1 Peoples Hosp, Shuyuan Rd, Changshu 215500, Jiangsu, Peoples R China.
C3 Soochow University - China; China University of Mining & Technology;
   Changshu Institute of Technology; Chinese Academy of Sciences; Xi'an
   Institute of Optics & Precision Mechanics, CAS; Soochow University -
   China
RP Xia, KJ (corresponding author), Changshu 1 Peoples Hosp, Shuyuan Rd, Changshu 215500, Jiangsu, Peoples R China.
EM xiakaijian@163.com; xuzhouyhs@sina.com; jinyong@cslg.edu.cn;
   qiushi215@163.com; tiantan11@163.com
RI Qiu, Shi/HGI-9191-2022
FU Jiangsu Committee of Health on the subject [H2018071]; open Fund Project
   of Jiangsu Key Laboratory of Media Design and Software Technology
   (Jiangnan University) [19ST0205]
FX This work was supported in part by the Jiangsu Committee of Health on
   the subject (No. H2018071), and by the open Fund Project of Jiangsu Key
   Laboratory of Media Design and Software Technology (Jiangnan University)
   under Grant 19ST0205.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Agrawal R, 2018, BMJ CASE REP, V2018, P1, DOI DOI 10.1007/S40031-018-0314-Z
   Altameem T, 2015, CONNECT SCI, V27, P305, DOI 10.1080/09540091.2014.970126
   [Anonymous], 2006, P ADV NEUR INF PROC
   [Anonymous], 2012, COMPUT VIS ECCV
   AqilBurney S. M., 2014, Int. J. Comput. Appl., V96, P1, DOI [10.5120/16779-6360, DOI 10.5120/16779-6360]
   Babcock L, 2012, BRAIN INJURY, V26, P1372, DOI 10.3109/02699052.2012.694565
   Bezdek James C., 1981, PATTERN RECOGN
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Chuen Rue Ng, 2015, 2015 International Conference on BioSignal Analysis, Processing and Systems (ICBAPS), P52, DOI 10.1109/ICBAPS.2015.7292217
   Davis L.S., 1975, Comput. Graph. Image Process, V4, P248, DOI DOI 10.1016/0146-664X(75)90012-X
   Deng ZH, 2016, IEEE T FUZZY SYST, V24, P1210, DOI 10.1109/TFUZZ.2015.2505330
   Elazab A, 2016, J X-RAY SCI TECHNOL, V24, P489, DOI 10.3233/XST-160563
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Gómez D, 2015, KNOWL-BASED SYST, V87, P26, DOI 10.1016/j.knosys.2015.07.017
   Nguyen HS, 2016, NEURORADIOL J, V29, P372, DOI 10.1177/1971400916658795
   Han DM, 2018, EXPERT SYST APPL, V95, P43, DOI 10.1016/j.eswa.2017.11.028
   Hathaway RJ, 2000, IEEE T FUZZY SYST, V8, P576, DOI 10.1109/91.873580
   Hooijmans MT, 2015, J MAGN RESON IMAGING, V42, P217, DOI 10.1002/jmri.24775
   Huang H, 2019, IEEE ACCESS, V7, P12386, DOI 10.1109/ACCESS.2019.2893063
   Hung CC, 2011, IEEE J-STSP, V5, P543, DOI 10.1109/JSTSP.2010.2096797
   Kalemis A, 2013, MAGN RESON MATER PHY, V26, P5, DOI 10.1007/s10334-012-0330-y
   Kurnmamuru K, 2003, IEEE INT CONF FUZZY, P772
   Son LH, 2012, EXPERT SYST APPL, V39, P9848, DOI 10.1016/j.eswa.2012.02.167
   Lee MC, 2016, BRIT J RADIOL, V89, DOI 10.1259/bjr.20150059
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ortiz A, 2013, COMPUT MATH METHOD M, V2013, DOI 10.1155/2013/638563
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pal NR, 2005, IEEE T FUZZY SYST, V13, P517, DOI 10.1109/TFUZZ.2004.840099
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pan WK, 2013, ARTIF INTELL, V197, P39, DOI 10.1016/j.artint.2013.01.003
   Qian PJ, 2017, KNOWL-BASED SYST, V130, P33, DOI 10.1016/j.knosys.2017.05.018
   Qian PJ, 2016, PATTERN RECOGN, V50, P155, DOI 10.1016/j.patcog.2015.08.009
   Sheikh YA, 2007, IEEE I CONF COMP VIS, P1175
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Smith SM, 2002, HUM BRAIN MAPP, V17, P143, DOI 10.1002/hbm.10062
   Taha AA, 2015, BMC MED IMAGING, V15, DOI 10.1186/s12880-015-0068-x
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang XY, 2016, NEURAL NETWORKS, V74, P1, DOI 10.1016/j.neunet.2015.10.012
   Wang Y, 2018, J MED IMAG HEALTH IN, V8, P602, DOI 10.1166/jmihi.2018.2309
   Wenhao Jiang, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P789, DOI 10.1007/978-3-642-33486-3_50
   Zhang XT, 2017, NEUROCOMPUTING, V251, P145, DOI 10.1016/j.neucom.2017.04.029
   Zhao XM, 2018, MED IMAGE ANAL, V43, P98, DOI 10.1016/j.media.2017.10.002
NR 44
TC 5
Z9 5
U1 1
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 61
DI 10.1145/3357233
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FH
UT WOS:000583712600004
DA 2024-07-18
ER

PT J
AU Yu, DF
   Fu, JL
   Tian, XM
   Mei, T
AF Yu, Dongfei
   Fu, Jianlong
   Tian, Xinmei
   Mei, Tao
TI Multi-source Multi-level Attention Networks for Visual Question
   Answering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual question answering; attention model; multi-modal representations;
   visual relationship
AB In recent years, Visual Question Answering (VQA) has attracted increasing attention due to its requirement on cross-modal understanding and reasoning of vision and language. VQA is proposed to automatically answer natural language questions with reference to a given image. VQA is challenging, because the reasoning process on a visual domain needs a full understanding of the spatial relationship, semantic concepts, as well as the common sense for a real image. However, most existing approaches jointly embed the abstract low-level visual features and high-level question features to infer answers. These works have limited reasoning ability due to the lack of modeling of the rich spatial context of regions, high-level semantics of images, and knowledge across multiple sources. To solve the challenges, we propose multi-source multi-level attention networks for visual question answering that can benefit both spatial inferences by visual attention on context-aware region representation and reasoning by semantic attention on concepts as well as external knowledge. Indeed, we learn to reason on image representation by question-guided attention at different levels across multiple sources, including region and concept level representation from image source as well as sentence level representation from the external knowledge base. First, we encode region-based middle-level outputs from Convolutional Neural Networks (CNNs) into spatially embedded representation by a multi-directional two-dimensional recurrent neural network and, further, locate the answer-related regions by Multiple Layer Perceptron as visual attention. Second, we generate semantic concepts from high-level semantics in CNNs and select those question-related concepts as concept attention. Third, we query semantic knowledge from the general knowledge base by concepts and selected question-related knowledge as knowledge attention. Finally, we jointly optimize visual attention, concept attention, knowledge attention, and question embedding by a softmax classifier to infer the final answer. Extensive experiments show the proposed approach achieved significant improvement on two very challenging VQA datasets.
C1 [Yu, Dongfei] Univ Sci & Technol China, Bldg 8,West Campus, Hefei, Anhui, Peoples R China.
   [Fu, Jianlong] Microsoft Res Asia, Microsoft Bldg 2,Danling St, Beijing, Peoples R China.
   [Tian, Xinmei] Univ Sci & Technol China, Room 1203,Tech Bldg, Hefei, Anhui, Peoples R China.
   [Mei, Tao] North Star Century Ctr, JD AI Res 8F,Bldg A,8 Beichen West St, Beijing 100105, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS
RP Yu, DF (corresponding author), Univ Sci & Technol China, Bldg 8,West Campus, Hefei, Anhui, Peoples R China.
EM ydf2010@mail.ustc.edu.cn; jianf@microsoft.com; xinmei@ustc.edu.cn;
   tmei@live.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
FU National Key R&D Program of China [2017YFB1002203]
FX This work was supported in part by the National Key R&D Program of China
   under Contract No. 2017YFB1002203.
CR Agrawal Aishwarya, 2016, P 2016 C EMP METH NA
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2017, P CVPR
   [Anonymous], 2017, ICCV
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2015, PROC 3 INT C LEARNIN
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2016, ARXIV160401485
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Fu JL, 2015, IEEE I CONF COMP VIS, P1985, DOI 10.1109/ICCV.2015.230
   Fu JL, 2015, IEEE T CIRC SYST VID, V25, P1409, DOI 10.1109/TCSVT.2014.2380211
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao H., 2015, INT C NEURAL INF PRO, P2296
   Graves A, 2007, LECT NOTES COMPUT SC, V4668, P549
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P5814, DOI 10.1109/TIP.2016.2614132
   Hong RC, 2014, IEEE T CYBERNETICS, V44, P669, DOI 10.1109/TCYB.2013.2265601
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44
   Kafle K, 2017, IEEE I CONF COMP VIS, P1983, DOI 10.1109/ICCV.2017.217
   Kim J., 2017, P 5 INT C LEARN REPR
   Kiros Ryan., 2015, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, P3294
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Q., 2018, C EMP METH NAT LANG, P1338
   Liang JW, 2018, PROC CVPR IEEE, P6135, DOI 10.1109/CVPR.2018.00642
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P1444, DOI 10.1109/ICMA.2017.8016029
   Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289
   Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9
   Mao J, 2015, INT CONF COMP SCI ED, P38, DOI 10.1109/ICCSE.2015.7250214
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Ren M., 2015, NEURIPS, P2953
   Rohrbach M, 2017, ADV COMPUT VIS PATT, P301, DOI 10.1007/978-3-319-50077-5_12
   Santoro A., 2017, ADV NEURAL INFORM PR, V30, P4967
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wang Jingwen., 2016, IJCAI, P3484
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao T, 2017, P IEEE INT C COMP VI, P4894
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhu B, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE), P1291, DOI 10.1109/ICISCE.2017.268
NR 55
TC 16
Z9 16
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 51
DI 10.1145/3316767
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900004
DA 2024-07-18
ER

PT J
AU Xie, HT
   Fang, SC
   Zha, ZJ
   Yang, YT
   Li, Y
   Zhang, YD
AF Xie, Hongtao
   Fang, Shancheng
   Zha, Zheng-Jun
   Yang, Yating
   Li, Yan
   Zhang, Yongdong
TI Convolutional Attention Networks for Scene Text Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Text recognition; text detection; convolutional neural networks;
   multi-level supervised information; attention model
ID IMAGES
AB In this article, we present Convoluitional Attention Networks (CAN) for unconstrained scene text recognition. Recent dominant approaches for scene text recognition are mainly based on Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), where the CNN encodes images and the RNN generates character sequences. Our CAN is different from these methods; our CAN is completely built on CNN and includes an attention mechanism. The distinctive characteristics of our method include (i) CAN follows encoder-decoder architecture, in which the encoder is a deep two-dimensional CNN and the decoder is a one-dimensional CNN; (ii) the attention mechanism is applied in every convolutional layer of the decoder, and we propose a novel spatial attention method using average pooling; and (iii) position embeddings are equipped in both a spatial encoder and a sequence decoder to give our networks a sense of location. We conduct experiments on standard datasets for scene text recognition, including Street View Text, IIIT5K, and ICDAR datasets. The experimental results validate the effectiveness of different components and show that our convolutional-based method achieves state-of-the-art or competitive performance over prior works, even without the use of RNN.
C1 [Xie, Hongtao; Zha, Zheng-Jun; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Anhui, Peoples R China.
   [Fang, Shancheng] Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China.
   [Fang, Shancheng] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China.
   [Yang, Yating] Chinese Acad Sci, Xinjiang Tech Inst Phys & Chem, Urumqi, Peoples R China.
   [Li, Yan] Beijing Kuaishou Technol Co Ltd, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Institute of Information
   Engineering, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Chinese Academy of Sciences; Xinjiang
   Technical Institute of Physics & Chemistry, CAS
RP Zha, ZJ (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Anhui, Peoples R China.; Fang, SC (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China.; Fang, SC (corresponding author), Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China.
EM htxie@ustc.edu.cn; fangshancheng@iie.ac.cn; zhazj@ustc.edu.cn;
   yangyt@ms.xjb.ac.cn; liyan@kuaishou.com; zhyd73@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020; Zha, Zheng-Jun/AAE-8408-2020
OI Zha, Zheng-Jun/0000-0003-2510-8993
FU National Key Research and Development Program of China [2017YFC0820600];
   National Nature Science Foundation of China [61525206, 61771468,
   61622211, 61472392, 61620106009]; Youth Innovation Promotion Association
   Chinese Academy of Sciences [2017209]; Fundamental Research Funds for
   the Central Universities [WK2100100030]
FX This work is supported by the National Key Research and Development
   Program of China (2017YFC0820600), the National Nature Science
   Foundation of China (61525206,61771468,61622211,61472392 and
   61620106009), the Youth Innovation Promotion Association Chinese Academy
   of Sciences (2017209) and the Fundamental Research Funds for the Central
   Universities under Grant WK2100100030.
CR Almazán J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814
   Alsharif  Ouais, 2013, ABS13101811 CORR
   [Anonymous], 2013, ICML
   [Anonymous], ABS14125903 CORR
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102
   Chen ZN, 2014, J COMPUT SCI TECH-CH, V29, P785, DOI 10.1007/s11390-014-1468-z
   Chen ZB, 2017, MOB INF SYST, V2017, DOI 10.1155/2017/7538190
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Fang SC, 2018, NEUROINFORMATICS, V16, P445, DOI 10.1007/s12021-017-9350-0
   Fang SC, 2017, MULTIMED TOOLS APPL, V76, P15083, DOI 10.1007/s11042-017-4538-8
   Gehring J., 2017, P MACHINE LEARNING R, P1243
   Ghosh Suman K., 2017, ABS170601487 CORR
   Gordo A, 2015, PROC CVPR IEEE, P2956, DOI 10.1109/CVPR.2015.7298914
   Graves  A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   Graves A., 2013, GENERATING SEQUENCES
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He P, 2016, AAAI CONF ARTIF INTE, P3501
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jaderberg Max, 2014, WORKSH DEEP LEARN NI
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Jaderberg  Max, 2014, ABS14121842 CORR
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Lucas SM, 2003, PROC INT CONF DOC, P682
   Luong T., 2015, P 2015 C EMP METH NA, DOI [DOI 10.18653/V1/D15-1166, 10.18653/v1/D15-1166]
   Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Rodriguez-Serrano JA, 2015, INT J COMPUT VISION, V113, P193, DOI 10.1007/s11263-014-0793-6
   Rodriguez-Serrano JA, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.5
   Salimans T, 2016, ADV NEUR IN, V29
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452
   Su BL, 2015, LECT NOTES COMPUT SC, V9003, P35, DOI 10.1007/978-3-319-16865-4_3
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43
   Wang T, 2012, INT C PATT RECOG, P3304
   Wojna  Zbigniew, 2017, ABS170403549 CORR
   Xie HT, 2019, PATTERN RECOGN, V85, P109, DOI 10.1016/j.patcog.2018.07.031
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P220, DOI 10.1109/TITS.2017.2749977
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P284, DOI 10.1109/TITS.2017.2749965
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Zhang XS, 2016, IEEE T IMAGE PROCESS, V25, P1033, DOI 10.1109/TIP.2015.2511585
   Zhu B, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808210
NR 56
TC 56
Z9 113
U1 9
U2 61
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 3
DI 10.1145/3231737
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100003
DA 2024-07-18
ER

PT J
AU Wu, J
   Hu, HF
   Wu, Y
AF Wu, Jie
   Hu, Haifeng
   Wu, Yi
TI Image Captioning via Semantic Guidance Attention and Consensus Selection
   Strategy
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image captioning; spatial attention mechanism; semantic guidance
   attention mechanism; beam search; consensus selection strategy
AB Recently, a series of attempts have incorporated spatial attention mechanisms into the task of image captioning, which achieves a remarkable improvement in the quality of generative captions. However, the traditional spatial attention mechanism adopts latent and delayed semantic representations to decide which area should be paid more attention to, resulting in inaccurate semantic guidance and the introduction of redundant information. In order to optimize the spatial attention mechanism, we propose the Semantic Guidance Attention (SGA) mechanism in this article. Specifically, SGA utilizes semantic word representations to provide an intuitive semantic guidance that focuses accurately on semantic-related regions. Moreover, we reduce the difficulty of generating fluent sentences by updating the attention information in time. At the same time, the beam search algorithm is widely used to predict words during sequence generation. This algorithm generates a sentence according to the probabilities of words, so it is easy to push out a generic sentence and discard some distinctive captions. In order to overcome this limitation, we design the Consensus Selection (CS) strategy to choose the most descriptive and informative caption, which is selected by the semantic similarity of captions instead of the probabilities of words:Me consensus caption is determined by selecting the one with the highest cumulative semantic similarity with respect to the reference captions. Our proposed model (SGA-CS) is validated on Flickr30k and MSCOCO, which shows that SGA-CS outperforms state-of-the-art approaches. To our best knowledge, SGA-CS is the first attempt to jointly produce semantic attention guidance and select descriptive captions for image captioning tasks, achieving one of the best performance ratings among any cross-entropy training methods.
C1 [Wu, Jie; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Engn, Guangzhou 510006, Guangdong, Peoples R China.
   [Wu, Yi] Sun Yat Sen Univ, Sch Informat Management, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM wujie23@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn;
   wuyi29@mail2.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; Natural Science Foundation of Guangdong Province
   [2017A030311029, 2016B010109002]; Science and Technology Program of
   Guangzhou, China [201704020180]; Fundamental Research Funds for the
   Central Universities of China
FX This work was supported in part by the National Natural Science
   Foundation of China, under Grant 61673402, 61273270, and 60802069; the
   Natural Science Foundation of Guangdong Province (2017A030311029 and
   2016B010109002); and by the Science and Technology Program of Guangzhou,
   China, under Grant 201704020180, and the Fundamental Research Funds for
   the Central Universities of China.
CR [Anonymous], 2015, ARXIV150904942
   [Anonymous], PROC CVPR IEEE
   [Anonymous], ARXIV170508759
   [Anonymous], SEMANTIC REGULARISAT
   [Anonymous], 2016, P IEEE INT C COMP VI
   [Anonymous], ARXIV161200576
   [Anonymous], 2018, ARXIV180804505
   [Anonymous], 2017, ARXIV PREPRINT ARXIV
   [Anonymous], 2015, ARXIV150504467
   [Anonymous], 2017, ARXIV170407489
   [Anonymous], ELECT LETT
   [Anonymous], 2015, arXiv
   [Anonymous], 2016, ARXIV161105594
   [Anonymous], 2004, 42 ANN M ASS COMP LI
   [Anonymous], 2016, OPEN REV
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Bengio S, 2015, ADV NEUR IN, V28
   Bengio Y., 2014, TECHNICAL REPORT
   Chen H, 2018, AAAI CONF ARTIF INTE, P6706
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   King DB, 2015, ACS SYM SER, V1214, P1
   Lu J., 2016, arXiv1612.01887
   Mao Junhua, 2014, CoRR
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I, 2014, ADV NEUR IN, V27
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Xia Yingce, 2017, ADV NEURAL INFORM PR, P1784, DOI DOI 10.5555/3294771.3294941
   Xu K., 2015, COMPUTER SCI, P2048
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
NR 38
TC 5
Z9 6
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2018
VL 14
IS 4
AR 87
DI 10.1145/3271485
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JB
UT WOS:000457139200009
DA 2024-07-18
ER

PT J
AU Mazaheri, A
   Gong, BQ
   Shah, M
AF Mazaheri, Amir
   Gong, Boqing
   Shah, Mubarak
TI Learning a Multi-Concept Video Retrieval Model with Multiple Latent
   Variables
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video retrieval; multi-concept retrieval; video indexing; structural
   learning
ID DETECTORS; FUSION
AB Effective and efficient video retrieval has become a pressing need in the "big video" era. The objective of this work is to provide a principled model for computing the ranking scores of a video in response to one or more concepts, where the concepts could be directly supplied by users or inferred by the system from the user queries. Indeed, how to deal with multi-concept queries has become a central component in modern video retrieval systems that accept text queries. However, it has been long overlooked and simply implemented by weighted averaging of the corresponding concept detectors' scores. Our approach, which can be considered as a latent ranking SVM, integrates the advantages of various recent works in text and image retrieval, such as choosing ranking over structured prediction, modeling inter-dependencies between querying concepts, and so on. Videos consist of shots, and we use latent variables to account for the mutually complementary cues within and across shots. Concept labels of shots are scarce and noisy. We introduce a simple and effective technique to make our model robust to outliers. Our approach gives superior performance when it is tested on not only the queries seen at training but also novel queries, some of which consist of more concepts than the queries used for training.
C1 [Mazaheri, Amir; Gong, Boqing; Shah, Mubarak] Univ Cent Florida, Ctr Comp Vis Res, 4328 Scorpius St,Suite 245, Orlando, FL 32816 USA.
C3 State University System of Florida; University of Central Florida
RP Mazaheri, A (corresponding author), Univ Cent Florida, Ctr Comp Vis Res, 4328 Scorpius St,Suite 245, Orlando, FL 32816 USA.
EM amirmazaheri@knights.ucf.edu; boqinggo@outlook.com; shah@crcv.ucf.edu
RI Sahoo, Sarat Kumar/J-8765-2014
OI Sahoo, Sarat Kumar/0000-0001-5734-6844; Shah,
   Mubarak/0000-0001-6172-5572; Mazaheri, Amir/0000-0002-2178-2684
FU Intelligence Advanced Research Projects Activity (IARPA) via Department
   of Interior National Business Center [D11PC20066]
FX Supported by the Intelligence Advanced Research Projects Activity
   (IARPA) via Department of Interior National Business Center Contract No.
   D11PC20066. The U.S. Government is authorized to reproduce and
   distribute reprints for Governmental purposes notwithstanding any
   copyright annotation thereon.
CR Althoff Tim, 2012, P ACM C MULT
   [Anonymous], GLOB MOB DAT TRAFF F
   [Anonymous], 2010, P NIPS
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], P INT C COMP VIS ICC
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2013, CISCO VISUAL NETWORK
   Aslandogan YA, 1999, IEEE T KNOWL DATA EN, V11, P56, DOI 10.1109/69.755615
   Assari Shayan, 2014, P P IEEE C COMP VIS
   Aytar Yusuf, 2008, P P IEEE C COMP VIS
   Chang S.-F., 2005, P NIST TRECVID WORKS
   Chang Shih-Fu, 2007, P WORKSH MULT INF RE
   Chapelle O, 2010, INFORM RETRIEVAL, V13, P201, DOI 10.1007/s10791-009-9109-9
   Dehghan Afshin, 2014, P NIST TRECVID WORKS
   Farhadi Alireza, 2009, P P IEEE C COMP VIS
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Haubold Alexander, 2008, P INT C CONT BAS IM
   HAUPTMANN A., 2007, P ACM INT C IM VID R
   Herbrich R, 2000, ADV NEUR IN, P115
   Hou Rui, 2014, P EUR C COMP VIS ECC
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Ishikawa Satoru, 2013, P NIST TRECVID WORKS
   Iyengar Giridharan, 2003, P ACM C MULT
   Järvelin K, 2008, LECT NOTES COMPUT SC, V4956, P4
   Jiang Lu, 2015, P ACM C MULT
   Joachims T., 2002, P ACM SIGKDD C
   Kennedy L, 2008, P IEEE, V96, P567, DOI 10.1109/JPROC.2008.916345
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lan Tian, 2012, P EUR C COMP VIS ECC
   Li Quannan, 2013, P P IEEE C COMP VIS
   Li Xirong, 2007, P ACM INT C IM VID R
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mazaheri A., 2015, NIST TRECVID WORKSH
   McDonald K, 2005, LECT NOTES COMPUT SC, V3568, P61
   Natsev Apostol Paul, 2007, P ACM C MULT
   Neo SY, 2006, LECT NOTES COMPUT SC, V4071, P143
   Over Paul, 2014, P NIST TRECVID WORKS
   Over Paul, 2015, P NIST TRECVID WORKS
   Paul O., 2011, P NIST TRECVID WORKS
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Petterson James, 2010, ADV NEURAL INFORM PR, V23
   Ping Wei, 2014, MARGINAL STRUCTURED
   Qi Guo-Jun, 2007, P ACM C MULT
   Sadanand Sreemanananth, 2012, P P IEEE C COMP VIS
   Shashua A., 2002, NIPS, P937
   Siddiquie Behjat, 2011, P P IEEE C COMP VIS
   Smeaton AF, 2007, INFORM SYST, V32, P545, DOI 10.1016/j.is.2006.09.001
   Smeaton AF, 2009, SIGNALS COMMUN TECHN, P151, DOI 10.1007/978-0-387-76569-3_6
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Snoek C. G. M., 2013, P NIST TRECVID WORKS
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Snoek Cees G. M., 2006, P ACM C MULT
   Tao ChenDamian Borth., 2014, DEEPSENTIBANK VISUAL
   Torresani L., 2010, P EUR C COMP VIS ECC
   Tsochantaridis I., 2004, ICML
   Vapnik V., 1999, NATURE STAT LEARNING
   Vedaldi A., 2010, P ACM C MULT
   Wang Dong, 2007, P ACM C MULT
   Wang Heng, 2013, P INT C COMP VIS ICC
   Wei XY, 2008, IEEE T MULTIMEDIA, V10, P1085, DOI 10.1109/TMM.2008.2001382
   Wu J, 2012, IEEE T MULTIMEDIA, V14, P291, DOI 10.1109/TMM.2011.2174969
   Yan Rong, 2003, P ACM C MULT
   Yang Yang, 2012, P EUR C COMP VIS ECC
   Ye Guangnan, 2015, P ACM C MULT
   Yilmaz E., 2006, Fifteenth ACM International Conference on Information and Knowledge Management, P102
   Yu Felix X., 2012, P IEEE C COMP VIS PA
   Zhang Y, 2016, PROC CVPR IEEE, P5985, DOI 10.1109/CVPR.2016.644
NR 69
TC 8
Z9 8
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
AR 46
DI 10.1145/3176647
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GG9BS
UT WOS:000432996000003
OA Bronze
DA 2024-07-18
ER

PT J
AU Balazia, M
   Sojka, P
AF Balazia, Michal
   Sojka, Petr
TI Gait Recognition from Motion Capture Data
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Gait recognition; MoCap; maximal margin criterion
ID PERCEPTION; EXTRACTION
AB Gait recognition from motion capture data, as a pattern classification discipline, can be improved by the use of machine learning. This article contributes to the state of the art with a statistical approach for extracting robust gait features directly from raw data by a modification of Linear Discriminant Analysis with Maximum Margin Criterion. Experiments on the CMU MoCap database show that the suggested method outperforms 13 relevant methods based on geometric features and a method to learn the features by a combination of Principal Component Analysis and Linear Discriminant Analysis. The methods are evaluated in terms of the distribution of biometric templates in respective feature spaces expressed in a number of class separability coefficients and classification metrics. Results also indicate a high portability of learned features, what means that we can learn what aspects of walk people generally differ in and extract those as general gait features. Recognizing people without needing group-specific features is convenient, as particular people might not always provide annotated learning data. As a contribution to reproducible research, our evaluation framework and database have been made publicly available. This research makes motion capture technology directly applicable for human recognition.
C1 [Balazia, Michal; Sojka, Petr] Masaryk Univ, Fac Informat, Bot 68A, Brno 60200, Czech Republic.
C3 Masaryk University Brno
RP Balazia, M (corresponding author), Masaryk Univ, Fac Informat, Bot 68A, Brno 60200, Czech Republic.
RI Sojka, Petr/JWO-4565-2024; Sojka, Petr/E-5438-2013; Balazia,
   Michal/AAE-9496-2021
OI Sojka, Petr/0000-0002-5768-4007; Balazia, Michal/0000-0001-7153-9984
FU NSF [EIA-0196217]
FX The authors thank the reviewers for their detailed commentary and
   suggestions. The data used in this project were created with funding
   from NSF EIA-0196217 and were obtained from http://mocap.cs.cmu.edu
   [16]. Our extracted database and evaluation framework are available
   online at https://gait.fi.muni.cz to support reproducibility of results.
CR Ahmed F, 2015, VISUAL COMPUT, V31, P915, DOI 10.1007/s00371-015-1092-0
   Ahmed M, 2014, PROC SPIE, V9139, DOI 10.1117/12.2052588
   Ali S, 2016, LECT NOTES COMPUT SC, V9550, P125, DOI 10.1007/978-3-662-49247-5_8
   Andersson VO, 2015, AAAI CONF ARTIF INTE, P425
   Balazia M, 2016, INT C PATT RECOG, P901, DOI 10.1109/ICPR.2016.7899750
   Balazia M, 2017, LECT NOTES COMPUT SC, V10214, P33, DOI 10.1007/978-3-319-56414-2_3
   Balazia M, 2016, LECT NOTES COMPUT SC, V10029, P310, DOI 10.1007/978-3-319-49055-7_28
   Balazia Michal, 2017, GAIT RECOGNITION MOT
   Ball A, 2012, ACMIEEE INT CONF HUM, P225
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Boulgouris N.V., 2009, Biometrics: Theory, Methods and Applications
   Chen X, 2013, LECT NOTES COMPUT SC, V7944, P640
   CMU Graphics Lab, 2003, CARN MELL MOT CAPT M
   CUTTING JE, 1977, B PSYCHONOMIC SOC, V9, P353, DOI 10.3758/BF03337021
   Das Choudhury S, 2015, PATTERN RECOGN, V48, P798, DOI 10.1016/j.patcog.2014.09.022
   DEVANNE M, 2016, P 23 INT C PATT REC, P895, DOI DOI 10.1109/ICPR.2016.7899749
   DIKOVSKI B, 2014, P 37 INT CONV INF CO, P1304, DOI DOI 10.1109/MIPRO.2014.6859769
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Florez-Revuelta F, 2015, 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1
   Han S., 2013, Visualization in Engineering Volume, V1, P1, DOI [DOI 10.1186/2213-7459-1-6, 10.1186/2213-7459-1-6]
   Hong J, 2014, MULTIMED TOOLS APPL, V71, P1999, DOI 10.1007/s11042-012-1319-2
   Jiang SM, 2015, LECT NOTES COMPUT SC, V9008, P46, DOI 10.1007/978-3-319-16628-5_4
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Kamruzzaman J, 2006, IEEE T BIO-MED ENG, V53, P2479, DOI 10.1109/TBME.2006.883697
   Kastaniotis D, 2015, PATTERN RECOGN LETT, V68, P327, DOI 10.1016/j.patrec.2015.06.020
   Kocsor Andras, 2004, MARGIN MAXIMIZING DI, P227, DOI [10.1007/978-3-540-30115-8_23, DOI 10.1007/978-3-540-30115-8_23]
   Krzeszowski T, 2014, LECT NOTES COMPUT SC, V8671, P356, DOI 10.1007/978-3-319-11331-9_43
   Kumar M., 2012, Proceedings of the Eighth Indian Conference on Computer Vision, Graphics and Image Processing, P20
   Kumar R, 2013, 2013 THIRD INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING AND COMMUNICATIONS (ICACC 2013), P102, DOI 10.1109/ICACC.2013.26
   Kwolek B, 2014, LECT NOTES ARTIF INT, V8398, P595, DOI 10.1007/978-3-319-05458-2_61
   Li HF, 2006, IEEE T NEURAL NETWOR, V17, P157, DOI 10.1109/TNN.2005.860852
   Loog M, 2001, IEEE T PATTERN ANAL, V23, P762, DOI 10.1109/34.935849
   Makihara Y, 2011, LECT NOTES COMPUT SC, V6493, P440
   Castro FM, 2017, LECT NOTES COMPUT SC, V10306, P257, DOI 10.1007/978-3-319-59147-6_23
   Murray M P, 1967, Am J Phys Med, V46, P290
   Preis J., 2012, 1st international workshop on kinect in pervasive computing, P1
   Sedmidubsky J, 2012, LECT NOTES COMPUT SC, V7432, P11, DOI 10.1007/978-3-642-33191-6_2
   Su H, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-6, P3198, DOI 10.1109/ICMLC.2009.5212776
   Tafazzoli F, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.1.013036
   Vapnik V., 1999, NATURE STAT LEARNING
   Wahab Y., 2011, 2011 IEEE 15th International Symposium on Consumer Electronics, P20, DOI 10.1109/ISCE.2011.5973775
   Zeng W, 2014, IEEE IJCNN, P3465, DOI 10.1109/IJCNN.2014.6889507
NR 42
TC 23
Z9 24
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2018
VL 14
IS 1
SU S
AR 22
DI 10.1145/3152124
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZK
UT WOS:000433517100008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Houle, ME
   Ma, XG
   Oria, V
   Sun, JC
AF Houle, Michael E.
   Ma, Xiguo
   Oria, Vincent
   Sun, Jichao
TI Query Expansion for Content-Based Similarity Search Using Local and
   Global Features
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Content-based similarity search; unsupervised feature selection;
   subjective feature space; query expansion; flexible aggregation
ID FEATURE-SELECTION; IMAGE RETRIEVAL
AB This article presents an efficient and totally unsupervised content-based similarity search method for multimedia data objects represented by high-dimensional feature vectors. The assumption is that the similarity measure is applicable to feature vectors of arbitrary length. During the offline process, different sets of features are selected by a generalized version of the Laplacian Score in an unsupervised way for individual data objects in the database. Online retrieval is performed by ranking the query object in the feature spaces of candidate objects. Those candidates for which the query object is ranked highly are selected as the query results. The ranking scheme is incorporated into an automated query expansion framework to further improve the semantic quality of the search result. Extensive experiments were conducted on several datasets to show the capability of the proposed method in boosting effectiveness without losing efficiency.
C1 [Houle, Michael E.] Natl Inst Informat, Chiyoda Ku, 2-1-2 Hitotsubashi, Tokyo 1018430, Japan.
   [Ma, Xiguo] Google Mt View, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
   [Oria, Vincent; Sun, Jichao] New Jersey Inst Technol, Newark, NJ 07102 USA.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; Google Incorporated; New Jersey
   Institute of Technology
RP Houle, ME (corresponding author), Natl Inst Informat, Chiyoda Ku, 2-1-2 Hitotsubashi, Tokyo 1018430, Japan.
EM meh@nii.ac.jp; maxiguo@google.com; oria@njit.edu; js87@njit.edu
OI Houle, Michael/0000-0001-8486-8015
FU National Science Foundation (NSF) [DGE 1565478]; Japan Society for the
   Promotion of Science (JSPS) under Kakenhi [25240036, 15H02736];
   Grants-in-Aid for Scientific Research [25240036, 15H02736, 15H02753]
   Funding Source: KAKEN
FX This research was supported by the National Science Foundation (NSF)
   under Grant DGE 1565478 and Japan Society for the Promotion of Science
   (JSPS) under Kakenhi Kiban (A) Research Grant 25240036 and Kiban (B)
   Research Grant 15H02736.
CR [Anonymous], 2006, P BRIT MACH VIS C BM, DOI DOI 10.5244/C.20.92
   [Anonymous], 2005, ADV NEURAL INF PROCE
   [Anonymous], PATTERN RECOGN
   [Anonymous], P INT JOINT C NEUR N
   [Anonymous], IEEE RIVF INT C COMP
   Bache K, 2013, UCI machine learning repository
   Carpineto C, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2071389.2071390
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Dong W., 2011, P 20 INT C WORLD WID, P577
   Drugman T, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P179, DOI 10.1109/MMSP.2007.4412847
   Dy JG, 2003, IEEE T PATTERN ANAL, V25, P373, DOI 10.1109/TPAMI.2003.1182100
   Fanty M., 1991, ADV NEURAL INFORM PR, P220
   Feng BL, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P721
   Fleuret F, 2004, J MACH LEARN RES, V5, P1531
   Gao LL, 2017, MULTIMEDIA SYST, V23, P303, DOI 10.1007/s00530-015-0494-1
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Guldogan E, 2008, SIGNAL IMAGE VIDEO P, V2, P241, DOI 10.1007/s11760-007-0049-9
   Houle ME, 2005, PROC INT CONF DATA, P619
   Houle ME, 2015, IEEE T PATTERN ANAL, V37, P136, DOI 10.1109/TPAMI.2014.2343223
   Houle ME, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2487736
   Houle MichaelE., 2014, P INT C MULTIMEDIA R, P89
   Jiang W, 2006, IEEE T IMAGE PROCESS, V15, P702, DOI 10.1109/TIP.2005.863105
   Karger D. R., 2002, Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, P741, DOI [DOI 10.1145/509907.510013, 10.1145/509907.510013]
   Kuo Yin-Hsi., 2009, ACM Multimedia, P65
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leong CW, 2011, LECT NOTES COMPUT SC, V6941, P137, DOI 10.1007/978-3-642-23708-9_16
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li Y., 2011, International Conference on Management of Data, P1009
   Li Y., 2011, Proceedings of the ACM International Conference on Multimedia, P1177
   Liu Y, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P2083, DOI 10.1109/ICME.2004.1394676
   Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045
   Ortega-Binderberger M, 2004, MULTIMEDIA SYST, V9, P535, DOI 10.1007/s00530-003-0126-z
   Rahman MM, 2011, INFORM PROCESS MANAG, V47, P676, DOI 10.1016/j.ipm.2010.12.001
   Rashedi E, 2013, KNOWL-BASED SYST, V39, P85, DOI 10.1016/j.knosys.2012.10.011
   Rashedi E, 2009, INFORM SCIENCES, V179, P2232, DOI 10.1016/j.ins.2009.03.004
   Saari P, 2011, IEEE T AUDIO SPEECH, V19, P1802, DOI 10.1109/TASL.2010.2101596
   Srinivasan S, 2000, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P50, DOI 10.1109/IVL.2000.853839
   Sun Y, 2010, IEEE IMAGE PROC, P3209, DOI 10.1109/ICIP.2010.5651984
   Vasconcelos N, 2004, PROC CVPR IEEE, P770
   Wang Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P79, DOI 10.1145/2733373.2806233
   Yang Y., 2011, P 22 INT JOINT C ART, P1589
   Zhai Y, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P965, DOI 10.1109/ICME.2006.262693
   Zhao Z., 2007, P 24 INT C MACHINE L, P1151
NR 43
TC 7
Z9 8
U1 0
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
AR 25
DI 10.1145/3063595
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD5TE
UT WOS:000407591900003
DA 2024-07-18
ER

PT J
AU Riegler, M
   Pogorelov, K
   Eskeland, SL
   Schmidt, PT
   Albisser, Z
   Johansen, D
   Griwodz, C
   Halvorsen, P
   De Lange, T
AF Riegler, Michael
   Pogorelov, Konstantin
   Eskeland, Sigrun Losada
   Schmidt, Peter Thelin
   Albisser, Zeno
   Johansen, Dag
   Griwodz, Carsten
   Halvorsen, Pal
   De lange, Thomas
TI From Annotation to Computer-Aided Diagnosis: Detailed Evaluation of a
   Medical Multimedia System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Medical multimedia system; gastrointestinal tract; evaluation
ID CAPSULE ENDOSCOPY IMAGES; POLYP DETECTION; COLONOSCOPY; DEEP; MORTALITY
AB Holisticmedical multimedia systems covering end-to-end functionality from data collection to aided diagnosis are highly needed, but rare. In many hospitals, the potential value of multimedia data collected through routine examinations is not recognized. Moreover, the availability of the data is limited, as the health care personnel may not have direct access to stored data. However, medical specialists interact with multimedia content daily through their everyday work and have an increasing interest in finding ways to use it to facilitate their work processes. In this article, we present a novel, holistic multimedia system aiming to tackle automatic analysis of video from gastrointestinal (GI) endoscopy. The proposed system comprises the whole pipeline, including data collection, processing, analysis, and visualization. It combines filters using machine learning, image recognition, and extraction of global and local image features. The novelty is primarily in this holistic approach and its real-time performance, where we automate a complete algorithmic GI screening process. We built the system in a modular way to make it easily extendable to analyze various abnormalities, and we made it efficient in order to run in real time. The conducted experimental evaluation proves that the detection and localization accuracy are comparable or even better than existing systems, but it is by far leading in terms of real-time performance and efficient resource consumption.
C1 [Riegler, Michael; Pogorelov, Konstantin; Albisser, Zeno; Griwodz, Carsten; Halvorsen, Pal] Simula Res Lab, POB 134, N-1325 Lysaker, Norway.
   [Riegler, Michael; Pogorelov, Konstantin; Albisser, Zeno; Griwodz, Carsten; Halvorsen, Pal] Univ Oslo, Oslo, Norway.
   [Eskeland, Sigrun Losada; De lange, Thomas] Vestre Viken Hosp Trust, Baerum Hosp, Sogneprest Munthe Kaas Vei 100, N-1346 Gjettum, Norway.
   [Schmidt, Peter Thelin] Karolinska Inst, Dept Med, Solna, Sweden.
   [Schmidt, Peter Thelin] Karolinska Univ Hosp, Ctr Digest Dis, S-17176 Stockholm, Sweden.
   [Johansen, Dag] UiT Arctic Univ Norway, Tromso, Norway.
   [Johansen, Dag] Univ Tromso, Postboks 6050 Langnes, N-9037 Tromso, Norway.
   [De lange, Thomas] Canc Registry Norway, Postboks 5313 Majorstuen, N-0304 Oslo, Norway.
C3 University of Oslo; Karolinska Institutet; Karolinska Institutet;
   Karolinska University Hospital; UiT The Arctic University of Tromso; UiT
   The Arctic University of Tromso; University of Oslo
RP Riegler, M (corresponding author), Simula Res Lab, POB 134, N-1325 Lysaker, Norway.; Riegler, M (corresponding author), Univ Oslo, Oslo, Norway.
EM michael@simula.no; konstantin@simula.no; sigesk@vestreviken.no;
   peter.thelin-schmidt@karolinska.se; zeno@simula.no; dag@cs.uit.no;
   griff@simula.no; paalh@simula.no; Thomas.de.Lange@kreftregisteret.no
RI Riegler, Michael A/E-5443-2015; de Lange, Thomas/Q-9063-2016
OI de Lange, Thomas/0000-0003-3989-7487; Halvorsen, Pal/0000-0003-2073-7029
FU Norwegian FRINATEK project "EONS" [231687]
FX This work is founded by the Norwegian FRINATEK project "EONS" (#231687).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Albisser Z., 2015, Proceedings of the 6th ACM Multimedia Systems Conference, P73
   Alexandre LA, 2007, LECT NOTES ARTIF INT, V4702, P358
   Ameling S., 2009, BILDVERARBEITUNG MED, P346
   [Anonymous], 2014, ARXIV14121897
   [Anonymous], 1998, The art of computer programming: Sorting and searching
   [Anonymous], 2012, Estimated Cancer Incidence Mortality and Prevalence Worldwide in 2012
   [Anonymous], 2013, The New York Times
   [Anonymous], 2016, DEEP LEARNING IDENTI
   [Anonymous], 2008, P 1 ACM INT C MULTIM
   [Anonymous], ARXIV161009157
   Buades A, 2011, IMAGE PROCESS ON LIN, V1, P208, DOI 10.5201/ipol.2011.bcm_nlm
   Cheng DC, 2008, LECT NOTES ARTIF INT, V5108, P62, DOI 10.1007/978-3-540-70715-8_6
   Chin C, 2000, J RES SCI TEACH, V37, P109, DOI 10.1002/(SICI)1098-2736(200002)37:2<109::AID-TEA3>3.0.CO;2-7
   Cong Y, 2015, PATTERN RECOGN, V48, P907, DOI 10.1016/j.patcog.2014.09.010
   de Lange T, 2005, GASTROINTEST ENDOSC, V61, P715, DOI 10.1016/S0016-5107(05)00337-8
   de Vries AH, 2010, EUR RADIOL, V20, P1404, DOI 10.1007/s00330-009-1683-0
   Efron B, 1997, J AM STAT ASSOC, V92, P548, DOI 10.2307/2965703
   Ferlay J, 2013, EUR J CANCER, V49, P1374, DOI 10.1016/j.ejca.2012.12.027
   Fitzgibbon A. W., 1995, BMVC '95 Proceedings of the 6th British Machine Vision Conference, P513
   Jiang ML, 2015, IEEE T BIO-MED ENG, V62, P783, DOI 10.1109/TBME.2014.2365494
   Kaminski MF, 2010, NEW ENGL J MED, V362, P1795, DOI 10.1056/NEJMoa0907667
   Khaleghi A, 2015, IEEE ENG MED BIO, P4081, DOI 10.1109/EMBC.2015.7319291
   KUDO S, 1994, J CLIN PATHOL, V47, P880, DOI 10.1136/jcp.47.10.880
   Li BP, 2012, IEEE T INF TECHNOL B, V16, P323, DOI 10.1109/TITB.2012.2185807
   Li BP, 2009, COMPUT BIOL MED, V39, P141, DOI 10.1016/j.compbiomed.2008.11.007
   Li Xirong., 2010, Proceedings of the ACM International Conference on Image and Video Retrieval, CIVR '10, P10
   Liu D, 2007, COMPUT METH PROG BIO, V88, P152, DOI 10.1016/j.cmpb.2007.07.011
   Lux M., 2013, P 21 ACM INT C MULT, P843
   Lux M., 2013, P 4 ACM MULT SYST C, P141
   Mallery S, 2000, MED CLIN N AM, V84, P1059, DOI 10.1016/S0025-7125(05)70276-5
   Mamonov AV, 2014, IEEE T MED IMAGING, V33, P1488, DOI 10.1109/TMI.2014.2314959
   Nawarathna R, 2014, NEUROCOMPUTING, V144, P70, DOI 10.1016/j.neucom.2014.02.064
   Oba S, 2011, DIGESTION, V83, P167, DOI 10.1159/000321807
   Pogorelov Konstantin, 2017, P MMSYS
   Redmon J., 2016, P IEEE C COMP VIS PA, P779, DOI DOI 10.1109/CVPR.2016.91
   Riegler M., 2016, P CBMI
   Riegler M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P397, DOI 10.1145/2647868.2654894
   Riegler Michael, 2014, P ACMM INT C MULT RE, V534, P534, DOI [10.1145/2578726.2582621, DOI 10.1145/2578726.2582621]
   Riegler Riegler M. M., P 24 ACM INT C MULT, P968, DOI [10.1145/2964284.2976760, DOI 10.1145/2964284.2976760]
   Sae Hwang, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P465
   Satodate H, 2003, GASTROINTEST ENDOSC, V58, P288, DOI 10.1067/mge.2003.361
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   SELBY JV, 1992, NEW ENGL J MED, V326, P653, DOI 10.1056/NEJM199203053261001
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Song Jingkuan, 2013, PSIGM PODS PHD S, P55
   SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997
   Tanaka K, 2013, GASTROINTEST ENDOSC, V78, P659, DOI 10.1016/j.gie.2013.05.025
   von Karsa L, 2012, ENDOSCOPY, V44, pSE1, DOI 10.1055/s-0032-1309822
   Wang Y, 2015, COMPUT METH PROG BIO, V120, P164, DOI 10.1016/j.cmpb.2015.04.002
   Wang Y, 2014, IEEE J BIOMED HEALTH, V18, P1379, DOI 10.1109/JBHI.2013.2285230
   Wang Y, 2013, IEEE J BIOMED HEALTH, V17, P143, DOI 10.1109/TITB.2012.2226595
   Wang Y, 2010, IEEE T BIO-MED ENG, V57, P685, DOI 10.1109/TBME.2009.2034466
   Wang Y, 2011, INT CONF ACOUST SPEE, P1, DOI 10.1109/PLASMA.2011.5993071
   Zhou MD, 2014, 2014 7TH INTERNATIONAL CONFERENCE ON BIOMEDICAL ENGINEERING AND INFORMATICS (BMEI 2014), P237, DOI 10.1109/BMEI.2014.7002777
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 65
TC 17
Z9 18
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
AR 26
DI 10.1145/3079765
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD5TE
UT WOS:000407591900004
DA 2024-07-18
ER

PT J
AU Fu, B
   Staehle, D
   Kunzmann, G
   Steinbach, E
   Kellerer, W
AF Fu, Bo
   Staehle, Dirk
   Kunzmann, Gerald
   Steinbach, Eckehard
   Kellerer, Wolfgang
TI QoE-Based SVC Layer Dropping in LTE Networks Using Content-Aware Layer
   Priorities
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE QoE; SVC; priority marking; mobile video streaming; LTE
ID RESOURCE-ALLOCATION; VIDEO; H.264/AVC; QUALITY
AB The increasing popularity of mobile video streaming applications has led to a high volume of video traffic in mobile networks. As the base station, for instance, the eNB in LTE networks, has limited physical resources, it can be overloaded by this traffic. This problem can be addressed by using Scalable Video Coding (SVC), which allows the eNB to drop layers of the video streams to dynamically adapt the bitrate. The impact of bitrate adaptation on the Quality of Experience (QoE) for the users depends on the content characteristics of videos. As the current mobile network architectures do not support the eNB in obtaining video content information, QoE optimization schemes with explicit signaling of content information have been proposed. These schemes, however, require the eNB or a specific optimization module to process the video content on the fly in order to extract the required information. This increases the computation and signaling overhead significantly, raising the OPEX for mobile operators. To address this issue, in this article, a content-aware (CA) priority marking and layer dropping scheme is proposed. The CA priority indicates a transmission order for the layers of all transmitted videos across all users, resulting from a comparison of their utility versus rate characteristics. The CA priority values can be determined at the P-GW on the fly, allowing mobile operators to control the priority marking process. Alternatively, they can be determined offline at the video servers, avoiding real-time computation in the core network. The eNB can perform content-aware SVC layer dropping using only the priority values. No additional content processing is required. The proposed scheme is lightweight both in terms of architecture and computation. The improvement in QoE is substantial and very close to the performance obtained with the computation and signaling-intensive QoE optimization schemes.
C1 [Fu, Bo; Staehle, Dirk; Kunzmann, Gerald] DOCOMO Commun Labs Europe, Munich, Germany.
   [Steinbach, Eckehard; Kellerer, Wolfgang] Tech Univ Munich, D-80290 Munich, Germany.
C3 NTT Docomo; Technical University of Munich
RP Fu, B (corresponding author), DOCOMO Commun Labs Europe, Munich, Germany.
EM lindamor@gmail.com
RI Kellerer, Wolfgang/E-7271-2017
OI Kellerer, Wolfgang/0000-0003-4358-8038; Steinbach,
   Eckehard/0000-0001-8853-2703
CR 3GPP, 2014, SP140153 3GPP TD
   3GPP, 2014, 23705 3GPP TR
   Agboma F, 2007, MOB INF SYST, V3, P153, DOI 10.1155/2007/719840
   Al-Khateb N., 2013, Proceedings geoConvention, Calgary, Canada, P1
   [Anonymous], JOINT SCALABLE VIDEO
   [Anonymous], J COMPUT NETW COMMUN
   [Anonymous], 2003, Final report from the video quality experts group on the validation of objective models of video quality assessment
   [Anonymous], IEEE COMPUTER COMMUN
   Baum DS, 2005, IEEE VTS VEH TECHNOL, P3132
   Chakareski J, 2006, IEEE T MULTIMEDIA, V8, P207, DOI 10.1109/TMM.2005.864284
   Dahlman E., 2010, 3G EVOLUTION HSPA LT
   Fu B., 2013, P 8 ACM WORKSHOP PER, P173
   Khan S, 2006, IEEE COMMUN MAG, V44, P122, DOI 10.1109/MCOM.2006.1580942
   Khan S, 2007, ADV MULTIMED, V2007, DOI 10.1155/2007/94918
   Maani E, 2008, IEEE T IMAGE PROCESS, V17, P1663, DOI 10.1109/TIP.2008.2001402
   Olsson M, 2009, SAE AND THE EVOLVED PACKET CORE: DRIVING THE MOBILE BROADBAND REVOLUTION, P1
   Pahalawatta PV, 2007, WIREL COMMUN MOB COM, V7, P131, DOI 10.1002/wcm.469
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Srinivasan SK, 2010, IEEE T BROADCAST, V56, P281, DOI 10.1109/TBC.2010.2049610
   Staelens N, 2010, IEEE T BROADCAST, V56, P458, DOI 10.1109/TBC.2010.2067710
   Thakolsri Srisakul, 2009, Journal of Communications, V4, P669, DOI 10.4304/jcm.4.9.669-680
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
NR 23
TC 9
Z9 9
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2015
VL 12
IS 1
AR 7
DI 10.1145/2754167
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CP9LG
UT WOS:000360215200007
DA 2024-07-18
ER

PT J
AU Botta, M
   Cavagnino, D
   Pomponiu, V
AF Botta, Marco
   Cavagnino, Davide
   Pomponiu, Victor
TI Protecting the Content Integrity of Digital Imagery with Fidelity
   Preservation: An Improved Version
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Content integrity; information hiding; fragile watermarking
AB Fragile watermarking has attracted a lot of attention in the last decade. An interesting approach, presented in 2011 by Lin et al., results in very high quality of the watermarked images. However, after a thorough examination of the paper, a few improvements are proposed in our revised version of the algorithm in order to overcome some shortcomings. In particular, changes to the pseudocode and modifications to deal with pixel saturation are suggested, along with a way to improve the scheme security. Finally, a deeper analysis of the security is presented.
C1 [Botta, Marco; Cavagnino, Davide] Univ Turin, Dipartimento Informat, I-10149 Turin, Italy.
   [Pomponiu, Victor] Dept Radiol, Pittsburgh, PA 15213 USA.
C3 University of Turin
RP Botta, M (corresponding author), Univ Turin, Dipartimento Informat, Cso Svizzera 185, I-10149 Turin, Italy.
EM marco.botta@unito.it; davide.cavagnino@unito.it; vpomponiu@acm.org
CR Cox J., 2008, DIGITAL WATERMARKING
   Li LJ, 2007, LECT NOTES ARTIF INT, V4456, P1
   Lin PY, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000489
   WU ML, 1984, IEEE T SOFTWARE ENG, V10, P185, DOI 10.1109/TSE.1984.5010221
   Zhang XP, 2006, IEEE COMMUN LETT, V10, P781, DOI 10.1109/LCOMM.2006.060863
NR 5
TC 11
Z9 11
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2014
VL 10
IS 3
AR 29
DI 10.1145/2568224
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF3AM
UT WOS:000334583800007
OA Green Published
DA 2024-07-18
ER

PT J
AU Hoque, MA
   Siekkinen, M
   Nurminen, JK
   Tarkoma, S
   Aalto, M
AF Hoque, Mohammad Asharful
   Siekkinen, Matti
   Nurminen, Jukka K.
   Tarkoma, Sasu
   Aalto, Mika
TI Saving Energy in Mobile Devices for On-Demand Multimedia Streaming - A
   Cross-Layer Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Experimentation; Measurement; Performance; Constant bit rate;
   cross-layer; DASH; energy efficiency; multimedia streaming; radio
   signaling; rate-adaptive streaming; video streaming; wireless network
AB This article proposes a novel energy-efficient multimedia delivery system called EStreamer. First, we study the relationship between buffer size at the client, burst-shaped TCP-based multimedia traffic, and energy consumption of wireless network interfaces in smartphones. Based on the study, we design and implement EStreamer for constant bit rate and rate-adaptive streaming. EStreamer can improve battery lifetime by 3x, 1.5x, and 2x while streaming over Wi-Fi, 3G, and 4G, respectively.
C1 [Hoque, Mohammad Asharful; Siekkinen, Matti; Nurminen, Jukka K.; Tarkoma, Sasu; Aalto, Mika] Aalto Univ, Sch Sci, Dept Comp Sci & Engn, Espoo, Finland.
C3 Aalto University
RP Hoque, MA (corresponding author), Aalto Univ, Sch Sci, Dept Comp Sci & Engn, Espoo, Finland.
EM moham-mad.hoque@aalto.fi
RI NURMINEN, Jukka/JBJ-0709-2023; Tarkoma, Sasu/V-2345-2019; Nurminen,
   Jukka K/H-7824-2012; Siekkinen, Matti/H-2447-2018
OI Tarkoma, Sasu/0000-0003-4220-3650; Siekkinen, Matti/0000-0003-0423-1060;
   Nurminen, Jukka/0000-0001-5083-1927
FU Academy of Finland [253860]; FIGS; Academy of Finland (AKA) [253860]
   Funding Source: Academy of Finland (AKA)
FX This work was supported by the Academy of Finland: grant no. 253860 and
   FIGS.
CR [Anonymous], P 14 IEEE INT S WORL
   [Anonymous], 2013, Cisco Visual Networking Index: Global Mobile Data Traffic Forecast Update, 2013-2018
   [Anonymous], 2013, PROC 5 WORKSHOP MOBI, DOI DOI 10.1145/2457413.2457417
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], UND SMARTPH BEH NETW
   [Anonymous], IEEE COMM SURV TUTOR
   [Anonymous], P 6 INT C MOB SYST A
   [Anonymous], COMPUT COMM
   [Anonymous], SMARTPH 3G NETW RED
   [Anonymous], P 20 ACM INT C MULT
   [Anonymous], 2010, P 10 ACM SIGCOMM C I
   Balasubramanian N, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P280
   Cheng X, 2013, IEEE T MULTIMEDIA, V15, P1184, DOI 10.1109/TMM.2013.2265531
   Deng S., 2012, Proc. 8th Int. Conf. on Emerging Netw. Ex- periments and Technologies (CoNEXT), P181
   Falaki H., 2010, P 10 ANN C INTERNET, P281, DOI DOI 10.1145/1879141.1879176
   Feng Qian, 2010, 2010 18th IEEE International Conference on Network Protocols (ICNP 2010), P285, DOI 10.1109/ICNP.2010.5762777
   Finamore A., 2011, ACM IMC, P345, DOI DOI 10.1145/2068816.2068849
   Guo L., 2006, Proc. ACM SIGCOMM Internet Measurement Conference (IMC), P217
   Hoque M. A., 2011, 2011 IEEE Consumer Communications and Networking Conference (CCNC 2011), P891, DOI 10.1109/CCNC.2011.5766635
   Hoque M. A., 2013, P 23 ACM WORKSH NETW, P13
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Hwang KW, 2012, LECT NOTES COMPUT SC, V7290, P44, DOI 10.1007/978-3-642-30054-7_4
   Korhonen J., 2005, Proceedings of the 15th International Workshop on Network and Operating Systems Support for Digital Audio and Video. NOSSDAV 2005, P39, DOI 10.1145/1065983.1065994
   Lederer S., 2012, P 3 MULT SYST C, P89
   Lee CC, 2004, 2004 WIRELESS TELECOMMUNICATIONS SYMPOSIUM, PROCEEDINGS, P15
   Prochkova I, 2012, INT CONF NEXT GEN, P147, DOI 10.1109/NGMAST.2012.32
   Qian Feng, 2011, P 9 INT C MOB SYST A, P321, DOI DOI 10.1145/1999995.2000026
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Tan E, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, P123, DOI 10.1109/ICNP.2007.4375843
   Xiao Y., 2010, Proc. 1st Int. Conf. on Energy-efficient Computing and Networking, P75
   Xiao Y, 2008, INT CONF NEXT GEN, P61, DOI 10.1109/NGMAST.2008.26
   Yan HJ, 2006, IEEE T MOBILE COMPUT, V5, P1575, DOI 10.1109/TMC.2006.159
NR 32
TC 15
Z9 18
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2014
VL 10
IS 3
AR 25
DI 10.1145/2556942
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF3AM
UT WOS:000334583800003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, N
   Cui, HJ
   Chan, SHG
   Chen, ZP
   Zhuang, YR
AF Liu, Ning
   Cui, Huajie
   Chan, S. -H. Gary
   Chen, Zhipeng
   Zhuang, Yirong
TI Dissecting User Behaviors for a Simultaneous Live and VoD IPTV System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurement; Experimentation; Human Factors; IPTV; live TV; VoD; user
   behavior
ID DISTRIBUTED SERVERS ARCHITECTURE; VIDEO
AB IPTV services deployed nowadays often consist of both live TV and Video-on-Demand (VoD), offered by the same service provider to the same pool of users over the same managed network. Understanding user behaviors in such a setting is hence an important step for system modelling and optimization. Previous studies on user behavior on video services were on either live TV or VoD. For the first time, we conduct an in-depth large-scale behavior study for IPTV users offering simultaneously live TV and VoD choices at the same time. Our data is from the largest IPTV service provider in China, offering hundreds of live channels and hundreds of thousands of VoD files, with traces covering more than 1.9 million users over a period of 5 months. This large dataset provides us a unique opportunity to cross-compare user viewing behaviors for these services on the same platform, and sheds valuable insights on how users interact with such a simultaneous system.
   Our results lead to new understanding on IPTV user behaviors which have strong implications on system design. For example, we find that the average holding time for VoD is significantly longer than live TV. live TV users tend to surf more. However, if such channel surfing is discounted, the holding times of both services are not much different. While users in VoD tend to view HD longer, channel popularity for live TV is much less dependent on its video quality. In contrast to some popular assumptions on user interactivity, the transitions among live TV, VoD, and offline modes are far from a Markov model.
C1 [Liu, Ning; Cui, Huajie; Chen, Zhipeng] Sun Yat Sen Univ, Sch Software, Guangzhou 510275, Guangdong, Peoples R China.
   [Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
   [Zhuang, Yirong] China Telecom Co Ltd, Guangdong Res Inst, Guangzhou 510630, Guangdong, Peoples R China.
C3 Sun Yat Sen University; Hong Kong University of Science & Technology;
   China Telecom Corp. Ltd.
RP Liu, N (corresponding author), Sun Yat Sen Univ, Sch Software, Guangzhou 510275, Guangdong, Peoples R China.
EM liuning2@mail.sysu.edu.cn
OI Chan, Gary Shueng Han/0000-0003-4207-764X
FU Fundamental Research Funds for the Central Universities
   [2010620003161035]; Hong Kong Research Grant Council General Research
   Fund [610713]; HKUST [FSGRF12EG05, FS-GRF13EG15]; Hong Kong Innovation
   and Technology Fund [UIM/246]
FX This work was supported in part by Fundamental Research Funds for the
   Central Universities (under grant no. 2010620003161035), Hong Kong
   Research Grant Council General Research Fund (610713), HKUST
   (FSGRF12EG05 and FS-GRF13EG15), and Hong Kong Innovation and Technology
   Fund (UIM/246).
CR Abrahamsson H., 2013, 2013 International Conference on Computing, Networking and Communications (ICNC 2013), P916, DOI 10.1109/ICCNC.2013.6504212
   Agrawal D, 2007, 2007 10TH IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2009), VOLS 1 AND 2, P353, DOI 10.1109/INM.2007.374800
   Annapureddy S., 2007, Proc. Int'l WWW Conference, P903
   [Anonymous], P 3 ACM SIGOPS EUROS
   [Anonymous], P 20 INT C COMP COMM
   [Anonymous], P INT WORKSH NETW OP
   [Anonymous], P 26 IEEE INT C COMP
   Bin Chang, 2008, 2008 28th International Conference on Distributed Computing Systems Workshops (ICDCS Workshops), P7, DOI 10.1109/ICDCS.Workshops.2008.94
   Bommaiah E, 2000, SIXTH IEEE REAL-TIME TECHNOLOGY AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P111, DOI 10.1109/RTTAS.2000.852456
   Branch P., 1999, 1999 IEEE International Conference on Communications (Cat. No. 99CH36311), P978, DOI 10.1109/ICC.1999.765419
   Cha M, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P71
   Chan SHG, 2002, IEEE T BROADCAST, V48, P19, DOI 10.1109/11.992850
   Chan SHG, 2001, IEEE ACM T NETWORK, V9, P125, DOI 10.1109/90.917070
   Chan SHG, 2001, IEEE COMMUN LETT, V5, P384, DOI 10.1109/4234.951385
   Cherkasova L, 2004, IEEE ACM T NETWORK, V12, P781, DOI 10.1109/TNET.2004.836125
   Choi J, 2012, IEEE COMMUN SURV TUT, V14, P156, DOI 10.1109/SURV.2011.030811.00051
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Gopalakrishnan Vijay, 2010, P 29 C INFORM COMMUN, P1
   Gürsun G, 2011, IEEE INFOCOM SER, P16, DOI 10.1109/INFCOM.2011.5934965
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Lee CY, 2010, IEEE T BROADCAST, V56, P321, DOI 10.1109/TBC.2010.2051494
   Lee C, 2012, IEEE T CONSUM ELECTR, V58, P382, DOI 10.1109/TCE.2012.6227437
   Luo JG, 2009, IEEE T PARALL DISTR, V20, P59, DOI 10.1109/TPDS.2008.68
   Mu M, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P255, DOI 10.1109/ISM.2012.55
   Qiu TQ, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P430
   Qiu TQ, 2009, PERF E R SI, V37, P275
   Sen S, 1999, IEEE INFOCOM SER, P1310, DOI 10.1109/INFCOM.1999.752149
   Sripanidkulchai K., 2004, Proceedings of the 4th ACM SIGCOMM conference on Internet measurement, P41
   Ullah I, 2012, IEEE COMMUN SURV TUT, V14, P734, DOI 10.1109/SURV.2011.082611.00134
   Veloso E, 2006, IEEE ACM T NETWORK, V14, P133, DOI 10.1109/TNET.2005.863709
   Wang B, 2002, IEEE INFOCOM SER, P1726, DOI 10.1109/INFCOM.2002.1019426
   Won Young J., 2008, 2008 2nd IEEE International Workshop on Bandwidth on Demand, P95
   Wu D, 2009, IEEE INFOCOM SER, P2726, DOI 10.1109/INFCOM.2009.5062220
   Wu D, 2009, IEEE INFOCOM SER, P73, DOI 10.1109/INFCOM.2009.5061908
   Ying QF, 2011, 2011 IET 4TH INTERNATIONAL CONFERENCE ON WIRELESS, MOBILE & MULTIMEDIA NETWORKS (ICWMMN 2011), P283, DOI 10.1049/cp.2011.1007
NR 36
TC 20
Z9 22
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2014
VL 10
IS 3
AR 23
DI 10.1145/2568194
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF3AM
UT WOS:000334583800001
DA 2024-07-18
ER

PT J
AU Chou, PA
AF Chou, Philip A.
TI Advances in Immersive Communication: (1) Telephone, (2) Television, (3)
   Teleportation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Immersion; telepresence; virtual reality; depth cameras
AB The last great advances in immersive communication were the invention of the telephone over 137 years ago and the invention of the video telephone (ne television) over 86 years ago. However, a perfect storm is brewing for the next advance in immersive communication, thanks to the convergence of massive amounts of computation, bandwidth, resolution, new sensors, and new displays. It could well be the Multimedia community that turns this brew into the next great advance in immersive communication, something akin to teleportation.
EM pachou@miscrosoft.com
CR Aggoun A, 2013, IEEE MULTIMEDIA, V20, P28, DOI 10.1109/MMUL.2012.42
   Apostolopoulos JG, 2012, P IEEE, V100, P974, DOI 10.1109/JPROC.2011.2182069
   DU MAURIER G., 1978, PUNCHS ALMANACK
   GUIZZO E., 2010, IEEE SPECTRUM    SEP
   GUIZZO E., 2010, IEEE SPECTRUM    APR
   Kaku Michio., 2008, PHYS IMPOSSIBLE
   LANIER J., 1997, NITT
   Minsky M., 1980, Omni, V2, P44
   NAHRSTEDT K., 2005, TEEVE TELEIMMERSIVE
   PERLIN K., 2001, P 1 INT C SEM GAM NE
   STEELE E., 2013, SKYPE BLOG
   Zhang C, 2013, IEEE MULTIMEDIA, V20, P17, DOI 10.1109/MMUL.2013.12
NR 12
TC 4
Z9 4
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 41
DI 10.1145/2492704
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700011
DA 2024-07-18
ER

PT J
AU Silva, JM
   Orozco, M
   Cha, J
   El Saddik, A
   Petriu, EM
AF Silva, Juan M.
   Orozco, Mauricio
   Cha, Jongeun
   El Saddik, Abdulmotaleb
   Petriu, Emil M.
TI Human Perception of Haptic-to-Video and Haptic-to-Audio Skew in
   Multimedia Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Haptics; synchronization
ID DELAY; TIME
AB The purpose of this research is to assess the sensitivity of humans to perceive asynchrony among media signals coming from a computer application. Particularly we examine haptic-to-video and haptic-to-audio skew. For this purpose we have designed an experimental setup, where users are exposed to a basic multimedia presentation resembling a ping-pong game. For every collision between a ball and a racket, the user is able to perceive auditory, visual, and haptic cues about the collision event. We artificially introduce negative and positive delay to the auditory and visual cues with respect to the haptic stream. We subjectively evaluate the perception of inter-stream asynchrony perceived by the users using two types of haptic devices. The statistical results of our evaluation show perception rates of around 100 ms regardless of modality and type of device.
C1 [Silva, Juan M.; Cha, Jongeun; El Saddik, Abdulmotaleb; Petriu, Emil M.] Univ Ottawa, Sch Elect Engn & Comp Sci, MCRLab, Ottawa, ON K1N 6N5, Canada.
   [Orozco, Mauricio] NYU, Div Engn, Abu Dhabi, U Arab Emirates.
C3 University of Ottawa
RP Silva, JM (corresponding author), Univ Ottawa, Sch Elect Engn & Comp Sci, MCRLab, Ottawa, ON K1N 6N5, Canada.
EM jmanuel.silva@gmail.com
RI /D-4159-2009
OI /0000-0002-7690-8547
CR Adams R. J., 1998, Proceedings of the ASME Dynamic Systems and Control Division-1998, P227
   Adams RJ, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P1254, DOI 10.1109/IROS.1998.727471
   Adelstein B., 2003, P 5 INT C MULTIMODAL, P73, DOI [DOI 10.1145/958432.958448, 10.1145/958432.958448]
   ALLAN LG, 1974, PERCEPT PSYCHOPHYS, V15, P37, DOI 10.3758/BF03205825
   COLGATE JE, 1995, IROS '95 - 1995 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS: HUMAN ROBOT INTERACTION AND COOPERATIVE ROBOTS, PROCEEDINGS, VOL 3, P140, DOI 10.1109/IROS.1995.525875
   Conti F.B. Francois., 2005, CHAI 3D OPEN SOURCE
   Diolaiti N, 2006, IEEE T ROBOT, V22, P256, DOI 10.1109/TRO.2005.862487
   DIXON N. F., 1980, PERCEPT, V9, P6
   DIXON WJ, 1948, J AM STAT ASSOC, V43, P109, DOI 10.2307/2280071
   Eid M., 2007, International Journal of Advanced Media and Communication, V1, P71, DOI 10.1504/IJAMC.2007.013918
   El Saddik A, 2007, IEEE INSTRU MEAS MAG, V10, P10, DOI 10.1109/MIM.2007.339540
   Fujimoto T, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P113
   HINTERSEER P., 2007, IEEE T SIGNAL PROCES, V55, P12
   Ishibashi Y., 2004, P 12 ANN ACM INT C M, P604
   Jay C, 2007, ACM T COMPUT-HUM INT, V14, DOI 10.1145/1275511.1275514
   KAMEYAMA S., 2006, P SPIE OPTICS E MULT, V6391, P8
   Levitin DJ, 2000, AIP CONF PROC, V517, P323, DOI 10.1063/1.1291270
   LEVITT H, 1971, J ACOUST SOC AM, V49, P467, DOI 10.1121/1.1912375
   MCGRATH M, 1985, J ACOUST SOC AM, V77, P678, DOI 10.1121/1.392336
   Miyasato T, 1996, IEICE T FUND ELECTR, VE79A, P655
   NOVINT FALCON HAPTICDEVICE, NOV FALC HAPT
   PHANTOMHAPTICDEVICE, PHANT
   Steinmetz R, 1996, IEEE J SEL AREA COMM, V14, P61, DOI 10.1109/49.481694
   TATEMATSU A., 2010, P IEEE INT COMM QUAL
   Vogels IMLC, 2004, HUM FACTORS, V46, P118, DOI 10.1518/hfes.46.1.118.30394
NR 25
TC 15
Z9 15
U1 2
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2013
VL 9
IS 2
AR 9
DI 10.1145/2457450.2457451
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 144MS
UT WOS:000318944400001
DA 2024-07-18
ER

PT J
AU Liu, D
   Yan, SC
   Ji, RR
   Hua, XS
   Zhang, HJ
AF Liu, Dong
   Yan, Shuicheng
   Ji, Rong-Rong
   Hua, Xian-Sheng
   Zhang, Hong-Jiang
TI Image Retrieval with Query-Adaptive Hashing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Image retrieval; query-adaptive; hashing; joint sparsity
AB Hashing-based approximate nearest-neighbor search may well realize scalable content-based image retrieval. The existing semantic-preserving hashing methods leverage the labeled data to learn a fixed set of semantic-aware hash functions. However, a fixed hash function set is unable to well encode all semantic information simultaneously, and ignores the specific user's search intention conveyed by the query. In this article, we propose a query-adaptive hashing method which is able to generate the most appropriate binary codes for different queries. Specifically, a set of semantic-biased discriminant projection matrices are first learnt for each of the semantic concepts, through which a semantic-adaptable hash function set is learnt via a joint sparsity variable selection model. At query time, we further use the sparsity representation procedure to select the most appropriate hash function subset that is informative to the semantic information conveyed by the query. Extensive experiments over three benchmark image datasets well demonstrate the superiority of our proposed query-adaptive hashing method over the state-of-the-art ones in terms of retrieval accuracy.
C1 [Liu, Dong; Ji, Rong-Rong] Harbin Inst Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Singapore 117548, Singapore.
   [Hua, Xian-Sheng] Microsoft Res Asia, Beijing, Peoples R China.
   [Zhang, Hong-Jiang] Microsoft Adv Teachnol Ctr, Beijing, Peoples R China.
C3 Harbin Institute of Technology; National University of Singapore;
   Microsoft Research Asia; Microsoft
RP Liu, D (corresponding author), Harbin Inst Technol, 92 Xidazhi St, Harbin 150001, Heilongjiang, Peoples R China.
EM dongliu.hit@gmail.com
RI Yan, Shuicheng/HCI-1431-2022; Liu, Dong/AAL-8559-2021
CR [Anonymous], 2009, NIPS
   [Anonymous], 2009, NEURIPS
   [Anonymous], SIAM J OPTI IN PRESS
   Bosch A, 2006, LECT NOTES COMPUT SC, V3954, P517
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   CANDES E, 2007, L1 MAGIC
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jegou H., 2008, P INT C AC SPEECH SI, P12
   Jiang Y., 2011, P ACM INT C MULT RET
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151
   Lee DC, 2010, LECT NOTES COMPUT SC, V6311, P648, DOI 10.1007/978-3-642-15549-9_47
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750
   Shakhnarovich R., 2007, P INT C ART INT STAT, P1
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   WEISS Y, 2008, ADV NEURAL INFORM PR, V21, P1753
   Winder S.A. J., 2007, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1
   Zhou XS, 2001, PROC CVPR IEEE, P11
NR 22
TC 2
Z9 2
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2013
VL 9
IS 1
AR 2
DI 10.1145/2422956.2422958
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 097EO
UT WOS:000315457000002
DA 2024-07-18
ER

PT J
AU Weng, MF
   Chuang, YY
AF Weng, Ming-Fang
   Chuang, Yung-Yu
TI Collaborative Video Reindexing via Matrix Factorization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Multimedia content analysis; semantic video
   indexing; concept detection; unsupervised learning; TRECVID
ID FRAMEWORK
AB Concept-based video indexing generates a matrix of scores predicting the possibilities of concepts occurring in video shots. Based on the idea of collaborative filtering, this article presents unsupervised methods to refine the initial scores generated by concept classifiers by taking into account the concept-to-concept correlation and shot-to-shot similarity embedded within the score matrix. Given a noisy matrix, we refine the inaccurate scores via matrix factorization. This method is further improved by learning multiple local models and incorporating contextual-temporal structures. Experiments on the TRECVID 2006-2008 datasets demonstrate relative performance gains ranging from 13% to 52% without using any user annotations or external knowledge resources.
C1 [Weng, Ming-Fang; Chuang, Yung-Yu] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
C3 National Taiwan University
RP Weng, MF (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
EM mfueng@cmlab.csie.ntu.edu.tw; cyy@csie.ntu.edu.tw
RI mingfang, weng/HSG-7737-2023
OI mingfang, weng/0000-0003-1636-4717
FU National Science Council of Taiwan, R.O.C., under NSC
   [99-2628-E-002-015, 099-2811E-002-096, 99-2622-E-002-026-CC2]
FX This work was supported by the National Science Council of Taiwan,
   R.O.C., under NSC grants 99-2628-E-002-015, 099-2811E-002-096, and
   99-2622-E-002-026-CC2.
CR Adams WH, 2003, EURASIP J APPL SIG P, V2003, P170, DOI 10.1155/S1110865703211173
   AMIR A., 2005, ONL P TRECVID WORKSH
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], ACM INT C IM VID
   [Anonymous], 2009, NUSWIDE: A real-world web image database from National University of Singapore, DOI DOI 10.1145/1646396.1646452
   [Anonymous], 2007, COLUMBIA U BASELINE
   Candes E., 2009, ROBUST PRINCIPAL COM
   Jiang W, 2007, INT CONF ACOUST SPEE, P949
   JIANG Y.-G., 2008, CU VIREO374 FUSING C
   Jiang Y.-G., 2009, P IEEE INT C COMP VI
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   KENNEDY L, 2006, DTO CHALL WORKSH LAR
   Kennedy L.S., 2007, Proceedings of the 6th ACM international conference on Image and video retrieval, P333
   Khoshgoftaar T.M., 2009, ADV ARTIFICIAL INTEL, DOI [10.1155/2009/421425, DOI 10.1155/2009/421425]
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Liu KH, 2008, IEEE T MULTIMEDIA, V10, P240, DOI 10.1109/TMM.2007.911826
   Liu YA, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P500, DOI 10.1145/1571941.1572027
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Naphade MR, 2002, IEEE T CIRC SYST VID, V12, P40
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Qi GJ, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1404880.1404883
   Rennie Jasson D. M., 2005, P 22 INT C MACH LEAR, P713, DOI DOI 10.1145/1102351.1102441
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   SNOEK C. G. M., 2009, ONL P TRECVID WORKSH
   Weng M.F., 2008, PROC ACM INT C MULTI, P71, DOI DOI 10.1145/1459359.1459370
   Yang YH, 2009, IEEE T CIRC SYST VID, V19, P1880, DOI 10.1109/TCSVT.2009.2026978
   Yilmaz Emine, 2006, Proceedings of the 2006 ACM CIKM International Conference on Information and Knowledge Management, Arlington, Virginia, USA, November 6-11, 2006, P102, DOI [10.1145/1183614.1183633 (cit. on p. 34, DOI 10.1145/1183614.1183633(CIT.ONP.34]
NR 33
TC 19
Z9 21
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2012
VL 8
IS 2
AR 23
DI 10.1145/2168996.2169003
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 945BX
UT WOS:000304248900007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Nyström, M
   Holmqvist, K
AF Nystrom, Marcus
   Holmqvist, Kenneth
TI Effect of Compressed Offline Foveated Video on Viewing Behavior and
   Subjective Quality
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Eye-tracking; subjective quality; video compression;
   foveation
AB Offline foveation is a technique to improve the compression efficiency of digitized video. The general idea behind offline foveation is to blur video regions where no or a small number of previewers look without decreasing the subjective quality for later viewers. It relies on the fact that peripheral vision is reduced compared to central vision, and the observation that during free-viewing humans' gaze positions generally coincide when watching video. In this article, we conduct two experiments to assess how offline foveation affects viewing behavior and subjective quality. In the first experiment, 15 subjects free-viewed six video clips before and after offline foveation whereas in the second experiment we had 17 subjects assessing the quality of these videos after one, two, and three consecutive viewings. Eye movements were measured during the experiments. Results showed that, although offline foveation prior to encoding with H. 264 yielded data reductions up to 52% (20% average) on the tested videos, it had little or no effect on where people looked, their intersubject dispersion, fixation duration, saccade amplitude, or the experienced quality during first-time viewing. However, seeing the videos more than once increased the intersubject dispersion and decreased the subjective quality. In view of these results, we discuss the usage of offline foveated video in practical applications.
C1 [Nystrom, Marcus; Holmqvist, Kenneth] Lund Univ, Lund Humanities Lab, S-22100 Lund, Sweden.
C3 Lund University
RP Nyström, M (corresponding author), Lund Univ, Lund Humanities Lab, Box 201, S-22100 Lund, Sweden.
EM marcus@eit.lth.se
OI Nystrom, Marcus/0000-0002-2089-9012; Holmqvist,
   Kenneth/0000-0003-1738-3207
CR [Anonymous], 1935, How people look at pictures: A study of the psychology and perception in art
   [Anonymous], 2003, Final report from the video quality experts group on the validation of objective models of video quality assessment
   BERGSTROM P, 2003, THESIS LINKOPING U S
   Duchowski AT, 1998, P SOC PHOTO-OPT INS, V3299, P318, DOI 10.1117/12.320122
   Duchowski AT, 2000, IEEE T IMAGE PROCESS, V9, P1437, DOI 10.1109/83.855439
   Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120
   GEISLER WS, 1999, SOC INFORMATION DISP, V30, P420
   Henderson JM, 2003, TRENDS COGN SCI, V7, P498, DOI 10.1016/j.tics.2003.09.006
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   JOHANNESSON E, 2005, THESIS LUND U
   Kortum P, 1996, P SOC PHOTO-OPT INS, V2657, P350, DOI 10.1117/12.238732
   Le Meur O, 2007, VISION RES, V47, P2483, DOI 10.1016/j.visres.2007.06.015
   Nyström M, 2007, J ELECTRON IMAGING, V16, DOI 10.1117/1.2435199
   NYSTROMM M, 2008, J EYE MOVEM RES, V2, P21
   NYSTROMM M, 2004, P PICT COD S
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   PEDERSEN B, 2006, P ANN M COGNITIVE SC
   Rajashekar U, 2004, PROC SPIE, V5292, P296, DOI 10.1117/12.537118
   SHEIKH HR, 2001, SIGNAL PROCESS, V3, P1781
   STELMACH LB, 1994, VISUAL PROCESSING DI, V2179, P90
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   Wang Z, 2003, IEEE T IMAGE PROCESS, V12, P243, DOI 10.1109/TIP.2003.809015
   Wang Z, 2001, IEEE T IMAGE PROCESS, V10, P1397, DOI 10.1109/83.951527
   Yarbus A. L., 1967, Eye Movements and Vision
NR 24
TC 17
Z9 23
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2010
VL 6
IS 1
AR 4
DI 10.1145/1671954.1671958
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 563VL
UT WOS:000275163200004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shipman, F
   Girgensohn, A
   Wilcox, L
AF Shipman, Frank
   Girgensohn, Andreas
   Wilcox, Lynn
TI Authoring, Viewing, and Generating Hypervideo: An Overview of
   Hyper-Hitchcock
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Human Factors; Hypervideo; video
   summarization; link generation; video editing
AB Hyper-Hitchcock consists of three components for creating and viewing a form of interactive video called detail-on-demand video: a hypervideo editor, a hypervideo player, and algorithms for automatically generating hypervideo summaries. Detail-on-demand video is a form of hypervideo that supports one hyperlink at a time for navigating between video sequences. The Hyper-Hitchcock editor enables authoring of detail-on-demand video without programming and uses video processing to aid in the authoring process. The Hyper-Hitchcock player uses labels and keyframes to support navigation through and back hyperlinks. Hyper-Hitchcock includes techniques for automatically generating hypervideo summaries of one or more videos that take the form of multiple linear summaries of different lengths with links from the shorter to the longer summaries. User studies on authoring and viewing provided insight into the various roles of links in hypervideo and found that player interface design greatly affects people's understanding of hypervideo structure and the video they access.
C1 [Shipman, Frank] Texas A&M Univ, Dept Comp Sci, College Stn, TX 77843 USA.
   [Girgensohn, Andreas; Wilcox, Lynn] FX Palo Alto Lab, Palo Alto, CA USA.
C3 Texas A&M University System; Texas A&M University College Station
RP Shipman, F (corresponding author), Texas A&M Univ, Dept Comp Sci, College Stn, TX 77843 USA.
EM shipman@cs.tamu.edu; andreasg@fxpal.com; wilcox@fxpal.com
RI Shipman, Frank/W-2686-2019
CR [Anonymous], 2002, P ACM MULT JUAN LES
   [Anonymous], **NON-TRADITIONAL**
   BOISSIERE G, 1998, P ACM HYP 98, P279
   Chang HB, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P2219
   Christel M. G., 1998, CHI 98. Human Factors in Computing Systems. CHI 98 Conference Proceedings, P171, DOI 10.1145/274644.274670
   CUTTING DR, 1992, SIGIR 92 : PROCEEDINGS OF THE FIFTEENTH ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P318
   Girgensohn A, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P464
   GIRGENSOHN A., 2004, P WORKING C ADV VISU, P290, DOI [10.1145/989863.989913, DOI 10.1145/989863.989913]
   GIRGENSOHN A, 2000, P UIST 00, P81
   GUIMARES N, 2000, INTERACT MULTIMED J, V2
   HIRATA K, 1996, P ACM HYP 96 C WASH, P11
   Li F. C., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P169, DOI 10.1145/332040.332425
   Lippman A., 1980, Computer Graphics, V14, P32, DOI 10.1145/965105.807465
   Rui Y, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P237, DOI 10.1109/MMCS.1998.693648
   Rui Y., 2004, A unified framework for video summarization, browsing and retrieval
   Sawhney N, 1997, IEEE MULTIMEDIA, V4, P30, DOI 10.1109/93.641877
   Shipman F., 2003, P 11 ACM INT C MULT, P392, DOI DOI 10.1145/957013.957096
   SHIPMAN F, 2003, P INTERACT 2003, P33
   Shipman F., 2005, P 16 ACM C HYP HYP S, P217, DOI [https://doi.org/10.1145/1083356, DOI 10.1145/1083356]
   SMITH JM, 2000, P ACM HYP 2000, P11
   TOLVA J., 1998, MediaLoom: an Interactive Authoring Tool for Hypervideo
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Wildemuth BM, 2003, ACM-IEEE J CONF DIG, P221, DOI 10.1109/JCDL.2003.1204866
   [No title captured]
NR 24
TC 26
Z9 29
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2008
VL 5
IS 2
AR 15
DI 10.1145/1413862.1413868
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 376AU
UT WOS:000261155900006
DA 2024-07-18
ER

PT J
AU Gleicher, ML
   Liu, F
AF Gleicher, Michael L.
   Liu, Feng
TI Re-Cinematography: Improving the Camerawork of Casual Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT ACM Multimedia Conference 2007
CY SEP 24-29, 2007
CL Augsburg, GERMANY
SP ACM
DE Design; Experimentation; Human Factors; Image stabilization; casual
   video; cinematography
ID MODEL
AB This article presents an approach to postprocessing casually captured videos to improve apparent camera movement. Recinematography transforms each frame of a video such that the video better follows cinematic conventions. The approach breaks a video into shorter segments. Segments of the source video where there is no intentional camera movement are made to appear as if the camera is completely static. For segments with camera motions, camera paths are keyframed automatically and interpolated with matrix logarithms to give velocity-profiled movements that appear intentional and directed. Closeups are inserted to provide compositional variety in otherwise uniform segments. The approach automatically balances the tradeoff between motion smoothness and distortion to the original imagery. Results from our prototype show improvements to poor quality home videos.
C1 [Gleicher, Michael L.; Liu, Feng] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Gleicher, ML (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
CR Achanta RSV, 2006, IEEE MULTIMEDIA, V13, P46, DOI 10.1109/MMUL.2006.12
   ADAMS B, 2005, ACM T MULTIM COMPUT, V1, P211
   Alexa M, 2002, ACM T GRAPHIC, V21, P380, DOI 10.1145/566570.566592
   Ang T., 2005, DIGITAL VIDEO HDB
   [Anonymous], 2006, Technical Report TCDCS- 2006-46
   [Anonymous], 1991, Film directing shot by shot: visualizing from concept to screen
   [Anonymous], 2006, 2006 IEEE INT C VID
   [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], 1991, Grammar of the film language
   Bennett E P., 2003, Proceedings o fACM Multimedia, P177
   Block BruceA., 2001, VISUAL STORY SEEING
   Bordwell D., 1997, FILM ART INTRO
   BRANDON B, 2005, COMPLETE DIGITAL VID
   Brown B., 2002, Cinematography: Theory and practice: Image making for cinematographers, directors, and videographers
   Buehler C, 2001, PROC CVPR IEEE, P609
   Casares Juan, 2002, P 4 C DES INT SYST P, P157
   Chalfen Richard., 1987, SNAPSHOT VERSIONS LI
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Christie M, 2005, LECT NOTES COMPUT SC, V3638, P40
   Crow F. C., 1984, Computers & Graphics, V18, P207
   Dony RD, 2005, IEE P-VIS IMAGE SIGN, V152, P425, DOI 10.1049/ip-vis:20045109
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   GIRGENSOHN A, 2000, P UIST 00, P81
   Gleicher M, 1997, PROC CVPR IEEE, P331, DOI 10.1109/CVPR.1997.609345
   GLEICHER M, 2007, P 15 INT C MULT
   GOVINDU VM, 2004, P IEEE C COMP VIS PA
   HANSEN M, 2001, Patent No. 6211913
   Heck R, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198306
   Hua XS, 2004, IEEE T CIRC SYST VID, V14, P572, DOI 10.1109/TCSVT.2004.826750
   IRANI M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P605, DOI 10.1109/ICCV.1995.466883
   IRANI M, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P454, DOI 10.1109/CVPR.1994.323866
   Irani M, 1998, P IEEE, V86, P905, DOI 10.1109/5.664279
   Johnston Ollie., 1981, The illusion of life: Disney animation
   KENDER J, 2000, P ACCV
   Kirk D, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P61, DOI 10.1145/1240624.1240634
   Litvin A, 2003, PROC SPIE, V5022, P663, DOI 10.1117/12.476436
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matsushita Y, 2006, IEEE T PATTERN ANAL, V28, P1150, DOI 10.1109/TPAMI.2006.141
   MEI T, 2005, P ACM MULT SING NOV, P531
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Nistér D, 2005, MACH VISION APPL, V16, P321, DOI 10.1007/s00138-005-0006-y
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Osian M, 2004, MACH VISION APPL, V15, P172, DOI 10.1007/s00138-004-0141-x
   Pan Zailiang., 2004, MIR 04, P69
   Rosenholtz R, 1999, VISION RES, V39, P3157, DOI 10.1016/S0042-6989(99)00077-2
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Szeliski R, 1996, IEEE COMPUT GRAPH, V16, P22, DOI 10.1109/38.486677
   SZELISKI R, 2006, MSRT200492
   Teodosio L, 2005, ACM T MULTIM COMPUT, V1, P16, DOI 10.1145/1047936.1047940
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Wood D. N., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P243, DOI 10.1145/258734.258859
   Yan Wei-Qi., 2002, MULTIMEDIA 02, P107
NR 53
TC 81
Z9 95
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2008
VL 5
IS 1
AR 2
DI 10.1145/1404880.1404882
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 376AS
UT WOS:000261155700002
DA 2024-07-18
ER

PT J
AU Lin, T
   Wang, C
   Lin, PC
AF Lin, Tsungnan
   Wang, Chiapin
   Lin, Po-Chiang
TI A neural-network-based context-aware handoff algorithm for multimedia
   computing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; performance; multimedia computing; handoff; context-aware;
   neural networks
ID CALL ADMISSION; RESOURCE-ALLOCATION; WIRELESS; PERFORMANCE;
   PRIORITIZATION; PREDICTION; SERVICES; SCHEMES
AB The access of multimedia computing in wireless networks is concerned with the performance of handoff because of the irretrievable property of real-time data delivery. To lessen throughput degradation incurred by unnecessary handoffs or handoff latencies leading to media disruption perceived by users, this paper presents a link quality based handoff algorithm. Neural networks are used to learn the cross-layer correlation between the link quality estimator such as packet success rate and the corresponding context metric indictors, for example, the transmitting packet length, received signal strength, and signal to noise ratio. Based on a pre-processed learning of link quality profile, neural networks make essential handoff decisions efficiently with the evaluations of link quality instead of the comparisons between relative signal strength. The experiment and simulation results show that the proposed algorithm improves the user perceived qualities in a transmission scenario of VoIP applications by minimizing both the number of lost packets and unnecessary handoffs.
C1 [Lin, Tsungnan] Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
   [Lin, Tsungnan; Lin, Po-Chiang] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10764, Taiwan.
   [Wang, Chiapin] Natl Taiwan Normal Univ, Inst Appl Elect Technol, Taipei, Taiwan.
C3 National Taiwan University; National Taiwan University; National Taiwan
   Normal University
RP Lin, T (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
EM tsungnan@ntu.edu.tw
OI LIN, TSUNGNAN/0000-0001-5659-1194
CR *3GPP, 1999, TSGS4 3GPP
   Aljadhai AR, 1999, IEEE INFOCOM SER, P1019, DOI 10.1109/INFCOM.1999.751656
   [Anonymous], 2001, Communication Systems
   [Anonymous], 1990, Adv Neural Inform Process Syst.
   Bahl P., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P775, DOI 10.1109/INFCOM.2000.832252
   CHANDRA A, 1999, P IEEE VTC IEEE LOS, P1397
   Chang RS, 2004, IEEE T WIREL COMMUN, V3, P1526, DOI 10.1109/TWC.2004.833515
   Chiu MH, 2000, IEEE J SEL AREA COMM, V18, P510, DOI 10.1109/49.840208
   Cole RG, 2001, ACM SIGCOMM COMP COM, V31, P9, DOI 10.1145/505666.505669
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   DASSANAYAKE P, 1994, VTC 1994 - 1994 IEEE 44TH VEHICULAR TECHNOLOGY CONFERENCE, VOLS 1-3, P73, DOI 10.1109/VETEC.1994.345162
   Ebersman HG, 1999, IEEE T VEH TECHNOL, V48, P20, DOI 10.1109/25.740057
   FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8
   Ganguly S, 2006, IEEE J SEL AREA COMM, V24, P2147, DOI 10.1109/JSAC.2006.881594
   Han M, 2000, IEEE ICC, P1519, DOI 10.1109/ICC.2000.853750
   Haratcherev I., 2004, P 2 INT WORKSHOP MOB, P10
   Haykin S, 1996, IEEE SIGNAL PROC MAG, V13, P24, DOI 10.1109/79.487040
   Heusse M, 2003, IEEE INFOCOM SER, P836
   IRIE B, 1988, P IEEE INT C NEUR NE, P641
   Kanter TG, 2003, IEEE INTERNET COMPUT, V7, P43, DOI 10.1109/MIC.2003.1189188
   Lal D, 2003, GLOB TELECOMM CONF, P446
   Lau SSF, 1995, GLOB TELECOMM CONF, P509, DOI 10.1109/GLOCOM.1995.501979
   Levine DA, 1997, IEEE ACM T NETWORK, V5, P1, DOI 10.1109/90.554717
   LI W, 2003, P IEEE RAD WIR C AUG, P71
   LIN T, 2005, P IEEE ICASSP IEEE L, P1129
   LIODAKIS G, 1994, VTC 1994 - 1994 IEEE 44TH VEHICULAR TECHNOLOGY CONFERENCE, VOLS 1-3, P1820, DOI 10.1109/VETEC.1994.345413
   Liu T, 1998, IEEE J SEL AREA COMM, V16, P922, DOI 10.1109/49.709453
   Naghshineh M, 1996, IEEE J SEL AREA COMM, V14, P711, DOI 10.1109/49.490422
   OHTA K, 2002, P 3 IEEE PAC RIM C M, P9
   Oliveira C, 1998, IEEE J SEL AREA COMM, V16, P858, DOI 10.1109/49.709449
   Oliver M, 1999, IEEE INFOCOM SER, P1187, DOI 10.1109/INFCOM.1999.751675
   ONEL T, 2002, THESIS BOGAZICI U IS
   PACK S, 2002, P IEEE NETW C IEEE L
   Ramanathan P, 1999, IEEE J SEL AREA COMM, V17, P1270, DOI 10.1109/49.778185
   Ramani I, 2005, IEEE INFOCOM SER, P675
   Rappaport T.S., 2003, WIRELESS COMMUNICATI, V2nd
   Sharma S, 2004, IEEE J SEL AREA COMM, V22, P643, DOI 10.1109/JSAC.2004.825988
   TEKINAY S, 1992, IEEE J SEL AREA COMM, V10, P1343, DOI 10.1109/49.166761
   Tripathi ND, 1998, ICC 98 - 1998 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS VOLS 1-3, P1733, DOI 10.1109/ICC.1998.683126
   Velayos H, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P3844, DOI 10.1109/ICC.2004.1313272
   WANG SS, 2001, P IEEE EM TECHN S BR, P97
   WANG SS, 2002, P IEEE VTC SPRING IE, P1936
   Wong KD, 2000, IEEE J SEL AREA COMM, V18, P1301, DOI 10.1109/49.857930
   Yap JH, 2002, IEE CONF PUBL, P500, DOI 10.1049/cp:20020447
   Yokota H., 2002, Proceedings of the 8th annual international conference on Mobile computing and networking (MobiCom'02), (New York, NY, USA), P131
   YOON CH, 1993, IEEE J SEL AREA COMM, V11, P911, DOI 10.1109/49.232300
   Yu F, 2001, IEEE INFOCOM SER, P518, DOI 10.1109/INFCOM.2001.916771
   Yu OTW, 1997, IEEE J SEL AREA COMM, V15, P1208, DOI 10.1109/49.622906
   Zonoozi MM, 1997, IEEE J SEL AREA COMM, V15, P1239, DOI 10.1109/49.622908
   2000, AIROPEEK WILDPACKETS
NR 50
TC 17
Z9 18
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2008
VL 4
IS 3
AR 17
DI 10.1145/1386109.1386110
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 351OI
UT WOS:000259433300001
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhang, C
   Rui, Y
   Crawford, J
   He, LW
AF Zhang, Cha
   Rui, Yong
   Crawford, Jim
   He, Li-Wei
TI An automated end-to-end lecture capture and broadcasting system
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE design; automated lecture capture; live/on-demand broadcasting; lecture
   broadcasting
AB Remote viewing of lectures presented to a live audience is becoming increasingly popular. At the same time, the lectures can be recorded for subsequent on-demand viewing over the Internet. Providing such services, however, is often prohibitive due to the labor-intensive cost of capturing and pre/post-processing. This article presents a complete automated end-to-end system that supports capturing, broadcasting, viewing, archiving and searching of presentations. Specifically, we describe a system architecture that minimizes the pre- and post-production time, and a fully automated lecture capture system called iCam2 that synchronously captures all contents of the lecture, including audio, video, and presentation material. No staff is needed during lecture capture and broadcasting, so the operational cost of the system is negligible. The system has been used on a daily basis for more than 4 years, during which 522 lectures have been captured. These lectures have been viewed over 20,000 times.
C1 [Zhang, Cha; Rui, Yong; Crawford, Jim; He, Li-Wei] Microsoft Res, Redmond, WA 98052 USA.
C3 Microsoft
RP Zhang, C (corresponding author), Microsoft Res, 1 Microsoft Way, Redmond, WA 98052 USA.
EM chazhang@microsoft.com; yongrui@microsoft.com; jamescr@microsoft.com;
   lhe@microsoft.com
CR Abowd GD, 1999, IBM SYST J, V38, P508, DOI 10.1147/sj.384.0508
   Amir A, 2004, J VIS COMMUN IMAGE R, V15, P467, DOI 10.1016/j.jvcir.2004.04.008
   [Anonymous], BIBS LECT WEBCASTING
   [Anonymous], 1998, P JOINT DARPA NIST S
   BAECKER R, 2003, P CTR ADV STUD COLL
   Bianchi M., 2004, MUM 04, P117
   CHERKASOVA L, 2002, P NOSSDAV
   Cruz G., 1994, Proceedings ACM Multimedia '94, P193, DOI 10.1145/192593.192654
   FINN K, 1977, VIDEO MEDIATED COMMU
   Gleicher M., 2000, Proceedings ACM Multimedia 2000, P375, DOI 10.1145/354384.354537
   HE L, 2001, P ACM C COMP SUPP CO
   HE LW, 2004, MSRTR200491
   Heck R, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198306
   HERLOCKER JL, 1995, P ACM MULT ACM NEW Y, P155
   LIU T, 2004, P IEEE INT WORKSH MU
   MACHNICKI E, 2002, P SPIE MULT COMP NET
   MATSUO Y, 2002, P 10 ACM INT C MULT, P255, DOI DOI 10.1145/641007.641058
   Mukhopadhyay S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P477, DOI 10.1145/319463.319690
   ONISHI M, 2004, P INT C PATT REC ICP
   ROWE LA, 2006, IEEE MULT, V13, P4
   RUI Y, 2004, ACM MULTIMED SYST J, V10, P13
   RUI Y, 2004, IEEE INT C AC SPEECH
   Rui Y., 2001, P 9 ACM INT C MULTIM, P2, DOI DOI 10.1145/500141.500145
   SCOTT P, 2001, P WEBN WORLD C WWW I
   STEINMETZ A, 2001, P SPIE MULT COMP NET
   TANO J, 1993, COMPUT SUPPORT COOP, V3, P163
   Tashev I, 2005, P IEEE INT C AC SPEE P IEEE INT C AC SPEE
   WALLICK M, 2004, P ICME
   Wang F., 2003, MULTIMEDIA '03: Proceedings of the eleventh ACM international conference on Multimedia, P315
   YOKOI T, 2004, P ICME
   ZHANG C, 2005, P ICME
NR 31
TC 39
Z9 44
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 1
AR 6
DI 10.1145/1324287.1324293
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 264UK
UT WOS:000253315700006
DA 2024-07-18
ER

PT J
AU Kumar, P
   Bhatt, G
   Ingle, O
   Goyal, D
   Raman, B
AF Kumar, Puneet
   Bhatt, Gaurav
   Ingle, Omkar
   Goyal, Daksh
   Raman, Balasubramanian
TI Affective Feedback Synthesis Towards Multimodal Text and Image Data
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Affective computing; feedback synthesis; multimodal input; dataset
   construction; context vector
AB In this article, we have defined a novel task of affective feedback synthesis that generates feedback for input text and corresponding images in a way similar to humans responding to multimodal data. A feedback synthesis system has been proposed and trained using ground-truth human comments along with image-text input. We have also constructed a large-scale dataset consisting of images, text, Twitter user comments, and the number of likes for the comments by crawling news articles through Twitter feeds. The proposed system extracts textual features using a transformer-based textual encoder. The visual features have been extracted using a Faster region-based convolutional neural networks model. The textual and visual features have been concatenated to construct multimodal features that the decoder uses to synthesize the feedback. We have compared the results of the proposed system with baseline models using quantitative and qualitative measures. The synthesized feedbacks have been analyzed using automatic and human evaluation. They have been found to be semantically similar to the ground-truth comments and relevant to the given text-image input.
C1 [Kumar, Puneet; Ingle, Omkar; Raman, Balasubramanian] Indian Inst Technol Roorkee, Roorkee 247667, India.
   [Bhatt, Gaurav] Indian Inst Technol Hyderabad, Hyderabad 502285, India.
   [Goyal, Daksh] Natl Inst Technol Karnataka, Karnataka, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Hyderabad; National
   Institute of Technology (NIT System); National Institute of Technology
   Karnataka
RP Kumar, P (corresponding author), Indian Inst Technol Roorkee, Roorkee 247667, India.
EM pkumar99@cs.iitr.ac.in; gauravbhatt@iith.ac.in; iuttam@mt.iitr.ac.in;
   dakshgoyal.171cv111@nitk.edu.in; bala@cs.iitr.ac.in
RI Kumar, Dr. Puneet/ILC-4863-2023
OI Kumar, Puneet/0000-0002-4318-1353; Raman,
   Balasubramanian/0000-0001-6277-6267
FU Ministry of Education INDIA [1-3146198040]
FX The Ministry of Education INDIA has supported this research through
   grant no. 1-3146198040.
CR Alamri H, 2019, PROC CVPR IEEE, P7550, DOI 10.1109/CVPR.2019.00774
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Bhandari M, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P9347
   Blikstein P., 2016, Journal of Learning Analytics, V3, P220, DOI [10.18608/jla.2016.32.11, DOI 10.18608/JLA.2016.32.11]
   Bouras C, 2014, TELEMAT INFORM, V31, P237, DOI 10.1016/j.tele.2013.06.003
   Chen FL, 2020, AAAI CONF ARTIF INTE, V34, P7504
   Chen JQ, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4046
   Chen KY, 2015, IEEE-ACM T AUDIO SPE, V23, P1322, DOI 10.1109/TASLP.2015.2432578
   Cho J, 2021, PR MACH LEARN RES, V139
   Craswell N., 2009, Encyclopedia of Database Systems, P1703
   Dornel Benjamin, 2020, NEW YORK TIMES ARTIC
   El-Kassas WS, 2021, EXPERT SYST APPL, V165, DOI 10.1016/j.eswa.2020.113679
   Fortin MP, 2019, PROCEEDINGS OF THE ACM WORKSHOP ON CROSSMODAL LEARNING AND APPLICATION (WCRML'19), P3, DOI 10.1145/3326459.3329165
   Gallo FR, 2020, FUTURE GENER COMP SY, V110, P918, DOI 10.1016/j.future.2019.10.044
   Gao S, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4854
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gu X., 2018, ARXIV180512352
   He XD, 2017, IEEE SIGNAL PROC MAG, V34, P109, DOI 10.1109/MSP.2017.2741510
   Hoekstra M., 2013, HASP@ ISCA, P11, DOI 10.1145/
   Hori C, 2019, INT CONF ACOUST SPEE, P2352, DOI 10.1109/ICASSP.2019.8682583
   Hu A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P350, DOI 10.1145/3219819.3219853
   Hu JJ, 2020, AAAI CONF ARTIF INTE, V34, P7969
   Huang T.-H., 2016, NAACL HLT, P1233
   Jiang XZ, 2020, AAAI CONF ARTIF INTE, V34, P11125
   Kang GC, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2024
   Kryscinski W, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P9332
   Kumar P, 2021, IEEE IMAGE PROC, P314, DOI 10.1109/ICIP42928.2021.9506714
   Ladhak F, 2020, Arxiv, DOI arXiv:2010.03093
   Lahat D, 2015, P IEEE, V103, P1449, DOI 10.1109/JPROC.2015.2460697
   Lavie A, 2009, MACH TRANSL, V23, P105, DOI 10.1007/s10590-009-9059-4
   Li HR, 2020, AAAI CONF ARTIF INTE, V34, P8188
   Li MZ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P9360
   Li NX, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P297, DOI 10.1145/3323873.3325050
   Li ZC, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2822907
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Liu YH, 2019, Arxiv, DOI arXiv:1907.11692
   Makiuchi Mariana Rodrigues, 2021, IEEE AUTOMATIC SPEEC
   McCutcheon S, 2021, ACAD PSYCHIATR, V45, P288, DOI 10.1007/s40596-021-01425-y
   McDarby Gary, 2003, ENABLING TECHNOLOGIE
   Muszynski M, 2021, IEEE T AFFECT COMPUT, V12, P36, DOI 10.1109/TAFFC.2019.2902091
   Narayan S, 2017, Arxiv, DOI arXiv:1704.04530
   Niu YL, 2019, PROC CVPR IEEE, P6672, DOI 10.1109/CVPR.2019.00684
   Pan X., 2019, IEEE T VISUALIZATION
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11073009
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Runeson P, 2007, PROC INT CONF SOFTW, P499
   Rush A.M., 2017, ACLWEB P 2015 C EMPI
   Samani ZR, 2018, SADHANA-ACAD P ENG S, V43, DOI 10.1007/s12046-018-0908-9
   Shang XD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1756, DOI 10.1145/3474085.3475321
   Simon Ian, 2007, 11 IEEECVF INT C COM, P1
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Von Lohmann Fred, 2017, COPYRIGHT LAW, P169
   Wu Y, 2019, AAAI CONF ARTIF INTE, P7281
   Xu XN, 2018, Arxiv, DOI arXiv:1809.06873
   Xu XN, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3981
   Zhao TC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P654, DOI 10.18653/v1/P17-1061
   Zhou H, 2018, AAAI CONF ARTIF INTE, P730
   Zhou XD, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1128
   Zhu JN, 2020, AAAI CONF ARTIF INTE, V34, P9749
   Zhu JN, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4154
NR 66
TC 0
Z9 0
U1 2
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 190
DI 10.1145/3589186
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, YK
   Wei, XX
   Dai, PW
   Cao, XC
AF Xu, Yikun
   Wei, Xingxing
   Dai, Pengwen
   Cao, Xiaochun
TI A<SUP>2</SUP>SC: Adversarial Attacks on Subspace Clustering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Subspace clustering; unsupervised learning; adversarial attack;
   black-box attack; adversarial set
AB Many studies demonstrate that supervised learning techniques are vulnerable to adversarial examples. However, adversarial threats in unsupervised learning have not drawn sufficient scholarly attention. In this article, we formally address the unexplored adversarial attacks in the equally important unsupervised clustering field and propose the concept of the adversarial set and adversarial set attack for clustering. To illustrate the basic idea, we design a novel adversarial space-mapping attack algorithm to confuse subspace clustering, one of the mainstream branches of unsupervised clustering. It maps a sample into one wrong class by moving it towards the closest point on the linear subspace of the target class, that is, along the normal of the closest point. This simple single-step algorithm has the power to craft the adversarial set where the image samples can be wrongly clustered, even into the targeted labels. Empirical results on different image datasets verify the effectiveness and superiority of our algorithm. We further show that deep supervised learning algorithms (such as VGG and ResNet) are also vulnerable to our crafted adversarial set, which illustrates the good cross-task transferability of the adversarial set.
C1 [Xu, Yikun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, 19 Shucun Rd, Beijing 100093, Peoples R China.
   [Xu, Yikun] Univ Chinese Acad Sci, Sch Cyber Secur, 19 Yuquan Rd, Beijing 100049, Peoples R China.
   [Wei, Xingxing] Beihang Univ, Inst Artificial Intelligence, Hangzhou Innovat Inst, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
   [Dai, Pengwen; Cao, Xiaochun] Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen Campus,66 Gongchang Rd, Beijing 518107, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Beihang University; Sun Yat Sen
   University
RP Wei, XX (corresponding author), Beihang Univ, Inst Artificial Intelligence, Hangzhou Innovat Inst, 37 Xueyuan Rd, Beijing 100191, Peoples R China.; Cao, XC (corresponding author), Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen Campus,66 Gongchang Rd, Beijing 518107, Peoples R China.
EM xuyikun18@mails.ucas.ac.cn; xxwei@buaa.edu.cn; daipw@mail.sysu.edu.cn;
   caoxiaochun@mail.sysu.edu.cn
RI xie, jing/KDO-9486-2024; Zhou, Xinyi/KGM-6689-2024; Liu,
   yuqing/KEI-3260-2024; Liu, Donghua/KEJ-1974-2024; yang,
   le/KFB-5420-2024; ren, jun/KHG-7717-2024; li, feiyang/KHW-5210-2024;
   WANG, YANAN/KCL-4840-2024; zhang, can/KHC-5357-2024; wang,
   rong/KFQ-7187-2024; Huang, Yong/KFA-1191-2024; Chen, Zheng/KCY-2338-2024
OI Liu, Donghua/0000-0002-5830-9540; Xu, Yikun/0000-0001-8111-7566
FU National Key R&D Program of China [2020YFB1406704]; Beijing Natural
   Science Foundation [L212004, M22006]; National Natural Science
   Foundation of China [62025604, U2001202, 62076018]
FX This work was supported by the National Key R&D Program of China under
   grant no. 2020YFB1406704, Beijing Natural Science Foundation (grant nos.
   L212004 and M22006), National Natural Science Foundation of China (grant
   nos. 62025604, U2001202, and 62076018).
CR Ahmad Z, 2021, DIAGN PATHOL, V16, DOI 10.1186/s13000-021-01085-4
   Bhagoji AN, 2018, LECT NOTES COMPUT SC, V11216, P158, DOI 10.1007/978-3-030-01258-8_10
   Biggio B., 2013, P ACM WORKSH ART INT, P87, DOI DOI 10.1145/2517312.2517321
   Buch VH, 2018, BRIT J GEN PRACT, V68, P143, DOI 10.3399/bjgp18X695213
   Cai D, 2005, IEEE T KNOWL DATA EN, V17, P1624, DOI 10.1109/TKDE.2005.198
   Cao Xiaochun, 2013, INT JOINT C ARTIFICI
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chang JL, 2020, IEEE T PATTERN ANAL, V42, P809, DOI 10.1109/TPAMI.2018.2889949
   Chen PY, 2017, P 10 ACM WORKSH ART, P15, DOI [10.1145/3128572.3140448, DOI 10.1145/3128572.3140448]
   Chen Y, 2020, PROC CVPR IEEE, P4154, DOI 10.1109/CVPR42600.2020.00421
   Chhabra A, 2020, AAAI CONF ARTIF INTE, V34, P3625
   Cinà AE, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108306
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Q, 2019, IEEE I CONF COMP VIS, P4732, DOI 10.1109/ICCV.2019.00483
   Ji P, 2017, ADV NEUR IN, V30
   Jiang YBY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P718, DOI 10.1145/3240508.3240582
   Kantipudi Jayendra, 2020, IEEE Transactions on Artificial Intelligence, V1, P181, DOI 10.1109/TAI.2020.3046167
   Krizhevsky A., 2009, Tech. Rep.
   Kurakin A., 2016, Adversarial machine learning at scale
   Kurakin A., 2016, WORKSHOP TRACK P
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li CG, 2017, IEEE T IMAGE PROCESS, V26, P2988, DOI 10.1109/TIP.2017.2691557
   Li JS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3418217
   Li Xiaoyan, 2023, Infect Dis Immun, V3, P20, DOI [10.1097/ID9.0000000000000076, 10.1109/TAI.2021.3107807]
   Litman, 2017, AUTONOMOUS VEHICLE I
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Liu H, 2019, IEEE I CONF COMP VIS, P2941, DOI 10.1109/ICCV.2019.00303
   Liu S, 2022, IEEE T PATTERN ANAL, V44, P4761, DOI 10.1109/TPAMI.2021.3079993
   Liu Y., 2017, INT C LEARNING REPRE
   Ma C, 2021, PROC CVPR IEEE, P11830, DOI 10.1109/CVPR46437.2021.01166
   Metzen JH, 2017, IEEE I CONF COMP VIS, P2774, DOI 10.1109/ICCV.2017.300
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Osadchy M, 2017, IEEE T INF FOREN SEC, V12, P2640, DOI 10.1109/TIFS.2017.2718479
   Park S, 2021, PROC CVPR IEEE, P12273, DOI 10.1109/CVPR46437.2021.01210
   Parsons L., 2004, SIGKDD Explor Newsl, V6, P90, DOI 10.1145/1007730.1007731
   Peng B, 2020, IEEE T CIRC SYST VID, V30, P131, DOI 10.1109/TCSVT.2018.2889514
   Peng X, 2020, IEEE T NEUR NET LEAR, V31, P5509, DOI 10.1109/TNNLS.2020.2968848
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shafahi Ali, 2018, INT C LEARNING REPRE
   Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392
   Shi YC, 2019, PROC CVPR IEEE, P6512, DOI 10.1109/CVPR.2019.00668
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Thys S, 2019, IEEE COMPUT SOC CONF, P49, DOI 10.1109/CVPRW.2019.00012
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Wang HJ, 2022, IEEE T PATTERN ANAL, V44, P1725, DOI 10.1109/TPAMI.2020.3032061
   Wang M, 2021, NEUROCOMPUTING, V429, P215, DOI 10.1016/j.neucom.2020.10.081
   Wu H, 2022, ACM T SENSOR NETWORK, V18, DOI 10.1145/3470850
   Xia CQ, 2018, IEEE ACM T COMPUT BI, V15, P1315, DOI 10.1109/TCBB.2017.2712607
   Xiao C, 2020, PROC CVPR IEEE, P409, DOI 10.1109/CVPR42600.2020.00049
   Xiao CW, 2019, PROC CVPR IEEE, P6891, DOI 10.1109/CVPR.2019.00706
   Xiao H, 2017, Arxiv, DOI [arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747]
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153
   Xu Yikun, 2022, IEEE INT C MULTIMEDI
   Yanai H, 2011, STAT SOC BEHAV SC, P1, DOI 10.1007/978-1-4419-9887-3
   Yang AY, 2008, COMPUT VIS IMAGE UND, V110, P212, DOI 10.1016/j.cviu.2007.07.005
   YANG X, 2020, ADV NEURAL INFORM PR, V33
   Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017
   Zhang HC, 2019, IEEE I CONF COMP VIS, P421, DOI 10.1109/ICCV.2019.00051
   Zhang JM, 2021, IEEE T MULTIMEDIA, V23, P2575, DOI 10.1109/TMM.2020.3013376
   Zhang JJ, 2019, PROC CVPR IEEE, P5468, DOI 10.1109/CVPR.2019.00562
   Zhang T, 2019, PR MACH LEARN RES, V97
   Zhou L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4440
NR 70
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 191
DI 10.1145/3587097
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200014
OA Bronze
DA 2024-07-18
ER

PT J
AU Chen, HZ
   Zhou, H
   Zhang, J
   Chen, DD
   Zhang, WM
   Chen, KJ
   Hua, G
   Yu, NH
AF Chen, Haozhe
   Zhou, Hang
   Zhang, Jie
   Chen, Dongdong
   Zhang, Weiming
   Chen, Kejiang
   Hua, Gang
   Yu, Nenghai
TI Perceptual Hashing of Deep Convolutional Neural Networks for Model Copy
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Artificial intelligence safety; convolutional neural networks; neural
   networks
ID ROBUST
AB In recent years, many model intellectual property (IP) proof methods for IP protection have been proposed, such as model watermarking and model fingerprinting. However, with the increasing number of models transmitted and deployed on the Internet, quickly finding the suspect model among thousands of models on model-sharing platforms such as GitHub is in great demand, which concurrently triggers the new security problem of model copy detection for IP protection. As an important part of the model IP protection system, the model copy detection task has not received enough attention. Due to the high computational complexity, both model watermarking and model fingerprinting lack the capability to efficiently find suspected infringing models among tens of millions of models. In this article, inspired by the hash-based image retrieval methods, we introduce a novel model copy detection mechanism: perceptual hashing for convolutional neural networks (CNNs). The proposed perceptual hashing algorithm can convert the weights of CNNs to fixed-length binary hash codes so that the lightly modified version has the similar hash code as the original model. By comparing the similarity of a pair of hash codes between a query model and a test model in the model library, similar versions of a query model can be retrieved efficiently. To the best of our knowledge, this is the first perceptual hashing algorithm for deep neural network models. Specifically, we first select the important model weights based on the model compression theory, then calculate the normal test statistics (NTS) on the segments of important weights, and finally encode the NTS features into hash codes. The experiment performed on a model library containing 3,565 models indicates that our perceptual hashing scheme has a superior copy detection performance.
C1 [Chen, Haozhe; Zhang, Jie; Zhang, Weiming; Chen, Kejiang; Yu, Nenghai] Univ Sci & Technol China, Chinese Acad Sci, Key Lab Elect Space Informat, Hefei, Anhui, Peoples R China.
   [Zhou, Hang] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC, Canada.
   [Chen, Dongdong] Microsoft Cloud& AI, Redmond, WA 98052 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Simon Fraser University
RP Zhang, WM (corresponding author), Univ Sci & Technol China, Chinese Acad Sci, Key Lab Elect Space Informat, Hefei, Anhui, Peoples R China.; Zhou, H (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC, Canada.
EM chzchen@mail.ustc.edu.cn; zhouhang2991@gmail.com;
   zjzac@mail.ustc.edu.cn; cddlyf@gmail.com; zhangwm@ustc.edu.cn;
   chenkj@ustc.edu.cn; ganghua@gmail.com; ynh@ustc.edu.cn
RI Chen, Dongdong/AAR-4481-2020; Zhou, Hang/AAI-5565-2021
OI Chen, Dongdong/0000-0002-7016-9288; Zhang, Jie/0000-0002-4230-1077;
   Chen, Dongdong/0000-0002-4642-4373; Zhou, Hang/0000-0001-7860-8452;
   Zhang, Weiming/0000-0001-5576-6108; Chen, Haozhe/0000-0001-6491-2023
FU Natural Science Foundation of China [U20B2047, 62072421, 62002334,
   62102386, 62121002]; Exploration Fund Project of University of Science
   and Technology of China [YD3480002001]; Fundamental Research Funds for
   the Central Universities [WK2100000011]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant U20B2047, 62072421, 62002334, 62102386 and 62121002,
   Exploration Fund Project of University of Science and Technology of
   China under Grant YD3480002001, and by Fundamental Research Funds for
   the Central Universities under Grant WK2100000011.
CR Abdullahi SM, 2020, IEEE T INF FOREN SEC, V15, P2587, DOI 10.1109/TIFS.2020.2971142
   Adi Y, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P1615
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Blalock D., 2020, What is the state of neural network pruning?, P129
   Blundell C, 2015, PR MACH LEARN RES, V37, P1613
   Cao Xiaoyu, 2019, ARXIV, DOI [10.1145/3433210.3437526, DOI 10.1145/3433210.3437526]
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chatzikonstantinou C, 2020, IEEE COMPUT SOC CONF, P3077, DOI 10.1109/CVPRW50498.2020.00366
   Cheng-Hao Tu, 2021, IEEE Transactions on Artificial Intelligence, V2, P42, DOI 10.1109/TAI.2021.3068322
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Ding KM, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9152972
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Maimó LF, 2018, IEEE ACCESS, V6, P7700, DOI 10.1109/ACCESS.2018.2803446
   Guo J, 2018, ICCAD-IEEE ACM INT, DOI 10.1145/3240765.3240862
   Guo JY, 2020, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR42600.2020.00158
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Khelifi F, 2010, IEEE T IMAGE PROCESS, V19, P981, DOI 10.1109/TIP.2009.2038637
   Kolouri S, 2020, PROC CVPR IEEE, P298, DOI 10.1109/CVPR42600.2020.00038
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee SY, 2023, IEEE T DEPEND SECURE, V20, P3434, DOI 10.1109/TDSC.2022.3196790
   Li YC, 2021, ISSTA '21: PROCEEDINGS OF THE 30TH ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON SOFTWARE TESTING AND ANALYSIS, P139, DOI 10.1145/3460319.3464816
   Liang XP, 2023, IEEE T KNOWL DATA EN, V35, P3765, DOI 10.1109/TKDE.2021.3131188
   Liu SG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355394
   Lukas N., 2019, arXiv
   Lv ZH, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3418215
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Özyurt F, 2019, ARAB J SCI ENG, V44, P3173, DOI 10.1007/s13369-018-3454-1
   Razali N. M., 2011, J. Stat. Model. and Anal., V2, P21, DOI DOI 10.1515/BILE-2015-0008
   Ribeiro M, 2015, 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P896, DOI 10.1109/ICMLA.2015.152
   Richang Hong, 2015, IEEE Transactions on Big Data, V1, P152, DOI 10.1109/TBDATA.2016.2515640
   Rivest R., 1992, Tech. Rep.
   Rouhani BD, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P485, DOI 10.1145/3297858.3304051
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591
   Shi QHY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485665
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Stallkamp J, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1453, DOI 10.1109/IJCNN.2011.6033395
   Swaminathan A, 2006, IEEE T INF FOREN SEC, V1, P215, DOI 10.1109/TIFS.2006.873601
   Tang ZJ, 2019, IEEE T KNOWL DATA EN, V31, P549, DOI 10.1109/TKDE.2018.2837745
   Do TT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3314051
   Tian XY, 2022, IEEE T CIRC SYST VID, V32, P4804, DOI 10.1109/TCSVT.2021.3121987
   Uchida Y, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P274, DOI 10.1145/3078971.3078974
   Wang C, 2020, IEEE T SUST COMPUT, V5, P365, DOI 10.1109/TSUSC.2019.2930526
   Wu D, 2009, SIGNAL PROCESS, V89, P2415, DOI 10.1016/j.sigpro.2009.05.016
   Zareapoor M, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3458280
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
   Zhang JL, 2018, PROCEEDINGS OF THE 2018 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIACCS'18), P159, DOI 10.1145/3196494.3196550
   Zhang J, 2022, IEEE T PATTERN ANAL, V44, P4005, DOI 10.1109/TPAMI.2021.3064850
   Zhang J, 2021, Arxiv, DOI arXiv:2108.02360
   Zhang J, 2020, AAAI CONF ARTIF INTE, V34, P12805
   Zhang Jie, 2020, NEURAL INF PROCESS S, V33
   Zhao JJ, 2020, COMPUT COMMUN, V150, P488, DOI 10.1016/j.comcom.2019.12.016
   Zhou ZL, 2017, IEEE T INF FOREN SEC, V12, P48, DOI 10.1109/TIFS.2016.2601065
   Zhu AJ, 2022, KNOWL-BASED SYST, V248, DOI 10.1016/j.knosys.2022.108843
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 63
TC 5
Z9 5
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 123
DI 10.1145/3572777
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300023
DA 2024-07-18
ER

PT J
AU Wang, XH
   Zhu, LC
   Wu, F
   Yang, Y
AF Wang, Xiaohan
   Zhu, Linchao
   Wu, Fei
   Yang, Yi
TI A Differentiable Parallel Sampler for Efficient Video Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video recognition; efficient video classification; video sampling
AB It is crucial to sample a small portion of relevant frames for efficient video classification. The existing methods mainly develop hand-designed sampling strategies or learn sequential selection policies. However, there are two challenges to be solved. First, hand-designed sampling strategies are intrinsically non-adaptive to different video backbones. Second, sequential frame selection policies ignore temporal relations among all video frames. The sequential selection process also hinders the application of these video samplers in speed-critical systems. In this article, we propose a differentiable parallel video sampling network (PSN) to tackle the aforementioned challenges, First, we optimize the video sampler with a differentiable surrogate loss, allowing to dynamically learn the sampler with the cooperation from the video classification model. Our sampler considers the feedback from all frames jointly, eliminating the learning difficulties of sequential decision making. The learning process is fully gradient-based, making the sampler be learned efficiently. Our video sampler can assess a set of frames swiftly and determine the importance of each frame in parallel. Second, we propose to model the inter-relation among contextual frames, which encourages the sampler to select frames based on a comprehensive inspection of the entire video. We observe that a simple context relation mining instantiation would significantly improve the classification performance. The experimental results on three standard video recognition benchmarks demonstrate the efficacy and efficiency of our framework.
C1 [Wang, Xiaohan; Zhu, Linchao; Wu, Fei; Yang, Yi] Zhejiang Univ, 38 Zheda Rd, Hangzhou 310007, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Wang, XH (corresponding author), Zhejiang Univ, 38 Zheda Rd, Hangzhou 310007, Zhejiang, Peoples R China.
EM xiaohan.wang@zju.edu.cn; zhulinchao@zju.edu.cn; wufei@zju.edu.cn;
   yangyics@zju.edu.cn
RI yang, yang/HGT-7999-2022; Wang, Xiaohan/JKI-4414-2023
OI Wang, Xiaohan/0000-0001-5273-4223
FU Fundamental Research Funds for the Central Universities
   [226-2022-00051]; Zhejiang Lab Open Research Project [2021KG0AB06]
FX This work is supported by the Fundamental Research Funds for the Central
   Universities (No. 226-2022-00051) and partially supported by Zhejiang
   Lab Open Research Project (No. 2021KG0AB06).
CR Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen ZR, 2019, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR.2019.00939
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Donahue J., 2019, EMBC
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan HH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P705
   Fan HH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3390891
   Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu HZ, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3422360
   Huang Y, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409332
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Korbar B, 2019, IEEE I CONF COMP VIS, P6241, DOI 10.1109/ICCV.2019.00633
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Simonyan K, 2014, ADV NEUR IN, V27
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang XH, 2023, IEEE T PATTERN ANAL, V45, P6605, DOI 10.1109/TPAMI.2020.3015894
   Wang XH, 2023, IEEE T MULTIMEDIA, V25, P6079, DOI 10.1109/TMM.2022.3204444
   Wu WH, 2019, IEEE I CONF COMP VIS, P6231, DOI 10.1109/ICCV.2019.00632
   Wu ZX, 2019, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2019.00137
   Wu ZX, 2019, ADV NEUR IN, V32
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Ye J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3038917
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yue Meng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P86, DOI 10.1007/978-3-030-58571-6_6
   Zhang HW, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2978656
   Zheng YD, 2020, IEEE T IMAGE PROCESS, V29, P7970, DOI 10.1109/TIP.2020.3007826
   Zhu C, 2018, LECT NOTES COMPUT SC, V11209, P139, DOI 10.1007/978-3-030-01228-1_9
   Zhu LC, 2022, IEEE T MULTIMEDIA, V24, P668, DOI 10.1109/TMM.2021.3057503
   Zhu LC, 2017, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2017.147
   Zhu Y, 2019, LECT NOTES COMPUT SC, V11363, P363, DOI 10.1007/978-3-030-20893-6_23
   Zhuang YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3366710
NR 40
TC 5
Z9 5
U1 6
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 112
DI 10.1145/3569584
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300012
DA 2024-07-18
ER

PT J
AU Chan, KH
   Im, SK
AF Chan, Ka-Hou
   Im, Sio-Kei
TI Using Four Hypothesis Probability Estimators for CABAC in Versatile
   Video Coding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE VVC; CABAC; entropy coding; probability estimator; adaptation rate;
   video compression
AB This article introduces the key technologies involved in four hypothetical probability estimators for Context-based Adaptive Binary Arithmetic Coding (CABAC). The focus is on the selected adaptation rate performed in these estimators, which are selected based on coding efficiency and memory considerations, and also the relationship with the current size of the coding block. The proposed scheme can linearly realize the quantitative representation of probabilistic prediction and describes the scalability potential for higher accuracy. Besides a description of the design concept, this work also discusses motivation and implementation aspects, which are based on simple operations such as bitwise operations and single subsampling for subinterval updates. The experimental results verify the effectiveness of the proposed CABAC method specified in Versatile Video Coding (VVC).
C1 [Chan, Ka-Hou] Macao Polytech Univ, Fac Appl Sci, Macau 999078, Peoples R China.
   [Im, Sio-Kei] Macao Polytech Univ, Macau 999078, Peoples R China.
C3 Macao Polytechnic University; Macao Polytechnic University
RP Chan, KH (corresponding author), Macao Polytech Univ, Fac Appl Sci, Macau 999078, Peoples R China.
EM chankahou@mpu.edu.mo; marcusim@mpu.edu.mo
RI Chan, Ka-Hou/JFS-2196-2023
OI Chan, Ka-Hou/0000-0002-0183-0685; IM, SIO KEI/0000-0002-5599-4300
CR Alshin Alexander., 2013, 2013 Visual Communications and Image Processing (VCIP), P1, DOI DOI 10.1109/VCIP.2013.6706454
   [Anonymous], 2023, ACM T MULTIM COMPUT, V19
   Belyaev E, 2006, I SYMP CONSUM ELECTR, P194
   Bjontegaard G., 2001, P VID COD EXP GROUP, P1520
   Bossen Frank, 2018, CE5 RELATED IMPLEMEN
   Bossen Frank, 2020, REFERENCE SOFTWARE V
   Boyce J., 2018, JVETJ1010
   Bross B, 2019, Document JVET-N1001
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Chan KH, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P623, DOI 10.1145/3460426.3463664
   Chan KH, 2021, PROC SPIE, V11878, DOI 10.1117/12.2601046
   Chan KH, 2021, IEEE ACCESS, V9, P120380, DOI 10.1109/ACCESS.2021.3107579
   Haase P, 2020, IEEE DATA COMPR CONF, P163, DOI 10.1109/DCC47342.2020.00024
   Holt CC, 2004, INT J FORECASTING, V20, P5, DOI 10.1016/j.ijforecast.2004.09.015
   Howard P. G., 1992, Image and text compression, P85
   Im S.-K., 2020, P 2020 IEEE 5 INT C, DOI [10.1109/icsip49896.2020.9339374, DOI 10.1109/ICSIP49896.2020.9339374]
   Im SK, 2020, IET IMAGE PROCESS, V14, P125, DOI 10.1049/iet-ipr.2018.6602
   JVET, 2021, VERS VID CODID VVC R
   Ka-Hou Chan, 2021, 2021 4th International Conference on Information and Communications Technology (ICOIACT), P299, DOI 10.1109/ICOIACT53268.2021.9563992
   Kim IK, 2012, IEEE T CIRC SYST VID, V22, P1697, DOI 10.1109/TCSVT.2012.2223011
   Kirchhoffer Heiner, 2019, DESCRIPTION CORE EXP
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   Niu BB, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954508
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sze V, 2012, IEEE T CIRC SYST VID, V22, P1778, DOI 10.1109/TCSVT.2012.2221526
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wien M., 2020, P 2020 IEEE INT C VI, P1, DOI DOI 10.1109/VCIP49819.2020.9301820
NR 27
TC 3
Z9 3
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
AR 40
DI 10.1145/3531015
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800014
DA 2024-07-18
ER

PT J
AU Li, R
   Wei, W
   Hao, PN
   Su, J
   Sun, FY
AF Li, Ran
   Wei, Wei
   Hao, Peinan
   Su, Jian
   Sun, Fengyuan
TI Context-aware Pseudo-true Video Interpolation at 6G Edge
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 6G network; edge computing; motion-compensated frame interpolation;
   context layer; bidirectional motion estimation
ID MOTION ESTIMATION; FRAME; ALGORITHM
AB In the 6G network, lots of edge devices facilitate the low-latency transmission of video. However, with limited processing and storage capabilities, the edge devices cannot afford to reconstruct the vast amount of video data. On the condition of edge computing in the 6G network, this article fuses a self-similarity-based context feature into Frame Rate Up-Conversion (FRUC) to generate the pseudo-true video sequences at high frame rate, and its core is the extraction of the context layer for each video frame. First, we extract the patch centered at each pixel and use the self-similarity descriptor to generate the correlation surface. Then, the expectation or skewness of the correlation surface in statistics is computed to represent its context feature. By attaching an expectation or a skewness to each pixel, the context layer is constructed and added to the video frame as a new channel. According to the context layer, we predict the motion vector field of the absent frame by using the bidirectional context match and finally produce the interpolated frame. From the experimental results, it can be seen that by deploying the proposed FRUC algorithm on edge devices, the output pseudo-true video sequences have satisfying objective and subjective qualities.
C1 [Li, Ran; Hao, Peinan] Xinyang Normal Univ, Sch Comp & Informat Technol, 237 Nanhu Rd, Xinyang 464000, Henan, Peoples R China.
   [Wei, Wei] Xian Univ Technol, Sch Comp Sci & Engn, 8 Jinhuanan Rd, Xian 7140048, Shaanxi, Peoples R China.
   [Su, Jian] Nangjing Univ Informat Sci & Technol, Sch Comp & Software, 219 Ningliu Rd, Nanjing 213001, Jiangsu, Peoples R China.
   [Sun, Fengyuan] Guilin Univ Elect Technol, Guangxi Key Lab Wireless Wideband Commun & Signal, 1 Jinji Rd, Guilin 541004, Guangxi, Peoples R China.
C3 Xinyang Normal University; Xi'an University of Technology; Nanjing
   University of Information Science & Technology; Guilin University of
   Electronic Technology
RP Li, R (corresponding author), Xinyang Normal Univ, Sch Comp & Informat Technol, 237 Nanhu Rd, Xinyang 464000, Henan, Peoples R China.
EM liran@xynu.edu.cn; haopn_xynu@163.com; sj890718@gmail.com;
   fengyuansun@foxmail.com
RI Li, Ran/N-3389-2013
OI Li, Ran/0000-0001-7475-759X; Sun, Fengyuan/0000-0002-5882-7727
FU Project of Science and Technology Department of Henan Province in China
   [212102210106]; National Natural Science Foundation of China [31872704];
   Guangxi Key Laboratory of Wireless Wideband Communication and Signal
   Processing of China
FX This work is supported in part by the Project of Science and Technology
   Department of Henan Province in China (212102210106), in part by the
   National Natural Science Foundation of China (31872704), and in part by
   the Guangxi Key Laboratory of Wireless Wideband Communication and Signal
   Processing of China.
CR Choi BD, 2007, IEEE T CIRC SYST VID, V17, P407, DOI 10.1109/TCSVT.2007.893835
   Choi G, 2019, IEEE T CIRC SYST VID, V29, P1251, DOI 10.1109/TCSVT.2018.2840842
   Choi G, 2017, IEEE IMAGE PROC, P800, DOI 10.1109/ICIP.2017.8296391
   Dikbas S, 2013, IEEE T IMAGE PROCESS, V22, P2931, DOI 10.1109/TIP.2012.2222893
   Ding XL, 2019, IEEE T CIRC SYST VID, V29, P1893, DOI 10.1109/TCSVT.2018.2852799
   Guo Y, 2016, J DISP TECHNOL, V12, P89, DOI 10.1109/JDT.2015.2466104
   He JL, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3382506
   Hu HZ, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3422360
   Hu Yuzhang, 2022, ACM T MULTIM COMPUT, DOI [10.1145/3528173JustAccepted, DOI 10.1145/3528173JUSTACCEPTED]
   Jacobson N, 2010, IEEE T IMAGE PROCESS, V19, P2924, DOI 10.1109/TIP.2010.2050928
   Jeon J, 2021, IEEE ROBOT AUTOM LET, V6, P5332, DOI 10.1109/LRA.2021.3075141
   Jeong SG, 2013, IEEE T IMAGE PROCESS, V22, P4497, DOI 10.1109/TIP.2013.2274731
   Kafadar Ö, 2021, IEEE SENS J, V21, P6306, DOI 10.1109/JSEN.2020.3043753
   Kim D, 2013, IEEE T CIRC SYST VID, V23, P445, DOI 10.1109/TCSVT.2012.2207271
   Lee GG, 2014, IEEE J EM SEL TOP C, V4, P29, DOI 10.1109/JETCAS.2014.2298923
   Lee K, 2016, ELECTRON LETT, V52, P354, DOI 10.1049/el.2015.3612
   Li M, 2022, INT CONF ACOUST SPEE, P2844, DOI 10.1109/ICASSP43922.2022.9746119
   Liu YQ, 2020, IEEE INTERNET THINGS, V7, P6722, DOI 10.1109/JIOT.2020.3004500
   Lu QC, 2016, J DISP TECHNOL, V12, P45, DOI 10.1109/JDT.2015.2453252
   Pan ZQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159170
   Pham CDK, 2019, IEEE ACCESS, V7, P112535, DOI 10.1109/ACCESS.2019.2935378
   Pokhrel Shiva Raj, 2020, DroneCom '20: Proceedings of the 2nd MobiCom Workshop on Drone Assisted Wireless Communications for 5G and Beyond, P49, DOI 10.1145/3414045.3415949
   Romano Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2576402
   Shechtman E, 2007, PROC CVPR IEEE, P1744
   Song W, 2018, IEEE IMAGE PROC, P1478, DOI 10.1109/ICIP.2018.8451228
   Vranjes D, 2020, IEEE CONSUM ELECTR M, V9, P17, DOI 10.1109/MCE.2019.2956208
   Wu C, 2021, IEEE T CLOUD COMPUT, V9, P182, DOI 10.1109/TCC.2018.2858266
   Yoon SJ, 2018, IEEE T IMAGE PROCESS, V27, P5918, DOI 10.1109/TIP.2018.2861567
   Yunlong Zhao, 2019, 2019 7th International Conference on Information, Communication and Networks (ICICN), P158, DOI 10.1109/ICICN.2019.8834946
   Zhang CX, 2020, IEEE T MULTIMEDIA, V22, P349, DOI 10.1109/TMM.2019.2929934
   Zhang YB, 2020, IEEE T CIRC SYST VID, V30, P11, DOI 10.1109/TCSVT.2018.2885564
   Zhang YB, 2010, IEEE T IMAGE PROCESS, V19, P1248, DOI 10.1109/TIP.2009.2039055
NR 32
TC 0
Z9 0
U1 3
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 133
DI 10.1145/3555313
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800001
DA 2024-07-18
ER

PT J
AU Xiao, HC
   Zhao, WL
   Lin, J
   Hong, YG
   Ngo, CW
AF Xiao, Hui-Chu
   Zhao, Wan-Lei
   Lin, Jie
   Hong, Yi-Geng
   Ngo, Chong-Wah
TI Deeply Activated Salient Region for Instance Search
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Instance search; back-propagation; response peak; instance-level
AB The performance of instance search relies heavily on the ability to locate and describe a wide variety of object instances in a video/image collection. Due to the lack of a proper mechanism for locating instances and deriving feature representation, instance search is generally only effective when the instances are from known object categories. In this article, a simple but effective instance-level feature representation approach is presented. Different from the existing approaches, the issues of class-agnostic instance localization and distinctive feature representation are considered. The former is achieved by detecting salient instance regions from an image by a layer-wise back-propagation process. The back-propagation starts from the last convolution layer of a pre-trained CNNs that is originally used for classification. The back-propagation proceeds layer by layer until it reaches the input layer. This allows the salient instance regions in the input image from both known and unknown categories to be activated. Each activated salient region covers the full or, more usually, a major range of an instance. The distinctive feature representation is produced by average-pooling on the feature map of a certain layer with the detected instance region. Experiments show that this kind of feature representation demonstrates considerably better performance than most of the existing approaches.
C1 [Xiao, Hui-Chu; Zhao, Wan-Lei; Lin, Jie; Hong, Yi-Geng] Xiamen Univ, Haiyun Campus, Xiamen 361005, Fujian, Peoples R China.
   [Ngo, Chong-Wah] Singapore Management Univ, Singapore, Singapore.
C3 Xiamen University; Singapore Management University
RP Zhao, WL (corresponding author), Xiamen Univ, Haiyun Campus, Xiamen 361005, Fujian, Peoples R China.
EM hcxiao@stu.xmu.edu.cn; wlzhao@xmu.edu.cn; linjie037@stu.xmu.edu.cn;
   yghong@stu.xmu.edu.cn; cwngo@smu.edu.sg
FU National Natural Science Foundation of China [61572408, 61972326];
   Xiamen University [20720180074]
FX This work was supported by National Natural Science Foundation of China
   under grants 61572408 and 61972326, and the grants of Xiamen University
   20720180074.
CR Awad G, 2017, INT J MULTIMED INF R, V6, P1, DOI 10.1007/s13735-017-0121-3
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Douze M, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646421
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong YG, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3807, DOI 10.1145/3474085.3475530
   Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Iscen A, 2017, PROC CVPR IEEE, P926, DOI 10.1109/CVPR.2017.105
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jimenez Albert, 2017, P BRIT MACHINE VISIO
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Kim J., 2018, BMVC, P209
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Lin J, 2021, NEUROCOMPUTING, V424, P117, DOI 10.1016/j.neucom.2019.11.029
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maron O, 1998, ADV NEUR IN, V10, P570
   Mohedano E., 2018, In: 2018 International Conference on Content-Based Multimedia Indexing (CBMI), P1
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Pan JT, 2018, Arxiv, DOI arXiv:1701.01081
   Paulin M, 2015, IEEE I CONF COMP VIS, P91, DOI 10.1109/ICCV.2015.19
   Razavian A. S., 2016, ITE Trans. Media Technol. Appl., V4, P251, DOI [DOI 10.3169/MTA.4.251, 10.3169/mta.4.251]
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Salvador A, 2016, IEEE COMPUT SOC CONF, P394, DOI 10.1109/CVPRW.2016.56
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Tolias G, 2016, Arxiv, DOI [arXiv:1511.05879, DOI 10.48550/ARXIV.1511.05879]
   Wan F, 2019, PROC CVPR IEEE, P2194, DOI 10.1109/CVPR.2019.00230
   Wang S, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700292
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhan Y, 2019, Arxiv, DOI arXiv:1806.03576
   Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou YZ, 2018, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2018.00399
NR 46
TC 3
Z9 3
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 147
DI 10.1145/3510004
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, XM
   Wang, S
   Zhang, Y
   Yuan, Q
AF Liu, Xiaoming
   Wang, Shuo
   Zhang, Ying
   Yuan, Quan
TI Scribble-Supervised Meibomian Glands Segmentation in Infrared Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Infrared imaging; scribble-supervised; transformation consistency;
   meibomian gland segmentation
ID FLUID SEGMENTATION; MEIBOGRAPHY
AB Infrared imaging is currently the most effective clinical method to evaluate the morphology of the meibomian glands (MGs) in patients. As an important indicator for monitoring the development of MG dysfunction, it is necessary to accurately measure gland-drop and gland morphology. Although there are existing methods for automatic segmentation of MGs using deep learning frameworks, they require fully annotated ground-truth labels for training, which is time-consuming and laborious. In this article, we proposed a new scribble-supervised deep learning framework for segmenting the MGs, which only requires easily attainable scribble annotations for training. To cope with the shortage of supervision and regularize the network, a transformation consistent strategy is incorporated, which requires the prediction to follow the same transformation if a transform is performed on an input image of the network. The proposed segmentation method consists of two stages. In the first stage, a U-Net network is used to obtain the meibomian region segmentation map. In the second stage, we concentrate on segmenting glands in the meibomian region. We utilize the gradient prior information of the original image at the decoder part of the segmentation network, which can coarsely locate the target contour. We automatically generate reliable labels using the exponential moving average of the predictions during training and filter out the unreliable pseudo-label by uncertainty threshold. Experimental results on a local MG dataset and two other public medical image datasets demonstrate the effectiveness of the proposed segmentation framework.
C1 [Liu, Xiaoming] Wuhan Univ Sci & Technol, Hubei Prov Key Lab Intelligent Informat Proc & Re, Wuhan, Peoples R China.
   [Liu, Xiaoming; Wang, Shuo; Yuan, Quan] Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, 2 Huangjiahu West Rd, Wuhan 430070, Peoples R China.
   [Zhang, Ying] Wuhan Aier Eye Hosp, 481 Zhongshan Rd, Wuhan 430064, Peoples R China.
   [Liu, Xiaoming] Hubei Prov Key Lab Intelligent Informat Proc & Re, Wuhan, Peoples R China.
C3 Wuhan University of Science & Technology; Wuhan University of Science &
   Technology
RP Liu, XM (corresponding author), Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, 2 Huangjiahu West Rd, Wuhan 430070, Peoples R China.
EM lxmspace@gmail.com
RI yuan, quan/GZM-5597-2022; Liu, Xiaoming/AFF-8698-2022
OI Liu, Xiaoming/0000-0003-3467-5607
FU National Natural Science Foundation of China [62176190]; Research
   Project of Hubei Health Committee [WJ2021F017]
FX This work was supported in part by the National Natural Science
   Foundation of China under grant no. 62176190 and the Research Project of
   Hubei Health Committee under grant no. WJ2021F017.
CR Arita R, 2008, OPHTHALMOLOGY, V115, P911, DOI 10.1016/j.ophtha.2007.06.031
   Arita R, 2014, BRIT J OPHTHALMOL, V98, P746, DOI 10.1136/bjophthalmol-2012-303014
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bearman Amy, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Chen XK, 2021, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR46437.2021.00264
   Devalla SK, 2018, BIOMED OPT EXPRESS, V9, P3244, DOI 10.1364/BOE.9.003244
   Driver PJ, 1996, SURV OPHTHALMOL, V40, P343, DOI 10.1016/S0039-6257(96)80064-6
   Foulks Gary N, 2003, Ocul Surf, V1, P107
   Harada D, 2019, IEEE ENG MED BIO, P1026, DOI [10.1109/EMBC.2019.8856465, 10.1109/embc.2019.8856465]
   Hassanzadeh S, 2021, OCUL IMMUNOL INFLAMM, V29, P66, DOI 10.1080/09273948.2020.1755441
   He JJ, 2021, APPL SOFT COMPUT, V113, DOI 10.1016/j.asoc.2021.107947
   Hyeonsoo Lee, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P14, DOI 10.1007/978-3-030-59710-8_2
   Khan ZK, 2021, BMJ OPEN OPHTHALMOL, V6, DOI 10.1136/bmjophth-2020-000436
   Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181
   Kingma D. P., 2014, arXiv
   Knop E, 2011, INVEST OPHTH VIS SCI, V52, P1938, DOI 10.1167/iovs.10-6997c
   Ko TH, 2004, OPHTHALMOLOGY, V111, P2033, DOI 10.1016/j.ophtha.2004.05.021
   Koh YW, 2012, J BIOMED OPT, V17, DOI 10.1117/1.JBO.17.8.086008
   Kumar N, 2017, IEEE T MED IMAGING, V36, P1550, DOI 10.1109/TMI.2017.2677499
   Laradji IH, 2018, LECT NOTES COMPUT SC, V11206, P560, DOI 10.1007/978-3-030-01216-8_34
   Lee S, 2021, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR46437.2021.00545
   Li XM, 2021, IEEE T NEUR NET LEAR, V32, P523, DOI 10.1109/TNNLS.2020.2995319
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin F, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3440694
   Liu F, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3125976
   Liu XM, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108341
   Liu XM, 2021, NEUROCOMPUTING, V452, P576, DOI 10.1016/j.neucom.2020.07.143
   Liu XM, 2021, COMPUT BIOL MED, V135, DOI 10.1016/j.compbiomed.2021.104543
   Liu XM, 2020, IEEE J-STSP, V14, P1176, DOI 10.1109/JSTSP.2020.3013418
   Liu XM, 2019, INT J MACH LEARN CYB, V10, P155, DOI 10.1007/s13042-017-0706-4
   Liu XM, 2019, IEEE J BIOMED HEALTH, V23, P1404, DOI [10.1109/JBHI.2018.2856276, 10.1145/3297156.3297158]
   Ljosa V, 2012, NAT METHODS, V9, P637, DOI 10.1038/nmeth.2083
   Marcet MM, 2015, OPHTHALMOLOGY, V122, P1681, DOI 10.1016/j.ophtha.2015.04.034
   Nguyen, 2020, 2020 INT C LEARN REP
   Nichols JJ, 2005, CORNEA, V24, P382, DOI 10.1097/01.ico.0000148291.38076.59
   Pflugfelder SC, 1998, CORNEA, V17, P38, DOI 10.1097/00003226-199801000-00007
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   Prabhu SM, 2020, BIOMED SIGNAL PROCES, V57, DOI 10.1016/j.bspc.2019.101776
   Pult H, 2013, CONTACT LENS ANTERIO, V36, P22, DOI 10.1016/j.clae.2012.10.074
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy AG, 2017, BIOMED OPT EXPRESS, V8, P3627, DOI 10.1364/BOE.8.003627
   Sahasrabudhe Mihir, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P393, DOI 10.1007/978-3-030-59722-1_38
   Saleh F, 2016, LECT NOTES COMPUT SC, V9912, P413, DOI 10.1007/978-3-319-46484-8_25
   Schlegl T, 2019, MED IMAGE ANAL, V54, P30, DOI 10.1016/j.media.2019.01.010
   Tang M, 2018, LECT NOTES COMPUT SC, V11220, P524, DOI 10.1007/978-3-030-01270-0_31
   Tarvainen A, 2017, ADV NEUR IN, V30
   Villani E, 2020, OCUL SURF, V18, P871, DOI 10.1016/j.jtos.2020.09.001
   Wang B, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3663
   Wang JY, 2019, TRANSL VIS SCI TECHN, V8, DOI 10.1167/tvst.8.6.37
   Wang Y., 2020, 2020 IEEE CVF C COMP
   Wong S, 2019, CONTACT LENS ANTERIO, V42, P311, DOI 10.1016/j.clae.2018.10.014
   Yang ZZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446618
   Yu LQ, 2019, LECT NOTES COMPUT SC, V11765, P605, DOI 10.1007/978-3-030-32245-8_67
   Yushkevich PA, 2016, IEEE ENG MED BIO, P3342, DOI 10.1109/EMBC.2016.7591443
   Zhang J., 2020, 2020 IEEECVF C COMPU
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 56
TC 1
Z9 1
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 88
DI 10.1145/3497747
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600022
DA 2024-07-18
ER

PT J
AU Khan, A
   Ul Haq, I
   Hussain, T
   Muhammad, K
   Hijji, M
   Sajjad, M
   De Albuquerque, VHC
   Baik, SW
AF Khan, Abbas
   Ul Haq, Ijaz
   Hussain, Tanveer
   Muhammad, Khan
   Hijji, Mohammad
   Sajjad, Muhammad
   De Albuquerque, Victor Hugo C.
   Baik, Sung Wook
TI PMAL: A Proxy Model Active Learning Approach for Vision Based Industrial
   Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Active learning; convolution neural networks; data analytics; image
   classification; proxy model; patterns matching; uncertainty sampling;
   vision transformer
AB Deep Learning models' performance strongly correlate with availability of annotated data; however, massive data labeling is laborious, expensive, and error-prone when performed by human experts. Active Learning (AL) effectively handles this challenge by selecting the uncertain samples from unlabeled data collection, but the existing AL approaches involve repetitive human feedback for labeling uncertain samples, thus rendering these techniques infeasible to be deployed in industry related real-world applications. In the proposed Proxy Model based Active Learning technique (PMAL), this issue is addressed by replacing human oracle with a deep learning model, where human expertise is reduced to label only two small subsets of data for training proxy model and initializing the AL loop. In the PMAL technique, firstly, proxy model is trained with a small subset of labeled data, which subsequently acts as an oracle for annotating uncertain samples. Secondly, active model's training, uncertain samples extraction via uncertainty sampling, and annotation through proxy model is carried out until predefined iterations to achieve higher accuracy and labeled data. Finally, the active model is evaluated using testing data to verify the effectiveness of our technique for practical applications. The correct annotations by the proxy model are ensured by employing the potentials of explainable artificial intelligence. Similarly, emerging vision transformer is used as an active model to achieve maximum accuracy. Experimental results reveal that the proposed method outperforms the state-of-the-art in terms of minimum labeled data usage and improves the accuracy with 2.2%, 2.6%, and 1.35% on Caltech-101, Caltech-256, and CIFAR-10 datasets, respectively. Since the proposed technique offers a highly reasonable solution to exploit huge multimedia data, it can be widely used in different evolutionary industrial domains.
C1 [Khan, Abbas; Ul Haq, Ijaz; Hussain, Tanveer; Baik, Sung Wook] Sejong Univ, Dept Software Convergence, 209 Neungdong Ro, Seoul 05006, South Korea.
   [Muhammad, Khan] Sungkyunkwan Univ, Coll Comp & Informat, Sch Convergence, Dept Appl Artificial Intelligence,Visual Analyt K, Seoul 03063, South Korea.
   [Hijji, Mohammad] Univ Tabuk, Fac Comp & Informat Technol, Comp Sci Dept, Tabuk 47711, Saudi Arabia.
   [Sajjad, Muhammad] Islamia Coll Peshawar, Dept Comp Sci, Digital Image Proc Lab, Peshawar 25000, Pakistan.
   [De Albuquerque, Victor Hugo C.] Univ Fed Ceara, Dept Teleinformat Engn, BR-60020181 Fortaleza, Ceara, Brazil.
C3 Sejong University; Sungkyunkwan University (SKKU); University of Tabuk;
   University of Peshawar; Universidade Federal do Ceara
RP Baik, SW (corresponding author), Sejong Univ, Dept Software Convergence, 209 Neungdong Ro, Seoul 05006, South Korea.
RI Khan, Muhammad/IXN-8470-2023; de Albuquerque, Victor Hugo
   C./C-3677-2016; Asif, Muhammad/KHD-0023-2024; Sajjad,
   Muhammad/L-5269-2016
OI de Albuquerque, Victor Hugo C./0000-0003-3886-4309; Sajjad,
   Muhammad/0000-0001-5646-0338; Hussain, Tanveer/0000-0003-4861-8347; ,
   Abbas/0000-0002-7551-6082; Muhammad, Khan/0000-0002-5302-1150; Haq, Ijaz
   Ul/0000-0001-8201-7372
FU Institute of Information & Communications Technology Planning &
   Evaluation (IITP) - Korea government (MSIT) [2020-0-00062]
FX This work was supported by the Institute of Information & Communications
   Technology Planning & Evaluation (IITP) grant funded by the Korea
   government (MSIT) [No. 2020-0-00062, Project Name: Development of data
   augmentation technology by using heterogeneous data and external data
   integration].
CR Agrawal A, 2021, PROG ARTIF INTELL, V10, P113, DOI 10.1007/s13748-021-00230-w
   Alizadeh A., 2021, Advances in Parallel Distributed Processing, P609
   Aloufi S., 2022, ACMTRANSACTIONS ONMU, V18, P1
   Brinker K., 2003, P 20 INT C MACHINE L, P59
   C. Education, 2021, DATA ENG PREPARATION
   Cheng Y, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1311
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Ducoffe M, 2018, Arxiv, DOI arXiv:1802.09841
   Ebrahimi S, 2021, Arxiv, DOI arXiv:2012.10467
   Elhamifar E, 2013, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2013.33
   Fredriksson T, 2020, LECT NOTES COMPUT SC, V12562, P202, DOI 10.1007/978-3-030-64148-1_13
   Fujii K, 2016, ADV NEUR IN, V29
   Grand View Research, 2021, DATA COLLECTION LABE
   Griffin G., 2007, CALTECH 256 OBJECT C
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kee S, 2018, INFORM SCIENCES, V454, P401, DOI 10.1016/j.ins.2018.05.014
   Khan S.H., 2021, arXiv
   Kim K, 2021, PROC CVPR IEEE, P8162, DOI 10.1109/CVPR46437.2021.00807
   Kizilkaya B, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473037
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Kumar P, 2020, J COMPUT SCI TECH-CH, V35, P913, DOI 10.1007/s11390-020-9487-4
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li X, 2013, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2013.116
   Lu HD, 2021, IEEE NETWORK, V35, P83, DOI 10.1109/MNET.201.2100069
   Mayer C, 2020, IEEE WINT CONF APPL, P3060, DOI [10.1109/WACV45572.2020.9093556, 10.1109/wacv45572.2020.9093556]
   Muhammad K, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3444693
   Pawan Kumar M., 2010, NIPS
   Prathiba SB, 2021, IEEE T VEH TECHNOL, V70, P13340, DOI 10.1109/TVT.2021.3122257
   Ren PZ, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3472291
   Ripley BD, 1996, PATTERN RECOGNITION
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroder Christopher., Revisiting uncertainty-based query strategies for active learning with transformers
   Sener O, 2018, Arxiv, DOI arXiv:1708.00489
   Settles B., 2009, ACTIVE LEARNING LIT, DOI DOI 10.2200/S00429ED1V01Y201207AIM018
   Shahid H, 2021, ACM T INTERNET TECHN, V21, DOI 10.1145/3418204
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh A, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3415155
   Smailovic J, 2014, INFORM SCIENCES, V285, P181, DOI 10.1016/j.ins.2014.04.034
   Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7_27
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang XCA, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3454009
   Tanveer M., 2020, INTRO SPECIAL ISSUE, V16, P1
   Tran T, 2019, PR MACH LEARN RES, V97
   Tuia D, 2009, IEEE T GEOSCI REMOTE, V47, P2218, DOI 10.1109/TGRS.2008.2010404
   Ul Haq I, 2021, INFORM FUSION, V76, P24, DOI 10.1016/j.inffus.2021.04.016
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3447530
   Cho JW, 2022, Arxiv, DOI arXiv:2107.11049
   Wu ZF, 2019, PATTERN RECOGN, V90, P119, DOI 10.1016/j.patcog.2019.01.006
   Xu CH, 2020, IEEE T NETW SCI ENG, V7, P104, DOI 10.1109/TNSE.2018.2843326
   Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x
   Yin TX, 2021, NEUROCOMPUTING, V424, P1, DOI 10.1016/j.neucom.2020.11.019
   Zhang B, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3449359
NR 54
TC 3
Z9 4
U1 5
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 123
DI 10.1145/3534932
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000010
OA Bronze
DA 2024-07-18
ER

PT J
AU Luo, DZ
   Zhou, Y
   Fang, B
   Zhou, YC
   Wu, DY
   Wang, WP
AF Luo, Dezhao
   Zhou, Yu
   Fang, Bo
   Zhou, Yucan
   Wu, Dayan
   Wang, Weiping
TI Exploring Relations in Untrimmed Videos for Self-Supervised Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Self-supervised learning; action recognition; action detection
AB Existing video self-supervised learning methods mainly rely on trinuned videos for model training. They apply their methods and verify the effectiveness on trimmed video datasets including UCF101 and Kinetics-400, among others. However, trimmed datasets are manually annotated from untrimmed videos. In this sense, these methods are not truly unsupervised. In this article, we propose a novel self-supervised method, referred to as Exploring Relations in Untrimmed Videos (ERUV), which can be straightforwardly applied to untrimmed videos (real unlabeled) to learn spatio-temporal features. ERUV first generates single-shot videos by shot change detection. After that, some designed sampling strategies are used to model relations for video clips. The strategies are saved as our self-supervision signals. Finally, the network learns representations by predicting the category of relations between the video clips. ERUV is able to compare the differences and similarities of video clips, which is also an essential procedure for video-related tasks. We validate our learned models with action recognition, video retrieval, and action similarity labeling tasks with four kinds of 3D convolutional neural networks. Experimental results show that ERUV is able to learn richer representations with untrimmed videos, and it outperforms state-of-the-art self-supervised methods with significant margins.
C1 [Luo, Dezhao; Zhou, Yu; Fang, Bo; Zhou, Yucan; Wu, Dayan; Wang, Weiping] Chinese Acad Sci, Inst Informat Engn, A 89,Minzhuang Rd, Beijing 100093, Peoples R China.
   [Luo, Dezhao; Zhou, Yu; Fang, Bo] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Zhou, Y (corresponding author), Chinese Acad Sci, Inst Informat Engn, A 89,Minzhuang Rd, Beijing 100093, Peoples R China.; Zhou, Y (corresponding author), Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China.
EM luodezhao@iie.ac.cn; zhouyu@iie.ac.cn; fangbo@iie.ac.cn;
   zhouyucan@iie.ac.cn; wudayan@iie.ac.cn; wangweiping@iie.ac.cn
RI Guo, Li/KCK-9540-2024; liu, xq/JDW-2596-2023; wu, dayan/HHZ-8311-2022
OI , Fang/0000-0002-5475-8840; ZHOU, Yu/0000-0003-4188-9953
FU Open Research Project of the State Key Laboratory of Media Convergence
   and Communication, Communication University of China, China
   [SKLMCC2020KF004]; Beijing Municipal Science & Technology Commission
   [Z191100007119002]; Key Research Program of Frontier Sciences, CAS
   [ZDBS-LY-7024]; National Natural Science Foundation of China [62006221]
FX This work was supported by the Open Research Project of the State Key
   Laboratory of Media Convergence and Communication, Communication
   University of China, China (No. SKLMCC2020KF004), the Beijing Municipal
   Science & Technology Commission (Z191100007119002), the Key Research
   Program of Frontier Sciences, CAS, Grant No ZDBS-LY-7024, the National
   Natural Science Foundation of China (No. 62006221).
CR [Anonymous], 2012, CoRR
   Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Benaim S., 2020, CVPR, P9919
   Büchler U, 2018, LECT NOTES COMPUT SC, V11219, P797, DOI 10.1007/978-3-030-01267-0_47
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen YD, 2021, INT C PATT RECOG, P850, DOI 10.1109/ICPR48806.2021.9412258
   Chen YD, 2019, LECT NOTES ARTIF INT, V11672, P137, DOI 10.1007/978-3-030-29894-4_11
   Chuang Gan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5589, DOI 10.1109/CVPR.2018.00586
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dongbao Yang, 2020, ARXIV PREPRINT ARXIV
   Dongbao Yang, 2021, ARXIV PREPRINT ARXIV
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607
   Gidaris S., 2018, P 6 INT C LEARNING R
   Han Tengda, 2020, ARXIV PREPRINT ARXIV
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kay W., 2017, ARXIV170506950
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Kliper-Gross O, 2012, IEEE T PATTERN ANAL, V34, P615, DOI 10.1109/TPAMI.2011.209
   Kolesnikov A, 2019, PROC CVPR IEEE, P1920, DOI 10.1109/CVPR.2019.00202
   Kong Quan, 2020, ARXIV PREPRINT ARXIV, V2020
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Lee W, 2019, PROC CVPR IEEE, P4979, DOI 10.1109/CVPR.2019.00512
   Li XM, 2022, COMPLEX INTELL SYST, V8, P851, DOI 10.1007/s40747-021-00448-0
   Luo DZ, 2020, AAAI CONF ARTIF INTE, V34, P11701
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pan Tian, 2021, PROC CVPR IEEE, DOI DOI 10.1109/CVPR46437.2021.01105
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Piergiovanni AJ, 2019, PROC CVPR IEEE, P9937, DOI 10.1109/CVPR.2019.01018
   Qian Rui, 2020, ARXIV PREPRINT ARXIV
   Qiao Z, 2021, INT C PATT RECOG, P3328, DOI 10.1109/ICPR48806.2021.9412806
   Qin XG, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4350, DOI 10.1109/ICASSP39728.2021.9413821
   Qin Xugong, 2019, ICDAR, P559
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Tian Y., 2018, ARXIV PREPRINT ARXIV
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vondrick C, 2016, 30 C NEURAL INFORM P, V29
   Wang JL, 2019, PROC CVPR IEEE, P4001, DOI 10.1109/CVPR.2019.00413
   Wang Jiangliu, 2020, ECCV, P504, DOI DOI 10.1007/978-3-030-58520-430
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P1316, DOI 10.1109/TMM.2020.2995290
   Wei DL, 2018, PROC CVPR IEEE, P8052, DOI 10.1109/CVPR.2018.00840
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058
   Yao Y, 2020, PROC CVPR IEEE, P6547, DOI 10.1109/CVPR42600.2020.00658
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297
   Zhang YF, 2021, INT C PATT RECOG, P8476, DOI 10.1109/ICPR48806.2021.9412301
   Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2
   Zhao YR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1933, DOI 10.1145/3123266.3123451
   Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354
   Zhong BN, 2019, IEEE T IMAGE PROCESS, V28, P2331, DOI 10.1109/TIP.2018.2885238
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou QQ, 2020, IEEE T IMAGE PROCESS, V29, P7578, DOI 10.1109/TIP.2020.3004267
   Zhou Y, 2020, ARXIV200508891
NR 72
TC 8
Z9 8
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 35
DI 10.1145/3473342
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, YY
   Jhong, SY
   Hsia, CH
   Hua, KL
AF Chen, Yung-Yao
   Jhong, Sin-Ye
   Hsia, Chih-Hsien
   Hua, Kai-Lung
TI Explainable AI: A Multispectral Palm-Vein Identification System with New
   Augmentation Features
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Principal component analysis on symmetric discrete wavelet transform;
   explainable AI; convolutional neural networks; palm-vein identification
ID RECOGNITION; PATTERNS
AB Recently, as one of the most promising biometric traits, the vein has attracted the attention of both academia and industry because of its living body identification and the convenience of the acquisition process. State-of-the-art techniques can provide relatively good performance, yet they are limited to specific light sources. Besides, it still has poor adaptability to multispectral images. Despite the great success achieved by convolutional neural networks (CNNs) in various image understanding tasks, they often require large training samples and high computation that are infeasible for palm-vein identification. To address this limitation, this work proposes a palm-vein identification system based on lightweight CNN and adaptive multi-spectral method with explainable AI. The principal component analysis on symmetric discrete wavelet transform (SMDWT-PCA) technique for vein images augmentation method is adopted to solve the problem of insufficient data and multispectral adaptability. The depth separable convolution (DSC) has been applied to reduce the number of model parameters in this work. To ensure that the experimental result demonstrates accurately and robustly, a multispectral palm image of the public dataset (CASIA) is also used to assess the performance of the proposed method. As result, the palm-vein identification system can provide superior performance to that of the former related approaches for different spectrums.
C1 [Chen, Yung-Yao] Natl Taiwan Univ Sci & Technol, Dept Elect & Comp Engn, 43,Sec 4,Keelung Rd, Taipei 106, Taiwan.
   [Jhong, Sin-Ye] Natl Cheng Kung Univ, Dept Engn Sci, 1 Dasyue Rd, Tainan 701, Taiwan.
   [Hsia, Chih-Hsien] Natl Ilan Univ, Dept Comp Sci & Informat Engn, 1,Sec 1,Shennong Rd, Yilan 260, Yilan County, Taiwan.
   [Hua, Kai-Lung] Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, 43,Sec 4,Keelung Rd, Taipei 106, Taiwan.
C3 National Taiwan University of Science & Technology; National Cheng Kung
   University; National Ilan University; National Taiwan University of
   Science & Technology
RP Hsia, CH (corresponding author), Natl Ilan Univ, Dept Comp Sci & Informat Engn, 1,Sec 1,Shennong Rd, Yilan 260, Yilan County, Taiwan.
EM yungyaochen@mail.ntust.edu.tw; a888999a100@gmail.com;
   chhsia625@gmail.com; hua@mail.ntust.edu.tw
RI Chen, Yung-Yao/IQU-8095-2023
OI Chen, Yung-Yao/0000-0001-6852-8862; Hua, Kai-Lung/0000-0002-7735-243X;
   Hsia, Chih-Hsien/0000-0003-2665-0821
CR Ahmad Radzi S, 2016, TURK J ELECTR ENG CO, V24, P1863, DOI 10.3906/elk-1311-43
   [Anonymous], 2015, 3 INT C LEARN REPR
   Arora P, 2019, IEEE INTELL SYST, V34, P25, DOI 10.1109/MIS.2018.2881494
   Bhilare S, 2018, MACH VISION APPL, V29, P1269, DOI 10.1007/s00138-018-0959-2
   Bontrager P, 2018, INT CONF BIOMETR THE
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Das R, 2019, IEEE T INF FOREN SEC, V14, P360, DOI 10.1109/TIFS.2018.2850320
   Dong WG, 2017, 2017 32ND YOUTH ACADEMIC ANNUAL CONFERENCE OF CHINESE ASSOCIATION OF AUTOMATION (YAC), P1154, DOI 10.1109/YAC.2017.7967586
   Fang YX, 2018, NEUROCOMPUTING, V290, P100, DOI 10.1016/j.neucom.2018.02.042
   Farmanbar M, 2017, SIGNAL IMAGE VIDEO P, V11, P1253, DOI 10.1007/s11760-017-1082-y
   Guo JM, 2012, EXPERT SYST APPL, V39, P11728, DOI 10.1016/j.eswa.2012.04.081
   Hengyi Zhang, 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P77, DOI 10.1109/BTAS.2012.6374560
   Hollingsworth KP, 2011, IEEE T PATTERN ANAL, V33, P2465, DOI 10.1109/TPAMI.2011.89
   Hong HG, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17061297
   Hsia CH, 2018, IEEE SENS J, V18, P790, DOI 10.1109/JSEN.2017.2772799
   Jhong S-Y., 2020, PROC INT C ADV ROBOT, P1, DOI DOI 10.1109/ARIS50834.2020
   Kang WX, 2014, IEEE T INF FOREN SEC, V9, P1974, DOI 10.1109/TIFS.2014.2361020
   Kang WX, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0097548
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar A, 2012, IEEE T IMAGE PROCESS, V21, P2228, DOI 10.1109/TIP.2011.2171697
   Li X, 2019, PROC CVPR IEEE, P2677, DOI 10.1109/CVPR.2019.00279
   Li Xueyan, 2007, 2007 1st International Conference on Bioinformatics and Biomedical Engineering, P612, DOI 10.1109/ICBBE.2007.160
   Lin Y.C., 2012, P 20 ACM INT C MULT, P1053
   Liu YF, 2014, IEEE T INF FOREN SEC, V9, P1953, DOI 10.1109/TIFS.2014.2355495
   Menotti David, 2015, IEEE Transactions on Information Forensics and Security, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Mirmohamadsadeghi L, 2014, IET BIOMETRICS, V3, P198, DOI 10.1049/iet-bmt.2013.0041
   Miura N, 2004, MACH VISION APPL, V15, P194, DOI 10.1007/s00138-004-0149-2
   Piciucco E, 2018, IET BIOMETRICS, V7, P439, DOI 10.1049/iet-bmt.2017.0192
   Prabhakar S, 2003, PATTERN RECOGN, V36, P1847, DOI 10.1016/S0031-3203(02)00322-9
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Sanchez-Riera J, 2018, IEEE T CIRC SYST VID, V28, P2289, DOI 10.1109/TCSVT.2017.2718622
   Simard PY, 2003, PROC INT CONF DOC, P958
   Song JM, 2019, IEEE ACCESS, V7, P66845, DOI 10.1109/ACCESS.2019.2918503
   Struc V, 2009, IET SIGNAL PROCESS, V3, P258, DOI 10.1049/iet-spr.2008.0152
   Suarez Pascual J. Enrique, 2010, Proceedings of the 2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIHMSP 2010), P318, DOI 10.1109/IIHMSP.2010.85
   Thapar D., 2019, 2019 IEEE 5th International Conference on Identity, Security, and Behavior Analysis (ISBA), P1
   Tirunagari S, 2015, IEEE T INF FOREN SEC, V10, P762, DOI 10.1109/TIFS.2015.2406533
   Toygar O, 2020, IEEE ACCESS, V8, P82461, DOI 10.1109/ACCESS.2020.2991475
   Wang GQ, 2017, COMPUT MATH METHOD M, V2017, DOI 10.1155/2017/2373818
   Wang J, 2018, IEEE ACCESS, V6, P28563, DOI 10.1109/ACCESS.2018.2839720
   Wu JD, 2009, EXPERT SYST APPL, V36, P5793, DOI 10.1016/j.eswa.2008.07.042
   Wu W, 2019, IET BIOMETRICS, V8, P206, DOI 10.1049/iet-bmt.2018.5027
   Xie CH, 2017, ADV COMPUT VIS PATT, P109, DOI 10.1007/978-3-319-61657-5_5
   Xie H.-X., 2020, P 28 ACM INT C MULT, P2871
   Zhou YB, 2011, IEEE T INF FOREN SEC, V6, P1259, DOI 10.1109/TIFS.2011.2158423
NR 46
TC 10
Z9 10
U1 4
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 111
DI 10.1145/3468873
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600013
DA 2024-07-18
ER

PT J
AU Li, JG
   Zhang, XF
   Xu, JZ
   Ma, SW
   Gao, W
AF Li, Jiguo
   Zhang, Xinfeng
   Xu, Jizheng
   Ma, Siwei
   Gao, Wen
TI Learning to Fool the Speaker Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Audio forensics; adversarial attack; deep neural network
AB Due to the widespread deployment of fingerprint/face/speaker recognition systems, the risk in these systems, especially the adversarial attack, has drawn increasing attention in recent years. Previous researches mainly studied the adversarial attack to the vision-based systems, such as fingerprint and face recognition. While the attack for speech-based systems has not been well studied yet, although it has been widely used in our daily life. In this article, we attempt to fool the state-of-the-art speaker recognition model and present speaker recognition attacker, a lightweight multi-layer convolutional neural network to fool the well-trained state-of-the-art speaker recognition model by adding imperceptible perturbations onto the raw speech waveform. We find that the speaker recognition system is vulnerable to the adversarial attack, and achieve a high success rate on both the non-targeted attack and targeted attack. Besides, we present an effective method by leveraging a pretrained phoneme recognition model to optimize the speaker recognition attacker to obtain a tradeoff between the attack success rate and the perceptual quality. Experimental results on the TIMIT and LibriSpeech datasets demonstrate the effectiveness and efficiency of our proposed model. And the experiments for frequency analysis indicate that high-frequency attack is more effective than low-frequency attack, which is different from the conclusion drawn in previous image-based works. Additionally, the ablation study gives more insights into our model.
C1 [Li, Jiguo] Chinese Acad Sci, Inst Comp Technol, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.
   [Li, Jiguo] Univ Chinese Acad Sci, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.
   [Zhang, Xinfeng] Univ Chinese Acad Sci, 19A Yuquan Rd, Beijing 100049, Peoples R China.
   [Xu, Jizheng] Bytedance Inc, 48A Zhichun Rd, Beijing 100191, Peoples R China.
   [Ma, Siwei; Gao, Wen] Peking Univ, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Peking University
RP Ma, SW (corresponding author), Peking Univ, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
EM jiguo.li@vipl.ict.ac.cn; xfzhang@ucas.ac.cn; xujizheng@bytedance.com;
   swma@pku.edu.cn; wgao@pku.edu.cn
RI Xu, Jizheng/JDD-5152-2023; Zhang, Xinfeng/X-8148-2019
OI Li, Jiguo/0000-0002-1447-4798
FU National Science Foundation of China [62025101620, 61961130392];
   PKU-Baidu Fund [2019BD003]; High-performance Computing Platform of
   Peking University
FX This work was supported in part by the National Science Foundation of
   China under Grants 62025101620 and 61961130392, PKU-Baidu Fund 2019BD003
   and High-performance Computing Platform of Peking University.
CR Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Alzantot Moustafa, 2018, NIPS 2017 Machine Deception Workshop
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bai T, 2021, P 30 INT JOINT C ART, P4312, DOI [DOI 10.24963/IJCAI.2021/591, 10.24963/ijcai.2021/591]
   Baluja S, 2018, AAAI CONF ARTIF INTE, P2687
   Biggio B., 2012, ARXIV12066389, P1467
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Carlini N, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P513
   Chakraborty A., 2018, arXiv
   Dalvi N. N., 2004, P 10 ACM SIGKDD INT, P99, DOI DOI 10.1145/1014052.1014066
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   Diao W., 2014, P 4 ACM WORKSH SEC P, P63, DOI DOI 10.1145/2666620.2666623
   Ertas Figen, 2000, J ENG SCI, V2, P185
   Fletcher Roger., 2013, PRACTICAL METHODS OP, P33
   Garofolo John S, 1993, Tech.Rep. NISTIR 4930
   Gong Y., 2017, CoRR
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jung JW, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5349, DOI 10.1109/ICASSP.2018.8462575
   Kingma D. P., 2014, arXiv
   Korshunov P, 2016, 8 IEEE INT C BIOMETR
   Kreuk F, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1962, DOI 10.1109/ICASSP.2018.8462693
   Kurakin A., 2016, WORKSHOP TRACK P
   Li JG, 2020, INT CONF ACOUST SPEE, P2937, DOI [10.1109/ICASSP40776.2020.9053058, 10.1109/icassp40776.2020.9053058]
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17
   Muckenhirn H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4884, DOI 10.1109/ICASSP.2018.8462165
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Qin Y, 2019, PR MACH LEARN RES, V97
   Ravanelli M., 2018, INTERPRETABLE CONVOL
   Ravanelli M, 2019, INT CONF ACOUST SPEE, P6465, DOI [10.13140/rg.2.2.18985.44647, 10.1109/ICASSP.2019.8683713]
   Ravanelli M, 2018, IEEE W SP LANG TECH, P1021, DOI 10.1109/SLT.2018.8639585
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Rix AW, 2001, INT CONF ACOUST SPEE, P749, DOI 10.1109/ICASSP.2001.941023
   Schönherr L, 2019, 26TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2019), DOI 10.14722/ndss.2019.23288
   Sharma Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3389
   Sharma Yash, 2018, ARXIV181001268
   Snyder D, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5329
   Snyder D, 2016, IEEE W SP LANG TECH, P165, DOI 10.1109/SLT.2016.7846260
   Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858
   Szegedy C, 2014, INT C LEARN REPR
   Szurley Joseph, 2019, ARXIV190606355
   Vladu Adrian, 2018, PROC 6 INT C LEARN R
   Xu H, 2020, INT J AUTOM COMPUT, V17, P151, DOI 10.1007/s11633-019-1211-x
   Yuan XJ, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P49
   Zhang GM, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P103, DOI 10.1145/3133956.3134052
NR 47
TC 0
Z9 0
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 109
DI 10.1145/3468673
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU An, N
   Yan, WQ
AF An, Na
   Yan, Wei Qi
TI Multitarget Tracking Using Siamese Neural Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE SSD; SiamRPN; SiamFC; ResNet50; AlexNet
AB In this article, we detect and track visual objects by using Siamese network or twin neural network. The Siamese network is constructed to classify moving objects based on the associations of object detection network and object tracking network, which are thought of as the two branches of the twin neural network. The proposed tracking method was designed for single-target tracking, which implements multitarget tracking by using deep neural networks and object detection. The contributions of this article are stated as follows. First, we implement the proposed method for visual object tracking based on multiclass classification using deep neural networks. Then, we attain multitarget tracking by combining the object detection network and the single-target tracking network. Next, we uplift the tracking performance by fusing the outcomes of the object detection network and object tracking network. Finally, we speculate on the object occlusion problem based on IoU and similarity score, which effectively diminish the influence of this issue in multitarget tracking.
C1 [An, Na; Yan, Wei Qi] Auckland Univ Technol, 2-14 Wakefield St, Auckland 1010, New Zealand.
C3 Auckland University of Technology
RP An, N (corresponding author), Auckland Univ Technol, 2-14 Wakefield St, Auckland 1010, New Zealand.
EM jmj8905@autuni.ac.nz; wyan@aut.ac.nz
CR Abdel-Aziz AS, 2013, COMM COM INF SC, V381, P219
   An N., 2020, THESIS AUCKLAND U TE
   [Anonymous], 2015, Ua-detrac: A new benchmark and protocol for multi-object detection and tracking
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], 2000, SYSTEM VIDEO SURVEIL
   [Anonymous], 2010, P OD
   [Anonymous], 2018, P EUR C COMP VIS ECC
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2019, CVPR
   [Anonymous], 2018, P IEEE COMPUTER SOC
   [Anonymous], 2014, ARXIV14097618
   Bashir F Porikli F., 2006, P 9 IEEE INT WORKSHO, P7
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bochinski E, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P435
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chandan G., 2018, 2018 INT C INVENTIVE, P1305
   Chu P, 2019, IEEE I CONF COMP VIS, P6171, DOI 10.1109/ICCV.2019.00627
   Chu P, 2019, IEEE WINT CONF APPL, P161, DOI 10.1109/WACV.2019.00023
   Chu Q, 2017, IEEE I CONF COMP VIS, P4846, DOI 10.1109/ICCV.2017.518
   Cui SG, 2019, LECT NOTES COMPUT SC, V11954, P128, DOI 10.1007/978-3-030-36711-4_12
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Deutsch S. S., 2019, THESIS U POLITECNICA
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He Z, 2019, PROC CVPR IEEE, P1318, DOI 10.1109/CVPR.2019.00141
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Holland O, 2016, 2016 23RD INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), DOI 10.1109/ICT.2016.7500442
   Huang Z., 2019, P BICS, P96
   JONKER R, 1986, OPER RES LETT, V5, P171, DOI 10.1016/0167-6377(86)90073-8
   KEANE RD, 1992, APPL SCI RES, V49, P191, DOI 10.1007/BF00384623
   Kuncheva LI, 2010, INT J MACH LEARN CYB, V1, P53, DOI 10.1007/s13042-010-0002-z
   Lee M. C., 1999, U.S. Patent, Patent No. [5,970,173, 5970173]
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li D., 2019, P IEEE ICCE AS, P16
   Lin L, 2017, IEEE T PATTERN ANAL, V39, P1089, DOI 10.1109/TPAMI.2016.2567386
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lotter W., 2015, ARXIV151106380
   Milan A, 2017, AAAI CONF ARTIF INTE, P4225
   Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shafiee M.J., 2017, PREPRINT
   Shen C, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1942, DOI 10.1145/3123266.3123452
   Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394
   Varior Rahul Rama, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Yan C., 2020, ARXIV200200169
   Yan Wei., 2019, Introduction to intelligent surveillance: surveillance data capture, transmission, and analytics, DOI 10.1007/978-3-030-10713-0
   Yan Wei Qi, 2020, COMPUTATIONAL METHOD
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   Yoon Y.-C, 2019, ARXIV190700831
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang Yunhua, 2018, ARXIV180904320
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhang ZS, 2018, PROC CVPR IEEE, P5813, DOI 10.1109/CVPR.2018.00609
   Zheng LY, 2020, NEUROCOMPUTING, V401, P36, DOI 10.1016/j.neucom.2020.02.080
NR 59
TC 24
Z9 24
U1 9
U2 97
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 75
DI 10.1145/3441656
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100017
DA 2024-07-18
ER

PT J
AU Mythili, K
   Narwaria, M
AF Mythili, K.
   Narwaria, Manish
TI Assessment of Machine Learning-Based Audiovisual Quality Predictors: Why
   Uncertainty Matters
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Audiovisual quality; machine learning; uncertainty; validation
AB Quality assessment of signals is important from the perspective of system design, optimization, and management of a modern multimedia communication system. However, automatic prediction of AV quality via the use of computational models remains challenging. In this context, machine learning (ML) appears to be an attractive alternative to the traditional approaches. This is especially when such assessment needs to be made in no-reference (i.e., the original signal is unavailable) fashion. While development of ML-based quality predictors is desirable, we argue that proper assessment and validation of such predictors is also crucial before they can be deployed in practice. To this end, we raise some fundamental questions about the current approach of ML-based model development for AV quality assessment and signal processing for multimedia communication in general. We also identify specific limitations associated with the current validation strategy which have implications on analysis and comparison of ML-based quality predictors. These include a lack of consideration of: (a) data uncertainty, (b) domain knowledge, (c) explicit learning ability of the trained model, and (d) interpretability of the resultant model. Therefore, the primary goal of this article is to shed some light into mentioned factors. Our analysis and proposed recommendations are of particular importance in the light of significant interests in ML methods for multimedia signal processing (specifically in cases where human-labeled data is used), and a lack of discussion of mentioned issues in existing literature.
C1 [Mythili, K.] BITS Pilani, Hyderabad Campus, Hyderabad 500078, Telangana, India.
   [Narwaria, Manish] Indian Inst Technol Jodhpur, NH 65 Nagaur Rd, Jodhpur, Rajasthan, India.
C3 Birla Institute of Technology & Science Pilani (BITS Pilani); Indian
   Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Jodhpur
RP Mythili, K (corresponding author), BITS Pilani, Hyderabad Campus, Hyderabad 500078, Telangana, India.
EM mythilik9801@gmail.com; narwaria@iitj.ac.in
CR Akhtar Z, 2017, IEEE ACCESS, V5, P21090, DOI 10.1109/ACCESS.2017.2750918
   Martinez HAB, 2018, MULTIMED TOOLS APPL, V77, P23993, DOI 10.1007/s11042-018-5656-7
   Belmudez B, 2015, T-LAB SER TELECOMMUN, P1, DOI 10.1007/978-3-319-14166-4
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Carvalho DV, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8080832
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785
   Dagiuklas T, 2017, MULTIMED TOOLS APPL, V76, P22213, DOI 10.1007/s11042-017-5188-6
   Demirbilek E., 2016, Proceedings of the 24th ACM international conference on Multimedia, P167
   Demirbilek E, 2017, IEEE INT CON MULTI, P571, DOI 10.1109/ICME.2017.8019462
   Demirbilek E, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3051482
   Demirbilek Edip, 2018, ABS180105889 CORR
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1, DOI DOI 10.1007/978-0-387-84858-7
   Garcia MN, 2013, IEEE INT WORKSH MULT, P482, DOI 10.1109/MMSP.2013.6659336
   Garcia MN, 2013, INT WORK QUAL MULTIM, P194, DOI 10.1109/QoMEX.2013.6603236
   Garcia MN, 2011, EURASIP J IMAGE VIDE, DOI 10.1155/2011/629284
   Garcia Marie-Neige, 2016, PARAMETRIC PACKETBAS, V1st
   Garcia Marie-Neige, 2016, P 2016 IEEE INT C QU
   Gastaldo P, 2002, IEEE T NEURAL NETWOR, V13, P939, DOI 10.1109/TNN.2002.1021894
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hamam A, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540991
   ITU-T Recommendation G.1070, 2018, OP MOD VID TEL APPL
   ITU-T Recommendation G.1071, 2016, OP MOD NETW PLANN VI
   ITU-T Recommendation P.1201, 2012, PAR NON ASS AUD MED
   Iwamiya Shin ichiro, 1994, INTERACTIONS AUDITOR
   Konuk B., 2016, Video content analysis method for audiovisual quality assessment, P1, DOI [10.1109/QoMEX.2016.7498965, DOI 10.1109/QOMEX.2016.7498965]
   Martinez H, 2019, EUR SIGNAL PR CONF, DOI 10.23919/eusipco.2019.8902975
NR 27
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 45
DI 10.1145/3430376
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000012
DA 2024-07-18
ER

PT J
AU Rajput, AS
   Tanwar, VK
   Raman, B
AF Rajput, Amitesh Singh
   Tanwar, Vishesh Kumar
   Raman, Balasubramanian
TI <i>Z</i>-Score-Based Secure Biomedical Model for Effective Skin Lesion
   Segmentation Over eHealth Cloud
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Privacy-preservation; image segmentation; eHealth cloud application;
   multimedia security
ID DATA ANONYMIZATION; PRIVACY
AB This study aims to process the private medical data over eHealth cloud platform. The current pandemic situation, caused by Covid19 has made us to realize the importance of automatic remotely operated independent services, such as cloud. However, the cloud servers are developed and maintained by third parties, and may access user's data for certain benefits. Considering these problems, we propose a specialized method such that the patient's rights and changes in medical treatment can be preserved. The problem arising due to Melanoma skin cancer is carefully considered and a privacy-preserving cloud-based approach is proposed to achieve effective skin lesion segmentation. The work is accomplished by the development of a Z-score-based local color correction method to differentiate image pixels from ambiguity, resulting the segmentation quality to be highly improved. On the other hand, the privacy is assured by partially order homomorphic Permutation Ordered Binary (POB) number system and image permutation. Experiments are performed over publicly available images from the ISIC 2016 and 2017 challenges, as well as PH2 dataset, where the proposed approach is found to achieve significant results over the encrypted images (known as encrypted domain), as compared to the existing schemes in the plain domain (unencrypted images). We also compare the results with the winners of the ISBI 2016 and 2017 challenges, and show that the proposed approach achieves a very close result with them, even after processing test images in the encrypted domain. Security of the proposed approach is analyzed using a challenge-response game model.
C1 [Rajput, Amitesh Singh] Birla Inst Technol & Sci BITS Pilani, Pilani 333031, Rajasthan, India.
   [Tanwar, Vishesh Kumar; Raman, Balasubramanian] Indian Inst Technol Roorkee, Roorkee, Uttarakhand, India.
C3 Birla Institute of Technology & Science Pilani (BITS Pilani); Indian
   Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee
RP Rajput, AS (corresponding author), Birla Inst Technol & Sci BITS Pilani, Pilani 333031, Rajasthan, India.
EM amitesh.singh@pilai.bits-pilani.ac.in; vtanwar@ma.iitr.ac.in;
   bala@cs.iitr.ac.in
RI Tanwar, Dr. Vishesh Kumar/AFJ-6309-2022; Tanwar, Dr. Vishesh
   Kumar/AGI-9932-2022
OI Tanwar, Dr. Vishesh Kumar/0000-0002-4802-7582; 
CR Akhter R, 2013, 2013 IEEE 37TH ANNUAL COMPUTER SOFTWARE AND APPLICATIONS CONFERENCE WORKSHOPS (COMPSACW), P121, DOI 10.1109/COMPSACW.2013.53
   [Anonymous], 2016, arXiv
   [Anonymous], 2012, P IFIP INT C TRUST M
   Chen Siguang, 2020, IEEE T SUST COMPUT
   Chen Y, 2019, J MED SYST, V43, DOI 10.1007/s10916-018-1121-4
   Eyupoglu C, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20050373
   Fan HD, 2017, COMPUT BIOL MED, V85, P75, DOI 10.1016/j.compbiomed.2017.03.025
   Gheid Z, 2016, IEEE TRUST, P791, DOI [10.1109/TrustCom.2016.0140, 10.1109/TrustCom.2016.319]
   Huang HP, 2017, IEEE T IND INFORM, V13, P1227, DOI 10.1109/TII.2017.2687618
   Jahanifar M, 2019, IEEE J BIOMED HEALTH, V23, P509, DOI 10.1109/JBHI.2018.2839647
   Jin H, 2019, IEEE ACCESS, V7, P61656, DOI 10.1109/ACCESS.2019.2916503
   Jin Xin, 2020, IEEE INTERNET THINGS, V2020
   Kalshetti P, 2017, COMPUT BIOL MED, V83, P22, DOI 10.1016/j.compbiomed.2017.02.002
   Karalcis R, 2015, COMPUT BIOL MED, V67, P172, DOI 10.1016/j.compbiomed.2015.10.011
   Kumar S, 2018, MULTIMED TOOLS APPL, V77, P11017, DOI 10.1007/s11042-017-4966-5
   Lathey A, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2656205
   Megías D, 2017, EXPERT SYST APPL, V71, P147, DOI 10.1016/j.eswa.2016.11.015
   Mendonça T, 2013, IEEE ENG MED BIO, P5437
   Mohanty M, 2016, IEEE T INF FOREN SEC, V11, P2542, DOI 10.1109/TIFS.2016.2585085
   Moroney N, 2000, EIGHTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P108
   Nag A., 2019, MULTIMEDIA TOOLS APP, V76, P1
   Nayahi JJV, 2017, FUTURE GENER COMP SY, V74, P393, DOI 10.1016/j.future.2016.10.022
   Ni LN, 2018, IEEE ACCESS, V6, P21053, DOI 10.1109/ACCESS.2018.2824798
   Öztürk S, 2020, J DIGIT IMAGING, V33, P958, DOI 10.1007/s10278-020-00343-z
   Paillier P, 1999, LECT NOTES COMPUT SC, V1592, P223
   Pang Hongping, 2020, IEEE SYST J
   Paverd A., 2014, Tech. Rep
   Polatidis N, 2017, EXPERT SYST APPL, V71, P18, DOI 10.1016/j.eswa.2016.11.018
   Rajput AS, 2018, MULTIMED TOOLS APPL, V77, P21509, DOI 10.1007/s11042-017-5580-2
   Sahi A, 2016, COMPUT BIOL MED, V78, P1, DOI 10.1016/j.compbiomed.2016.09.003
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Singh AK, 2017, MULTIMED TOOLS APPL, V76, P8881, DOI 10.1007/s11042-016-3514-z
   Singh P, 2018, IEEE T CIRC SYST VID, V28, P2116, DOI 10.1109/TCSVT.2017.2716828
   Sreekumar A., 2009, P HACK, V33
   Tajeddin Neda Zamani, 2016, 2016 23rd Iranian Conference on Biomedical Engineering and 2016 1st International Iranian Conference on Biomedical Engineering (ICBME), P134, DOI 10.1109/ICBME.2016.7890944
   Tschandl P, 2019, COMPUT BIOL MED, V104, P111, DOI 10.1016/j.compbiomed.2018.11.010
   Xing K, 2017, IEEE T IND INFORM, V13, P2066, DOI 10.1109/TII.2017.2695487
   Yang JJ, 2015, FUTURE GENER COMP SY, V43-44, P74, DOI 10.1016/j.future.2014.06.004
   Yu LQ, 2017, IEEE T MED IMAGING, V36, P994, DOI 10.1109/TMI.2016.2642839
   Yuan YD, 2017, IEEE T MED IMAGING, V36, P1876, DOI 10.1109/TMI.2017.2695227
   Yuan Yading, 2017, IEEE J BIOMED HEALTH
   Zhang JL, 2020, IEEE INTERNET THINGS, V7, P4016, DOI 10.1109/JIOT.2020.2978286
   Zhang QC, 2022, IEEE T BIG DATA, V8, P25, DOI 10.1109/TBDATA.2017.2701816
   Zhang XY, 2014, IEEE T PARALL DISTR, V25, P363, DOI 10.1109/TPDS.2013.48
   Zhou ZL, 2017, IEEE T INF FOREN SEC, V12, P48, DOI 10.1109/TIFS.2016.2601065
NR 45
TC 5
Z9 5
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 65
DI 10.1145/3430806
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100008
DA 2024-07-18
ER

PT J
AU Kieu, M
   Bagdanov, AD
   Bertini, M
AF My Kieu
   Bagdanov, Andrew D.
   Bertini, Marco
TI Bottom-up and Layerwise Domain Adaptation for Pedestrian Detection in
   Thermal Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Domain adaptation; bottom-up adaptation; thermal; pedestrian detection;
   privacy-preserving; object detection; fine-tuning; layerwise adaptation;
   transfer learning
ID DEEP NEURAL-NETWORKS; FASTER
AB Pedestrian detection is a canonical problem for safety and security applications, and it remains a challenging problem due to the highly variable lighting conditions in which pedestrians must be detected. This article investigates several domain adaptation approaches to adapt RGB-trained detectors to the thermal domain. Building on our earlier work on domain adaptation for privacy-preserving pedestrian detection, we conducted an extensive experimental evaluation comparing top-down and bottom-up domain adaptation and also propose two new bottom-up domain adaptation strategies. For top-down domain adaptation, we leverage a detector pre-trained on RGB imagery and efficiently adapt it to perform pedestrian detection in the thermal domain. Our bottom-up domain adaptation approaches include two steps: first, training an adapter segment corresponding to initial layers of the RGB-trained detector adapts to the new input distribution; then, we reconnect the adapter segment to the original RGB-trained detector for final adaptation with a top-down loss. To the best of our knowledge, our bottom-up domain adaptation approaches outperform the best-performing single-modality pedestrian detection results on KAIST and outperform the state of the art on FLIR.
C1 [My Kieu; Bagdanov, Andrew D.; Bertini, Marco] Univ Florence, Media Integrat & Commun Ctr MICC, Florence, Italy.
C3 University of Florence
RP Kieu, M (corresponding author), Univ Florence, Media Integrat & Commun Ctr MICC, Florence, Italy.
EM my.kieu@unifi.it; andrew.bagdanov@unifi.it; marco.bertini@unifi.it
CR Angelini F, 2019, INT CONF ACOUST SPEE, P8444, DOI 10.1109/ICASSP.2019.8683026
   [Anonymous], 2015, BMVC, DOI DOI 10.5244/C.29.32
   [Anonymous], 2016, P BRIT MACH VIS C BM
   [Anonymous], 2017, P IEEE INT C COMPUTE
   Baek J, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17081850
   Benenson R, 2015, LECT NOTES COMPUT SC, V8926, P613, DOI 10.1007/978-3-319-16181-5_47
   Brazil G, 2017, IEEE I CONF COMP VIS, P4960, DOI 10.1109/ICCV.2017.530
   Cao YP, 2019, ISPRS J PHOTOGRAMM, V150, P70, DOI 10.1016/j.isprsjprs.2019.02.005
   Devaguptapu C, 2019, IEEE COMPUT SOC CONF, P1029, DOI 10.1109/CVPRW.2019.00135
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Du XZ, 2017, IEEE WINT CONF APPL, P953, DOI 10.1109/WACV.2017.111
   FLIR, 2018, FLIR START THERM DAT
   Fritz K, 2019, PROC SPIE, V10988, DOI 10.1117/12.2520705
   Ghose D., 2019, P IEEE C COMP VIS PA
   Guan DY, 2019, INFORM FUSION, V50, P148, DOI 10.1016/j.inffus.2018.11.017
   Hazan Alon, 2018, ARXIV180511601
   Herrmann C, 2018, PROC SPIE, V10643, DOI 10.1117/12.2304400
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   IHS Markit, 2019, 245 MILLION VIDEO SU
   John V, 2015, 2015 14TH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), P246, DOI 10.1109/MVA.2015.7153177
   Kieu M, 2019, LECT NOTES COMPUT SC, V11752, P203, DOI 10.1007/978-3-030-30645-8_19
   Koenig D, 2017, IEEE COMPUT SOC CONF, P243, DOI 10.1109/CVPRW.2017.36
   Kouw W. M., 2018, ARXIV181211806
   Lee Y, 2018, ASIAPAC SIGN INFO PR, P694, DOI 10.23919/APSIPA.2018.8659688
   Li C., 2018, BRIT MACH VIS C BMVC, P1
   Li CY, 2019, PATTERN RECOGN, V85, P161, DOI 10.1016/j.patcog.2018.08.005
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Nakashima S, 2010, PROCD SOC BEHV, V2, P213, DOI 10.1016/j.sbspro.2010.01.038
   Ouyang WL, 2016, INT J COMPUT VISION, V120, P14, DOI 10.1007/s11263-016-0890-9
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Teng YF, 2019, COMPUTATION, V7, DOI 10.3390/computation7020020
   Tian YL, 2015, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2015.7299143
   Vandersteegen M, 2018, LECT NOTES COMPUT SC, V10882, P419, DOI 10.1007/978-3-319-93000-8_47
   Wagner J., 2016, EUR S ART NEUR NETW
   Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang Lu, 2019, ARXIV190102645
   Zheng Y, 2019, ARXIV190306999
NR 43
TC 17
Z9 17
U1 2
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 32
DI 10.1145/3418213
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200012
OA Green Published
DA 2024-07-18
ER

PT J
AU Chen, H
   Ding, GG
   Lin, ZJ
   Zhao, SC
   Gu, XP
   Xu, WY
   Han, JG
AF Chen, Hui
   Ding, Guiguang
   Lin, Zijia
   Zhao, Sicheng
   Gu, Xiaopeng
   Xu, Wenyuan
   Han, Jungong
TI ACMNet: Adaptive Confidence Matching Network for Human Behavior Analysis
   via Cross-modal Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modality retrieval; human behavior analysis; image-text retrieval;
   adaptive confidence matching network
ID TRACKING; GENOME
AB Cross-modality human behavior analysis has attracted much attention from both academia and industry. In this article, we focus on the cross-modality image-text retrieval problem for human behavior analysis, which can learn a common latent space for cross-modality data and thus benefit the understanding of human behavior with data from different modalities. Existing state-of-the-art cross-modality image-text retrieval models tend to be fine-grained region-word matching approaches, where they begin with measuring similarities for each image region or text word followed by aggregating them to estimate the global image-text similarity. However, it is observed that such fine-grained approaches often encounter the similarity bias problem, because they only consider matched text words for an image region or matched image regions for a text word for similarity calculation, but they totally ignore unmatched words/regions, which might still be salient enough to affect the global image-text similarity. In this article, we propose an Adaptive Confidence Matching Network (ACMNet), which is also a fine-grained matching approach, to effectively deal with such a similarity bias. Apart from calculating the local similarity for each region(/word) with its matched words(/regions), ACMNet also introduces a confidence score for the local similarity by leveraging the global text(/image) information, which is expected to help measure the semantic relatedness of the region(/word) to the whole text(/image). Moreover, ACMNet also incorporates the confidence scores together with the local similarities in estimating the global image-text similarity. To verify the effectiveness of ACMNet, we conduct extensive experiments and make comparisons with state-of-the-art methods on two benchmark datasets, i.e., Flicicr30k and MS COCO. Experimental results show that the proposed ACMNet can outperform the state-of-the-art methods by a clear margin, which well demonstrates the effectiveness of the proposed ACMNet in human behavior analysis and the reasonableness of tackling the mentioned similarity bias issue.
C1 [Chen, Hui; Ding, Guiguang] Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China.
   [Chen, Hui; Ding, Guiguang] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Lin, Zijia] Microsoft Res, Beijing, Peoples R China.
   [Zhao, Sicheng] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Gu, Xiaopeng] Natl Engn Lab Publ Safety Risk Percept & Control, Beijing, Peoples R China.
   [Gu, Xiaopeng] China Acad Elect & Informat Technol, Beijing, Peoples R China.
   [Xu, Wenyuan] Zhejiang Univ, Ubiquitous Syst Secur Lab USSLab, Hangzhou, Zhejiang, Peoples R China.
   [Han, Jungong] Univ Warwick, Coventry, W Midlands, England.
C3 Tsinghua University; Microsoft; University of California System;
   University of California Berkeley; Zhejiang University; University of
   Warwick
RP Ding, GG (corresponding author), Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China.; Ding, GG (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM hui2012@gmail.com; dinggg@tsinghua.edu.cn; zijlin@microsoft.com;
   schzhao@gmail.com; gxr0212@163.com; wyxu@zju.edu.cn;
   jungonghan77@gmail.com
RI Han, Jungong/ABE-6812-2020; Ding, Guiguang/KIL-3528-2024
FU National Key R&D Program of China [2018YFC0806900]; National Natural
   Science Foundation of China [61571269, 61701273]; National Engineering
   Laboratory for Public Safety Risk Perception and Control by Big Data
   (PSRPC)
FX This work was supported by the National Key R&D Program of China (Grant
   No. 2018YFC0806900), the National Natural Science Foundation of China
   (Grants No. 61571269 and No. 61701273) and the National Engineering
   Laboratory for Public Safety Risk Perception and Control by Big Data
   (PSRPC).
CR Mora-González CA, 2018, GIST-EDUC LEARN RES, P6
   [Anonymous], 2017, 31 C NEUR INF PROC S
   [Anonymous], 2014, EMNLP
   [Anonymous], 2018, P ANN C N AM CHAPT A
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2017, ARXIV170705612
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2017, ARXIV171005106
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bahdanau D., 2014, 3 INT C LEARN REPR
   Bojanowski P., 2017, Transactions of the Association for Computational Linguistics, V5, P135, DOI DOI 10.1162/TACLA00051
   Bose S, 2019, PALG STUD IMPACT FIN, P19, DOI 10.1007/978-3-030-05624-7_2
   Chen H, 2018, 2018 NINTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME 2018), P606, DOI 10.1109/ITME.2018.00139
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Chorowski J, 2015, ADV NEUR IN, V28
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J., 2018, BERT PRE TRAINING DE
   Ding GG, 2019, IEEE T IMAGE PROCESS, V28, P3752, DOI 10.1109/TIP.2019.2902115
   Ding GG, 2018, IEEE T INTELL TRANSP, V19, P140, DOI 10.1109/TITS.2017.2774778
   Ding Guiguang, 2019, P ASS ADV ART INT AA
   Eisenschtat Aviv, 2017, P IEEE C COMP VIS PA, P4601
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huang YF, 2017, IEEE INT CONF COMP V, P2313, DOI 10.1109/ICCVW.2017.273
   Jiang F, 2015, J MACH LEARN RES, V16, P227
   Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim JH, 2018, ADV NEUR IN, V31
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618
   Lan XY, 2018, IEEE T IMAGE PROCESS, V27, P2022, DOI 10.1109/TIP.2017.2777183
   Lan XY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2481325
   Lee K, 2018, CONSERV GENET RESOUR, V10, P201, DOI 10.1007/s12686-017-0798-x
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Mikolov T., 2013, P 26 INT C NEURAL IN, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Mikolov Tomas, 2013, P 1 INT C LEARN REPR
   Mithun NC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1856, DOI 10.1145/3240508.3240712
   Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Qi YK, 2019, IEEE T PATTERN ANAL, V41, P1116, DOI 10.1109/TPAMI.2018.2828817
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren S, 2015, NIPS, P91, DOI DOI 10.1109/TPAMI.2016.2577031
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vaswani A., 2017, P ADV NEUR INF PROC, V2017, P5998
   Vendrov Ivan, 2015, Order-embeddings of images and language
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wehrmann J ^onatas, 2018, P IEEE C COMP VIS PA, P7718
   Wu GS, 2019, IEEE T IND ELECTRON, V66, P9868, DOI 10.1109/TIE.2018.2873547
   Wu GS, 2019, IEEE T IMAGE PROCESS, V28, P1993, DOI 10.1109/TIP.2018.2882155
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yan CL, 2019, IEEE VTS VEH TECHNOL, DOI 10.1109/vtcspring.2019.8746411
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Zhang SP, 2017, IEEE T NEUR NET LEAR, V28, P2357, DOI 10.1109/TNNLS.2016.2586194
   Zhang SP, 2017, IEEE T CIRC SYST VID, V27, P421, DOI 10.1109/TCSVT.2016.2539860
   Zhang SP, 2015, IEEE T CIRC SYST VID, V25, P1749, DOI 10.1109/TCSVT.2015.2406194
   Zheng Zhedong, 2017, ARXIV171105535
   Zukov-Gregoric A, 2017, PROC INT C TOOLS ART, P652, DOI 10.1109/ICTAI.2017.00104
NR 61
TC 5
Z9 5
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 27
DI 10.1145/3362065
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300009
DA 2024-07-18
ER

PT J
AU Gong, YY
   Li, SR
   Wattanachote, K
   Luo, XN
AF Gong, Yongyi
   Li, Shangru
   Wattanachote, Kanoksak
   Luo, Xiaonan
TI Advanced Stereo Seam Carving by Considering Occlusions on Both Sides
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Stereo image retargeting; occluded regions; occluding regions; avoiding
   strategy; line segment detection
ID IMAGE; DEPTH; SALIENCY
AB Stereo image retargeting plays a significant role in the field of image processing, which aims at making major objects as prominent as possible when the resolution of an image is changed, including maintaining disparity and depth information at the same time. Some seam carving methods are proposed to preserve the geometric consistency of the images. However, the regions of occlusion on both sides are not considered properly. In this article, we propose a solution to solve this problem. A new strategy of seams finding is designed by considering occluded and occluding regions on both of the input images, and leaving geometric consistency in both images intact. We also introduced the method of line segment detection and superpixel segmentation to further improve the quality of the images. Imaging effects are optimized in the process and visual comfort, which is also influenced by other factors, can be boosted as well.
C1 [Gong, Yongyi] Guangdong Univ Foreign Studies, Guangzhou Higher Educ Mega Ctr, Guangzhou 510006, Guangdong, Peoples R China.
   [Li, Shangru; Wattanachote, Kanoksak] Guangdong Univ Foreign Studies, Guangzhou, Guangdong, Peoples R China.
   [Luo, Xiaonan] Guilin Univ Elect Technol, Guilin 541004, Peoples R China.
C3 Guangdong University of Foreign Studies; Guangdong University of Foreign
   Studies; Guilin University of Electronic Technology
RP Gong, YY (corresponding author), Guangdong Univ Foreign Studies, Guangzhou Higher Educ Mega Ctr, Guangzhou 510006, Guangdong, Peoples R China.
EM gongyongyi@gdufs.edu.cn; lsr97mail@yeah.net; kanoxak@hotmail.com;
   luoxn@guet.edu.cn
OI Wattanachote, Kanoksak/0000-0001-6558-9395
FU National Science Foundation of China [61370160, 61772149]; Guangdong
   Province Natural Science Foundation [2015A030313578]; Guangdong
   Scientific and Technological Plan Project [2015B010106005, 01904010228]
FX The work was supported by National Science Foundation Grant of China
   61370160, 61772149, Guangdong Province Natural Science Foundation
   Project (2015A030313578), Guangdong Scientific and Technological Plan
   Project 2015B010106005, 01904010228.
CR [Anonymous], AS C COMP VIS
   [Anonymous], 2010, 2010 3DTV C TRUE VIS
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   [Anonymous], 2013, SCI WORLD J, DOI [10. 1155/2013/231768, DOI 10.1155/2013/231768]
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bare B, 2015, IEEE INT CON MULTI
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Chen YX, 2015, NEUROCOMPUTING, V151, P645, DOI 10.1016/j.neucom.2014.05.089
   Chen ZY, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/819324
   Cho SH, 2013, LECT NOTES COMPUT SC, V8033, P290, DOI 10.1007/978-3-642-41914-0_29
   Dekel T, 2013, IEEE T PATTERN ANAL, V35, P2513, DOI 10.1109/TPAMI.2013.46
   Du JS, 2017, BIO WEB CONF, V8, DOI 10.1051/bioconf/20170802001
   Fang YM, 2016, INFORM SCIENCES, V372, P347, DOI 10.1016/j.ins.2016.08.062
   Fusiello A, 2000, MACH VISION APPL, V12, P16, DOI 10.1007/s001380050120
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Huguet F, 2007, IEEE I CONF COMP VIS, P1342, DOI 10.1109/iccv.2007.4409000
   Jung SW, 2013, IEEE T IMAGE PROCESS, V22, P3892, DOI 10.1109/TIP.2013.2263150
   Kooi FL, 2004, DISPLAYS, V25, P99, DOI 10.1016/j.displa.2004.07.004
   Lee KY, 2012, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2012.6247657
   Li B, 2014, IEEE IMAGE PROC, P2903, DOI 10.1109/ICIP.2014.7025587
   Li B, 2015, IEEE T IMAGE PROCESS, V24, P2811, DOI 10.1109/TIP.2015.2431441
   Liu Y, 2016, J HIGH ENERGY PHYS, DOI 10.1007/JHEP02(2016)116
   NAKAYAMA K, 1990, VISION RES, V30, P1811, DOI 10.1016/0042-6989(90)90161-D
   Peng JT, 2016, IEEE T CYBERNETICS, V46, P1616, DOI 10.1109/TCYB.2015.2453091
   Setlur Vidya., 2005, MUM, V154, P59, DOI DOI 10.1145/1149488.1149499
   Shao F, 2017, OPT EXPRESS, V25, P33202, DOI 10.1364/OE.25.033202
   Shao F, 2017, IEEE T IMAGE PROCESS, V26, P4790, DOI 10.1109/TIP.2017.2721546
   Shao F, 2016, J DISP TECHNOL, V12, P22, DOI 10.1109/JDT.2015.2446973
   von Gioi RG, 2012, IMAGE PROCESS ON LIN, V2, P35, DOI 10.5201/ipol.2012.gjmr-lsd
   Wang Q, 2014, NEUROCOMPUTING, V131, P357, DOI 10.1016/j.neucom.2013.10.007
   Wattanachote K, 2015, IEEE T INF FOREN SEC, V10, P2477, DOI 10.1109/TIFS.2015.2464776
   Yan WQ, 2017, MULTIMED TOOLS APPL, V76, P10465, DOI 10.1007/s11042-016-3442-y
   Yan WQ, 2016, J MOD OPTIC, V63, P207, DOI 10.1080/09500340.2015.1073805
   Yi Liu, 2013, Advances in Multimedia Information Processing - PCM 2013. 14th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 8294, P13, DOI 10.1007/978-3-319-03731-8_2
   Yin T, 2015, COMPUT SECUR, V55, P130, DOI 10.1016/j.cose.2015.09.003
   Yoo JW, 2013, IEEE SIGNAL PROC LET, V20, P519, DOI 10.1109/LSP.2013.2252165
   Zhang DY, 2017, J VIS COMMUN IMAGE R, V48, P281, DOI 10.1016/j.jvcir.2017.07.006
   Zhao HJ, 2017, IEEE T INTELL TRANSP, V18, P192, DOI 10.1109/TITS.2016.2571726
NR 40
TC 4
Z9 4
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2019
VL 15
IS 3
AR 69
DI 10.1145/3321513
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ6YR
UT WOS:000494301200001
DA 2024-07-18
ER

PT J
AU Alameda-Pineda, X
   Redi, M
   Soleymani, M
   Sebe, N
   Chang, SF
   Gosling, S
AF Alameda-Pineda, Xavier
   Redi, Miriam
   Soleymani, Mohammad
   Sebe, Nicu
   Chang, Shih-Fu
   Gosling, Samuel
TI Special Section on Multimodal Understanding of Social, Affective, and
   Subjective Attributes
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Subjective attributes; multimodal data
AB Multimedia scientists have largely focused their research on the recognition of tangible properties of data such as objects and scenes. Recently, the field has started evolving toward the modeling of more complex properties. For example, the understanding of social, affective, and subjective attributes of visual data has attracted the attention of many research teams at the crossroads of computer vision, multimedia, and social sciences. These intangible attributes include, for example, visual beauty, video popularity, or user behavior. Multiple, diverse challenges arise when modeling such properties from multimedia data. The sections concern technical aspects such as reliable groundtruth collection, the effective learning of subjective properties, or the impact of context in subjective perception; see Refs. [2] and [3].
C1 [Alameda-Pineda, Xavier] Inria, Montbonnot St Martin, France.
   [Redi, Miriam] Kings Coll London, London, England.
   [Soleymani, Mohammad] Univ Geneva, Geneva, Switzerland.
   [Sebe, Nicu] Univ Trento, Via Sommar 9, I-38123 Povo, Italy.
   [Chang, Shih-Fu] Columbia Univ, 500 W 120th St Rm 1312, New York, NY 10027 USA.
   [Gosling, Samuel] Univ Texas Austin, 1 Univ Stn, Austin, TX 78712 USA.
   [Alameda-Pineda, Xavier] Inria Grenoble Rhone Alpes, 655 Ave Europe, F-38334 Montbonnot St Martin, France.
   [Redi, Miriam] Kings Coll London, 106 Salisbury Walk, London N19 5DU, England.
   [Soleymani, Mohammad] USC Inst Creat Technol, 12015 Waterfront Dr, Playa Vista, CA 90094 USA.
C3 Inria; University of London; King's College London; University of
   Geneva; University of Trento; Columbia University; University of Texas
   System; University of Texas Austin; University of London; King's College
   London
RP Alameda-Pineda, X (corresponding author), Inria, Montbonnot St Martin, France.; Alameda-Pineda, X (corresponding author), Inria Grenoble Rhone Alpes, 655 Ave Europe, F-38334 Montbonnot St Martin, France.
EM xavier.alameda-pineda@inria.fr; miriam.redi@gmail.com;
   soleymani@ict.usc.edu; sebe@disi.unitn.it; shih.fu.chang@columbia.edu;
   gosling@psy.utexas.edu
RI Soleymani, Mohammad/AAS-2161-2020; Gosling, Sam/AAL-5718-2020; ARSLAN,
   Okan/AAA-3232-2020; Sebe, Niculae/KEC-2000-2024
OI Soleymani, Mohammad/0000-0003-2770-7236; Gosling,
   Sam/0000-0001-8970-591X; Sebe, Niculae/0000-0002-6597-7248
CR [Anonymous], ACM INT C MULT
   [Anonymous], 2015, ACM INT C MULT
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], 2013, ICCV
   [Anonymous], IEEE CVPR
   [Anonymous], IEEE CVPR
   [Anonymous], ACM ICMR
   Khosla A, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P867, DOI 10.1145/2566486.2567996
   Porzi L, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P139, DOI 10.1145/2733373.2806273
   Zhou BL, 2014, ADV NEUR IN, V27
NR 10
TC 0
Z9 0
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 11
DI 10.1145/3292061
PG 3
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100011
DA 2024-07-18
ER

PT J
AU Dong, HS
   Lu, P
   Liu, CP
   Ji, Y
   Gong, SR
AF Dong, Husheng
   Lu, Ping
   Liu, Chunping
   Ji, Yi
   Gong, Shengrong
TI Learning Multiple Kernel Metrics for Iterative Person Re-Identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; kernel method; nullspace; metric learning
ID ENSEMBLE
AB In person re-identification most metric learning methods learn from training data only once, and then they are deployed for testing. Although impressive performance has been achieved, the discriminative information from successfully identified test samples are ignored. In this work, we present a novel re-identification framework termed Iterative Multiple Kernel Metric Learning (IMKML). Specifically, there are two main modules in IMKML. In the first module, multiple metrics are learned via a new derived Kernel Marginal Nullspace Learning (KMNL) algorithm. Taking advantage of learning a discriminative nullspace from neighborhood manifold, KMNL can well tackle the Small Sample Size (SSS) problem in re-identification distance metric learning. The second module is to construct a pseudo training set by performing re-identification on the testing set. The pseudo training set, which consists of the test image pairs that are highly probable correct matches, is then inserted into the labeled training set to retrain the metrics. By iteratively alternating between the two modules, many more samples will be involved for training and significant performance gains can be achieved. Experiments on four challenging datasets, including VIPeR, PRID450S, CUHK01, and Market-1501, show that the proposed method performs favorably against the state-of-the-art approaches, especially on the lower ranks.
C1 [Dong, Husheng; Liu, Chunping; Ji, Yi; Gong, Shengrong] Soochow Univ, Sch Comp Sci & Technol, Suzhou 215006, Peoples R China.
   [Dong, Husheng; Lu, Ping] Suzhou Inst Trade & Commerce, Suzhou 215009, Peoples R China.
   [Liu, Chunping] Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Jilin, Peoples R China.
   [Ji, Yi] Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 210046, Jiangsu, Peoples R China.
   [Gong, Shengrong] Changshu Inst Sci & Technol, Changshu 215500, Jiangsu, Peoples R China.
   [Gong, Shengrong] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing, Peoples R China.
C3 Soochow University - China; Jilin University; Beijing Jiaotong
   University
RP Dong, HS (corresponding author), Soochow Univ, Sch Comp Sci & Technol, Suzhou 215006, Peoples R China.; Dong, HS (corresponding author), Suzhou Inst Trade & Commerce, Suzhou 215009, Peoples R China.
EM hsdong2012@gmail.com; plu@szjm.edu.cn; cpliu@suda.edu.cn;
   yiji@suda.edu.cn
FU National Natural Science Foundation of China (NSFC) [61170124, 61272258,
   61301299, 61272005, 61572085, 61702055]; Provincial Natural Science
   Foundation of Jiangsu [BK20151254, BK20151260]; Six Talents Peaks
   project of Jiangsu [DZXX-027]; Key Laboratory of Symbolic Computation
   and Knowledge Engineering of Ministry of Education, Jilin University
   [93K172016K08]; Collaborative Innovation Center of Novel Software
   Technology and Industrialization
FX This work was partially supported by National Natural Science Foundation
   of China (NSFC Grants No. 61170124, No. 61272258, No. 61301299, No.
   61272005, No. 61572085, No. 61702055), Provincial Natural Science
   Foundation of Jiangsu (Grants No. BK20151254, No. BK20151260), Six
   Talents Peaks project of Jiangsu (DZXX-027), Key Laboratory of Symbolic
   Computation and Knowledge Engineering of Ministry of Education, Jilin
   University (Grant No. 93K172016K08), and Collaborative Innovation Center
   of Novel Software Technology and Industrialization.
CR Bai S, 2017, AAAI CONF ARTIF INTE, P1281
   Bai S, 2016, IEEE T IMAGE PROCESS, V25, P1056, DOI 10.1109/TIP.2016.2514498
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805
   Chen YC, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3402
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68
   Davis J. V., 2007, ICML, P209
   Donoser M, 2013, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2013.174
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   García J, 2017, IEEE T IMAGE PROCESS, V26, P1650, DOI 10.1109/TIP.2017.2652725
   Gong SG, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4_1
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Hirzer M, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P203, DOI 10.1109/AVSS.2012.55
   Hu WM, 2006, IEEE T PATTERN ANAL, V28, P663, DOI 10.1109/TPAMI.2006.80
   Jia JR, 2017, COMPUT VIS IMAGE UND, V160, P87, DOI 10.1016/j.cviu.2017.04.003
   Karaman S, 2014, PATTERN RECOGN, V47, P3767, DOI 10.1016/j.patcog.2014.06.003
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Kviatkovsky Igor, 2013, IEEE Trans Pattern Anal Mach Intell, V35, P1622, DOI 10.1109/TPAMI.2012.246
   Kwolek B, 2017, 11TH INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS (ICDSC 2017), P82, DOI 10.1145/3131885.3131917
   Leng QM, 2015, MULTIMED TOOLS APPL, V74, P6989, DOI 10.1007/s11042-014-1949-7
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lisanti G., 2014, Proceedings of the International Conference on Distributed Smart Cameras, p10:1
   Lisanti G, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3038916
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Liu Jiawei, 2016, P 24 ACM INT C MULT, P192
   Liu XK, 2015, IEEE WINT CONF APPL, P868, DOI 10.1109/WACV.2015.120
   Loy CC, 2009, PROC CVPR IEEE, P1988, DOI 10.1109/CVPRW.2009.5206827
   Ma BP, 2014, IMAGE VISION COMPUT, V32, P379, DOI 10.1016/j.imavis.2014.04.002
   Ma LY, 2014, IEEE T IMAGE PROCESS, V23, P3656, DOI 10.1109/TIP.2014.2331755
   Martinel N, 2015, IEEE T IMAGE PROCESS, V24, P5645, DOI 10.1109/TIP.2015.2487048
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Paisitkriangkrai S, 2017, COMPUT VIS IMAGE UND, V156, P51, DOI 10.1016/j.cviu.2016.10.015
   Roth PM, 2014, ADV COMPUT VIS PATT, P247, DOI 10.1007/978-1-4471-6296-4_12
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Shen C, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1942, DOI 10.1145/3123266.3123452
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Song Bai, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P774, DOI 10.1109/ICCV.2017.90
   Sugiyama M, 2007, J MACH LEARN RES, V8, P1027
   Sun C, 2017, IEEE T IMAGE PROCESS, V26, P23, DOI 10.1109/TIP.2016.2619261
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Varior RR, 2016, IEEE T IMAGE PROCESS, V25, P3395, DOI 10.1109/TIP.2016.2531280
   Vezzani R, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543596
   Wang B, 2014, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2014.161
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang Y, 2016, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2016.143
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2015, IEEE INT C COMP VIS
   Zheng Liang, 2016, ARXIV161002984
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng WM, 2005, IEEE T NEURAL NETWOR, V16, P1, DOI 10.1109/TNN.2004.836239
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhou SP, 2017, PROC CVPR IEEE, P5028, DOI 10.1109/CVPR.2017.534
NR 69
TC 2
Z9 2
U1 0
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
AR 78
DI 10.1145/3234929
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GT7CI
UT WOS:000444677600011
DA 2024-07-18
ER

PT J
AU Cheung, M
   She, J
   Junus, A
   Cao, L
AF Cheung, Ming
   She, James
   Junus, Alvin
   Cao, Lei
TI Prediction of Virality Timing Using Cascades in Social Media
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Measurement; Virality timing; virality prediction; social
   cascade; social media and networks
AB Predicting content going viral in social networks is attractive for viral marketing, advertisement, entertainment, and other applications, but it remains a challenge in the big data era today. Previous works mainly focus on predicting the possible popularity of content rather than the timing of reaching such popularity. This work proposes a novel yet practical iterative algorithm to predict virality timing, in which the correlation between the timing and growth of content popularity is captured by using its own big data naturally generated from users' sharing. Such data is not only able to correlate the dynamics and associated timings in social cascades of viral content but also can he useful to self-correct the predicted timing against the actual timing of the virality in each iterative prediction. The proposed prediction algorithm is verified by datasets from two popular social networks Twitter and Digg-as well as two synthesized datasets with extreme network densities and infection rates. With about 50% of the required content virality data available (i.e., halfway before reaching its actual virality timing), the error of the predicted timing is proven to be bounded within a 40% deviation from the actual timing. To the best of our knowledge, this is the first work that predicts content virality timing iteratively by capturing social cascades dynamics.
C1 [Cheung, Ming; She, James; Junus, Alvin; Cao, Lei] Hong Kong Univ Sci & Technol, HKUST NIE Soc Media Lab, Clear Water Bay,Room 3117, Kowloon, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Cheung, M (corresponding author), Hong Kong Univ Sci & Technol, HKUST NIE Soc Media Lab, Clear Water Bay,Room 3117, Kowloon, Peoples R China.
EM cpming@connect.ust.hk; eejames@connect.ust.hk; jalvin@connect.ust.hk;
   eeclxab@gmail.com
RI Junus, Alvin/HNI-5096-2023
OI Junus, Alvin/0000-0002-8819-0472
FU HKUST-NIE Social Media Lab, HKUST
FX This work was supported by the HKUST-NIE Social Media Lab, HKUST.
CR [Anonymous], 2011, P 20 INT C COMPANION, DOI [10.1145/1963192.1963222, DOI 10.1145/1963192.1963222]
   [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI DOI 10.1145/2433396.2433443
   [Anonymous], 2010, WOSN
   Bai Yu, 2009, Proceedings of the 2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2009), P566, DOI 10.1109/FSKD.2009.719
   Bakshy E., 2012, P 21 INT C WORLD WID, P519, DOI DOI 10.1145/2187836.2187907
   Bandari R., 2012, ICWSM, P26
   Baños RA, 2013, ENTROPY-SWITZ, V15, P4553, DOI 10.3390/e15114553
   Bauckhage C, 2015, 9 INT AAAI C WEB SOC
   Bauckhage C, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P223, DOI 10.1145/2567948.2577298
   Bauckhage Christian, 2013, P 7 INT AAAI C WEBL
   Broxton T, 2013, J INTELL INF SYST, V40, P241, DOI 10.1007/s10844-011-0191-2
   Cha M, 2009, WWW 09 P 18 INT WORL, DOI DOI 10.1145/1526709.1526806
   Cha M., 2008, PROC 1 WORKSHOP ONLI, P13
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Cheng J, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P925, DOI 10.1145/2566486.2567997
   Cheng Xu, 2007, IEEE T MULTIMEDIA, V5, P1184
   Cui P, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P901
   Eysenbach G, 2011, J MED INTERNET RES, V13, DOI 10.2196/jmir.2012
   Figueiredo Flavio, 2014, ACM Transactions on Internet Technology, V14, DOI 10.1145/2665065
   Goel Sharad, 2013, P 14 ACM C EL COMM E, P623
   Goyal A., 2010, P 3 ACM INT C WEB SE, P241, DOI DOI 10.1145/1718487.1718518
   GRANOVETTER M, 1978, AM J SOCIOL, V83, P1420, DOI 10.1086/226707
   Guerini M, 2013, 2013 ASE/IEEE INTERNATIONAL CONFERENCE ON SOCIAL COMPUTING (SOCIALCOM), P671, DOI 10.1109/SocialCom.2013.101
   Hodas Nathan O., 2014, NATURE SCI REPORTS
   Jenders M, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P657
   Jiang L., 2014, P INT C MULTIMEDIA R, P193
   Junus Alvin, 2015, P 2015 IEEE 1 INT C
   Lehmann J., 2012, Proc. ACM Intl. World Wide Web Conf. (WWW), P251, DOI DOI 10.1145/2187836.2187871
   Lerman K., 2010, Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media, V4, P90, DOI [10.1146/annurev.an.03.100174.001431, DOI 10.1146/ANNUREV.AN.03.100174.001431]
   Leskovec J., 2005, P 11 ACM SIGKDD INT, P177
   Leskovec J, 2010, J MACH LEARN RES, V11, P985
   Leskovec J, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P551
   May RM, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.066112
   Ming Cheung, 2013, 2013 IEEE International Conference on Green Computing and Communications (GreenCom) and IEEE Internet of Things (iThings) and IEEE Cyber, Physical and Social Computing (CPSCom), P970, DOI 10.1109/GreenCom-iThings-CPSCom.2013.167
   Pei Sen, 2012, NATURE SCI REPORTS
   Saito K, 2008, LECT NOTES ARTIF INT, V5179, P67, DOI 10.1007/978-3-540-85567-5_9
   Shamma David A., 2011, P 5 INT AAAI C WEBL
   Sun E., 2009, ICWSM
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Tang J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P807
   Wang SZ, 2015, AAAI CONF ARTIF INTE, P325
   Weng Lilian, 2013, NATURE SCI REPORTS
   WOLFE P, 1959, COMMUN ACM, V2, P12, DOI 10.1145/368518.368542
   Wu F, 2007, P NATL ACAD SCI USA, V104, P17599, DOI 10.1073/pnas.0704916104
   Yang J., 2011, P 4 ACM INT C WEB SE, P177, DOI DOI 10.1145/1935826.1935863
   Yu HL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P869, DOI 10.1145/2647868.2655037
   Yu Honglin, 2015, P 9 INT AAAI C WEB S
   Yu LY, 2015, IEEE DATA MINING, P559, DOI 10.1109/ICDM.2015.79
   Zhang XY, 2003, TRANSPORT RES C-EMER, V11, P187, DOI 10.1016/S0968-090X(03)00026-3
   Zhao QY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1513, DOI 10.1145/2783258.2783401
NR 50
TC 9
Z9 11
U1 2
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2017
VL 13
IS 1
AR 2
DI 10.1145/2978771
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM7WJ
UT WOS:000395522700002
DA 2024-07-18
ER

PT J
AU Almowuena, S
   Hefeeda, M
AF Almowuena, Saleh
   Hefeeda, Mohamed
TI Mobile Video Streaming over Dynamic Single-Frequency Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 6th ACM International Conference on Multimedia Systems (MMSys)
   Co-Located with 25th ACM Workshop on Network and Operating Systems
   Support for Digital Audio and Video (NOSSDAV)
CY MAR 18-20, 2015
CL Portland, OR
SP ACM
DE Mobile multimedia; single-frequency network; wireless streaming
ID RESOURCE-ALLOCATION; MULTICAST SERVICE; LTE; PERFORMANCE
AB The demand for multimedia streaming over mobile networks has been steadily increasing over the past several years. For instance, it has become common for mobile users to stream full TV episodes, sports events, and movies while on the go. Unfortunately, this growth in demand has strained the wireless networks despite the significant increase of their capacities with recent generations. Hence, efficient utilization of the expensive and limited wireless spectrum remains an important problem, especially in the context of multimedia streaming services that consume a large portion of the bandwidth capacity. In this article, we introduce the idea of dynamically configuring cells in wireless cellular networks to form single-frequency networks based on the multimedia traffic demands from users in each cell. We formulate the resource allocation problem in such complex networks with the goal of maximizing the number of served multimedia streams, and we prove that this problem is NP-Complete. Then we present an optimal solution to maximize the number of served multimedia streams within a cellular network. This optimal solution, however, may suffer from an exponential time complexity in the worst case, which is not practical for real-time streaming over large-scale networks. Therefore, we propose a heuristic algorithm with polynomial running time to provide faster and more practical solution for real-time deployments. Through detailed packet-level simulations, we assess the performance of the proposed algorithms with respect to the average service ratio, energy saving, video quality, frame loss rate, initial buffering time, rate of re-buffering events, and bandwidth overhead. We show that the proposed algorithms achieve substantial improvements in all of these performance metrics compared to the state-of-the-art approaches. For example, for the service ratio metric, our algorithms can serve up to 11 times more users compared to the unicast approach, and they achieve up to 54% improvement over the closest multicast approaches in the literature.
C1 [Almowuena, Saleh; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, 8888 Univ Dr, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Almowuena, S (corresponding author), Simon Fraser Univ, Sch Comp Sci, 8888 Univ Dr, Burnaby, BC V5A 1S6, Canada.
EM salmowue@sfu.ca; mhefeeda@cs.sfu.ca
FU National Science, Technology and Innovation Plan (NSTIP) of the Kingdom
   of Saudi Arabia; Natural Sciences and Engineering Research Council
   (NSERC) of Canada; Qatar National Research Fund [NPRP8-519-1-108]
FX This work is supported in part by the National Science, Technology and
   Innovation Plan (NSTIP) of the Kingdom of Saudi Arabia, the Natural
   Sciences and Engineering Research Council (NSERC) of Canada, and by the
   Qatar National Research Fund (grant # [NPRP8-519-1-108]).
CR 3GPP, 2010, 26903 3GPP TR
   Adobe, 2009, BIT RAT LIV STREAM
   Afolabi RO, 2013, IEEE COMMUN SURV TUT, V15, P240, DOI 10.1109/SURV.2012.013012.00074
   Alexiou A, 2012, WIREL NETW, V18, P227, DOI 10.1007/s11276-011-0341-z
   Almowuena Saleh, 2015, P ACM MULT SYST C, P153
   [Anonymous], 2013, 4G: LTE/LTE-advanced for mobile broadband
   [Anonymous], 2014, 36300 3GPP TS
   Araniti G, 2014, IEEE T BROADCAST, V60, P358, DOI 10.1109/TBC.2014.2321678
   Araniti G, 2013, IEEE T BROADCAST, V59, P658, DOI 10.1109/TBC.2013.2271387
   Bell, 2014, CRAV TV VID ON DEM S
   Chen J., 2013, Proceedings of the 19th Annual International Conference on Mobile Computing Networking, P389
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Cicconetti C, 2006, IEEE NETWORK, V20, P50, DOI 10.1109/MNET.2006.1607896
   Cisco, 2016, CISC VIS NETW IND GL
   Deng H, 2012, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2012-195
   Eager D, 2001, IEEE T KNOWL DATA EN, V13, P742, DOI 10.1109/69.956098
   Elsherif AR, 2013, IEEE T WIREL COMMUN, V12, P6439, DOI 10.1109/TWC.2013.101813.130732
   Finamore A, 2013, PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT '13), P321, DOI 10.1145/2535372.2535414
   Hefeeda M, 2011, IEEE T COMPUT, V60, P964, DOI 10.1109/TC.2011.57
   Hefeeda M, 2010, IEEE ACM T NETWORK, V18, P610, DOI 10.1109/TNET.2009.2030326
   Hlavacs H, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1324287.1324295
   Hoque MA, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2556942
   Hsu CH, 2010, IEEE ACM T NETWORK, V18, P681, DOI 10.1109/TNET.2009.2033058
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   Hua K. A., 1997, Computer Communication Review, V27, P89, DOI 10.1145/263109.263144
   Keller Lorenzo., 2012, ACM MOBISYS, P57
   Kim Hye Joo, 2012, Evid Based Complement Alternat Med, V2012, P639469, DOI 10.1155/2012/639469
   Lee JM, 2009, IEEE T BROADCAST, V55, P468, DOI 10.1109/TBC.2009.2015605
   Lee SJ, 2011, IEEE T COMMUN, V59, P1264, DOI 10.1109/TCOMM.2011.020811.090200
   Liang YS, 2012, IEEE T VEH TECHNOL, V61, P2243, DOI 10.1109/TVT.2012.2191164
   Lu ZX, 2013, IEEE T MOBILE COMPUT, V12, P1943, DOI 10.1109/TMC.2012.157
   Microsoft, 2010, IEEE T BROADCAST
   Monserrat JF, 2012, IEEE T BROADCAST, V58, P157, DOI 10.1109/TBC.2012.2191030
   Netflix, 2014, NETFL LETT SHAR
   Nicosia Marco, 2010, INTERNET VIDEO NEW R
   Nokia, 2014, NOK LTE NAT TV BROAD
   OPNET, 2012, RIV LTE MOD US GUID
   OPNET, 2010, RIV OPNET MOD SUIT
   Paris Jehan-Francois, 2001, P 2001 IEEE INT C MU, P113
   Parruca D., 2013, 2013 International Conference on Computing, Networking and Communications (ICNC 2013), P1184, DOI 10.1109/ICCNC.2013.6504261
   Rahman M.H., 2014, INT MECH ENG C EXP, P1, DOI [10.1115/IMECE2014-36484, DOI 10.1115/IMECE2014-36484]
   Rong L., 2008, Proceedings of the IEEE Global Telecommunications Conference, P1, DOI [10.1109/GLOCOM.2008.ECP.459 rsity, DOI 10.1109/GLOCOM.2008.ECP.459RSITY]
   Shomi, 2014, SHOM VID ON DEM SERV
   Talarico Salvatore, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6994, DOI 10.1109/ICASSP.2014.6854956
   Urie A, 2013, BELL LABS TECH J, V18, P57, DOI 10.1002/bltj.21605
   Verizon, 2014, VER WIR CUST US 1 9
   Won H, 2009, IEEE T WIREL COMMUN, V8, P4540, DOI 10.1109/TWC.2009.080330
   Xu JS, 2010, NAT COMMUN, V1, DOI 10.1038/ncomms1005
   YouTube, 2014, YOUTUBE PROD STAT
   Yu YJ, 2012, IEEE T MOBILE COMPUT, V11, P1508, DOI 10.1109/TMC.2011.186
   Zaki Yasir, 2011, OPNET WORKSH, P1
NR 51
TC 3
Z9 3
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2016
VL 12
IS 5
SU S
AR 81
DI 10.1145/2983635
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EJ0VT
UT WOS:000392929700012
DA 2024-07-18
ER

PT J
AU Ye, CH
   Ling, HF
   Xiong, ZG
   Zou, FH
   Liu, C
   Xu, F
AF Ye, Conghuan
   Ling, Hefei
   Xiong, Zenggang
   Zou, Fuhao
   Liu, Cong
   Xu, Fang
TI Secure Social Multimedia Big Data Sharing Using Scalable JFE in the
   TSHWT Domain
SO ACM Transactions on Multimedia Computing Communications and Applications
LA English
DT Article
DE Tree-structured Haar wavelet transform (TSHWT); joint fingerprinting and
   encryption; social multimedia big data sharing; collusion attack; social
   network
ID IMAGE ENCRYPTION; CHAOTIC MAPS; PROTECTION; ALGORITHM; NETWORKS; ATTACK;
   CODES
AB With the advent of social networks and cloud computing, the amount of multimedia data produced and communicated within social networks is rapidly increasing. In the meantime, social networking platforms based on cloud computing have made multimedia big data sharing in social networks easier and more efficient. The growth of social multimedia, as demonstrated by social networking sites such as Facebook and YouTube, combined with advances in multimedia content analysis, underscores potential risks for malicious use, such as illegal copying, piracy, plagiarism, and misappropriation. Therefore, secure multimedia sharing and traitor tracing issues have become critical and urgent in social networks. In this article, a joint fingerprinting and encryption (JFE) scheme based on tree-structured Haar wavelet transform (TSHWT) is proposed with the purpose of protecting media distribution in social network environments. The motivation is to map hierarchical community structure of social networks into a tree structure of Haar wavelet transform for fingerprinting and encryption. First, fingerprint code is produced using social network analysis (SNA). Second, the content is decomposed based on the structure of fingerprint code by the TSHWT. Then, the content is fingerprinted and encrypted in the TSHWT domain. Finally, the encrypted contents are delivered to users via hybrid multicast-unicast. The proposed method, to the best of our knowledge, is the first scalable JFE method for fingerprinting and encryption in the TSHWT domain using SNA. The use of fingerprinting along with encryption using SNA not only provides a double layer of protection for social multimedia sharing in social network environment but also avoids big data superposition effect. Theory analysis and experimental results show the effectiveness of the proposed JFE scheme.
C1 [Ye, Conghuan; Xiong, Zenggang; Xu, Fang] Hubei Engn Univ, Sch Comp & Informat Sci, Xiaogan, Hubei, Peoples R China.
   [Ling, Hefei; Zou, Fuhao; Liu, Cong] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Hubei Sheng, Peoples R China.
C3 Hubei Engineering University; Huazhong University of Science &
   Technology
RP Xiong, ZG (corresponding author), Hubei Engn Univ, Sch Comp & Informat Sci, Xiaogan, Hubei, Peoples R China.
EM ychzzw@163.com; lhefei@hust.edu.cn; xzg@hbeu.edu.cn;
   fuhao_zou@hust.edu.cn; liucongwust@gmail.com; fxu@hbeu.edu.cn
FU Natural Science Foundation of China [61502154, U1536203, 61272409,
   61370092, 61370223]; Natural Science Foundation of Hubei Province of
   China [2015CFB236]; Youth Innovation Team Project in the Hubei
   Provincial Department of Education [T201410]; Major Scientific and
   Technological Innovation Project of Hubei Province [2015AAA013]
FX This work was supported by the Natural Science Foundation of China under
   grants 61502154, U1536203, 61272409, 61370092, and 61370223; the Natural
   Science Foundation of Hubei Province of China (2015CFB236); the Youth
   Innovation Team Project in the Hubei Provincial Department of Education
   (T201410); and the Major Scientific and Technological Innovation Project
   of Hubei Province under grant 2015AAA013.
CR [Anonymous], 2011, P 19 ACM INT C MULTI
   Bianchi T, 2015, EURASIP J INF SECUR, DOI 10.1186/s13635-015-0023-y
   Bianchi T, 2009, IEEE T INF FOREN SEC, V4, P86, DOI 10.1109/TIFS.2008.2011087
   Boneh D, 1998, IEEE T INFORM THEORY, V44, P1897, DOI 10.1109/18.705568
   Bouslimi D, 2012, COMPUT METH PROG BIO, V106, P47, DOI 10.1016/j.cmpb.2011.09.015
   Cancellaro M, 2011, SIGNAL PROCESS-IMAGE, V26, P1, DOI 10.1016/j.image.2010.11.001
   Cheng H, 2000, IEEE T SIGNAL PROCES, V48, P2439, DOI 10.1109/78.852023
   Egiazarian K, 2002, J MATH IMAGING VIS, V16, P269, DOI 10.1023/A:1020385811959
   Feng H, 2013, APPL SOFT COMPUT, V13, P3482, DOI 10.1016/j.asoc.2013.04.008
   Feng H, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2344436.2344442
   Fouda JSAE, 2014, COMMUN NONLINEAR SCI, V19, P578, DOI 10.1016/j.cnsns.2013.07.016
   Gao F, 2014, IEEE T INFORM THEORY, V60, P5257, DOI 10.1109/TIT.2014.2331989
   Guan ZH, 2005, PHYS LETT A, V346, P153, DOI 10.1016/j.physleta.2005.08.006
   Hefeeda M, 2015, IEEE T MULTIMEDIA, V17, P420, DOI 10.1109/TMM.2015.2389628
   Huang HC, 2009, SOFT COMPUT, V13, P383, DOI 10.1007/s00500-008-0326-8
   Kundur D, 2004, P IEEE, V92, P918, DOI 10.1109/JPROC.2004.827356
   Lian SG, 2008, IEEE T CIRC SYST VID, V18, P1462, DOI 10.1109/TCSVT.2008.2002829
   Liu C, 2014, IEEE T INF FOREN SEC, V9, P2232, DOI 10.1109/TIFS.2014.2360583
   Liu HJ, 2010, COMPUT MATH APPL, V59, P3320, DOI 10.1016/j.camwa.2010.03.017
   Memon N, 2001, IEEE T IMAGE PROCESS, V10, P643, DOI 10.1109/83.913598
   Rivest R., 1992, MD5 MESSAGE DIGEST A
   Saikia M., 2010, P INT C ADV COMP ENG, P120
   Skoric B, 2011, IEEE T INF FOREN SEC, V6, P906, DOI 10.1109/TIFS.2011.2116783
   Tardos G, 2008, J ACM, V55, DOI 10.1145/1346330.1346335
   Wu XJ, 2015, APPL SOFT COMPUT, V37, P24, DOI 10.1016/j.asoc.2015.08.008
   Ye C., 2012, P 20 ACM INT C MULT, P1117
   Ye CH, 2014, IEEE INT CONF TRUST, P616, DOI 10.1109/TrustCom.2014.79
   Ye CH, 2014, J VISUAL LANG COMPUT, V25, P658, DOI 10.1016/j.jvlc.2014.10.020
   Ye CH, 2013, TELECOMMUN SYST, V54, P315, DOI 10.1007/s11235-013-9736-8
   Ye Conghuan, 2013, LECT NOTES COMPUTER, V8389, P507
   Ye Conghuan, 2012, P 2012 INT S COMP CO, P789
   Ye Conghuan, 2015, P 21 INT C DISTR MUL, P264
   YOSHIDA T, 1983, J STAT PHYS, V31, P279, DOI 10.1007/BF01011583
   Yuen CH, 2011, APPL SOFT COMPUT, V11, P5092, DOI 10.1016/j.asoc.2011.05.050
   Zhang K, 2014, IEEE COMMUN MAG, V52, P58, DOI 10.1109/MCOM.2014.6766086
   Zhao H, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P849
   Zheng P., 2013, P 21 ACM INT C MULT, P803
NR 37
TC 4
Z9 4
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2016
VL 12
IS 4
SU S
AR 61
DI 10.1145/2978571
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8DS
UT WOS:000392736100005
DA 2024-07-18
ER

PT J
AU Yang, XS
   Zhang, TZ
   Xu, CS
AF Yang, Xiaoshan
   Zhang, Tianzhu
   Xu, Changsheng
TI Semantic Feature Mining for Video Event Understanding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video recognition; event
ID IMAGES
AB Content-based video understanding is extremely difficult due to the semantic gap between low-level vision signals and the various semantic concepts (object, action, and scene) in videos. Though feature extraction from videos has achieved significant progress, most of the previous methods rely only on low-level features, such as the appearance and motion features. Recently, visual-feature extraction has been improved significantly with machine-learning algorithms, especially deep learning. However, there is still not enough work focusing on extracting semantic features from videos directly. The goal of this article is to adopt unlabeled videos with the help of text descriptions to learn an embedding function, which can be used to extract more effective semantic features from videos when only a few labeled samples are available for video recognition. To achieve this goal, we propose a novel embedding convolutional neural network (ECNN). We evaluate our algorithm by comparing its performance on three challenging benchmarks with several popular state-of-the-art methods. Extensive experimental results show that the proposed ECNN consistently and significantly outperforms the existing methods.
C1 [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Yang, XS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM xiaoshan.yang@nlpr.ia.ac.cn; tzzhang10@gmail.com; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Zhang, Tianzhu/AGY-9389-2022
OI Zhang, Tianzhu/0000-0003-0764-6106
FU National Natural Science Foundation of China [61225009, 61303173,
   61432019, 61572498, 61532009, 61472379, 61572296]; National Basic
   Research Program of China [2012CB316304]; Importation and Development of
   High-Caliber Talents Project of Beijing Municipal Institutions
   [IDHT20140224]
FX This work is supported in part by the National Natural Science
   Foundation of China (Grant Nos. 61225009, 61303173, 61432019, 61572498,
   61532009, 61472379, 61572296), the National Basic Research Program of
   China (Grant No. 2012CB316304), and the Importation and Development of
   High-Caliber Talents Project of Beijing Municipal Institutions under
   Grant IDHT20140224.
CR [Anonymous], 2015, P INT C MACH LEARN
   [Anonymous], 2014, CORR
   [Anonymous], 2014, Transactions of the Association for Computational Linguistics
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], 2011, P 1 INT C MULT RETR
   [Anonymous], 2014, CORR
   [Anonymous], P 27 AAAI C ART INT
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], IEEE I CONF COMP VIS
   [Anonymous], P 20 ACM INT C MULT
   [Anonymous], N AM CHAPTER ASS COM
   [Anonymous], P 21 ACM INT C MULT
   [Anonymous], P TRECVID 2013
   [Anonymous], 2009, P 17 ACM INT C MULT, DOI DOI 10.1145/1631272.1631371
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Bengio Y., ADV NEURAL INFORM PR, P153
   Bojanowski P, 2015, IEEE I CONF COMP VIS, P4462, DOI 10.1109/ICCV.2015.507
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Duan LX, 2012, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2012.6247819
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Garcia-Perez A., 2019, Designing and tracking knowledge management metrics, P163
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Jia Y., 2014, P 22 ACM INT C MULT, P675
   Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655, DOI 10.3115/v1/p14-1062
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu MY, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P274, DOI 10.1109/ICCVW.2015.44
   Luo J., 2008, ACM International Conference on Multimedia, P1071, DOI DOI 10.1145/1459359.1459574MULTIMEDIA-MM'PLACE
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Qian SS, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P99, DOI 10.1145/2733373.2806234
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Qian SS, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2659521
   Ramanathan V, 2013, IEEE I CONF COMP VIS, P905, DOI 10.1109/ICCV.2013.117
   Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P295, DOI 10.1109/ICCVW.2015.47
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2009, AISTATS
   Simonyan K., 2014, CORR
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Strassel S, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P2573
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tapaswi M, 2015, PROC CVPR IEEE, P1827, DOI 10.1109/CVPR.2015.7298792
   Thomason J., 2014, COLING, P1218
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang L, 2015, 2015 International Conference on Computer Science and Mechanical Automation (CSMA), P30, DOI 10.1109/CSMA.2015.13
   Wang LM, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P287, DOI 10.1109/ICCVW.2015.46
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Y, 2013, IEEE I CONF COMP VIS, P2104, DOI 10.1109/ICCV.2013.456
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang TZ, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2602633
NR 67
TC 9
Z9 10
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2016
VL 12
IS 4
AR 55
DI 10.1145/2962719
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EH
UT WOS:000382877500009
DA 2024-07-18
ER

PT J
AU Chen, SN
   Gao, ZH
   Nahrstedt, K
   Gupta, I
AF Chen, Shannon
   Gao, Zhenhuan
   Nahrstedt, Klara
   Gupta, Indranil
TI 3DTI Amphitheater: Towards 3DTI Broadcasting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; 3D Tele-immersion; broadcasting; content
   dissemination
AB 3DTI Amphitheater is a live broadcasting system for dissemination of 3DTI (3D Tele-immersive) content. The virtual environment constructed by the system mimics an amphitheater in the real world, where performers interact with each other in the central circular stage, and the audience is placed in virtual seats that surround the stage. Users of the Amphitheater can be geographically dispersed and the streams created by the performer sites are disseminated in a P2P network among the participants. To deal with the high bandwidth demand and strict latency bound of the service, we identify the hierarchical priority of streams in construction of the content dissemination forest. Result shows that the Amphitheater outperforms prior 3DTI systems by boosting the application QoS by a factor of 2.8 while sustaining the same hundred-scale audience group.
C1 [Chen, Shannon; Gao, Zhenhuan; Nahrstedt, Klara; Gupta, Indranil] Univ Illinois, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Chen, SN (corresponding author), Univ Illinois, Siebel Hall,201 North Goodwin Ave, Urbana, IL 61801 USA.
EM Kcircnc@gmail.com; zgaoll@illinois.edu; klara@illinois.edu;
   indy@illinois.edu
FU NSF Grant [CNS10-12194, CNS09-64081KN]; Division Of Computer and Network
   Systems; Direct For Computer & Info Scie & Enginr [1012194] Funding
   Source: National Science Foundation
FX This material is based upon work supported by NSF Grant CNS10-12194,
   CNS09-64081KN.
CR Agarwal P., 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P25, DOI 10.1109/ISM.2010.14
   [Anonymous], P 5 ACM MULT SYST C
   [Anonymous], P 21 ACM INT C MULT
   Arefin A., 2012, P 32 IEEE INT C DIST
   Coulouris G F, 2011, Distributed Systems: Concepts and Design, V5th
   DeVincenzi Anthony., 2011, P ACM 2011 C COMPUTE, P621
   Eugster PT, 2003, ACM COMPUT SURV, V35, P114, DOI 10.1145/857076.857078
   Feldman Dima, 2007, P WORKSH END TO END, P1
   Jain M, 2003, IEEE ACM T NETWORK, V11, P537, DOI 10.1109/TNET.2003.815304
   Nahrstedt K, 2011, MULTIMED TOOLS APPL, V51, P593, DOI 10.1007/s11042-010-0629-5
   Wang Z, 1996, IEEE J SEL AREA COMM, V14, P1228, DOI 10.1109/49.536364
   Wu WM, 2008, INT CON DISTR COMP S, P647, DOI 10.1109/ICDCS.2008.47
   Yang Z, 2006, NAS: 2006 INTERNATIONAL WORKSHOP ON NETWORKING, ARCHITECTURE, AND STORAGES, PROCEEDINGS, P73
   Yin H., 2009, Proceedings of ACM International Conference on Multimedia, P25
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 15
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2015
VL 11
IS 2
SU S
SI SI
AR 47
DI 10.1145/2700297
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC7RV
UT WOS:000350567000007
DA 2024-07-18
ER

PT J
AU Deng, ZY
   Yan, M
   Sang, JT
   Xu, CS
AF Deng, Zhengyu
   Yan, Ming
   Sang, Jitao
   Xu, Changsheng
TI Twitter is Faster: Personalized Time-Aware Video Recommendation from
   Twitter to YouTube
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Short-term interest;
   personalization; video recommendation; time-aware; cross-platform
AB Traditional personalized video recommendation methods focus on utilizing user profile or user history behaviors to model user interests, which follows a static strategy and fails to capture the swift shift of the short-term interests of users. According to our cross-platform data analysis, the information emergence and propagation is faster in social textual stream-based platforms than that in multimedia sharing platforms at micro user level. Inspired by this, we propose a dynamic user modeling strategy to tackle personalized video recommendation issues in the multimedia sharing platform YouTube, by transferring knowledge from the social textual stream-based platform Twitter. In particular, the cross-platform video recommendation strategy is divided into two steps. (1) Real-time hot topic detection: the hot topics that users are currently following are extracted from users' tweets, which are utilized to obtain the related videos in YouTube. (2) Time-aware video recommendation: for the target user in YouTube, the obtained videos are ranked by considering the user profile in YouTube, time factor, and quality factor to generate the final recommendation list. In this way, the short-term (hot topics) and long-term (user profile) interests of users are jointly considered. Carefully designed experiments have demonstrated the advantages of the proposed method.
C1 [Deng, Zhengyu; Yan, Ming; Sang, Jitao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   China Singapore Inst Digital Media, Singapore, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM zydeng@nlpr.ia.ac.cn; ming.yan@nlpr.ia.ac.cn; jtsang@nlpr.ia.ac.cn;
   csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61332016, 61303176];
   Beijing Natural Science Foundation [4131004]; Singapore National
   Research Foundation under its International Research Centre @ Singapore
   Funding Initiative
FX This work is supported in part by the National Basic Research Program of
   China (No. 2012CB316304), National Natural Science Foundation of China
   (No. 61225009, 61332016, 61303176), and Beijing Natural Science
   Foundation (No. 4131004). This work is also supported by the Singapore
   National Research Foundation under its International Research Centre @
   Singapore Funding Initiative and administered by the IDM Programme
   Office.
CR Abel F., 2011, P 3 ACM INT C WEB SC
   Abel F, 2011, LECT NOTES COMPUT SC, V6787, P1, DOI 10.1007/978-3-642-22362-4_1
   Abel F, 2011, LECT NOTES COMPUT SC, V6757, P28, DOI 10.1007/978-3-642-22233-7_3
   Abel F, 2011, LECT NOTES COMPUT SC, V6644, P375, DOI 10.1007/978-3-642-21064-8_26
   Ahn Y.-Y., 2007, P 16 INT C WORLD WID, DOI [10.1145/1242572.1242685, DOI 10.1145/1242572.1242685]
   [Anonymous], 2012, SIG 2012 WORKSH TIM
   [Anonymous], 2012, ACM MM'12'
   [Anonymous], 2012, RECSYS
   [Anonymous], 2012, P 20 ACM INT C MULTI
   Baluja S, 2008, WORLD WID WEB C, P895
   Benevenuto F, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1596990.1596994
   Bennett PN, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P185, DOI 10.1145/2348283.2348312
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   Deng Z., 2013, P IEEE INT C MULT EX
   Fortuna B., 2011, ARXIV11035002
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Jin YH, 2010, IEEE INT C SEMANT CO, P126, DOI 10.1109/ICSC.2010.79
   Koenigstein N, 2011, P 5 ACM C REC SYST, P165, DOI [DOI 10.1145/2043932.2043964, 10.1145/20]
   Koren Y, 2010, COMMUN ACM, V53, P89, DOI 10.1145/1721654.1721677
   Kwak H., WWW'10, DOI DOI 10.1145/1772690.1772751
   Liu A., 2009, Proceedings of the 17th ACM International Conference on Multimedia, P845, DOI [DOI 10.1145/1631272.1631429, 10.1145/1631272.1631429]
   Magnani M, 2011, 2011 INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2011), P5, DOI 10.1109/ASONAM.2011.114
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Park J., 2011, IEEE MULTIMEDIA, V18, P1
   Qi Gao, 2011, 2011 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies, P100, DOI 10.1109/WI-IAT.2011.74
   Szomszor M, 2008, LECT NOTES COMPUT SC, V5318, P632, DOI 10.1007/978-3-540-88564-1_40
   Szomszor MN., 2008, 19 ACM C HYPERTEXT H, P33, DOI DOI 10.1145/1379092.1379103
   Wang JJ, 2007, IEEE T MULTIMEDIA, V9, P576, DOI 10.1109/TMM.2006.888013
   Weng J., 2010, P 3 ACM INT C WEB SE, P261, DOI [10.1145/1718487.1718520, DOI 10.1145/1718487.1718520]
   Xiang L., 2010, P 16 ACM SIGKDD INT, P723, DOI 10.1145/1835804.1835896
   Xiong L, 2010, P SIAM INT C DAT MIN, P211, DOI DOI 10.1137/1.9781611972801.19
   Yang J., 2011, P 4 ACM INT C WEB SE, P177, DOI DOI 10.1145/1935826.1935863
   Zhao HY, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P200, DOI 10.1109/GSIS.2013.6714773
   Zhao WNX, 2011, LECT NOTES COMPUT SC, V6611, P338, DOI 10.1007/978-3-642-20161-5_34
   Zhao Xiaojian, 2011, P 19 ACM INT C MULTI, P1521
NR 35
TC 18
Z9 18
U1 8
U2 42
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2014
VL 11
IS 2
AR 31
DI 10.1145/2637285
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6DM
UT WOS:000348308800008
DA 2024-07-18
ER

PT J
AU Murray, N
   Lee, B
   Qiao, YS
   Muntean, GM
AF Murray, Niall
   Lee, Brian
   Qiao, Yuansong
   Muntean, Gabriel-Miro
TI Multiple-Scent Enhanced Multimedia Synchronization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Design; Experimentation; Olfaction; multimedia
   synchronization; subjective quality assessment; quality of experience
ID SMELL; IRAQ
AB This study looked at users' perception of interstream synchronization between audiovisual media and two olfactory streams. The ability to detect skews and the perception and impact of skews on user Quality of Experience (QoE) is analyzed. The olfactory streams are presented with the same skews (i.e., delay) and with variable skews (i.e., jitter and mix of scents). This article reports the limits beyond which desynchronization reduces user-perceived quality levels. Also, a minimum gap between the presentations of consecutive scents is identified, necessary to ensuring enhanced user-perceived quality. There is no evidence (not considering scent type) that overlapping or mixing of scents increases user QoE levels for olfaction-enhanced multimedia.
C1 [Murray, Niall; Lee, Brian; Qiao, Yuansong] Athlone Inst Technol, Athlone, Ireland.
   [Muntean, Gabriel-Miro] Dublin City Univ, Dublin 9, Ireland.
C3 Technological University of the Shannon: Midlands Midwest; Dublin City
   University
RP Murray, N (corresponding author), Athlone Inst Technol, Dublin Rd, Athlone, Ireland.
EM nmurray@research.ait.ie
RI Muntean, Gabriel-Miro/U-6783-2019; Qiao, Yuansong/A-1140-2017
OI Muntean, Gabriel-Miro/0000-0002-9332-4770; Lee,
   Brian/0000-0002-8475-4074; Qiao, Yuansong/0000-0002-1543-1589
CR [Anonymous], 2011, ACM INT C MULTIMEDIA
   Ariyakul Y, 2011, P IEEE VIRT REAL ANN, P193, DOI 10.1109/VR.2011.5759464
   Borromeo S, 2010, IEEE T INSTRUM MEAS, V59, P2602, DOI 10.1109/TIM.2010.2057531
   Brewster S. A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P653
   Cha J., 2009, MM '09: Proceedings of the seventeen ACM international conference on Multimedia, P1135, DOI [10.1145/1631272.1631535, DOI 10.1145/1631272.1631535]
   Chebat JC, 2003, J BUS RES, V56, P529, DOI 10.1016/S0148-2963(01)00247-8
   Dann G. M. S., 2003, Tourism Geographies, V5, P3, DOI 10.1080/1461668032000034033
   Depledge MH, 2011, ENVIRON SCI TECHNOL, V45, P4660, DOI 10.1021/es103907m
   Eid M, 2011, IEEE T INSTRUM MEAS, V60, P21, DOI 10.1109/TIM.2010.2065530
   Emsenhuber B., 2011, SCI TECHNOL INNOV ST, V7, P1
   Gerardi M, 2008, J TRAUMA STRESS, V21, P209, DOI 10.1002/jts.20331
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   Ghinea G, 2011, MULTIMED TOOLS APPL, V55, P601, DOI 10.1007/s11042-010-0581-4
   Ghinea G, 2010, IEEE T SYST MAN CY A, V40, P657, DOI 10.1109/TSMCA.2010.2041224
   Gueguen N., 2006, INT J HOSP MANAG, V25, P335, DOI [DOI 10.1016/J.IJHM.2005.04.007, 10.1016/j.ijhm.2005.04.007]
   Haehner A, 2009, EXPERT REV NEUROTHER, V9, P1773, DOI 10.1586/ERN.09.115
   HIRSCH AR, 1995, PSYCHOL MARKET, V12, P585, DOI 10.1002/mar.4220120703
   Hoshino S., 2011, P IEEE INT WORKSH TE, P1, DOI [10.1109/CQR.2011.5996082, DOI 10.1109/CQR.2011.5996082]
   ISO, 2006, ISO 5496:2006
   ISO, 2010, 230053 ISOIEC FDIS
   ISO, 2007, 8589 ISOIEC
   ISO, 2008, 54922008 ISO
   ITU, 2002, BT500 ITUT
   ITU, 2008, Rec. ITU-T P.910
   Kaye J.N., 2001, THESIS MIT CAMBRIDGE
   Longino B., 1999, ATLANTA J CONSTITUT
   Murray N., 2014, ACM COMPUT SURV
   Murray N., 2013, QUESTIONNAIRE SPECIA
   Murray N, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540994
   Murray Niall., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys'13, P162, DOI [https://doi.org/10.1145/2483977.2483999, DOI 10.1145/2483977.2483999]
   Nakamoto T, 2008, IEEE COMPUT GRAPH, V28, P75, DOI 10.1109/MCG.2008.3
   Nakamoto T, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P179
   Nakamoto T, 2006, IEICE T FUND ELECTR, VE89A, P3327, DOI 10.1093/ietfec/e89-a.l 1.3327
   Narumi T, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P93
   Noguchi D, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P83
   Pair J, 2006, P IEEE VIRT REAL ANN, P67, DOI 10.1109/VR.2006.23
   Schiavone G, 2013, IEEE INT C MICROELEC, P13, DOI 10.1109/ICMTS.2013.6528138
   Shams L, 2008, TRENDS COGN SCI, V12, P411, DOI 10.1016/j.tics.2008.07.006
   Spencer BS, 2006, IEEE T INF TECHNOL B, V10, P168, DOI 10.1109/TITB.2005.856851
   Steinmetz R, 1996, IEEE J SEL AREA COMM, V14, P61, DOI 10.1109/49.481694
   Sugimoto Sayumi., 2010, Proceedings of the 18th ACM International Conference on Multimedia, MM'10, P301, DOI DOI 10.1145/1873951.1873994
   Tillotson J, 2008, COMM COM INF SC, V7, P403
   Timmerer C, 2012, SIGNAL PROCESS-IMAGE, V27, P909, DOI 10.1016/j.image.2012.01.016
   Tomono A., 2004, P HUM INT S, P249
   Warnock D., 2011, 2011 5th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth 2011), P461, DOI 10.4108/icst.pervasivehealth.2011.246001
   Washburn D. A., 2003, MODELING SIMULATION, V2, P19
   Washburn DA, 2004, COMPUT SCI ENG, V6, P80, DOI 10.1109/MCSE.2004.66
   Zixia H., 2012, P 3 MULTIMEDIA SYSTE, P29
NR 48
TC 17
Z9 17
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2014
VL 11
IS 1
SU S
SI SI
AR 12
DI 10.1145/2637293
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AS0RG
UT WOS:000343984800004
DA 2024-07-18
ER

PT J
AU Lei, YQ
   Qiu, GP
   Zheng, LG
   Huang, JW
AF Lei, Yanqiang
   Qiu, Guoping
   Zheng, Ligang
   Huang, Jiwu
TI Fast Near-Duplicate Image Detection Using Uniform Randomized Trees
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Verification; Fast near-duplicate image
   detection; indexing structure; uniform randomized tree
AB Indexing structure plays an important role in the application of fast near-duplicate image detection, since it can narrow down the search space. In this article, we develop a cluster of uniform randomized trees (URTs) as an efficient indexing structure to perform fast near-duplicate image detection. The main contribution in this article is that we introduce "uniformity" and "randomness" into the indexing construction. The uniformity requires classifying the object images into the same scale subsets. Such a decision makes good use of the two facts in near-duplicate image detection, namely: (1) the number of categories is huge; (2) a single category usually contains only a small number of images. Therefore, the uniform distribution is very beneficial to narrow down the search space and does not significantly degrade the detection accuracy. The randomness is embedded into the generation of feature subspace and projection direction, improveing the flexibility of indexing construction. The experimental results show that the proposed method is more efficient than the popular locality-sensitive hashing and more stable and flexible than the traditional KD-tree.
C1 [Lei, Yanqiang] Sun Yat Sen Univ, Sch Informat Sci Technol, Guangzhou 510006, Guangdong, Peoples R China.
   [Qiu, Guoping] Univ Nottingham, Sch Comp Sci, Nottingham NG8 1BB, England.
   [Zheng, Ligang] Guangzhou Univ, Sch Comp, Guangzhou 510006, Guangdong, Peoples R China.
   [Huang, Jiwu] Shenzhen Univ, Coll Informat Engn, Shenzhen Key Lab Media Secur, Shenzhen 518060, Peoples R China.
C3 Sun Yat Sen University; University of Nottingham; Guangzhou University;
   Shenzhen University
RP Huang, JW (corresponding author), Shenzhen Univ, Coll Informat Engn, Shenzhen Key Lab Media Secur, Shenzhen 518060, Peoples R China.
EM jwhuang@szu.edu.cn
RI huang, jw/KVY-9917-2024
OI Qiu, Guoping/0000-0002-5877-5648
FU 973 Program [2011CB302200]; National Science & Technology Pillar Program
   [2012BAK16B06]; NSFC [U1135001, 61332012, 61173147, 61300205]
FX This work was supported in part by 973 Program (2011CB302200), National
   Science & Technology Pillar Program (2012BAK16B06), NSFC (U1135001,
   61332012, 61173147, 61300205).
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], P ACM INT C MULT INF
   [Anonymous], ACM MULTIMEDIA, DOI DOI 10.1145/1027527.1027729
   Bhat DN, 1998, IEEE T PATTERN ANAL, V20, P415, DOI 10.1109/34.677275
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Chang EY, 1998, P SOC PHOTO-OPT INS, V3527, P58, DOI 10.1117/12.325852
   Chum Ondrej., 2007, CIVR 07, P549, DOI DOI 10.1145/1282280.1282359
   Douze M, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646421
   Hu Y, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P657, DOI 10.1109/ICME.2008.4607520
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Kim C, 2003, SIGNAL PROCESS-IMAGE, V18, P169, DOI 10.1016/S0923-5965(02)00130-3
   Lei YQ, 2011, SIGNAL PROCESS-IMAGE, V26, P280, DOI 10.1016/j.image.2011.04.007
   Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Qamra A, 2005, IEEE T PATTERN ANAL, V27, P379, DOI 10.1109/TPAMI.2005.54
   Qiu GP, 2007, PATTERN RECOGN, V40, P1711, DOI 10.1016/j.patcog.2006.09.020
   Ramírez J, 2009, ELECTRON LETT, V45, P604, DOI 10.1049/el.2009.1111
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Shakhnarovich G., 2008, SOURCE CODE LOCALITY
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Wu MN, 2007, J SYST SOFTWARE, V80, P1057, DOI 10.1016/j.jss.2006.12.001
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Yu G, 2011, IEEE T MULTIMEDIA, V13, P507, DOI 10.1109/TMM.2011.2128301
   Yudong Cao, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2461, DOI 10.1109/ICIP.2011.6116159
   Zheng LG, 2012, IEEE T INF FOREN SEC, V7, P1578, DOI 10.1109/TIFS.2012.2206386
   Zhipeng Wu, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P842, DOI 10.1109/ICPR.2010.212
NR 29
TC 9
Z9 9
U1 4
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2014
VL 10
IS 4
AR 35
DI 10.1145/2602186
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO4FY
UT WOS:000341292900005
DA 2024-07-18
ER

PT J
AU Yeh, CH
   Barsky, BA
   Ouhyoung, M
AF Yeh, Che-Hua
   Barsky, Brian A.
   Ouhyoung, Ming
TI Personalized Photograph Ranking and Selection System Considering
   Positive and Negative User Feedback
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Performance; Photograph ranking; personalized ranking;
   example-based reranking; aesthetic rules; photograph composition;
   relevance feedback
ID IMAGE QUALITY ASSESSMENT; RETRIEVAL; SCENE
AB In this article, we propose a novel personalized ranking system for amateur photographs. The proposed framework treats the photograph assessment as a ranking problem and we introduce the idea of personalized ranking, which ranks photographs considering both their aesthetic qualities and personal preferences. Photographs are described using three types of features: photo composition, color and intensity distribution, and personalized features. An aesthetic prediction model is learned from labeled photographs by using the proposed image features and RBF-ListNet learning algorithm. The experimental results show that the proposed framework outperforms in the ranking performance: a Kendall's tau value of 0.432 is significantly higher than those obtained by the features proposed in one of the state-of-the-art approaches (0.365) and by learning based on support vector regression (0.384). To realize personalization in ranking, three approaches are proposed: the feature-based approach allows users to select photographs with specific rules, the example-based approach takes the positive feedback from users to rerank the photograph, and the list-based approach takes both positive and negative feedback from users into consideration. User studies indicate that all three approaches are effective in both aesthetic and personalized ranking.
C1 [Yeh, Che-Hua; Ouhyoung, Ming] Natl Taiwan Univ, Taipei 106, Taiwan.
   [Barsky, Brian A.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
C3 National Taiwan University; University of California System; University
   of California Berkeley
RP Yeh, CH (corresponding author), Natl Taiwan Univ, R503,CSIE Bldg,1,Sec 4,Roosevelt Rd, Taipei 106, Taiwan.
EM chyei@cmlab.csie.ntu.edu.tw
OI OUHYOUNG, MING/0000-0002-3038-6958
CR [Anonymous], P INT C MULT MM 10
   [Anonymous], P INT S COMP AESTH G
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2008, P 25 INT C MACH LEAR, DOI [DOI 10.1145/1390156.1390306, 10.1145/1390156.1390306]
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], THESIS NATL TAIWAN U
   [Anonymous], 2010, ACM MULTIMEDIA
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao Z., 2007, P 24 INT C MACHINE L, P129, DOI DOI 10.1145/1273496.1273513
   Chellappa R., 1989, Journal of the Institution of Electronics and Telecommunication Engineers, V35, P114
   CHEN TP, 1995, IEEE T NEURAL NETWOR, V6, P904, DOI 10.1109/72.392252
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Datta R., 2006, P ECCV, P7
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Grill Tom., 1990, Photographic composition
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Karayiannis NB, 2003, IEEE T NEURAL NETWOR, V14, P835, DOI 10.1109/TNN.2003.813841
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Krages B., 2005, Photography: the art of composition
   KRUSKAL WH, 1958, J AM STAT ASSOC, V53, P814, DOI 10.2307/2281954
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Liu Ligang, 2010, COMPUT GRAPH FORUM, V29, P2
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Martinez Benjamin., 1988, Visual Forces: An Introduction to Design
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perronnin F., 2007, P IEEE C COMPUTER VI, P1
   Peters G, 2007, IEEE INT CONF INF VI, P316
   Ro YM, 2001, ETRI J, V23, P41, DOI 10.4218/etrij.01.0101.0201
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Rui Y, 2000, PROC CVPR IEEE, P236, DOI 10.1109/CVPR.2000.855825
   San Pedro Jose., 2009, WWW, P771, DOI DOI 10.1145/1526709.1526813
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Tong HG, 2004, LECT NOTES COMPUT SC, V3331, P198
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE IMAGE PROC, P477
   Yang S, 2007, IEEE T CIRC SYST VID, V17, P324, DOI 10.1109/TCSVT.2007.890829
   Yang YH, 2011, IEEE T AUDIO SPEECH, V19, P762, DOI 10.1109/TASL.2010.2064164
   Yeh C.-H., 2010, Proceedings of the international conference on Multimedia - MM'10, page, P211
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 48
TC 12
Z9 14
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2014
VL 10
IS 4
AR 36
DI 10.1145/2584105
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO4FY
UT WOS:000341292900006
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhao, SH
   Ooi, WT
   Carlier, A
   Morin, G
   Charvillat, V
AF Zhao, Shanghong
   Ooi, Wei Tsang
   Carlier, Axel
   Morin, Geraldine
   Charvillat, Vincent
TI Bandwidth Adaptation for 3D Mesh Preview Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Human Factors; Performance; Progressive mesh streaming; 3D
   preview; camera path
ID MODELS
AB Online galleries of 3D models typically provide two ways to preview a model before the model is downloaded and viewed by the user: (i) by showing a set of thumbnail images of the 3D model taken from representative views (or keyviews); (ii) by showing a video of the 3D model as viewed from a moving virtual camera along a path determined by the content provider. We propose a third approach called preview streaming for mesh-based 3D objects: by streaming and showing parts of the mesh surfaces visible along the virtual camera path. This article focuses on the preview streaming architecture and framework and presents our investigation into how such a system would best handle network congestion effectively. We present three basic methods: (a) STOP-AND-WAIT, where the camera pauses until sufficient data is buffered; (b) REDUCE-SPEED, where the camera slows down in accordance to reduce network bandwidth; and (c) REDUCE-QUALITY, where the camera continues to move at the same speed but fewer vertices are sent and displayed, leading to lower mesh quality. We further propose two advanced methods: (d) KEYVIEW-AWARE, which trades off mesh quality and camera speed appropriately depending on how close the current view is to the keyviews, and (e) ADAPTIVE-ZOOM, which improves visual quality by moving the virtual camera away from the original path. A user study reveals that our KEYVIEW-AWARE method is preferred over the basic methods. Moreover, the ADAPTIVE-ZOOM scheme compares favorably to the KEYVIEW-AWARE method, showing that path adaptation is a viable approach to handling bandwidth variation.
C1 [Zhao, Shanghong; Ooi, Wei Tsang] Natl Univ Singapore, Singapore 117548, Singapore.
   [Carlier, Axel; Morin, Geraldine; Charvillat, Vincent] Univ Toulouse, Toulouse, France.
C3 National University of Singapore; Universite de Toulouse
RP Zhao, SH (corresponding author), Natl Univ Singapore, Singapore 117548, Singapore.
EM shzhao17@comp.nus.edu.sg; ooiwt@comp.nus.edu.sg;
   axel.carlier@enseeiht.fr; morin@n7.fr; charvi@n7.fr
RI Ooi, Wei Tsang/HLW-5142-2023; Ooi, Wei Tsang/AAE-7810-2019
OI Ooi, Wei Tsang/0000-0001-8994-1736; Ooi, Wei Tsang/0000-0001-8994-1736
CR Al-Regib G., 2005, IEEE T MULTIMEDIA, V7, P1149
   Al-Regib G., 2005, ACM T GRAPHIC, V24, P182
   Andújar C, 2004, COMPUT GRAPH FORUM, V23, P499, DOI 10.1111/j.1467-8659.2004.00781.x
   [Anonymous], 2010, P ACM SIGGRAPH EUROG, DOI [DOI 10.2312/SCA/SCA10/139-148, 10.2312/SCA/SCA10/139-148]
   [Anonymous], 2011, P 6 INT C FDN DIGITA
   [Anonymous], ACM MULTIMEDIA
   Burelli P, 2011, LECT NOTES COMPUT SC, V6815, P25, DOI 10.1007/978-3-642-22571-0_3
   Burtnyk N., 2006, Proc. of Symposium on Interactive 3D Graphics and Games, P167
   Chen JYC, 2007, IEEE T SYST MAN CY A, V37, P1063, DOI 10.1109/TSMCA.2007.904779
   Chen Z., 2005, MULTIMEDIA SYST, V10, P230
   Cheng Wei, 2007, P AUGSBURG GERMANY 1, P737, DOI [10.1145/1291233.1291399, DOI 10.1145/1291233.1291399]
   Cheng Wei, 2008, P 18 INT WORKSH NETW, P9, DOI DOI 10.1145/1496046.1496049
   De Silva R. N., 2009, P 17 ACM INT C MULT, P881
   De Silva RN, 2010, NOSSDAV 2010: PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P123
   Dutagaci H., 2010, P ACM WORKSH 3D OBJ, P45, DOI DOI 10.1145/1877808.1877819
   Feixas M, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462056
   Han SR, 2010, IEEE IMAGE PROC, P2945, DOI 10.1109/ICIP.2010.5652185
   Harris A F., 2002, Proc. NOSSDAV, P43
   Hoppe H, 1998, COMPUT GRAPH-UK, V22, P27, DOI 10.1016/S0097-8493(97)00081-2
   KAMADA T, 1988, COMPUT VISION GRAPH, V41, P43, DOI 10.1016/0734-189X(88)90116-8
   Khan Azam., 2005, Proceedings of the 2005 symposium on Interactive 3D graphics and games, I3D '05, P73, DOI DOI 10.1145/1053427.1053439
   Li H, 2006, ACM T MULTIM COMPUT, V2, P282, DOI 10.1145/1201730.1201733
   Park SB, 2003, IEEE IMAGE PROC, P773
   Park SB, 2006, IEEE T MULTIMEDIA, V8, P885, DOI 10.1109/TMM.2006.879914
   Plemenos D., 1996, P INT C COMP GRAPH V
   Ranon R, 2010, LECT NOTES COMPUT SC, V6133, P91, DOI 10.1007/978-3-642-13544-6_9
   Sokolov D, 2008, VISUAL COMPUT, V24, P173, DOI 10.1007/s00371-007-0182-z
   Tang ZY, 2011, MULTIMED TOOLS APPL, V51, P779, DOI 10.1007/s11042-010-0634-8
   Tian DT, 2006, SIGNAL PROCESS-IMAGE, V21, P396, DOI 10.1016/j.image.2006.01.003
   Vazquez P.-P., 2001, Vision, Modeling, and Visualization 2001. Proceedings, P273
   Vázquez PP, 2009, VISUAL COMPUT, V25, P441, DOI 10.1007/s00371-009-0326-4
   Yan ZD, 2001, IEEE T CIRC SYST VID, V11, P860, DOI 10.1109/76.931112
   Yannakakis GN, 2010, USER MODEL USER-ADAP, V20, P313, DOI 10.1007/s11257-010-9078-0
   Zhao S., 2013, P 4 ACM MULT SYST C, P178
NR 34
TC 7
Z9 7
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2014
VL 10
IS 1
SU S
SI SI
AR 13
DI 10.1145/2537854
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA2EF
UT WOS:000330907200005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, L
   Rui, Y
AF Zhang, Lei
   Rui, Yong
TI Image Search-From Thousands to Billions in 20 Years
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Documentation; Performance; Review; image retrieval; Web
   image search; content-based; visual representation; image feature;
   global feature; local feature; indexing; big data; image knowledge base
ID RELEVANCE FEEDBACK; SCALE; RETRIEVAL; REPRESENTATION; RECOGNITION;
   FEATURES; DATABASE; SCENE; SIFT
AB This article presents a comprehensive review and analysis on image search in the past 20 years, emphasizing the challenges and opportunities brought by the astonishing increase of dataset scales from thousands to billions in the same time period, which was witnessed first-hand by the authors as active participants in this research area. Starting with a retrospective review of three stages of image search in the history, the article highlights major breakthroughs around the year 2000 in image search features, indexing methods, and commercial systems, which marked the transition from stage two to stage three. Subsequent sections describe the image search research from four important aspects: system framework, feature extraction and image representation, indexing, and big data's potential. Based on the review, the concluding section discusses open research challenges and suggests future research directions in effective visual representation, image knowledge base construction, implicit user feedback and crowdsourcing, mobile image search, and creative multimedia interfaces.
C1 [Zhang, Lei; Rui, Yong] Microsoft Res Asia, 5 Danling St, Beijing 100080, Peoples R China.
C3 Microsoft; Microsoft Research Asia
RP Zhang, L (corresponding author), Microsoft Res Asia, 5 Danling St, Beijing 100080, Peoples R China.
EM leizhang@microsoft.com; yongrui@microsoft.com
CR Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469
   [Anonymous], 2004, P 20 ACM S COMP
   [Anonymous], ENCY DATA WAREHOUSIN
   [Anonymous], 2010, Proceedings of the 23rd International Conference on Computational Linguistics
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2006, Book Annosearch: Image auto-annotation by search, DOI DOI 10.1109/CVPR.2006.58
   [Anonymous], 2009, P 17 ACM INT C MULTI
   [Anonymous], 2011, ACM T INTEL SYST TEC
   [Anonymous], 1981, Proceedings of the 7th International Joint Conference on Artificial Intelligence
   [Anonymous], 2012, P 21 INT C WORLD WID
   [Anonymous], P SPIE C STOR RETR I
   [Anonymous], GOOD VIS LIF
   [Anonymous], P C NEUR INF PROC SY
   [Anonymous], 2010, Fundamentals of Computational Neuroscience
   [Anonymous], P NIPS WORKSH STAT M
   [Anonymous], PROC IEEE INT CONF A
   [Anonymous], IEEE MULTIMEDIA MAG
   [Anonymous], 2008, Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
   [Anonymous], P 20 ACM INT C MULT
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Blaser A., 1979, Lecture Notes in Computer Science), V81
   Cao Y, 2011, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2011.5995460
   Cao Y, 2010, PROC CVPR IEEE, P3352, DOI 10.1109/CVPR.2010.5540021
   Cao Yang, 2010, P 18 ACM INT C MULT, P1605
   Chang N. S., 1980, Pictorial information systems, P288
   CHANG NS, 1980, IEEE T SOFTWARE ENG, V6, P519, DOI 10.1109/TSE.1980.230801
   CHANG SK, 1988, IEEE T SOFTWARE ENG, V14, P681, DOI 10.1109/32.6147
   CHANG SK, 1981, COMPUTER, V14, P13, DOI [10.1109/C-M.1981.220241, 10.1109/C-M.1981.220243]
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan X., 2005, P 7 ACM SIGMM INT WO, P143
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gordo A, 2011, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2011.5995505
   Harris C., 1988, P ALV VIS C, P5210
   Hays J, 2008, PROC CVPR IEEE, P3436
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   He JF, 2011, PROC CVPR IEEE, P753, DOI 10.1109/CVPR.2011.5995518
   Hua G., 2007, P IEEE INT C COMPUTE
   Jain R., 1993, ACM SIGMOD Rec., V22, P57
   Jing F., 2006, PROC MM 06, P377, DOI DOI 10.1145/1180639.1180720
   Kadir T, 2004, LECT NOTES COMPUT SC, V3021, P228
   Ke Y, 2004, PROC CVPR IEEE, P506
   KOENDERINK JJ, 1984, BIOL CYBERN, V50, P363, DOI 10.1007/BF00336961
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li B, 2011, PROC CVPR IEEE, P1737
   Li J, 2008, NEUROCOMPUTING, V71, P1771, DOI 10.1016/j.neucom.2007.11.032
   Li X., 2006, MULTIMEDIA 06, P607, DOI DOI 10.1145/1180639.1180764
   Li ZW, 2007, LECT NOTES COMPUT SC, V4577, P33
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Ma WY, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P568, DOI 10.1109/ICIP.1997.647976
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173
   Nister David, 2006, CVPR
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Olston Christopher, 2010, Foundations and Trends in Information Retrieval, V4, P175, DOI 10.1561/1500000017
   Quack T., 2004, Proc. ACM Multimedia, P508
   Robinson J. J., 1981, Paper, 32nd Annual Meeting of the European Association for Animal Production
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Shrivastava A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024188
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Tsai D, 2011, IEEE I CONF COMP VIS, P611, DOI 10.1109/ICCV.2011.6126295
   Tuytelaars T, 2004, INT J COMPUT VISION, V59, P61, DOI 10.1023/B:VISI.0000020671.28016.e8
   Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017
   von Ahn Luis., 2004, CHI, DOI DOI 10.1145/985692.985733
   VONAHN L, 2006, P SIGCHI C HUM FACT, P55, DOI DOI 10.1145/1124772.1124782
   Wang B, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P353, DOI 10.1109/ICME.2006.262509
   Wang JG, 2010, MAGNETIC FRINGE FIELDS AND INTERFERENCE IN HIGH INTENSITY ACCELERATORS, P1
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Wang X, 2012, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON APAC 2011
   Wang XJ, 2010, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2010.5540046
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   Witkin A.P., 1983, P INT JOINT C ART IN, P1019, DOI DOI 10.1007/978-3-8348-9190-729
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zhang X, 2012, PROC CVPR IEEE, P2058, DOI 10.1109/CVPR.2012.6247910
   Zhang X, 2012, IEEE T MULTIMEDIA, V14, P995, DOI 10.1109/TMM.2012.2186121
   Zhang X, 2009, IEEE I CONF COMP VIS, P1103, DOI 10.1109/ICCV.2009.5459354
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 99
TC 37
Z9 42
U1 0
U2 42
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 36
DI 10.1145/2490823
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700006
DA 2024-07-18
ER

PT J
AU Yang, Y
   Yang, Y
   Shen, HT
AF Yang, Yang
   Yang, Yi
   Shen, Heng Tao
TI Effective Transfer Tagging from Image to Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Video tagging; cross media;
   transfer learning; semi-supervised learning
AB Recent years have witnessed a great explosion of user-generated videos on the Web. In order to achieve an effective and efficient video search, it is critical for modern video search engines to associate videos with semantic keywords automatically. Most of the existing video tagging methods can hardly achieve reliable performance due to deficiency of training data. It is noticed that abundant well-tagged data are available in other relevant types of media (e. g., images). In this article, we propose a novel video tagging framework, termed as Cross-Media Tag Transfer (CMTT), which utilizes the abundance of well-tagged images to facilitate video tagging. Specifically, we build a "cross-media tunnel" to transfer knowledge from images to videos. To this end, an optimal kernel space, in which distribution distance between images and video is minimized, is found to tackle the domain-shift problem. A novel cross-media video tagging model is proposed to infer tags by exploring the intrinsic local structures of both labeled and unlabeled data, and learn reliable video classifiers. An efficient algorithm is designed to optimize the proposed model in an iterative and alternative way. Extensive experiments illustrate the superiority of our proposal compared to the state-of-the-art algorithms.
C1 [Yang, Yang; Shen, Heng Tao] Univ Queensland, UQ, St Lucia, Qld 4072, Australia.
   [Yang, Yi] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
C3 University of Queensland; Carnegie Mellon University
RP Yang, Y (corresponding author), Univ Queensland, UQ, Rm 625,Bld 78, St Lucia, Qld 4072, Australia.
EM yang.yang@itee.uq.edu.au
RI Yang, Yi/B-9273-2017; yang, yang/GWB-9426-2022; yang,
   yang/GVT-5210-2022; Shen, Heng Tao/ABD-5331-2021; yang,
   yang/HGT-7999-2022; Lang, Ming/HIK-0758-2022
OI Yang, Yi/0000-0002-0512-880X; 
CR [Anonymous], 2009, P 17 ACM INT C MULTI
   [Anonymous], 2008, AAAI
   [Anonymous], P INT C MULT
   [Anonymous], 2007, TREC VID RETR EV
   [Anonymous], 2009, NUSWIDE: A real-world web image database from National University of Singapore, DOI DOI 10.1145/1646396.1646452
   [Anonymous], 2007, P 15 ACM INT C MULT, DOI DOI 10.1145/1291233.1291430
   [Anonymous], 2011, P 25 AAAI C ART INT
   [Anonymous], P 19 ACM INT C MULT
   [Anonymous], P ACM MM
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Cortes C., 2009, P 25 C UNCERTAINTY A, P109, DOI DOI 10.5555/1795114.1795128
   Dai Wenyuan, 2007, P 24 INT C MACHINE L, P193
   Duan LX, 2010, PROC CVPR IEEE, P1959, DOI 10.1109/CVPR.2010.5539870
   Fan JP, 2010, PROC CVPR IEEE, P802, DOI 10.1109/CVPR.2010.5540135
   Grant M., 2020, CVX MATLAB SOFTWARE
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang W, 2008, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2008.4711716
   Jiang Y.-G., 2009, Proc. ACM MM, P155, DOI DOI 10.1145/1631272.1631296
   Jiang YG, 2009, IEEE I CONF COMP VIS, P1420, DOI 10.1109/ICCV.2009.5459295
   Loui A.C., 2008, KODAKS CONSUMER VIDE
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Rockafellar R, 2005, Variational Analysis
   Tang JH, 2008, IEEE T MULTIMEDIA, V10, P620, DOI 10.1109/TMM.2008.921853
   Tang Jinhui., 2009, Proceedings of ACM international conference on Multimedia, P223, DOI DOI 10.1145/1631272.1631305
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang M., 2011, IEEE T MULTIMEDIA, V14, P1
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, COMPUT VIS IMAGE UND, V113, P384, DOI 10.1016/j.cviu.2008.08.003
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Yang Y., 2012, P ACM MULT C
   Yang Y, 2011, PROC CVPR IEEE, P881, DOI 10.1109/CVPR.2011.5995499
   Yang Y, 2011, WORLD WIDE WEB, V14, P133, DOI 10.1007/s11280-010-0099-8
   Yang Yi., 2009, Proceedings of the 17th ACM international conference on Multimedia, P175
   Yao Y, 2010, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2010.5539857
   Zha Z.-J., 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587384
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zha ZJ, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823747
   Zhu X., 2008, COMPUTER SCI T, V1530
NR 42
TC 52
Z9 57
U1 1
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2013
VL 9
IS 2
AR 14
DI 10.1145/2457450.2457456
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 144MS
UT WOS:000318944400006
DA 2024-07-18
ER

PT J
AU Shen, ZJ
   Zimmermann, R
AF Shen, Zhijie
   Zimmermann, Roger
TI ISP-Friendly P2P Live Streaming: A Roadmap to Realization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurement; Design; Performance; P2P; streaming; ISP-friendly; traffic
   locality
AB Peer-to-Peer (P2P) applications generate large amounts of Internet network traffic. The wide-reaching connectivity of P2P systems is creating resource inefficiencies for network providers. Recent studies have demonstrated that localizing cross-ISP (Internet service provider) traffic can mitigate this challenge. However, bandwidth sensitivity and display quality requirements complicate the ISP-friendly design for live streaming systems. To this date, although some prior techniques focusing on live streaming systems exist, the correlation between traffic localization and streaming quality guarantee has not been well explored. Additionally, the proposed solutions are often not easy to apply in practice.
   In our presented work, we demonstrate that the cross-ISP traffic of P2P live streaming systems can be significantly reduced with little impact on the streaming quality. First, we analytically investigate and quantify the tradeoff between traffic localization and streaming quality guarantee, determining the lower bound of the inter-AS (autonomous system) streaming rate below which streaming quality cannot be preserved. Based on the analysis, we further propose a practical ISP-friendly solution, termed IFPS, which requires only minor changes to the peer selection mechanism and can easily be integrated into both new and existing systems. Additionally, the significant opportunity for localizing traffic is underscored by our collected traces from PPLive, which also enabled us to derive realistic parameters to guide our simulations. The experimental results demonstrate that IFPS reduces cross-ISP traffic from 81% up to 98% while keeping streaming quality virtually unaffected.
C1 [Shen, Zhijie; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 National University of Singapore
RP Shen, ZJ (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
EM z-shen@comp.nus.edu.sg; rogerz@comp.nus.edu.sg
RI Zimmermann, Roger/D-7944-2015
OI Zimmermann, Roger/0000-0002-7410-2590
FU Center of Social Media Innovations for Communications (CoSMIC); Media
   Development Authority of Singapore
FX This work is supported by the Center of Social Media Innovations for
   Communications (CoSMIC), sponsored by the Media Development Authority of
   Singapore.
CR Abboud O, 2009, INT PARALL DISTRIB P, P1790
   AGGARWAL V., 2008, P ANN JOINT C IEEE C
   Aggarwal V, 2007, ACM SIGCOMM COMP COM, V37, P31, DOI 10.1145/1273445.1273449
   [Anonymous], CISC VIS NETW IND FO
   Bindal R., 2006, IEEE INT C DISTRIBUT, P66, DOI [10.1109/ICDCS.2006.48, DOI 10.1109/ICDCS.2006.48]
   CHEN Y.-F., 2007, P ACM INT WORKSH NET
   CHIU D, 2009, IEEE T PARALL DISTR
   Choffnes DR, 2008, ACM SIGCOMM COMP COM, V38, P363, DOI 10.1145/1402946.1403000
   Dimitropoulos X, 2007, ACM SIGCOMM COMP COM, V37, P31, DOI 10.1145/1198255.1198259
   Gao LX, 2001, IEEE ACM T NETWORK, V9, P733, DOI 10.1109/90.974527
   GUPTA I., 2007, P INT S HIGH PERF DI
   HEFEEDA M, 2008, P ACM CONEXT C
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Huang C, 2007, ACM SIGCOMM COMP COM, V37, P133, DOI 10.1145/1282427.1282396
   Izal M, 2004, LECT NOTES COMPUT SC, V3015, P1
   James S., 2010, P IEEE 10 INT C PEER, P1
   Karagiannis T., 2005, P 5 ACM SIGCOMM C IN
   Li B, 2008, IEEE INFOCOM SER, P1705
   Liang C, 2008, INT CON DISTR COMP S, P53, DOI 10.1109/ICDCS.2008.103
   Liu Y, 2009, INT CON DISTR COMP S, P423, DOI 10.1109/ICDCS.2009.50
   MAGHAREI N., 2009, ISP FRIENDLY LIVE P2
   Magharei N, 2009, IEEE ACM T NETWORK, V17, P1052, DOI 10.1109/TNET.2008.2007434
   Picconi F, 2009, INT CON DISTR COMP S, P413, DOI 10.1109/ICDCS.2009.37
   Ren SS, 2010, IEEE INFOCOM SER
   Ren Shansi., 2006, P INT C DISTRIBUTED, P70, DOI [10.1109/ ICDCS.2006.18, DOI 10.1109/ICDCS.2006.18]
   SHEN Z, 2009, P ACM MULT
   Tomozei DC, 2010, IEEE INFOCOM SER
   WANG J., 2008, P ACM MULT
   Wu C, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1386109.1386112
   Xie HY, 2008, ACM SIGCOMM COMP COM, V38, P351, DOI 10.1145/1402946.1402999
   Xie HY, 2008, LECT NOTES COMPUT SC, V4982, P375
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 32
TC 2
Z9 2
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2012
VL 8
IS 1
SI SI
AR 11
DI 10.1145/2089085.2089088
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 898YN
UT WOS:000300778400003
DA 2024-07-18
ER

PT J
AU Hu, WM
   Zuo, HQ
   Wu, O
   Chen, YF
   Zhang, ZF
   Suter, D
AF Hu, Weiming
   Zuo, Haiqiang
   Wu, Ou
   Chen, Yunfei
   Zhang, Zhongfei
   Suter, David
TI Recognition of Adult Images, Videos, and Web Page Bags
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Human Factors; Skin patch modeling; recognition of
   adult images; recognition of adult videos; recognition of adult Web page
   bags
ID SYSTEM; SUBSPACE; ENGINE
AB In this article, we develop an integrated adult-content recognition system which can detect adult images, adult videos, and adult Web page bags, where a Web page bag consists of a Web page and a predefined number of Web pages linked to it through hyperlinks. In our adult image-recognition algorithm, we model skin patches rather than skin pixels, resulting in better results than state-of-the-art algorithms which model skin pixels. In our adult video-recognition algorithm, information from the accompanying audio section around an image in an adult video is used to obtain a prior classification of the image. The algorithm achieves a better performance than the ones which use image information alone or audio information alone. The adult Web page bag recognition is carried out using multi-instance learning based on the combination of classifying texts, images and videos in Web pages. Both the speed and the accuracy for recognizing the Web adult content are increased, in contrast to recognizing Web pages one-by-one.
C1 [Hu, Weiming; Zuo, Haiqiang; Wu, Ou; Chen, Yunfei] Chinese Acad Sci, Inst Automat, Beijing 100864, Peoples R China.
   [Suter, David] Univ Adelaide, Adelaide, SA 5005, Australia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; University of
   Adelaide
RP Hu, WM (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100864, Peoples R China.
EM wmhu@nlpr.ia.ac.cn
RI Liu, Kai/IST-6808-2023; Chen, Yunfei/H-7820-2014
OI Chen, Yunfei/0000-0002-8682-868X; Suter, David/0000-0001-6306-3023
FU National Science Foundation of China (NSFC) [60825204, 60935002]
FX This work was partially supported by the National Science Foundation of
   China (NSFC) under Grant Nos. 60825204, 60935002.
CR Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   [Anonymous], P INT C IM SCI SYST
   [Anonymous], P IEEE INT C INF INF
   [Anonymous], P INT C COMP VIS THE
   [Anonymous], P WSEAS INT C COMP
   [Anonymous], INT J COMPUT COGNITI
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], THESIS U EDINBURGH
   [Anonymous], ELECT LETT COMPUTER
   [Anonymous], P IND C COMP VIS GRA
   [Anonymous], P INT C MACH LEARN
   [Anonymous], P AS C COMP VIS
   [Anonymous], P IEEE INT C SYST MA
   [Anonymous], 2009, P EUR SIGN PROC C GL
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], P INT C IM PROC
   Aragon CR, 2007, P SOC PHOTO-OPT INS, V6496, P49607, DOI 10.1117/12.703666
   Arentz WA, 2004, COMPUT VIS IMAGE UND, V94, P295, DOI 10.1016/j.cviu.2003.10.007
   Bian W, 2011, IEEE T PATTERN ANAL, V33, P1037, DOI 10.1109/TPAMI.2010.189
   Breiman L., 2001, Mach. Learn., V45, P5
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deselaers T, 2008, INT C PATT RECOG, P3551
   Du RB, 2003, ICON 2003: 11TH IEEE INTERNATIONAL CONFERENCE ON NETWORKS, P325
   Endeshaw T, 2008, IEEE APP IMG PAT, P13
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Forsyth DA, 1999, INT J COMPUT VISION, V32, P63, DOI 10.1023/A:1008145029462
   Hammami M, 2006, IEEE T KNOWL DATA EN, V18, P272, DOI 10.1109/TKDE.2006.34
   Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696, DOI 10.1109/34.1000242
   Hu WM, 2007, IEEE T PATTERN ANAL, V29, P1019, DOI 10.1109/TPAMI.2007.1133
   Ioffe S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1092, DOI 10.1109/ICCV.1999.790398
   Ioffe S, 2001, INT J COMPUT VISION, V43, P45, DOI 10.1023/A:1011179004708
   Jansohn C., 2009, PROC 17 ACM INT C MU, P601, DOI [DOI 10.1145/1631272.1631366, 10.1145/1631272.1631366]
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kovac J, 2003, IEEE REGION 8 EUROCON 2003, VOL B, PROCEEDINGS, P144
   Lee PY, 2005, IEEE T MULTIMEDIA, V7, P1183, DOI 10.1109/TMM.2005.858414
   Lee S, 2009, IEEE T CONSUM ELECTR, V55, P677, DOI 10.1109/TCE.2009.5174439
   Lienhart R, 2009, IEEE INT CON MULTI, P1472, DOI 10.1109/ICME.2009.5202781
   SADKA AH, 2004, P WORKSH IM AN MULT
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Shih JL, 2007, PATTERN RECOGN LETT, V28, P2367, DOI 10.1016/j.patrec.2007.08.002
   Tao DC, 2009, IEEE T PATTERN ANAL, V31, P260, DOI 10.1109/TPAMI.2008.70
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang J, 2000, P 17 INT C MACH LEAR, P1119
   Wang JZ, 1998, LECT NOTES COMPUT SC, V1483, P113, DOI 10.1007/BFb0055310
   Wang JZ, 1998, COMPUT COMMUN, V21, P1355, DOI 10.1016/S0140-3664(98)00203-5
   ZHANG ML, 2008, P IEEE INT C DAT MIN, P688
   Zhou ZH, 2005, APPL INTELL, V22, P135, DOI 10.1007/s10489-005-5602-z
   Zuo H., 2010, International Conference on World Wide Web: ACM, P1227
NR 51
TC 9
Z9 10
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 28
DI 10.1145/2037676.2037685
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 857NR
UT WOS:000297725800009
DA 2024-07-18
ER

PT J
AU Li, FWB
   Lau, RWH
   Kilis, D
   Li, LWF
AF Li, Frederick W. B.
   Lau, Rynson W. H.
   Kilis, Danny
   Li, Lewis W. F.
TI Game-On-Demand: An Online Game Engine based on Geometry Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Performance; Geometry streaming; multiplayer online games; game
   engine; continuous synchronization
ID TIME; ANIMATION
AB In recent years, online gaming has become very popular. In contrast to stand-alone games, online games tend to be large-scale and typically support interactions among users. However, due to the high network latency of the Internet, smooth interactions among the users are often difficult. The huge and dynamic geometry data sets also make it difficult for some machines, such as handheld devices, to run those games. These constraints have stimulated some research interests on online gaming, which may be broadly categorized into two areas: technological support and user-perceived visual quality. Technological support concerns the performance issues while user-perceived visual quality concerns the presentation quality and accuracy of the game. In this article, we propose a game-on-demand engine that addresses both research areas. The engine distributes game content progressively to each client based on the player's location in the game scene. It comprises a two-level content management scheme and a prioritized content delivery scheme to help identify and deliver relevant game content at appropriate quality to each client dynamically. To improve the effectiveness of the prioritized content delivery scheme, it also includes a synchronization scheme to minimize the location discrepancy of avatars (game players). We demonstrate the performance of the proposed engine through numerous experiments.
C1 [Li, Frederick W. B.] Univ Durham, Sch Engn & Comp Sci, Durham, England.
   [Lau, Rynson W. H.; Li, Lewis W. F.] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Kilis, Danny] Morgan Stanley Informat Technol Shanghai Ltd, Shanghai 200002, Peoples R China.
C3 Durham University; City University of Hong Kong
RP Li, FWB (corresponding author), Univ Durham, Sch Engn & Comp Sci, Durham, England.
EM frederick.li@durham.ac.uk; rynson@cs.cityu.edu.hk; kilisd@gmail.com
RI Li, Frederick W. B./AAM-6662-2021
OI LAU, Rynson W H/0000-0002-8957-8129; Li, Frederick W.
   B./0000-0002-4283-4228
FU Research Grants Council of Hong Kong [CityU 116008]
FX This work was partially supported by a GRF grant from the Research
   Grants Council of Hong Kong (RGC Reference Number: CityU 116008).
CR [Anonymous], CSTR941505 STANF U D
   Balmelli L., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P487, DOI 10.1109/ICIP.1999.822944
   BERNIER Y, 2001, P GAM DEV C
   BERNSTEIN PA, 1981, COMPUT SURV, V13, P185, DOI 10.1145/356842.356846
   Botev Jean, 2008, International Journal of Advanced Media and Communication, V2, P331, DOI 10.1504/IJAMC.2008.022219
   Cavagna R., 2006, Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST), P269
   Chan A., 2005, ACM Transactions on Internet Technology, V5, P70, DOI 10.1145/1052934.1052937
   Chang CF, 2002, LECT NOTES COMPUT SC, V2532, P1105
   Chengzheng Sun, 2002, ACM Transactions on Computer-Human Interaction, V9, P1, DOI 10.1145/505151.505152
   Chim J, 2003, IEEE T MULTIMEDIA, V5, P503, DOI 10.1109/TMM.2003.819094
   Chim J. H. P., 1998, Proceedings ACM Multimedia 98, P171, DOI 10.1145/290747.290769
   Cronin Eric., 2002, NETGAMES 02, P67
   DAS TK, 1997, P ACM S VIRT REAL SO, P157
   *DIS STEER COMM, 1998, 1278 IEEE DIS STEER
   Epic Games, UNR ENG
   FALBY JS, 1993, COMPUT GRAPH, V17, P65, DOI 10.1016/0097-8493(93)90052-B
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   GREENHALGH C, 1995, INT CON DISTR COMP S, P27, DOI 10.1109/ICDCS.1995.499999
   Gulliver SR, 2006, ACM T MULTIM COMPUT, V2, P241, DOI 10.1145/1201730.1201731
   Hagsand O, 1996, IEEE MULTIMEDIA, V3, P30, DOI 10.1109/93.486702
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hu S.-Y., 2004, Proceedings of NetGames, Association for Computer Machinery, P129
   Hu S.-Y., 2006, P 11 INT C 3D WEB TE, P57
   Ivanov D, 2000, COMPUT GRAPH FORUM, V19, pC283, DOI 10.1111/1467-8659.00420
   LAMPORT L, 1978, COMMUN ACM, V21, P558, DOI 10.1145/359545.359563
   Lau RWH, 1997, P IEEE VIRT REAL ANN, P20, DOI 10.1109/VRAIS.1997.583040
   Leigh J, 1996, P IEEE VIRT REAL ANN, P253, DOI 10.1109/VRAIS.1996.490535
   Li F.W., 2004, P ACM S VIRT REAL SO, P129
   Li FWB, 1997, COMPUT GRAPH FORUM, V16, pC47, DOI 10.1111/1467-8659.00141
   MACEDONIA MR, 1995, IEEE COMPUT GRAPHICS
   Mauve M, 2004, IEEE T MULTIMEDIA, V6, P47, DOI 10.1109/TMM.2003.819751
   MILLS DL, 1991, IEEE T COMMUN, V39, P1482, DOI 10.1109/26.103043
   Pazzi RWN, 2008, IEEE T INSTRUM MEAS, V57, P1894, DOI 10.1109/TIM.2008.919901
   Saar K., 1999, Proceedings VRML 99. Fourth Symposium on the Virtual Reality Modeling Language, P141, DOI 10.1145/299246.299287
   Singhal S., 1999, Networked Virtual Environments
   Teler E, 2001, COMPUT GRAPH FORUM, V20, pC17, DOI 10.1111/1467-8659.00494
   To D, 2001, PRESENCE-TELEOP VIRT, V10, P62, DOI 10.1162/105474601750182324
   Waters RC, 1997, PRESENCE-TELEOP VIRT, V6, P461, DOI 10.1162/pres.1997.6.4.461
   Zhou S., 2004, ACM Transactions on Modeling and Computer Simulation, V14, P31, DOI 10.1145/974734.974736
NR 39
TC 17
Z9 18
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2011
VL 7
IS 3
AR 19
DI 10.1145/2000486.2000493
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 814DB
UT WOS:000294425100007
DA 2024-07-18
ER

PT J
AU Van Leuken, RH
   Veltkamp, RC
AF Van Leuken, Reinier H.
   Veltkamp, Remco C.
TI Selecting Vantage Objects for Similarity Indexing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Multimedia retrieval;
   indexing; embedding methods; vantage objects
AB Indexing has become a key element in the pipeline of a multimedia retrieval system, due to continuous increases in database size, data complexity, and complexity of similarity measures. The primary goal of any indexing algorithm is to overcome high computational costs involved with comparing the query to every object in the database. This is achieved by efficient pruning in order to select only a small set of candidate matches. Vantage indexing is an indexing technique that belongs to the category of embedding or mapping approaches, because it maps a dissimilarity space onto a vector space such that traditional access methods can be used for querying. Each object is represented by a vector of dissimilarities to a small set of m reference objects, called vantage objects. Querying takes place within this vector space. The retrieval performance of a system based on this technique can be improved significantly through a proper choice of vantage objects. We propose a new technique for selecting vantage objects that addresses the retrieval performance directly, and present extensive experimental results based on three data sets of different size and modality, including a comparison with other selection strategies. The results clearly demonstrate both the efficacy and scalability of the proposed approach.
C1 [Van Leuken, Reinier H.; Veltkamp, Remco C.] Univ Utrecht, NL-3508 TB Utrecht, Netherlands.
C3 Utrecht University
RP Van Leuken, RH (corresponding author), Univ Utrecht, POB 80-089, NL-3508 TB Utrecht, Netherlands.
EM reinier@cs.uu.nl; r.c.veltkamp@cs.uu.nl
CR ARKIN EM, 1991, IEEE T PATTERN ANAL, V13, P209, DOI 10.1109/34.75509
   ARYA S, 1994, PROCEEDINGS OF THE FIFTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P573
   Athitsos V, 2004, PROC CVPR IEEE, P268
   BECKMANN N, 1990, SIGMOD REC, V19, P322, DOI 10.1145/93605.98741
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Böhm C, 2001, ACM COMPUT SURV, V33, P322, DOI 10.1145/502807.502809
   BOZKAYA T, 1999, T DATAB SYST, V24
   Bozkaya Tolga, 1997, P ACM SIGMOD INT C M
   Brisaboa NR, 2006, IEEE INT SYM MULTIM, P881
   BUCKLEY C, 2000, RES DEV INFORM RETRI, P33
   Bustos B, 2003, PATTERN RECOGN LETT, V24, P2357, DOI 10.1016/S0167-8655(03)00065-5
   Chávez E, 2001, ACM COMPUT SURV, V33, P273, DOI 10.1145/502807.502808
   Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426
   Faloutsos C., 1995, P 1995 ACM SIGMOD IN, P163
   Gaede V, 1998, ACM COMPUT SURV, V30, P170, DOI 10.1145/280277.280279
   Giannopoulos P, 2002, LECT NOTES COMPUT SC, V2352, P715
   GUTMAN A, 1984, P ACM SIGMOD INT C M, P47
   Hennig C, 2003, PATTERN RECOGN, V36, P2187, DOI 10.1016/S0031-3203(02)00326-6
   HISTECRU G, 1999, CLUSTER PRESERVING E
   Hjaltason GR, 2003, IEEE T PATTERN ANAL, V25, P530, DOI 10.1109/TPAMI.2003.1195989
   HRISTESCU G, 1999, DIMACS, V8
   Kruskal JB, 1978, MULTIDIMENSIONAL SCA
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   LINIAL N, 1995, COMBINATORICA, V15, P215, DOI 10.1007/BF01200757
   MOKHTARIAN F, 1996, P BRIT MACH VIS C BM
   PEKALSKA E, 1905, PATTERN RECOGN, P189
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   Samet H, 2006, FDN MULTIDIMENSIONAL
   Sellis T., 1987, Proceedings of the Thirteenth International Conference on Very Large Data Bases: 1987 13th VLDB, P507
   TYPKE R, 2003, P 4 INT S MUS INF RE, P107
   van Leuken RH, 2006, INT C PATT RECOG, P453
   Venkateswaran J., 2006, P INT C VERY LARGE D, P906
   Vleugels J, 2002, PATTERN RECOGN, V35, P69, DOI 10.1016/S0031-3203(00)00120-5
   WANG X, 2000, KNOWL INF SYST, P161
   YIANILOS PN, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P311
NR 35
TC 10
Z9 11
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2011
VL 7
IS 3
AR 16
DI 10.1145/2000486.2000490
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 814DB
UT WOS:000294425100004
DA 2024-07-18
ER

PT J
AU Wu, C
   Li, BC
   Zhao, SQ
AF Wu, Chuan
   Li, Baochun
   Zhao, Shuqiao
TI Exploring large-scale peer-to-peer live streaming topologies
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE measurement; peer-to-peer streaming; topology characterization
AB Real-world live peer-to-peer ( P2P) streaming applications have been successfully deployed in the Internet, delivering live multimedia content to millions of users at any given time. With relative simplicity in design with respect to peer selection and topology construction protocols and without much algorithmic sophistication, current-generation live P2P streaming applications are able to provide users with adequately satisfying viewing experiences. That said, little existing research has provided sufficient insights on the time-varying internal characteristics of peer-to-peer topologies in live streaming. This article presents Magellan, our collaborative work with UUSee Inc., Beijing, China, for exploring and charting graph theoretical properties of practical P2P streaming topologies, gaining important insights in their topological dynamics over a long period of time.
   With more than 120 GB worth of traces starting September 2006 from a commercially deployed P2P live streaming system that represents UUSee's core product, we have completed a thorough and in-depth investigation of the topological properties in large-scale live P2P streaming, as well as their evolutionary behavior over time, for example, at different times of the day and in flash crowd scenarios. We seek to explore real-world P2P streaming topologies with respect to their graph theoretical metrics, such as the degree, clustering coefficient, and reciprocity. In addition, we compare our findings with results from existing studies on topological properties of P2P file sharing applications, and present new and unique observations specific to streaming. We have observed that live P2P streaming sessions demonstrate excellent scalability, a high level of reciprocity, a clustering phenomenon in each ISP, and a degree distribution that does not follow the power-law distribution.
C1 [Wu, Chuan; Li, Baochun] Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 1A2, Canada.
   [Zhao, Shuqiao] UUSee Inc, Multimedia Dev Grp, Beijing, Peoples R China.
C3 University of Toronto
RP Wu, C (corresponding author), Univ Toronto, Dept Elect & Comp Engn, 215 Huron St, Toronto, ON M5S 1A2, Canada.
EM chuanwu@eecg.toronto.edu; bli@eecg.toronto.edu; zhaosq@uusee.com
RI baochun, Li/AAD-3188-2022; Wu, Chuan/E-9919-2010
OI Wu, Chuan/0000-0002-3144-4398
CR Adamic LA, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.046135
   [Anonymous], P 5 INT WORKSH PEER
   [Anonymous], [No title captured]
   [Anonymous], P WORKSH REC ADV PEE
   [Anonymous], P 4 INT WORKSH PEER
   Baset SalmanA., 2006, P IEEE INFOCOM
   CHEN KT, 2006, P ACM SIGCOMM C 2006
   CHENG B, 2007, P 6 INT WORKSH PEER
   GARLASCHELLI D, 2004, PHYS REV LETT, V93, P26
   GUMMADI K, 2003, P 19 ACM S OP SYST P
   GUO L, 2005, P INT MEAS C IMC
   HEI X, 2006, WORKSH INT PROT TV I
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   IZAL M, 2004, P 5 PASS ACT MEAS WO
   Jovanovic M., 2001, P 9 TEL FOR TELFOR B
   LI L, 2004, P ACM SIGCOMM C 2004
   Liang J, 2006, COMPUT NETW, V50, P842, DOI 10.1016/j.comnet.2005.07.014
   Ripeanu M, 2002, IEEE INTERNET COMPUT, V6, P50, DOI 10.1109/4236.978369
   SILVERSTON T, 2007, P 1M INT WO IN PRESS
   SILVERSTON T, 2006, P 2 C FUT NETW TECHN
   STEINER M, 2007, P 6 INT WORKSH PEER
   STUTZBACH D, 2006, P INT MEAS C IMC
   STUTZBACH D, 2005, P INT MEAS C IMC
   Watts D. J., 2003, Six degrees: The science of a connected age
   Wu C., 2007, P 27 INT C DISTR COM
   ZHANG X, 2005, P IEEE INFOCOM 2005
NR 26
TC 37
Z9 41
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2008
VL 4
IS 3
AR 19
DI 10.1145/1386109.1386112
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 351OI
UT WOS:000259433300003
DA 2024-07-18
ER

PT J
AU Chen, HY
   Li, SW
AF Chen, Herng-Yow
   Li, Sheng-Wei
TI Exploring many-to-one speech-to-text correlation for web-based language
   learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE design; human factors; cross-media correlation; analysis and
   presentation; speech-to-text alignment; lips sync; computed
   synchronization
AB This article investigates the correlations between multimedia objects (particularly speech and text) involved in language lectures in order to design an effective presentation mechanism for web-based learning. The cross-media correlations are classified into implicit relations (retrieved by computing) and explicit relations (recorded during the preprocessing stage). The implicit temporal correlation between speech and text is primarily to help to negotiate supplementary lecture navigations like tele-pointer movement, lips-sync movement, and content scrolling. We propose a speech-text alignment framework, using an iterative algorithm based on local alignment, to probe many-to-one temporal correlations, and not the one-to-one only. The proposed framework is a more practical method for analyzing general language lectures, and the algorithm's time complexity conforms to the best-possible computation cost, O(nm), without introducing additional computation. In addition, we have shown the feasibility of creating vivid presentations by exploiting implicit relations and artificially simulating some explicit media. To facilitate the navigation of integrated multimedia documents, we develop several visualization techniques for describing media correlations, including guidelines for speech-text correlations, visible-automatic scrolling, and levels of detail of timeline, to provide intuitive and easy-to-use random access mechanisms. We evaluated the performance of the analysis method and human perceptions of the synchronized presentation. The overall performance of the analysis method is that about 99.5% of the words analyzed are of a temporal error within 0.5sec and the subjective evaluation result shows that the synchronized presentation is highly acceptable to human beings.
RP Chen, HY (corresponding author), Dept Comp Sci & Informat Engn, Natl Chi Nan Univ Rd, Puli 545, Nantou Hsein, Taiwan.
EM Herng.yow.chen2005@gmail.com
OI Chen, Herng-Yow/0009-0006-3664-0706
CR Abowd GD, 1999, IBM SYST J, V38, P508, DOI 10.1147/sj.384.0508
   [Anonymous], 2005, VOICE AM
   [Anonymous], PROGR DIG SIGN PROC
   [Anonymous], 1990, COMPUT LANG
   [Anonymous], 1997, ACM SIGACT NEWS
   Bregler C., 1997, P 24 ANN C COMP GRAP, V31, P353, DOI DOI 10.1145/258734.258880
   Chen HY, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P887, DOI 10.1109/MMCS.1999.778605
   Cheng W.-H., 2003, P 5 ACM SIGMM INT WO, P109, DOI DOI 10.1145/973264.973282
   Chu WT, 2005, MULTIMEDIA SYST, V10, P183, DOI 10.1007/s00530-004-0150-7
   CHU WT, 2002, P 10 ACM INT C MULT, P57
   CHU WT, 2001, THESIS
   DOHI H, 1997, P IJCAI 97 WORKSH IN, P17
   GADD TN, 1988, PROGRAM-AUTOM LIBR, V22, P222, DOI 10.1108/eb046999
   HALL PAV, 1980, COMPUT SURV, V12, P381, DOI 10.1145/356827.356830
   HUANG X, 1993, COMPUT SPEECH LANG, V2, P137
   LOPRESTI D, 1999, P 6 INT S STRING PRO, P120
   Moreno Pedro J., 1998, P 5 INT C SPOKEN LAN
   Müller R, 2000, MULTIMEDIA SYST, V8, P158, DOI 10.1007/s005300000042
   NITTA N, 2002, P 8 INT WORKSH MULT, P110
   Okimi K., 1981, IEEE Communications Magazine, V19, P12, DOI 10.1109/MCOM.1981.1090506
   OWEN CB, 1998, COMPUTED SYNCHRONIZA
   Philips L., 1990, Computer Language Magazine, V7, P38
   Pratt W.K, 1978, DIGITAL IMAGE PROCES
   RICHTER H, 1999, GITGVU9930
   Steinmetz R, 1996, IEEE J SEL AREA COMM, V14, P61, DOI 10.1109/49.481694
   WAGNER RA, 1974, J ACM, V21, P168, DOI 10.1145/321796.321811
   WATERMAN MS, 1987, J MOL BIOL, V197, P723, DOI 10.1016/0022-2836(87)90478-5
   WATERS K, 1993, P 2 ACM INT C MULT, P149
   Weide R.L., 1998, The CMU pronouncing dictionary
   *WSML, 1997, NCNU MULT ENGL CLASS
   WU HL, 2002, THESIS TAIWAN U
NR 31
TC 2
Z9 2
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 3
AR 13
DI 10.1145/1236471.1236472
PG 31
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230JB
UT WOS:000250871700001
DA 2024-07-18
ER

PT J
AU Oshima, C
   Nishimoto, K
   Hagita, N
AF Oshima, Chika
   Nishimoto, Kazushi
   Hagita, Norihiro
TI A piano duo support system for parents to lead children to practice
   musical performances
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE experimentation; piano duo; musical expression; score tracking;
   entertainment; support system
AB In this article, we propose "Family Ensemble," a piano duo support system for a musically inept parent and his/her child who is a beginner at playing the piano. The system makes it easier for parents to correctly reproduce a given sequence of pitches along with the child's performance by using score tracking and note-replacement functions. The experiments with this support system showed that the parents can immediately participate in the piano duo. Furthermore, we found that during joint practices using Family Ensemble some subjects discussed musical ideas that they would not have talked about without using the system.
C1 ATR Cognit Informat Sci Labs, Kyoto 6190288, Japan.
   Natl Inst Informat & Commun Technol, Kyoto 6190288, Japan.
   Japan Adv Inst Sci & Technol, Ishikawa, Japan.
C3 National Institute of Information & Communications Technology (NICT) -
   Japan; Japan Advanced Institute of Science & Technology (JAIST)
RP Oshima, C (corresponding author), ATR Cognit Informat Sci Labs, 2-2-2 Hikaridai, Kyoto 6190288, Japan.
EM chika-o@atr.jp
CR AGAY D, 1987, LEARNING PLAY PIANO, V1
   [Anonymous], P 12 ANN ACM INT C M
   Blaine Tina., 2000, P 3 C DESIGNING INTE, P165
   Bloch J.J., 1985, Proceedings of the International Computer Music Conference, P279
   *CASIO, 2000, LK55 FULL SIZ KEYB K
   Dannenberg R, 1984, P 1984 INT COMP MUS, P193
   EVERETT C, 1999, PIANO ED PAGE ARTIST
   Farbood MM, 2004, IEEE COMPUT GRAPH, V24, P50, DOI 10.1109/MCG.2004.1255809
   Grubb L., 1997, P ICMC, P301
   Jennings K., 2003, Music Education International, V2, P3
   KIHARA R, 1991, PIANO LAND, V1
   LYKE J, 2004, PIANO ED PAGE ARTIST
   McCarthy C, 2005, COMPUT EDUC, V44, P173, DOI 10.1016/j.compedu.2004.08.004
   MIYOSHI A, 2000, MUSICA NOVA, V31, P30
   MIYOSHI A, 1977, MIYOSHI PIANO METHOD, V1
   Orio N., 2003, Proc. International Conference on New Interfaces for Musical Expression (NIME), P36, DOI DOI 10.5281/ZENODO.1176547.URL
   OSHIMA C, 2003, P 25 ANN C COGN SCI
   PACHET F, 2003, ACM COMPUTERS ENTERT, V2
   Raphael C, 2001, J COMPUT GRAPH STAT, V10, P487, DOI 10.1198/106186001317115081
   *RENCON, 2000, RENC PERF REND PIAN
   SMOLIAR SW, 1995, P MULT
   SUBOTNICK M, 1997, GHOST
   TAKEUCHI Y, 1995, IPSJ SIGHNOTES MUSIC, P37
   TOYODA KS, 1995, COMPACT DISC JAPANES
   VERCOE B, 1984, P INT COMP MUS C ICM, P301
   ZEIGLER J, 2004, PIANO ED PAGE TIPS P
   [No title captured]
NR 27
TC 9
Z9 9
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 2
AR 9
DI 10.1145/1230812.1230815
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230JA
UT WOS:000250871600003
DA 2024-07-18
ER

PT J
AU Baker, HH
   Bhatti, N
   Tanguay, D
   Sobel, I
   Gelb, D
   Goss, ME
   Culbertson, WB
   Malzbender, T
AF Baker, H. Harlyn
   Bhatti, Nina
   Tanguay, Donald
   Sobel, Irwin
   Gelb, Dan
   Goss, Michael E.
   Culbertson, W. Bruce
   Malzbender, Thomas
TI Understanding Performance in Coliseum, An Immersive Videoconferencing
   System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Measurement; Performance;
   Telepresence; videoconferencing; view synthesis; 3D virtual
   environments; performance measurement; streaming media; network
   applications
ID SCENE
AB Coliseum is a multiuser immersive remote teleconferencing system designed to provide collaborative workers the experience of face-to-face meetings from their desktops. Five cameras are attached to each PC display and directed at the participant. From these video streams, view synthesis methods produce arbitrary-perspective renderings of the participant and transmit them to others at interactive rates, currently about 15 frames per second. Combining these renderings in a shared synthetic environment gives the appearance of having all participants interacting in a common space. In this way, Coliseum enables users to share a virtual world, with acquired-image renderings of their appearance replacing the synthetic representations provided by more conventional avatar-populated virtual worlds. The system supports virtual mobility - participants may move around the shared space - and reciprocal gaze, and has been demonstrated in collaborative sessions of up to ten Coliseum workstations, and sessions spanning two continents.
   Coliseum is a complex software system which pushes commodity computing resources to the limit. We set out to measure the different aspects of resource, network, CPU, memory, and disk usage to uncover the bottlenecks and guide enhancement and control of system performance. Latency is a key component of Quality of Experience for video conferencing. We present how each aspect of the system - cameras, image processing, networking, and display - contributes to total latency. Performance measurement is as complex as the system to which it is applied. We describe several techniques to estimate performance through direct light-weight instrumentation as well as use of realistic end-to-end measures that mimic actual user experience. We describe the various techniques and how they can be used to improve system performance for Coliseum and other network applications. This article summarizes the Coliseum technology and reports on issues related to its performance - its measurement, enhancement, and control.
C1 [Baker, H. Harlyn; Bhatti, Nina; Tanguay, Donald; Sobel, Irwin; Gelb, Dan; Goss, Michael E.; Culbertson, W. Bruce; Malzbender, Thomas] Hewlett Packard Labs, Palo Alto, CA 94304 USA.
C3 Hewlett-Packard
RP Baker, HH (corresponding author), Hewlett Packard Labs, 1501 Page Mill Rd, Palo Alto, CA 94304 USA.
EM harlyn.baker@hp.com; nina.bhatti@hp.com; donald.tanguay@hp.com;
   irwin.sobel@hp.com; dan.gelb@hp.com; mike.goss@hp.com;
   bruce.culbterson@hp.com; malzbender@hp.com
CR [Anonymous], P INT WORKSH IMM TEL
   [Anonymous], P 9 ACM INT C MULT M
   BAKER HH, 2004, P VIS MOD VIS WORKSH, P133
   BAKER HH, 2003, P 11 ACM INT C MULT, P470
   BUCK J, 2001, MORGANKAUFMANN SYSTE, P527
   DEVERNAY F, 1995, P SOC PHOTO-OPT INS, V2567, P62, DOI 10.1117/12.218487
   Gharai L., 2002, P 8 INT C DISTR MULT
   Henderson Peter, 1976, P 3 ACM SIGACT SIGPL, P95, DOI DOI 10.1145/800168.811543
   HIRSH S, 2004, HPL2004140R1
   Lanier J, 2001, SCI AM, V284, P66, DOI 10.1038/scientificamerican0401-66
   LOHSE M, 2003, LECT NOTES COMPUTER, P327
   LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   Narayanan PJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P3, DOI 10.1109/ICCV.1998.710694
   Pesce MarkD., 2002, PROGRAMMING MICROSOF
   POLLEFEYS M, 1999, THESIS ESAT PSI KU L
   Prince S., 2002, P SIGGRAPH 2002 ACM, P238
   Schreer O., 2001, Proc. Intl. Conf. eWork and eBusiness, P184
   Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462
   Slabaugh G, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P704, DOI 10.1109/TDPVT.2002.1024145
   Tanguay D., 2004, HPL2004132
   Wilcox J.R., 2000, VIDEOCONFERENCING WH, V3rd ed.
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   [No title captured]
NR 24
TC 6
Z9 7
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2005
VL 1
IS 2
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DV
UT WOS:000205012300005
DA 2024-07-18
ER

PT J
AU Bulterman, DCA
   Hardman, L
AF Bulterman, Dick C. A.
   Hardman, Lynda
TI Structured Multimedia Authoring
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Languages; Verification; Multimedia authoring; hypermedia;
   synchronization; SMIL
AB Authoring context sensitive, interactive multimedia presentations is much more complex than authoring either purely audiovisual applications or text. Interactions among media objects need to be described as a set of spatio-temporal relationships that account for synchronous and asynchronous interactions, as well as on-demand linking behavior. This article considers the issues that need to be addressed by an authoring environment. We begin with a partitioning of concerns based on seven classes of authoring problems. We then describe a selection of multimedia authoring environments within four different authoring paradigms: structured, timeline, graph and scripting. We next provide observations and insights into the authoring process and argue that the structured paradigm provides the most useful framework for presentation authoring. We close with an example application of the structured multimedia authoring paradigm in the context of our own structure-based system GRiNS.
C1 [Bulterman, Dick C. A.; Hardman, Lynda] CWI, NL-1090 GB Amsterdam, Netherlands.
   [Hardman, Lynda] Tech Univ Eindhoven, NL-5600 MB Eindhoven, Netherlands.
C3 Eindhoven University of Technology
RP Bulterman, DCA (corresponding author), CWI, POB 94079, NL-1090 GB Amsterdam, Netherlands.
EM Dick.Bulterman@cwi.nl; Lynda.Hardman@cwi.nl
CR *AD SYST, 2002, ENC 1 5 DVD ED SOFTW
   [Anonymous], THESIS U AMSTERDAM
   [Anonymous], SMIL 2 0 INTERACTIVE
   BAEKCER R, 1996, P ACM MULT 96 BOST M, P31
   Bailey B., 1998, Proceedings ACM Multimedia 98, P257, DOI 10.1145/290747.290779
   Buchanan MC, 2005, ACM T MULTIM COMPUT, V1, P60, DOI 10.1145/1047936.1047942
   BULTERMAN DCA, 1991, PROCEEDINGS OF THE SUMMER 1991 USENIX CONFERENCE, P137
   BULTERMAN DCA, 1997, P WWW 7 BRISB AUSTR
   BULTERMAN DCA, 2003, P 2003 ACM S DOC ENG, P32
   Bulterman Dick C.A., 2004, INT MULT C P 12 ANN, V10, P492
   EUN S, 1994, MULTIMEDIA SYSTEMS, V2, P129
   FALKOVYCH K, 2004, P 1 INT WORKSH INT D
   FUJIKAWA K, 1991, PROCEEDINGS OF THE SUMMER 1991 USENIX CONFERENCE, P75
   HAMAKAWA R, 1994, MULTIMEDIA SYSTEMS, V2, P26
   HARDMAN L, 1994, COMMUN ACM, V37, P50, DOI 10.1145/175235.175239
   Hardman L., 2000, New Review of Hypermedia and Multimedia, V6, P89, DOI 10.1080/13614560008914719
   Hardman L., 1993, Proceedings ACM Multimedia 93, P283, DOI 10.1145/166266.168402
   HARDMAN L, 1995, STATE ART REPORT EUR
   HODGES ME, 1989, IEEE SOFTWARE    JAN, P37
   *IETF, 1996, REALT STREAM PROT
   *ISO, 1999, 144961999MPEG4 ISOIE
   Koegel J. F., 1993, P ED MEDIA 93 WORLD, P286
   KOEGKEL F, 1994, MULTIMEDIA SYSTEMS
   LITTLE TDC, 1990, PROCEEDINGS : 6TH INTERNATIONAL CONFERENCE ON DATA ENGINEERING, P20
   *MACR, 1997, AUTH VERS 4 DIR VERS
   Ogawa R., 1990, P 1 EUROPEAN C HYPER, P38
   Rubin B., 1989, SIGCHI Bulletin, V21, P78, DOI 10.1145/70609.70620
   SIOCHI A, 1991, INTERACT MULTIMED, V2, P5
   Soares LFG, 2000, MULTIMEDIA SYST, V8, P118, DOI 10.1007/s005300050155
   Tran-Thuong T., 2002, International Journal of Software Engineering and Knowledge Engineering, V12, P473, DOI 10.1142/S0218194002001037
   van Rossum G., 1993, Proceedings ACM Multimedia 93, P183, DOI 10.1145/166266.166287
   Van Rossum G., 2003, An introduction to Python
   *WAM RES GROUP, 2003, LIMSEE2 AUTH SYST SM
NR 33
TC 49
Z9 54
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2005
VL 1
IS 1
BP 89
EP 109
DI 10.1145/1047936.1047943
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DU
UT WOS:000205012200007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tang, GY
   Gao, XY
   Chen, ZY
AF Tang, Geyu
   Gao, Xingyu
   Chen, Zhenyu
TI Learning Semantic Representation on Visual Attribute Graph for Person
   Re-identification and Beyond
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; attribute-based image retrieval; Graph Neural
   Network; representation learning
ID NEURAL-NETWORKS
AB Person re-identification (re-ID) aims tomatch pedestrian pairs captured fromdifferent cameras. Recently, various attribute-based models have been proposed to combine the pedestrian attribute as an auxiliary semantic information to learn a more discriminative pedestrian representation. However, these methods usually directly concatenate the visual branch and attribute branch embeddings as the final pedestrian representation, which ignores the semantic relation between the pedestrian revealed by attribute similarity. To capture and explore such semantic relation, we propose a unified pedestrian representation framework, called Visual Attribute Graph Embedding Network (VAGEN), to simultaneously learn attribute and visual representation. We unify the visual embedding and attribute similarity into a Visual Attribute Graph, where pedestrian is considered as a node and attribute similarity as an edge. Then, we learn graph node embedding to generate pedestrian representation through Graph Neural Network. Except for this unified representation for visual and attribute embeddings, VAGEN also conducts implicitly hard example mining for visual similar false-positive results, which has not been explored yet among existing attribute-based methods. We conduct extensive empirical studies on several person re-ID datasets to evaluate our proposed algorithm from different aspects. The results show that our proposed method outperforms state-of-the-art techniques with considerable margins.
C1 [Tang, Geyu] Chinese Acad Sci, Inst Microelect, Beijing, Peoples R China.
   [Tang, Geyu] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Gao, Xingyu] Chinese Acad Sci, Inst Microelect, Beijing, Peoples R China.
   [Chen, Zhenyu] State Grid Corp China, BigData Ctr, Beijing, Peoples R China.
   [Chen, Zhenyu] China Elect Power Res Inst, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Microelectronics, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences; Institute of Microelectronics, CAS; State
   Grid Corporation of China
RP Gao, XY (corresponding author), Chinese Acad Sci, Inst Microelect, Beijing, Peoples R China.
EM tanggeyu@ime.ac.cn; gxy9910@gmail.com; czy9907@gmail.com
RI Chen, Zhenyu/AAA-6776-2022; Gao, Xingyu/AAL-3288-2021
OI Gao, Xingyu/0000-0002-4660-8092
FU STI 2030-Major Projects [2022ZD0208700]; National Natural Science
   Foundation of China [61702491]
FX This work was supported in part by the STI 2030-Major Projects No.
   2022ZD0208700 and National Natural Science Foundation of China under
   Grant No. 61702491.
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Bao LQ, 2019, IEEE COMPUT SOC CONF, P1496, DOI 10.1109/CVPRW.2019.00191
   Bruna J, 2014, Arxiv, DOI [arXiv:1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen DP, 2018, PROC CVPR IEEE, P8649, DOI 10.1109/CVPR.2018.00902
   Cheng D, 2018, PATTERN RECOGN, V82, P94, DOI 10.1016/j.patcog.2018.05.007
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2040, DOI 10.1145/3240508.3240550
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hintze JL, 1998, AM STAT, V52, P181, DOI 10.2307/2685478
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, Arxiv, DOI [arXiv:1502.03167, DOI 10.48550/ARXIV.1502.03167]
   Jiang B, 2019, Arxiv, DOI arXiv:1911.10544
   Kim W, 2018, LECT NOTES COMPUT SC, V11205, P760, DOI 10.1007/978-3-030-01246-5_45
   Kingma D. P., 2014, arXiv
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Leng QM, 2020, IEEE T CIRC SYST VID, V30, P1092, DOI 10.1109/TCSVT.2019.2898940
   Levie R, 2019, IEEE T SIGNAL PROCES, V67, P97, DOI 10.1109/TSP.2018.2879624
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li SZ, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107016
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo H, 2019, PATTERN RECOGN, V94, P53, DOI 10.1016/j.patcog.2019.05.028
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   PeterWelinder SteveBranson., 2010, Caltech-UCSD Birds 200
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schumann A, 2017, IEEE COMPUT SOC CONF, P1435, DOI 10.1109/CVPRW.2017.186
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Sperduti A, 1997, IEEE T NEURAL NETWOR, V8, P714, DOI 10.1109/72.572108
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang J, 2017, IEEE T CIRC SYST VID, V27, P513, DOI 10.1109/TCSVT.2016.2586851
   Wang KJ, 2018, CAAI T INTELL TECHNO, V3, P219, DOI 10.1049/trit.2018.1001
   Wang MJ, 2020, Arxiv, DOI [arXiv:1909.01315, DOI 10.48550/ARXIV.1909.01315]
   Wu YM, 2020, IEEE T IMAGE PROCESS, V29, P8821, DOI 10.1109/TIP.2020.3001693
   Wu ZH, 2019, Arxiv, DOI arXiv:1901.00596
   Yan YC, 2020, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR42600.2020.00297
   Yan YC, 2019, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR.2019.00226
   Yang JR, 2020, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR42600.2020.00335
   Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94
   Zhang ZW, 2020, Arxiv, DOI [arXiv:1812.04202, DOI 10.1109/TKDE.2020.2981333]
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou J, 2021, Arxiv, DOI [arXiv:1812.08434, DOI 10.1016/J.AIOPEN.2021.01.001]
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou SP, 2017, PROC CVPR IEEE, P5028, DOI 10.1109/CVPR.2017.534
NR 65
TC 3
Z9 3
U1 4
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 206
DI 10.1145/3487044
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200029
DA 2024-07-18
ER

PT J
AU Zeng, RF
   Su, M
   Yu, RY
   Wang, XW
AF Zeng, Rongfei
   Su, Mai
   Yu, Ruiyun
   Wang, Xingwei
TI CD<SUP>2</SUP>: Fine-grained 3D Mesh Reconstruction with Twice Chamfer
   Distance
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3Dreconstruction; machine learning; chamfer distance; mesh deformation
ID EARTH MOVERS DISTANCE
AB Monocular 3D reconstruction is to reconstruct the shape of object and its other information from a single RGB image. In 3D reconstruction, polygon mesh, with detailed surface information and low computational cost, is the most prevalent expression form obtained from deep learning models. However, the state-of-theart schemes fail to directly generate well-structured meshes, and we identify that most meshes have severe Vertices Clustering (VC) and Illegal Twist (IT) problems. By analyzing the mesh deformation process, we pinpoint that the inappropriate usage of Chamfer Distance (CD) loss is a root cause of VC and IT problems in deep learning model. In this article, we initially demonstrate these two problems induced by CD loss with visual examples and quantitative analyses. Then, we propose a fine-grained reconstruction method CD2 by employing Chamfer distance twice to perform a plausible and adaptive deformation. Extensive experiments on two 3D datasets and comparisons with five latest schemes demonstrate that our CD2 directly generates a well-structured mesh and outperforms others in terms of several quantitative metrics.
C1 [Zeng, Rongfei; Su, Mai; Yu, Ruiyun; Wang, Xingwei] Northeastern Univ, 195 Chuangxin St, Shenyang 110169, Peoples R China.
C3 Northeastern University - China
RP Zeng, RF; Wang, XW (corresponding author), Northeastern Univ, 195 Chuangxin St, Shenyang 110169, Peoples R China.
EM zengrf@swc.neu.edu.cn; sumai1998@foxmail.com; yury@mail.neu.edu.cn;
   wangxw@mail.neu.edu.cn
RI 苏, 迈/ADL-1588-2022
OI 苏, 迈/0000-0002-7991-3114
FU National Natural Science Foundation of China [62172083, 62032013,
   62072094]; Fundamental Research Funds for the Central Universities
   [N2217007]; NUDT Fund [ZK41]
FX This work was supported by National Natural Science Foundation of China
   (No. 62172083, 62032013, and 62072094), Fundamental Research Funds for
   the Central Universities (No. N2217007), and NUDT Fund (No. ZK41).
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Nguyen AD, 2019, IEEE I CONF COMP VIS, P8627, DOI 10.1109/ICCV.2019.00872
   Barrow Harry G, 1977, Tech. Rep.
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351
   Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3
   Botev A, 2021, Arxiv, DOI arXiv:2111.05458
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3183, DOI 10.1109/TIP.2019.2957935
   Cignoni P., 2008, P EUR IT CHAPT C, P129, DOI [DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136, 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136]
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Digne J, 2014, IMAGE PROCESS ON LIN, V4, P149, DOI 10.5201/ipol.2014.81
   Duggal S, 2022, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR52688.2022.00159
   EDELSBRUNNER H, 1994, ACM T GRAPHIC, V13, P43, DOI 10.1145/174462.156635
   Gkioxari G, 2019, IEEE I CONF COMP VIS, P9784, DOI 10.1109/ICCV.2019.00988
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Jiongchao Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P295, DOI 10.1007/978-3-030-58589-1_18
   Li CL, 2019, PROC CVPR IEEE, P11959, DOI 10.1109/CVPR.2019.01224
   Li JX, 2019, IEEE I CONF COMP VIS, P361, DOI 10.1109/ICCV.2019.00045
   Lim I, 2019, COMPUT GRAPH FORUM, V38, P99, DOI 10.1111/cgf.13792
   Lin CH, 2018, AAAI CONF ARTIF INTE, P7114
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Meagher D., 1980, Tech. Rep. IPLTR-80-111
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Nie YY, 2020, PROC CVPR IEEE, P52, DOI 10.1109/CVPR42600.2020.00013
   Oechsle M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5569, DOI 10.1109/ICCV48922.2021.00554
   Pala Pietro., 2019, ACM Transactions on Multimedia Computing, Communications, and Applications, V15, P1
   Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006
   Paschalidou D, 2021, PROC CVPR IEEE, P3203, DOI 10.1109/CVPR46437.2021.00322
   Pontes JK, 2019, LECT NOTES COMPUT SC, V11361, P365, DOI 10.1007/978-3-030-20887-5_23
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Shin DY, 2019, IEEE I CONF COMP VIS, P2172, DOI 10.1109/ICCV.2019.00226
   Shirdhonkar S, 2008, PROC CVPR IEEE, P2494, DOI 10.1109/CVPR.2008.4587662
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Tulsiani S, 2017, IEEE T PATTERN ANAL, V39, P719, DOI 10.1109/TPAMI.2016.2574713
   Urbach Dahlia, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P545, DOI 10.1007/978-3-030-58621-8_32
   Wagner N, 2022, VISIGRAPP, P811, DOI 10.5220/0010772500003124
   Wang LJ, 2020, Arxiv, DOI arXiv:2010.00321
   Wang M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P961, DOI 10.1145/3123266.3123340
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wen X., 2022, P IEEE CVF C COMP VI, P3803
   Wu SZ, 2020, PROC CVPR IEEE, P1, DOI 10.1109/CVPR42600.2020.00008
   Wu T., 2021, arXiv
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xie HZ, 2019, IEEE I CONF COMP VIS, P2690, DOI 10.1109/ICCV.2019.00278
   Yu H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3514248
   Zeng RF, 2022, IEEE NETWORK, V36, P229, DOI 10.1109/MNET.112.2100706
   Zhang C., 2021, P IEEE CVF C COMP VI, P8833
   Zhou QY, 2018, Arxiv, DOI arXiv:1801.09847
   Zou Yi-sheng, 2008, Application Research of Computers, V25, P2906
NR 51
TC 0
Z9 0
U1 3
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 181
DI 10.1145/3582694
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200004
DA 2024-07-18
ER

PT J
AU Hung, HY
   Cong, RM
   Yang, LH
   Du, L
   Wang, C
   Kwong, S
AF Hung, Heyu
   Cong, Runmin
   Yang, Lianhe
   Du, Ling
   Wang, Cong
   Kwong, Sam
TI Feedback Chain Network for Hippocampus Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; hippocampus segmentation; multi-level feature fusion;
   feedback chain
AB The hippocampus plays a vital role in the diagnosis and treatment of many neurological disorders. Recent years, deep learning technology has made great progress in the field of medical image segmentation, and the performance of related tasks has been constantly refreshed. In this article, we focus on the hippocampus segmentation task and propose a novel hierarchical feedback chain network. The feedback chain structure unit learns deeper and wider feature representation of each encoder layer through the hierarchical feature aggregation feedback chains and achieves feature selection and feedback through the feature handover attention module. Then, we embed a global pyramid attention unit between the feature encoder and the decoder to further modify the encoder features, including the pairwise pyramid attention module for achieving adjacent attention interaction and the global context modeling module for capturing the long-range knowledge. The proposed approach achieves state-of-the-art performance on three publicly available datasets compared with existing hippocampus segmentation approaches. The code and results can be found from the link of https://github.com/easymoneysniper183/sematic_seg.
C1 [Hung, Heyu; Yang, Lianhe; Du, Ling] Tiangong Univ, Sch Comp Sci & Technol, Tianjin, Peoples R China.
   [Cong, Runmin] Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
   [Wang, Cong] Huawei Technol, Distributed & Parallel Software Lab, Shenzhen, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Tiangong University; Beijing Jiaotong University; Huawei Technologies;
   City University of Hong Kong
RP Cong, RM (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
EM huangheyu22@gmail.com; rmcong@bjtu.edu.cn
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261
CR Artaechevarria X, 2009, IEEE T MED IMAGING, V28, P1266, DOI 10.1109/TMI.2009.2014372
   Ataloglou D, 2019, NEUROINFORMATICS, V17, P563, DOI 10.1007/s12021-019-09417-y
   Bremner JD, 2000, AM J PSYCHIAT, V157, P115, DOI 10.1176/ajp.157.1.115
   Cao L, 2018, MULTIMED TOOLS APPL, V77, P29669, DOI 10.1007/s11042-017-5581-1
   Cárdenas-Peña D, 2019, J MED IMAGING, V6, DOI 10.1117/1.JMI.6.1.014003
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cui RX, 2019, IEEE J BIOMED HEALTH, V23, P2099, DOI 10.1109/JBHI.2018.2882392
   Fischl B, 2012, NEUROIMAGE, V62, P774, DOI 10.1016/j.neuroimage.2012.01.021
   Folle L., 2019, BILDVERARBEITUNG MED, P68
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Girum KB, 2021, IEEE T MED IMAGING, V40, P1542, DOI 10.1109/TMI.2021.3060497
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025
   Isensee F, 2018, Arxiv, DOI arXiv:1809.10486
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918
   Lian CF, 2018, MED IMAGE ANAL, V46, P106, DOI 10.1016/j.media.2018.02.009
   Lin F, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3440694
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Liu DG, 2018, CHIN CONT DECIS CONF, P113, DOI [10.1109/CCDC.2018.8407115, 10.1109/SERA.2018.8477213]
   Liu Hui, 2020, ACM T INTEL SYST TEC, V11, P1, DOI DOI 10.1145/3386090
   Liu MH, 2020, NEUROIMAGE, V208, DOI 10.1016/j.neuroimage.2019.116459
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Madan CR, 2017, NEUROIMAGE, V156, P14, DOI 10.1016/j.neuroimage.2017.04.065
   Oktay O, 2018, Arxiv, DOI arXiv:1804.03999
   Prince M, 2015, World Alzheimer Report 2015. The global impact of dementia. An analysis of prevalence, incidence, cost and trends
   Punn NS, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3376922
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rousseau F, 2011, IEEE T MED IMAGING, V30, P1852, DOI 10.1109/TMI.2011.2156806
   Saribudak A, 2020, IEEE ACM T COMPUT BI, V17, P608, DOI 10.1109/TCBB.2018.2870363
   Shi JL, 2021, IEEE J BIOMED HEALTH, V25, P504, DOI 10.1109/JBHI.2020.2994114
   Styner M, 2004, MED IMAGE ANAL, V8, P197, DOI 10.1016/j.media.2004.06.004
   Suk HI, 2015, BRAIN STRUCT FUNCT, V220, P841, DOI 10.1007/s00429-013-0687-3
   Tanveer M, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3344998
   Thyreau B, 2018, MED IMAGE ANAL, V43, P214, DOI 10.1016/j.media.2017.11.004
   Wang BH, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322983
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang ZZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446618
   Yuan Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321512
   Zarpalas Dimitrios, 2014, IEEE J Transl Eng Health Med, V2, P1800116, DOI 10.1109/JTEHM.2014.2297953
   Zhang QJ, 2021, IEEE T IMAGE PROCESS, V30, P1305, DOI 10.1109/TIP.2020.3042084
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou YX, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3397464
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu HC, 2019, FRONT NEUROINFORM, V13, DOI 10.3389/fninf.2019.00030
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
   Zotti C, 2019, IEEE J BIOMED HEALTH, V23, P1119, DOI 10.1109/JBHI.2018.2865450
NR 53
TC 0
Z9 0
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 133
DI 10.1145/3571744
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xing, K
   Li, T
   Wang, XH
AF Xing, Kai
   Li, Tao
   Wang, Xuanhan
TI ProposalVLAD with Proposal-Intra Exploring for Temporal Action Proposal
   Generation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video analysis; temporal action proposal generation; ProposalVLAD
AB Temporal action proposal generation aims to localize temporal segments of human activities in videos. Current boundary-based proposal generation methods can generate proposals with precise boundary but often suffer from the inferior quality of confidence scores used for proposal retrieving. In this article, we propose an effective and end-to-end action proposal generation method, named ProposalVLAD, with Proposal-Intra Exploring Network (PVPI-Net). We first propose a ProposalVLAD module to dynamically generate global features of the entire video, then we combine the global features and proposal local features to generate the final feature representations for all candidate proposals. Then, we design a novel Proposal-Intra Loss function (PI-Loss) to generate more reliable proposal confidence scores. Extensive experiments on large-scale and challenging datasets demonstrate the effectiveness of our proposed method. Experimental results show that our PVPI-Net achieves significant improvements on two benchmark datasets (i.e., THUMOS'14 and ActivityNet-1.3) and sets new records for temporal action detection task.
C1 [Xing, Kai; Li, Tao; Wang, Xuanhan] Univ Elect Sci & Technol China, Chengdu 611731, Sichuan, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Wang, XH (corresponding author), Univ Elect Sci & Technol China, Chengdu 611731, Sichuan, Peoples R China.
EM xjgnqnl@gmail.com; rheecoder@gmail.com; wxuanhan@hotmail.com
FU National Key Research and Development Program of China [2018AAA0102200];
   National Natural Science Foundation of China [62122018, U22A2097,
   62020106008, 61872064]; Fok Ying-Tong Education Foundation [171106]
FX This work is supported by National Key Research and Development Program
   of China (Grant No. 2018AAA0102200), the National Natural Science
   Foundation of China (Grants No. 62122018, No. U22A2097, No. 62020106008,
   and No. 61872064), Fok Ying-Tong Education Foundation (Grant No.
   171106).
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao J., 2017, P BMVC
   Gao JL, 2020, AAAI CONF ARTIF INTE, V34, P10810
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Ghanem B, 2017, Arxiv, DOI arXiv:1710.08011
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Ignat Oana, 2022, ACMTRANS MULTIMEDIA
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472
   Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin TW, 2018, Arxiv, DOI arXiv:1707.06750
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu H., 2021, J. Mach. Learn. Res., V22, P202
   Liu YF, 2019, PROC CVPR IEEE, P7099, DOI [10.1109/CVPR.2019.00726, 10.1109/CVPR.2019.00372]
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Meng QL, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3350840
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh G, 2016, Arxiv, DOI arXiv:1607.01979
   Su HS, 2021, AAAI CONF ARTIF INTE, V35, P2602
   Tan J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13506, DOI 10.1109/ICCV48922.2021.01327
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2015, Arxiv, DOI arXiv:1507.02159
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang XH, 2023, IEEE T MULTIMEDIA, V25, P979, DOI 10.1109/TMM.2021.3135145
   Wang XH, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1670, DOI 10.1145/3503161.3547811
   Xie SN, 2018, Arxiv, DOI [arXiv:1712.04851, DOI 10.48550/ARXIV.1712.04851]
   Xiong YJ, 2017, Arxiv, DOI arXiv:1703.02716
   Xiong YJ, 2016, Arxiv, DOI arXiv:1608.00797
   Xu M., 2020, CVPR, P10156
   Yang XT, 2019, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2019.00035
   Yixuan Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P68, DOI 10.1007/978-3-030-58517-4_5
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
   Zhang JX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321511
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhu SY, 2020, ACM T SENSOR NETWORK, V16, DOI 10.1145/3399678
NR 52
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 118
DI 10.1145/3571747
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300018
DA 2024-07-18
ER

PT J
AU Xu, K
   Li, WX
   Wang, X
   Hu, XY
   Yan, K
   Wang, XJ
   Dong, X
AF Xu, Kang
   Li, Weixin
   Wang, Xia
   Hu, Xiaoyan
   Yan, Ke
   Wang, Xiaojie
   Dong, Xuan
TI CUR Transformer: A Convolutional Unbiased Regional Transformer for Image
   Denoising
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image denoising; non-local filters; regional self-attention; computer
   vision; local non-overlapped windows
ID NETWORK
AB Image denoising is a fundamental problem in computer vision and multimedia computation. Non-local filters are effective for image denoising. But existing deep learning methods that use non-local computation structures are mostly designed for high-level tasks, and global self-attention is usually adopted. For the task of image denoising, they have high computational complexity and have a lot of redundant computation of uncorrelated pixels. To solve this problem and combine the marvelous advantages of non-local filter and deep learning, we propose a Convolutional Unbiased Regional (CUR) transformer. Based on the prior that, for each pixel, its similar pixels are usually spatially close, our insights are that (1) we partition the image into non-overlapped windows and perform regional self-attention to reduce the search range of each pixel, and (2) we encourage pixels across different windows to communicate with each other. Based on our insights, the CUR transformer is cascaded by a series of convolutional regional self-attention (CRSA) blocks with U-style short connections. In each CRSA block, we use convolutional layers to extract the query, key, and value features, namely Q, K, and V, of the input feature. Then, we partition the Q, K, and V features into local non-overlapped windows and perform regional self-attention within each window to obtain the output feature of this CRSA block. Among different CRSA blocks, we perform the unbiased window partition by changing the partition positions of the windows. Experimental results show that the CUR transformer outperforms the state-of-the-art methods significantly on four low-level vision tasks, including real and synthetic image denoising, JPEG compression artifact reduction, and low-light image enhancement.
C1 [Xu, Kang; Wang, Xia; Hu, Xiaoyan; Wang, Xiaojie; Dong, Xuan] Beijing Univ Posts & Telecommun, Beijing, Peoples R China.
   [Li, Weixin] Beihang Univ, Beijing, Peoples R China.
   [Li, Weixin] Zhongguancun Lab, Beijing, Peoples R China.
   [Yan, Ke] DAMO Acad, Beijing, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beihang University;
   Zhongguancun Laboratory
RP Dong, X (corresponding author), Beijing Univ Posts & Telecommun, Beijing, Peoples R China.
EM xukang981@foxmail.com; weixinli@buaa.edu.cn; wangxia17863982566@163.com;
   hxy@bupt.edu.cn; yankethu@gmail.com; xjwang@bupt.edu.cn;
   dongxuan8811@bupt.edu.cn
FU National Nature Science Foundation of China [61802026, 61806016,
   62076032, 62276018]; CAAI-Huawei MindSpore Open Fund; MindSpore; CANN
   (Compute Architecture for Neural Networks); Ascend AI Processor
FX This work is sponsored by the National Nature Science Foundation of
   China (No. 61802026, 61806016, 62076032, and 62276018) and CAAI-Huawei
   MindSpore Open Fund. We gratefully acknowledge the support of MindSpore,
   CANN (Compute Architecture for Neural Networks) and Ascend AI Processor
   used for this research.
CR Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Adams A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531327
   Brox T, 2008, IEEE T IMAGE PROCESS, V17, P1083, DOI 10.1109/TIP.2008.924281
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chang M., 2020, EUR C COMP VIS, P171
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Chu X., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2102.10882
   Dabov K, 2007, IEEE IMAGE PROC, P313, DOI 10.1109/icip.2007.4378954
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong X, 2020, AAAI CONF ARTIF INTE, V34, P10721
   Dong X, 2021, IEEE T IMAGE PROCESS, V30, P6609, DOI 10.1109/TIP.2021.3096385
   Dong X, 2022, IEEE T VIS COMPUT GR, V28, P1469, DOI 10.1109/TVCG.2020.3022480
   Dong X, 2011, IEEE INT CON MULTI
   Dong X, 2019, AAAI CONF ARTIF INTE, P8255
   Dong X, 2015, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2015.7298671
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Du ZY, 2021, IEEE T AFFECT COMPUT, V12, P565, DOI 10.1109/TAFFC.2019.2940224
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jancsary J, 2012, LECT NOTES COMPUT SC, V7578, P112, DOI 10.1007/978-3-642-33786-4_9
   Joo J, 2014, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2014.35
   Khaw HY, 2019, IET IMAGE PROCESS, V13, P365, DOI 10.1049/iet-ipr.2018.5776
   Kingma D. P., 2014, arXiv
   Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2833, DOI 10.1109/CVPR.2011.5995309
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li WX, 2017, IEEE T MULTIMEDIA, V19, P367, DOI 10.1109/TMM.2016.2616279
   Li WB, 2022, IEEE T COMPUT SOC SY, V9, P667, DOI 10.1109/TCSS.2021.3127935
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liang ZT, 2021, IEEE T IMAGE PROCESS, V30, P2248, DOI 10.1109/TIP.2021.3051486
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Z, 2021, Arxiv, DOI [arXiv:2103.14030, DOI 10.48550/ARXIV.2103.14030]
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265
   MindSpore, ABOUT US
   Ren C, 2021, PROC CVPR IEEE, P8592, DOI 10.1109/CVPR46437.2021.00849
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2021, PROC CVPR IEEE, P12889, DOI 10.1109/CVPR46437.2021.01270
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang X., 2021, IEEE T AFFECT COMPUT
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XP, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355397
   Wang YH, 2012, IEEE T SYST MAN CY B, V42, P1107, DOI 10.1109/TSMCB.2012.2187051
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wu HP, 2021, Arxiv, DOI [arXiv:2103.15808, DOI 10.48550/ARXIV.2103.15808]
   Ying ZQ, 2017, Arxiv, DOI arXiv:1711.00591
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Yuan K, 2021, Arxiv, DOI [arXiv:2103.11816, DOI 10.48550/ARXIV.2103.11816]
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P2058, DOI 10.1109/TPAMI.2020.3026740
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zhang YL, 2019, Arxiv, DOI arXiv:1903.10082
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 69
TC 10
Z9 10
U1 3
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 104
DI 10.1145/3566125
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300004
DA 2024-07-18
ER

PT J
AU Singh, KN
   Singh, OP
   Singh, AK
   Agrawal, AK
AF Singh, K. N.
   Singh, O. P.
   Singh, Amit Kumar
   Agrawal, Amrit Kumar
TI EiMOL: A Secure Medical Image Encryption Algorithm based on Optimization
   and the Lorenz System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Healthcare system; medical image; encryption; optimization; security
AB Nowadays, the demand for digital images from different intelligent devices and sensors has dramatically increased in smart healthcare. Due to advanced low-cost and easily available tools and software, manipulation of these images is an easy task. Thus, the security of digital images is a serious challenge for the content owners, healthcare communities, and researchers against illegal access and fraudulent usage. In this article, a secure medical image encryption algorithm, EiMOL, based on optimization and the Lorenz system, is proposed for smart healthcare applications. In the first stage, an optimized random sequence (ORS) is generated through directedweighted complex network particle swarm optimization using the genetic algorithm (GDWCN-PSO). This random number matrix and the Lorenz system are adopted to encrypt plain medical images, obtaining the cipher messages with a relationship to the plain images. According to our obtained results, the proposed EiMOL encryption algorithm is effective and resistant to the many attacks on benchmark Kaggle and Open-i datasets. Further, extensive experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art approaches.
C1 [Singh, K. N.; Singh, O. P.; Singh, Amit Kumar] Natl Inst Technol Patna, Dept CSE, Patna 800005, Bihar, India.
   [Agrawal, Amrit Kumar] Galgotias Coll Engn & Technol, Dept CSE, Greater Noida 201310, Uttar Pradesh, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Patna; Galgotias College of Engineering & Technology (GCET)
RP Singh, AK (corresponding author), Natl Inst Technol Patna, Dept CSE, Patna 800005, Bihar, India.
EM knsinghait@gmail.com; omprakash7667@gmail.com; amit.singh@nitp.ac.in;
   agrawal.amrit4@gmail.com
RI SINGH, OM PRAKASH/GLS-8702-2022; Singh, Kedar Nath/HKV-0830-2023; SINGH,
   OM PRAKASH/AFR-7033-2022; Singh, Amit Kumar/D-1300-2015
OI SINGH, OM PRAKASH/0000-0003-2582-5669; Singh, Kedar
   Nath/0000-0001-7857-5945; Singh, Amit Kumar/0000-0001-7359-2068
CR Al-Hazaimeh OM, 2019, NEURAL COMPUT APPL, V31, P2395, DOI 10.1007/s00521-017-3195-1
   Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Amine K, 2022, J CIRCUIT SYST COMP, V31, DOI 10.1142/S0218126622500979
   Anand A, 2023, IEEE T COMPUT SOC SY, V10, P2033, DOI 10.1109/TCSS.2022.3140862
   Bharadwaj V., 2022, IEEE MULTIMEDIA, V29, P34
   Bharti V, 2021, ACM T INTERNET TECHN, V21, DOI 10.1145/3397679
   Bhuiyan MN, 2021, IEEE INTERNET THINGS, V8, P10474, DOI 10.1109/JIOT.2021.3062630
   Chai XL, 2022, NONLINEAR DYNAM, V108, P2671, DOI 10.1007/s11071-022-07328-3
   Hayat U, 2019, SIGNAL PROCESS, V155, P391, DOI 10.1016/j.sigpro.2018.10.011
   Hossain MS, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3505281
   kaggle, US
   Kaur M, 2021, MULTIDIM SYST SIGN P, V32, P281, DOI 10.1007/s11045-020-00739-8
   Lakshmi C, 2021, NEURAL COMPUT APPL, V33, P6671, DOI 10.1007/s00521-020-05447-9
   Luo YL, 2022, SOFT COMPUT, V26, P5409, DOI 10.1007/s00500-021-06554-y
   Masood F, 2022, WIRELESS PERS COMMUN, V127, P1405, DOI 10.1007/s11277-021-08584-z
   Nardo LG, 2019, CHAOS SOLITON FRACT, V123, P69, DOI 10.1016/j.chaos.2019.03.026
   opencellid, US
   Ravichandran D, 2021, MED BIOL ENG COMPUT, V59, P589, DOI 10.1007/s11517-021-02328-8
   Saravanan S, 2021, SOFT COMPUT, V25, P5299, DOI 10.1007/s00500-020-05528-w
   Sarosh P, 2022, MULTIMED TOOLS APPL, V81, P7253, DOI 10.1007/s11042-021-11812-0
   Sharma N, 2022, J ELECTRON IMAGING, V31, DOI 10.1117/1.JEI.31.4.041207
   Singh AK, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3422816
   Singh KN, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498342
   Singh OP, 2022, COMPUT COMMUN, V191, P368, DOI 10.1016/j.comcom.2022.05.010
   Song W, 2022, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03643-6
   Souyah A, 2016, NONLINEAR DYNAM, V86, P639, DOI 10.1007/s11071-016-2912-0
   Toktas A, 2022, NEURAL COMPUT APPL, V34, P4295, DOI 10.1007/s00521-021-06552-z
   Wang BW, 2022, IEEE T NETW SCI ENG, V9, P2188, DOI 10.1109/TNSE.2022.3157867
   Wang XY, 2015, OPT LASER ENG, V73, P53, DOI 10.1016/j.optlaseng.2015.03.022
   Wang XY, 2012, SIGNAL PROCESS, V92, P1101, DOI 10.1016/j.sigpro.2011.10.023
   Wu Y, 2011, Cyber J Multidiscip J Sci Technol J Sel Areas Telecommun (JSAT), V1, P31
   Ye GD, 2018, NONLINEAR DYNAM, V94, P745, DOI 10.1007/s11071-018-4391-y
   Yin SL, 2021, EVOL INTELL, V14, P1817, DOI 10.1007/s12065-020-00440-6
NR 33
TC 14
Z9 14
U1 7
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 94
DI 10.1145/3561513
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300019
DA 2024-07-18
ER

PT J
AU Tang, YM
   Yu, Y
AF Tang, Yiming
   Yu, Yi
TI Query-Guided Prototype Learning with Decoder Alignment and Dynamic
   Fusion in Few-Shot Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Few-shot segmentation; prototype learning; query set
AB Few-shot segmentation aims to segment objects belonging to a specific class under the guidance of a few annotated examples. Most existing approaches follow the prototype learning paradigm and generate category prototypes by squeezing masked feature maps extracted from images in the support set. These support prototypes may lead to inaccurate predictions when directly compared with features extracted from the query set due to the considerable distribution discrepancy between support and query features. We propose a query-guided prototype learning architecture to address this problem from two aspects: (i) We propose a cross-alignment loss for training the segmentation decoder. This loss function will help the decoder improve its robustness against the distribution discrepancy between support and query features. (ii) We build a dynamic fusion module to strengthen the original support prototype with another prototype extracted from query features. Experiments show that our method achieves promising results compared to previous prototype learning methods on PASCAL-5(i) and COCO-20(i) datasets.
C1 [Tang, Yiming] Fudan Univ, 220 Handan Rd, Shanghai 200433, Peoples R China.
   [Yu, Yi] Natl Inst Informat, 2-1-2 Hitotsubashi,Chiyoda Ku, Tokyo 1018430, Japan.
C3 Fudan University; Research Organization of Information & Systems (ROIS);
   National Institute of Informatics (NII) - Japan
RP Tang, YM (corresponding author), Fudan Univ, 220 Handan Rd, Shanghai 200433, Peoples R China.
EM ymtang18@fudan.edu.cn; yiyu@nii.ac.jp
CR Boudiaf M, 2021, PROC CVPR IEEE, P13974, DOI 10.1109/CVPR46437.2021.01376
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dong N., 2018, P BRIT MACH VIS C
   Dosovitskiy Alexey, 2021, ICLR
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Li G, 2021, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR46437.2021.00823
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BH, 2021, PROC CVPR IEEE, P9742, DOI 10.1109/CVPR46437.2021.00962
   Liu WD, 2020, PROC CVPR IEEE, P4164, DOI 10.1109/CVPR42600.2020.00422
   Liu Y., 2020, P EUR C COMP VIS ECC, P142, DOI DOI 10.1007/978-3-030-58545-79
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Rakelly K, 2018, CONDITIONAL NETWORKS
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shaban A, 2017, ARXIV PREPRINT ARXIV, DOI 10.5244/C.31.167
   Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535
   Snell J, 2017, ADV NEUR IN, V30
   Tian ZH, 2020, IEEE INTERNET THINGS, V7, P3901, DOI [10.1109/TPAMI.2020.3013717, 10.1109/JIOT.2019.2951620]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei XS, 2019, IEEE T IMAGE PROCESS, V28, P6116, DOI 10.1109/TIP.2019.2924811
   Xie GS, 2021, PROC CVPR IEEE, P5471, DOI 10.1109/CVPR46437.2021.00543
   Xiu-ShenWei Yang, 2021, ADV NEURAL INFORM PR, V34
   Zhang BF, 2021, PROC CVPR IEEE, P8308, DOI 10.1109/CVPR46437.2021.00821
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 41
TC 3
Z9 3
U1 2
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 84
DI 10.1145/3555314
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300009
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Zhang, FH
   Jin, Y
   Cen, YG
   Voronin, V
   Wan, SH
AF Zhang, Yue
   Zhang, Fanghui
   Jin, Yi
   Cen, Yigang
   Voronin, Viacheslav
   Wan, Shaohua
TI Local Correlation Ensemble with GCN Based on Attention Features for
   Cross-domain Person Re-ID
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-domain; cluster; person Re-ID; GCN; attention
ID REIDENTIFICATION; INFORMATION
AB Person re-identification (Re-ID) has achieved great success in single-domain. However, it remains a challenging task to adapt a Re-ID model trained on one dataset to another one. Unsupervised domain adaption (UDA) was proposed to migrate a model from a labeled source domain to an unlabeled target domain. The main difference in the cross-domain is different background styles. Although the style transfer approach effectively reduces inter-domain gaps, it ignores the reduction of intra-class differences. Clustering-based pipelinesmaintain state-of-the-art performance for UDA by learning domain-independent features; however, most existing models do not sufficiently exploit the rich unlabeled samples in target domains due to unsatisfactory clustering. Thus, we propose a novel local correlation ensemble model that focuses on the diversity of intra-class information and the reliability of class centers. Specifically, a pedestrian attention module is proposed to enable the encoder to pay more attention to the person's features to relieve interference caused by the shared background style. Furthermore, we propose a priority-distance graph convolutional network (PDGCN) module that employs a graph convolutional network network to predict the priority of a node as a class center and then calculates the distance between nodes with high priority values to screen out the class center nodes. Finally, the encoder features ( local) and PDGCN features (context-aware) are combined to perform person Re-ID. The results of experiments on the large-scale public Re-ID datasets verified the effectiveness of the proposed method.
C1 [Zhang, Yue; Zhang, Fanghui; Cen, Yigang] Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
   [Zhang, Yue; Zhang, Fanghui; Cen, Yigang] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing, Peoples R China.
   [Jin, Yi] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing, Peoples R China.
   [Voronin, Viacheslav] Moscow State Univ Technol STANKIN, Ctr Cognit Technol & Machine Vis, Moscow, Russia.
   [Wan, Shaohua] Univ Elect Sci & Technol China, Shenzhen Inst Adv Study, Shenzhen, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Beijing
   Jiaotong University; Moscow State Technology University Stankin;
   Shenzhen Institute for Advanced Study, UESTC; University of Electronic
   Science & Technology of China
RP Cen, YG (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.; Cen, YG (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing, Peoples R China.
EM 17112065@bjtu.edu.cn; 18112013@bjtu.edu.cn; yjin@bjtu.edu.cn;
   ygcen@bjtu.edu.cn; voroninslava@gmail.com; shaohua.wan@ieee.org
RI Cen, Yigang/AAC-1999-2019; Wan, Shaohua/L-8492-2019; Voronin, Viacheslav
   V/H-7334-2013; 张, 悦/JFS-1131-2023; Wan, Shaohua/B-9243-2014
OI Voronin, Viacheslav V/0000-0001-8114-6383; Zhang,
   Fanghui/0000-0001-5739-7131; Wan, Shaohua/0000-0001-7013-9081; Jin,
   Yi/0000-0001-8408-3816
FU National Key R&D Program of China [2021YFE0110500]; National Natural
   Science Foundation of China [61872034, 61972030, 62062021, 62011530042];
   Beijing Municipal Natural Science Foundation [4202055]; RFBR
   [20-57-53012]; NSFC [FSFS-2020-0031];  [NoFSFS-2020-0031]
FX This work was supported in part by the National Key R&D Program of China
   2021YFE0110500, in part by the National Natural Science Foundation of
   China under Grant 61872034, 61972030, 62062021, and 62011530042, in part
   by the Beijing Municipal Natural Science Foundation under Grant 4202055,
   in part by RFBR and NSFC according to the research project No20-57-53012
   and by project under Grant NoFSFS-2020-0031.
CR Amigó E, 2009, INFORM RETRIEVAL, V12, P461, DOI 10.1007/s10791-008-9066-8
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Chiang WL, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P257, DOI 10.1145/3292500.3330925
   Dai YX, 2021, IEEE T IMAGE PROCESS, V30, P7815, DOI 10.1109/TIP.2021.3104169
   Dai ZZ, 2021, Arxiv, DOI [arXiv:2103.11568, 10.48550/arXiv.2103.11568]
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, ADV NEURAL INFORM PR
   Ge Yixiao, 2020, P 8 INT C LEARNING R
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang B, 2021, IEEE T MULTIMEDIA, V24, P3218, DOI 10.1109/TMM.2021.3095789
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kan SC, 2020, PATTERN RECOGN, V99, DOI 10.1016/j.patcog.2019.107086
   Kan SC, 2019, SIGNAL PROCESS-IMAGE, V78, P494, DOI 10.1016/j.image.2019.08.009
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Lei Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13366, DOI 10.1109/CVPR42600.2020.01338
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Li Yu-Jhe, 2018, P IEEE C COMPUTER VI
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo H, 2019, PATTERN RECOGN, V94, P53, DOI 10.1016/j.patcog.2019.05.028
   Mekhazni Djebril, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P159, DOI 10.1007/978-3-030-58583-9_10
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Pan HH, 2022, KNOWL-BASED SYST, V236, DOI 10.1016/j.knosys.2021.107300
   Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Qian W, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3162, DOI 10.1145/3474085.3475462
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Vinh NX, 2010, J MACH LEARN RES, V11, P2837
   Wang D., 2020, P IEEECVF C COMPUTER
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu AC, 2019, IEEE I CONF COMP VIS, P6921, DOI 10.1109/ICCV.2019.00702
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zhang Y, 2020, IEEE MULTIMEDIA, V27, P23, DOI 10.1109/MMUL.2020.2999445
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
NR 53
TC 44
Z9 47
U1 22
U2 33
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 56
DI 10.1145/3542820
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000006
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Xiang, T
   Zeng, HH
   Chen, BW
   Guo, SW
AF Xiang, Tao
   Zeng, Honghong
   Chen, Biwen
   Guo, Shangwei
TI BMIF: Privacy-preserving Blockchain-based Medical Image Fusion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Medical image fusion; blockchain; privacy protection; consensus
   mechanism; fully homomorphic encryption
ID TRANSFORM
AB Medical image fusion generates a fused image containing multiple features extracted from different source images, and it is of great help in clinical analysis and diagnosis. However, training a deep learning model for image fusion usually requires enormous computing power, especially for large volumes of medical data. Meanwhile, the privacy of images is also a critical issue. In this article, we propose a privacy-preserving blockchain-based medical image fusion (BMIF) framework. First, to ensure fusion performance, we design a newmedical image fusion model based on convolutional neural network and Inception network and integrate the proposed model into the consensus process of blockchain. Next, to save computing power of blockchain, we design a consensus mechanism by requesting consensus nodes to train the fusion model instead of calculating useless hash values in traditional blockchain. Then, to protect data privacy, we further present an efficient homomorphic encryption to realize the training of fusion model on encrypted medical data. Finally, we conduct theoretical analysis and extensive experiments on public datasets to evaluate the feasibility and the performance of our proposed BMIF. The results exhibit that BMIF is efficient and secure, and our medical image fusion network performs better than state-of-the-art approaches.
C1 [Xiang, Tao; Zeng, Honghong; Chen, Biwen; Guo, Shangwei] Chongqing Univ, Coll Comp Sci, Chongqing, Peoples R China.
C3 Chongqing University
RP Xiang, T (corresponding author), Chongqing Univ, Coll Comp Sci, Chongqing, Peoples R China.
EM txiang@cqu.edu.cn; hhzeng@cqu.edu.cn; macrochen@cqu.edu.cn;
   swguo@cqu.edu.cn
RI Xiang, Tao/N-3706-2016
OI Xiang, Tao/0000-0002-9439-4623; CHEN, BIWEN/0000-0002-7314-8271; Zeng,
   hh/0000-0001-5561-856X
FU National Natural Science Foundation of China [U20A20176, 62072062,
   62102050, 62102052]; Natural Science Foundation of Chongqing, China
   [cstc2019jcyjjqX0026]; China Postdoctoral Science Foundation [BX2021399]
FX This work was supported by the National Natural Science Foundation of
   China (Nos. U20A20176, 62072062, 62102050 and 62102052), the Natural
   Science Foundation of Chongqing, China (No. cstc2019jcyjjqX0026), and
   China Postdoctoral Science Foundation (No. BX2021399).
CR Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   Ball M., 2017, Paper 2017/203., V2017, P203
   Bui T, 2020, IEEE T MULTIMEDIA, V22, P2858, DOI 10.1109/TMM.2020.2967640
   Canetti R, 2001, ANN IEEE SYMP FOUND, P136, DOI 10.1109/sfcs.2001.959888
   Chenli C, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON BLOCKCHAIN AND CRYPTOCURRENCY (ICBC), P19, DOI [10.1109/BLOC.2019.8751419, 10.1109/bloc.2019.8751419]
   Cheon JH, 2017, LECT NOTES COMPUT SC, V10624, P409, DOI 10.1007/978-3-319-70694-8_15
   Chou ED, 2018, Arxiv, DOI [arXiv:1811.09953, 10.48550/ARXIV.1811.09953]
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Fan FD, 2019, Arxiv, DOI arXiv:1906.00225
   Ghimire S, 2020, IEEE T MULTIMEDIA, V22, P108, DOI 10.1109/TMM.2019.2925961
   Haddadpour M, 2017, BIOMED J, V40, P219, DOI 10.1016/j.bj.2017.05.002
   Haghighat M, 2014, I C APPL INF COMM TE, P424
   Hermessi H, 2018, NEURAL COMPUT APPL, V30, P2029, DOI 10.1007/s00521-018-3441-1
   Hesamifard E., 2017, ARXIV
   Himanshi, 2016, ADV INTELL SYST, V379, P1, DOI 10.1007/978-81-322-2517-1_1
   Hou RC, 2020, IEEE T COMPUT IMAG, V6, P640, DOI 10.1109/TCI.2020.2965304
   James AP, 2014, INFORM FUSION, V19, P4, DOI 10.1016/j.inffus.2013.12.002
   Jiang LZ, 2020, IEEE T DEPEND SECURE, V17, P179, DOI 10.1109/TDSC.2017.2751476
   Jin X, 2016, J SENSORS, V2016, DOI 10.1155/2016/8359602
   Johnson KA., The Whole Brain Atlas
   Jung H, 2020, IEEE T IMAGE PROCESS, V29, P3845, DOI 10.1109/TIP.2020.2966075
   Li BY, 2019, IEEE COMPUT SOC CONF, P2802, DOI 10.1109/CVPRW.2019.00339
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li WS, 2019, IEEE T BIO-MED ENG, V66, P1172, DOI 10.1109/TBME.2018.2869432
   Li YF, 2021, LECT NOTES COMPUT SC, V12680, P414, DOI 10.1007/978-3-030-73216-5_28
   Liang Haoran, 2020, ACM T MULTIM COMPUT, V16, p3s
   Liu XB, 2017, NEUROCOMPUTING, V235, P131, DOI 10.1016/j.neucom.2017.01.006
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Madanala Sindhuja, 2016, 2016 International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES), P1089, DOI 10.1109/SCOPES.2016.7955608
   pyfhel, PYFH PYTHON HOM ENCR
   Shreyamsha Kumar BK, 2015, SIGNAL IMAGE VIDEO P, V9, P1193, DOI 10.1007/s11760-013-0556-9
   Singh A. K., 2021, ACM T MULTIM COMPUT, V17
   Singh Amit Kumar, 2021, ACM T MULTIM COMPUT, V17, p2s
   Singh S, 2015, BIOMED SIGNAL PROCES, V18, P91, DOI 10.1016/j.bspc.2014.11.009
   TCIA, CANC IM ARCH TCIA
   Dinh TTA, 2018, IEEE T KNOWL DATA EN, V30, P1366, DOI 10.1109/TKDE.2017.2781227
   Wu Hongjiao, 2021, ACM T MULTIM COMPUT, V17, p2s
   Zhang Chao, 2021, WIREL COMMUN MOB COM, V2021
NR 44
TC 5
Z9 5
U1 3
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
AR 36
DI 10.1145/3531016
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800010
DA 2024-07-18
ER

PT J
AU Xu, KX
   Zhang, HJ
   Long, KP
   Wang, JQ
   Sun, L
AF Xu, Kexin
   Zhang, Haijun
   Long, Keping
   Wang, Jianquan
   Sun, Lei
TI DRL based Joint Affective Services Computing and Resource Allocation in
   ISTN
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Affective services; computing offloading; resource allocation; deep
   reinforcement learning
ID SATELLITE; MANAGEMENT; INTERNET
AB Affective services will become a research hotspot in artificial intelligence (AI) in the next decade. In this paper, a novel service paradigm combined with wireless communication in integrated satellite-terrestrial network (ISTN) is proposed. On this basis, an affective services computing offloading and transmission network (ASCTN) with a three-tier computation architecture is proposed, which is able to assist users to obtain affective computing services and regulate emotions. The optimization problem is investigated in the ASCTN, which is a discrete, non-linear, and non-convex problem with the limitation of computation ability of satellite and transmit power. Specifically, with the objective to minimize the cost utility related to latency and energy consumption, a joint affective services tasks computing offloading strategy, sub-channel, and power allocation algorithm based on dueling deep Q-network (Dueling-DQN) is proposed, which is in possession of better stability. The simulation results reveal the effectiveness of the optimization algorithm in terms of the cost utility in the ASCTN system.
C1 [Xu, Kexin; Zhang, Haijun; Long, Keping] Univ Sci & Technol Beijing, Beijing Adv Innovat Ctr Mat Genome Engn, Beijing Engn & Technol Res Ctr Convergence Networ, 30 Xueyuan Rd, Beijing, Peoples R China.
   [Wang, Jianquan; Sun, Lei] Univ Sci & Technol Beijing, Sch Automat Sci & Elect Engn, 30 Xueyuan Rd, Beijing, Peoples R China.
C3 University of Science & Technology Beijing; University of Science &
   Technology Beijing
RP Zhang, HJ (corresponding author), Univ Sci & Technol Beijing, Beijing Adv Innovat Ctr Mat Genome Engn, Beijing Engn & Technol Res Ctr Convergence Networ, 30 Xueyuan Rd, Beijing, Peoples R China.; Wang, JQ (corresponding author), Univ Sci & Technol Beijing, Sch Automat Sci & Elect Engn, 30 Xueyuan Rd, Beijing, Peoples R China.
EM xukexin@xs.ustb.edu.cn; haijunzhang@ieee.org; longkeping@ustb.edu.cn;
   wangjianquan@ustb.edu.cn; sun_lei@ustb.edu.cn
RI guo, ppdop/KAL-9865-2024; WU, SHAN/KGM-5484-2024
OI Zhang, Haijun/0000-0002-0236-6482; Xu, Kexin/0000-0003-0672-4360
FU National Key R&D Program of China [2020YFB1806103]; National Natural
   Science Foundation of China [62225103]; Beijing Natural Science
   Foundation [L212004]; China University IndustryUniversity-Research
   Collaborative Innovation Fund [2021FNA05001]
FX This work is supported in part by the National Key R&D Program of China
   (2020YFB1806103), the National Natural Science Foundation of China under
   Grant 62225103, Beijing Natural Science Foundation (L212004), and China
   University IndustryUniversity-Research Collaborative Innovation Fund
   (2021FNA05001).
CR Arti MK, 2016, IEEE T VEH TECHNOL, V65, P10173, DOI 10.1109/TVT.2016.2529661
   Cambria E, 2016, IEEE INTELL SYST, V31, P102, DOI 10.1109/MIS.2016.31
   Chen M, 2015, IEEE WIREL COMMUN, V22, P20, DOI 10.1109/MWC.2015.7054715
   Chen SZ, 2020, CHINA COMMUN, V17, P156, DOI 10.23919/JCC.2020.12.011
   Chen X, 2015, IEEE T PARALL DISTR, V26, P974, DOI 10.1109/TPDS.2014.2316834
   Chu JH, 2021, IEEE INTERNET THINGS, V8, P1959, DOI 10.1109/JIOT.2020.3015995
   Cui GF, 2021, IEEE SYST J, V15, P3958, DOI 10.1109/JSYST.2020.3017710
   Feng J, 2020, IEEE INTERNET THINGS, V7, P6214, DOI 10.1109/JIOT.2019.2961707
   Fu B, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2754167
   Gokhale V, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3052821
   Hu SH, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3328997
   Huang L, 2020, IEEE T MOBILE COMPUT, V19, P2581, DOI 10.1109/TMC.2019.2928811
   Jia J, 2019, IEEE T MULTIMEDIA, V21, P1853, DOI 10.1109/TMM.2018.2887016
   Jiang CX, 2020, IEEE T WIREL COMMUN, V19, P4685, DOI 10.1109/TWC.2020.2986114
   Jiao J, 2020, IEEE INTERNET THINGS, V7, P3230, DOI 10.1109/JIOT.2020.2966503
   Doan KN, 2020, IEEE T COMMUN, V68, P630, DOI 10.1109/TCOMM.2019.2947418
   Krishnan P, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3377390
   Liu XN, 2022, IEEE INTERNET THINGS, V9, P14818, DOI 10.1109/JIOT.2021.3112907
   Mai SJ, 2020, IEEE T MULTIMEDIA, V22, P122, DOI 10.1109/TMM.2019.2925966
   Qiu C, 2019, IEEE T VEH TECHNOL, V68, P5871, DOI 10.1109/TVT.2019.2907682
   Tang QQ, 2021, IEEE INTERNET THINGS, V8, P9164, DOI 10.1109/JIOT.2021.3056569
   van Hasselt H, 2015, Arxiv, DOI [arXiv:1509.06461, DOI 10.48550/ARXIV.1509.06461]
   Wang AQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3226037
   Wang AY, 2021, IEEE T VEH TECHNOL, V70, P900, DOI 10.1109/TVT.2020.3047453
   Wang LN, 2021, IEEE T WIREL COMMUN, V20, P1065, DOI 10.1109/TWC.2020.3030704
   Wang SF, 2019, IEEE T AFFECT COMPUT, V10, P155, DOI 10.1109/TAFFC.2017.2702749
   Wang XH, 2015, IEEE T AFFECT COMPUT, V6, P286, DOI 10.1109/TAFFC.2015.2400917
   Wang ZY, 2016, PR MACH LEARN RES, V48
   Wu DP, 2021, IEEE J SEL AREA COMM, V39, P479, DOI 10.1109/JSAC.2020.3020677
   Xie RC, 2020, IEEE NETWORK, V34, P224, DOI 10.1109/MNET.011.1900369
   Yu S, 2022, IEEE INTERNET THINGS, V9, P5742, DOI 10.1109/JIOT.2021.3052542
   Yu S, 2018, IEEE T VEH TECHNOL, V67, P11098, DOI 10.1109/TVT.2018.2869144
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang HJ, 2021, IEEE T COMMUN, V69, P1375, DOI 10.1109/TCOMM.2020.3037597
   Zhang HJ, 2020, IEEE T NETW SCI ENG, V7, P2406, DOI 10.1109/TNSE.2020.3004333
   Zhang Y, 2019, IEEE J-STARS, V12, P2833, DOI 10.1109/JSTARS.2019.2918189
   Zhang Y, 2017, INT GEOSCI REMOTE SE, P6142, DOI 10.1109/IGARSS.2017.8128410
   Zhang ZJ, 2019, IEEE NETWORK, V33, P70, DOI 10.1109/MNET.2018.1800172
   Zhao SC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3363560
   Zheng G, 2012, IEEE T WIREL COMMUN, V11, P2308, DOI 10.1109/TWC.2012.040412.111629
NR 40
TC 0
Z9 0
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 135
DI 10.1145/3561821
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800004
DA 2024-07-18
ER

PT J
AU Sun, GF
   Wong, YK
   Kankanhalli, MS
   Li, XD
   Geng, WD
AF Sun, Guofei
   Wong, Yongkang
   Kankanhalli, Mohan S.
   Li, Xiangdong
   Geng, Weidong
TI Enhanced 3D Shape Reconstruction With Knowledge Graph of Category
   Concept
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; 3D reconstruction; conceptual knowledge
ID SIMULTANEOUS LOCALIZATION
AB Reconstructing three-dimensional (3D) objects from images has attracted increasing attention due to its wide applications in computer vision and robotic tasks. Despite the promising progress of recent deep learning-based approaches, which directly reconstruct the full 3D shape without considering the conceptual knowledge of the object categories, existing models have limited usage and usually create unrealistic shapes. 3D objects have multiple forms of representation, such as 3D volume, conceptual knowledge, and so on. In this work, we show that the conceptual knowledge for a category of objects, which represents objects as prototype volumes and is structured by graph, can enhance the 3D reconstruction pipeline. We propose a novel multimodal framework that explicitly combines graph-based conceptual knowledge with deep neural networks for 3D shape reconstruction from a single RGB image. Our approach represents conceptual knowledge of a specific category as a structure-based knowledge graph. Specifically, conceptual knowledge acts as visual priors and spatial relationships to assist the 3D reconstruction framework to create realistic 3D shapes with enhanced details. Our 3D reconstruction framework takes an image as input. It first predicts the conceptual knowledge of the object in the image, then generates a 3D object based on the input image and the predicted conceptual knowledge. The generated 3D object satisfies the following requirements: (1) it is consistent with the predicted graph in concept, and (2) consistent with the input image in geometry. Extensive experiments on public datasets (i.e., ShapeNet, Pix3D, and Pascal3D+) with 13 object categories show that (1) our method outperforms the state-of-the-art methods, (2) our prototype volume-based conceptual knowledge representation is more effective, and (3) our pipeline-agnostic approach can enhance the reconstruction quality of various 3D shape reconstruction pipelines.
C1 [Sun, Guofei; Geng, Weidong] Zhejiang Univ, State Key Lab CAD & CG, Coll Comp Sci & Technol, Zheda Rd 38, Hangzhou 310027, Zhejiang, Peoples R China.
   [Wong, Yongkang] Natl Univ Singapore, Sch Comp, 3 Res Link,I4-0 Bldg, Singapore 117602, Singapore.
   [Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, 11 Comp Dr,AS6 Bldg, Singapore 117416, Singapore.
   [Li, Xiangdong] Zhejiang Univ, Coll Comp Sci & Technol, Zheda Rd 38, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University; National University of Singapore; National
   University of Singapore; Zhejiang University
RP Geng, WD (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Coll Comp Sci & Technol, Zheda Rd 38, Hangzhou 310027, Zhejiang, Peoples R China.; Li, XD (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Zheda Rd 38, Hangzhou 310027, Zhejiang, Peoples R China.
EM guofeisun@zju.edu.cn; yongkang.wong@nus.edu.sg; mohan@comp.nus.edu.sg;
   axli@zju.edu.cn; gengwd@zju.edu.cn
RI sun, guofei/ITT-6545-2023; Kankanhalli, Mohan/Q-9284-2019
OI sun, guofei/0000-0002-0739-0844; Kankanhalli, Mohan/0000-0002-4846-2015;
   Wong, Yongkang/0000-0002-1239-4428
FU National Key Research and Development Program of China [2017YFB1303201,
   2017YFB1002802]; National Natural Science Foundation of China [61972346]
FX This work is supported by the National Key Research and Development
   Program of China (No. 2017YFB1303201 and No. 2017YFB1002802), and the
   National Natural Science Foundation of China (No. 61972346).
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   Afzal H, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177756
   Nguyen AD, 2019, IEEE I CONF COMP VIS, P8627, DOI 10.1109/ICCV.2019.00872
   [Anonymous], 2019, 33 AAAI C ART INT
   Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Chang A. X., 2015, ARXIV
   Chen HT, 2020, PROC CVPR IEEE, P1638, DOI 10.1109/CVPR42600.2020.00171
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Cheung GKM, 2003, PROC CVPR IEEE, P77
   Choi S, 2019, IEEE IMAGE PROC, P2379, DOI [10.1109/ICIP.2019.8803350, 10.1109/icip.2019.8803350]
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Gkioxari G, 2019, IEEE I CONF COMP VIS, P9784, DOI 10.1109/ICCV.2019.00988
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Han XF, 2021, IEEE T PATTERN ANAL, V43, P1578, DOI 10.1109/TPAMI.2019.2954885
   Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Insafutdinov E, 2018, ADV NEUR IN, V31
   Jianren Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P498, DOI 10.1007/978-3-030-58601-0_30
   Johnston A, 2017, IEEE INT CONF COMP V, P930, DOI 10.1109/ICCVW.2017.114
   Kanazawa A, 2018, LECT NOTES COMPUT SC, V11219, P386, DOI 10.1007/978-3-030-01267-0_23
   Kar A., 2017, P ADV NEUR INF PROC
   Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807
   Kato H, 2019, PROC CVPR IEEE, P9770, DOI 10.1109/CVPR.2019.01001
   King DB, 2015, ACS SYM SER, V1214, P1
   Kurenkov A, 2018, IEEE WINT CONF APPL, P858, DOI 10.1109/WACV.2018.00099
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li J, 2020, PROC CVPR IEEE, P3348, DOI 10.1109/CVPR42600.2020.00341
   Liao YL, 2019, INT CONF ACOUST SPEE, P3857, DOI 10.1109/ICASSP.2019.8682813
   Lin CH, 2018, AAAI CONF ARTIF INTE, P7114
   Meagher D, 1980, OCTREE ENCODING NEW
   Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Pala Pietro., 2019, ACM Transactions on Multimedia Computing, Communications, and Applications, V15, P1
   Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006
   Pan JY, 2018, INT CONF 3D VISION, P719, DOI 10.1109/3DV.2018.00087
   Pan YH, 2019, FRONT INFORM TECH EL, V20, P1021, DOI 10.1631/FITEE.1910001
   Pinheiro PO, 2019, IEEE I CONF COMP VIS, P7637, DOI 10.1109/ICCV.2019.00773
   Richter SR, 2018, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR.2018.00207
   Richter SR, 2015, PROC CVPR IEEE, P1128, DOI 10.1109/CVPR.2015.7298716
   Shi Dingfeng, 2020, 2020 IEEE INT C MULT, P1
   Smith EJ, 2019, PR MACH LEARN RES, V97
   Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269
   Stutz D, 2020, INT J COMPUT VISION, V128, P1162, DOI 10.1007/s11263-018-1126-y
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Tang JP, 2019, PROC CVPR IEEE, P4536, DOI 10.1109/CVPR.2019.00467
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Tulsiani S, 2018, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR.2018.00306
   Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30
   Wang HQ, 2019, AAAI CONF ARTIF INTE, P8941
   Wang JL, 2019, AAAI CONF ARTIF INTE, P8949
   Wang M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P961, DOI 10.1145/3123266.3123340
   Wang NY, 2021, IEEE T PATTERN ANAL, V43, P3600, DOI 10.1109/TPAMI.2020.2984232
   Wang WY, 2017, IEEE I CONF COMP VIS, P2317, DOI 10.1109/ICCV.2017.252
   Westoby MJ, 2012, GEOMORPHOLOGY, V179, P300, DOI 10.1016/j.geomorph.2012.08.021
   WITKIN AP, 1981, ARTIF INTELL, V17, P17, DOI 10.1016/0004-3702(81)90019-9
   Wu J., 2017, PROC ADVNEURAL INF P, V30, P153
   Wu JJ, 2018, LECT NOTES COMPUT SC, V11215, P673, DOI 10.1007/978-3-030-01252-6_40
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang Nan, 2019, P INT C COMP AN SOC P 32 INT C COMP AN, P79, DOI DOI 10.1145/3328756.3328766
   Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101
   Xie HZ, 2019, IEEE I CONF COMP VIS, P2690, DOI 10.1109/ICCV.2019.00278
   Xie HZ, 2020, INT J COMPUT VISION, V128, P2919, DOI 10.1007/s11263-020-01347-6
   Yan XC, 2016, ADV NEUR IN, V29
   Yang B, 2019, IEEE T PATTERN ANAL, V41, P2820, DOI 10.1109/TPAMI.2018.2868195
   Yang GD, 2018, LECT NOTES COMPUT SC, V11219, P90, DOI 10.1007/978-3-030-01267-0_6
   Yang S, 2021, PROC CVPR IEEE, P3151, DOI 10.1109/CVPR46437.2021.00317
   Yao Chun-Han, 2021, P IEEE INT C COMP VI, P12981
   Yao Y, 2020, PROC CVPR IEEE, P528, DOI 10.1109/CVPR42600.2020.00061
   Zhang C, 2019, IEEE ACCESS, V7, P49882, DOI 10.1109/ACCESS.2019.2911119
   Zhang C, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17102260
   Zhou QY, 2015, PROC CVPR IEEE, P632, DOI 10.1109/CVPR.2015.7298662
NR 76
TC 0
Z9 0
U1 4
U2 35
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 71
DI 10.1145/3491224
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600005
DA 2024-07-18
ER

PT J
AU Wu, YK
   Zhao, SW
   Zhang, Y
   Yuan, XJ
   Su, Z
AF Wu, Yike
   Zhao, Shiwan
   Zhang, Ying
   Yuan, Xiaojie
   Su, Zhong
TI When Pairs Meet Triplets: Improving Low-Resource Captioning via
   Multi-Objective Optimization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image captioning; low-resource; paired dataset; triplet dataset; bridge
   the gap; auxiliary training objective
AB Image captioning for low-resource languages has attracted much attention recently. Researchers propose to augment the low-resource caption dataset into (image, rich-resource language, and low-resource language) triplets and develop the dual attention mechanism to exploit the existence of triplets in training to improve the performance. However, datasets in triplet form are usually small due to their high collecting cost. On the other hand, there are already many large-scale datasets, which contain one pair from the triplet, such as caption datasets in the rich-resource language and translation datasets from the rich-resource language to the low-resource language. In this article, we revisit the caption-translation pipeline of the translation-based approach to utilize not only the triplet dataset but also large-scale paired datasets in training. The caption-translation pipeline is composed of two models, one caption model of the rich-resource language and one translation model from the rich-resource language to the low-resource language. Unfortunately, it is not trivial to fully benefit from incorporating both the triplet dataset and paired datasets into the pipeline, due to the gap between the training and testing phases and the instability in the training process. We propose to jointly optimize the two models of the pipeline in an end-to-end manner to bridge the training and testing gap, and introduce two auxiliary training objectives to stabilize the training process. Experimental results show that the proposed method improves significantly over the state-of-the-art methods.
C1 [Wu, Yike; Zhang, Ying; Yuan, Xiaojie] Nankai Univ, Coll Comp Sci, 38 Tongyan Rd, Tianjin 300350, Peoples R China.
   [Zhao, Shiwan; Su, Zhong] IBM Res China, 28 Zhongguancun Software Pk 8 Dongbeiwang Western, Beijing 100193, Peoples R China.
C3 Nankai University
RP Yuan, XJ (corresponding author), Nankai Univ, Coll Comp Sci, 38 Tongyan Rd, Tianjin 300350, Peoples R China.
EM wuyike@dbis.nankai.edu.cn; zhaosw@gmail.com; yingzhang@uankai.edu.cn;
   yuanxj@uankai.edu.cn; suzhong@cn.ibm.com
RI cheng, qian/KFB-6227-2024; YE, Chen/KFR-3858-2024; Wang,
   Shiyao/JLL-7826-2023; Wang, Xintong/JJE-1189-2023; Li,
   Kun/JLL-6505-2023; yang, kun/JGM-4169-2023; xuan, li/JNI-7432-2023;
   Yuan, Yi/KGL-5178-2024; Yang, YiChen/KEI-0140-2024; CAO,
   ying/KFA-2972-2024; Lin, Yi/KEH-1784-2024; peng, jin/JRW-4493-2023
OI Li, Kun/0000-0002-3638-2974; Zhao, Shiwan/0000-0001-5068-025X
FU Chinese Scientific and Technical Innovation Project 2030
   [2018AAA0102100]; NSFC-Xinjiang Joint Fund [U1903128]; NSFC-General
   Technology Joint Fund for Basic Research [U1936206]
FX This research is partially supported by the Chinese Scientific and
   Technical Innovation Project 2030 (2018AAA0102100), NSFC-Xinjiang Joint
   Fund (No. U1903128), NSFC-General Technology Joint Fund for Basic
   Research (No. U1936206).
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Banerjee S., 2005, P WORKSHOP INTRINSIC, P65
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Ding ST, 2020, NEUROCOMPUTING, V398, P520, DOI 10.1016/j.neucom.2019.04.095
   Ding ST, 2019, FUTURE GENER COMP SY, V93, P583, DOI 10.1016/j.future.2018.10.054
   Elliott D., 2016, P ACL 2016, P70
   Elliott D., 2015, CoRR
   Gu JT, 2018, AAAI CONF ARTIF INTE, P5125
   Gu JX, 2018, LECT NOTES COMPUT SC, V11205, P519, DOI 10.1007/978-3-030-01246-5_31
   Gumbel Emil J., 1954, Statistical Theory of Extreme Values and Some Practical Applications, V33
   Guo TZ, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P751
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jaffe A., 2017, P 2 C MACH TRANSL, P458
   Jang E., 2016, ARXIV161101144
   Ke L, 2019, IEEE I CONF COMP VIS, P8887, DOI 10.1109/ICCV.2019.00898
   King DB, 2015, ACS SYM SER, V1214, P1
   Lan WY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1549, DOI 10.1145/3123266.3123366
   Li XR, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P271, DOI 10.1145/2911996.2912049
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Maddison C. J., 2014, Advances in Neural Information Processing Systems
   Maddison Chris J, 2016, ARXIV161100712
   Miyazaki T, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1780
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2017, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2017.130
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu YK, 2019, IEEE INT CON MULTI, P362, DOI 10.1109/ICME.2019.00070
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
NR 43
TC 0
Z9 0
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 83
DI 10.1145/3492325
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600017
DA 2024-07-18
ER

PT J
AU Jiao, YY
   Chen, HP
   Feng, RY
   Chen, HM
   Wu, SF
   Yin, YF
   Liu, ZG
AF Jiao, Yingying
   Chen, Haipeng
   Feng, Runyang
   Chen, Haoming
   Wu, Sifan
   Yin, Yifang
   Liu, Zhenguang
TI GLPose: Global-Local Representation Learning for Human Pose Estimation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human pose estimation; feature aggregation; pose estimation;
   global-local representation
AB Multi-frame human pose estimation is at the core of many computer vision tasks. Although state-of-the-art approaches have demonstrated remarkable results for human pose estimation on static images, their performances inevitably come short when being applied to videos. A central issue lies in the visual degeneration of video frames induced by rapid motion and pose occlusion in dynamic environments. This problem, by nature, is insurmountable for a single frame. Therefore, incorporating complementary visual cues from other video frames becomes an intuitive paradigm. Current state-of-the-art methods usually leverage information from adjacent frames, which unfortunately place excessive focus on only the temporally nearby frames. In this paper, we argue that combining global semantically similar information and local temporal visual context will deliver more comprehensive and more robust representations for human pose estimation. Towards this end, we present an effective framework, namely global-local enhanced pose estimation (GLPose) network. Our framework consists of a feature processing module that conditionally incorporates global semantic information and local visual context to generate a robust human representation and a feature enhancement module that excavates complementary information from this aggregated representation to enhance keyframe features for precise estimation. We empirically find that the proposed GLpose outperforms existing methods by a large margin and achieves new state-of-the-art results on large benchmark datasets.
C1 [Jiao, Yingying; Chen, Haipeng] Jilin Univ, Qianjin St, Changchun 130000, Jilin, Peoples R China.
   [Feng, Runyang; Chen, Haoming; Wu, Sifan] Zhejiang Gongshang Univ, Baiyang St, Hangzhou 323000, Zhejiang, Peoples R China.
   [Yin, Yifang] ASTAR, Inst Infocomm Res, Singapore 999002, Singapore.
   [Liu, Zhenguang] Zhejiang Univ, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Jilin University; Zhejiang Gongshang University; Agency for Science
   Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research
   (I2R); Zhejiang University
RP Chen, HP (corresponding author), Jilin Univ, Qianjin St, Changchun 130000, Jilin, Peoples R China.; Feng, RY (corresponding author), Zhejiang Gongshang Univ, Baiyang St, Hangzhou 323000, Zhejiang, Peoples R China.
EM jiaoyingy17@gmail.com; chenhp@jlu.edu.cn; runyang2019.feng@gmail.com;
   chenhaomingbob@gmail.com; wusifan2021@gmail.com;
   yin_yifang@i2r.a-star.edu.sg; liuzhenguang2008@gmail.com
OI Chen, Haoming/0000-0002-4137-0417; Feng, Runyang/0000-0002-4912-6646
CR Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   Artacho B, 2020, PROC CVPR IEEE, P7033, DOI 10.1109/CVPR42600.2020.00706
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Bertasius G, 2019, ADV NEUR IN, V32
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chang SN, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4630, DOI 10.1145/3394171.3416299
   Charles J, 2016, PROC CVPR IEEE, P3063, DOI 10.1109/CVPR.2016.334
   Chen Y., 2020, CVPR, P10337
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Dantone M, 2013, PROC CVPR IEEE, P3041, DOI 10.1109/CVPR.2013.391
   Doering A, 2018, Arxiv, DOI [arXiv:1805.04596, DOI 10.48550/ARXIV.1805.04596]
   Fan Z., 2021, P IEEE CVF INT C COM, P11719
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Gao Z, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2699, DOI 10.1145/3474085.3475450
   Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044
   Guo H., 2018, P EUROPEAN C COMPUTE
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hwang J, 2019, IEEE IJCNN
   Iqbal U, 2017, IEEE INT CONF AUTOMA, P438, DOI 10.1109/FG.2017.61
   Jin S, 2019, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2019.00581
   Li JF, 2021, Arxiv, DOI arXiv:2107.11291
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu M, 2020, IEEE T IMAGE PROCESS, V29, P9360, DOI 10.1109/TIP.2020.3026625
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu ZG, 2023, IEEE T PATTERN ANAL, V45, P681, DOI 10.1109/TPAMI.2021.3139918
   Liu ZG, 2021, PROC CVPR IEEE, P525, DOI 10.1109/CVPR46437.2021.00059
   Luo Y, 2018, PROC CVPR IEEE, P5207, DOI 10.1109/CVPR.2018.00546
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie BX, 2015, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2015.7298734
   Nie XC, 2019, IEEE I CONF COMP VIS, P6941, DOI 10.1109/ICCV.2019.00704
   Park D, 2011, IEEE I CONF COMP VIS, P2627, DOI 10.1109/ICCV.2011.6126552
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Raaj Y, 2019, PROC CVPR IEEE, P4615, DOI 10.1109/CVPR.2019.00475
   Sapp B, 2010, LECT NOTES COMPUT SC, V6312, P406, DOI 10.1007/978-3-642-15552-9_30
   Schmidtke L, 2021, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR46437.2021.00251
   Song J, 2017, PROC CVPR IEEE, P5563, DOI 10.1109/CVPR.2017.590
   Su K, 2019, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2019.00582
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun M, 2012, PROC CVPR IEEE, P3394, DOI 10.1109/CVPR.2012.6248079
   Tan Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P592, DOI 10.1145/3474085.3475218
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Varamesh Ali, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13083, DOI 10.1109/CVPR42600.2020.01310
   Wang F, 2013, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2013.83
   Wang Manchen, 2020, IEEECVF C COMPUTVIS, P11088
   Wang Y, 2008, LECT NOTES COMPUT SC, V5304, P710, DOI 10.1007/978-3-540-88690-7_53
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu HP, 2019, IEEE I CONF COMP VIS, P9216, DOI 10.1109/ICCV.2019.00931
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiu YL, 2018, Arxiv, DOI [arXiv:1802.00977, DOI 10.48550/ARXIV.1802.00977]
   Yang X, 2018, IEEE T IMAGE PROCESS, V27, P791, DOI 10.1109/TIP.2017.2765836
   Yang YD, 2021, PROC CVPR IEEE, P8070, DOI 10.1109/CVPR46437.2021.00798
   Yuexi Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P609, DOI 10.1007/978-3-030-58520-4_36
   Lin KZ, 2019, Arxiv, DOI arXiv:1812.09899
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang JB, 2019, Arxiv, DOI arXiv:1908.05593
   Zhang XQ, 2009, IEEE I CONF COMP VIS, P1349, DOI 10.1109/ICCV.2009.5459306
NR 59
TC 4
Z9 4
U1 7
U2 28
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 128
DI 10.1145/3519305
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000016
DA 2024-07-18
ER

PT J
AU Zhang, L
   Guo, HY
   Zhu, K
   Qiao, HL
   Huang, GP
   Zhang, S
   Zhang, HC
   Sun, J
   Wang, JQ
AF Zhang, La
   Guo, Haiyun
   Zhu, Kuan
   Qiao, Honglin
   Huang, Gaopan
   Zhang, Sen
   Zhang, Huichen
   Sun, Jian
   Wang, Jinqiao
TI Hybrid Modality Metric Learning for Visible-Infrared Person
   Re-Identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visible-infrared person re-identification; cross-modality; metric
   learning
AB Visible-infrared person re-identification (Re-ID) has received increasing research attention for its great practical value in night-time surveillance scenarios. Due to the large variations in person pose, viewpoint, and occlusion in the same modality, as well as the domain gap brought by heterogeneous modality, this hybrid modality person matching task is quite challenging. Different from the metric learning methods for visible person re-ID, which only pose similarity constraints on class level, an efficient metric learning approach for visible-infrared person Re-ID should take both the class-level and modality-level similarity constraints into full consideration to learn sufficiently discriminative and robust features. In this article, the hybrid modality is divided into two types, within modality and cross modality. We first fully explore the variations that hinder the ranking results of visible-infrared person re-ID and roughly summarize them into three types: within-modality variation, cross-modality modality-related variation, and cross-modality modality-unrelated variation. Then, we propose a comprehensive metric learning framework based on four kinds of paired-based similarity constraints to address all the variations within and cross modality. This framework focuses on both class-level and modality-level similarity relationships between person images. Furthermore, we demonstrate the compatibility of our framework with any paired-based loss functions by giving detailed implementation of combing it with triplet loss and contrastive loss separately. Finally, extensive experiments of our approach on SYSIJ-MM01 and RegDB demonstrate the effectiveness and superiority of our proposed metric learning framework for visible-infrared person Re-ID.
C1 [Zhang, La; Sun, Jian] Beijing Inst Technol, 5 South St, Beijing 100081, Peoples R China.
   [Guo, Haiyun; Zhu, Kuan; Wang, Jinqiao] Chinese Acad Sci, Inst Automat, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Qiao, Honglin] Alibaba Cloud, Radiance JinHui Tower,Bldg 6,4th Dist, Beijing, Peoples R China.
   [Huang, Gaopan] Alibaba Cloud, Ali Ctr, Nanjing, Jiangsu, Peoples R China.
   [Zhang, Sen; Zhang, Huichen] Minist Publ Secur, Traff Management Res Inst, 88 Qianrong Rd, Wuxi, Jiangsu, Peoples R China.
C3 Beijing Institute of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Alibaba Group; Ministry of Public Security (China)
RP Guo, HY (corresponding author), Chinese Acad Sci, Inst Automat, 95 Zhongguancun East Rd, Beijing, Peoples R China.
EM 3120185467@bit.edu.cn; haiyun.guo@nlpr.ia.ac.cn; kuan.zhu@nlpr.ia.ac.cn;
   kenny.qhl@alibaba-inc.com; huanggpan@163.com; 76806915@qq.com;
   496306636@qq.com; sunjian@bit.edu.cn; jqwang@nlpr.ia.ac.cn
RI GUO, HAI/HTO-5024-2023; Zhu, Kuan/AGP-6028-2022; zhang, la/JZE-0595-2024
OI Zhang, La/0000-0002-2377-6907
FU Key-Area Research and Development Program of Guangdong Province
   [2020B010165001]; National Natural Science Foundation of China
   [61772527, 62002356, 61925303]; Open Project of Key Laboratory of
   Ministry of Public Security for Road Traffic Safety [2020ZDSYSKFKT04]
FX This work was supported by Key-Area Research and Development Program of
   Guangdong Province (No. 2020B010165001), National Natural Science
   Foundation of China (No. 61772527, 62002356, 61925303), Open Project of
   Key Laboratory of Ministry of Public Security for Road Traffic Safety
   (No. 2020ZDSYSKFKT04).
CR [Anonymous], 2018, P 27 INT JOINT C ART
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   Hermans Alexander, 2017, ARXIV170307737
   Huang Y, 2019, IEEE T IMAGE PROCESS, V28, P1391, DOI 10.1109/TIP.2018.2874715
   Jia Mengxi, 2020, P INT JOINT C ART IN
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu HJ, 2020, NEUROCOMPUTING, V398, P11, DOI 10.1016/j.neucom.2020.01.089
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Lu YT, 2020, PROC CVPR IEEE, P937, DOI 10.1109/CVPR42600.2020.00102
   [罗浩 Luo Hao], 2019, [自动化学报, Acta Automatica Sinica], V45, P2032
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Qian Xuelin, 2018, P 15 EUR C MUN GERM
   Ristani E, 2018, PROC CVPR IEEE, P6036, DOI 10.1109/CVPR.2018.00632
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Sohn K, 2016, ADV NEUR IN, V29
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GuanAn, 2020, IEEE CVF INT C COMP
   Wang YC, 2018, PROC CVPR IEEE, P1470, DOI 10.1109/CVPR.2018.00159
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P347, DOI 10.1145/3343031.3351043
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Ye Mang, 2018, P 27 INT JOINT C ART
   Zhu YX, 2020, NEUROCOMPUTING, V386, P97, DOI 10.1016/j.neucom.2019.12.100
   Zhang SZ, 2021, IEEE T IMAGE PROCESS, V30, P8861, DOI 10.1109/TIP.2021.3120881
   Zhao YB, 2019, IET IMAGE PROCESS, V13, P2897, DOI 10.1049/iet-ipr.2019.0699
   Zheng Liang., 2016, Person re-identification: Past, present and future
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
NR 40
TC 12
Z9 12
U1 3
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 25
DI 10.1145/3473341
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300002
DA 2024-07-18
ER

PT J
AU Gao, W
   Zhou, LJ
   Tao, L
AF Gao, Wei
   Zhou, Linjie
   Tao, Lvfang
TI A Fast View Synthesis Implementation Method for Light Field Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning-based view synthesis; real-time acceleration; compact
   representation; filter pruning; deep neural network compression; light
   field systems
ID NETWORK; PROJECTION
AB View synthesis (VS) for light field images is a very time-consuming task due to the great quantity of involved pixels and intensive computations, which may prevent it from the practical three-dimensional real-time systems. In this article, we propose an acceleration approach for deep learning-based light field view synthesis, which can significantly reduce calculations by using compact-resolution (CR) representation and super-resolution (SR) techniques, as well as light-weight neural networks. The proposed architecture has three cascaded neural networks, including a CR network to generate the compact representation for original input views, a VS network to synthesize new views from down-scaled compact views, and a SR network to reconstruct high-quality views with full resolution. All these networks are jointly trained with the integrated losses of CR, VS, and SR networks. Moreover, due to the redundancy of deep neural networks, we use the efficient light-weight strategy to prune filters for simplification and inference acceleration. Experimental results demonstrate that the proposed method can greatly reduce the processing time and become much more computationally efficient with competitive image quality.
C1 [Gao, Wei; Zhou, Linjie; Tao, Lvfang] Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
   [Gao, Wei; Zhou, Linjie; Tao, Lvfang] Peng Cheng Lab, Artificial Intelligence Res Ctr, Shenzhen 518055, Peoples R China.
C3 Peking University; Peng Cheng Laboratory
RP Gao, W (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.; Gao, W (corresponding author), Peng Cheng Lab, Artificial Intelligence Res Ctr, Shenzhen 518055, Peoples R China.
EM gaowei262@pku.edu.cn; zhoulinjie@pku.edu.cn; ltao@pku.edu.cn
FU Ministry of Science and Technology of China -Science and Technology
   Innovations 2030 [2019AAA0103501]; Natural Science Foundation of China
   [61801303, 62031013]; Guangdong Basic and Applied Basic Research
   Foundation [2019A1515012031]; Shenzhen Science and Technology Plan Basic
   Research Project [JCYJ20190808161805519]; Open Projects Program of
   National Laboratory of Pattern Recognition (NLPR) [202000045];
   CCF-Tencent Open Fund [RAGR20200114]
FX This work was supported by Ministry of Science and Technology of China
   -Science and Technology Innovations 2030 (2019AAA0103501), Natural
   Science Foundation of China (61801303 and 62031013), Guangdong Basic and
   Applied Basic Research Foundation (2019A1515012031), Shenzhen Science
   and Technology Plan Basic Research Project (JCYJ20190808161805519), Open
   Projects Program of National Laboratory of Pattern Recognition (NLPR)
   (202000045), and CCF-Tencent Open Fund (RAGR20200114).
CR [Anonymous], 1990, Adv Neural Inform Process Syst.
   [Anonymous], 2015, CoRR
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Balle J, 2018, ICLR
   Bergstra J., 2011, Adv. Neural Inf. Process. Syst., V24
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Chen XZ, 2018, IEEE T PATTERN ANAL, V40, P1259, DOI 10.1109/TPAMI.2017.2706685
   Cheng Yu, 2017, ARXIV
   Denton E, 2014, ADV NEUR IN, V27
   Didyk Piotr, 2013, ACM T GRAPHIC, V32, P1
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fang L, 2012, IEEE T CIRC SYST VID, V22, P740, DOI 10.1109/TCSVT.2011.2179458
   Farrugia RA, 2017, IEEE J-STSP, V11, P1058, DOI 10.1109/JSTSP.2017.2747127
   Feng MT, 2021, IEEE T IMAGE PROCESS, V30, P92, DOI 10.1109/TIP.2020.3031371
   Feurer M, 2015, ADV NEUR IN, V28
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Gao W, 2020, IEEE COMPUT SOC CONF, P607, DOI 10.1109/CVPRW50498.2020.00085
   Gao W, 2017, IEEE T IMAGE PROCESS, V26, P6074, DOI 10.1109/TIP.2017.2745099
   Gao W, 2016, IEEE T CIRC SYST VID, V26, P139, DOI 10.1109/TCSVT.2015.2444671
   Georgiev T. G., 2006, P 17 EUROGRAPHICS C, V2006
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gordon A, 2018, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2018.00171
   Guo Y., 2016, Advances in neural information processing systems, P1387
   Han S, 2015, ADV NEUR IN, V28
   Hanson S. J., 1989, Advances in Neural Information Processing Systems, P177
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hassibi B., 1993, P ADV NEUR INF PROC, P164
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447
   He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48
   Hinton G., 2015, COMPUT SCI, V2
   Hubara I, 2018, J MACH LEARN RES, V18
   Hutter Frank, 2011, Learning and Intelligent Optimization. 5th International Conference, LION 5. Selected Papers, P507, DOI 10.1007/978-3-642-25566-3_40
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Jaderberg M., 2014, CORR
   Jiang F, 2018, IEEE T CIRC SYST VID, V28, P3007, DOI 10.1109/TCSVT.2017.2734838
   Jiang QP, 2019, IEEE SIGNAL PROC LET, V26, P1588, DOI 10.1109/LSP.2019.2940105
   Jiang QP, 2019, IEEE T IMAGE PROCESS, V28, P1866, DOI 10.1109/TIP.2018.2881828
   Jones DR, 2001, J GLOBAL OPTIM, V21, P345, DOI 10.1023/A:1012771025575
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kauvar I, 2015, ACM T GRAPHIC, V34, DOI [10.1145/2682631, 10.1145/2816795.2818070]
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   Kingma D. P., 2014, arXiv
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Levin A, 2010, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR.2010.5539854
   Li H., 2017, PRUNING FILTERS EFFI, DOI DOI 10.48550/ARXIV.1608.08710
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P1092, DOI 10.1109/TIP.2018.2872876
   Liao GB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2436, DOI 10.1145/3394171.3413523
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Luo P, 2016, AAAI CONF ARTIF INTE, P3560
   Molchanov P, 2019, PROC CVPR IEEE, P11256, DOI 10.1109/CVPR.2019.01152
   Pearson J, 2013, IEEE T IMAGE PROCESS, V22, P3405, DOI 10.1109/TIP.2013.2268939
   Pujades S, 2014, PROC CVPR IEEE, P3906, DOI 10.1109/CVPR.2014.499
   Raj Abhilash Sunder, 2020, STANFORD LYTRO LIGHT
   Rigamonti R, 2013, PROC CVPR IEEE, P2754, DOI 10.1109/CVPR.2013.355
   Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shin C, 2018, PROC CVPR IEEE, P4748, DOI 10.1109/CVPR.2018.00499
   Srinivasan PP, 2017, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2017.246
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Vagharshakyan S, 2018, IEEE T PATTERN ANAL, V40, P133, DOI 10.1109/TPAMI.2017.2653101
   Vagharshakyan S, 2017, IEEE J-STSP, V11, P1082, DOI 10.1109/JSTSP.2017.2738617
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wanner S, 2012, LECT NOTES COMPUT SC, V7576, P608, DOI 10.1007/978-3-642-33715-4_44
   Wu GC, 2017, PROC CVPR IEEE, P1638, DOI 10.1109/CVPR.2017.178
   Yoon Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P57, DOI 10.1109/ICCVW.2015.17
   You Y, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126912
   Yu J., 2019, ARXIV190311728
   Yuan H, 2015, IEEE T MULTIMEDIA, V17, P2134, DOI 10.1109/TMM.2015.2477682
   Zhang FL, 2017, IEEE T VIS COMPUT GR, V23, P1561, DOI 10.1109/TVCG.2016.2532329
   Zhang J, 2016, IEEE T CIRC SYST VID, V26, P479, DOI 10.1109/TCSVT.2014.2367356
   Zhang ZT, 2015, PROC CVPR IEEE, P3800, DOI 10.1109/CVPR.2015.7299004
   Zhou A., 2017, ARXIV170203044
NR 77
TC 5
Z9 5
U1 2
U2 32
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 134
DI 10.1145/3459098
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800018
DA 2024-07-18
ER

PT J
AU Qian, SS
   Hu, J
   Fang, Q
   Xu, CS
AF Qian, Shengsheng
   Hu, Jun
   Fang, Quan
   Xu, Changsheng
TI Knowledge-aware Multi-modal Adaptive Graph Convolutional Networks for
   Fake News Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fake news detection; graph convolutional network; multi-modal learning
AB In this article, we focus on fake news detection task and aim to automatically identify the fake news from vast amount of social media posts. To date, many approaches have been proposed to detect fake news, which includes traditional learning methods and deep learning-based models. However, there are three existing challenges: (i) How to represent social media posts effectively, since the post content is various and highly complicated; (ii) how to propose a data-driven method to increase the flexibility of the model to deal with the samples in different contexts and news backgrounds; and (iii) how to fully utilize the additional auxiliary information (the background knowledge and multi-modal information) of posts for better representation learning. To tackle the above challenges, we propose a novel Knowledge-aware Multi-modal Adaptive Graph Convolutional Networks (KMAGCN) to capture the semantic representations by jointly modeling the textual information, knowledge concepts, and visual information into a unified framework for fake news detection. We model posts as graphs and use a knowledge-aware multi-modal adaptive graph learning principal for the effective feature learning. Compared with existing methods, the proposed KMAGCN addresses challenges from three aspects: (1) It models posts as graphs to capture the non-consecutive and long-range semantic relations; (2) it proposes a novel adaptive graph convolutional network to handle the variability of graph data; and (3) it leverages textual information, knowledge concepts and visual information jointly for model learning. We have conducted extensive experiments on three public real-world datasets and superior results demonstrate the effectiveness of KMAGCN compared with other state-of-the-art algorithms.
C1 [Qian, Shengsheng; Hu, Jun; Fang, Quan; Xu, Changsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.
   [Qian, Shengsheng; Hu, Jun; Fang, Quan] Univ Chinese Acad Sci, 95 ZhongGuanChun East Rd, Beijing 100190, Peoples R China.
   [Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, 95 ZhongGuanChun East Rd, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Qian, SS (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.; Qian, SS (corresponding author), Univ Chinese Acad Sci, 95 ZhongGuanChun East Rd, Beijing 100190, Peoples R China.
EM shengsheng.qian@nlpr.ia.ac.cn; hujunxianligong@gmail.com;
   qfang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022
FU National Key Research and Development Program of China [2017YFB1002804];
   National Natural Science Foundation of China [62036012, 61721004,
   61720106006, 61802405, 62072456, 61832002, 61936005, U1705262]; Key
   Research Program of Frontier Sciences, CAS [QYZDJSSWJSC039]; K.C.Wong
   Education Foundation
FX This work was supported by National Key Research and Development Program
   of China (No. 2017YFB1002804), National Natural Science Foundation of
   China (No. 62036012, 61721004, 61720106006, 61802405, 62072456,
   61832002, 61936005 and U1705262), the Key Research Program of Frontier
   Sciences, CAS, Grant NO. QYZDJSSWJSC039, and the K.C.Wong Education
   Foundation.
CR Allcott H, 2017, J ECON PERSPECT, V31, P211, DOI 10.1257/jep.31.2.211
   Bastings J., 2017, P 2017 C EMP METH NA, P1957, DOI 10.18653/v1/d17-1209
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Castillo C., 2011, P 20 INT C WORLD WID, P675, DOI DOI 10.1145/1963405.1963500
   Chen LH, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P457, DOI 10.1145/3269206.3271809
   Cho K., 2014, ARXIV14061078
   Cui LM, 2019, PROCEEDINGS OF THE 2019 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2019), P41, DOI 10.1145/3341161.3342894
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ganea OE, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P927, DOI 10.1145/2872427.2882988
   Globerson A, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P621
   Guo ZC, 2018, SEMANT WEB, V9, P459, DOI 10.3233/SW-170273
   Gupta A, 2014, LECT NOTES COMPUT SC, V8851, P228, DOI 10.1007/978-3-319-13734-6_16
   Gupta M., 2012, P 2012 SIAM INT C DA, DOI [10.1137/1.9781611972825.14, DOI 10.1137/1.9781611972825.14]
   Hu GY, 2019, LECT NOTES ARTIF INT, V11838, P698, DOI 10.1007/978-3-030-32233-5_54
   Hu Jun, 2021, Eur Actuar J
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   Kalamaras I, 2014, IEEE T MULTIMEDIA, V16, P1460, DOI 10.1109/TMM.2014.2316473
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Kolitsas Nikolaos, 2018, P 22 C COMP NAT LANG, P519, DOI DOI 10.18653/V1/K18-1050
   Kwon S, 2013, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2013.61
   Lazic N., 2015, Transactions of the Association for Computational Linguistics, V3, P503, DOI [DOI 10.1162/TACL_A_00154, 10.1162/tacl_a_00154]
   Liu S, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1379, DOI 10.1145/3397271.3401086
   Ma J., 2015, P 24 ACM INT C INF K, P1751, DOI DOI 10.1145/2806416.2806607
   Ma J., 2016, P 25 INT JOINT C ART, P3818
   Marcheggiani D, 2018, P 2018 C N AM CHAPT, P1, DOI [DOI 10.18653/V1/N18-2078, 1804.08313]
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Milne D, 2008, P 17 ACM C INF KNOWL, P509, DOI DOI 10.1145/1458082.1458150
   Le P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1595
   Potthast M, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P231
   Qi Peng, 2019, IEEE DATA MINING, DOI DOI 10.1109/ICDM.2019.00062
   Rousseau F, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1702
   Shen W, 2015, IEEE T KNOWL DATA EN, V27, P443, DOI 10.1109/TKDE.2014.2327028
   Shi BX, 2016, KNOWL-BASED SYST, V104, P123, DOI 10.1016/j.knosys.2016.04.015
   Shu K., 2017, ACM SIGKDD EXPLOR NE, V19, P22, DOI [DOI 10.1145/3137597.3137600, 10.1145/3137597.3137600]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singhal S, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P39, DOI [10.1109/BigMM.2019.00-44, 10.1109/BigMM.2019.00018]
   Singhal Shivangi, 2020, 34 AAAI C ARTIFICIAL, P13915
   Suchanek FM, 2008, J WEB SEMANT, V6, P203, DOI 10.1016/j.websem.2008.06.001
   Tian D.P., 2013, International Journal of Multimedia and Ubiquitous Engineering, V8, P385
   Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243
   Udandarao Vishaal, 2020, ARXIVCSLG200503687
   Velickovic Petar, 2017, ARXIV171010903, DOI DOI 10.48550/ARXIV.1710.10903
   Wang YQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P849, DOI 10.1145/3219819.3219903
   Wu M, 2020, IEEE DATA MINING, P681, DOI 10.1109/ICDM50108.2020.00077
   Wu M, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1457, DOI 10.1145/3366423.3380219
   Wu W T, 2012, P 2012 ACM SIGMOD IN, P481, DOI DOI 10.1145/2213836.2213891
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Wu Z., 2019, IEEE T NEUR NET LEAR, DOI [DOI 10.1109/TNNLS.2020.2978386, 10.1109/TNNLS.2020.2978386]
   Yamada I., 2017, P T ASS COMP LING, V5, P397, DOI [DOI 10.1162/TACL_A_00069, 10.1162/tacl_a_00069]
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang ZL, 2019, ADV NEUR IN, V32
   Yao L., 2018, Graph convolutional networks for text classification
   Yao L, 2019, AAAI CONF ARTIF INTE, P7370
   Youze Wang, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P540, DOI 10.1145/3372278.3390713
   Yu F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3901
   Zhang HW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1942, DOI 10.1145/3343031.3350850
   Zhang YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1089, DOI 10.1145/3343031.3351033
   Zhao L, 2015, IEEE T MULTIMEDIA, V17, P1936, DOI 10.1109/TMM.2015.2477058
   Zhou XY, 2020, LECT NOTES ARTIF INT, V12085, P354, DOI 10.1007/978-3-030-47436-2_27
   Zubiaga A., 2017, P INT C SOCIAL INFOR, P109, DOI [10.1007/978-3-319-67217-5_8SeriesTitle:LectureNotesinComputerScience, DOI 10.1007/978-3-319-67217-5_8SERIESTITLE:LECTURENOTESINCOMPUTERSCIENCE, 10.1007/978-3-319-67217-58]
NR 64
TC 35
Z9 38
U1 4
U2 58
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 98
DI 10.1145/3451215
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400022
OA Bronze
DA 2024-07-18
ER

PT J
AU Liu, XX
   Xu, QY
AF Liu, Xiaoxiao
   Xu, Qingyang
TI Adaptive Attention-based High-level Semantic Introduction for Image
   Caption
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image caption; high-level semantic; adaptive attention; CNN; LSTM;
   visual sentinel
ID FUSION
AB There have been several attempts to integrate a spatial visual attention mechanism into an image caption model and introduce semantic concepts as the guidance of image caption generation. High-level semantic information consists of the abstractedness and generality indication of an image, which is beneficial to improve the model performance. However, the high-level information is always static representation without considering the salient elements. Therefore, a semantic attention mechanism is used for the high-level information instead of conventional of static representation in this article. The salient high-level semantic information can be considered as redundant semantic information for image caption generation. Additionally, the generation of visual words and non-visual words can be separated, and an adaptive attention mechanism is employed to realic the guidance information of image caption generation switching between new fusion information (fusion of image feature and high-level semantics) and a language model. Therefore, visual words can be generated according to the image features and high-level semantic information, and non-visual words can be predicted by the language model. The semantics attention, adaptive attention, and previous generated words are fused to construct a special attention module for the input and output of long short-term memory. An image caption can be generated as a concise sentence on the basis of accurately grasping the rich content of the image. The experimental results show that the performance of the proposed model is promising for the evaluation metrics, and the captions can achieve logical and rich descriptions.
C1 [Liu, Xiaoxiao; Xu, Qingyang] Shandong Univ, Sch Mech Elect & Informat Engn, Weihai 264209, Shandong, Peoples R China.
C3 Shandong University
RP Xu, QY (corresponding author), Shandong Univ, Sch Mech Elect & Informat Engn, Weihai 264209, Shandong, Peoples R China.
EM sdulxx@163.com; qingyangxu@sdu.edu.cn
RI Liu, Xiaoxiao/HNI-6180-2023; xu, qingyang/C-1808-2012
OI Xu, Qingyang/0000-0003-3870-5551
FU National Natural Science Foundation of China [61573213, 61803227,
   61603214, 61673245]; National Key Research and Development Plan of China
   [2017YFB1300205]; Shandong Province Key Research and Development Plan
   [2018GGX101039, 2016ZDJS02A07]; China Postdoctoral Science Foundation
   [2018M630778]
FX This work was supported by the National Natural Science Foundation of
   China under Grants No. 61573213, No. 61803227, No. 61603214, and No.
   61673245; the National Key Research and Development Plan of China under
   Grant No. 2017YFB1300205, the Shandong Province Key Research and
   Development Plan under Grants No. 2018GGX101039 and No. 2016ZDJS02A07,
   and the China Postdoctoral Science Foundation under Grant No.
   2018M630778.
CR Al-Dabbagh RD, 2015, COMPUT SYST SCI ENG, V30, P125
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2013, P 2013 C EMP METH NA
   [Anonymous], 2014, ARXIV14128419
   Chen H, 2018, AAAI CONF ARTIF INTE, P6706
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Choi Y, 2011, P 15 C COMPUTATIONAL, P220
   Devlin J, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P100
   Ding ST, 2020, NEUROCOMPUTING, V398, P520, DOI 10.1016/j.neucom.2019.04.095
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Er MJ, 2016, INFORM SCIENCES, V373, P388, DOI 10.1016/j.ins.2016.08.084
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   He C., 2019, ACM T MULTIM COMPUT, V15, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hossain MZ, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3295748
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   JIN J, 2015, ARXIV150606272
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Kulkarni G, 2011, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2011.5995466
   Kuznetsova Polina, 2012, Association for Computational Linguistics
   Lavie A., 2007, P 2 WORKSH STAT MACH, P228
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mao J, 2014, CELL DEATH DIS, V5, DOI 10.1038/cddis.2013.515
   Mao JH, 2015, IEEE I CONF COMP VIS, P2533, DOI 10.1109/ICCV.2015.291
   Mao Junhua., 2014, Explain images with multimodal recurrent neural networks
   Mnih V, 2014, ADV NEUR IN, V27
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pedersoli M, 2017, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2017.140
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Su JS, 2019, NEUROCOMPUTING, V367, P144, DOI 10.1016/j.neucom.2019.08.012
   Tavakoli HR, 2017, IEEE I CONF COMP VIS, P2506, DOI 10.1109/ICCV.2017.272
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang AR, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115932
   Wang N, 2016, NEUROCOMPUTING, V174, P651, DOI 10.1016/j.neucom.2015.09.090
   Wang SW, 2019, IEEE ACCESS, V7, P66680, DOI 10.1109/ACCESS.2019.2917979
   Wu J., 2018, ACM T MULTIM COMPUT, P1
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang XD, 2020, NEUROCOMPUTING, V395, P212, DOI 10.1016/j.neucom.2018.02.112
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
NR 54
TC 12
Z9 12
U1 2
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 128
DI 10.1145/3409388
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800012
DA 2024-07-18
ER

PT J
AU Tian, T
   Wang, HL
   Kwong, S
   Kuo, CCJ
AF Tian, Tao
   Wang, Hanli
   Kwong, Sam
   Kuo, C-C Jay
TI Perceptual Image Compression with Block-Level Just Noticeable Difference
   Prediction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Perceptual image compression; just noticeable difference; block-level
   prediction; convolutional neural network
ID JND
AB A block-level perceptual image compression framework is proposed in this work, including a block-level just noticeable difference (JND) prediction model and a preprocessing scheme. Specifically speaking, block-level JND values are first deduced by utilizing the OTSU method based on the variation of block-level structural similarity values between two adjacent picture-level JND values in the MCL-JCI dataset. After the JND value for each image block is generated, a convolutional neural network-based prediction model is designed to forecast block-level JND values for a given target image. Then, a preprocessing scheme is devised to modify the discrete cosine transform coefficients during JPEG compression on the basis of the distribution of blocklevel JND values of the target test image. Finally, the test image is compressed by the max JND value across all of its image blocks in the light of the initial quality factor setting. The experimental results demonstrate that the proposed block-level perceptual image compression method is able to achieve 16.75% bit saving as compared to the state-of-the-art method with similar subjective quality. The project page can be found at https://mic.tongji.edu.cn/43/3f/c9778a148287/page.htm.
C1 [Tian, Tao] Tongji Univ, Dept Comp Sci & Technol, Shanghai 201804, Peoples R China.
   [Wang, Hanli] Tongji Univ, Dept Comp Sci & Technol, Key Lab Embedded Syst & Serv Comp, Minist Educ Shanghai Inst Intelligent Sci & Techn, Shanghai 201804, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong 000000, Peoples R China.
   [Kuo, C-C Jay] Univ Southern Calif, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90089 USA.
C3 Tongji University; Tongji University; City University of Hong Kong;
   University of Southern California
RP Wang, HL (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Key Lab Embedded Syst & Serv Comp, Minist Educ Shanghai Inst Intelligent Sci & Techn, Shanghai 201804, Peoples R China.
EM 1989tiantao@tongji.edu.cn; hanliwang@tongji.edu.cn; cssamk@cityu.edu.hk;
   cckuo@sipi.usc.edu
RI Kwong, Sam/C-9319-2012; Wang, Hanli/G-5111-2014; Kuo, C.-C.
   Jay/A-7110-2011
OI Kwong, Sam/0000-0001-7484-7261; Wang, Hanli/0000-0002-9999-4871; Kuo,
   C.-C. Jay/0000-0001-9474-5035
FU National Natural Science Foundation of China [61976159]; Ministry of
   Science and Technology of China [2018AAA0101303]; Shanghai Engineering
   Research Center of Industrial Vision Perception & Intelligent Computing
   [17DZ2251600]
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 61976159, Key Project of Science and Technology
   Innovation 2030 supported by the Ministry of Science and Technology of
   China (Grant No. 2018AAA0101303), and Shanghai Engineering Research
   Center of Industrial Vision Perception & Intelligent Computing
   (17DZ2251600).
CR [Anonymous], 2003, IEEE T CIRC SYST VID, DOI DOI 10.1109/TCSVT.2003
   Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P3343, DOI 10.1109/TIP.2016.2568459
   Bouchakour M, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P201, DOI 10.1109/ICME.2008.4607406
   Chiu YJ, 1999, IEEE T CIRC SYST VID, V9, P438, DOI 10.1109/76.754773
   Chou CH, 2010, IEEE T IMAGE PROCESS, V19, P2966, DOI 10.1109/TIP.2010.2052261
   Cock J. D., 2016, P IEEE INT C IM PROC, P179
   Ernawan F., 2014, J THEORETICAL APPL I, V70, P566
   Franzen R., 1999, KODAK IMAGE DATASET
   Hu SD, 2016, INT CONF ACOUST SPEE, P1070, DOI 10.1109/ICASSP.2016.7471840
   Jin L., 2016, Electronic Imaging, V2016, P1
   Ki S, 2018, IEEE T IMAGE PROCESS, V27, P3178, DOI 10.1109/TIP.2018.2818439
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P918, DOI 10.1109/LSP.2014.2320743
   Li YM, 2016, J VIS COMMUN IMAGE R, V40, P600, DOI 10.1016/j.jvcir.2016.07.025
   Lin J. Y., 2015, PROC SPIE, V9599, P1
   Lin WS, 2005, IEEE T CIRC SYST VID, V15, P900, DOI 10.1109/TCSVT.2005.848345
   Liu AM, 2010, IEEE T CIRC SYST VID, V20, P1648, DOI 10.1109/TCSVT.2010.2087432
   Lu ZK, 2005, IEEE T IMAGE PROCESS, V14, P1928, DOI 10.1109/TIP.2005.854478
   Pan ZQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159170
   RECOMMENDATION ITU-R BT, 2002, METH SUBJ ASS QUAL T
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Takeuchi M, 2018, PICT COD SYMP, P179, DOI 10.1109/PCS.2018.8456297
   Tian T, 2018, FRONT COMPUT SCI-CHI, V12, P825, DOI 10.1007/s11704-018-7304-9
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WATSON AB, 1993, P SOC PHOTO-OPT INS, V1913, P202, DOI 10.1117/12.152694
   Watson AB, 2001, J ELECTRON IMAGING, V10, P20, DOI 10.1117/1.1329896
   Wei ZY, 2009, IEEE T CIRC SYST VID, V19, P337, DOI 10.1109/TCSVT.2009.2013518
   Wolfgang RB, 1999, P IEEE, V87, P1108, DOI 10.1109/5.771067
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1705, DOI 10.1109/TMM.2013.2268053
   Yang XK, 2005, IEEE T CIRC SYST VID, V15, P742, DOI 10.1109/TCSVT.2005.848313
   Zhang X., 2017, IEEE SIGNAL PROCESS, V24, P1
   Zhang XH, 2005, SIGNAL PROCESS, V85, P795, DOI 10.1016/j.sigpro.2004.12.002
NR 35
TC 9
Z9 9
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 126
DI 10.1145/3408320
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800010
DA 2024-07-18
ER

PT J
AU Liu, YT
   Gu, K
   Li, X
   Zhang, YB
AF Liu, Yutao
   Gu, Ke
   Li, Xiu
   Zhang, Yongbing
TI Blind Image Quality Assessment by Natural Scene Statistics and
   Perceptual Characteristics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Blind image quality assessment (BIQA); natural scene statistics (NSS);
   free-energy principle; sparse representation
ID FREE-ENERGY PRINCIPLE
AB Opinion-unaware blind image quality assessment (OU BIQA) refers to establishing a blind quality prediction model without using the expensive subjective quality scores, which is a highly promising direction in the BIQA research. In this article, we focus on OU BIQA and propose a novel OU BIQA method. Specifically, in our proposed method, we deeply investigate the natural scene statistics (NSS) and the perceptual characteristics of the human brain for visual perception. Accordingly, a set of quality-aware NSS and perceptual characteristics-related features are designed to characterize the image quality effectively. For inferring the image quality, we learn a pristine multivariate Gaussian (MVG) model on a collection of pristine images, which serves as the reference information for quality evaluation. At last, the quality of a new given image is defined by measuring the divergence between its MVG model and the learned pristine MVG model. Thorough experiments performed on seven popular image databases demonstrate that the proposed OU BIQA method delivers superior performance to the state-of-the-art OU BIQA methods. The Matlab source code of the proposed method will be made publicly available at https://github.com/YT2015?tab=repositories.
C1 [Liu, Yutao; Li, Xiu; Zhang, Yongbing] Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol,Beijing Key Lab Computat Int, Engn Res Ctr Intelligent Percept & Autonomous Con, Beijing Artificial Intelligence Inst,Minist Educ, Beijing 100124, Peoples R China.
C3 Tsinghua Shenzhen International Graduate School; Beijing University of
   Technology
RP Li, X (corresponding author), Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
EM ytliu18@sz.tsinghua.edu.cn; guke.doctor@gmail.com;
   li.xiu@sz.tsinghua.edu.cn; zhang.yongbing@sz.tsinghua.edu.cn
RI Gu, Ke/AAJ-9684-2021
FU China Postdoctoral Science Foundation [2019M650686]; National Natural
   Science Foundation of China [41876098]
FX This work was supported in part by the China Postdoctoral Science
   Foundation under Grant 2019M650686 and in part by the National Natural
   Science Foundation of China under Grant 41876098.
CR [Anonymous], 2011, MICT Image Quality Evaluation Database
   [Anonymous], 2010, Categorical image quality (CSIQ) database
   [Anonymous], 2006, IEEE T IMAGE PROCESS
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Geusebroek JM, 2005, INT J COMPUT VISION, V62, P7, DOI 10.1007/s11263-005-4632-7
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Gulliver SR, 2006, ACM T MULTIM COMPUT, V2, P241, DOI 10.1145/1201730.1201731
   Hassen R, 2010, INT CONF ACOUST SPEE, P2434, DOI 10.1109/ICASSP.2010.5496297
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim JB, 2017, INT J ANTENN PROPAG, V2017, DOI 10.1155/2017/1862391
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li XG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282445
   Liu Y., 2019, ADV ENG MATER, V21, P1
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Liu YT, 2017, J VIS COMMUN IMAGE R, V46, P70, DOI 10.1016/j.jvcir.2017.03.007
   Liu YT, 2016, IEEE INT SYMP CIRC S, P1586, DOI 10.1109/ISCAS.2016.7538867
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Rohaly A.M., 2000, ITU T STANDARDS CONT, P9
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Soundararajan R, 2012, IEEE T IMAGE PROCESS, V21, P517, DOI 10.1109/TIP.2011.2166082
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu Q, 2018, ORG CHEM FRONT, V5, P2496, DOI 10.1039/c8qo00556g
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Wu XL, 2011, IEEE T IMAGE PROCESS, V20, P36, DOI 10.1109/TIP.2010.2061860
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhu Y, 2018, J GREY SYST-UK, V30, P1
NR 52
TC 35
Z9 35
U1 3
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 91
DI 10.1145/3414837
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200017
DA 2024-07-18
ER

PT J
AU Yuan, J
   Zhang, L
   Guo, SR
   Xiao, Y
   Li, ZY
AF Yuan, Jin
   Zhang, Lei
   Guo, Songrui
   Xiao, Yi
   Li, Zhiyong
TI Image Captioning with a Joint Attention Mechanism by Visual Concept
   Samples
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image captioning; LSTM; attention; visual concept; cross-domain
AB The attention mechanism has been established as an effective method for generating caption words in image captioning; it explores one noticed subregion in an image to predict a related caption word. However, even though the attention mechanism could offer accurate subregions to train a model, the learned captioner may predict wrong, especially for visual concept words, which are the most important parts to understand an image. To tackle the preceding problem, in this article we propose Visual Concept Enhanced Captioner, which employs a joint attention mechanism with visual concept samples to strengthen prediction abilities for visual concepts in image captioning. Different from traditional attention approaches that adopt one LSTM to explore one noticed subregion each time, Visual Concept Enhanced Captioner introduces multiple virtual LSTMs in parallel to simultaneously receive multiple subregions from visual concept samples. Then, the model could update parameters by jointly exploring these subregions according to a composite loss function. Technically, this joint learning is helpful in finding the common characters of a visual concept, and thus it enhances the prediction accuracy for visual concepts. Moreover, by integrating diverse visual concept samples from different domains, our model can be extended to bridge visual bias in cross-domain learning for image captioning, which saves the cost for labeling captions. Extensive experiments have been conducted on two image datasets (MSCOCO and Flickr30K), and superior results are reported when comparing to state-of-theart approaches. It is impressive that our approach could significantly increase BLUE-1 and F1 scores, which demonstrates an accuracy improvement for visual concepts in image captioning.
C1 [Yuan, Jin; Zhang, Lei; Guo, Songrui; Xiao, Yi; Li, Zhiyong] Hunan Univ, 2 Lushannan Rd, Changsha, Peoples R China.
C3 Hunan University
RP Yuan, J (corresponding author), Hunan Univ, 2 Lushannan Rd, Changsha, Peoples R China.
EM yuanjin@hnu.edu.cn; zhangleizl@hnu.edu.cn; guosongrui001@hnu.edu.cn;
   yixiao_cess@hnu.edu.cn; zhiyong.li@hnu.edu.cn
RI li, zy/HZM-1892-2023; Zhang, Lei/JRZ-1345-2023; Wang,
   Zejun/KBB-8454-2024
OI Zhang, Lei/0009-0002-4489-6745; 
FU National Key Research and Development Program of China [2018YFB0203904];
   Hunan Key RD Program [2017GK2224]; National Natural Science Foundation
   of China [61502157, 61502158, 61502137]
FX This work was supported by the National Key Research and Development
   Program of China (2018YFB0203904), the Hunan Key R&D Program
   (2017GK2224), and the National Natural Science Foundation of China
   (61502157, 61502158, and 61502137).
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   [Anonymous], 2018, PROC 2018 DIGITAL IM, DOI DOI 10.1109/DICTA.2018.8615810
   [Anonymous], 2004, WORKSHOP TEXT SUMMAR
   Bin Y, 2019, IEEE T CYBERNETICS, V49, P2631, DOI 10.1109/TCYB.2018.2831447
   Chen H, 2018, 2018 NINTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME 2018), P606, DOI 10.1109/ITME.2018.00139
   Chen H, 2018, AAAI CONF ARTIF INTE, P6706
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen TH, 2017, IEEE I CONF COMP VIS, P521, DOI 10.1109/ICCV.2017.64
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gao LL, 2019, AAAI CONF ARTIF INTE, P8320
   Gu JX, 2018, AAAI CONF ARTIF INTE, P6837
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hendricks LA, 2016, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2016.8
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jiang WH, 2018, AAAI CONF ARTIF INTE, P6959
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li L., 2018, Mobile Information Systems, V2018, P3, DOI DOI 10.26355/eurrev_201806_15146
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li YH, 2019, PROC CVPR IEEE, P12489, DOI 10.1109/CVPR.2019.01278
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu AA, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P821
   Liu CX, 2017, AAAI CONF ARTIF INTE, P4176
   Liu DQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1416, DOI 10.1145/3240508.3240632
   Lu D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4013
   Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mao YZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4258
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Parikh, 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7299087
   Pedersoli M, 2017, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2017.140
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song J., 2018, ARXIV181211004
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Song LY, 2019, AAAI CONF ARTIF INTE, P8885
   Sutskever I., 2014, ADV NEURAL INFORM PR, V4, P3104, DOI DOI 10.5555/2969033.2969173
   Venugopalan S, 2017, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2017.130
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang WX, 2019, AAAI CONF ARTIF INTE, P8957
   Wang YF, 2017, PROC CVPR IEEE, P7378, DOI 10.1109/CVPR.2017.780
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yao T, 2017, PROC CVPR IEEE, P5263, DOI 10.1109/CVPR.2017.559
   Ye SM, 2018, IEEE T IMAGE PROCESS, V27, P5514, DOI 10.1109/TIP.2018.2855406
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhao W, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1205
   Zhou Yanzhao, 2020, PROC IEEE C COMPUTER, P10307
   Zhu WH, 2018, IEEE INT CONF COMMUN, P353, DOI 10.1109/ICCChina.2018.8641115
   Zhu Zhihao, 2018, P BRIT MACH VIS C, P82
NR 64
TC 13
Z9 13
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 83
DI 10.1145/3394955
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200009
DA 2024-07-18
ER

PT J
AU Si, W
   Liu, C
   Bi, ZQ
   Shan, MJ
AF Si, Wen
   Liu, Cong
   Bi, Zhongqin
   Shan, Meijing
TI Modeling Long-Term Dependencies from Videos Using Deep Multiplicative
   Neural Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; temporal dependencies; video recognition
AB Understanding temporal dependencies of videos is fundamental for vision problems, but deep learning-based models are still insufficient in this field. In this article, we propose a novel deep multiplicative neural network (DMNN) for learning hierarchical long-term representations from video. The DMNN is built upon the multiplicative block that remembers the pairwise transformations between consecutive frames using multiplicative interactions rather than the regular weighted-sum ones. The block is slided over the timesteps to update the memory of the networks on the frame pairs. Deep architecture can be implemented by stacking multiple layers of the sliding blocks. The multiplicative interactions lead to exact, rather than approximate, modeling of temporal dependencies. The memory mechanism can remember the temporal dependencies for an arbitrary length of time. The multiple layers output multiple-level representations that reflect the multi-timescale structure of video. Moreover, to address the difficulty of training DMNNs, we derive a theoretically sound convergent method, which leads to a fast and stable convergence. We demonstrate a new state-of-the-art classification performance with proposed networks on the UCF101 dataset and the effectiveness of capturing complicate temporal dependencies on a variety of synthetic datasets.
C1 [Si, Wen] Shanghai Business Sch, Dept Informat & Comp Sci, Shanghai, Peoples R China.
   [Si, Wen] Fudan Univ, Huashan Hosp, Dept Rehabil, Shanghai, Peoples R China.
   [Liu, Cong] Zhejiang Univ, Ningbo Inst Technol, Ningbo, Peoples R China.
   [Bi, Zhongqin] Shanghai Univ Elect Power, Coll Comp Sci & Technol, Shanghai, Peoples R China.
   [Shan, Meijing] East China Univ Polit Sci & Law, Inst Informat Sci & Technol, Shanghai, Peoples R China.
C3 Shanghai Business School; Fudan University; Zhejiang University;
   Shanghai University of Electric Power; East China University Political
   Science & Law
RP Liu, C (corresponding author), Zhejiang Univ, Ningbo Inst Technol, Ningbo, Peoples R China.
EM siwen@fudan.edu.cn; tjcongliu@gmail.com; zqbi@shiep.edu.cn;
   shanmeijing@ecupl.edu.cn
FU National Natural Science Foundation of China [61633019, 31702393,
   16BFX085]; Natural Science Foundation of Shanghai (CN) [14ZR1429800,
   15ZR1430000, 18ZR1427400, 18ZR1427500]; Ministry of Education of the
   People's Republic of China [EIA140412]; Natural Science Foundation of
   Ningbo [2018A610165, 2018C80004]; Science Foundation of the Department
   of Education of Zhejiang [Y201941227]
FX This work was supported by the National Natural Science Foundation of
   China (61633019, 31702393, 16BFX085), the Natural Science Foundation of
   Shanghai (CN) (14ZR1429800, 15ZR1430000, 18ZR1427400, 18ZR1427500), the
   Ministry of Education of the People's Republic of China (EIA140412), the
   Natural Science Foundation of Ningbo (2018A610165, 2018C80004), and the
   Science Foundation of the Department of Education of Zhejiang
   (Y201941227).
CR [Anonymous], 2013, P 30 INT C MACH LEAR
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2011, P 4 INT C ART INT ST
   [Anonymous], 2012, INT C ART INT STAT
   [Anonymous], 2014, CVPR
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90
   Bengio S, 2015, ADV NEUR IN, V28
   Desjardins G., 2015, NIPS, P2071
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Glorot X., 2010, P INT C ART INT STAT, P249
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Konda Kishore Reddy, 2013, ARXIV13063162
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Memisevic R, 2013, IEEE T PATTERN ANAL, V35, P1829, DOI 10.1109/TPAMI.2013.53
   Memisevic R, 2011, IEEE I CONF COMP VIS, P1591, DOI 10.1109/ICCV.2011.6126419
   Michalski V, 2014, ADV NEUR IN, V27
   Mobahi H., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P737
   Montavon Gregoire, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P621, DOI 10.1007/978-3-642-35289-8_33
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Simonyan Karen., 2014, Advances in neural information processing systems, P568
   Simonyan Karen, 2015, P INT C LEARN REPR, P2017
   Soomro K., 2012, ARXIV12120402CS
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I, 2014, ADV NEUR IN, V27
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Venugopalan S, 2015, DEMYSTIFYING EXPLOSIVES: CONCEPTS IN HIGH ENERGY MATERIALS, P157, DOI 10.1016/B978-0-12-801576-6.00007-0
   Wang L, 2015, IEEE T IMAGE PROCESS, V24, P1424, DOI 10.1109/TIP.2015.2403231
   Wang N., 2013, Advances in Neural Information Processing Systems, P809
   West R, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P515, DOI 10.1145/2566486.2568032
NR 37
TC 1
Z9 1
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 63
DI 10.1145/3357797
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FH
UT WOS:000583712600006
DA 2024-07-18
ER

PT J
AU de Almeida, MA
   Vieira, CC
   de Melo, POSV
   Assuncao, RM
AF de Almeida, Marcos Alves
   Vieira, Carolina Coimbra
   Stancioli Vaz de Melo, Pedro Olmo
   Assuncao, Renato Martins
TI Random Playlists Smoothly Commuting Between Styles
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Music; sound and music computing; system applications and experience;
   knowledge and data engineering tools and techniques
ID MUSIC; GENERATION
AB Someone enjoys listening to playlists while commuting. He wants a different playlist of n songs each day, but always starting from Locked Out of Heaven, a Bruno Mars song. The list should progress in smooth transitions between successive and randomly selected songs until it ends up at Stairway to Heaven, a Led Zeppelin song. The challenge of automatically generating random and heterogeneous playlists is to find the appropriate balance among several conflicting goals. We propose two methods for solving this problem. One is called ROPE, and it depends on a representation of the songs in a Euclidean space. It generates a random path through a Brownian Bridge that connects any two songs selected by the user in this music space. The second is STRAW, which constructs a graph representation of the music space where the nodes are songs and edges connect similar songs. STRAW creates a playlist by traversing the graph through a steering random walk that starts on a selected song and is directed toward a target song also selected by the user. When compared with the state-of-the-art algorithms, our algorithms are the only ones that satisfy the following quality constraints: heterogeneity, smooth transitions, novelty, scalability, and usability. We demonstrate the usefulness of our proposed algorithms by applying them to a large collection of songs and make available a prototype.
C1 [de Almeida, Marcos Alves; Vieira, Carolina Coimbra; Stancioli Vaz de Melo, Pedro Olmo; Assuncao, Renato Martins] Univ Fed Minas Gerais, Dept Comp Sci, Ave Antonio Carlos 6627, Pampulha Belo Horizonte, MG, Brazil.
C3 Universidade Federal de Minas Gerais
RP de Almeida, MA (corresponding author), Univ Fed Minas Gerais, Dept Comp Sci, Ave Antonio Carlos 6627, Pampulha Belo Horizonte, MG, Brazil.
EM marcos.almeida@dcc.ufmg.br; carolcoimbra@dcc.ufmg.br; olmo@dcc.ufmg.br;
   assuncao@dcc.ufmg.br
RI Assuncao, R. M. M Assuncao/I-8025-2012
OI Alvees de Almeida, Marcos/0000-0002-3061-6522
CR Alghoniemy M., 2000, Proceedings ACM Multimedia 2000, P356, DOI 10.1145/354384.375451
   [Anonymous], P INT S MUS INF RETR
   [Anonymous], 2012, ISMIR
   [Anonymous], P 10 INT C MUS INF R
   [Anonymous], 2017, THEY COULD DESTROY A
   [Anonymous], FINDING SONGS SOUND
   [Anonymous], 2013, P 14 INT SOC MUS INF
   [Anonymous], P 6 WORKSH REC SYST
   [Anonymous], P INT S MUS INF RETR
   [Anonymous], ULTIMATE WEDDING PLA
   Aucouturier JJ, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P105, DOI 10.1109/ICME.2002.1035729
   Ben-Elazar S, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P445, DOI 10.1145/3018661.3018718
   Bonnin G, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2652481
   Chen S., 2012, P 18 ACM SIGKDD INT, P714, DOI [DOI 10.1145/2339530.2339643, 10.1145/2339530.2339643]
   Cunningham S.J., 2006, P 7 INT C MUSIC INFO, P240
   Dias R, 2017, MULTIMED TOOLS APPL, V76, P14375, DOI 10.1007/s11042-016-3836-x
   DURRETT R., 2010, PROBABILITY THEORY E
   Fu ZY, 2011, IEEE T MULTIMEDIA, V13, P303, DOI 10.1109/TMM.2010.2098858
   Goussevskaia Olga, 2008, Wl 2008. 2008 IEEE/WIC/ACM International Conference on Web Intelligence. IAT 2008. 2008 IEEE/WIC/ACM International Conference on Intelligent Agent Technology. Wl-IAT Workshop 2008 2008 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology Workshops, P242, DOI 10.1109/WIIAT.2008.20
   Hsu JL, 2011, IEEE SYS MAN CYBERN, P1417, DOI 10.1109/ICSMC.2011.6083868
   Jannach D., 2015, P RECSYS, P187
   Kamehkhosh Iman, 2019, USER MODEL USER-ADAP, P1
   Lee J, 2018, IEEE T MULTIMEDIA, V20, P3173, DOI 10.1109/TMM.2018.2820903
   Levy M, 2009, IEEE T MULTIMEDIA, V11, P383, DOI 10.1109/TMM.2009.2012913
   Mauch M, 2015, ROY SOC OPEN SCI, V2, DOI 10.1098/rsos.150081
   McFee B., 2011, Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR, P537
   McFee B., 2012, The International Society for Music Information Retrieval (ISMIR), P343
   McFee B., 2010, ISMIR, P345
   Miotto R, 2012, ACM T INFORM SYST, V30, DOI 10.1145/2180868.2180870
   Pampalk E., 2005, P 6 INT C MUS INF RE, P634, DOI [DOI 10.5281/ZENODO.1414932, 10.5281/zenodo.1414932]
   Pauws S, 2008, INFORM SCIENCES, V178, P647, DOI 10.1016/j.ins.2007.08.019
   Pohle T, 2007, IEEE T MULTIMEDIA, V9, P567, DOI 10.1109/TMM.2006.887991
   Pontello LF, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3105969
   Ragno R., 2005, MIR 05, P73, DOI [10.1145/1101826.1101840, DOI 10.1145/1101826.1101840]
   Schedl M, 2018, INT J MULTIMED INF R, V7, P95, DOI 10.1007/s13735-018-0154-2
   Taramigkou M., 2013, P 7 ACM C REC SYST, P335, DOI DOI 10.1145/2507157.2507223
   Vall Andreu, 2015, P 9 ACM C REC SYST, P387
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
NR 38
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 104
DI 10.1145/3361742
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800004
DA 2024-07-18
ER

PT J
AU Yuan, Y
   Fang, J
   Lu, XQ
   Feng, YC
AF Yuan, Yuan
   Fang, Jie
   Lu, Xiaoqiang
   Feng, Yachuang
TI Spatial Structure Preserving Feature Pyramid Network for Semantic Image
   Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Semantic image segmentation; spatial resolution; feature pyramid
   network; discriminative capability
AB Recently, progress on semantic image segmentation is substantial, benefiting from the rapid development of Convolutional Neural Networks. Semantic image segmentation approaches proposed lately have been mostly based on Fully convolutional Networks (FCNs). However, these FCN-based methods use large receptive fields and too many pooling layers to depict the discriminative semantic information of the images. Specifically, on one hand, convolutional kernel with large receptive field smooth the detailed edges, since too much contexture information is used to depict the "center pixel." However, the pooling layer increases the receptive field through zooming out the latest feature maps, which loses many detailed information of the image, especially in the deeper layers of the network. These operations often cause low spatial resolution inside deep layers, which leads to spatially fragmented prediction. To address this problem, we exploit the inherent multi-scale and pyramidal hierarchy of deep convolutional networks to extract the feature maps with different resolutions and take full advantages of these feature maps via a gradually stacked fusing way. Specifically, for two adjacent convolutional layers, we upsample the features from deeper layer with stride of 2 and then stack them on the features from shallower layer. Then, a convolutional layer with kernels of 1 x 1 is followed to fuse these stacked features. The fused feature preserves the spatial structure information of the image; meanwhile, it owns strong discriminative capability for pixel classification. Additionally, to further preserve the spatial structure information and regional connectivity of the predicted category label map, we propose a novel loss term for the network. In detail, two graph model-based spatial affinity matrixes are proposed, which are used to depict the pixel-level relationships in the input image and predicted category label map respectively, and then their cosine distance is backward propagated to the network. The proposed architecture, called spatial structure preserving feature pyramid network, significantly improves the spatial resolution of the predicted category label map for semantic image segmentation. The proposed method achieves state-of-the-art results on three public and challenging datasets for semantic image segmentation.
C1 [Yuan, Yuan] Northwestern Polytech Univ, Ctr OPT Imagery Anal & Learning OPTIMAL, Xian 710072, Shaanxi, Peoples R China.
   [Fang, Jie; Lu, Xiaoqiang; Feng, Yachuang] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Key Lab Spectral Imaging Technol, Xian 710119, Shaanxi, Peoples R China.
   [Fang, Jie] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Northwestern Polytechnical University; Chinese Academy of Sciences;
   Xi'an Institute of Optics & Precision Mechanics, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS
RP Lu, XQ (corresponding author), Chinese Acad Sci, Xian Inst Opt & Precis Mech, Key Lab Spectral Imaging Technol, Xian 710119, Shaanxi, Peoples R China.
EM y.yuan1.ieee@gmail.com; fangjie2015@opt.cn; luxq666666@gmail.com;
   fengyachuang@opt.cn
RI Yuan, Yuan/GVS-5120-2022; Yuan, Yuan/ABB-2379-2020; yuan,
   Yuan/ISA-0923-2023
FU National Natural Science Foundation of China [61772510, 61702498]; CAS
   [QYZDY-SSW-JSC044, XAB2017B15]; Chinese Academy of Sciences
   [QYZDB-SSW-JSC015]; National Key R&D Program of China [2017YFB0502900]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772510 and 61702498, in part by the
   Key Research Program of Frontier Sciences, CAS under Grant
   QYZDY-SSW-JSC044, in part by the Young Top-notch Talent Program of
   Chinese Academy of Sciences under Grant QYZDB-SSW-JSC015, in part by the
   National Key R&D Program of China under Grant 2017YFB0502900, in part by
   the CAS "Light of West China" Program under Grant XAB2017B15.
CR Aguilar E, 2018, IEEE T MULTIMEDIA, V20, P3266, DOI 10.1109/TMM.2018.2831627
   [Anonymous], 2017, IEEE SPONSORED INT C
   [Anonymous], N 4 FIELDS NEURAL NE
   [Anonymous], 2006, P IEEE COMPUTER SOC, DOI DOI 10.1109/CVPR.2006.95
   [Anonymous], COMPUTER VISION IMAG
   [Anonymous], 2015, COMPUT SCI
   [Anonymous], 2017, PROC IEEE C COMPUT V
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.348
   [Anonymous], 2015, IEEE T PATTERN ANAL
   [Anonymous], 2013, OVERFEAT INTEGRATED
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-46484-8_29
   [Anonymous], 2013, COMPUTER SCI
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2016, ENET DEEP NEURAL NET
   [Anonymous], STD2P RGBD SEMANTIC
   [Anonymous], SEMI WEAKLY SUPERVIS
   [Anonymous], 2014, 2014 INT C LEARNING
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Castillo C, 2017, INT CONF ACOUST SPEE, P1348, DOI 10.1109/ICASSP.2017.7952376
   Chen L.-C., 2017, IEEE C COMP VIS PATT
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dechesne C, 2017, ISPRS J PHOTOGRAMM, V126, P129, DOI 10.1016/j.isprsjprs.2017.02.011
   Eigen D, 2014, ADV NEUR IN, V27
   Ghiasi G, 2016, LECT NOTES COMPUT SC, V9907, P519, DOI 10.1007/978-3-319-46487-9_32
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   Honari S, 2016, PROC CVPR IEEE, P5743, DOI 10.1109/CVPR.2016.619
   Hong S, 2016, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2016.349
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kampffmeyer M, 2016, IEEE COMPUT SOC CONF, P680, DOI 10.1109/CVPRW.2016.90
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kemker R., 2017, High-Resolution Multispectral Dataset for Semantic Segmentation
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5
   Ravì D, 2017, IEEE T MED IMAGING, V36, P1845, DOI 10.1109/TMI.2017.2695523
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sevilla-Lara L, 2016, PROC CVPR IEEE, P3889, DOI 10.1109/CVPR.2016.422
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xu WD, 2013, BIOINORG CHEM APPL, V2013, DOI 10.1155/2013/959764
   Zhou H, 2016, 2016 INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2016), P9, DOI 10.1109/ICIVC.2016.7571265
NR 54
TC 19
Z9 19
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2019
VL 15
IS 3
AR 73
DI 10.1145/3321512
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ6YR
UT WOS:000494301200005
DA 2024-07-18
ER

PT J
AU Bentaleb, A
   Begen, AC
   Harous, S
   Zimmermann, R
AF Bentaleb, Abdelhak
   Begen, Ali C.
   Harous, Saad
   Zimmermann, Roger
TI Game of Streaming Players: Is Consensus Viable or an Illusion?
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE HAS; QoE; ABR; scalability; instability; unfairness; underutilization;
   game theory; consensus theory
ID RATE ADAPTATION; VIDEO; FAIRNESS
AB The dramatic growth of HTTP adaptive streaming (HAS) traffic represents a practical challenge for service providers in satisfying the demand from their customers. Achieving this in a network where multiple players share the network capacity has so far proved hard because of the bandwidth competition among the HAS players. This competition is exacerbated by the bandwidth overestimation that is introduced due to the isolated and selfish behavior of the HAS players. Each player strives individually to select the maximum bitrate without considering the co-existing players or network resource dynamics. As a result, the HAS players suffer from video quality instability, quality unfairness, and network underutilization or oversubscription, and the players observe a poor quality of experience (QoE). To address this issue, we propose a fully distributed game theory and consensus-based collaborative adaptive bitrate solution for shared network environments, termed Game Theory and consensus-based Approach for Cooperative HAS delivery systems (GTAC). Our solution consists of two-stage games that run in parallel during a streaming session. We extensively evaluate GTAC on a broad set of trace-driven and real-world experiments. Results show that GTAC enhances the viewer QoE by up to 22%, presentation quality stability by up to 24%, fairness by at least 31%, and network utilization by 28% compared to the well-known schemes.
C1 [Bentaleb, Abdelhak; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, 21 Lower Kent Ridge Rd, Singapore 119077, Singapore.
   [Begen, Ali C.] Ozyegin Univ & Networked Media, Dept Comp Sci, Istanbul, Turkey.
   [Harous, Saad] UAE Univ, Coll Informat Technol, Istanbul, Turkey.
C3 National University of Singapore
RP Bentaleb, A (corresponding author), Natl Univ Singapore, Sch Comp, 21 Lower Kent Ridge Rd, Singapore 119077, Singapore.
EM bentaleb@comp.nus.edu.sg; ali.begen@ozyegin.edu.tr; harous@uaeu.ac.ae;
   rogerz@comp.nus.edu.sg
RI Bentaleb, Abdelhak/ABI-3704-2020; Zimmermann, Roger/D-7944-2015; Begen,
   Ali C./R-5897-2016; Harous, Saad/AAU-6859-2020
OI Zimmermann, Roger/0000-0002-7410-2590; Begen, Ali
   C./0000-0002-0835-3017; Harous, Saad/0000-0001-6524-7352; Bentaleb,
   Abdelhak/0000-0002-5382-6530
FU Singapore Ministry of Education Academic Research Fund Tier 1 under
   MOE's grant [T1251RES1820]; UAE University [31T102-UPAR-1-2017]
FX This research has been supported in part by the Singapore Ministry of
   Education Academic Research Fund Tier 1 under MOE's grant T1251RES1820
   and by the grant 31T102-UPAR-1-2017 from UAE University.
CR Akhshabi S, 2012, SIGNAL PROCESS-IMAGE, V27, P271, DOI 10.1016/j.image.2011.10.003
   [Anonymous], SPIE IS T ELECT IMAG
   [Anonymous], 2013, GameTheory
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], 2001, INMON CORPORATIONS S
   [Anonymous], 2014, GUID IMPL DASH AVC 2
   [Anonymous], 230091 DASH ISOIEC
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], 2016, CISC VIS NETW IND GL
   [Anonymous], SSIMWAVES VID QOE MO
   [Anonymous], 1992, The Annals of Applied Probability
   AuYoung A., 2006, Proceedings of the 15th IEEE International Symposium on High Performance Distributed Computing (IEEE Cat. No.06TH8878), P119
   Bacci G, 2016, IEEE SIGNAL PROC MAG, V33, P94, DOI 10.1109/MSP.2015.2451994
   Bentaleb A, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P13, DOI 10.1145/3204949.3204961
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Bentaleb A, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3219752
   Bentaleb A, 2017, IEEE T MULTIMEDIA, V19, P2136, DOI 10.1109/TMM.2017.2733344
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Duanmu ZF, 2017, IEEE J-STSP, V11, P154, DOI 10.1109/JSTSP.2016.2608329
   Habibi G, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1315
   Han Z., 2012, GAME THEORY WIRELESS
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Ieong S., 2005, P 6 ACM C EL COMM, P193, DOI DOI 10.1145/1064009.1064030
   Jiang JC, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P393
   Jiang JC, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P137
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Juluri P, 2015, IEEE INT CONF COMM, P1765, DOI 10.1109/ICCW.2015.7247436
   Kreutz D, 2015, P IEEE, V103, P14, DOI 10.1109/JPROC.2014.2371999
   Krishnamoorthi V, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P76, DOI 10.1145/3083187.3083193
   La Q. D., 2016, POTENTIAL GAME THEOR
   Lederer S., 2012, P 3 MULT SYST C, P89
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   Mu M, 2016, IEEE J SEL AREA COMM, V34, P2168, DOI 10.1109/JSAC.2016.2577318
   Olfati-Saber R, 2007, P IEEE, V95, P215, DOI 10.1109/JPROC.2006.887293
   Palomar DP, 2006, IEEE J SEL AREA COMM, V24, P1439, DOI 10.1109/JSAC.2006.879350
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Sherry J, 2015, ACM SIGCOMM COMP COM, V45, P213, DOI 10.1145/2829988.2787502
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Thomas Emmanuel, 2017, Motion Imaging Journal, V126, P22, DOI 10.5594/JMI.2016.2632338
   Wang Y, 2010, IEEE T CONTR SYST T, V18, P267, DOI 10.1109/TCST.2009.2017934
   Wolpert D. H., 2001, Adv. Complex Syst., V4, P265, DOI DOI 10.1142/S0219525901000188
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 47
TC 3
Z9 3
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 65
DI 10.1145/3336496
PG 30
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900018
DA 2024-07-18
ER

PT J
AU Mao, H
   She, J
   Cheung, M
AF Mao, Hui
   She, James
   Cheung, Ming
TI Visual Arts Search on Mobile Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual arts search; mobile devices; feature learning
AB Visual arts, especially paintings, appear everywhere in our daily lives. They are not only liked by art lovers but also by ordinary people, both of whom are curious about the stories behind these artworks and also interested in exploring related artworks. Among various methods, the mobile visual search has its merits in providing an alternative solution to text and voice searches, which are not always applicable. Mobile visual search for visual arts is far more challenging than the general image visual search. Conventionally, visual search, such as searching products and plant, focuses on locating images containing similar objects. Hence, approaches are designed to locate objects and extract scale-invariant features from distorted photos that are captured by the mobile camera. However, the objects are only part of the visual art piece; the background and the painting style are both important factors that are not considered in the conventional approaches. In this article, an empirical investigation is conducted to study issues in photos taken by mobile cameras, such as orientation variance and motion blur, and how they influence the results of the mobile visual arts search. Based on the empirical investigation results, a photo-rectification pipeline is designed to rectify the photos into perfect images for feature extraction. A new method is proposed to learn high discriminative features for visual arts, which considers both the content information and style information in visual arts. Apart from conducting solid experiments, a real-world system is built to prove the effectiveness of the proposed methods. To the best of our knowledge, this is the first article to solve problems for visual arts search on mobile devices.
C1 [Mao, Hui; She, James; Cheung, Ming] HKUST NIE Social Media Lab, Hong Kong, Peoples R China.
RP Mao, H (corresponding author), HKUST NIE Social Media Lab, Hong Kong, Peoples R China.
EM hmaoaa@connect.ust.hk; eejames@usi.lik; cpming@connect.ust.hk
FU HKUST-NIE Social Media Lab., HKUST
FX This work is supported by HKUST-NIE Social Media Lab., HKUST.
CR Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], 2017, CVPR
   [Anonymous], 2010, P 18 ACM INT C MULT
   [Anonymous], IJCAI P INT JOINT C
   [Anonymous], P EUR C COMP VIS WOR
   [Anonymous], P 11 ACM INT C MULT
   [Anonymous], 2015, PROC 28 INT C NEURAL
   Anwer RM, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P339, DOI 10.1145/2911996.2912063
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen DM, 2010, IEEE DATA COMPR CONF, P525, DOI 10.1109/DCC.2010.53
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Crowley EJ, 2015, LECT NOTES COMPUT SC, V8925, P54, DOI 10.1007/978-3-319-16178-5_4
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gatys L., 2015, NIPS
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ignatov A., 2017, P IEEE INT C COMP VI
   Ignatov Andrey, 2018, CVPRW, P691
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   King DB, 2015, ACS SYM SER, V1214, P1
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mao H, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1183, DOI 10.1145/3123266.3123405
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Sang JT, 2013, IEEE T MULTIMEDIA, V15, P1665, DOI 10.1109/TMM.2013.2268052
   Simonyan K., 2014, 14091556 ARXIV
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Noord N, 2015, IEEE SIGNAL PROC MAG, V32, P46, DOI 10.1109/MSP.2015.2406955
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Wang Y, 2011, PROCEEDINGS OF THE 4TH CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE AND SYSTEMS DYNAMICS, SSMSSD10, VOL 4, P73
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yu F.X., 2011, Proceedings_of_the_19th_ACM_International_Conference_on_Multimedia, MM'11, P3
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 48
TC 5
Z9 5
U1 1
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 60
DI 10.1145/3326336
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900013
DA 2024-07-18
ER

PT J
AU Liu, ZG
   Xia, YJ
   Liu, Q
   He, QM
   Zhang, C
   Zimmermann, R
AF Liu, Zhenguang
   Xia, Yingjie
   Liu, Qi
   He, Qinming
   Zhang, Chao
   Zimmermann, Roger
TI Toward Personalized Activity Level Prediction in Community Question
   Answering Websites
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Question answering website; activity level prediction; logistic
   regression; personalized model
AB Community Question Answering (CQA) websites have become valuable knowledge repositories. Millions of internet users resort to CQA websites to seek answers to their encountered questions. CQA websites provide information far beyond a search on a site such as Google due to (1) the plethora of high-quality answers, and (2) the capabilities to post new questions toward the communities of domain experts. While most research efforts have been made to identify experts or to preliminarily detect potential experts of CQA websites, there has been a remarkable shift toward investigating how to keep the engagement of experts. Experts are usually the major contributors of high-quality answers and questions of CQA websites. Consequently, keeping the expert communities active is vital to improving the lifespan of these websites. In this article, we present an algorithm termed PALP to predict the activity level of expert users of CQA websites. To the best of our knowledge, PALP is the first approach to address a personalized activity level prediction model for CQA websites. Furthermore, it takes into consideration user behavior change over time and focuses specifically on expert users. Extensive experiments on the Stack Overflow website demonstrate the competitiveness of PALP over existing methods.
C1 [Liu, Zhenguang; Liu, Qi; Zimmermann, Roger] Natl Univ Singapore, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore.
   [Liu, Zhenguang; Xia, Yingjie; He, Qinming] Zhejiang Univ, 38 Zheda Rd, Hangzhou 310027, Zhejiang, Peoples R China.
   [Zhang, Chao] Univ Illinois, 801 South Wright St, Urbana, IL 61801 USA.
C3 National University of Singapore; Zhejiang University; University of
   Illinois System; University of Illinois Urbana-Champaign
RP Liu, ZG (corresponding author), Natl Univ Singapore, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore.; Liu, ZG (corresponding author), Zhejiang Univ, 38 Zheda Rd, Hangzhou 310027, Zhejiang, Peoples R China.
EM liuzhenguang2008@gmail.com; xiayingjie@zju.edu.cn; qiliumize@gmail.com;
   hqm@zju.edu.cn; czhang82@illinois.edu; rogerz@comp.nus.edu.sg
RI Zhang, Chao/AAR-7251-2020; Zimmermann, Roger/D-7944-2015; Wang,
   Yiru/JMB-2281-2023; Liu, Qi/AAE-3162-2019
OI Zhang, Chao/0000-0003-3009-598X; Zimmermann, Roger/0000-0002-7410-2590;
   Liu, Qi/0000-0001-5378-6404
FU NSFC [61472359]; Singapore's Ministry of Education (MOE) Academic
   Research Fund Tier 1 [T1 251RES1415]; National University of Singapore
   (Suzhou) Research Institute
FX This research was supported by the NSFC Grant no. 61472359, Singapore's
   Ministry of Education (MOE) Academic Research Fund Tier 1 under Grant
   no. T1 251RES1415, and by the National University of Singapore (Suzhou)
   Research Institute.
CR Anderson Ashton, 2012, P 18 ACM SIGKDD INT, DOI DOI 10.1145/2339530.2339665
   Asaduzzaman M, 2013, IEEE WORK CONF MIN S, P97, DOI 10.1109/MSR.2013.6624015
   Belletti F., 2017, CORR
   Chapelle O, 2011, MACH LEARN, V85, P149, DOI 10.1007/s10994-010-5231-6
   Chen Y H, 2012, Conference on Lasers and Electro-Optics (CLEO) 39, P1
   Dalip DH, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P543
   Dasgupta K., 2008, EDBT, P668
   Dror Gideon, 2012, P 21 INT C WORLD WID, P829, DOI 10.1145/2187980.2188207
   Grant S, 2013, IEEE WORK CONF MIN S, P65, DOI 10.1109/MSR.2013.6624007
   Guyon Isabelle., 2009, JMLR WCP, V7, P1
   Jiang Yang, 2010, P 4 INT C WEBL SOC M
   Kawale Jaya, 2009, 2009 International Conference on Computational Science and Engineering (CSE), P423, DOI 10.1109/CSE.2009.80
   Lampe C., 2005, P 2005 INT ACM SIGGR, P11, DOI [DOI 10.1145/1099203.1099206, https://doi.org/10.1145/1099203.1099206]
   Liu DR, 2013, INFORM PROCESS MANAG, V49, P312, DOI 10.1016/j.ipm.2012.07.002
   Liu D, 2014, INT J APPROX REASON, V55, P197, DOI 10.1016/j.ijar.2013.02.013
   Liu XL, 2016, MULTIMED TOOLS APPL, V75, P1495, DOI 10.1007/s11042-014-2085-0
   Movshovitz-Attias Dana, 2013, 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), P886
   Nie LQ, 2015, IEEE T KNOWL DATA EN, V27, P2107, DOI 10.1109/TKDE.2015.2399298
   PAL A, 2012, P 6 INT AAAI C WEBL, P274, DOI DOI 10.1609/ICWSM.V6I1.14262
   Pal A, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P923, DOI 10.1145/2766462.2767774
   Pal A, 2011, LECT NOTES COMPUT SC, V6787, P231, DOI 10.1007/978-3-642-22362-4_20
   Pang Lei, 2016, TOMCCAP, V12
   Pudipeddi J, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P469, DOI 10.1145/2567948.2576965
   Riahi F., 2012, P 21 INT C WORLD WID, P791, DOI [10.1145/2187980.2188202, DOI 10.1145/2187980.2188202]
   Shah C., 2010, P 33 INT ACM SIGIR C, DOI [10. 1145/1835449. 1835518, DOI 10.1145/1835449.1835518]
   Tian Qiongjie, 2013, P ICWSM
   Wang W, 2016, IEEE MULTIMEDIA, V23, P80, DOI 10.1109/MMUL.2016.69
   Wang YZ, 2016, IEEE IJCNN, P1896, DOI 10.1109/IJCNN.2016.7727431
   Wu Ming-Ju, 2015, TOMCCAP, V12
   Zhou Zhao, 2015, IEEE T KNOWL DATA EN, V27, P1
   Zhu Y, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P159, DOI 10.1145/2505515.2505518
NR 31
TC 7
Z9 7
U1 1
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 41
DI 10.1145/3187011
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TD
UT WOS:000434634700014
DA 2024-07-18
ER

PT J
AU Akputu, OK
   Seng, KP
   Lee, Y
   Ang, LM
AF Akputu, Oryina Kingsley
   Seng, Kah Phooi
   Lee, Yunli
   Ang, Li-Minn
TI Emotion Recognition Using Multiple Kernel Learning toward E-learning
   Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE E-learning; emotion recognition; multiple kernel learning; SimpleMKL;
   decision tree; multiclass classification
AB Adaptive Educational Hypermedia (AEH) e-learning models aim to personalize educational content and learning resources based on the needs of an individual learner. The Adaptive Hypermedia Architecture (AHA) is a specific implementation of the AEH model that exploits the cognitive characteristics of learner feedback to adapt resources accordingly. However, beside cognitive feedback, the learning realm generally includes both the affective and emotional feedback of the learner, which is often neglected in the design of e-learning models. This article aims to explore the potential of utilizing affect or emotion recognition research in AEH models. The framework is referred to as Multiple Kernel Learning Decision Tree Weighted Kernel Alignment (MKLDT-WFA). The MKLDT-WFA has two merits over classical MKL. First, the WFA component only preserves the relevant kernel weights to reduce redundancy and improve the discrimination for emotion classes. Second, training via the decision tree reduces the misclassification issues associated with the SimpleMKL. The proposed work has been evaluated on different emotion datasets and the results confirm the good performances. Finally, the conceptual Emotion-based E-learning Model (EEM) with the proposed emotion recognition framework is proposed for future work.
C1 [Akputu, Oryina Kingsley; Lee, Yunli] Sunway Univ, Comp & Informat Syst Dept, 5 Jalan Univ, Bandar Sunway 47500, Selangor, Malaysia.
   [Seng, Kah Phooi; Ang, Li-Minn] Charles Sturt Univ, Sch Comp & Math, Locked Bag 588, Wagga Wagga, NSW 2678, Australia.
C3 Sunway University; Charles Sturt University
RP Akputu, OK (corresponding author), Sunway Univ, Comp & Informat Syst Dept, 5 Jalan Univ, Bandar Sunway 47500, Selangor, Malaysia.
EM oryinak@gmail.com
RI ang, li-minn/AAG-5986-2020; Jemni, Mohamed/HCI-9541-2022; Lee, Yun
   Li/W-8656-2019; Akputu, Oryina K./AAA-1802-2022; Lee, Yun
   Li/IUQ-2277-2023
OI ang, li-minn/0000-0002-2402-7529; Jemni, Mohamed/0000-0001-8841-5224;
   Lee, Yun Li/0000-0003-3531-5873; Akputu, Oryina K./0000-0003-4520-0084;
   Lee, Yun Li/0000-0003-3531-5873
CR [Anonymous], 2003, Authoring Tools for Advanced Technology Learning Environments: Toward cost-effective adaptive, interactive, and intelligent educational software, DOI [10.1007/978-94-017-0819-7_13, DOI 10.1007/978-94-017-0819-7_13]
   [Anonymous], 2014, P 16 INT C MULT INT
   [Anonymous], 2009, INT J GEOMATH, DOI DOI 10.1007/S13137-020-00149-9
   Bahreini K, 2016, INTERACT LEARN ENVIR, V24, P590, DOI 10.1080/10494820.2014.908927
   Bahreini K, 2015, LECT NOTES COMPUT SC, V9221, P107, DOI 10.1007/978-3-319-22960-7_11
   Bucak SS, 2014, IEEE T PATTERN ANAL, V36, P1354, DOI 10.1109/TPAMI.2013.212
   Cristianini N, 2006, STUD FUZZ SOFT COMP, V194, P205
   D'Mello S, 2012, INT J HUM-COMPUT ST, V70, P377, DOI 10.1016/j.ijhcs.2012.01.004
   DAUGMAN JG, 1988, IEEE T ACOUST SPEECH, V36, P1169, DOI 10.1109/29.1644
   de Bra P., 1998, New Review of Hypermedia and Multimedia, V4, P115, DOI 10.1080/13614569808914698
   Dhall A., 2014, PROC ICMI, P461, DOI DOI 10.1145/2663204.2666275
   Ekman P, 1978, FACIAL ACTION CODING
   Litman DJ, 2006, SPEECH COMMUN, V48, P559, DOI 10.1016/j.specom.2005.09.008
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Oryina AK, 2016, P INT COMP SOFTW APP, P509, DOI 10.1109/COMPSAC.2016.106
   Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424
   Paule-Ruiz MPuerto, 2013, P 18 ACM C INN TECHN, P34
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Rakotomamonjy Alain, 2008, SOFTWARES TOOLBOXES
   Sarrafzadeh A, 2008, COMPUT HUM BEHAV, V24, P1342, DOI 10.1016/j.chb.2007.07.008
   Sebe N, 2006, INT C PATT RECOG, P1136
   Sebe N, 2009, J AMB INTEL SMART EN, V1, P23, DOI 10.3233/AIS-2009-0003
   Shen LP, 2008, INFORM SYST FRONT, V10, P461, DOI 10.1007/s10796-008-9104-5
   Song T, 2012, INT CONF SIGN PROCES, P1583, DOI 10.1109/ICoSP.2012.6491882
   Taheri S, 2013, IEEE T AFFECT COMPUT, V4, P360, DOI 10.1109/T-AFFC.2013.28
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang YJ, 2008, IEEE T MULTIMEDIA, V10, P659, DOI 10.1109/TMM.2008.921734
   Wong JJ, 2009, EXPERT SYST APPL, V36, P804, DOI 10.1016/j.eswa.2007.10.030
NR 28
TC 20
Z9 20
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2018
VL 14
IS 1
AR 1
DI 10.1145/3131287
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW9HO
UT WOS:000425646500001
DA 2024-07-18
ER

PT J
AU Cizmeci, B
   Xu, X
   Chaudhari, R
   Bachhuber, C
   Alt, N
   Steinbach, E
AF Cizmeci, Burak
   Xu, Xiao
   Chaudhari, Rahul
   Bachhuber, Christoph
   Alt, Nicolas
   Steinbach, Eckehard
TI A Multiplexing Scheme for Multimodal Teleoperation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Haptics; haptic compression and communication; teleoperation;
   human-robot interaction over communication networks; multiplexing;
   congestion control; rate control
ID DATA REDUCTION; HAPTIC DATA; TELEPRESENCE; TRANSMISSION; BANDWIDTH;
   DELAY; TCP; PERCEPTION; TIME
AB This article proposes an application-layer multiplexing scheme for teleoperation systems with multimodal feedback (video, audio, and haptics). The available transmission resources are carefully allocated to avoid delay-jitter for the haptic signal potentially caused by the size and arrival time of the video and audio data. Themultiplexing scheme gives high priority to the haptic signal and applies a preemptive-resume scheduling strategy to stream the audio and video data. The proposed approach estimates the available transmission rate in real time and adapts the video bitrate, data throughput, and force buffer size accordingly. Furthermore, the proposed scheme detects sudden transmission rate drops and applies congestion control to avoid abrupt delay increases and converge promptly to the altered transmission rate. The performance of the proposed scheme is measured objectively in terms of end-to-end signal latencies, packet rates, and peak signal-to-noise ratio (PSNR) for visual quality. Moreover, peak-delay and convergence time measurements are carried out to investigate the performance of the congestion control mode of the system.
C1 [Cizmeci, Burak; Xu, Xiao; Chaudhari, Rahul; Bachhuber, Christoph; Alt, Nicolas; Steinbach, Eckehard] Tech Univ Munich, Chair Media Technol, Arcisstr 21, D-80333 Munich, Germany.
C3 Technical University of Munich
RP Cizmeci, B (corresponding author), Tech Univ Munich, Chair Media Technol, Arcisstr 21, D-80333 Munich, Germany.
EM burak.cizmeci@tum.de; xiao.xu@tum.de; rahul.chaudhari@tum.de;
   christoph.bachhuber@tum.de; n.alt@tum.de; eckehard.steinbach@tum.de
OI Bachhuber, Christoph/0000-0002-9808-0162; Steinbach,
   Eckehard/0000-0001-8853-2703; xu, xiao/0000-0002-4375-3884
FU European Research Council under European Union's Seventh Framework
   Programme (FP7)/ERC Grant [258941]
FX This work has been supported by the European Research Council under the
   European Union's Seventh Framework Programme (FP7/2007-2013)/ERC Grant
   agreement no. 258941. Special thanks to Kerem Cizmeci, who designed the
   3-D models of the experimental platform.
CR Al Osman H., 2007, H. Al Osman, M. Eid, A. El Saddik, ALPHAN: Application Layer Protocol for HAptic Networking. In proceeding of IEEE HAVE, pp.96-101, 2007., P96
   Allied Vision, 1992, MAKO G 223 GIG ETH I
   [Anonymous], 1996, FORCE TOUCH FEEDBACK
   [Anonymous], 1851, LEHRE TASTSINN GEMEI
   Bachhuber C, 2016, IEEE IMAGE PROC, P2132, DOI 10.1109/ICIP.2016.7532735
   BRAKMO LS, 1995, IEEE J SEL AREA COMM, V13, P1465, DOI 10.1109/49.464716
   Buss M., 2005, IFAC P VOLUMES, V38, P77
   Capone A, 2004, IEEE T MOBILE COMPUT, V3, P129, DOI 10.1109/TMC.2004.5
   Casetti C, 2002, WIREL NETW, V8, P467, DOI 10.1023/A:1016590112381
   Cen Z., 2005, IEEE INT C MOB ADH S, P340
   Cen ZW, 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, Vols 1-4, P2158
   Cha J, 2007, WORLD HAPTICS 2007: SECOND JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P274
   Cizmeci B, 2014, LECT NOTES COMPUT SC, V8619, P131, DOI 10.1007/978-3-662-44196-1_17
   COLGATE JE, 1994, IEEE INT CONF ROBOT, P3205, DOI 10.1109/ROBOT.1994.351077
   Demircin MU, 2008, IEEE T MULTIMEDIA, V10, P1155, DOI 10.1109/TMM.2008.2001383
   Dinc E, 2015, IEEE VEH TECHNOL MAG, V10, P86, DOI 10.1109/MVT.2015.2410786
   Eid M, 2011, IEEE T INSTRUM MEAS, V60, P21, DOI 10.1109/TIM.2010.2065530
   FERRELL WR, 1965, IEEE TRANS HUM FACT, VHFE6, P24, DOI 10.1109/THFE.1965.6591253
   Gao M, 2015, 2015 Picture Coding Symposium (PCS) with 2015 Packet Video Workshop (PV), P210, DOI 10.1109/PCS.2015.7170077
   Goeller M., 2012, Proceedings of the 2012 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2018, DOI 10.1109/ROBIO.2012.6491265
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P840, DOI 10.1109/TCSVT.2002.804883
   Hinterseer P, 2005, INT CONF ACOUST SPEE, P1097
   Hinterseer P, 2008, IEEE T SIGNAL PROCES, V56, P588, DOI 10.1109/TSP.2007.906746
   Isomura E, 2011, 2011 IEEE REGION 10 CONFERENCE TENCON 2011, P1085, DOI 10.1109/TENCON.2011.6129278
   *ITU T, 2005, H264 ITUT
   ITU-T, 1993, ITU T JPEG STAND DIG
   Kaede S, 2015, 2015 IEEE 18TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND ENGINEERING (CSE), P35, DOI 10.1109/CSE.2015.24
   KUKA Robotics, 1898, KUKA LEIGHT WEIGHT R, V1898
   Kuschel M, 2006, IEEE INT CONF ROBOT, P2933, DOI 10.1109/ROBOT.2006.1642147
   Lee SCM, 2006, LECT NOTES COMPUT SC, V4033, P13, DOI 10.1145/1230040.1230078
   Li P, 2005, 2005 IEEE NETWORKING, SENSING AND CONTROL PROCEEDINGS, P1049
   Marshall A, 2008, ADV MULTIMED, V2008, DOI 10.1155/2008/841590
   Merritt L., 2004, 2004 X264 HIGH PERFO
   PortAudio, 1999, PORT CROSS PLATF AUD
   Pyke J, 2007, P ANN INT IEEE EMBS, P3094, DOI 10.1109/IEMBS.2007.4352982
   Ramming John, 1983, JR3 6 DOF FORCE TORQ
   Rank M, 2010, PRESENCE-TELEOP VIRT, V19, P389, DOI 10.1162/pres_a_00021
   Ryu JH, 2010, MECHATRONICS, V20, P812, DOI 10.1016/j.mechatronics.2010.07.006
   SHERIDAN TB, 1993, IEEE T ROBOTIC AUTOM, V9, P592, DOI 10.1109/70.258052
   Silva JM, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457451
   Stoica I., 1998, Computer Communication Review, V28, P118, DOI 10.1145/285243.285273
   Uchimura Y, 2002, 7TH INTERNATIONAL WORKSHOP ON ADVANCED MOTION CONTROL, PROCEEDINGS, P63, DOI 10.1109/AMC.2002.1026892
   Valin JM, 2010, IEEE T AUDIO SPEECH, V18, P58, DOI 10.1109/TASL.2009.2023186
   Vogels IMLC, 2004, HUM FACTORS, V46, P118, DOI 10.1518/hfes.46.1.118.30394
   Walraevens J., 2005, THESIS
   Xu X, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P512, DOI 10.1109/WHC.2015.7177763
   Yamamoto Shingo, 2014, 2014 IEEE 13th International Workshop on Advanced Motion Control (AMC), P569, DOI 10.1109/AMC.2014.6823344
   Yashiro D, 2013, ELECTR COMMUN JPN, V96, P26, DOI 10.1002/ecj.11516
   Zhang F, 2011, INT CONF ACOUST SPEE, P813
NR 49
TC 26
Z9 27
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2017
VL 13
IS 2
AR 21
DI 10.1145/3063594
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EV1VK
UT WOS:000401537300009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Grant, JM
   Flynn, PJ
AF Grant, Jason M.
   Flynn, Patrick J.
TI Crowd Scene Understanding from Video: A Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Crowd analysis; human activity; datasets
ID TRACKING; BEHAVIORS; PEOPLE
AB Crowd video analysis has applications in crowd management, public space design, and visual surveillance. Example tasks potentially aided by automated analysis include anomaly detection (such as a person walking against the grain of traffic or rapid assembly/dispersion of groups of people), population and density measurements, and interactions between groups of people. This survey explores crowd analysis as it relates to two primary research areas: crowd statistics and behavior understanding. First, we survey methods for counting individuals and approximating the density of the crowd. Second, we showcase research efforts on behavior understanding as related to crowds. These works focus on identifying groups, interactions within small groups, and abnormal activity detection such as riots and bottlenecks in large crowds. Works presented in this section also focus on tracking groups of individuals, either as a single entity or a subset of individuals within the frame of reference. Finally, a summary of datasets available for crowd activity video research is provided.
C1 [Grant, Jason M.; Flynn, Patrick J.] Univ Notre Dame, Dept Comp Sci & Engn, 384 Fitzpatrick Hall Engn, Notre Dame, IN 46556 USA.
C3 University of Notre Dame
RP Grant, JM (corresponding author), Univ Notre Dame, Dept Comp Sci & Engn, 384 Fitzpatrick Hall Engn, Notre Dame, IN 46556 USA.
EM jgrant3@nd.edu; flynn@nd.edu
OI Grant, Jason/0000-0003-0751-1623
CR Albiol A, 2001, IEEE T INTELL TRANSP, V2, P204, DOI 10.1109/6979.969366
   Ali S, 2008, LECT NOTES COMPUT SC, V5303, P1, DOI 10.1007/978-3-540-88688-4_1
   Ali S, 2007, PROC CVPR IEEE, P65
   Allan P., 1983, P 1 AUSTR MACADAMIA, P1
   [Anonymous], 2008, P 2008 IEEE C COMPUT, DOI DOI 10.1109/CVPR.2008.4587735
   [Anonymous], UT-Interaction Dataset, ICPR contest on Semantic Description of Human Activities (SDHA)
   [Anonymous], P 10 INT C INF SYST
   [Anonymous], P 13 EUR C COMP VIS
   [Anonymous], DET UN CROWD ACT
   [Anonymous], CVPR
   [Anonymous], 2008, IEEE C COMP VIS PATT
   [Anonymous], EARTH OBSERV MAG
   Arteta C, 2013, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2013.415
   Arteta C, 2012, LECT NOTES COMPUT SC, V7510, P348, DOI 10.1007/978-3-642-33415-3_43
   Bandini S., 2012, Proceedings of Measuring Behavior, V2012, P308
   Bartoli Federico, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P19, DOI 10.1109/CVPRW.2015.7301279
   Birchfield S, 1998, PROC CVPR IEEE, P232, DOI 10.1109/CVPR.1998.698614
   Briassouli A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P928, DOI 10.1109/ICCVW.2011.6130351
   Chan AB, 2012, IEEE T IMAGE PROCESS, V21, P2160, DOI 10.1109/TIP.2011.2172800
   Chen TH, 2006, 2006 IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS, VOLS 1 AND 2, P97
   Cheriyadat AM, 2008, IEEE J-STSP, V2, P568, DOI 10.1109/JSTSP.2008.2001306
   Cho SY, 1999, IEEE T SYST MAN CY B, V29, P535, DOI 10.1109/3477.775269
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Cong Y, 2009, PROC CVPR IEEE, P1093, DOI 10.1109/CVPRW.2009.5206648
   DAVIES AC, 1995, ELECTRON COMMUN ENG, V7, P37, DOI 10.1049/ecej:19950106
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Ferris J, 2009, PALG STUD THEAT PERF, P1
   Fruin J., 1992, Designing for Pedestrians
   Fu HY, 2014, MULTIMED TOOLS APPL, V73, P273, DOI 10.1007/s11042-013-1608-4
   Fujiyoshi H, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P15, DOI 10.1109/ACV.1998.732852
   Gao CQ, 2016, MULTIMED TOOLS APPL, V75, P9315, DOI 10.1007/s11042-016-3344-z
   Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   Helbing D, 2007, PHYS REV E, V75, DOI 10.1103/PhysRevE.75.046109
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jodoin PM, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P1, DOI 10.1109/AVSS.2013.6636607
   Kiryati N, 2008, INT C PATT RECOG, P3015
   Kong D, 2006, INT C PATT RECOG, P1187
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li Z, 2008, INT C WAVEL ANAL PAT, P1, DOI 10.1109/ICWAPR.2008.4635740
   Lim MK, 2014, INT C PATT RECOG, P3957, DOI 10.1109/ICPR.2014.678
   Lin SF, 2001, IEEE T SYST MAN CY A, V31, P645, DOI 10.1109/3468.983420
   Liu JC, 2010, PROC CVPR IEEE, P1839, DOI 10.1109/CVPR.2010.5539855
   Liu Z, 2000, IEEE IMAGE PROC, P53, DOI 10.1109/ICIP.2000.900890
   Loy CC, 2013, IEEE I CONF COMP VIS, P2256, DOI 10.1109/ICCV.2013.270
   Ma Z, 2015, PROC CVPR IEEE, P3689, DOI 10.1109/CVPR.2015.7298992
   Ma Z, 2013, PROC CVPR IEEE, P2539, DOI 10.1109/CVPR.2013.328
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Marana AN, 1998, SIBGRAPI '98 - INTERNATIONAL SYMPOSIUM ON COMPUTER GRAPHICS, IMAGE PROCESSING, AND VISION, PROCEEDINGS, P354, DOI 10.1109/SIBGRA.1998.722773
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Ming-Ching Chang, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P56, DOI 10.1109/AVSS.2010.65
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   Moussaïd M, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010047
   National Research Council, 2000, Highway Capacity Manual
   Parameswaran V, 2006, INT J COMPUT VISION, V66, P83, DOI 10.1007/s11263-005-3671-4
   Perera AGA, 2006, IEEE COMP SOC C COMP, P666
   Rahmalan H., 2006, Institution of Engineering and Technology Conference on Crime and Security, P540
   Rao S, 2003, TENCON IEEE REGION, P369, DOI 10.1109/TENCON.2003.1273347
   REGAZZONI CS, 1993, PROCEEDINGS OF THE IECON 93 - INTERNATIONAL CONFERENCE ON INDUSTRIAL ELECTRONICS, CONTROL, AND INSTRUMENTATION, VOLS 1-3, P1860, DOI 10.1109/IECON.1993.339357
   Rodrigues de Almeida Igor, 2013, 2013 XXVI Conference on Graphics, Patterns and Images (SIBGRAPI 2013), P63, DOI 10.1109/SIBGRAPI.2013.18
   Rodriguez M, 2011, IEEE I CONF COMP VIS, P2423, DOI 10.1109/ICCV.2011.6126526
   Rodriguez M, 2011, IEEE I CONF COMP VIS, P1235, DOI 10.1109/ICCV.2011.6126374
   Rodriguez M, 2009, IEEE I CONF COMP VIS, P1389, DOI 10.1109/ICCV.2009.5459301
   Sánchez J, 2013, IMAGE PROCESS ON LIN, V3, P137, DOI 10.5201/ipol.2013.26
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285
   Jacques JCS, 2010, IEEE SIGNAL PROC MAG, V27, P66, DOI 10.1109/MSP.2010.937394
   Solera Francesco, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P43, DOI 10.1109/CVPRW.2015.7301282
   Solmaz B, 2012, IEEE T PATTERN ANAL, V34, P2064, DOI 10.1109/TPAMI.2012.123
   Soomro K., 2012, ARXIV12120402CS
   Wongun Choi, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1282, DOI 10.1109/ICCVW.2009.5457461
   Wu XY, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-3, P214, DOI 10.1109/ROBIO.2006.340379
   Yamaguchi K, 2011, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2011.5995468
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yu T, 2004, PROC CVPR IEEE, P834
   Zhan BB, 2008, MACH VISION APPL, V19, P345, DOI 10.1007/s00138-008-0132-4
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang XC, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P215, DOI 10.1109/AVSS.2012.82
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhou BL, 2015, INT J COMPUT VISION, V111, P50, DOI 10.1007/s11263-014-0735-3
   Zhou BL, 2014, IEEE T PATTERN ANAL, V36, P1586, DOI 10.1109/TPAMI.2014.2300484
   Zhou BL, 2012, PROC CVPR IEEE, P2871, DOI 10.1109/CVPR.2012.6248013
NR 82
TC 61
Z9 67
U1 3
U2 75
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2017
VL 13
IS 2
AR 19
DI 10.1145/3052930
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EV1VK
UT WOS:000401537300007
DA 2024-07-18
ER

PT J
AU Chiu, CY
   Liou, YC
   Prayoonwong, A
AF Chiu, Chih-Yi
   Liou, Yu-Cyuan
   Prayoonwong, Amorntip
TI Approximate Asymmetric Search for Binary Embedding Codes
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Nearest neighbor search; multi-index hashing and voting; asymmetric
   distance
AB In this article, we propose a method of approximate asymmetric nearest-neighbor search for binary embedding codes. The asymmetric distance takes advantage of less information loss at the query side. However, calculating asymmetric distances through exhaustive search is prohibitive in a large-scale dataset. We present a novel method, called multi-index voting, that integrates the multi-index hashing technique with a voting mechanism to select appropriate candidates and calculate their asymmetric distances. We show that the candidate selection scheme can be formulated as the tail of the binomial distribution function. In addition, a binary feature selection method based on minimal quantization error is proposed to address the memory insufficiency issue and improve the search accuracy. Substantial experimental evaluations were made to demonstrate that the proposed method can yield an approximate accuracy to the exhaustive search method while significantly accelerating the runtime. For example, one result shows that in a dataset of one billion 256-bit binary codes, examining only 0.5% of the dataset, can reach 95-99% close accuracy to the exhaustive search method and accelerate the search by 73-128 times. It also demonstrates an excellent tradeoff between the search accuracy and time efficiency compared to the state-of-the-art nearest-neighbor search methods. Moreover, the proposed feature selection method shows its effectiveness and improves the accuracy up to 8.35% compared with other feature selection methods.
C1 [Chiu, Chih-Yi; Liou, Yu-Cyuan; Prayoonwong, Amorntip] Natl Chiayi Univ, Dept Comp Sci & Informat Engn, No 300 Syuefu Rd, Chiayi 60004, Taiwan.
C3 National Chiayi University
RP Chiu, CY (corresponding author), Natl Chiayi Univ, Dept Comp Sci & Informat Engn, No 300 Syuefu Rd, Chiayi 60004, Taiwan.
EM cychiu@mail.ncyu.edu.tw; s1020435@mail.ncyu.edu.tw;
   s1040463@mail.ncyu.edu.tw
RI Chiu, Chih-Yi/AAN-2961-2020
OI Chiu, Chih-Yi/0000-0002-2859-6120
FU Ministry of Science and Technology, Taiwan [MOST 103-2221-E-415-009-MY3,
   MOST 104-2628-E-415-002-MY2]
FX This work was supported by the Ministry of Science and Technology,
   Taiwan, wider grants MOST 103-2221-E-415-009-MY3 and MOST
   104-2628-E-415-002-MY2.
CR [Anonymous], P ACM S THEOR COMP S
   [Anonymous], 2011, ICML
   [Anonymous], 1998, The art of computer programming: Sorting and searching
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P PAC RIM C MULT PCM
   [Anonymous], P IEEE INT C COMP VI
   Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150
   Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124
   Cai D., 2010, P ACM C KNOWL DISC D
   CHERNOFF H, 1952, ANN MATH STAT, V23, P493, DOI 10.1214/aoms/1177729330
   Chiu C. Y., 2015, P WIR OPT COMM C WOC
   Chiu C. Y., 2016, P IEEE INT C AC SPEE
   Dong W., 2008, P ACM INT C INF RETR
   Efron B., 1993, INTRO BOOTSTRAP, DOI 10.1007/978-1-4899-4541-9
   Esmaeili MM, 2012, IEEE T PATTERN ANAL, V34, P2481, DOI 10.1109/TPAMI.2012.170
   Fang XZ, 2014, NEUROCOMPUTING, V128, P304, DOI 10.1016/j.neucom.2013.08.040
   Gordo A, 2014, IEEE T PATTERN ANAL, V36, P33, DOI 10.1109/TPAMI.2013.101
   He X., 2005, P ANN C NEUR INF PRO
   Irie G., 2014, P IEEE INT C COMP VI
   Jain M., 2011, P ACM INT C MULT ACM
   Jegou H., 2010, P IEEE INT C COMP VI
   Jegou H., 2011, P IEEE INT C AC SPEE
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Liu DR, 2013, ALLERGY ASTHMA CL IM, V9, DOI 10.1186/1710-1492-9-30
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Muja M., 2012, P INT C COMP ROB VIS
   Norouzi M., 2013, P IEEE INT C COMP VI
   Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pei YL, 2015, INT C INTEL HUM MACH, DOI 10.1109/IHMSC.2015.148
   Song JK, 2016, IEEE T MULTIMEDIA, V18, P484, DOI 10.1109/TMM.2016.2515990
   Wan J., 2013, P IEEE INT C IM PROC
   Wang J., 2015, J LATEX CLASS FILES, V13, P9
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wei HL, 2007, IEEE T PATTERN ANAL, V29, P162, DOI 10.1109/TPAMI.2007.250607
   Weiss Y., 2012, P EUR C COMP VIS ECC
   Zhang X, 2013, PROC INT CONF ENV SC
NR 38
TC 6
Z9 6
U1 3
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2017
VL 13
IS 1
AR 3
DI 10.1145/2990504
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM7WJ
UT WOS:000395522700003
DA 2024-07-18
ER

PT J
AU Shivani, S
   Agarwal, S
AF Shivani, Shivendra
   Agarwal, Suneeta
TI Progressive Visual Cryptography with Unexpanded Meaningful Shares
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Verification; Algorithms; Progressive visual cryptography;
   secret sharing; visual cryptography; meaningful shares; unexpanded
   shares; basis matrix
AB The traditional k-out-of-n Visual Cryptography (VC) scheme is the conception of "all or nothing" for n participants to share a secret image. The original secret image can be visually revealed only when a subset of k or more shares are superimposed together, but if the number of stacked shares are less than k, nothing will be revealed. On the other hand, a Progressive Visual Cryptography (PVC) scheme differs from the traditional VC with respect to decoding. In PVC, clarity and contrast of the decoded secret image will be increased progressively with the number of stacked shares. Much of the existing state-of-the-art research on PVC has problems with pixel expansion and random pattern of the shares. In this article, a novel scheme of progressive visual cryptography with four or more number of unexpanded as well as meaningful shares has been proposed. For this, a novel and efficient Candidate Block Replacement preprocessing approach and a basis matrix creation algorithm have also been introduced. The proposed method also eliminates many unnecessary encryption constraints like a predefined codebook for encoding and decoding the secret image, restriction on the number of participants, and so on. From the experiments, it is observed that the reconstruction probability of black pixels in the decoded image corresponding to the black pixel in the secret image is always 1, whereas that of white pixels is 0.5 irrespective of the meaningful contents visible in the shares, thus ensuring the value of contrast to alwasys be 50%. Therefore, a reconstructed image can be easily identified by a human visual system without any computation.
C1 [Shivani, Shivendra] Natl Inst Technol, Dept Comp Sci & Engn, Allahabad, Uttar Pradesh, India.
   [Agarwal, Suneeta] Natl Inst Technol, Allahabad, Uttar Pradesh, India.
C3 National Institute of Technology (NIT System); Motilal Nehru National
   Institute of Technology; National Institute of Technology (NIT System);
   Motilal Nehru National Institute of Technology
RP Shivani, S (corresponding author), Natl Inst Technol, Dept Comp Sci & Engn, Allahabad, Uttar Pradesh, India.
EM shivendrashivani@gmail.com; suneeta@mnnit.ac.in
RI Shivani, Shivendra/AFN-2368-2022
OI Shivani, Shivendra/0000-0002-5931-6603
CR [Anonymous], 2008, Modern Digital Halftoning
   [Anonymous], 2013, 2013 26 IEEE CAN C E, DOI [DOI 10.1109/CCECE.2013.6567726, 10.1109/CCECE.2013.6567726]
   Ateniese G, 1996, INFORM COMPUT, V129, P86, DOI 10.1006/inco.1996.0076
   Ateniese G, 2001, THEOR COMPUT SCI, V250, P143, DOI 10.1016/S0304-3975(99)00127-9
   Chen SK, 2005, PATTERN RECOGN, V38, P2466, DOI 10.1016/j.patcog.2005.04.002
   Chen TH, 2009, PATTERN RECOGN, V42, P2203, DOI 10.1016/j.patcog.2008.11.015
   Chou C. L., 2002, TECHNICAL REPORT
   Fang W.-P., 2006, Pattern Recognition and Image Analysis, V16, P632, DOI 10.1134/S1054661806040080
   Fang WP, 2007, 2007 THIRD INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P519
   Fang WP, 2007, ELE COM ENG, P108
   Fang WP, 2008, PATTERN RECOGN, V41, P1410, DOI 10.1016/j.patcog.2007.09.004
   Fu MS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P975
   Hou YC, 2003, PATTERN RECOGN, V36, P1619, DOI 10.1016/S0031-3203(02)00258-3
   Hou YC, 2014, IEEE T CIRC SYST VID, V24, P733, DOI 10.1109/TCSVT.2013.2280097
   Hou YC, 2013, INFORM SCIENCES, V233, P290, DOI 10.1016/j.ins.2013.01.006
   Hou YC, 2011, IEEE T CIRC SYST VID, V21, P1760, DOI 10.1109/TCSVT.2011.2106291
   Jin Duo, 2005, J ELECT IMAG, V14
   Lin SJ, 2010, J VIS COMMUN IMAGE R, V21, P900, DOI 10.1016/j.jvcir.2010.08.006
   Lu HP, 2004, IEEE SIGNAL PROC LET, V11, P228, DOI 10.1109/LSP.2003.821748
   MacPherson L. A., 2002, TECHNICAL REPORT
   Nakajima M, 2002, WSCG'2002, VOLS I AND II, CONFERENCE PROCEEDINGS, P303
   Naor M, 1997, LECT NOTES COMPUT SC, V1294, P322
   Shamir A., 1995, VISUAL CRYPTOGRAPHY, V950, P112
   Shyu SH, 2007, PATTERN RECOGN, V40, P1014, DOI 10.1016/j.patcog.2006.02.025
   Thien CC, 2002, COMPUT GRAPH-UK, V26, P766
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZM, 2009, IEEE T INF FOREN SEC, V4, P383, DOI 10.1109/TIFS.2009.2024721
   Weir J, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2344436.2344438
   Yan Wei-Qi, 2004, CIRC SYST 2004 ISCAS, V5
   Young D. P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P317
   Zhou Z, 2006, IEEE T IMAGE PROCESS, V15, P2441, DOI 10.1109/TIP.2006.875249
NR 31
TC 13
Z9 13
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2016
VL 12
IS 4
AR 50
DI 10.1145/2935618
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EH
UT WOS:000382877500004
DA 2024-07-18
ER

PT J
AU Yang, XY
   Mei, T
   Xu, YQ
   Rui, Y
   Li, SP
AF Yang, Xuyong
   Mei, Tao
   Xu, Ying-Qing
   Rui, Yong
   Li, Shipeng
TI Automatic Generation of Visual-Textual Presentation Layout
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual-textual presentation layout; design templates; typography; color
   harmonization; rich media presentation
AB Visual-textual presentation layout (e.g., digital magazine cover, poster, Power Point slides, and any other rich media), which combines beautiful image and overlaid readable texts, can result in an eye candy touch to attract users' attention. The designing of visual-textual presentation layout is therefore becoming ubiquitous in both commercially printed publications and online digital magazines. However, handcrafting aesthetically compelling layouts still remains challenging for many small businesses and amateur users. This article presents a system to automatically generate visual-textual presentation layouts by investigating a set of aesthetic design principles, through which an average user can easily create visually appealing layouts. The system is attributed with a set of topic-dependent layout templates and a computational framework integrating high-level aesthetic principles (in a top-down manner) and low-level image features (in a bottom-up manner). The layout templates, designed with prior knowledge from domain experts, define spatial layouts, semantic colors, harmonic color models, and font emotion and size constraints. We formulate the typography as an energy optimization problem by minimizing the cost of text intrusion, the utility of visual space, and the mismatch of information importance in perception and semantics, constrained by the automatically selected template and further preserving color harmonization. We demonstrate that our designs achieve the best reading experience compared with the reimplementation of parts of existing state-of-the-art designs through a series of user studies.
C1 [Yang, Xuyong] Univ Sci & Technol China, Hefei 230026, Peoples R China.
   [Mei, Tao; Rui, Yong; Li, Shipeng] Microsoft Res Asia, Beijing, Peoples R China.
   [Xu, Ying-Qing] Tsinghua Univ, Beijing 100084, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia; Tsinghua University
RP Yang, XY (corresponding author), Univ Sci & Technol China, Hefei 230026, Peoples R China.; Mei, T; Rui, Y; Li, SP (corresponding author), Microsoft Res Asia, Beijing, Peoples R China.; Xu, YQ (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM yxy8023@live.cn; tmei@microsoft.com; yqxu@tsinghua.edu.cn;
   yongrui@microsoft.com; spli@microsoft.com
RI Mei, Tao/GQZ-0596-2022; Li, Shipeng/AAA-3374-2020
OI Mei, Tao/0000-0002-5990-7307; 
FU National Basic Research Program of China [2012CB725300]; NSFC
   [61373072]; National Natural Science Foundation of China [61371192]
FX This work is supported in part by the National Basic Research Program of
   China under Grant 2012CB725300, in part by NSFC under Grant 61373072 and
   in part by the National Natural Science Foundation of China under Grant
   61371192. The authors would like to thank Junjie Yu and other designers'
   help.
CR Ali Jahanian, 2012, IS T SPIE ELECT IMAG
   [Anonymous], 1991, Color Image Scale
   [Anonymous], 1954, ART VISUAL PERCEPTIO
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bauerly M, 2006, INT J HUM-COMPUT ST, V64, P670, DOI 10.1016/j.ijhcs.2006.01.002
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Havasi Catherine, 2010, P AAAI FALL S SERIES
   Huo Q, 2003, PROC INT CONF DOC, P364
   Jacobs C, 2003, ACM T GRAPHIC, V22, P838, DOI 10.1145/882262.882353
   Jahanian Ali, 2013, P 2013 INT C INTELLI, P95
   Kroner Alexander, 1999, P AAAI FALL S SERIES
   Kuhna M., 2012, Proceedings of the 20th ACM international conference on Multimedia, P379, DOI DOI 10.1145/2393347.2393403
   Liang L, 2008, LECT NOTES COMPUT SC, V5303, P72, DOI 10.1007/978-3-540-88688-4_6
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Lok S., 2001, Proceedings of Smart Graphics, P61
   Mei T, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071402
   Munsell A.H., 1950, MUNSELL BOOK COLOR, DOI DOI 10.1007/s10336-015-1188-3
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Tokumaru M, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 & 2, P378, DOI 10.1109/FUZZ.2002.1005020
   Vaquero Daniel, 2010, P SOC PHOTO-OPT INS, V7798
   Yin W., 2013, P 21 ACM INT C MULT, P927, DOI [DOI 10.1145/2502081.2502116, 10.1145/2502081]
   Zhou Michelle X., 1999, P 2000 AAAI SPRING S, P16
NR 23
TC 42
Z9 42
U1 2
U2 37
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2016
VL 12
IS 2
AR 33
DI 10.1145/2818709
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PK
UT WOS:000373906200007
DA 2024-07-18
ER

PT J
AU Wang, H
   Chan, MC
   Ooi, WT
AF Wang, Hui
   Chan, Mun Choon
   Ooi, Wei Tsang
TI Wireless Multicast for Zoomable Video Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Performance; Measurement; Wireless
   multicast; zoomable video; optimal multicast algorithm; dynamic
   programming
AB Zoomable video streaming refers to a new class of interactive video applications, where users can zoom into a video stream to view a selected region of interest in higher resolutions and pan around to move the region of interest. The zoom and pan effects are typically achieved by breaking the source video into a grid of independently decodable tiles. Streaming the tiles to a set of heterogeneous users using broadcast is challenging, as users have different link rates and different regions of interest at different resolution levels. In this article, we consider the following problem: Given the subset of tiles that each user requested, the link rate of each user, and the available time slots, at which resolution should each tile be sent, to maximize the overall video quality received by all users. We design an efficient algorithm to solve this problem and evaluate the solution on a testbed using 10 mobile devices. Our method is able to achieve up to 12dB improvements over other heuristic methods.
C1 [Wang, Hui; Chan, Mun Choon; Ooi, Wei Tsang] Natl Univ Singapore, Comp 1, Sch Comp, Singapore 117417, Singapore.
C3 National University of Singapore
RP Wang, H (corresponding author), Natl Univ Singapore, Comp 1, Sch Comp, 13 Comp Dr, Singapore 117417, Singapore.
EM wanghui@comp.nus.edu.sg; chanmc@comp.nus.edu.sg; ooiwt@comp.nus.edu.sg
RI Ooi, Wei Tsang/AAE-7810-2019; Ooi, Wei Tsang/HLW-5142-2023
OI Ooi, Wei Tsang/0000-0001-8994-1736; Ooi, Wei Tsang/0000-0001-8994-1736
FU Singapore National Research Foundation; Interactive Digital Media R&D
   Program Office of Media Development Authority [WBS:R-252-300-001-490];
   Singapore National Research Foundation under its International Research
   Centre @ Singapore Funding Initiative
FX This research was conducted under the NExT Search Center, supported by
   the Singapore National Research Foundation and the Interactive Digital
   Media R&D Program Office of Media Development Authority under research
   grant WBS:R-252-300-001-490. This research was supported by the
   Singapore National Research Foundation under its International Research
   Centre @ Singapore Funding Initiative and administered by the IDM
   Programme Office.
CR [Anonymous], P 15 EUR SIGN PROC C
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], P PACK VID 2007
   [Anonymous], P 20 ACM INT C MULT
   [Anonymous], 1997, PSYCHOPHYSICS FUNDAM
   [Anonymous], P 27 IEEE INT C COMP
   [Anonymous], 2012, 802112007 IEEE
   [Anonymous], P 17 IEEE INT C NETW
   Bicket J.C., 2005, THESIS MIT
   Chandra R, 2009, I C NETWORK PROTOCOL, P161, DOI 10.1109/ICNP.2009.5339686
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Feng WC, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000491
   Khiem N. Quang Minh, 2011, P 2 ANN ACM C MULT S, P211, DOI DOI 10.1145/1943552.1943581
   Kohler E, 2000, ACM T COMPUT SYST, V18, P263, DOI 10.1145/354871.354874
   Liu JC, 2003, IEEE MULTIMEDIA, V10, P22, DOI 10.1109/MMUL.2003.1167919
   Mavlankar A., 2010, 2010 18th International Packet Video Workshop (PV 2010), P64, DOI 10.1109/PV.2010.5706821
   Minh Khiem Ngo Quang., 2010, Proceedings of the first annual ACM SIGMM conference on Multimedia systems, P259, DOI DOI 10.1145/1730836.1730868
   Pefkianakis I, 2013, IEEE T MOBILE COMPUT, V12, P502, DOI 10.1109/TMC.2012.18
   Peilong Li, 2010, 2010 18th IEEE International Conference on Network Protocols (ICNP 2010), P295, DOI 10.1109/ICNP.2010.5762778
   Piamrat Kandaraj, 2009, P GLOBAL TELECOMMUNI, P1
   Sen Sayandeep., 2010, Proceedings of the 7th USENIX conference on Networked systems design and implementation, NSDI'10, P13
   van Brandenburg R., 2011, 2011 15th International Conference on Intelligence in Next Generation Networks (ICIN): "From Bits to Data, from Pipes to Clouds", P151, DOI 10.1109/ICIN.2011.6081064
   Wang H., 2014, PROC NETW OPERATING, P25
   Wong Starsky H. Y., 2013, 2013 IEEE International Conference on Sensing, Communications and Networking (SECON), P344, DOI 10.1109/SAHCN.2013.6645004
   Wong SHY, 2006, MOBICOM 2006, P146
   Yoon J, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P209
NR 26
TC 6
Z9 6
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2015
VL 12
IS 1
AR 5
DI 10.1145/2801123
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CP9LG
UT WOS:000360215200005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, HY
   Wang, XY
   Niu, PP
   Wang, AL
AF Yang, Hong-Ying
   Wang, Xiang-Yang
   Niu, Pan-Pan
   Wang, Ai-Long
TI Robust Color Image Watermarking Using Geometric Invariant Quaternion
   Polar Harmonic Transform
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Management; Security; Color image watermarking; quaternion
   polar harmonic transform; geometric invariance; robustness;
   imperceptibility
ID SPREAD-SPECTRUM WATERMARKING; ZERNIKE MOMENTS; SCHEME
AB It is a challenging work to design a robust color image watermarking scheme against geometric distortions. Moments and moment invariants have become a powerful tool in robust image watermarking owing to their image description capability and geometric invariance property. However, the existing moment-based watermarking schemes were mainly designed for gray images but not for color images, and detection quality and robustness will be lowered when watermark is directly embedded into the luminance component or three color channels of color images. Furthermore, the imperceptibility of the embedded watermark is not well guaranteed. Based on algebra of quaternions and polar harmonic transform (PHT), we introduced the quaternion polar harmonic transform (QPHT) for invariant color image watermarking in this article, which can be seen as the generalization of PHT for gray-level images. It is shown that the QPHT can be obtained from the PHT of each color channel. We derived and analyzed the rotation, scaling, and translation (RST) invariant property of QPHT. We also discussed the problem of color image watermarking using QPHT. Experimental results are provided to illustrate the efficiency of the proposed color image watermarking against geometric distortions and common image processing operations (including color attacks).
C1 [Yang, Hong-Ying; Wang, Xiang-Yang; Niu, Pan-Pan; Wang, Ai-Long] Liaoning Normal Univ, Sch Comp & Informat Technol, Dalian 116029, Peoples R China.
C3 Liaoning Normal University
RP Yang, HY (corresponding author), Liaoning Normal Univ, Sch Comp & Informat Technol, Dalian 116029, Peoples R China.
EM yhy_65@126.com; wxy37@126.com
RI Yang, Jing/JFK-4046-2023
OI Yang, Jing/0009-0004-8274-9863
FU National Natural Science Foundation of China [61472171, 61272416];
   Jiangsu Key Laboratory of Image and Video Understanding for Social
   Safety (Nanjing University of Science and Technology) [30920130122006];
   Open Foundation of Zhejiang Key Laboratory for Signal Processing
   [ZJKL_4_SP-OP2013-01]; Open Foundation of Provincial Key Laboratory for
   Computer Information Processing Technology (Soochow University)
   [KJS1325]; State Key Lab of CADCG [A1425]; Zhejiang University; Liaoning
   Research Project for Institutions of Higher Education of China
   [L2013407]
FX This work was supported by the National Natural Science Foundation of
   China under Grant No. 61472171 & 61272416, the Open Project Program of
   Jiangsu Key Laboratory of Image and Video Understanding for Social
   Safety (Nanjing University of Science and Technology) under Grant No.
   30920130122006, the Open Foundation of Zhejiang Key Laboratory for
   Signal Processing under Grant No. ZJKL_4_SP-OP2013-01, the Open
   Foundation of Provincial Key Laboratory for Computer Information
   Processing Technology (Soochow University) under Grant No. KJS1325, the
   Open Project Program of the State Key Lab of CAD&CG (Grant No. A1425),
   Zhejiang University, and Liaoning Research Project for Institutions of
   Higher Education of China under Grant No. L2013407.
CR Alghoniemy M, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P73, DOI 10.1109/ICIP.2000.899229
   Ali M, 2014, SIGNAL PROCESS, V94, P545, DOI 10.1016/j.sigpro.2013.07.024
   Amayeh G, 2005, LECT NOTES COMPUT SC, V3804, P462
   Barni M, 2005, IEEE SIGNAL PROC LET, V12, P158, DOI 10.1109/LSP.2004.840872
   Cheddad A, 2010, SIGNAL PROCESS, V90, P727, DOI 10.1016/j.sigpro.2009.08.010
   Chen BJ, 2012, SIGNAL PROCESS, V92, P308, DOI 10.1016/j.sigpro.2011.07.018
   Chou CH, 2010, IEEE T IMAGE PROCESS, V19, P2966, DOI 10.1109/TIP.2010.2052261
   Coatrieux G, 2013, IEEE T INF FOREN SEC, V8, P111, DOI 10.1109/TIFS.2012.2224108
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Elshoura SM, 2013, J VIS COMMUN IMAGE R, V24, P567, DOI 10.1016/j.jvcir.2013.03.021
   Gao XB, 2010, IEEE T SYST MAN CY C, V40, P278, DOI 10.1109/TSMCC.2009.2037512
   Guo LQ, 2011, PATTERN RECOGN, V44, P187, DOI 10.1016/j.patcog.2010.08.017
   Kantor I.L., 1989, Hypercomplex Number: An Elementary Introduction to Algebras
   Kim HS, 2003, IEEE T CIRC SYST VID, V13, P766, DOI 10.1109/TCSVT.2003.815955
   KUMAR A, 1994, INT J DEV NEUROSCI, V12, P31, DOI 10.1016/0736-5748(94)90093-0
   Lai CC, 2010, IEEE T INSTRUM MEAS, V59, P3060, DOI 10.1109/TIM.2010.2066770
   Li LD, 2012, INFORM SCIENCES, V199, P1, DOI 10.1016/j.ins.2012.02.062
   [李旭东 LI Xu-dong], 2010, [光电工程, Opto-Electronic Engineering], V37, P96
   Li YN, 2013, IEEE SIGNAL PROC LET, V20, P803, DOI 10.1109/LSP.2013.2267775
   Lin PY, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000489
   Liu HJ, 2012, PROC SPIE, V8303, DOI 10.1117/12.907861
   Nikolaidis A., 2012, 18 IEEE INT C IM PRO, P2729
   Niu PP, 2011, EXPERT SYST APPL, V38, P2081, DOI 10.1016/j.eswa.2010.07.147
   Savelonas MA, 2010, SIGNAL PROCESS, V90, P2521, DOI 10.1016/j.sigpro.2010.02.021
   Seo JS, 2006, IEEE T SIGNAL PROCES, V54, P1537, DOI 10.1109/TSP.2006.870581
   Simitopoulos D, 2003, IEEE T CIRC SYST VID, V13, P732, DOI 10.1109/TCSVT.2003.815947
   Singh C, 2013, DIGIT SIGNAL PROCESS, V23, P1470, DOI 10.1016/j.dsp.2013.05.006
   Singhal N, 2009, J VIS COMMUN IMAGE R, V20, P408, DOI 10.1016/j.jvcir.2009.04.002
   Su QT, 2014, SIGNAL PROCESS, V94, P219, DOI 10.1016/j.sigpro.2013.06.025
   Suk T, 2009, LECT NOTES COMPUT SC, V5702, P334, DOI 10.1007/978-3-642-03767-2_41
   Tsai JS, 2011, IEEE T IMAGE PROCESS, V20, P735, DOI 10.1109/TIP.2010.2073475
   Tsougenis ED, 2012, J SYST SOFTWARE, V85, P1864, DOI 10.1016/j.jss.2012.02.045
   Tsui TK, 2008, IEEE T INF FOREN SEC, V3, P16, DOI 10.1109/TIFS.2007.916275
   Valizadeh A., 2011, IEEE Transactions on Information Forensics and Security, V6, P267, DOI 10.1109/TIFS.2010.2103061
   Valizadeh A, 2012, IEEE T INF FOREN SEC, V7, P1127, DOI 10.1109/TIFS.2012.2199312
   Venkataramana A, 2007, ICCTA 2007: INTERNATIONAL CONFERENCE ON COMPUTING: THEORY AND APPLICATIONS, PROCEEDINGS, P676
   Wang XY, 2010, MULTIDIM SYST SIGN P, V21, P179, DOI 10.1007/s11045-009-0096-1
   Wang XY, 2007, IEEE T INF FOREN SEC, V2, P655, DOI 10.1109/TIFS.2007.908233
   Xin YQ, 2007, PATTERN RECOGN, V40, P3740, DOI 10.1016/j.patcog.2007.05.004
   Yap PT, 2010, IEEE T PATTERN ANAL, V32, P1259, DOI 10.1109/TPAMI.2009.119
   Yuan XC, 2013, SIGNAL PROCESS, V93, P2087, DOI 10.1016/j.sigpro.2013.01.024
   Zhang H, 2011, IEEE T IMAGE PROCESS, V20, P2189, DOI 10.1109/TIP.2011.2118216
   Zhang L, 2007, IET INFORM SECUR, V1, P97, DOI 10.1049/iet-ifs:20060105
   Zhao Y, 2012, SCI CHINA INFORM SCI, V55, P650, DOI 10.1007/s11432-011-4470-x
   Zheng D, 2009, IEEE T IMAGE PROCESS, V18, P1055, DOI 10.1109/TIP.2009.2014807
   Zheng D, 2007, ACM COMPUT SURV, V39, DOI 10.1145/1242471.1242473
   Zhong JD, 2006, IEEE T CIRC SYST VID, V16, P1491, DOI 10.1109/TCSVT.2006.885716
   Zhu HQ, 2010, DIGIT SIGNAL PROCESS, V20, P1612, DOI 10.1016/j.dsp.2010.01.010
NR 48
TC 44
Z9 45
U1 4
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2015
VL 11
IS 3
AR 40
DI 10.1145/2700299
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CB8BF
UT WOS:000349852500008
DA 2024-07-18
ER

PT J
AU Yin, YF
   Shen, ZJ
   Zhang, LM
   Zimmermann, R
AF Yin, Yifang
   Shen, Zhijie
   Zhang, Luming
   Zimmermann, Roger
TI Spatial-Temporal Tag Mining for Automatic Geospatial Video Annotation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Human Factors; Video tags; location sensors; mobile
   videos; geospatial; social media; clustering
AB Videos are increasingly geotagged and used in practical and powerful GIS applications. However, video search and management operations are typically supported by manual textual annotations, which are subjective and laborious. Therefore, research has been conducted to automate or semi-automate this process. Since a diverse vocabulary for video annotations is of paramount importance towards good search results, this article proposes to leverage crowdsourced data from social multimedia applications that host tags of diverse semantics to build a spatio-temporal tag repository, consequently acting as input to our auto-annotation approach. In particular, to build the tag store, we retrieve the necessary data from several social multimedia applications, mine both the spatial and temporal features of the tags, and then refine and index them accordingly. To better integrate the tag repository, we extend our previous approach by leveraging the temporal characteristics of videos as well. Moreover, we set up additional ranking criteria on the basis of tag similarity, popularity and location bias. Experimental results demonstrate that, by making use of such a tag repository, the generated tags have a wide range of semantics, and the resulting rankings are more consistent with human perception.
C1 [Yin, Yifang; Zhang, Luming; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Shen, Zhijie] Hortonworks Inc, Palo Alto, CA USA.
C3 National University of Singapore
RP Zhang, LM (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
EM yifang@comp.nus.edu.sg; zshen@hortonworks.com; zglumg@comp.nus.edu.sg;
   rogerz@comp.nus.edu.sg
RI Zimmermann, Roger/D-7944-2015; Lei, Ming/JAD-1050-2023; zhang,
   lu/GRO-2969-2022
OI Zimmermann, Roger/0000-0002-7410-2590; yin, yifang/0000-0002-6525-6133
FU Singapore National Research Foundation under its International Research
   Centre @ Singapore Funding Initiative
FX This research has been supported by the Singapore National Research
   Foundation under its International Research Centre @ Singapore Funding
   Initiative and administered by the IDM Programme Office through the
   Centre of Social Media Innovations for Communities (COSMIC).
CR Abdollahian Golnaz, 2009, PROCEEDINGS OF THE I
   Ahern Shane, 2007, PROCEEDINGS OF THE 7
   Ames Morgan, 2007, PROCEEDINGS OF THE S
   [Anonymous], 2000, AN INTRODUCTION TO S
   [Anonymous], P ACM MULT SCOTTSD A
   [Anonymous], P 1 ACM INT C MULT R
   [Anonymous], 2011, Proceedings of the 20th International Conference on World Wide Web-WWW'11, DOI [DOI 10.1145/1963405.1963443, 10.1145/1963405.1963443]
   Ay Sakire Arslan, 2008, PROCEEDINGS OF THE A
   Ballan L., 2010, Proceedings 79 of Second ACM SIGMM Workshop on Social Media, P3
   Brunsdon C., 2002, Computers, Environment and Urban Systems, V26, P501, DOI 10.1016/S0198-9715(01)00009-6
   Chang Chih-Chung, 2011, ACM TRANS INTELL SYS
   Dempster A. P., 1977, J ROYAL STATISTICAL
   Ester Martin, 1996, PROCEEDINGS OF THE 2
   Feng S. L., 2004, PROCEEDINGS OF THE I
   Gao Yue, 2010, PROCEEDINGS OF THE A
   Hall M., 2009, ACM SIGKDD Explor. Newsl, V11, P18, DOI DOI 10.1145/1656274.1656278
   Hauff C, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P1037
   Hershey JR, 2007, INT CONF ACOUST SPEE, P317, DOI 10.1109/icassp.2007.366913
   Hong R, 2011, ACM T MULTIM COMPUT, V7
   Intagorn S., 2011, Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, P425
   Jain Ramesh, 2010, PROCEEDINGS OF THE A
   Jeon J., 2003, P 26 ANN INT ACM SIG
   Larson Martha, 2011, PROCEEDINGS OF THE M
   Liu JJ, 2007, PROCEEDINGS OF THE 2007 CHINESE CONTROL AND DECISION CONFERENCE, P605, DOI 10.1145/1291233.1291380
   Lu Xin, 2010, PROCEEDINGS OF THE A
   Moxley Emily, 2008, PROCEEDINGS OF THE 1
   Perez DanielG., 2003, PROCEEDINGS OF THE A
   Qi Guo J., 2007, PROCEEDINGS OF THE A
   Rahman AMM, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1865106.1865112
   Rattenbury Tye, 2007, PROCEEDINGS OF THE A
   Rong Yan, 2008, PROCEEDINGS OF THE I
   Shen Zhijie, 2011, PROCEEDINGS OF THE A
   Siersdorfer Stefan, 2009, PROCEEDINGS OF THE A
   Sigurbjornsson Borkur, 2008, PROCEEDINGS OF THE A
   Sizov S., 2010, P 3 ACM INT C WEB SE, P281, DOI DOI 10.1145/1718487.1718522
   Snoek Cees G.M., 2011, THE MEDIAMILL TRECVI
   Suchanek Fabian M., 2008, PROCEEDINGS OF THE 1
   Thomee B, 2013, P 22 INT C WORLD WID, P1285
   Tian XM, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240139
   Toyama Kentaro, 2003, PROCEEDINGS OF THE A
   Wu Lei, 2009, PROCEEDINGS OF THE A
   Yanai Keiji, 2009, PROCEEDINGS OF THE A
   Yin Z., 2011, SIAM, P980
   Zhang Haipeng, 2012, PROCEEDINGS OF THE A
NR 44
TC 5
Z9 5
U1 2
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2014
VL 11
IS 2
AR 29
DI 10.1145/2658981
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6DM
UT WOS:000348308800006
DA 2024-07-18
ER

PT J
AU Sundaram, H
AF Sundaram, Hari
TI Experiential Media Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Multimedia; experiential systems; foundations
ID MULTISENSORY INTEGRATION
AB This article presents a personalized narrative on the early discussions within the Multimedia community and the subsequent research on experiential media systems. I discuss two different research initiatives-design of real-time, immersive multimedia feedback environments for stroke rehabilitation; exploratory environments for events that exploited the user's ability to make connections. I discuss the issue of foundations: the question of multisensory integration and superadditivity; the need for identification of "first-class" Multimedia problems; expanding the scope of Multimedia research.
C1 [Sundaram, Hari] Arizona State Univ, Tempe, AZ 85287 USA.
C3 Arizona State University; Arizona State University-Tempe
RP Sundaram, H (corresponding author), Sch Arts Media & Engn, Room 398,699 S Mill Ave, Tempe, AZ USA.
EM hari.sundaram@asu.edu
OI Sundaram, Hari/0000-0003-3315-6055
CR Chen Y., 2006, P 14 ANN ACM INT C M, P763, DOI [10.1145/1180639.1180804, DOI 10.1145/1180639.1180804]
   Davis Marc., 2003, Proceedings of the 2003 ACM SIGMM workshop on Experiential Telepresence, P45, DOI DOI 10.1145/982484.982491
   Jain R, 2003, COMMUN ACM, V46, P48, DOI 10.1145/792704.792729
   Jordan K., 2002, Multimedia from Wagner to virtual reality
   Rowe LA, 2005, ACM T MULTIM COMPUT, V1, P3, DOI 10.1145/1047936.1047938
   Stanford TR, 2007, NEUROREPORT, V18, P787, DOI 10.1097/WNR.0b013e3280c1e315
   Stein BE, 2009, EXP BRAIN RES, V198, P113, DOI 10.1007/s00221-009-1880-8
   Stein Barry E., 1993, The Merging of the Senses. The Merging of the Senses. Cognitive Neuroscience
   SUNDARAM H, 2002, THESIS COLUMBIA U
   SUNDARAM H., 2003, P ACM SIGMM WORKSH E
   Westermann U, 2007, IEEE MULTIMEDIA, V14, P19, DOI 10.1109/MMUL.2007.23
NR 11
TC 5
Z9 7
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 49
DI 10.1145/2502432
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700019
DA 2024-07-18
ER

PT J
AU Hendrikx, M
   Meijer, S
   van der Velden, J
   Iosup, A
AF Hendrikx, Mark
   Meijer, Sebastiaan
   van der Velden, Joeri
   Iosup, Alexandru
TI Procedural Content Generation for Games: A Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Theory; Design; Standardization; Algorithms; Game content generation;
   procedural; survey
ID ONLINE GAMES
AB Hundreds of millions of people play computer games every day. For them, game content-from 3D objects to abstract puzzles-plays a major entertainment role. Manual labor has so far ensured that the quality and quantity of game content matched the demands of the playing community, but is facing new scalability challenges due to the exponential growth over the last decade of both the gamer population and the production costs. Procedural Content Generation for Games (PCG-G) may address these challenges by automating, or aiding in, game content generation. PCG-G is difficult, since the generator has to create the content, satisfy constraints imposed by the artist, and return interesting instances for gamers. Despite a large body of research focusing on PCG-G, particularly over the past decade, ours is the first comprehensive survey of the field of PCG-G. We first introduce a comprehensive, six-layered taxonomy of game content: bits, space, systems, scenarios, design, and derived. Second, we survey the methods used across the whole field of PCG-G from a large research body. Third, we map PCG-G methods to game content layers; it turns out that many of the methods used to generate game content from one layer can be used to generate content from another. We also survey the use of methods in practice, that is, in commercial or prototype games. Fourth and last, we discuss several directions for future research in PCG-G, which we believe deserve close attention in the near future.
C1 [Hendrikx, Mark; Meijer, Sebastiaan; van der Velden, Joeri; Iosup, Alexandru] Delft Univ Technol, NL-2628 CD Delft, Netherlands.
C3 Delft University of Technology
RP Iosup, A (corresponding author), Delft Univ Technol, Mekelweg 4, NL-2628 CD Delft, Netherlands.
EM A.Iosup@tudelf.nl
RI Meijer, Sebastiaan/E-8283-2011; Iosup, Alexandru/G-4069-2012
OI Meijer, Sebastiaan/0000-0003-1126-3781; Iosup,
   Alexandru/0000-0001-8030-9398
FU STW/NWO Veni grant @larGe [11881]
FX This work was supported by the STW/NWO Veni grant @larGe (11881).
CR ADAMS D., 2002, THESIS U SHEFFIELD U
   Alexander C., 1977, PATTERN LANGUAGE TOW
   Alt H, 2009, THEOR COMPUT SYST, V44, P160, DOI 10.1007/s00224-008-9104-3
   [Anonymous], 2003, NAT COMP SER
   [Anonymous], 1998, Cellular Automata Modeling of Physical Systems
   [Anonymous], Proceedings of the 4th International Conference on Foundations of Digital Games. FDG'09. 2009, DOI DOI 10.1145/1536513.1536532
   Ashlock Daniel, 2010, 2010 IEEE Information Theory Workshop (ITW 2010), P289, DOI 10.1109/ITW.2010.5593341
   AURENHAMMER F, 1991, COMPUT SURV, V23, P345, DOI 10.1145/116873.116880
   Babcock J., 2005, Cellular automata method for generating random cave-like levels
   BARRON T., 1999, MULTIPLAYER GAME PRO
   Bartle R., 2003, Designing Virtual Worlds
   Brathwaite B., 2008, Challenges for Game Designers, V1st
   Chan C., 2009, PROCEEDING ACM CHI 2, P3589
   Chen GN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360702
   CHEN S., 2010, P INT NARR TECHN 3 W, P17
   CHEONG Y., 2008, P INT C ART INT INT
   CIVILIZATION FANATICS, 2005, MAP SCRIPT FULL RES
   CLYDE D., 2004, ADDING REALISTIC RIV
   COLTON S., 2002, P S AI CREAT ARTS SC
   COMPLEXITY-GAMING.COM, 2008, INCR WOW STATS
   COMPTON K., 2006, P INT C ART INT INT
   Davidsson Paul., 2001, Multiagent- based simulation, P141
   de Berg M., 2008, COMPUTATIONAL GEOMET, V3rd, DOI [10.1007/978-3-540-77974-2_1, DOI 10.1007/978-3-540-77974]
   de la Re A, 2009, LECT NOTES COMPUT SC, V5545, P801
   Deussen O., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P275, DOI 10.1145/280814.280898
   Doran J, 2010, IEEE T COMP INTEL AI, V2, P111, DOI 10.1109/TCIAIG.2010.2049020
   Dormans J., 2010, Proceedings of the 2010 Workshop on Procedural Content Generation in Games-PCGames, P1, DOI DOI 10.1145/1814256.1814257
   DORSEY J., 2009, ACM SIGGRAPH COURSES, P1
   DOULL A., 2007, WILDERNESS GENERATIO
   DOULL A., 2007, UNANGBAND DUNGEON 1
   Ebert David S, 2003, Texturing Modeling: A Procedural Approach
   EDELSTEIN-KESHET L., 2005, CLASSICS APPL MATH S, V46
   Edwards B., 1989, Drawing on the Right Side of the Brain, DOI DOI 10.1145/286498.286584
   Edwards M, 2011, COMMUN ACM, V54, P58, DOI 10.1145/1965724.1965742
   ELAS T., 2010, COST CREATING MAINTA
   ESA, 2010, ESA ANN REP SER 2003
   Fairclough CR, 2003, GAME-ON 2003: 4TH INTERNATIONAL CONFERENCE ON INTELLIGENT GAMES AND SIMULATION, P41
   FARNELL A., 2007, P AUD MOSTL C
   Fields T., 2010, DISTRIBUTED GAME DEV
   Flake G.W., 1999, COMPUTATIONAL BEAUTY
   Frade M, 2010, LECT NOTES COMPUT SC, V6024, P90, DOI 10.1007/978-3-642-12239-2_10
   FREECIV COMMUNITY, 2005, MAP COMP COMM DISC
   GARFIELD R., 2000, METAGAMES HORSEMEN A
   Gervas Pablo, 2004, INT C INN TECHN APPL, P33
   GOBEL S., 2004, P 2 INT C TECHN INT
   Goldberg D. E., 1989, GENETIC ALGORITHMS S
   GONZALEZ D., 2012, NPPANGBAND RELEASE N
   Greuter S., 2003, P 1 INT C COMPUTER G, P87, DOI [DOI 10.1145/604487.604490, DOI 10.1145/604471.604490]
   Groenewegen Saskia A., 2009, Proc. Eurographics 2009 - Short Pap, P45
   Hammes J., 2001, Digital Earth Moving. First International Symposium, DEM 2001. Proceedings (Lecture Notes in Computer Science Vol.2181), P98
   Hartsook K, 2011, IEEE CONF COMPU INTE, P297, DOI 10.1109/CIG.2011.6032020
   Hastings EJ, 2009, IEEE T COMP INTEL AI, V1, P245, DOI 10.1109/TCIAIG.2009.2038365
   Haykin S., 1994, NEURAL NETWORKS COMP
   Hidalgo JL, 2008, LECT NOTES COMPUT SC, V5102, P106, DOI 10.1007/978-3-540-69387-1_12
   HILLBERRY J. D., 1999, DRAWING REALISTIC TE
   IGN AUSTRALIA, 2008, 10 COMM DIABL
   Iosup A., 2010, Performance Analysis of Cloud Computing Services for Many-Tasks Scientific Computing, P1
   Iosup A, 2011, CONCURR COMP-PRACT E, V23, P158, DOI 10.1002/cpe.1638
   Iosup A, 2009, LECT NOTES COMPUT SC, V5704, P390, DOI 10.1007/978-3-642-03869-3_39
   Irish D., 2005, The Game Producer's Handbook
   JENNINGS-TEATS M., 2010, P 2010 WORKSH PROC C, P11
   Johnson Lawrence, 2010, P 2010 WORKSH PROC C, DOI DOI 10.1145/1814256.1814266
   JOHNSON S., 2006, LONG ZOOM
   Kelly G., 2006, I TECHNOLOGY BLANCHA, V14, P87
   Kelly G., 2007, P GDTW 2007 5 ANN C, P8
   KIM J. H., 2008, P SCGCHI C HUM FACT
   KRUEGER B. D., 2005, P GAM DEV C
   Kushner D., 2003, Masters of Doom: how two guys created an empire and transformed pop culture
   LARIVE M., 2006, P INT C COMP GRAPH I, P437
   LECHNER T., 2003, P 1 MIDW GRAPH C
   LECKY-THOMPSON G. W., 2001, ADV COMPUTER GRAPHIC
   Lefebvre S., 2003, P 2003 S INTERACTIVE, P203
   LI B., 2010, P 6 AI INT DIG ENT C
   Lintermann B, 1999, IEEE COMPUT GRAPH, V19, P56, DOI 10.1109/38.736469
   Loguidice Bill., 2009, Vintage Games: An Insider Look at the History of Grand Theft Auto, Super Mario, and the Most Influential Games of All Time
   Loomis Andrew., 1951, Successful Drawing
   Manocha D., 2009, SIGGRAPH'09: ACM SIGGRAPH 2009 Courses, P1
   Marks Joe., 2007, AIIDE, P25, DOI [10.1609/AIIDE.V3I1.18777, DOI 10.1609/AIIDE.V3I1.18777]
   Martin Jess, 2006, I3D '06: Proceedings of the 2006 Symposium on Interactive 3D Graphics and Games, P1
   MATEAS M., 2005, P C DIG ARTS CULT DI, P1
   Mondet S, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1556134.1556138
   Moss W, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805965
   MULLER P., 2006, ACM SIGGRAPH 06, P623
   Murray Janet H., 1997, Hamlet on the Holodeck: The Future of Narrative in Cyberspace
   Nareyek A, 2007, IEEE INTELL SYST, V22, P9, DOI 10.1109/MIS.2007.10
   Nelson MJ, 2007, LECT NOTES COMPUT SC, V4733, P626
   Nitsche M., 2009, VIDEO GAME SPACES IM
   Norman D.A., 2002, DESIGN EVERYDAY THIN
   Orwant J, 2000, IBM SYST J, V39, P782, DOI 10.1147/sj.393.0782
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   PATEL A., 2010, Polygonal map generation for games
   PELL B., 1993, UCAMCLTR277
   Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247
   PERLIN K., 1990, MAKING NOISE
   Pi XX, 2006, LECT NOTES COMPUT SC, V3942, P913
   Pizzi D, 2010, IEEE T COMP INTEL AI, V2, P149, DOI 10.1109/TCIAIG.2010.2070066
   PROCEDURAL CONTENT GENERATION WIKI, 2009, EV ONL
   Propp V., 1968, Morphology of the Folktale, V2 nd
   Prusinkiewicz P., 1993, Proceedings Graphics Interface '93, P174
   Pullen W.D., 2011, THINK LABYRINTH MAZE
   REYNOLDS C., 2010, P SIGGRAPH
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   REYNOLDS D., 2010, COST MAKE QUALITY MM
   RIEDL M., 2009, P INT C ART INT INT
   Riedl M, 2011, ARTIFICIAL INTELLIGENCE FOR COMPUTER GAMES, P125, DOI 10.1007/978-1-4419-8188-2_6
   Roden Timothy., 2005, Proceedings of the 2005 ACM SIGCHI International Conference on Advances in Computer Entertainment, P434
   Rossignol J., 2008, THIS GAMING LIFE TRA
   Russell S., 1995, Prentice Hall series in artificial intelligence, V25, P27
   SEXTON C., 2010, P WORKSH PROC CONT G, P5
   Shaker N., 2010, Proceedings of AIIDE, P63
   SIMS K, 1991, COMP GRAPH, V25, P319, DOI 10.1145/127719.122752
   SKORUPSKI J., 2010, P INT C ART INT INT
   SMELIK R., 2010, P 2010 WORKSH PROC C, P2
   Smelik R.M., 2009, P CASA WORKSHOP 3D A, V2009, P25
   SMITH A. M., 2010, P IEEE S COMP INT GA, P111
   SMITH ADAM M., 2009, CFML CONTEXT FREE MU
   Smith G, 2010, P 2010 WORKSH PROC C
   Smith Gillian, 2010, P 5 INT C FDN DIGITA, P209
   Smith Gillian., 2009, P 4 INT C FDN DIGITA, P175
   Sorenson N, 2010, LECT NOTES COMPUT SC, V6024, P131, DOI 10.1007/978-3-642-12239-2_14
   SPEEDTREE.COM, 2011, SPEEDTR LIST GAM
   Strogatz S. H., 1994, NONLINEAR DYNAMICS C
   Sun J., 2002, P ACM S VIRTUAL REAL, P33, DOI [DOI 10.1145/585740.585747, 10 . 1145 / 585740 . 585747]
   Takatsuki Y., 2007, COST HEADACHE GAME D
   TAYLOR J., 2010, LARC201001 U N TEX
   TENTONHAMMER.COM, 2009, INC INC EVE ONL Q A
   Togelius J, 2010, LECT NOTES COMPUT SC, V6024, P141, DOI 10.1007/978-3-642-12239-2_15
   Togelius J, 2008, 2008 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND GAMES, P111, DOI 10.1109/CIG.2008.5035629
   van Basten BJH, 2010, COMPUT ANIMAT VIRT W, V21, P433, DOI 10.1002/cav.342
   VAN VERTH J. M., 2008, ESSENTIAL MATH GAMES
   van Welbergen H, 2010, COMPUT GRAPH FORUM, V29, P2530, DOI 10.1111/j.1467-8659.2010.01822.x
   VIDEO GAME SALES WIKI, 2009, VID GAM COSTS
   Weber B, 2009, COMPUT GRAPH FORUM, V28, P481, DOI 10.1111/j.1467-8659.2009.01387.x
   Weber J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P119, DOI 10.1145/218380.218427
   WHITEHEAD J., 2010, P WORKSH PROC CONT G, P9
   Wixon Dennis, 2008, Interactions, V15, P52, DOI 10.1145/1330526.1330542
   Wonka P, 2003, ACM T GRAPHIC, V22, P669, DOI 10.1145/882262.882324
   YU D., 2008, SPELUNKY GAME SOURCE
NR 138
TC 67
Z9 111
U1 2
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2013
VL 9
IS 1
AR 1
DI 10.1145/2422956.2422957
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 097EO
UT WOS:000315457000001
DA 2024-07-18
ER

PT J
AU Ji, RR
   Yu, FX
   Zhang, TT
   Chang, SF
AF Ji, Rongrong
   Yu, Felix X.
   Zhang, Tongtao
   Chang, Shih-Fu
TI Active Query Sensing: Suggesting the Best Query View for Mobile Visual
   Search
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Measurement; Mobile visual search; mobile location
   recognition; active query sensing; content-based image retrieval
AB While much exciting progress is being made in mobile visual search, one important question has been left unexplored in all current systems. When searching objects or scenes in the 3D world, which viewing angle is more likely to be successful? More particularly, if the first query fails to find the right target, how should the user control the mobile camera to form the second query? In this article, we propose a novel Active Query Sensing system for mobile location search, which actively suggests the best subsequent query view to recognize the physical location in the mobile environment. The proposed system includes two unique components: (1) an offline process for analyzing the saliencies of different views associated with each geographical location, which predicts the location search precisions of individual views by modeling their self-retrieval score distributions. (2) an online process for estimating the view of an unseen query, and suggesting the best subsequent view change. Specifically, the optimal viewing angle change for the next query can be formulated as an online information theoretic approach. Using a scalable visual search system implemented over a NYC street view dataset (0.3 million images), we show a performance gain by reducing the failure rate of mobile location search to only 12% after the second query. We have also implemented an end-to-end functional system, including user interfaces on iPhones, client-server communication, and a remote search server. This work may open up an exciting new direction for developing interactive mobile media applications through the innovative exploitation of active sensing and query formulation.
C1 [Ji, Rongrong; Yu, Felix X.; Zhang, Tongtao; Chang, Shih-Fu] Columbia Univ, New York, NY 10027 USA.
C3 Columbia University
RP Ji, RR (corresponding author), Columbia Univ, 116th & Broadway, New York, NY 10027 USA.
EM rrji@ee.columbia.edu; yuxinnan@ee.columbia.edu; tzhang@ee.columbia.edu;
   sfchang@ee.columbia.edu
FU NSF [CNS-07-51078, CNS-07-16203]
FX This work was supported in part by NSF awards CNS-07-51078 and
   CNS-07-16203.
CR [Anonymous], INT J COMPUT VIS
   [Anonymous], P 3 INT S 3D DAT PRO
   [Anonymous], WORKSH VIS COGN TASK
   [Anonymous], 2003, P 9 IEEE INT C COMP
   [Anonymous], P IEEE COMP VIS PATT
   BAATZ G., 2010, P 10 EUR C COMP VIS
   CHANG S.-F., 2011, P ACM MULT
   CHEN D., 2011, P IEEE COMP VIS PATT
   Crandall D, 2009, P 18 INT WORLD WID W
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   EADE E, 2008, P BRIT MACH VIS C BM
   FENG J., 2012, P IEEE COMP VIS PATT
   GIROD B., 2011, IEEE SIG PROCESS MAG
   Irschara A., 2009, P IEEE COMP VIS PATT, P1
   KNOPP J., 2010, P 10 EUR C COMP VIS
   Kwok K., 2004, P 13 TEXT RETR C TRE
   Nister D., 2006, P IEEE COMP VIS PATT
   Oliva A, 2001, INT J COMPUT VIS
   OUNIS I., 2004, P INT S STRING PROC
   REHG J., 2009, P 9 IEEE INT C COMP
   Rui Y., 1999, Journal of Visual Communication and Image Representation
   TONG S, 2001, P ACM MULT
   WANG Z., 2009, P ACM MULT
   YOMTOV E., 2005, P ACM SIGIR
NR 24
TC 5
Z9 6
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 3
SU S
AR 40
DI 10.1145/2348816.2348819
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021VG
UT WOS:000309912900003
DA 2024-07-18
ER

PT J
AU Wang, XY
   Kankanhalli, M
AF Wang, Xiangyu
   Kankanhalli, Mohan
TI MultiFusion: A Boosting Approach for Multimedia Fusion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Decision fusion; boosting; atomic event multimodal fusion;
   adaboost
AB The multimodal data usually contain complementary, correlated and redundant information. Thus, multimodal fusion is useful for many multisensor applications. Here, a novel multimodal fusion algorithm is proposed, which is referred to as "MultiFusion." The approach adopts a boosting structure where the atomic event is considered as the fusion unit. The correlation of multimodal data is used to form an overall classifier in each iteration. Moreover, by adopting the Adaboost-like structure, the overall fusion performance is improved. Both the simulation experiment and the real application show the effectiveness of the MultiFusion approach. Our approach can be applied in different multimodal applications to exploit the multimedia data characteristics and improve the performance.
C1 [Wang, Xiangyu; Kankanhalli, Mohan] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 National University of Singapore
RP Wang, XY (corresponding author), Natl Univ Singapore, Sch Comp, Comp 1 COM1,13 Comp Dr, Singapore 117417, Singapore.
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
CR Aly R., 2009, P ACM INT C MULT, P233
   Amores J., 2004, PROC WORKSHOP MULTIM, P31
   Atrey PK, 2006, MULTIMEDIA SYST, V12, P239, DOI 10.1007/s00530-006-0063-8
   Barbu C, 2005, FIFTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P34, DOI 10.1109/ICDM.2005.40
   CONTRERAS FM, 2009, P IB C PATT REC IM A, P338
   Dasarathy B.V., 1994, DECISION FUSION
   FREUND, 1997, J COMPU SYST SCI, V55
   Freund Y., 1996, INT C MACHINE LEARNI, P148
   Guo G., 2001, MSRTR200116
   GUO W, 2008, P IEEE INT GEOSC REM
   Iyengar G, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P329
   JOSHI D, 2007, P INT C IM VID RETR, P309
   Maghooli K, 2004, LECT NOTES COMPUT SC, V3087, P332
   Parikh D, 2004, IEEE SYS MAN CYBERN, P1232
   PICKERING MJ, 2002, P INT C IM VID RETR, P309
   Polikar R., 2006, IEEE Circuits and Systems Magazine, V6, P21, DOI 10.1109/MCAS.2006.1688199
   Polikar R, 2001, IEEE T SYST MAN CY C, V31, P497, DOI 10.1109/5326.983933
   SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760
   Tieu K, 2004, INT J COMPUT VISION, V56, P17, DOI 10.1023/B:VISI.0000004830.93820.78
   Xue F, 2006, INT C PATT RECOG, P499
   Zhang L, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P95, DOI 10.1109/ACV.2002.1182164
   Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255
NR 22
TC 5
Z9 5
U1 2
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2010
VL 6
IS 4
AR 25
DI 10.1145/1865106.1865109
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 688VA
UT WOS:000284880900003
DA 2024-07-18
ER

PT J
AU Tjondronegoro, D
   Chen, YPP
   Joly, A
AF Tjondronegoro, Dian
   Chen, Yi-Ping Phoebe
   Joly, Adrien
TI A scalable and extensible segment-event-object-based sports video
   retrieval system
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE design; experimentation; languages; video database system; sports video
   retrieval; automatic content extraction; indexing; XML; XQuery; MPEG-7;
   mobile video interaction
ID MPEG-7
AB Sport video data is growing rapidly as a result of the maturing digital technologies that support digital video capture, faster data processing, and large storage. However, ( 1) semi-automatic content extraction and annotation, ( 2) scalable indexing model, and ( 3) effective retrieval and browsing, still pose the most challenging problems for maximizing the usage of large video databases. This article will present the findings from a comprehensive work that proposes a scalable and extensible sports video retrieval system with two major contributions in the area of sports video indexing and retrieval. The first contribution is a new sports video indexing model that utilizes semi-schema-based indexing scheme on top of an Object-Relationship approach. This indexing model is scalable and extensible as it enables gradual index construction which is supported by ongoing development of future content extraction algorithms. The second contribution is a set of novel queries which are based on XQuery to generate dynamic and user-oriented summaries and event structures. The proposed sports video retrieval system has been fully implemented and populated with soccer, tennis, swimming, and diving video. The system has been evaluated against 20 users to demonstrate and confirm its feasibility and benefits. The experimental sports genres were specifically selected to represent the four main categories of sports domain: period- ,set- point-, time (race)-, and performance-based sports. Thus, the proposed system should be generic and robust for all types of sports.
C1 [Tjondronegoro, Dian] Queensland Univ Technol, Sch Informat Syst, Brisbane, Qld 4001, Australia.
   [Chen, Yi-Ping Phoebe] Deakin Univ, Sch Informat Technol, Geelong, Vic 3217, Australia.
   [Joly, Adrien] Queensland Univ Technol, Fac Informat Technol, Brisbane, Qld 4001, Australia.
C3 Queensland University of Technology (QUT); Deakin University; Queensland
   University of Technology (QUT)
RP Tjondronegoro, D (corresponding author), Queensland Univ Technol, Sch Informat Syst, Brisbane, Qld 4001, Australia.
EM dian@qut.edu.au
RI Tjondronegoro, Dian/AAE-4685-2022; Joly, Adrien/C-1125-2009;
   Tjondronegoro, Dian/I-9843-2012; Chen, Yi-Ping Phoebe/B-8844-2008
OI Tjondronegoro, Dian/0000-0001-7446-2839; Chen, Yi-Ping
   Phoebe/0000-0002-4122-3767
CR Adali S, 1996, MULTIMEDIA SYST, V4, P172, DOI 10.1007/s005300050021
   Adams B, 2002, IEEE T MULTIMEDIA, V4, P472, DOI 10.1109/TMM.2002.802016
   Assfalg J, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P527
   Assfalg J, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P825, DOI 10.1109/ICME.2002.1035909
   ASSFALG J, 2002, P 9 INT C EL CIRC SY, V1053, P1059
   Babaguchi N, 2003, IEEE IMAGE PROC, P13
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   BABAGUCHI N, 2000, P ACM MULT 2000 WORK, P205
   BABAGUCHI N, 2003, P JOINT C 4 INT C IN
   Blaha M., 1998, Object-Oriented Modeling and Design for Database Applications
   BOAG S, 2004, IN PRESS XQUERY 1 0
   Brundage M., 2004, XQuery: The XML Query Language
   CHAIRSORN L, 2002, P 6 IFIP WORK C VIS, P94
   Chang SF, 2001, IEEE T CIRC SYST VID, V11, P688, DOI 10.1109/76.927421
   Dimitrova N, 2001, DESIGN AND MANAGEMENT OF MULTIMEDIA INFORMATION SYSTEMS: OPPORTUNITIES AND CHALLENGES, P22
   DOBBIE G, 2000, ORA SS OBJECT RELATI
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   EKIN A, 2003, P INT C MULT EXP, P6
   Han M, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P950
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   Heng WJ, 2002, IEEE T MULTIMEDIA, V4, P434, DOI 10.1109/TMM.2002.806532
   Kosch H., 2004, DISTRIBUTED MULTIMED
   Lu G., 1999, MULTIMEDIA DATABASE
   Manjunath B.S., 2002, INTRO MPEG 7
   MENG HM, 2001, P IEEE INT C AC SPEE, V1403, P1401
   Nepal S., 2001, ACM Multimedia, P261
   Ngai CH, 2002, P INT COMP SOFTW APP, P173, DOI 10.1109/CMPSAC.2002.1044549
   OH J, 2000, P 2000 ACM SIGMOD IN, P415
   Oomoto E, 1997, HANDBOOK OF MULTIMEDIA INFORMATION MANAGEMENT, P405
   Pérez A, 2001, J NEUROVIROL, V7, P1, DOI 10.1080/135502801300069575
   Ponceleon D., 1998, Proceedings ACM Multimedia 98, P99, DOI 10.1145/290747.290760
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Sato T, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P52, DOI 10.1109/CAIVD.1998.646033
   Tjondronegoro D, 2004, IEEE MULTIMEDIA, V11, P22, DOI 10.1109/MMUL.2004.28
   Tjondronegoro D., 2004, P 6 INT ACM MULT INF
   TJONDRONEGORO D, 2003, P 5 ACM SIGMM INT WO
   Tseng BL, 2004, IEEE MULTIMEDIA, V11, P42, DOI 10.1109/MMUL.2004.1261105
   Wu C, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P805, DOI 10.1109/ICME.2002.1035904
   Xie LX, 2002, INT CONF ACOUST SPEE, P4096
   XU P, 1998, P IEEE INT C MULT EX
   Yu X., 2003, PROC 11 ACM INT C MU, P11
   Zelnik-Manor L, 2001, PROC CVPR IEEE, P123
   ZHANG HJ, 1999, CONTENT BAASED VIDEO
NR 43
TC 5
Z9 7
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 2
AR 13
DI 10.1145/1352012.1352017
PG 40
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 306RG
UT WOS:000256264900005
DA 2024-07-18
ER

PT J
AU Xu, M
   Xu, CS
   Duan, LY
   Jin, JS
   Luo, SH
AF Xu, Min
   Xu, Changsheng
   Duan, Lingyu
   Jin, Jesse S.
   Luo, Suhuai
TI Audio keywords generation for sports video analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE documentation; languages; audio keywords; sports video analysis;
   semantics analysis; event detection; support vector machines
AB Sports video has attracted a global viewership. Research effort in this area has been focused on semantic event detection in sports video to facilitate accessing and browsing. Most of the event detection methods in sports video are based on visual features. However, being a significant component of sports video, audio may also play an important role in semantic event detection. In this paper, we have borrowed the concept of the "keyword" from the text mining domain to define a set of specific audio sounds. These specific audio sounds refer to a set of game-specific sounds with strong relationships to the actions of players, referees, commentators, and audience, which are the reference points for interesting sports events. Unlike low-level features, audio keywords can be considered as a mid-level representation, able to facilitate high-level analysis from the semantic concept point of view. Audio keywords are created from low-level audio features with learning by support vector machines. With the help of video shots, the created audio keywords can be used to detect semantic events in sports video by Hidden Markov Model ( HMM) learning. Experiments on creating audio keywords and, subsequently, event detection based on audio keywords have been very encouraging. Based on the experimental results, we believe that the audio keyword is an effective representation that is able to achieve satisfying results for event detection in sports video. Application in three sports types demonstrates the practicality of the proposed method.
C1 [Xu, Min; Jin, Jesse S.; Luo, Suhuai] Univ Newcastle, Callaghan, NSW 2308, Australia.
   [Xu, Changsheng; Duan, Lingyu] Inst Infocom Res, Singapore, Singapore.
C3 University of Newcastle; Agency for Science Technology & Research
   (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Xu, M (corresponding author), Univ Newcastle, Callaghan, NSW 2308, Australia.
EM m.xu@studentmail.newcastle.edu.au
RI xu, cj/HJZ-3488-2023
OI Luo, Suhuai/0000-0002-6185-6035; Xu, Min/0000-0001-9581-8849
CR [Anonymous], 1998, STAT LEARNING THEORY
   Ardizzone E., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P140, DOI 10.1109/ICPR.1996.546810
   Assfalg J, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P825, DOI 10.1109/ICME.2002.1035909
   BAILLIE M, 2004, P IEEE COMP SOC C CO
   Chang SF, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P531, DOI 10.1109/ICIP.1998.727321
   Chang YL, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P306, DOI 10.1109/MMCS.1996.534992
   Deller J., 1999, Discrete-time processing of speech signals
   Duan LY, 2005, IEEE T MULTIMEDIA, V7, P1066, DOI 10.1109/TMM.2005.858395
   Gong YH, 2000, INT C PATT RECOG, P860, DOI 10.1109/ICPR.2000.905551
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   Leonardi R, 2004, IEEE T CIRC SYST VID, V14, P634, DOI 10.1109/TCSVT.2004.826751
   LIU S, 2005, IEEE T MULTIMEDIA, V7, P1066
   Miyauchi S, 2002, INT C PATT RECOG, P1009, DOI 10.1109/ICPR.2002.1048476
   NEPAL S, 2001, P ACM MULT
   Pan H, 2001, INT CONF ACOUST SPEE, P1649, DOI 10.1109/ICASSP.2001.941253
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   SADLIER D, 2002, P IEEE INT C MULT EX, V2, P77
   Sadlier DA, 2005, IEEE T CIRC SYST VID, V15, P1225, DOI 10.1109/TCSVT.2005.854237
   Sebe N, 2000, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P15, DOI 10.1109/IVL.2000.853833
   Snoek CGM, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P481
   Sundaram H, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1145, DOI 10.1109/ICME.2000.871563
   Wei J, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P460, DOI 10.1109/MMCS.1999.778502
   Wu C, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P805, DOI 10.1109/ICME.2002.1035904
   Xie LX, 2004, PATTERN RECOGN LETT, V25, P767, DOI 10.1016/j.patrec.2004.01.005
   XIONG Z, 2003, P IEEE INT C IM PROC, V1, P14
   XIONG Z, 2003, IEEE INT C AC SPEECH
   Xu M, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P189
   Xu M, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P281
   XU M, 2003, P IEEE INT C AC SPEE
   Young S., 2002, HTK BOOK HTK VERSION
   ZHANG HJ, 1995, P SOC PHOTO-OPT INS, V2417, P389, DOI 10.1117/12.206066
   ZHANG T, 2001, CONTENT BASED AUDIO
NR 33
TC 47
Z9 52
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 2
AR 11
DI 10.1145/1352012.1352015
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 306RG
UT WOS:000256264900003
DA 2024-07-18
ER

PT J
AU Atrey, PK
   Kankanhalli, MS
   Oommen, JB
AF Atrey, Pradeep K.
   Kankanhalli, Mohan S.
   Oommen, John B.
TI Goal-oriented optimal subset selection of correlated multimedia streams
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; security; agreement coefficient; confidence fusion; media
   fusion; optimal media selection
AB A multimedia analysis system utilizes a set of correlated media streams, each of which, we assume, has a confidence level and a cost associated with it, and each of which partially helps in achieving the system goal. However, the fact that at any instant, not all of the media streams contribute towards a system goal brings up the issue of finding the best subset from the available set of media streams. For example, a subset of two video cameras and two microphones could be better than any other subset of sensors at some time instance to achieve a surveillance goal (e.g. event detection). This article presents a novel framework that finds. the optimal subset of media streams so as to achieve the system goal under specified constraints. The proposed framework uses a dynamic programming approach to find the optimal subset of media streams based on three different criteria: first, by maximizing the probability of achieving the goal under the specified cost and confidence; second, by maximizing the confidence in the achieved goal under the specified cost and probability with which the goal is achieved; and third, by minimizing the cost to achieve the goal with a specified probability and confidence. Each of these problems is proven to be NP-Complete. From an AI point of view, the solution we propose is heuristic-based, and for each criterion, utilizes a heuristic function which for a given problem, combines optimal solutions of small-sized subproblems to yield a potential near-optimal solution to the original problem. The proposed framework allows for a tradeoff among the aforementioned three criteria, and offers the flexibility to compare whether any one set of media streams of low cost would be better than any other set of higher cost, or whether any one set of media streams of high confidence would be better than any other set of low confidence. To show the utility of our framework, we provide the experimental results for event detection in a surveillance scenario.
C1 Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
   Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada.
C3 National University of Singapore; Carleton University
RP Atrey, PK (corresponding author), Natl Univ Singapore, Sch Comp, 3 Sci Dr 2, Singapore 117543, Singapore.
EM pradeepk@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019; Oommen, B. John/P-6323-2017
OI Kankanhalli, Mohan/0000-0002-4846-2015; Oommen, B.
   John/0000-0002-5105-1575
CR [Anonymous], 2006, PROC IEEE INT C ACOU
   [Anonymous], P IEEE WORKSH EV MIN
   Atrey P.K., 2005, P 3 ACM INT WORKSH V, DOI 10.1145/1099396.1099416.
   Atrey PK, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P305, DOI 10.1109/ICME.2005.1521421
   Debouk R, 2002, DISCRETE EVENT DYN S, V12, P417, DOI 10.1023/A:1019770124060
   Isler V, 2005, 2005 FOURTH INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P151
   JAIN R, 2004, UBIQUITY, V5, P29
   Jiang SB, 2003, IEEE T AUTOMAT CONTR, V48, P369, DOI 10.1109/TAC.2003.809144
   Lam K., 2004, Proc. ACM 2nd international workshop on Video surveillance sensor networks, P63
   Oommen BJ, 2005, ARTIF INTELL, V164, P1, DOI 10.1016/j.artint.2002.02.001
   Pahalawatta PV, 2004, IEEE IMAGE PROC, P3073
   SIEGEL M, 2004, P IEEE INT WORKSH RO, P96
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
NR 13
TC 15
Z9 17
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 1
AR 2
DI 10.1145/1198302.1198304
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IZ
UT WOS:000250871500002
DA 2024-07-18
ER

PT J
AU Kum, SU
   Mayer-Patel, K
AF Kum, Sang-Uok
   Mayer-Patel, Ketan
TI Real-Time Multidepth Stream Compression
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Real-time compression; tele-immersion; tele-presence;
   augmented reality; virtual reality; k-means algorithm; k-means
   initialization
ID 3D VIDEO
AB The goal of tele-immersion has long been to enable people at remote locations to share a sense of presence. A tele-immersion system acquires the 3D representation of a collaborator's environment remotely and sends it over the network where it is rendered in the user's environment. Acquisition, reconstruction, transmission, and rendering all have to be done in real-time to create a sense of presence. With added commodity hardware resources, parallelism can increase the acquisition volume and reconstruction data quality while maintaining real-time performance. However, this is not as easy for rendering since all of the data need to be combined into a single display.
   In this article, we present an algorithm to compress data from such 3D environments in real-time to solve this imbalance. We present a compression algorithm which scales comparably to the acquisition and reconstruction, reduces network transmission bandwidth, and reduces the rendering requirement for real-time performance. This is achieved by exploiting the coherence in the 3D environment data and removing them in real-time. We have tested the algorithm using a static office data set as well as a dynamic scene, the results of which are presented in the article.
C1 [Kum, Sang-Uok; Mayer-Patel, Ketan] Univ N Carolina, Chapel Hill, NC 27599 USA.
C3 University of North Carolina; University of North Carolina Chapel Hill
RP Kum, SU (corresponding author), Univ N Carolina, CB 3175,Sitterson Hall, Chapel Hill, NC 27599 USA.
EM kumsu@cs.unc.edu; kmp@cs.unc.edu
CR [Anonymous], P INT WORKSH IMM TEL
   Chang CF, 1999, COMP GRAPH, P291, DOI 10.1145/311535.311571
   Chen WC, 2000, IEEE VISUAL, P327, DOI 10.1109/VISUAL.2000.885712
   DeFanti TA, 1993, Proceedings of the 20th annual conference on Computer graphics and interactive techniques, P135, DOI 10.1145/166117.166134.
   Gross M, 2003, ACM T GRAPHIC, V22, P819, DOI 10.1145/882262.882350
   Grossman J. P., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P181
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Kauff Peter, 2002, P 4 INT C COLLABORAT, P105
   KELSHIKAR N, 2003, P TER PERF AN WORKSH
   KUM SU, 2003, P 11 ACM INT C MULT, P185
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   Molnar S., 1992, Proceedings of the 19th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '92, P231
   Peña JM, 1999, PATTERN RECOGN LETT, V20, P1027, DOI 10.1016/S0167-8655(99)00069-0
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Stoll G, 2001, COMP GRAPH, P141, DOI 10.1145/383259.383273
   Towles H., 2002, INT WORKSHOP IMMERSI
   Würmlin S, 2004, COMPUT GRAPH-UK, V28, P3, DOI 10.1016/j.cag.2003.10.015
NR 20
TC 1
Z9 1
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2005
VL 1
IS 2
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DV
UT WOS:000205012300002
DA 2024-07-18
ER

PT J
AU Li, B
   Zhang, Y
   Zhang, CY
   Piao, XL
   Yin, BC
AF Li, Bo
   Zhang, Yong
   Zhang, Chengyang
   Piao, Xinglin
   Yin, Baocai
TI Hypergraph AssociationWeakly Supervised Crowd Counting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Crowd counting; hypergraph neural network; uneven distribution of crowd
   density; hypergraph association
AB Weakly supervised crowd counting involves the regression of the number of individuals present in an image, using only the total number as the label. However, this task is plagued by two primary challenges: the large variation of head size and uneven distribution of crowd density. To address these issues, we propose a novel Hypergraph Association Crowd Counting (HACC) framework. Our approach consists of a new multi-scale dilated pyramid module that can efficiently handle the large variation of head size. Further, we propose a novel hypergraph association module to solve the problem of uneven distribution of crowd density by encoding higher-order associations among features, which opens a new direction to solve this problem. Experimental results on multiple datasets demonstrate that our HACC model achieves new state-of-the-art results.
C1 [Li, Bo; Zhang, Yong; Zhang, Chengyang; Piao, Xinglin; Yin, Baocai] Beijing Univ Technol, Beijing Artificial Intelligence Inst, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Zhang, Y (corresponding author), Beijing Univ Technol, Beijing Artificial Intelligence Inst, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM bo_li@emails.bjut.edu.cn; zhangyong2010@bjut.edu.cn;
   Cy_Zhang@emails.bjut.edu.cn; piaoxl@bjut.edu.cn; ybc@bjut.edu.cn
RI Li, Bo/JLN-1186-2023; Zhang, Yong/AAW-8880-2021
OI Zhang, Yong/0000-0001-6650-6790; Zhang, Chengyang/0000-0003-3658-5779;
   Li, Bo/0000-0003-0608-1502
FU National Key R&D Program of China [2021ZD0111902]; National Natural
   Science Foundation of China [62072015, U21B2038, U19B2039, 61902053];
   Beijing Natural Science Foundation [4222021]
FX This research project was partially supported by the National Key R&D
   Program of China (2021ZD0111902), the National Natural Science
   Foundation of China (62072015, U21B2038, U19B2039, and 61902053), and
   the Beijing Natural Science Foundation (4222021).
CR Abousamra S, 2021, AAAI CONF ARTIF INTE, V35, P872
   Atwood J, 2016, ADV NEUR IN, V29
   Balid W, 2018, IEEE T INTELL TRANSP, V19, P1784, DOI 10.1109/TITS.2017.2741507
   Boominathan L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P640, DOI 10.1145/2964284.2967300
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen X, 2022, arXiv
   Chen XY, 2020, NEUROCOMPUTING, V407, P399, DOI 10.1016/j.neucom.2020.04.117
   Cheng J, 2021, IEEE T IMAGE PROCESS, V30, P2862, DOI 10.1109/TIP.2021.3055631
   Defferrard M, 2016, ADV NEUR IN, V29
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Falk T, 2019, NAT METHODS, V16, P67, DOI 10.1038/s41592-018-0261-2
   Fan Q, 2020, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR42600.2020.00407
   Fang YC, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115664
   Feng YF, 2019, AAAI CONF ARTIF INTE, P3558
   Gao JY, 2021, Arxiv, DOI [arXiv:2108.00584, DOI 10.1016/J.NEUCOM.2022.09.113]
   Gao JY, 2019, NEUROCOMPUTING, V363, P1, DOI 10.1016/j.neucom.2019.08.018
   Grant JM, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3052930
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   He SH, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101892
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jiang JW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2635
   Kingma D. P., 2014, arXiv
   Laradji IH, 2018, LECT NOTES COMPUT SC, V11206, P560, DOI 10.1007/978-3-030-01216-8_34
   Lei YJ, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107616
   Lempitsky V., 2010, Advances in Neural Information Processing Systems, V23
   Li B, 2023, VISUAL COMPUT, V39, P2671, DOI 10.1007/s00371-022-02485-3
   Li Wang, 2020, P INT C ARTIFICIAL L
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Liang DK, 2022, LECT NOTES COMPUT SC, V13661, P38, DOI 10.1007/978-3-031-19769-7_3
   Liang DK, 2023, IEEE T MULTIMEDIA, V25, P6040, DOI 10.1109/TMM.2022.3203870
   Liang DK, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3445-y
   Lin H, 2022, PROC CVPR IEEE, P19596, DOI 10.1109/CVPR52688.2022.01901
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu L, 2020, IEEE T CIRC SYST VID, V30, P3513, DOI 10.1109/TCSVT.2019.2942970
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu Y., 2021, arXiv, DOI DOI 10.48550/ARXIV.2112.05561
   Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2020, AAAI CONF ARTIF INTE, V34, P11677
   Luo A, 2020, AAAI CONF ARTIF INTE, V34, P11693
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Ma ZT, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3123423
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Oñoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38
   Peng Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P322, DOI 10.1007/978-3-030-58577-8_20
   Sam DB, 2018, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2018.00381
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Savner SS, 2022, Arxiv, DOI arXiv:2203.03768
   Shu WB, 2022, PROC CVPR IEEE, P19586, DOI 10.1109/CVPR52688.2022.01900
   Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2019.00131
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Song QY, 2021, AAAI CONF ARTIF INTE, V35, P2576
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian Mengxiao, 2021, arXiv
   Tian Y, 2021, Arxiv, DOI [arXiv:2109.14483, DOI 10.48550/ARXIV.2109.14483]
   Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201
   Wang C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1299, DOI 10.1145/2733373.28063370-12345-67-8/90/01
   Wang Fusen, 2022, arXiv
   Wang M., 2022, arXiv
   Wang MJ, 2023, IEEE WINT CONF APPL, P167, DOI 10.1109/WACV56688.2023.00025
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu CF, 2022, INT J COMPUT VISION, V130, P405, DOI 10.1007/s11263-021-01542-z
   Yang JX, 2018, INT C PATT RECOG, P3244, DOI 10.1109/ICPR.2018.8545683
   Yifan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P1, DOI 10.1007/978-3-030-58598-3_1
   Zeng LK, 2017, IEEE IMAGE PROC, P465, DOI 10.1109/ICIP.2017.8296324
   Zhai Qiang, 2022, IEEE Transactions on Multimedia
   Zhang AR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3356019
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu L., 2019, arXiv, DOI DOI 10.48550/ARXIV.1902.01115
   Zou ZK, 2019, NEUROCOMPUTING, V367, P75, DOI 10.1016/j.neucom.2019.08.009
NR 83
TC 1
Z9 1
U1 7
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 195
DI 10.1145/3594670
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200018
DA 2024-07-18
ER

PT J
AU Ricci, S
   Uricchio, T
   Del Bimbo, A
AF Ricci, Simone
   Uricchio, Tiberio
   Del Bimbo, Alberto
TI Meta-learning Advisor Networks for Long-tail and Noisy Labels in Social
   Image Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Meta-learning; neural networks; long-tail; noisy labels
ID SMOTE
AB Deep neural networks (DNNs) for social image classification are prone to performance reduction and overfitting when trained on datasets plagued by noisy or imbalanced labels. Weight loss methods tend to ignore the influence of noisy or frequent category examples during the training, resulting in a reduction of final accuracy and, in the presence of extreme noise, even a failure of the learning process. A new advisor network is introduced to address both imbalance and noise problems, and is able to pilot learning of amain network by adjusting the visual features and the gradient with a meta-learning strategy. In a curriculum learning fashion, the impact of redundant data is reduced while recognizable noisy label images are downplayed or redirected. Meta Feature Re-Weighting (MFRW) and Meta Equalization Softmax (MES) methods are introduced to let the main network focus only on the information in an image deemed relevant by the advisor network and to adjust the training gradient to reduce the adverse effects of frequent or noisy categories. The proposed method is first tested on synthetic versions of CIFAR10 and CIFAR100, and then on the more realistic ImageNet-LT, Places-LT, and Clothing1M datasets, reporting state-of-the-art results.
C1 [Ricci, Simone; Uricchio, Tiberio; Del Bimbo, Alberto] Univ Florence, Viale Morgagni,65, I-50134 Florence, Italy.
C3 University of Florence
RP Ricci, S (corresponding author), Univ Florence, Viale Morgagni,65, I-50134 Florence, Italy.
EM simone.ricci@unifi.it; tiberio.uricchio@unifi.it;
   alberto.delbimbo@unifi.it
OI URICCHIO, TIBERIO/0000-0003-1025-4541; Ricci,
   Simone/0000-0001-9838-6076; DEL BIMBO, ALBERTO/0000-0002-1052-8322
FU European Commission under European Horizon 2020 Programme [951911]
FX This work was supported by the European Commission under European
   Horizon 2020 Programme, grant number 951911 AI4Media.
CR Algan G, 2021, KNOWL-BASED SYST, V215, DOI 10.1016/j.knosys.2021.106771
   Arazo E, 2019, PR MACH LEARN RES, V97
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Cao KD, 2019, Arxiv, DOI arXiv:1906.07413
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen Can, 2021, Advances in Neural Information Processing Systems, V34, P14097
   Cheng Yong, 2020, P 58 ANN M ASS COMPU, P5961, DOI 10.18653/v1/2020.acl-main.529
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DeVries T, 2017, Arxiv, DOI [arXiv:1708.04552, DOI 10.48550/ARXIV.1708.04552]
   Drummond Chris, 2003, ICML KDD 2003 WORKSH, V3
   Goldberger J., 2016, INT C LEARN REPR
   Han B, 2018, Arxiv, DOI arXiv:1804.06872
   Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91
   Hendrycks D, 2018, ADV NEUR IN, V31
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Jamal M. A., 2020, P IEEECVF C COMPUTER, P7610, DOI DOI 10.1109/CVPR42600.2020.00763
   Jiang L, 2020, PR MACH LEARN RES, V446, P4804, DOI DOI 10.48550/ARXIV.1911.09781
   Jingru Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11659, DOI 10.1109/CVPR42600.2020.01168
   Kang B., 2019, arXiv, DOI DOI 10.48550/ARXIV.1910.09217
   Karthik S, 2021, Arxiv, DOI arXiv:2108.11096
   Khan SH, 2018, IEEE T NEUR NET LEAR, V29, P3573, DOI 10.1109/TNNLS.2017.2732482
   Kingma D. P., 2014, arXiv
   Kordumova Svetlana, 2016, MultiMedia Modeling. 22nd International Conference, MMM 2016. Proceedings, P51, DOI 10.1007/978-3-319-27671-7_5
   Li J, 2020, INT C LEARNING REPRE
   Li JN, 2019, PROC CVPR IEEE, P5046, DOI 10.1109/CVPR.2019.00519
   Li MM, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P509, DOI 10.1145/3357384.3357912
   Li XR, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2906152
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu S., 2020, Adv. Neural Inf. Process. Syst, V33, P20331
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Loshchilov Ilya, 2016, arXiv
   Lu J, 2018, PR MACH LEARN RES, V80
   Ma XJ, 2018, PR MACH LEARN RES, V80
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Meghawat M, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P190, DOI 10.1109/MIPR.2018.00042
   Nettleton DF, 2010, ARTIF INTELL REV, V33, P275, DOI 10.1007/s10462-010-9156-z
   Nishi K, 2021, PROC CVPR IEEE, P8018, DOI 10.1109/CVPR46437.2021.00793
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Pawan Kumar M., 2010, NIPS
   Pechenizkiy M, 2006, COMP MED SY, P708, DOI 10.1109/CBMS.2006.65
   Peng Chu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P694, DOI 10.1007/978-3-030-58526-6_41
   Reed H., 2014, arXiv
   Ren JW, 2020, Arxiv, DOI arXiv:2007.10740
   Ren MY, 2018, PR MACH LEARN RES, V80
   Shu J, 2019, P ADV NEUR INF PROC, P1919
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Y.-X., 2017, Advances in Neural Information Processing Systems, P7032
   Wang Z, 2020, PROC CVPR IEEE, P4523, DOI 10.1109/CVPR42600.2020.00458
   Wei Hongxin, 2020, P IEEE C COMP VIS PA
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xu YJ, 2021, PROC CVPR IEEE, P144, DOI 10.1109/CVPR46437.2021.00021
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Yin X, 2019, PROC CVPR IEEE, P5697, DOI 10.1109/CVPR.2019.00585
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu LC, 2020, PROC CVPR IEEE, P4343, DOI 10.1109/CVPR42600.2020.00440
NR 59
TC 1
Z9 1
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2023
VL 19
IS 5
SU S
AR 169
DI 10.1145/3584360
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L0SK0
UT WOS:001020437100005
OA Bronze
DA 2024-07-18
ER

PT J
AU Zeng, SN
   Rao, YB
   Zhang, B
   Xu, Y
AF Zeng, Shaoning
   Rao, Yunbo
   Zhang, Bob
   Xu, Yong
TI Joint Augmented and Compressed Dictionaries for Robust Image
   Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Classification and regression; inference algorithms; supervised
   learning; machine learning
ID UNDERDETERMINED SYSTEMS; SPARSE REPRESENTATION; LINEAR-EQUATIONS; K-SVD;
   ALGORITHM
AB Dictionary-based Classification (DC) has been a promising learning theory in multimedia computing. Previous studies focused on learning a discriminative dictionary as well as the sparsest representation based on the dictionary, to cope with the complex conditions in real-world applications. However, robustness by learning only one single dictionary is far from the optimal level. What is worse, it cannot take advantage of the available techniques proven in modern machine learning, like data augmentation, to mitigate the same problem. In this work, we propose a novel method that utilizes joint Augmented and Compressed Dictionaries for Robust Dictionary-based Classification (ACD-RDC). For optimization under the noise model introduced by real-world conditions, the objective function of ACD-RDC incorporates only two simple, but well-designed constraints, including one enhanced sparsity constraint by the general data augmentation, which requires less case-by-case and sophisticated tuning, and another discriminative constraint solved by a jointly learned dictionary. The optimization of the objective function is then deduced theoretically to an approximate linear problem. The sparsity and discrimination enhanced by data augmentation guarantees the robustness for image classification under various conditions, which constructs the first positive case using data augmentation to obtain robust dictionary-based classification. Numerous experiments have been conducted on popular facial and object image datasets. The results demonstrate that ACD-RDC obtains more promising classification on diversely collected images than the current dictionary-based classification methods. ACD-RDC is also confirmed to be a state-of-the-art classification method when using deep features as inputs.
C1 [Zeng, Shaoning] Univ Elect Sci & Technol China, Yangtze Delta Reg Inst Huzhou, B1 Floor 8,South Taihu Technol Innovat Bldg,819 R, Huzhou 313000, Peoples R China.
   [Rao, Yunbo] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Xiyuan Ave,West Hitech Zone, Chengdu 611731, Peoples R China.
   [Zhang, Bob] Univ Macau, Dept Comp & Informat Sci, Avenida Univ, Taipa 999078, Macau, Peoples R China.
   [Xu, Yong] Harbin Inst Technol Shenzhen, Dept Comp Sci, Coll Town, Shenzhen 518000, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; University of Macau; Harbin
   Institute of Technology
RP Rao, YB (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Software Engn, Xiyuan Ave,West Hitech Zone, Chengdu 611731, Peoples R China.; Zhang, B (corresponding author), Univ Macau, Dept Comp & Informat Sci, Avenida Univ, Taipa 999078, Macau, Peoples R China.
EM zsn@outlook.com; cloudrao@gmail.com; bobzhang@umac.mo; yongxu@ymail.com
RI Zhang, Bob/ABD-5926-2021
OI Zhang, Bob/0000-0003-2497-9519; Rao, Yunbo/0000-0001-5433-7379
FU University of Macau [MYRG2018-00053-FST]; Science and Technology Project
   of Sichuan [2020YFG0459, 2021YFG0314, 2022ZHCG0033]; National Natural
   Science Foundation of China [U19A2078]; NVIDIA Corporation
FX This research was supported in part by the University of Macau (File no.
   MYRG2018-00053-FST), the Science and Technology Project of Sichuan (NOs.
   2020YFG0459, 2021YFG0314, 2022ZHCG0033), and the National Natural
   Science Foundation of China (Grant NO. U19A2078). We gratefully
   acknowledge the support of NVIDIA Corporation with the donation of the
   Titan Xp GPU used for this research.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Akhtar N, 2017, PROC CVPR IEEE, P3919, DOI 10.1109/CVPR.2017.417
   Akhtar N, 2017, PATTERN RECOGN, V65, P136, DOI 10.1016/j.patcog.2016.12.017
   Benavente R, 1998, 24 COMP VIS CTR
   Cai SJ, 2016, PROC CVPR IEEE, P2950, DOI 10.1109/CVPR.2016.322
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Chen B, 2015, IEEE T KNOWL DATA EN, V27, P1964, DOI 10.1109/TKDE.2015.2397444
   Chen ZT, 2019, AAAI CONF ARTIF INTE, P3379
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donoho DL, 2012, IEEE T INFORM THEORY, V58, P1094, DOI 10.1109/TIT.2011.2173241
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Dvornik N, 2018, LECT NOTES COMPUT SC, V11216, P375, DOI 10.1007/978-3-030-01258-8_23
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gou JP, 2020, NEURAL NETWORKS, V125, P104, DOI 10.1016/j.neunet.2020.01.020
   Gou JP, 2019, EXPERT SYST APPL, V115, P356, DOI 10.1016/j.eswa.2018.08.021
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hobert JP, 2011, CH CRC HANDB MOD STA, P253
   Huang K., 2007, P 19 INT C NEUR INF, P609, DOI DOI 10.7551/MITPRESS/7503.003.0081
   Hurley N, 2009, IEEE T INFORM THEORY, V55, P4723, DOI 10.1109/TIT.2009.2027527
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Kundu A, 2017, J MACH LEARN RES, V18
   Li S, 2018, IEEE T PATTERN ANAL, V40, P2963, DOI 10.1109/TPAMI.2017.2764893
   Li ZM, 2017, IEEE T NEUR NET LEAR, V28, P278, DOI 10.1109/TNNLS.2015.2508025
   MacWilliams F. J., 1977, The theory of error-correcting codes. II
   Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156
   Nene S. A., 1996, Tech. Rep. CUCS-005-96
   Ozdemir O, 2018, IEEE T PATTERN ANAL, V40, P2740, DOI 10.1109/TPAMI.2017.2774300
   Pirs G, 2019, J MACH LEARN RES, V20
   Polson NG, 2011, BAYESIAN ANAL, V6, P1, DOI 10.1214/11-BA601
   Ruan WJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5944
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   TANNER MA, 1987, J AM STAT ASSOC, V82, P528, DOI 10.2307/2289457
   Tao SZ, 2016, SIAM J OPTIMIZ, V26, P313, DOI 10.1137/151004549
   Tran L, 2019, IEEE T PATTERN ANAL, V41, P3007, DOI 10.1109/TPAMI.2018.2868350
   Tran T., 2017, NEURIPS 2017, P2797
   van Dyk DA, 2001, J COMPUT GRAPH STAT, V10, P1, DOI 10.1198/10618600152418584
   Wang H, 2013, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2013.398
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wen J, 2020, IEEE T CYBERNETICS, V50, P1418, DOI 10.1109/TCYB.2018.2884715
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xiao H, 2017, Arxiv, DOI [arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747]
   Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y
   Xu C, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2490539
   Xu Y, 2017, IEEE ACCESS, V5, P8502, DOI 10.1109/ACCESS.2017.2695239
   Xu Y, 2017, INFORM SCIENCES, V375, P171, DOI 10.1016/j.ins.2016.09.059
   Xu Y, 2013, INT J INNOV COMPUT I, V9, P543
   Yamaguchi S, 2020, AAAI CONF ARTIF INTE, V34, P6566
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yang ZG, 2020, IEEE T PATTERN ANAL, V42, P1243, DOI 10.1109/TPAMI.2019.2893953
   You S, 2018, AAAI CONF ARTIF INTE, P4390
   Zeng SN, 2022, IEEE T CYBERNETICS, V52, P4935, DOI 10.1109/TCYB.2020.3025757
   Zeng Shaoning, 2018, ASIAN C MACHINE LEAR, P502
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989
   Zhang Y, 2019, NEUROCOMPUTING, V360, P209, DOI 10.1016/j.neucom.2019.05.059
   Zhang Z, 2015, IEEE ACCESS, V3, P490, DOI 10.1109/ACCESS.2015.2430359
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhu J, 2014, J MACH LEARN RES, V15, P1073
NR 67
TC 0
Z9 0
U1 2
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 136
DI 10.1145/3572910
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700011
DA 2024-07-18
ER

PT J
AU Chu, BF
   Lin, YT
   Zhong, BN
   Tang, ZJ
   Li, XX
   Wang, J
AF Chu, Binfei
   Lin, Yiting
   Zhong, Bineng
   Tang, Zhenjun
   Li, Xianxian
   Wang, Jing
TI Robust Long-Term Tracking via Localizing Occluders
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Object tracking; occlusion detection; long-term tracking
ID VISUAL TRACKING; MULTIPLE
AB Occlusion is known as one of the most challenging factors in long-term tracking because of its unpredictable shape. Existing works devoted into the design of loss functions, training strategies or model architectures, which are considered to have not directly touched the key point. Alternatively, we came up with a direct and natural idea that is discarding things that covers the target. We propose a novel occluder-aware representation learning framework to develop this idea. First, we design a local occluders detection module (LODM) to localize the occluders, which works on the principle that discriminates the non-noumenal part from a target based on the general knowledge of this category. An extra dataset and a clustering strategy is proposed to support this general knowledge. Second, we devise a feature reconstruction module to guide the occluder-aware representation learning. With the help of above methods, our localizing occluders tracker, called LOTracker, can learn an occluder-free representation and promote the performance that tracks with occlusion scenarios. Extensive experimental results show that our LOTracker achieves a state-of-the-art performance in multiple benchmarks such as LaSOT, VOTLT2018, VOTLT2019, and OxUvALT.
C1 [Chu, Binfei; Lin, Yiting; Zhong, Bineng; Tang, Zhenjun; Li, Xianxian] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin, Peoples R China.
   [Wang, Jing] Huaqiao Univ, Dept Comp Sci & Technol, Xiamen, Peoples R China.
C3 Guangxi Normal University; Huaqiao University
RP Zhong, BN; Li, XX (corresponding author), Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin, Peoples R China.
EM binfeichu@stu.hqu.edu.cn; 893448256@qq.com; bnzhong@gxnu.edu.cn;
   tangzj230@163.com; lixx@gxnu.edu.cn
RI Lin, Yiting/HCI-5561-2022
OI Lin, Yiting/0000-0003-4159-3132; Tang, Zhenjun/0000-0003-3664-1363; Lin,
   YiTing/0000-0003-3138-4652
FU Project of Guangxi Science and Technology [2022GXNSFDA035079,
   GuiKeAD21075030]; National Natural Science Foundation of China
   [61972167]; Guangxi "Bagui Scholar" Teams for Innovation and Research
   Project; Guangxi Collaborative Innovation Center of Multi-source
   Information Integration and Intelligent Processing; Guangxi Talent
   Highland Project of Big Data Intelligence and Application
FX This work was supported by the Project of Guangxi Science and Technology
   (No. 2022GXNSFDA035079 and GuiKeAD21075030), the National Natural
   Science Foundation of China (No. 61972167), the Guangxi "Bagui Scholar"
   Teams for Innovation and Research Project, the Guangxi Collaborative
   Innovation Center of Multi-source Information Integration and
   Intelligent Processing, and the Guangxi Talent Highland Project of Big
   Data Intelligence and Application.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Branagan DJ, 2006, MAT SCI ENG A-STRUCT, V428, P116, DOI 10.1016/j.msea.2006.04.089
   Chen HT, 2020, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR42600.2020.00154
   Dai KN, 2020, PROC CVPR IEEE, P6297, DOI 10.1109/CVPR42600.2020.00633
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Davies ER, 2011, PATTERN RECOGN LETT, V32, P866, DOI 10.1016/j.patrec.2011.01.007
   DeVries T, 2017, Arxiv, DOI [arXiv:1708.04552, DOI 10.48550/ARXIV.1708.04552]
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kortylewski Adam, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8937, DOI 10.1109/CVPR42600.2020.00896
   Kortylewski A, 2019, IEEE INT CONF COMP V, P2029, DOI 10.1109/ICCVW.2019.00253
   Kristan Matej, P ICCV
   Kwak S, 2011, IEEE I CONF COMP VIS, P1551, DOI 10.1109/ICCV.2011.6126414
   Lee KH, 2015, IEEE T MULTIMEDIA, V17, P1429, DOI 10.1109/TMM.2015.2455418
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lukezič A, 2018, Arxiv, DOI arXiv:1804.07056
   Lukezic Alan, 2018, PROC ACCV
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Molchanov P, 2016, PROC CVPR IEEE, P4207, DOI 10.1109/CVPR.2016.456
   Nebehay G, 2015, PROC CVPR IEEE, P2784, DOI 10.1109/CVPR.2015.7298895
   Pantrigo JJ, 2010, PATTERN RECOGN LETT, V31, P1577, DOI 10.1016/j.patrec.2010.04.017
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Tang SY, 2017, PROC CVPR IEEE, P3701, DOI 10.1109/CVPR.2017.394
   Tu FW, 2017, IEEE T IND INFORM, V13, P2251, DOI 10.1109/TII.2017.2700943
   Valmadre J, 2018, Arxiv, DOI arXiv:1803.09502
   Valmadre J, 2018, LECT NOTES COMPUT SC, V11207, P692, DOI 10.1007/978-3-030-01219-9_41
   Wang X, 2018, PROC CVPR IEEE, P4864, DOI 10.1109/CVPR.2018.00511
   Wang XL, 2018, Arxiv, DOI arXiv:1711.07752
   Yan B, 2019, IEEE I CONF COMP VIS, P2385, DOI 10.1109/ICCV.2019.00247
   Zhang TZ, 2014, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2014.164
   Zhang YH, 2018, Arxiv, DOI arXiv:1809.04320
   Zhu Z, 2018, Arxiv, DOI arXiv:1808.06048
NR 36
TC 1
Z9 1
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 89
DI 10.1145/3557896
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300014
DA 2024-07-18
ER

PT J
AU Jabeen, S
   Li, X
   Amin, MS
   Bourahla, O
   Li, SY
   Jabbar, A
AF Jabeen, Summaira
   Li, Xi
   Amin, Muhammad Shoib
   Bourahla, Omar
   Li, Songyuan
   Jabbar, Abdul
TI A Review on Methods and Applications in Multimodal Deep Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Review
DE Deep learning; multimedia; multimodal learning; datasets; neural
   networks; survey
ID IMAGE CAPTION GENERATION; SEMANTIC ATTENTION; EMOTION DETECTION;
   VISUAL-ATTENTION; REPRESENTATION; DATABASE; NETWORK; FUSION; MODELS;
   CORPUS
AB Deep Learning has implemented a wide range of applications and has become increasingly popular in recent years. The goal ofmultimodal deep learning (MMDL) is to createmodels that can process and link information using various modalities. Despite the extensive development made for unimodal learning, it still cannot cover all the aspects of human learning. Multimodal learning helps to understand and analyze better when various senses are engaged in the processing of information. This article focuses on multiple types of modalities, i.e., image, video, text, audio, body gestures, facial expressions, physiological signals, flow, RGB, pose, depth, mesh, and point cloud. Detailed analysis of the baseline approaches and an in-depth study of recent advancements during the past five years (2017 to 2021) in multimodal deep learning applications has been provided. A fine-grained taxonomy of various multimodal deep learning methods is proposed, elaborating on different applications in more depth. Last, main issues are highlighted separately for each domain, along with their possible future research directions.
C1 [Jabeen, Summaira; Li, Xi; Bourahla, Omar; Li, Songyuan; Jabbar, Abdul] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Li, Xi] Zhejiang Univ, Shanghai Inst Adv Study, Shanghai 201203, Peoples R China.
   [Li, Xi] Zhejiang Singapore Innovat & AI Joint Res Lab, Shanghai 201203, Peoples R China.
   [Li, Xi] Shanghai AI Lab, Shanghai 201203, Peoples R China.
   [Amin, Muhammad Shoib] East China Normal Univ, Sch Software Engn, 3663 North Zhongshan Rd, Shanghai, Peoples R China.
C3 Zhejiang University; Zhejiang University; Shanghai Artificial
   Intelligence Laboratory; East China Normal University
RP Li, X (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.; Li, X (corresponding author), Zhejiang Univ, Shanghai Inst Adv Study, Shanghai 201203, Peoples R China.; Li, X (corresponding author), Zhejiang Singapore Innovat & AI Joint Res Lab, Shanghai 201203, Peoples R China.; Li, X (corresponding author), Shanghai AI Lab, Shanghai 201203, Peoples R China.
EM 11821129@zju.edu.cn; xilizju@zju.edu.cn; 52184501030@stu.ecnu.edu.cn;
   bourahla@zju.edu.cn; leizungjyun@zju.edu.cn; Jabbar@zju.edu.cn
RI Zhang, Xiaoxi/KBP-8753-2024; wang, shuo/KCL-3379-2024; Jabbar,
   Abdul/JQW-3721-2023; xu, chen/JNE-5010-2023; chen,
   yanhong/JVE-0289-2024; chen, bin/KBQ-8114-2024; Jabbar,
   Abdul/JON-7556-2023; Wang, Yuchen/JPW-9345-2023; Li, Chun/KBC-9591-2024
OI chen, bin/0000-0002-3398-1314; Amin, Muhammad Shoib/0000-0001-9046-1502
FU Zhejiang Provincial Natural Science Foundation of China [LR19F020004];
   National Key Research and Development Program of China [2020AAA0107400];
   National Natural Science Foundation of China [U20A20222]; National
   Science Foundation for Distinguished Young Scholars [62225605]; Ant
   Group; CAAI-HUAWEI MindSpore Open Fund
FX This work is supported in part by Zhejiang Provincial Natural Science
   Foundation of China under Grant LR19F020004, National Key Research and
   Development Program of China under Grant 2020AAA0107400, National
   Natural Science Foundation of China under Grant U20A20222, National
   Science Foundation for Distinguished Young Scholars under Grant
   62225605, Ant Group, and sponsored by CAAI-HUAWEI MindSpore Open Fund.
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Abacha A.B., 2019, CLEF (Working Notes)
   Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2011, P 1 ACM INT C MULT R
   [Anonymous], 2011, Visual Analysis of Humans, DOI DOI 10.1007/978-0-85729-997-0_26
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Arik SÖ, 2017, ADV NEUR IN, V30
   Arik SÖ, 2018, ADV NEUR IN, V31
   Arik SO, 2017, PR MACH LEARN RES, V70
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   BAHRICK LE, 1983, INFANT BEHAV DEV, V6, P429, DOI 10.1016/S0163-6383(83)90241-2
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Basu K, 2020, LECT NOTES COMPUT SC, V12007, P57, DOI 10.1007/978-3-030-39197-3_4
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Bisk Y, 2020, Arxiv, DOI arXiv:2004.10151
   Boateng George, 2020, ICMI '20: Proceedings of the 2020 International Conference on Multimodal Interaction, P748, DOI 10.1145/3382507.3421154
   Bollacker K., 2008, P 2008 ACM SIGMOD IN, P1247, DOI 10.1145/1376616.1376746
   Busso C, 2017, IEEE T AFFECT COMPUT, V8, P67, DOI 10.1109/TAFFC.2016.2515617
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Cao PF, 2019, NEURAL PROCESS LETT, V50, P103, DOI 10.1007/s11063-018-09973-5
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen J, 2021, NEURAL COMPUT APPL, V33, P8669, DOI 10.1007/s00521-020-05616-w
   Chen L, 2021, PROC CVPR IEEE, P16841, DOI 10.1109/CVPR46437.2021.01657
   Chen MH, 2017, AAAI CONF ARTIF INTE, P3981
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chen YY, 2018, LECT NOTES COMPUT SC, V11217, P367, DOI 10.1007/978-3-030-01261-8_22
   Cheng L, 2020, IEEE ACCESS, V8, P154953, DOI 10.1109/ACCESS.2020.3018752
   Chong LY, 2019, 5TH INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING AND COMMUNICATIONS (BIGCOM 2019), P213, DOI 10.1109/BIGCOM.2019.00037
   Cimtay Y, 2020, IEEE ACCESS, V8, P168865, DOI 10.1109/ACCESS.2020.3023871
   Cukurova M, 2020, BRIT J EDUC TECHNOL, V51, P1441, DOI 10.1111/bjet.13015
   Desta MT, 2018, IEEE WINT CONF APPL, P1814, DOI 10.1109/WACV.2018.00201
   Diwakar Parul, 2021, P INT C INNOVATIVE C
   Dang-Nguyen DT, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3103613
   Nguyen D, 2018, COMPUT VIS IMAGE UND, V174, P33, DOI 10.1016/j.cviu.2018.06.005
   Elias I, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P5709, DOI 10.1109/ICASSP39728.2021.9414718
   Fang ZY, 2020, Arxiv, DOI arXiv:2003.05162
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Gao J, 2020, NEURAL COMPUT, V32, P829, DOI 10.1162/neco_a_01273
   Gao RH, 2019, PROC CVPR IEEE, P324, DOI 10.1109/CVPR.2019.00041
   Gao Y, 2017, ACM T INTEL SYST TEC, V8, DOI 10.1145/2967502
   Garcia NC, 2021, IEEE WINT CONF APPL, P2754, DOI 10.1109/WACV48630.2021.00280
   Gunes H, 2006, INT C PATT RECOG, P1148
   Guo LT, 2019, PROC CVPR IEEE, P4199, DOI 10.1109/CVPR.2019.00433
   Guo WY, 2021, IEEE T IMAGE PROCESS, V30, P6730, DOI 10.1109/TIP.2021.3097180
   Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887
   Gurari D, 2018, PROC CVPR IEEE, P3608, DOI 10.1109/CVPR.2018.00380
   Hazarika D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2594
   Hazarika D, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P196, DOI 10.1109/MIPR.2018.00043
   He XW, 2019, PATTERN RECOGN LETT, V119, P229, DOI 10.1016/j.patrec.2017.10.018
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   HOFFMANPLOTKIN D, 1984, CHILD DEV, V55, P794, DOI 10.2307/1130130
   Hong DF, 2021, IEEE T GEOSCI REMOTE, V59, P4340, DOI 10.1109/TGRS.2020.3016820
   Huan RH, 2021, MULTIMED TOOLS APPL, V80, P8213, DOI 10.1007/s11042-020-10030-4
   Huang SN, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/6323942
   Huang Y, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409332
   Huang YR, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/2107451
   Park DH, 2017, Arxiv, DOI arXiv:1612.04757
   Ito K., 2017, The LJ Speech Dataset
   Jaiswal M, 2019, ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P174, DOI 10.1145/3340555.3353731
   Jaiswal M, 2019, INT CONF ACOUST SPEE, P7415, DOI 10.1109/ICASSP.2019.8682793
   Ji JY, 2021, AAAI CONF ARTIF INTE, V35, P1655
   Jiang WT, 2021, IEEE ACCESS, V9, P69700, DOI 10.1109/ACCESS.2021.3067607
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Jiaqi Hao, 2021, SA '21 Technical Communications: SIGGRAPH Asia 2021 Technical Communications, DOI 10.1145/3478512.3488610
   Jing LL, 2021, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR46437.2021.00316
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Jonghwan Mun, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6581, DOI 10.1109/CVPR.2019.00675
   JUANG BH, 1991, TECHNOMETRICS, V33, P251, DOI 10.2307/1268779
   Kafle K, 2017, IEEE I CONF COMP VIS, P1983, DOI 10.1109/ICCV.2017.217
   Kazakos E, 2019, IEEE I CONF COMP VIS, P5491, DOI 10.1109/ICCV.2019.00559
   Koutras P., 2018, 2018 IEEE 13th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP), P1, DOI [10.1109/IVMSPW.2018.8448977, DOI 10.1109/IVMSPW.2018.8448977]
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lai HL, 2020, IEEE ACCESS, V8, P119516, DOI 10.1109/ACCESS.2020.3005664
   LAZARUS AA, 1973, J NERV MENT DIS, V156, P404, DOI 10.1097/00005053-197306000-00005
   Lei Z, 2021, FUTURE INTERNET, V13, DOI 10.3390/fi13020055
   Li LJ, 2019, IEEE WINT CONF APPL, P339, DOI 10.1109/WACV.2019.00042
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li MJ, 2020, IEEE ACCESS, V8, P187208, DOI 10.1109/ACCESS.2020.3029288
   Li XR, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P271, DOI 10.1145/2911996.2912049
   Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d
   Liu MF, 2022, IEEE T CYBERNETICS, V52, P1247, DOI 10.1109/TCYB.2020.2997034
   Liu MF, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.102178
   Liu S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1425, DOI 10.1145/3240508.3240667
   Lobry S, 2020, IEEE T GEOSCI REMOTE, V58, P8555, DOI 10.1109/TGRS.2020.2988782
   Long Y, 2021, ELECTRON LETT, V57, P758, DOI 10.1049/ell2.12255
   Malinowski M, 2014, ADV NEUR IN, V27
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   Martin O., 2006, 22 INT C DATA ENG WO, P8, DOI [DOI 10.1109/ICDEW.2006.145, 10.1109/ICDEW.2006.145]
   Mathews A, 2016, AAAI CONF ARTIF INTE, P3574
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   McKeown G, 2012, IEEE T AFFECT COMPUT, V3, P5, DOI 10.1109/T-AFFC.2011.20
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Mittal T, 2020, AAAI CONF ARTIF INTE, V34, P1359
   Mogadala Aditya, 2021, Journal of Artificial Intelligence Research, P1183
   Morency Louis-Philippe, 2020, MULT MACH LEARN DEEP
   MULLIGAN RM, 1980, PERCEPT PSYCHOPHYS, V28, P471, DOI 10.3758/BF03204892
   Narasimhan M, 2018, LECT NOTES COMPUT SC, V11212, P460, DOI 10.1007/978-3-030-01237-3_28
   Nguyen D, 2017, IEEE WINT CONF APPL, P1215, DOI 10.1109/WACV.2017.140
   Ordonez V., 2011, Adv. Neural Inf. Process. Syst., V24
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Patro BN, 2020, IEEE WINT CONF APPL, P1566, DOI [10.1109/WACV45572.2020.9093295, 10.1109/wacv45572.2020.9093295]
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Perez-Martin J, 2021, IEEE WINT CONF APPL, P3038, DOI 10.1109/WACV48630.2021.00308
   Petajan E, 1996, MULTIMEDIA COMMUNICATIONS AND VIDEO CODING, P265
   Ping W, 2018, Arxiv, DOI arXiv:1710.07654
   Rahman MM, 2021, Arxiv, DOI arXiv:2009.07335
   Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Ren MY, 2015, ADV NEUR IN, V28
   Ringeval F, 2013, IEEE INT CONF AUTOMA
   Rohrbach A, 2014, LECT NOTES COMPUT SC, V8753, P184, DOI 10.1007/978-3-319-11752-2_15
   Sengupta S, 2020, KNOWL-BASED SYST, V194, DOI 10.1016/j.knosys.2020.105596
   Shen J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4779, DOI 10.1109/ICASSP.2018.8461368
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Snoek CGM, 2005, MULTIMED TOOLS APPL, V25, P5, DOI 10.1023/B:MTAP.0000046380.27575.a5
   Souza R, 2021, J PARALLEL DISTR COM, V148, P31, DOI 10.1016/j.jpdc.2020.10.001
   Taigman Y, 2018, Arxiv, DOI arXiv:1707.06588
   Tandon N, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P523, DOI 10.1145/2556195.2556245
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Torabi A, 2015, Arxiv, DOI arXiv:1503.01070
   Tripathi S, 2019, Arxiv, DOI arXiv:1804.05788
   Tur G, 2008, 2008 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY: SLT 2008, PROCEEDINGS, P69, DOI 10.1109/SLT.2008.4777842
   van den Oord A, 2018, PR MACH LEARN RES, V80, DOI arXiv:1711.10433
   Veaux C., 2016, CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit
   Waibel A, 2009, HUM-COMPUT INT-SPRIN, P3, DOI 10.1007/978-1-84882-054-8_1
   Wan CH, 2019, INT CONF ACOUST SPEE, P496, DOI [10.1109/ICASSP.2019.8682383, 10.1109/icassp.2019.8682383]
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang B, 2020, IEEE ACCESS, V8, P104543, DOI 10.1109/ACCESS.2020.2999568
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Wang P, 2017, PROC CVPR IEEE, P3909, DOI 10.1109/CVPR.2017.416
   Wang W, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3081, DOI 10.1109/ICASSP.2018.8461507
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Wang X, 2021, INFORM SCIENCES, V546, P298, DOI 10.1016/j.ins.2020.08.009
   Wang YX, 2017, Arxiv, DOI arXiv:1703.10135
   Wei R, 2020, J VIS COMMUN IMAGE R, V67, DOI 10.1016/j.jvcir.2020.102751
   Wei YW, 2020, NEUROCOMPUTING, V387, P91, DOI 10.1016/j.neucom.2019.12.073
   Wu HB, 2022, IEEE T CIRC SYST VID, V32, P1250, DOI [10.1109/TAI.2021.3092698, 10.1109/TCSVT.2021.3077512]
   Wu J, 2017, ELECTRON LETT, V53, P1642, DOI 10.1049/el.2017.3159
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Xi YL, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115648
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Yang ZG, 2020, IEEE T PATTERN ANAL, V42, P1243, DOI 10.1109/TPAMI.2019.2893953
   Yazdavar AH, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0226248
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yu J, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107563
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Yudistira N, 2020, SIGNAL PROCESS-IMAGE, V82, DOI 10.1016/j.image.2019.115731
   YUHAS BP, 1989, IEEE COMMUN MAG, V27, P65, DOI 10.1109/35.41402
   Zhang C, 2020, IEEE J-STSP, V14, P478, DOI 10.1109/JSTSP.2020.2987728
   Zhang S.-F., 2019, INT C MACH LEARN CYB, P1
   Zhang W, 2020, IEEE T PATTERN ANAL, V42, P3088, DOI 10.1109/TPAMI.2019.2920899
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
   Zhu JJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3332374
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 172
TC 18
Z9 18
U1 55
U2 106
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 76
DI 10.1145/3545572
PG 41
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Dong, SS
   Niu, TZ
   Luo, X
   Liu, W
   Xu, XS
AF Dong, Shanshan
   Niu, Tianzi
   Luo, Xin
   Liu, Wu
   Xu, Xinshun
TI Semantic Embedding Guided Attention with Explicit Visual Feature Fusion
   for Video Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video captioning; semantic embedding guided attention; explicit visual
   feature fusion
AB Video captioning, which bridges vision and language, is a fundamental yet challenging task in computer vision. To generate accurate and comprehensive sentences, both visual and semantic information is quite important. However, most existing methods simply concatenate different types of features and ignore the interactions between them. In addition, there is a large semantic gap between visual feature space and semantic embedding space, making the task very challenging. To address these issues, we propose a framework named semantic embedding guided attention with Explicit visual Feature Fusion for vidEo CapTioning, EFFECT for short, in which we design an explicit visual-feature fusion (EVF) scheme to capture the pairwise interactions between multiple visual modalities and fuse multimodal visual features of videos in an explicit way. Furthermore, we propose a novel attention mechanism called semantic embedding guided attention (SEGA), which cooperates with the temporal attention to generate a joint attention map. Specifically, in SEGA, the semantic word embedding information is leveraged to guide the model to pay more attention to the most correlated visual features at each decoding stage. In this way, the semantic gap between visual and semantic space is alleviated to some extent. To evaluate the proposed model, we conduct extensive experiments on two widely used datasets, i.e., MSVD and MSR-VTT. The experimental results demonstrate that our approach achieves state-of-the-art results in terms of four evaluation metrics.
C1 [Dong, Shanshan; Niu, Tianzi; Luo, Xin; Xu, Xinshun] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Liu, Wu] JD AI Res, Beijing, Peoples R China.
C3 Shandong University
RP Xu, XS (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
EM 201914779@mail.sdu.edu.cn; 201920560@mail.sdu.edu.cn; luoxin@sdu.edu.cn;
   liuwu@live.cn; xuxinshun@sdu.edu.cn
RI Luo, Xin/HNR-3191-2023
OI Luo, Xin/0000-0002-6901-5476
FU National Natural Science Foundation of China [62172256, 61872428];
   Shandong Province Key Research and Development Program [2019JZZY010127];
   Natural Science Foundation of Shandong Province [ZR2019ZD06]; Major
   Program of the National Natural Science Foundation of China [61991411]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62172256 and 61872428, in part by
   Shandong Province Key Research and Development Program under Grant
   2019JZZY010127, in part by Natural Science Foundation of Shandong
   Province under Grant ZR2019ZD06 and in part by the Major Program of the
   National Natural Science Foundation of China under Grant 61991411.
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Behnke M, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2664
   Bennett CL, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173650
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1523, DOI 10.1109/ICCV48922.2021.00157
   Chen SX, 2021, PROC CVPR IEEE, P8421, DOI 10.1109/CVPR46437.2021.00832
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chen YY, 2018, LECT NOTES COMPUT SC, V11217, P367, DOI 10.1007/978-3-030-01261-8_22
   Deng CR, 2021, PROC CVPR IEEE, P234, DOI 10.1109/CVPR46437.2021.00030
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082
   Gupta S, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3436494
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Hou JY, 2019, IEEE I CONF COMP VIS, P8917, DOI 10.1109/ICCV.2019.00901
   Hu YS, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P774, DOI 10.1145/3343031.3351072
   Hussein F, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3063532
   Jin T, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2001
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li XL, 2017, IEEE T IMAGE PROCESS, V26, P3652, DOI 10.1109/TIP.2017.2695887
   Lin Chin-Yew, 2004, P 42 ANN M AOC COMP, P605, DOI [DOI 10.3115/.1218955.1219032, DOI 10.3115/1218955.1219032]
   Liu S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1425, DOI 10.1145/3240508.3240667
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Song JK, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2737
   Song YQ, 2021, PROC CVPR IEEE, P11240, DOI 10.1109/CVPR46437.2021.01109
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan GC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P745
   Tang PJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303083
   Tu YB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1014, DOI 10.1145/3123266.3123354
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2016, EMNLP, P1961
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Venugopalan Subhashini, 2014, P 2015 C N AM CHAPT
   Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang HY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1519, DOI 10.1145/3240508.3240677
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wu J, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3271485
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zha ZJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3320061
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zhang XS, 2017, PROC CVPR IEEE, P6250, DOI 10.1109/CVPR.2017.662
   Zhao B, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2916757
   Zheng Q., 2020, 2020 CVPR, P13093, DOI 10.1109/CVPR42600.2020.01311
   Zhou BL, 2015, Arxiv, DOI arXiv:1512.02167
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 68
TC 2
Z9 2
U1 5
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 68
DI 10.1145/3550276
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000018
DA 2024-07-18
ER

PT J
AU Wang, J
   Ke, JC
   Shuai, HH
   Li, YH
   Cheng, WH
AF Wang, Jia
   Ke, Jingcheng
   Shuai, Hong-Han
   Li, Yung-Hui
   Cheng, Wen-Huang
TI Referring Expression Comprehension Via Enhanced Cross-modal Graph
   Attention Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Referring expression comprehension; object detection; Enhanced
   Cross-modal Graph Attention Networks; Graph Node Erase
AB Referring expression comprehension aims to localize a specific object in an image according to a given language description. It is still challenging to comprehend and mitigate the gap between various types of information in the visual and textual domains. Generally, it needs to extract the salient features from a given expression and match the features of expression to an image. One challenge in referring expression comprehension is the number of region proposals generated by object detection methods is far more than the number of entities in the corresponding language description. Remarkably, the candidate regions without described by the expression will bring a severe impact on referring expression comprehension. To tackle this problem, we first propose a novel Enhanced Cross-modal Graph Attention Networks (ECMGANs) that boosts the matching between the expression and the entity position of an image. Then, an effective strategy named Graph Node Erase (GNE) is proposed to assist ECMGANs in eliminating the effect of irrelevant objects on the target object. Experiments on three public referring expression comprehension datasets show unambiguously that our ECMGANs framework achieves better performance than other state-of-the-art methods. Moreover, GNE is able to obtain higher accuracies of visual-expression matching effectively.
C1 [Wang, Jia; Shuai, Hong-Han; Cheng, Wen-Huang] Natl Yang Ming Chiao Tung Univ, 1001 Univ Rd, Hsinchu 30010, Taiwan.
   [Ke, Jingcheng] Natl Tsing Hua Univ, 101,Sect 2,Kuang Fu Rd, Hsinchu 30010, Taiwan.
   [Li, Yung-Hui] Hon Hai Res Inst, 2 Ziyou St,Tucheng Dist 32 Jihu Rd, Hsinchu 30013, Taiwan.
C3 National Yang Ming Chiao Tung University; National Tsing Hua University
RP Cheng, WH (corresponding author), Natl Yang Ming Chiao Tung Univ, 1001 Univ Rd, Hsinchu 30010, Taiwan.
EM vicky.ee08@nycu.edu.tw; freedom6927@gmail.com; hhshuai@nycu.edu.tw;
   yunghui.li@foxconn.com; whcheng@nycu.edu.tw
OI Shuai, Hong-Han/0000-0003-2216-077X; Li, Yung-Hui/0000-0002-0475-3689;
   Cheng, Wen-Huang/0000-0002-4662-7875
FU Ministry of Science and Technology of Taiwan
   [MOST-109-2223-E-009-002-MY3, MOST-110-2218-E-A49-018,
   MOST-111-2634-F-007-002]
FX This work was supported in part by Ministry of Science and Technology of
   Taiwan under the Grants No. MOST-109-2223-E-009-002-MY3, No.
   MOST-110-2218-E-A49-018, and No. MOST-111-2634-F-007-002.
CR Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Berhich A, 2020, 3RD INTERNATIONAL CONFERENCE ON NETWORKING, INFORMATION SYSTEM & SECURITY (NISS'20), DOI 10.1145/3386723.3387865
   Chen C., 2021, P IEEE C COMPUTER VI, P12576, DOI DOI 10.1109/CVPR46437.2021.01239
   Chen L, 2021, AAAI CONF ARTIF INTE, V35, P1036
   Cheng WH, 2021, ACM COMPUT SURV, V54, DOI [10.1145/3447239, 10.1145/3552468.3554360]
   Cui Yuhao, 2021, ROSITA: Enhancing Vision-and-Language Semantic Alignments via Crossand Intra-Modal Knowledge Integration, P797, DOI [10.1145/3474085.3475251, DOI 10.1145/3474085.3475251]
   Dailan He, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P2344, DOI 10.1145/3474085.3475397
   Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808
   Deng J., 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision, P1769
   Fan HH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3390891
   Fazal Muhammad Abu Ul, 2020, ACM T MULTIM COMPUT, V16
   Gao P, 2018, Arxiv, DOI arXiv:1808.02632
   Gen Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10031, DOI 10.1109/CVPR42600.2020.01005
   Guzman AL, 2020, NEW MEDIA SOC, V22, P70, DOI 10.1177/1461444819858691
   Han GX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3243, DOI 10.1109/ICCV48922.2021.00325
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou QB, 2018, ADV NEUR IN, V31
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Hua KL, 2018, IEEE T CYBERNETICS, V48, P423, DOI 10.1109/TCYB.2016.2640288
   Huang FR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3388861
   Islam Md Zabirul, 2020, Inform Med Unlocked, V20, P100412, DOI 10.1016/j.imu.2020.100412
   Jing CC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4041, DOI 10.1145/3394171.3413902
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Laitala J, 2020, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING (SAC'20), P1104, DOI [10.1145/3341105.3373945, 10.1445/3341405.3373945]
   Li JS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3418217
   Li L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3359753
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li QZ, 2022, NEUROCOMPUTING, V467, P99, DOI 10.1016/j.neucom.2021.09.066
   Li X., 2020, P EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58577-8_8
   Liang S., 2020, IEEE 22nd Int Conf High Perform Comput Communications; IEEE 18th Int Conf Smart City; IEEE 6th Int Conf Data Sci Syst (HPCC/SmartCity/DSS), VYanuca Island, Cuvu, Fiji, P1316, DOI [10.1109/HPCC-SmartCity-DSS50907.2020.00169, DOI 10.1109/HPCC-SMARTCITY-DSS50907.2020.00169]
   Liao Yue, 2020, P IEEECVF C COMPUTER
   Ling Lo, 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), DOI 10.1109/ICME51207.2021.9428120
   Liu JY, 2017, IEEE I CONF COMP VIS, P4866, DOI 10.1109/ICCV.2017.520
   Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205
   Liu ZD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356728
   Livieris IE, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10030287
   Lu JS, 2019, ADV NEUR IN, V32
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Luo XF, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3381086
   Ma Y, 2021, ARXIV
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Ordonez Vicente, 2011, Adv. Neural Info. Process. Syst., V24
   Ren J, 2017, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2017.87
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Sanchez-Riera J, 2016, PATTERN RECOGN LETT, V73, P1, DOI 10.1016/j.patrec.2015.12.006
   Scaiella Antonio, 2019, Italian Journal of Computational Linguistics, V2, P49
   Sherstinsky A, 2020, PHYSICA D, V404, DOI 10.1016/j.physd.2019.132306
   Sio CH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1948, DOI 10.1145/3394171.3413611
   Song SJ, 2021, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR46437.2021.00140
   Su YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3387920
   Suo W., 2021, P 30 INT JOINT C ART, P1032, DOI [10.24963/ijcai.2021/143, DOI 10.24963/IJCAI.2021/143]
   Tsai TH, 2014, IEEE T IMAGE PROCESS, V23, P1047, DOI 10.1109/TIP.2014.2298982
   Wang HC, 2018, IEEE T CIRC SYST VID, V28, P3127, DOI 10.1109/TCSVT.2017.2733623
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Wu A., 2021, P IEEE CVF INT C COM, P9567
   Xia ZX, 2021, IEEE IND ELECTRON M, V15, P6, DOI 10.1109/MIE.2020.2970790
   Xie HX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2871, DOI 10.1145/3394171.3414012
   Xin Q, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460522
   Xu CY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3450410
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3458281
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Yang Sibei, 2020, P IEEECVF C COMPUTER
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhuang BH, 2018, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR.2018.00447
NR 78
TC 0
Z9 0
U1 5
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 65
DI 10.1145/3548688
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000015
DA 2024-07-18
ER

PT J
AU Liu, CM
   Ma, XJ
   Cao, SX
   Fu, JY
   Zhu, B
AF Liu, Changming
   Ma, Xiaojing
   Cao, Sixing
   Fu, Jiayun
   Zhu, Bin B.
TI Privacy-preserving Motion Detection for HEVC-compressed Surveillance
   Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Privacy-preserving motion detection; motion detection; privacy
   protection; video encryption; surveillance videos; HEVC
ID PROTECTION
AB In the cloud era, a large amount of data is uploaded to and processed by public clouds. The risk of privacy leakage has become a major concern for cloud users. Cloud-based video surveillance requires motion detection, which may reveal the privacy of people in a surveillance video. Privacy-preserving video surveillance allows motion detection while protecting privacy. The existing scheme [25], designed to detect motion on encrypted and H.264-compressed surveillance videos, does not work well on more advanced video compression schemes such as HEVC.
   In this article, we propose the first motion detection method on encrypted and HEVC-compressed videos. It adopts a novel approach that exploits inter-prediction reference relationships among coding blocks to detect motion regions. The partition pattern and the number of coding bits of each detection block used in prior art are also used to help detect motion regions. Spatial and temporal consistency of a moving object and Kalman filtering are applied to segment connected/merged motion regions, remove noise and background motions, and refine trajectories and shapes of detected moving objects. Experimental results indicate that our detection method achieves high detection recall, precision, and F1-score for surveillance videos of both high and low resolutions with various scenes. It has a similarly high detection accuracy on encrypted and HEVC-compressed videos as that of the existing motion detection method [25] on encrypted and H.264-compressed videos. Our proposed method incurs no bit-rate overhead and has a very low computational complexity for both motion detection and encryption of HEVC videos.
C1 [Liu, Changming; Ma, Xiaojing; Cao, Sixing; Fu, Jiayun] Huazhong Univ Sci & Technol, 1037 LuoYu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Zhu, Bin B.] Microsoft Res Asia, 5 Danling St, Beijing 100080, Peoples R China.
C3 Huazhong University of Science & Technology; Microsoft; Microsoft
   Research Asia
RP Zhu, B (corresponding author), Microsoft Res Asia, 5 Danling St, Beijing 100080, Peoples R China.
EM liuchangming@alumni.hust.edu.cn; lindahust@hust.edu.cn;
   sixing@alumni.hust.edu.cn; d202080514@hust.edu.cn; binzhu@microsoft.com
RI Liu, Changming/HGB-9726-2022; MA, XIAO/HHN-5611-2022; Yang,
   Bo/JTS-4309-2023; zheng, yi/JOZ-7204-2023; wang, qiang/IZW-1751-2023
OI Liu, Changming/0009-0006-6398-9460
FU National Natural Science Foundation of China [61771211]; Fundamental
   Research Funds for the Central Universities [2017KFYXJJ064]; Key-Area
   Research and Development Program of Guangdong Province [2019B010139001];
   Wuhan Applied Foundational Frontier Project [2020010601012188]
FX This work was supported in part by National Natural Science Foundation
   of China (61771211), Fundamental Research Funds for the Central
   Universities (2017KFYXJJ064), Key-Area Research and Development Program
   of Guangdong Province (2019B010139001), and Wuhan Applied Foundational
   Frontier Project (2020010601012188).
CR Barni M, 2015, IEEE SIGNAL PROC MAG, V32, P66, DOI 10.1109/MSP.2015.2438131
   Barra S, 2014, LECT NOTES COMPUT SC, V8897, P28, DOI 10.1007/978-3-319-13386-7_3
   CAVIAR Dataset, EC FUNDED CAVIAR PRO
   Chen YC, 2014, IEEE ICC, P793, DOI 10.1109/ICC.2014.6883416
   Chu K.-Y., 2013, Proceedings of the 21st ACM international conference on Multimedia, P597
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   Guo JT, 2020, IEEE T CIRC SYST VID, V30, P3268, DOI 10.1109/TCSVT.2019.2929855
   Guo JT, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3131342
   Huilai Li, 2014, 2014 17th International Symposium on Electromagnetic Launch Technology (EML), P1, DOI 10.1109/EML.2014.6920169
   Jodoin P.-M., 2012, IEEE CHANG DET WORKS
   Joint Collaborative Team on Video Coding (JCT-VC), 2013, HIGH EFFICIENCY VIDE
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kyushu University, 2008, LIMU DAT
   Laumer Marcus, 2013, International Conference on Computer Vision Theory and Applications (VISAPP 2013). Proceedings, P219
   Laumer M, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P282, DOI 10.1109/PCS.2015.7170091
   Lin CY, 2017, MULTIMED TOOLS APPL, V76, P9759, DOI 10.1007/s11042-016-3578-9
   Ma X., 2018, P IEEE C COMP COMM W
   Ma XJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P905, DOI 10.1145/3240508.3240672
   Ma XJ, 2018, IEEE IMAGE PROC, P4133, DOI 10.1109/ICIP.2018.8451279
   Ma XJ, 2016, IEEE T EMERG TOP COM, V4, P349, DOI 10.1109/TETC.2015.2460462
   Oh SM, 2011, PROC CVPR IEEE
   Poppe C, 2009, J VIS COMMUN IMAGE R, V20, P428, DOI 10.1016/j.jvcir.2009.05.001
   Saini M, 2014, MULTIMED TOOLS APPL, V68, P135, DOI 10.1007/s11042-012-1207-9
   Shahid Z, 2014, IEEE T MULTIMEDIA, V16, P24, DOI 10.1109/TMM.2013.2281029
   Shao ZF, 2018, IEEE T BIG DATA, V4, P105, DOI 10.1109/TBDATA.2017.2715815
   Sharma S, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P2291, DOI 10.1145/3243734.3278511
   Sohn H, 2010, LECT NOTES COMPUT SC, V6297, P622, DOI 10.1007/978-3-642-15702-8_57
   SORENSON HW, 1970, IEEE SPECTRUM, V7, P63, DOI 10.1109/MSPEC.1970.5213471
   Szczerba K, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P478, DOI 10.1109/AVSS.2009.78
   Sze V., 2014, HIGH EFFICIENCY VIDE, P113
   Tang YY, 2019, INT CONF ACOUST SPEE, P675, DOI 10.1109/ICASSP.2019.8682780
   Upmanyu M, 2009, IEEE I CONF COMP VIS, P1639, DOI 10.1109/ICCV.2009.5459370
   Wang Q, 2015, LECT NOTES COMPUT SC, V9327, P186, DOI 10.1007/978-3-319-24177-7_10
   Yu S, 2016, IEEE ACCESS, V4, P2751, DOI 10.1109/ACCESS.2016.2577036
   Zhang Mabel M., 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P10, DOI 10.1109/CVPRW.2015.7301291
   Zhang YH, 2017, IEEE DATA COMPR CONF, P475, DOI 10.1109/DCC.2017.32
   Zhao L, 2018, IEEE T CIRC SYST VID, V28, P1346, DOI 10.1109/TCSVT.2016.2645616
NR 37
TC 0
Z9 0
U1 5
U2 36
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 23
DI 10.1145/3472669
PG 27
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900023
DA 2024-07-18
ER

PT J
AU Lin, YS
   Liu, ZY
   Chen, YA
   Wang, YS
   Chang, YL
   Hsu, WH
AF Lin, Yu-Sheng
   Liu, Zhe-Yu
   Chen, Yu-An
   Wang, Yu-Siang
   Chang, Ya-Liang
   Hsu, Winston H.
TI xCos: An Explainable Cosine Metric for Face Verification Task
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE XAI; xCos; face verification; face recognition; explainable AI;
   explainable artificial intelligence
ID RECOGNITION
AB We study the XAI (explainable AI) on the face recognition task, particularly the face verification. Face verification has become a crucial task in recent days and it has been deployed to plenty of applications, such as access control, surveillance, and automatic personal log-on for mobile devices. With the increasing amount of data, deep convolutional neural networks can achieve very high accuracy for the face verification task. Beyond exceptional performances, deep face verification models need more interpretability so that we can trust the results they generate. In this article, we propose a novel similarity metric, called explainable cosine (xCos), that comes with a learnable module that can be plugged into most of the verification models to provide meaningful explanations. With the help of xCos, we can see which parts of the two input faces are similar, where the model pays its attention to, and how the local similarities are weighted to form the output xCos score. We demonstrate the effectiveness of our proposed method on LFW and various competitive benchmarks, not only resulting in providing novel and desirable model interpretability for face verification but also ensuring the accuracy as plugging into existing face recognition models.
C1 [Lin, Yu-Sheng; Liu, Zhe-Yu; Chen, Yu-An; Chang, Ya-Liang; Hsu, Winston H.] Natl Taiwan Univ, RM 512,CSIE Bldg 1,Sec 4,Roosevelt Rd, Taipei, Taiwan.
   [Wang, Yu-Siang] Univ Toronto, 27 Kings Coll, Toronto, ON M5S 1A1, Canada.
C3 National Taiwan University; University of Toronto
RP Lin, YS (corresponding author), Natl Taiwan Univ, RM 512,CSIE Bldg 1,Sec 4,Roosevelt Rd, Taipei, Taiwan.
EM biolin@cmlab.csie.ntu.edu.tw; zhe2325138@cmlab.csie.ntu.edu.tw;
   r07922076@cmlab.csie.ntu.edu.tw; yswang@cs.toronto.edu;
   yaliangchang@cmlab.csie.ntu.edu.tw; whsu@ntu.edu.tw
FU Ministry of Science and Technology, Taiwan [MOST 110-2634-F-002-026];
   Qualcomm Technologies, Inc.
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan, under Grant MOST 110-2634-F-002-026 and Qualcomm
   Technologies, Inc. We benefit from NVIDIA DGX-1 AI Supercomputer and are
   grateful to the National Center for High-performance Computing.
CR [Anonymous], 2015, BRIT MACH VIS C
   [Anonymous], 2014, ARXIV14117923
   [Anonymous], 2015, CoRR
   Bartlett Peter L., 2012, Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada
   Benavente R, 1998, 24 COMP VIS CTR
   Brendel Wieland, 2019, ARXIV190400760
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Castañón G, 2018, IEEE INT CONF AUTOMA, P16, DOI 10.1109/FG.2018.00013
   Chang YL, 2019, IEEE I CONF COMP VIS, P9065, DOI 10.1109/ICCV.2019.00916
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Chen RJ, 2019, IEEE I CONF COMP VIS, P9186, DOI 10.1109/ICCV.2019.00928
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2017, IEEE COMPUT SOC CONF, P2006, DOI 10.1109/CVPRW.2017.251
   Gu DH, 2020, IEEE T MULTIMEDIA, V22, P1720, DOI 10.1109/TMM.2020.2971170
   Gunning D., 2017, Explainable artificial intelligence (xai)
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hind M, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P123, DOI 10.1145/3306618.3314273
   Hinton G., 2015, COMPUT SCI, V2
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Kan M, 2014, PROC CVPR IEEE, P1883, DOI 10.1109/CVPR.2014.243
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu X, 2018, INT CONF DAT MIN WOR, P905, DOI 10.1109/ICDMW.2018.00132
   Lu C, 2015, AAAI CONF ARTIF INTE, P3811
   Meng LX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3146, DOI 10.1145/3394171.3413499
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sengupta S, 2016, IEEE WINT CONF APPL
   Sharma P, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901319
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Williford Jonathan R., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P248, DOI 10.1007/978-3-030-58621-8_15
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Yin Bangjie, 2019, P INT C COMP VIS ICC
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang N., 2007, Tech. Rep. 07-49, P7
   Zheng Tianyue, 2017, CROSS AGE LFW DATABA
   Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21
NR 44
TC 11
Z9 11
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 112
DI 10.1145/3469288
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Krishnan, P
   Jain, K
   Jose, PG
   Achuthan, K
   Buyya, R
AF Krishnan, Prabhakar
   Jain, Kurunandan
   Jose, Pramod George
   Achuthan, Krishnashree
   Buyya, Rajkumar
TI SDN Enabled QoE and Security Framework for Multimedia Applications in 5G
   Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE SDN; NFV; 5th Generation network (5G); Multi-Access Edge Computing
   (MEC); lightweight cryptography; QoE; network slicing; multimedia
   communication; network security
ID EFFICIENT; CIPHER
AB The technologies for real-time multimedia transmission and immersive 3D gaming applications are rapidly emerging, posing challenges in terms of performance, security, authentication, data privacy, and encoding. The communication channel for these multimedia applications must be secure and reliable from network attack vectors and data-contents must employ strong encryption to preserve privacy and confidentiality. Towards delivering secure multimedia application environment for 5G networks, we propose an SDN/NFV (Software-Defined-Networking/Network-Function-Virtualization) framework called STREK, which attempts to deliver highly adaptable Quality-of-Experience (QoE), Security, and Authentication functions for multidomain Cloud to Edge networks. The STREK architecture consists of a holistic SDNFV dataplane, NFV service-chaining and network slicing, a lightweight adaptable hybrid cipher scheme called TREK, and an open RESTful API for applications to deploy custom policies at runtime for multimedia services. For multi-domain/small-cell deployments, the key-generation scheme is dynamic at flow/session-level, and the handover authentication scheme uses a novel method to exchange security credentials with the Access Points (APs) of neighborhood cells. This scheme is designed to improve authentication function during handover with low overhead, delivering the 5G ultra-low latency requirements. We present the experiments with both software and hardware-based implementations and compare our solution with popular lightweight cryptographic solutions, standard open source software, and SDN-based research proposals for 5G multimedia. In the microbenchmarks, STREK achieves smaller hardware, low overhead, low computation, higher attack resistance, and offers better network performance for multimedia streaming applications. In real-time multimedia use-cases, STREK shows greater level of quality distortion for multimedia contents with minimal encryption bitrate overhead to deliver data confidentiality, immunity to common cryptanalysis, and significant resistance to communication channel attacks, in the context of low-latency 5G networks.
C1 [Krishnan, Prabhakar; Jain, Kurunandan; Jose, Pramod George; Achuthan, Krishnashree] Amrita Vishwa Vidyapeetham, Ctr Cyber Secur Syst & Networks, Clappana PO, Amritapuri 690525, Kerala, India.
   [Buyya, Rajkumar] Univ Melbourne, Sch Comp & Informat Syst, Cloud Comp & Distributed Syst Clouds, Doug McDonell Bldg,Parkville Campus Melbourne, Parkville, Vic 3010, Australia.
C3 Amrita Vishwa Vidyapeetham; Amrita Vishwa Vidyapeetham Amritapuri;
   University of Melbourne
RP Krishnan, P (corresponding author), Amrita Vishwa Vidyapeetham, Ctr Cyber Secur Syst & Networks, Clappana PO, Amritapuri 690525, Kerala, India.
EM kprabhakar@am.amrita.edu; kurunandanj@am.amrita.edu;
   pramodgeorgejose@am.students.amrita.edu; krishna@amrita.edu;
   rbuyya@unimelb.edu.au
RI Achuthan, Krishnashree/AGS-6660-2022; Buyya, Rajkumar/C-3424-2009
OI Jain, Kurunandan/0000-0003-2038-1114
CR Acar U, 2018, 2018 IEEE CONFERENCE ON NETWORK FUNCTION VIRTUALIZATION AND SOFTWARE DEFINED NETWORKS (NFV-SDN)
   Almajali S, 2019, MULTIMED TOOLS APPL, V78, P24617, DOI 10.1007/s11042-018-7049-3
   Alvarez F, 2019, IEEE T BROADCAST, V65, P369, DOI 10.1109/TBC.2019.2901400
   Alvi SA, 2015, AD HOC NETW, V33, P87, DOI 10.1016/j.adhoc.2015.04.006
   [Anonymous], 2021, ACM T MULTIMEDIA COM, V17
   [Anonymous], 2014, MOBILE EDGE COMPUTIN
   [Anonymous], 2018, IEEE INT S BROADB MU
   Asghar MN, 2017, MULTIMED TOOLS APPL, V76, P13139, DOI 10.1007/s11042-016-3725-3
   Awobuluyi Olatunde, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P1657, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.250
   Banchuen T, 2018, INT CONF ADV COMMUN, P600, DOI 10.23919/ICACT.2018.8323848
   Batalla JM, 2017, IEEE COMMUN MAG, V55, P98, DOI 10.1109/MCOM.2017.1600225CM
   Biryukov A., 2017, IACR CRYPTOLOGY EPRI
   Blondeau C., 2013, LECT NOTES COMPUTER, V7881, DOI [10.1007/978-3-642-38348-9_24, DOI 10.1007/978-3-642-38348-9_24]
   Bogdanov A, 2013, DESIGN CODE CRYPTOGR, V66, P75, DOI 10.1007/s10623-012-9660-z
   Chabaud F., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P356, DOI 10.1007/BFb0053450
   Chen Y, 2016, MATH PROBL ENG, V2016, DOI 10.1155/2016/7145138
   Cisco, 2020, CISC VNI FOR METH 20
   Daemen J, 2020, The design of rijndael: the advanced encryption standard (AES), V2nd, DOI DOI 10.1007/978-3-662-60769-53
   Duan Xiaoyu., 2016, IEEE ICC
   Dubrova E, 2017, CRYPTOGR COMMUN, V9, P273, DOI 10.1007/s12095-015-0173-2
   Fang DF, 2018, IEEE ACCESS, V6, P4850, DOI 10.1109/ACCESS.2017.2779146
   Gong Z, 2012, LECT NOTES COMPUT SC, V7055, P1, DOI 10.1007/978-3-642-25286-0_1
   Hatzivasilis G, 2018, J CRYPTOGR ENG, V8, P141, DOI 10.1007/s13389-017-0160-y
   IhsanMertOzcelik Cem., 2019, ACM T MULTIMED COMPU, V15, P3
   Jin YC, 2017, IEEE MULTIMEDIA, V24, P72, DOI 10.1109/MMUL.2017.3051519
   Krishnan P., 2017, P INT C UB COMM NETW, P116, DOI DOI 10.1007/978-3-319-73423-1_11
   Krishnan P, 2019, COMPUT COMMUN, V148, P215, DOI 10.1016/j.comcom.2019.09.014
   Krishnan P, 2019, MOBILE NETW APPL, V24, P1896, DOI 10.1007/s11036-019-01389-2
   Krishnan Prabhakar, 2018, P 6 INT S SEC COMP C
   Liu PC, 2018, CMC-COMPUT MATER CON, V57, P353, DOI 10.32604/cmc.2018.04142
   Manifavas C, 2014, LECT NOTES COMPUT SC, V8247, P333, DOI 10.1007/978-3-642-54568-9_21
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Nikaein N, 2014, ACM SIGCOMM COMP COM, V44, P33, DOI 10.1145/2677046.2677053
   Noura H, 2018, MULTIMED TOOLS APPL, V77, P18383, DOI 10.1007/s11042-018-5660-y
   Ongaro F, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND COMMUNICATIONS (ICNC), P505, DOI 10.1109/ICCNC.2015.7069395
   Philip Merly Annie, 2018, P 3 IEEE INT C REC T
   Shantharama P, 2018, IEEE ACCESS, V6, P57545, DOI 10.1109/ACCESS.2018.2873984
   Singhal C, 2016, IEEE COMMUN MAG, V54, P142, DOI 10.1109/MCOM.2016.1500464CM
   Son J, 2019, J SYST SOFTWARE, V152, P24, DOI 10.1016/j.jss.2019.02.030
   Ullah S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020327
   Xing L, 2018, WIREL COMMUN MOB COM, DOI 10.1155/2018/8974802
   Zaba M. R., 2014, J. Inf. Secur., V05, P114
   Zhang WT, 2015, SCI CHINA INFORM SCI, V58, DOI 10.1007/s11432-015-5459-7
   Zhao S, 2018, INT CONF COMPUT NETW, P77, DOI 10.1109/ICCNC.2018.8390315
NR 44
TC 7
Z9 7
U1 2
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 39
DI 10.1145/3377390
PG 29
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000001
DA 2024-07-18
ER

PT J
AU Yan, XH
   Liu, LT
   Li, LL
   Lu, YL
AF Yan, Xuehu
   Liu, Lintao
   Li, Longlong
   Lu, Yuliang
TI Robust Secret Image Sharing Resistant to Noise in Shares
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Secret image sharing; robust secret image sharing; Chinese remainder
   theorem; error-correcting codes; JPEG compression
ID CHINESE REMAINDER THEOREM; LOSSLESS (K; SCHEME
AB A secret image is split into n shares in the generation phase of secret image sharing (SIS) for a (k, n) threshold. In the recovery phase, the secret image is recovered when any k or more shares are collected, and each collected share is generally assumed to be lossless in conventional SIS during storage and transmission. However, noise will arise during real-world storage and transmission; thus, shares will experience data loss, which will also lead to data loss in the secret image being recovered. Secret image recovery in the case of lossy shares is an important issue that must be addressed in practice, which is the overall subject of this article. An SIS scheme that can recover the secret image from lossy shares is proposed in this article. First, robust SIS and its definition are introduced. Next, a robust SIS scheme for a (k, n) threshold without pixel expansion is proposed based on the Chinese remainder theorem (CRT) and error-correcting codes (ECC). By screening the random numbers, the share generation phase of the proposed robust SIS is designed to implement the error correction capability without increasing the share size. Particularly in the case of collecting noisy shares, our recovery method is to some degree robust to some noise types, such as least significant bit (LSB) noise, JPEG compression, and salt-and-pepper noise. A theoretical proof is presented, and experimental results are examined to evaluate the effectiveness of our proposed method.
C1 [Yan, Xuehu; Liu, Lintao; Li, Longlong; Lu, Yuliang] Natl Univ Def Technol, 460 HUANGSHAN Rd, Hefei 230037, Anhui, Peoples R China.
C3 National University of Defense Technology - China
RP Yan, XH (corresponding author), Natl Univ Def Technol, 460 HUANGSHAN Rd, Hefei 230037, Anhui, Peoples R China.
EM publictiger@126.com; liuta1989@163.com; lilongs8636@163.com;
   publicLuYL@126.com
RI Yan, Xuehu/AAG-1718-2022; Yan, Xuehu/AFK-3139-2022
OI Yan, Xuehu/0000-0001-6388-1720; Yan, Xuehu/0000-0001-6388-1720; Li,
   Longlong/0000-0001-7390-3647
FU National Natural Science Foundation of China [61602491]
FX This work was supported by the National Natural Science Foundation of
   China under Grant No. 61602491.
CR ASMUTH C, 1983, IEEE T INFORM THEORY, V29, P208, DOI 10.1109/TIT.1983.1056651
   Bao L, 2017, IEEE T IMAGE PROCESS, V26, P5618, DOI 10.1109/TIP.2017.2738561
   Belazi A, 2017, OPTIK, V130, P1438, DOI 10.1016/j.ijleo.2016.11.152
   Chen Z, 2018, IEEE T MULTIMEDIA, V20, P1610, DOI 10.1109/TMM.2017.2774004
   Cheng YQ, 2018, IEEE T INF FOREN SEC, V13, P2393, DOI 10.1109/TIFS.2018.2819125
   Chuang TW, 2016, 2016 INTERNATIONAL SYMPOSIUM ON COMPUTER, CONSUMER AND CONTROL (IS3C), P817, DOI 10.1109/IS3C.2016.208
   Cramer R, 2015, LECT NOTES COMPUT SC, V9057, P313, DOI 10.1007/978-3-662-46803-6_11
   Ghebleh M, 2018, MULTIMED TOOLS APPL, V77, P11903, DOI 10.1007/s11042-017-4841-4
   Hu C, 2012, INT J WAVELETS MULTI, V10, DOI 10.1142/S0219691312500233
   Jia XX, 2019, MULTIMED TOOLS APPL, V78, P8207, DOI 10.1007/s11042-018-6779-6
   Komargodski I, 2017, LECT NOTES COMPUT SC, V10678, P379, DOI 10.1007/978-3-319-70503-3_12
   Li LL, 2019, IEEE ACCESS, V7, P75113, DOI 10.1109/ACCESS.2019.2921612
   Li YN, 2012, IEEE T IMAGE PROCESS, V21, P1963, DOI 10.1109/TIP.2011.2171698
   Liu F, 2011, IEEE T INF FOREN SEC, V6, P307, DOI 10.1109/TIFS.2011.2116782
   Liu XB, 2012, IEEE T INF FOREN SEC, V7, P208, DOI 10.1109/TIFS.2011.2169790
   Liu YX, 2018, INFORM SCIENCES, V453, P21, DOI 10.1016/j.ins.2018.04.043
   Naor M., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P1, DOI 10.1007/BFb0053419
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Rishiwal Vinay, 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P11, DOI 10.1109/ICETET.2008.109
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Shankar K, 2016, PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS AND CONTROL (ISCO'16)
   Shyu SJ, 2008, 2008 IEEE ASIA-PACIFIC SERVICES COMPUTING CONFERENCE, VOLS 1-3, PROCEEDINGS, P1332, DOI 10.1109/APSCC.2008.223
   Thien CC, 2002, COMPUT GRAPH-UK, V26, P766
   Ulutas M, 2009, 2009 INTERNATIONAL CONFERENCE ON NETWORK AND SERVICE SECURITY, P24
   Wang GY, 2016, INT J DIGIT CRIME FO, V8, P85, DOI 10.4018/IJDCF.2016070106
   Wang P, 2019, COMPUT SECUR, V85, P107, DOI 10.1016/j.cose.2019.04.010
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZM, 2009, IEEE T INF FOREN SEC, V4, P383, DOI 10.1109/TIFS.2009.2024721
   Weir J, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2344436.2344438
   Yan WQ., 2000, J. North China Univ. Technol, V12, P6
   Yan XH, 2020, IEEE T INF FOREN SEC, V15, P3848, DOI 10.1109/TIFS.2020.3001735
   Yan XH, 2018, DIGIT SIGNAL PROCESS, V82, P80, DOI 10.1016/j.dsp.2018.07.015
   Yan XH, 2017, LECT NOTES COMPUT SC, V10603, P433, DOI 10.1007/978-3-319-68542-7_36
   Yang CN, 2019, IEEE T CIRC SYST VID, V29, P252, DOI 10.1109/TCSVT.2017.2771255
   Yang CN, 2010, IMAGE VISION COMPUT, V28, P1600, DOI 10.1016/j.imavis.2010.04.003
   Zhang X, 2018, IEEE T MULTIMEDIA, V20, P3223, DOI 10.1109/TMM.2018.2838334
   Zhou Z, 2006, IEEE T IMAGE PROCESS, V15, P2441, DOI 10.1109/TIP.2006.875249
   Zhou ZL, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383582
   Zhou ZL, 2019, SOFT COMPUT, V23, P4927, DOI 10.1007/s00500-018-3151-8
   Zhou ZL, 2018, IEEE ACCESS, V6, P15021, DOI 10.1109/ACCESS.2018.2811722
NR 40
TC 18
Z9 18
U1 1
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 24
DI 10.1145/3419750
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200004
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Li, MM
   Wang, SW
   Dai, SS
   Luo, L
   Zhu, E
   Xu, HY
   Zhu, XZ
   Yao, CY
   Zhou, HR
AF Zhang, Yi
   Li, Miaomiao
   Wang, Siwei
   Dai, Sisi
   Luo, Lei
   Zhu, En
   Xu, Huiying
   Zhu, Xinzhong
   Yao, Chaoyun
   Zhou, Haoran
TI Gaussian Mixture Model Clustering with Incomplete Data
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE GMM; clustering; EM; incomplete data
AB Gaussian mixturemodel (GMM) clustering has been extensively studied due to its effectiveness and efficiency. Though demonstrating promising performance in various applications, it cannot effectively address the absent features among data, which is not uncommon in practical applications. In this article, different from existing approaches that first impute the absence and then perform GMM clustering tasks on the imputed data, we propose to integrate the imputation and GMM clustering into a unified learning procedure. Specifically, the missing data is filled by the result of GMM clustering, and the imputed data is then taken for GMM clustering. These two steps alternatively negotiate with each other to achieve optimum. By this way, the imputed data can best serve for GMM clustering. A two-step alternative algorithm with proved convergence is carefully designed to solve the resultant optimization problem. Extensive experiments have been conducted on eight UCI benchmark datasets, and the results have validated the effectiveness of the proposed algorithm.
C1 [Zhang, Yi; Li, Miaomiao; Wang, Siwei; Dai, Sisi; Luo, Lei; Zhu, En] NUDT, Sch Comp, Changsha, Peoples R China.
   [Li, Miaomiao] Changsha Univ, Changsha, Hunan, Peoples R China.
   [Xu, Huiying; Zhu, Xinzhong] Zhejiang Normal Univ, Coll Math & Comp Sci, Hangzhou, Zhejiang, Peoples R China.
   [Xu, Huiying] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Yao, Chaoyun] NUDT, Lab Complex Electromagnet Environm Effects Elect, Changsha, Peoples R China.
   [Zhou, Haoran] Chongqing Univ Technol, Chongqing, Peoples R China.
C3 National University of Defense Technology - China; Changsha University;
   Zhejiang Normal University; City University of Hong Kong; National
   University of Defense Technology - China; Chongqing University of
   Technology
RP Li, MM (corresponding author), NUDT, Sch Comp, Changsha, Peoples R China.
EM miaomiaolinudt@gmail.com
RI Wang, Siwei/ABD-1733-2021
OI Wang, Siwei/0000-0001-9517-262X; ZHANG, YI/0000-0001-8700-7712
FU National Natural Science Foundation of China [61906020, 61701451,
   61872377, 61922088]
FX This work was supported by the National Natural Science Foundation of
   China (project no. 61906020, 61701451, 61872377, and 61922088).
CR Aljalbout E., 2018, Clustering with deep learning: Taxonomy and new methods
   Aste M, 2015, PATTERN ANAL APPL, V18, P1, DOI 10.1007/s10044-014-0411-9
   Bickel S, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P19, DOI 10.1109/ICDM.2004.10095
   Bouman C.A., 1997, CLUSTER UNSUPERVISED
   Celton M, 2010, BMC GENOMICS, V11, DOI 10.1186/1471-2164-11-15
   Du L, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3476
   DuVall Scott L, 2010, NIPS WORKSH, V224
   Eiben Eduard, 2019, ARXIV PREPRINT ARXIV
   Ezatpoor P, 2018, IEEE ACCESS, V6, P7872, DOI 10.1109/ACCESS.2018.2797048
   García-Laencina PJ, 2009, NEUROCOMPUTING, V72, P1483, DOI 10.1016/j.neucom.2008.11.026
   Ghahramani Z., 1994, ADV NEURAL INFORM PR, P120
   Gheyas IA, 2010, NEUROCOMPUTING, V73, P3039, DOI 10.1016/j.neucom.2010.06.021
   Guo XF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1753
   Hartigan J. A., 1975, CLUSTERING ALGORITHM, V458, P468
   Li SY, 2014, AAAI CONF ARTIF INTE, P1968
   Li TH, 2017, NEUROCOMPUTING, V237, P316, DOI 10.1016/j.neucom.2017.01.017
   Liu XW, 2020, IEEE T PATTERN ANAL, V42, P1303, DOI 10.1109/TPAMI.2019.2895608
   Liu XW, 2020, IEEE T PATTERN ANAL, V42, P1191, DOI 10.1109/TPAMI.2019.2892416
   Liu XW, 2017, AAAI CONF ARTIF INTE, P2266
   Liu XW, 2016, AAAI CONF ARTIF INTE, P1888
   Melnykov V, 2010, STAT SURV, V4, P80, DOI 10.1214/09-SS053
   Min EX, 2018, IEEE ACCESS, V6, P39501, DOI 10.1109/ACCESS.2018.2855437
   Song C., 2013, IBEROAMERICAN C PATT, V8258, P117
   Wang SW, 2019, IEEE ACCESS, V7, P69162, DOI 10.1109/ACCESS.2019.2910287
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wu L, 2019, IEEE T IMAGE PROCESS, V28, P1602, DOI 10.1109/TIP.2018.2878970
   Xie CH, 2013, OPTIK, V124, P6216, DOI 10.1016/j.ijleo.2013.05.028
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   Yu S, 2012, IEEE T PATTERN ANAL, V34, P1031, DOI 10.1109/TPAMI.2011.255
NR 31
TC 19
Z9 21
U1 2
U2 33
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 6
DI 10.1145/3408318
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900006
DA 2024-07-18
ER

PT J
AU Claypool, M
   Cockburn, A
   Gutwin, C
AF Claypool, Mark
   Cockburn, Andy
   Gutwin, Carl
TI The Impact of Motion and Delay on Selecting Game Targets with a Mouse
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Lag; user study; target selection
ID FITTS LAW; LATENCY; TIME
AB All real-time computer games, particularly networked computer games, have a delay from when a player starts an action (e.g., clicking the mouse) until the game renders the result (e.g., firing a projectile). This delay can degrade both player performance (e.g., reduced game score) and quality of experience (e.g., the game is less fun). While previous work has studied the effects of delay on commercial games and individual game actions, a more detailed understanding is needed of the effects of delay on moving target selection with realistic target motion, a common scenario in many games. This paper presents an in-depth study of the effects of delay on the fundamental game action of selecting a moving target with a mouse while varying two parameters for the target motion turn frequency and turn angle. We design and implement a custom game where players select moving targets using a mouse, while the game controls both the target motion and input delay. Analysis of data gathered in a 56-person user study shows both target selection time and accuracy degrade with delay. However, both selection time and accuracy increase with the frequency and angle of changes in the target's movement, because turning slows targets down even while making them less predictable. We set these results in the context of other studies of delay and target selection by comparing our findings to those in seven other previously published papers that investigated the effects of delay on target selection.
C1 [Claypool, Mark] Worcester Polytech Inst, Worcester, MA 01609 USA.
   [Cockburn, Andy] Univ Canterbury, Christchurch, New Zealand.
   [Gutwin, Carl] Univ Saskatchewan, Saskatoon, SK, Canada.
C3 Worcester Polytechnic Institute; University of Canterbury; University of
   Saskatchewan
RP Claypool, M (corresponding author), Worcester Polytech Inst, Worcester, MA 01609 USA.
EM claypool@cs.wpi.edu; andy@cosc.canterbury.ac.nz; gutwin@usask.ca
CR Accot Johnny, 1997, P ACM SIGCHI C HUMAN, P295, DOI [10.1145/258549.258760, DOI 10.1145/258549.258760]
   Al Hajri Abir, 2011, P IFIP TC HUM COMP I
   Amin Rahul, 2013, Human-Computer Interaction. Users and Contexts of Use. 15th International Conference, HCI International 2013. Proceedings: LNCS 8006, P97, DOI 10.1007/978-3-642-39265-8_11
   Andress Justin, WHY SUPER SMASH BROS
   Beigbeder Tom, 2004, P ACM NETW SYST SUPP
   Bernier Y. W., 2001, P GAM DEV C SAN FRAN
   Buckland M., 2010, PROGRAMMING GAME AI
   Chen K., 2006, P IEEE INFOCOM
   Chen Kuan-Ta, 2014, IEEE T MULTIMEDIA, V26, P2
   Claypool M, 2005, COMPUT NETW, V49, P52, DOI 10.1016/j.comnet.2005.04.008
   Claypool M, 2014, P 13 ACM NETW SYST S
   Claypool M, 2006, COMMUN ACM, V49, P40, DOI 10.1145/1167838.1167860
   Claypool Mark, 2019, P 10 ANN ACM MULT SY
   Claypool Mark, 2017, P 23 INT C MULT MOD
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   Friston S, 2016, IEEE T VIS COMPUT GR, V22, P1605, DOI 10.1109/TVCG.2015.2446467
   Fritsch T., 2005, P 4 ACM NETW SYST SU
   HOFFMANN ER, 1991, ERGONOMICS, V34, P211, DOI 10.1080/00140139108967307
   HOFFMANN ER, 1992, ERGONOMICS, V35, P37, DOI 10.1080/00140139208967796
   Ivkovic Z, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P135, DOI 10.1145/2702123.2702432
   JAGACINSKI RJ, 1980, HUM FACTORS, V22, P225, DOI 10.1177/001872088002200211
   Jarschel M., 2011, Proceedings of the 2011 Fifth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), P330, DOI 10.1109/IMIS.2011.92
   Jota Ricardo, 2013, P ACM SIGCHI C HUM F
   KERR R, 1973, J MOTOR BEHAV, V5, P175, DOI 10.1080/00222895.1973.10734962
   Long M., 2018, P ACM S COMP HUM INT
   Long Michael, 2019, P ACM SIGCHI C HUM F
   MacKenzie I. S., 1992, CHI '92 Conference Proceedings. ACM Conference on Human Factors in Computing Systems. Striking a Balance, P219, DOI 10.1145/142750.142794
   MacKenzie I. Scott, 1993, P CHI C HUM FACT COM, P6
   Nichols J., 2004, PROC 14 INT WORKSHOP, P146
   Pantel Lothar, 2002, P WORKSH NETW OP SYS
   Pavlovych A., 2011, Proceedings of Graphics Interface, P33
   Pavlovych A., 2012, P GRAPH INT C TOR ON
   Quax Peter, 2004, P ACM SIGCOMM WORKSH
   Raaen K., 2015, P 17 HCI INT C LOS A
   Raaen Kjetil, 2015, P ACM MULT SYST MARC
   Sabet SS, 2018, IEEE INT SYM MULTIM, P114, DOI 10.1109/ISM.2018.00028
   Sackl A., 2016, IEEE C COMM WORKSH I
   Soukoreff RW, 2004, INT J HUM-COMPUT ST, V61, P751, DOI 10.1016/j.ijhcs.2004.09.001
   Teather RJ, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P43, DOI 10.1109/3DUI.2009.4811204
   Ware C., 1987, P SIGCHIGI C HUM FAC
NR 40
TC 7
Z9 7
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 73
DI 10.1145/3390464
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA OJ1FH
UT WOS:000583712600016
DA 2024-07-18
ER

PT J
AU Vellingiri, S
   McMahan, RP
   Prabhakaran, B
AF Vellingiri, Shanthi
   McMahan, Ryan P.
   Prabhakaran, Balakrishnan
TI SCeVE: A Component-based Framework to Author Mixed Reality Tours
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Authoring; virtual reality; mixed reality; trave; gesture-based;
   non-natural; framework; components; services; case study; modeling; user
   experience
ID VIRTUAL-REALITY; NAVIGATION; LATENCY; QOE
AB Authoring a collaborative, interactive Mixed Reality (MR) tour requires flexible design and development of various software modules for tasks such as managing geographically distributed participants, adaptable travel and virtual camera techniques, data logging for assessment of the incorporated techniques, as well as for evaluating the Quality of Experiences (QoE). In most cases, authors might have to develop all these software modules, instead of focusing only on the virtual environment design. In this article, we propose SCeVE, a component-based framework that supports flexible design and authoring of interactive MR tours by offering ease of access to four major design choices: (i) Synchronization, (ii) Collaborative exploration, (iii) Visualization, and (iv) Evaluation. Based on tour requirements, an author can access one or more components (or software libraries) of design choices via SCeVE'.s API (Application Programming Interface) services, as demonstrated by the two case studies on group travel in a plant walk MR tour.
   SCeVE framework is innovative in the sense that it facilitates group travel in virtual environments involving live" models of participants from geographically distributed sites. SCeVE empowers authors to focus only on the design of the required virtual environments. They can quickly build a diverse set of collaborative MR tours by utilizing the flexibility of SCeVE in terms of the various available options for traveling, rendering on multiple devices, and virtual camera viewpoint computation strategies. By providing data logs of various components, SCeVE facilitates performance evaluation of the various strategies used as well as the user experience in collaborative MR tours. SCeVE is designed in an extensible manner, allowing authors to add devices and software services as additional components.
C1 [Vellingiri, Shanthi; Prabhakaran, Balakrishnan] Univ Texas Dallas, 800 W Campbell Rd, Richardson, TX 75080 USA.
   [McMahan, Ryan P.] Univ Cent Florida, 4000 Cent Florida Blvd, Orlando, FL 32816 USA.
C3 University of Texas System; University of Texas Dallas; State University
   System of Florida; University of Central Florida
RP Vellingiri, S (corresponding author), Univ Texas Dallas, 800 W Campbell Rd, Richardson, TX 75080 USA.
EM shanthi.vellingiri@utdallas.edu; rpm@ucf.edu; bprabhakaran@utdallas.edu
RI Vellingiri, Shanthi/AAA-5126-2022
OI McMahan, Ryan/0000-0001-9357-9696
FU US Army Research Office (ARO) [W911NF-17-1-0299]
FX This material is based upon work supported by the US Army Research
   Office (ARO) Grant W911NF-17-1-0299. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the author(s) and do not necessarily reflect the views of the ARO.
CR Al Zayer M, 2020, IEEE T VIS COMPUT GR, V26, P2315, DOI 10.1109/TVCG.2018.2887379
   Allison RS, 2001, P IEEE VIRT REAL ANN, P247, DOI 10.1109/VR.2001.913793
   Amamra A, 2016, PROCEEDINGS OF THE 2016 SAI COMPUTING CONFERENCE (SAI), P388, DOI 10.1109/SAI.2016.7556011
   [Anonymous], 2013, Proceedings of the 19th ACM Symposium on Virtual Reality Software and Technology, DOI DOI 10.1145/2503713.2503747
   [Anonymous], 2018, P 15 ACM S APPL PERC
   [Anonymous], 1999, THESIS
   Backman A., 2005, IPT/EGVE, P225
   Bauer M, 2001, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDINGS, P45, DOI 10.1109/ISAR.2001.970514
   Bednarz T., 2015, Machine Vision and Mechatronics in Practice, P39
   Bickerstaff I, 2012, PROC SPIE, V8288, DOI 10.1117/12.912067
   Bierbaum A, 2001, P IEEE VIRT REAL ANN, P89, DOI 10.1109/VR.2001.913774
   Borst CW, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P467, DOI 10.1109/VR.2018.8448286
   Brunnstrom K., 2018, ELECT IMAGING, P1, DOI DOI 10.23919/PSCC.2018.8442877
   Cantino A.S., 2007, Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems, P22
   Cho KH, 2013, AM J PHYS MED REHAB, V92, P371, DOI 10.1097/PHM.0b013e31828cd5d3
   Christie M, 2005, LECT NOTES COMPUT SC, V3638, P40
   Collins SH, 2009, P ROY SOC B-BIOL SCI, V276, P3679, DOI 10.1098/rspb.2009.0664
   Dang N.-T., 2008, Proceedings of the 3eme Journees de l'Association Francaise de Realite Virtuelle, P119
   DePriest D., 2011, 16th ANNUAL TCC Worldwide Online Conference. Hawa, P48
   Desai K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P250, DOI 10.1145/3204949.3204969
   Ellis SR, 1999, P IEEE VIRT REAL ANN, P218, DOI 10.1109/VR.1999.756954
   Feasel J, 2008, 3DUI: IEEE SYMPOSIUM ON 3D USER INTERFACES 2008, PROCEEDINGS, P97
   Friston S, 2014, IEEE T VIS COMPUT GR, V20, P616, DOI 10.1109/TVCG.2014.30
   Fritsch Tobias, 2007, P 1 INT WORKSH INT C
   Gajewska Hania, 2003, US Patent, Patent No. [6,654,038, 6654038]
   Goldstein Benjamin, 2000, THESIS
   Haller M., 2003, A generic framework for a training application based on mixed reality
   Hamam A, 2013, IEEE T INSTRUM MEAS, V62, P3315, DOI 10.1109/TIM.2013.2272859
   Irawati S, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P201
   Ishibashi Y., 2012, 2012 11th Annual Workshop on Network and Systems Support for Games (NetGames), P1, DOI 10.1109/FCS.2012.6243600
   Joele Dennis, 2005, THESIS
   Kato Hirokazu, 2007, P 1 IEEE INT WORKSH
   Kilteni K, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040867
   Kusunose Y., 2010, Network and Systems Support for Games (NetGames), 2010 9th Annual Workshop on, P1
   LaViola Joseph J., 2017, 3D User interfaces: theory and practice
   Lee G.A., 2013, Proceedings of the 12th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry, P207
   Li TY, 2008, LECT NOTES COMPUT SC, V5166, P118
   Li TY, 1999, COMP ANIM CONF PROC, P99, DOI 10.1109/CA.1999.781203
   Llobera J, 2013, EXP BRAIN RES, V225, P105, DOI 10.1007/s00221-012-3352-9
   Lucas GM, 2016, LECT NOTES ARTIF INT, V10011, P351, DOI 10.1007/978-3-319-47665-0_31
   Maher Mary Lou, 2002, CUMINCAD, DOI [10.1109/CW. 2005.13, DOI 10.1109/CW.2005.13]
   McMahan Ryan P., 2016, Virtual, Augmented and Mixed Reality. 8th International Conference, VAMR 2016, held as part of HCI International 2016. Proceedings: LNCS 9740, P59, DOI 10.1007/978-3-319-39907-2_6
   MILLS DL, 1991, IEEE T COMMUN, V39, P1482, DOI 10.1109/26.103043
   Minocha S, 2016, ISS ONLINE EDUC, P3
   Nabiyouni Mahdi, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P3, DOI 10.1109/3DUI.2015.7131717
   Nilsson NC, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P31, DOI 10.1109/3DUI.2013.6550193
   Nilsson NC, 2018, COMPUT ENTERTAIN, V16, DOI 10.1145/3180658
   Ohlenburg J., 2004, P ACM S VIRTUAL REAL, P166, DOI DOI 10.1145/1077534.1077568
   Park JS, 2011, MULTIMED TOOLS APPL, V55, P725, DOI 10.1007/s11042-010-0592-1
   Patel Roma, 2008, NARRATING VIRTUAL EN, P249
   Pavlik R. A., 2012, 2012 5th Workshop on Software Engineering and Architectures for Realtime Interactive Systems, P29, DOI 10.1109/SEARIS.2012.6231166
   Pazzaglia F., 2016, Spatial Cognition X, P55
   Piekarski W, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P247, DOI 10.1109/ISMAR.2003.1240708
   Raghuraman S, 2017, THESIS
   Rorke M. J., 2000, THESIS
   Roussou M., 2005, perspectives, V8, P9
   Schmalstieg D., 2011, P 13 INT C HUM COMP, P211, DOI [10.1145/2037373.2037406, DOI 10.1145/2037373.2037406]
   Sears A, 2003, HUM-COMPUT INTERACT, V18, P229, DOI 10.1207/S15327051HCI1803_2
   Shapiro A, 2014, COMPUT ANIMAT VIRT W, V25, P201, DOI 10.1002/cav.1579
   Sithu M, 2015, ASIA-PAC CONF COMMUN, P570, DOI 10.1109/APCC.2015.7412576
   Stoakley R., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P265
   Tatematsu A., 2010, Communications Quality and Reliability (CQR), 2010 IEEE International Workshop Technical Committee on, P1, DOI [DOI 10.1109/CQR.2010.5619913, 10.1109/CQR.2010.5619913]
   Unity Technologies, 2005, UNITY3D GAM ENG
   Veeraragavan NR, 2016, IEEE T PARALL DISTR, V27, P2667, DOI 10.1109/TPDS.2015.2503291
   Vellingiri Shanthi, 2014, 2014 IEEE International Symposium on Haptic, Audio and Visual Environments and Games (HAVE). Proceedings, P53, DOI 10.1109/HAVE.2014.6954331
   Vellingiri S, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P296, DOI 10.1145/3083187.3084018
   Vellingiri S, 2018, ALTMM'18: PROCEEDINGS OF THE 3RD INTERNATIONAL WORKSHOP ON MULTIMEDIA ALTERNATE REALITIES, P3, DOI 10.1145/3268998.3269002
   Vinson N.G., 1999, CHI 99, P278, DOI DOI 10.1145/302979.303062
   Wagner D, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P235
   Wei-Po Lee, 2011, International Journal of Computers & Applications, V33, P293, DOI 10.2316/Journal.202.2011.4.202-2813
   White M, 2001, INT J ELEC ENG EDUC, V38, P316, DOI 10.7227/IJEEE.38.4.5
NR 71
TC 4
Z9 4
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 40
DI 10.1145/3377353
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600001
DA 2024-07-18
ER

PT J
AU Ainam, JP
   Qin, K
   Liu, GS
   Luo, GC
   Agyemang, B
AF Ainam, Jean-Paul
   Qin, Ke
   Liu, Guisong
   Luo, Guangchun
   Agyemang, Brighter
TI Enforcing Affinity Feature Learning through Self-attention for Person
   Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person ReID; pedestrian retrieval; attention mechanism; deep residual
   network
ID NETWORK
AB Person re-identification is the task of recognizing an individual across heterogeneous non-overlapping camera views. It has become a crucial capability needed by many applications in public space video surveillance. However, it remains a challenging task due to the subtle inter-class similarity and large intra-class variation found in person images. Current CNN-based approaches have focused and investigated traditional identification or verification frameworks. Such approaches typically use the whole input image including the background and fail to pay attention to specific body parts, deviating the feature representation learning from informative parts. In this article, we introduce a self-attention mechanism coupled with cross-resolution to improve the feature representation learning of person re-identification task. The proposed self-attention module reinforces the most informative parts from a high-resolution image using its internal representation at the low-resolution. In particular, the model is fed with a pair of images on a different scale and consists of two branches. The upper branch processes the high-resolution image and learns high dimensional feature representation while the lower branch processes the low-resolution image and learns a filtering attention heatmap. The feature maps on the lower branch are subsequently weighted to reflect the importance of each patch of the input image using a softmax operation; whereas, on the upper branch, we apply a max pooling operation to downsample the high-resolution feature map before element-wise multiplied with the attention heatmap. Our attention module helps the network learn the most discriminative visual features of multiple regions of the image and is specifically optimized to attend and enforce feature representation at different scales. Extensive experiments on three large-scale datasets show that network architectures augmented with our self-attention module systematically improve their accuracy and outperform various state-of-the-art models by a large margin.
C1 [Ainam, Jean-Paul; Qin, Ke; Liu, Guisong; Luo, Guangchun; Agyemang, Brighter] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Ainam, Jean-Paul] Adventist Cosendai Univ, Sch Comp Sci & Business, Ctr 401, Nanga Eboko, Cameroon.
   [Liu, Guisong] Univ Elect Sci & Technol China, Sch Comp Sci, Zhongshan Inst, Zhongshan 5284, Guangdong, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China
RP Qin, K (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
EM jpainam@gmail.com; qinke@uestc.edu.cn; lgs@uestc.edu.cn;
   gcluo@uestc.edu.cn; brighteragyemang@gmail.com
RI Liu, Guisong/S-1263-2019; Qin, Ke/H-1140-2013; Ainam,
   Jean-Paul/AEX-0670-2022; Ainam, Jean-Paul/U-5891-2019
OI Liu, Guisong/0000-0003-2360-0466; Qin, Ke/0000-0001-6174-3877; Ainam,
   Jean-Paul/0000-0001-8081-6867
FU National Key RAMP;D Program of China [2018YFC0807500]; ministry of
   science and technology of Sichuan province [2018GZDZX0048, 2018JY0605]
FX This work is supported by National Key R&D Program of China (No.
   2018YFC0807500) and ministry of science and technology of Sichuan
   province (No. 2018GZDZX0048, 2018JY0605).
CR [Anonymous], 2017, ARXIV
   [Anonymous], 2018, ARXIV E PRINTS
   [Anonymous], 2018, ARXIV E PRINTS
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247939
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bai S, 2017, AAAI CONF ARTIF INTE, P1281
   Barbosa IB, 2018, COMPUT VIS IMAGE UND, V167, P50, DOI 10.1016/j.cviu.2017.12.002
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Chen D, 2018, INT CONF CLOUD COMPU, P507, DOI 10.1109/CCIS.2018.8691205
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Cheng J., 2016, P C EMP METH NAT LAN, P1
   Chorowski Jan, 2015, P INT C NEUR INF PRO, P577
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng Weijian, 2018, P C COMP VIS PATT RE
   Fan H., 2017, ARXIV E PRINTS
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Geng M., 2016, ARXIV E PRINTS
   Goodfellow J. Ian, 2014, P INT C NEUR INF PRO
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hermann Karl Moritz, 2015, Advances in Neural Information Processing Systems, P1693
   Jiang Minyue, 2018, P BRIT MACH VIS C
   Jing XY, 2017, IEEE T IMAGE PROCESS, V26, P1363, DOI 10.1109/TIP.2017.2651364
   Karianakis Nikolaos, 2018, P EUR C COMP VIS ECC
   Leibe B., 2017, ARXIV170307737CS
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289
   Mnih V, 2014, ADV NEUR IN, V27
   Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794
   Parikh AP., 2016, EMNLP
   Parmar N., 2018, ARXIV E PRINTS
   Rahimpour A, 2017, IEEE IMAGE PROC, P4242, DOI 10.1109/ICIP.2017.8297082
   Ristani Ergys, 2016, C COMP VIS WORKSH B
   Shan C., 2017, ARXIV E PRINTS
   Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Ustinova E., 2015, ARXIV E PRINTS
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Vaswani A., 2017, Proceedings of the 31st International Conference on Neural Information Processing Systems, P5998
   Vezzani R, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543596
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang J, 2016, JOINT 2016 INTERNATIONAL CONFERENCE ON SOCIAL SCIENCE AND ENVIRONMENTAL SCIENCE (SSES 2016) AND INTERNATIONAL CONFERENCE ON FOOD SCIENCE AND ENGINEERING (ICFSE 2016), P269
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wu Lin, 2016, PATTERN RECOG, V65
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Ying Zhang, 2011, Proceedings of the Sixth International Conference on Image and Graphics (ICIG 2011), P368, DOI 10.1109/ICIG.2011.40
   Zeyer A., 2018, ARXIV E PRINTS
   Zhang C., 2018, ARXIV E PRINTS
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhedong Zheng, 2017, ACM Transactions on Multimedia Computing, Communications and Applications, V14, DOI 10.1145/3159171
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhong Zhun, 2017, PROCEEDINGS OF THE I
NR 74
TC 10
Z9 10
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 16
DI 10.1145/3377352
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100015
DA 2024-07-18
ER

PT J
AU Yang, ZG
   Lin, ZH
   Kang, PP
   Lv, JM
   Li, Q
   Liu, WY
AF Yang, Zhenguo
   Lin, Zehang
   Kang, Peipei
   Lv, Jianming
   Li, Qing
   Liu, Wenyin
TI Learning Shared Semantic Space with Correlation Alignment for
   Cross-Modal Event Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modal retrieval; heterogeneous data; deep learning; Wiki-Flickr
   Event dataset
AB In this article, we propose to learn shared semantic space with correlation alignment (S-3 CA) for multimodal data representations, which aligns nonlinear correlations of multimodal data distributions in deep neural networks designed for heterogeneous data. In the context of cross-modal (event) retrieval, we design a neural network with convolutional layers and fully connected layers to extract features for images, including images on Flickr-like social media. Simultaneously, we exploit a fully connected neural network to extract semantic features for text documents, including news articles from news media. In particular, nonlinear correlations of layer activations in the two neural networks are aligned with correlation alignment during the joint training of the networks. Furthermore, we project the multimodal data into a shared semantic space for cross-modal (event) retrieval, where the distances between heterogeneous data samples can be measured directly. In addition, we contribute a Wiki-Flickr Event dataset, where the multimodal data samples are not describing each other in pairs like the existing paired datasets, but all of them are describing semantic events. Extensive experiments conducted on both paired and unpaired datasets manifest the effectiveness of S-3 CA, outperforming the state-of-the-art methods.
C1 [Yang, Zhenguo; Lin, Zehang; Kang, Peipei; Liu, Wenyin] Guangdong Univ Technol, Guangzhou, Peoples R China.
   [Yang, Zhenguo] City Univ Hong Kong, Hong Kong, Peoples R China.
   [Lv, Jianming] South China Univ Technol, Guangzhou, Peoples R China.
   [Li, Qing] Hong Kong Polytech Univ, Hong Kong, Peoples R China.
C3 Guangdong University of Technology; City University of Hong Kong; South
   China University of Technology; Hong Kong Polytechnic University
RP Yang, ZG; Liu, WY (corresponding author), Guangdong Univ Technol, Guangzhou, Peoples R China.; Yang, ZG (corresponding author), City Univ Hong Kong, Hong Kong, Peoples R China.
EM zhengyang5-c@my.cityu.edu.hk; gdutlin@outlook.com; ppkanggdut@126.com;
   jmlv@scut.edu.cn; csqli@comp.polyu.edu.hk; liuwy@gdut.edu.cn
RI Li, Qing/JMH-1365-2023; zheng, yi/JOZ-7204-2023; Yang,
   Zhenguo/X-9205-2019; lin, zehang/ABD-1029-2020
OI Li, Qing/0000-0003-3370-471X; Yang, Zhenguo/0000-0001-7674-977X; lin,
   zehang/0000-0002-0919-6770
FU National Natural Science Foundation of China [61703109, 91748107,
   61902077, 61876065, U1611461]; Guangdong Innovative Research Team
   Program [2014ZT05G157]; Natural Science Foundation of Guangdong
   Province, China [2018A0303130022]; Science and Technology Program of
   Guangzhou, China [201904010200]; Science and Technology Planning Project
   of Guangdong Province, China [2016A010101012]; Research Grants Council
   of the Hong Kong Special Administrative Region, China (Collaborative
   Research Fund) [C1031-18G]
FX This work was supported by the National Natural Science Foundation of
   China (nos. 61703109, 91748107, 61902077, 61876065, and U1611461); the
   Guangdong Innovative Research Team Program (no. 2014ZT05G157); the
   Natural Science Foundation of Guangdong Province, China (no.
   2018A0303130022); the Science and Technology Program of Guangzhou, China
   (no. 201904010200); the Science and Technology Planning Project of
   Guangdong Province, China (no. 2016A010101012); and the Research Grants
   Council of the Hong Kong Special Administrative Region, China
   (Collaborative Research Fund, project no. C1031-18G).
CR [Anonymous], 2017, ARXIV170608224
   [Anonymous], 2017, TRAINING
   [Anonymous], 2017, ARXIV170705612
   [Anonymous], 2016, AAAI
   [Anonymous], 2017, ARXIV170300573
   Bai B, 2010, INFORM RETRIEVAL, V13, P291, DOI 10.1007/s10791-009-9117-9
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Che Tong, 2016, CoRR
   Fan MD, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1698, DOI 10.1145/3123266.3123369
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li D., 2003, P 11 ACM INT C MULTI, P604
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Situ RW, 2018, LECT NOTES COMPUT SC, V11165, P147, DOI 10.1007/978-3-030-00767-6_14
   Tran TQN, 2016, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2016.225
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang L, 2018, IEEE T AUTOM SCI ENG, V15, P1521, DOI 10.1109/TASE.2018.2868471
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang ZG, 2020, IEEE T PATTERN ANAL, V42, P1243, DOI 10.1109/TPAMI.2019.2893953
   Yu Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3281746
   Zhang L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P907, DOI 10.1145/3123266.3123317
   Zhang Xi, 2017, ARXIV171109347
NR 35
TC 19
Z9 20
U1 2
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 9
DI 10.1145/3374754
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kairanbay, M
   See, J
   Wong, LK
AF Kairanbay, Magzhan
   See, John
   Wong, Lai-Kuan
TI Beauty Is in the Eye of the Beholder: Demographically Oriented Analysis
   of Aesthetics in Photographs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Photographer demographics; demographic attributes; image aesthetic
   evaluation; AVA; convolutional neural networks
ID CULTURE
AB Aesthetics is a subjective concept that is likely to be perceived differently among people of different ages, genders, and cultural backgrounds. While techniques that directly compute this concept in images has seen increasing attention by the multimedia and machine-learning community, there are very few attempts at encoding the influences from the photographer's viewpoint. This work demonstrates how the aesthetic quality of photos can be better learned by accounting for the demographic background of a photographer. A new AVA-PD (Photographer Demographic) dataset is created to supplement the AVA dataset by providing photographers' age, gender and location attributes. Two deep convolutional neural network (CNN) architectures are proposed to utilize demographic information for aesthetic prediction of photos; both are shown to yield better prediction capabilities compared to most existing approaches. By leveraging on AVA-PD meta-data, we also present some additional machine-learnable tasks such as identifying the photographer and predicting photography styles from a person's gallery of photos.
C1 [Kairanbay, Magzhan; See, John; Wong, Lai-Kuan] Multimedia Univ, Persiaran Multimedia, Cyberjaya 63100, Selangor, Malaysia.
C3 Multimedia University
RP See, J (corresponding author), Multimedia Univ, Persiaran Multimedia, Cyberjaya 63100, Selangor, Malaysia.
EM magzhan.kairanbay@gmail.com; johnsee@mmu.edu.my; lkwong@mmu.edu.my
RI See, John/C-8633-2013; Wong, Lai Kuan/AAO-7014-2021
OI See, John/0000-0003-3005-4109; Wong, Lai Kuan/0000-0002-4517-0391
FU Multimedia University, Malaysia [MMUI/160085]; Malaysia MOHE
   [FRGS/1/2018/ICT02/MMU/02/2]
FX This work is supported by Multimedia University, Malaysia under MMU-GRA
   Scheme MMUI/160085 and Malaysia MOHE Grant FRGS/1/2018/ICT02/MMU/02/2.
CR [Anonymous], DPCHALLENGE DAT
   [Anonymous], Recognizing image style
   [Anonymous], 2016, AS COMP VIS C ACCV
   [Anonymous], 2007, P 15 ACM INT C MULTI
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2016, P 24 ACM INT C MULT, DOI DOI 10.1145/2964284.2967251
   [Anonymous], ARXIV1604049705
   Bar Y, 2015, LECT NOTES COMPUT SC, V8925, P71, DOI 10.1007/978-3-319-16178-5_5
   Broer PN, 2014, J CRANIOFAC SURG, V25, pE157, DOI 10.1097/SCS.0000000000000406
   Cela-Conde CJ, 2009, P NATL ACAD SCI USA, V106, P3847, DOI 10.1073/pnas.0900304106
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Hii YL, 2017, IEEE IMAGE PROC, P1722, DOI 10.1109/ICIP.2017.8296576
   Jacobsen T, 2010, J ANAT, V216, P184, DOI 10.1111/j.1469-7580.2009.01164.x
   Jin Xin., 2016, 2016 8th International Conference, P1
   Kairanbay M, 2017, IEEE IMAGE PROC, P3051, DOI 10.1109/ICIP.2017.8296843
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Kao YY, 2016, SIGNAL PROCESS-IMAGE, V47, P500, DOI 10.1016/j.image.2016.05.004
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Masuda T, 2008, PERS SOC PSYCHOL B, V34, P1260, DOI 10.1177/0146167208320555
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Obrador P., 2009, PROC 1 ACM SIGMM WOR, P65
   Obrador P, 2010, IEEE IMAGE PROC, P3185, DOI 10.1109/ICIP.2010.5654231
   Redi Miriam, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163086
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Savakis AE, 2000, P SOC PHOTO-OPT INS, V3959, P111, DOI 10.1117/12.387147
   Schwarz K, 2018, IEEE WINT CONF APPL, P2048, DOI 10.1109/WACV.2018.00226
   Sun TC, 2017, IEEE T IMAGE PROCESS, V26, P4102, DOI 10.1109/TIP.2017.2710631
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Thomas C, 2016, PROC CVPR IEEE, P3494, DOI 10.1109/CVPR.2016.380
   Wang ZY, 2017, IEEE IJCNN, P941, DOI 10.1109/IJCNN.2017.7965953
   Wong LK, 2009, IEEE IMAGE PROC, P997, DOI 10.1109/ICIP.2009.5413825
   Yan R, 2018, IEEE INT C BIOINFORM, P957, DOI 10.1109/BIBM.2018.8621429
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhao MQ, 2016, SIGNAL PROCESS-IMAGE, V47, P511, DOI 10.1016/j.image.2016.05.009
   Zhe Dong, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P524, DOI 10.1007/978-3-319-14442-9_57
NR 43
TC 4
Z9 4
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 63
DI 10.1145/3328993
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900016
DA 2024-07-18
ER

PT J
AU Hsu, CF
   Wang, YS
   Lei, CL
   Chen, KT
AF Hsu, Chih-Fan
   Wang, Yu-Shuen
   Lei, Chin-Laung
   Chen, Kuan-Ta
TI Look at Me! Correcting Eye Gaze in Live Video Communication
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Eye contact; live video communication; gaze correction; image
   processing; convolutional neural network
ID MEDIATED COMMUNICATION; CONTACT
AB Although live video communication is widely used, it is generally less engaging than face-to-face communication because of limitations on social, emotional, and haptic feedback. Missing eye contact is one such problem caused by the physical deviation between the screen and camera on a device. Manipulating video frames to correct eye gaze is a solution to this problem. In this article, we introduce a system to rotate the eyeball of a local participant before the video frame is sent to the remote side. It adopts a warping-based convolutional neural network to relocate pixels in eye regions. To improve visual quality, we minimize the L2 distance between the ground truths and warped eyes. We also present several newly designed loss functions to help network training. These new loss functions are designed to preserve the shape of eye structures and minimize color changes around the periphery of eye regions. To evaluate the presented network and loss functions, we objectively and subjectively compared results generated by our system and the state-of-the-art, DeepWarp, in relation to two datasets. The experimental results demonstrated the effectiveness of our system. In addition, we showed that our system can perform eye-gaze correction in real time on a consumer-level laptop. Because of the quality and efficiency of the system, gaze correction by postprocessing through this system is a feasible solution to the problem of missing eye contact in video communication.
C1 [Hsu, Chih-Fan; Lei, Chin-Laung] Natl Taiwan Univ, Dept Elect Engn, Taipei, Taiwan.
   [Wang, Yu-Shuen] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
   [Chen, Kuan-Ta] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
C3 National Taiwan University; National Yang Ming Chiao Tung University;
   Academia Sinica - Taiwan
RP Hsu, CF (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Taipei, Taiwan.
EM chihfan@iis.sinica.edu.tw; yushuen@cs.nctu.edu.tw; cllei@ntu.edu.tw;
   swc@iis.sinica.edu.tw
OI Hsu, Chih-Fan/0000-0002-4180-8255; Wang, Yu-Shuen/0000-0003-2550-2990
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2012, ACM T GRAPH P ACM SI
   [Anonymous], P UBICOMP 2003 VID P
   [Anonymous], 2015, C MACH LEARN
   [Anonymous], WEBINAR WEBCAST MARK
   [Anonymous], EUROGRAPHICS
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], LATE LATE SHOW J COR
   [Anonymous], TECHNICAL REPORT
   Baek ET, 2017, SIGNAL IMAGE VIDEO P, V11, P187, DOI 10.1007/s11760-016-0918-1
   Bohannon LS, 2013, DISPLAYS, V34, P177, DOI 10.1016/j.displa.2012.10.009
   Criminisi A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P191
   Dodgson NA, 2004, PROC SPIE, V5291, P36, DOI 10.1117/12.529999
   DohertySneddon G, 1997, J EXP PSYCHOL-APPL, V3, P105, DOI 10.1037/1076-898X.3.2.105
   Dumont M, 2009, COMM COM INF SC, V48, P358
   Ganin Y, 2016, LECT NOTES COMPUT SC, V9906, P311, DOI 10.1007/978-3-319-46475-6_20
   Gemmell J., 2000, IEEE Multimedia, V7, P26, DOI 10.1109/93.895152
   Giger D, 2014, IEEE INT CON MULTI
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jaklic A, 2017, DISPLAYS, V46, P25, DOI 10.1016/j.displa.2016.12.002
   Jones A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531370
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Lee PSN, 2011, SOC INDIC RES, V100, P375, DOI 10.1007/s11205-010-9618-3
   OGITA T., 2012, United States Patent, Patent No. [US20120069042A1, 20120069042]
   Ponce J, 2002, COMPUTER VISION MODE, DOI 10.5555/580035
   Qin YL, 2015, LECT NOTES COMPUT SC, V9474, P599, DOI 10.1007/978-3-319-27857-5_54
   Rappoport B. M., 2016, United States Patent, Patent No. [US20160358543A1, 20160358543]
   Shu ZX, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2926713
   Smith B.A., 2013, P 26 ANN ACM S USER, P271, DOI DOI 10.1145/2501988.2501994
   Solina F., 2011, 2011 33rd International Conference on Information Technology Interfaces, P233
   Weiner D, 2003, 12TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P76, DOI 10.1109/ICIAP.2003.1234028
   Wolf L, 2010, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2010.5540133
NR 34
TC 8
Z9 8
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 38
DI 10.1145/3311784
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IM3ZV
UT WOS:000477935400009
DA 2024-07-18
ER

PT J
AU Miao, Y
   Dong, HW
   Al Jaam, JM
   El Saddik, A
AF Miao, Yu
   Dong, Haiwei
   Al Jaam, Jihad Mohamad
   El Saddik, Abdulmotaleb
TI A Deep Learning System for Recognizing Facial Expression in Real-Time
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Facial expression recognition; deep learning networks
ID CONVOLUTIONAL NEURAL-NETWORKS; RECOGNITION
AB This article presents an image-based real-time facial expression recognition system that is able to recognize the facial expressions of several subjects on a webcam at the same time. Our proposed methodology combines a supervised transfer learning strategy and a joint supervision method with center loss, which is crucial for facial tasks. A newly proposed Convolutional Neural Network (CNN) model, MobileNet, which has both accuracy and speed, is deployed in both offline and in a real-time framework that enables fast and accurate real-time output. Evaluations towards two publicly available datasets, JAFFE and CK+, are carried out respectively. The JAFFE dataset reaches an accuracy of 95.24%, while an accuracy of 96.92% is achieved on the 6-class CK+ dataset, which contains only the last frames of image sequences. At last, the average run-time cost for the recognition of the real-time implementation is around 3.57ms/frame on a NVIDIA Quadro K4200 GPU.
C1 [Miao, Yu; Dong, Haiwei; El Saddik, Abdulmotaleb] Univ Ottawa, 800 King Edward Ave, Ottawa, ON K1N 6N5, Canada.
   [Al Jaam, Jihad Mohamad] Qatar Univ, Ibn Khaldoon Hall, Doha, Qatar.
C3 University of Ottawa; Qatar University
RP Miao, Y (corresponding author), Univ Ottawa, 800 King Edward Ave, Ottawa, ON K1N 6N5, Canada.
EM ymiao036@uottawa.ca; hdong@uottawa.ca; jaam@qu.edu.qa;
   elsaddik@uottawa.ca
RI Dong, Haiwei/I-1273-2014; /D-4159-2009
OI Dong, Haiwei/0000-0003-1437-7805; /0000-0002-7690-8547
FU NPRP grant from the Qatar National Research Fund (a member of Qatar
   Foundation) [10-0205-170346]
FX This work was made possible by NPRP grant (10-0205-170346) from the
   Qatar National Research Fund (a member of Qatar Foundation). The
   statements made herein are solely the responsibility of the authors.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2014, PROCEDIA IEEE COMPUT, DOI DOI 10.1109/CVPR.2014.233
   [Anonymous], 2016, FACIAL EXPRESSION RE
   [Anonymous], 2018, ACM T MULTIM COMPUT, DOI [10.1145/3176646, DOI 10.1145/3176646]
   [Anonymous], SENTION FRAMEWORK SE
   [Anonymous], 2015, ARXIV150905371
   [Anonymous], 2019, ACM T MULTIMEDIA COM, V15
   [Anonymous], 2005, Int. J. Inf. Technol.
   Cheng F, 2010, IEEE T NEURAL NETWOR, V21, P1685, DOI 10.1109/TNN.2010.2064176
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   Darwin C., 1872, P374
   Ekman P., 2003, UNMASKING FACE GUIDE
   Ekman P., 2012, What the Face Reveals: Basic and Applied Studies of Spontaneous Epression Using the Facial Action Coding System (FACS), DOI [DOI 10.1093/ACPROF:OSO/9780195179644.001.0001, 10.1093/acprof:oso/9780195179644.001.0001]
   El Saddik A, 2018, IEEE MULTIMEDIA, V25, P87, DOI 10.1109/MMUL.2018.023121167
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Huang D, 2011, IEEE T SYST MAN CY C, V41, P765, DOI 10.1109/TSMCC.2011.2118750
   Jiang YX, 2018, IEEE ACCESS, V6, P60128, DOI 10.1109/ACCESS.2018.2876035
   Joshi K, 2013, INT CONF CONTEMP, P1, DOI 10.1109/IC3.2013.6612160
   Khorrami P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P19, DOI 10.1109/ICCVW.2015.12
   Kim BK, 2016, J MULTIMODAL USER IN, V10, P173, DOI 10.1007/s12193-015-0209-0
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar P, 2016, 2016 INTERNATIONAL CONFERENCE ON COMPUTING, ANALYTICS AND SECURITY TRENDS (CAST), P289, DOI 10.1109/CAST.2016.7914982
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liao S, 2006, IEEE IMAGE PROC, P665, DOI 10.1109/ICIP.2006.312418
   Lopes AT, 2015, SIBGRAPI, P273, DOI 10.1109/SIBGRAPI.2015.14
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Mayya V, 2016, PROCEDIA COMPUT SCI, V93, P453, DOI 10.1016/j.procs.2016.07.233
   Mehrabian A., 2008, Communication theory, V6, P193, DOI [10.4324/9781315080918, DOI 10.4324/9781315080918]
   Mistry Kamlesh, 2014, INT C SOFTW KNOWL IN, P1
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424
   Qiu Y, 2019, IEEE T MULTIMEDIA, V21, P1778, DOI 10.1109/TMM.2018.2883866
   Rahulamathavan Y, 2013, IEEE T AFFECT COMPUT, V4, P83, DOI 10.1109/T-AFFC.2012.33
   Rudovic Ognjen, 2018, PERSONALIZED MACHINE
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shih FY, 2008, INT J PATTERN RECOGN, V22, P445, DOI 10.1142/S0218001408006284
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Tang Y., 2013, DEEP LEARNING USING
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Yang HF, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152118
   Yao YQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3131345
   Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435
   Zhang LG, 2011, IEEE T AFFECT COMPUT, V2, P219, DOI 10.1109/T-AFFC.2011.13
   Zhang ZY, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P454, DOI 10.1109/AFGR.1998.670990
NR 53
TC 27
Z9 27
U1 1
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 33
DI 10.1145/3311747
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA IM3ZV
UT WOS:000477935400004
OA Green Published
DA 2024-07-18
ER

PT J
AU Shen, C
   Jin, ZM
   Chu, WQ
   Jiang, RX
   Chen, YW
   Qi, GJ
   Hua, XS
AF Shen, Chen
   Jin, Zhongming
   Chu, Wenqing
   Jiang, Rongxin
   Chen, Yaowu
   Qi, Guo-Jun
   Hua, Xian-Sheng
TI Multi-level Similarity Perception Network for Person Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; multi-level similarity perception; deep
   Siamese architecture; CNN
AB In this article, we propose a novel deep Siamese architecture based on a convolutional neural network (CNN) and multi-level similarity perception for the person re-identification (re-ID) problem. According to the distinct characteristics of diverse feature maps, we effectively apply different similarity constraints to both low-level and high-level feature maps during training stage. Due to the introduction of appropriate similarity comparison mechanisms at different levels, the proposed approach can adaptively learn discriminative local and global feature representations, respectively, while the former is more sensitive in localizing part-level prominent patterns relevant to re-identifying people across cameras. Meanwhile, a novel strong activation pooling strategy is utilized on the last convolutional layer for abstract local-feature aggregation to pursue more representative feature representations. Based on this, we propose final feature embedding by simultaneously encoding original global features and discriminative local features. In addition, our framework has two other benefits: First, classification constraints can be easily incorporated into the framework, forming a unified multi-task network with similarity constraints. Second, as similarity-comparable information has been encoded in the network's learning parameters via back-propagation, pairwise input is not necessary at test time. That means we can extract features of each gallery image and build an index in an off-line manner, which is essential for large-scale real-world applications. Experimental results on multiple challenging benchmarks demonstrate that our method achieves splendid performance compared with the current state-of-the-art approaches.
C1 [Shen, Chen; Jiang, Rongxin; Chen, Yaowu] Zhejiang Univ, Coll Biomed Engn & Instrument Sci, Hangzhou 310027, Zhejiang, Peoples R China.
   [Jin, Zhongming; Hua, Xian-Sheng] Alibaba DAMO Acad, Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China.
   [Chu, Wenqing] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Qi, Guo-Jun] Univ Cent Florida, Dept Comp Sci, Orlando, FL 32816 USA.
C3 Zhejiang University; Alibaba Group; Zhejiang University; State
   University System of Florida; University of Central Florida
RP Shen, C (corresponding author), Zhejiang Univ, Coll Biomed Engn & Instrument Sci, Hangzhou 310027, Zhejiang, Peoples R China.
EM zjushenchen@gmail.com; zhongming.jinzm@alibaba-inc.com;
   wqchu16@gmail.com; rongxinj@zju.edu.cn; cyw@mail.bme.zju.edu.cn;
   guojun.qi@ucf.edu; huaxiansheng@gmail.com
RI Qi, Guo-Jun/AAH-8294-2019
OI Qi, Guo-Jun/0000-0003-3508-1851
FU Fundamental Research Funds for the Central Universities
FX This work is supported in part by the Fundamental Research Funds for the
   Central Universities.
CR [Anonymous], 2016, SURVEY LEARNING HASH
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2017, P CVPR
   [Anonymous], 2015, P CVPR
   [Anonymous], 2016, P CVPR
   [Anonymous], 2015, P CVPR
   [Anonymous], 2016, MULTITASK DEEP NETWO
   Bottou L., 2012, Neural networks: Tricks of the trade, P421, DOI DOI 10.1007/978-3-642-35289-8_25
   Chang Xiaobin, 2018, P CVPR
   Chen D., 2015, P CVPR
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Chen Ying-Cong, 2015, P IJCAI
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Fei Xiong, 2014, P ECCV
   Gheissari Niloofar, 2006, P CVPR
   Girshick R., 2015, IEEE I CONF COMP VIS, DOI [DOI 10.1109/ICCV.2015.169, 10.1109/ICCV.2015.169]
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jose C, 2016, LECT NOTES COMPUT SC, V9909, P875, DOI 10.1007/978-3-319-46454-1_53
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Sheng, 2015, P IJCAI
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li Wei, 2012, AS C COMP VIS ACCV 2, P31
   Li Zhang, 2016, P CVPR
   Liang Zheng, 2015, P ICCV
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu Xiaokai, 2015, P WACV
   Mignon A., 2012, P CVPR
   Moon H, 2001, PERCEPTION, V30, P303, DOI 10.1068/p2896
   Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426
   Prosser B., 2010, P BRIT MACH VIS C BM, DOI DOI 10.5244/C.24.21
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Rui Zhao, 2014, P CVPR
   Rui Zhao, 2013, P ICCV
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Subramaniam Arulkumar, 2016, P NIPS
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tong Xiao, 2016, P CVPR
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Weinberger K.Q., 2006, Advances in neural information processing systems, P1473
   Yang Yang, 2016, P AAAI
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Ying Zhang, 2016, P CVPR
   Zhang KX, 2017, P AMER CONTR CONF, P1942, DOI 10.23919/ACC.2017.7963236
   Zheng Liang, 2016, ARXIV161002984
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
NR 56
TC 8
Z9 8
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 32
DI 10.1145/3309881
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IM3ZV
UT WOS:000477935400003
DA 2024-07-18
ER

PT J
AU Peng, YX
   Qi, JW
AF Peng, Yuxin
   Qi, Jinwei
TI CM-GANs: Cross-modal Generative Adversarial Networks for Common
   Representation Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Generative adversarial network; cross-modal adversarial mechanism;
   common representation learning; cross-modal retrieval
AB It is known that the inconsistent distributions and representations of different modalities, such as image and text, cause the heterogeneity gap, which makes it very challenging to correlate heterogeneous data and measure their similarities. Recently, generative adversarial networks (GANs) have been proposed and have shown their strong ability to model data distribution and learn discriminative representation. It has also been shown that adversarial learning can be fully exploited to learn discriminative common representations for bridging the heterogeneity gap. Inspired by this, we aim to effectively correlate large-scale heterogeneous data of different modalities with the power of GANs to model cross-modal joint distribution. In this article, we propose Cross-modal Generative Adversarial Networks (CM-GANs) with the following contributions. First, a cross-modal GAN architecture is proposed to model joint distribution over the data of different modalities. The inter-modality and intra-modality correlation can be explored simultaneously in generative and discriminative models. Both compete with each other to promote cross-modal correlation learning. Second, the cross-modal convolutional autoencoders with weight-sharing constraint are proposed to form the generative model They not only exploit the cross-modal correlation for learning the common representations but also preserve reconstruction information for capturing the semantic consistency within each modality. Third, a cross-modal adversarial training mechanism is proposed, which uses two kinds of discriminative models to simultaneously conduct intra-modality and inter-modality discrimination. They can mutually boost to make the generated common representations more discriminative by the adversarial training process. In summary, our proposed CM-GAN approach can use GANs to perform cross-modal common representation learning by which the heterogeneous data can be effectively correlated. Extensive experiments are conducted to verify the performance of CM-GANs on cross-modal retrieval compared with 13 state-of-the-art methods on 4 cross-modal datasets.
C1 [Peng, Yuxin; Qi, Jinwei] Peking Univ, Inst Comp Sci & Technol, 128th ZhongGuanCun North St, Beijing 100871, Peoples R China.
C3 Peking University
RP Peng, YX (corresponding author), Peking Univ, Inst Comp Sci & Technol, 128th ZhongGuanCun North St, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
RI peng, yu/GXW-2071-2022; Peng, Yuxin/U-7376-2019
FU National Natural Science Foundation of China [61771025, 61532005]
FX This work was supported by the National Natural Science Foundation of
   China under grant nos. 61771025 and 61532005.
CR Andrew G., 2013, ICML, P1247
   Chen HY, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1236471.1236472
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Denton E., 2015, ADV NEURAL INFORM PR, P1, DOI DOI 10.5555/2969239.2969405
   Feng FX, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808205
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Goodfellow I., 2016, ADV NEURAL INFORM PR, P64
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He L, 2017, IEEE INT CON MULTI, P1153, DOI 10.1109/ICME.2017.8019549
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kim JM, 2012, WORLD J BIOL PSYCHIA, V13, P579, DOI 10.3109/15622975.2011.588247
   Kim Y., 2014, P 2014 C EMP METH NA, P1746, DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li C, 2018, PATTERN RECOGN, V77, P276, DOI 10.1016/j.patcog.2017.12.023
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li D., 2003, P 11 ACM INT C MULTI, P604
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li  Kai, 2018, ACM T MULTIM COMPUT, V14
   Liu T, 2017, IEEE INT CON MULTI, P967, DOI 10.1109/ICME.2017.8019356
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Odena Augustus., 2016, Semi-supervised learning with generative adversarial networks
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2017, FRONT INFORM TECH EL, V18, P44, DOI 10.1631/FITEE.1601787
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Radford A., 2015, ARXIV
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen JL, 2006, IEEE T MULTIMEDIA, V8, P1179, DOI 10.1109/TMM.2006.884618
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   SOCHER R, 2010, PROC CVPR IEEE, P966, DOI DOI 10.1109/CVPR.2010.5540112
   Srivastava Neelam., 2012, The Postcolonial Gramsci, P1
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang HQ, 2018, 2018 14TH IEEE/ASME INTERNATIONAL CONFERENCE ON MECHATRONIC AND EMBEDDED SYSTEMS AND APPLICATIONS (MESA), DOI 10.1145/3204493.3204584
   Wang J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P515, DOI 10.1145/3077136.3080786
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wei YC, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2775109
   Wu F., 2013, P ACM INT C MULT, P877
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Xu X, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P302, DOI 10.1145/2964284.2967231
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Zhai X., 2013, PROC 27 AAAI C ARTIF, P1198
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang HJ, 2018, IEEE T FUZZY SYST, V26, P884, DOI 10.1109/TFUZZ.2017.2697403
NR 62
TC 177
Z9 186
U1 8
U2 61
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
AR 22
DI 10.1145/3284750
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9JB
UT WOS:000459798800006
DA 2024-07-18
ER

PT J
AU Zhao, SC
   Wang, SF
   Soleymani, M
   Joshi, D
   Ji, Q
AF Zhao, Sicheng
   Wang, Shangfei
   Soleymani, Mohammad
   Joshi, Dhiraj
   Ji, Qiang
TI Affective Computing for Large-scale Heterogeneous Multimedia Data: A
   Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Affective computing; emotion recognition; sentiment analysis;
   large-scale multimedia
ID CONTENT REPRESENTATION; MUSIC VIDEO; DATA FUSION; EMOTION; RECOGNITION;
   FEATURES; VALENCE; AROUSAL; CLASSIFICATION; DOMINANCE
AB The wide popularity of digital photography and social networks has generated a rapidly growing volume of multimedia data (i.e., images, music, and videos), resulting in a great demand for managing, retrieving, and understanding these data. Affective computing (AC) of these data can help to understand human behaviors and enable wide applications. In this article, we survey the state-of-the-art AC technologies comprehensively for large-scale heterogeneous multimedia data. We begin this survey by introducing the typical emotion representation models from psychology that are widely employed in AC. We briefly describe the available datasets for evaluating AC algorithms. We then summarize and compare the representative methods on AC of different multimedia types, i.e., images, music, videos, and multimodal data, with the focus on both hand-crafted features-based methods and deep learning methods. Finally, we discuss some challenges and future directions for multimedia affective computing.
C1 [Zhao, Sicheng] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Wang, Shangfei] Univ Sci & Technol China, Hefei 230027, Peoples R China.
   [Soleymani, Mohammad] Univ Southern Calif, Playa Vista, CA 90094 USA.
   [Joshi, Dhiraj] IBM Res AI, Yorktown Hts, NY 10598 USA.
   [Ji, Qiang] Rensselaer Polytech Inst, Troy, NY 12180 USA.
C3 University of California System; University of California Berkeley;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Southern California; Rensselaer Polytechnic
   Institute
RP Wang, SF (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.; Soleymani, M (corresponding author), Univ Southern Calif, Playa Vista, CA 90094 USA.
EM schzhao@gmail.com; sfwang@ustc.edu.cn; soleymani@ict.usc.edu;
   djoshi@us.ibm.com; qji@ecse.rpi.edu
RI Soleymani, Mohammad/AAS-2161-2020
OI Soleymani, Mohammad/0000-0003-2770-7236
FU Berkeley DeepDrive; National Natural Science Foundation of China
   [61701273, 91748129]; National Key RAMP;D Program of China
   [2017YFC011300]; U.S. Army
FX This work was supported by Berkeley DeepDrive, the National Natural
   Science Foundation of China (Nos. 61701273, 91748129), and the National
   Key R&D Program of China (Grant No. 2017YFC011300). The work of MS is
   supported in part by the U.S. Army. Any opinion, content, or information
   presented does not necessarily reflect the position or the policy of the
   United States Government, and no official endorsement should be
   inferred.
CR Abadi MK, 2015, IEEE T AFFECT COMPUT, V6, P209, DOI 10.1109/TAFFC.2015.2392932
   Acar E, 2017, MULTIMED TOOLS APPL, V76, P11809, DOI 10.1007/s11042-016-3618-5
   ALAMEDAPINEDA X, 2016, PROC CVPR IEEE, P5240, DOI DOI 10.1109/CVPR.2016.566
   Aljanaki A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173392
   Aljanaki Anna, 2018, P INT SOC MUS INF RE
   [Anonymous], 2007, MIREX TASK AUDIO MOO
   [Anonymous], 2016, THESIS
   [Anonymous], 2010, 11 INT SOC MUS INF R
   [Anonymous], 2012, PROC CVPR IEEE
   [Anonymous], 2016, LEARNING MULTILEVEL
   [Anonymous], 2014, Comput. Sci.
   [Anonymous], 2013, 10 IEEE INT C WORKSH
   Araque Oscar, 2018, DEPECHEMOOD BILINGUA
   Arifin S, 2008, IEEE T MULTIMEDIA, V10, P1325, DOI 10.1109/TMM.2008.2004911
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Baveye Y, 2018, IEEE T AFFECT COMPUT, V9, P396, DOI 10.1109/TAFFC.2017.2661284
   Baveye Y, 2015, INT CONF AFFECT, P77, DOI 10.1109/ACII.2015.7344554
   Baveye Y, 2015, IEEE T AFFECT COMPUT, V6, P43, DOI 10.1109/TAFFC.2015.2396531
   Ben-Ahmed O, 2018, INT WORK CONTENT MUL
   Benini S, 2011, IEEE T MULTIMEDIA, V13, P1356, DOI 10.1109/TMM.2011.2163058
   Bogdanov D., 2013, P 14 INT SOC MUS INF, P493, DOI DOI 10.1145/2502081.2502229
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Cambria E, 2011, LECT NOTES COMPUT SC, V6677, P601, DOI 10.1007/978-3-642-21111-9_68
   CAMRAS L, 1980, AM J PSYCHOL, V93, P751, DOI 10.2307/1422394
   Chen M, 2018, MULTIMED TOOLS APPL, V77, P3287, DOI 10.1007/s11042-017-5125-8
   Chen T, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P367, DOI 10.1145/2647868.2654935
   Chen XS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2018, DOI 10.1145/3240508.3266434
   Chen YA, 2015, INT CONF ACOUST SPEE, P693, DOI 10.1109/ICASSP.2015.7178058
   Correa J. A. M., 2017, AMIGOS DATASET MOOD
   Dalmia Vaidehi, 2016, COLUMBIA MVSO IMAGE
   Dan-Glauser ES, 2011, BEHAV RES METHODS, V43, P468, DOI 10.3758/s13428-011-0064-1
   Dellandrea E., 2019, ACM SIGMULTIM REC, V10, P6
   Dellandrea Emmanuel, 2017, P MEDIAEVAL WORKSH
   Dellandrea Emmanuel, 2018, P MEDIAEVAL WORKSH
   Dellandrea Emmanuel, 2016, P CEUR WORKSH
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding Y, 2013, 2013 INTERNATIONAL CONFERENCE ON SENSOR NETWORK SECURITY TECHNOLOGY AND PRIVACY COMMUNICATION SYSTEM (SNS & PCS), P13
   Eerola T, 2011, PSYCHOL MUSIC, V39, P18, DOI 10.1177/0305735610362821
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Eyben F., 2013, P 21 ACM INT C MULT, P835, DOI DOI 10.1145/2502081.2502224
   Eyben Florian, 2010, P 18 ACM INT C MULT, P1459
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gui DD, 2018, LECT NOTES COMPUT SC, V10705, P165, DOI 10.1007/978-3-319-73600-6_15
   Guo J, 2019, INFORM FUSION, V51, P224, DOI 10.1016/j.inffus.2019.02.007
   Gupta R, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P317, DOI 10.1145/2911996.2912059
   Han JW, 2015, IEEE T AFFECT COMPUT, V6, P337, DOI 10.1109/TAFFC.2015.2411280
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hansen JHL, 2015, IEEE SIGNAL PROC MAG, V32, P74, DOI 10.1109/MSP.2015.2462851
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Hu WM, 2016, IEEE T MULTIMEDIA, V18, P76, DOI 10.1109/TMM.2015.2496372
   Hu X., 2008, Proceedings of the International Society for Music Information Retrieval Conference, P462
   Hu X., 2007, P 8 INT C MUSIC INFO, P67
   Huang J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6837, DOI 10.1109/ICASSP.2018.8461963
   Huang Z, 2016, J MATER SCI-MATER EL, V27, P10003, DOI 10.1007/s10854-016-5071-7
   Inskip C, 2012, INT J DIGIT LIBRARIE, V12, P137, DOI 10.1007/s00799-012-0084-1
   Jansen BJ, 2009, J AM SOC INF SCI TEC, V60, P2169, DOI 10.1002/asi.21149
   Ji-long Han, 2018, Geofluids, V2018, DOI 10.1155/2018/1985784
   Jiang H, 2012, ICIM2012: PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON INDUSTRIAL MANAGEMENT, P721
   Jiang YG, 2014, AAAI CONF ARTIF INTE, P73
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Jou B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P159, DOI 10.1145/2733373.2806246
   Jou B, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P389, DOI 10.1145/2911996.2912022
   Kahn JH, 2007, AM J PSYCHOL, V120, P263, DOI 10.2307/20445398
   Khorrami P, 2016, IEEE IMAGE PROC, P619, DOI 10.1109/ICIP.2016.7532431
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Lang P.J., 1997, INT AFFECTIVE PICTUR, P39, DOI DOI 10.1027/0269-8803/A000147
   Larson M, 2017, IEEE MULTIMEDIA, V24, P93, DOI 10.1109/MMUL.2017.9
   Lartillot O, 2008, ST CLASS DAT ANAL, P261, DOI 10.1007/978-3-540-78246-9_31
   Lee J, 2011, IEEE T MULTIMEDIA, V13, P1031, DOI 10.1109/TMM.2011.2158530
   Li BJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02116
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Malandrakis N, 2011, INT CONF ACOUST SPEE, P2376
   Malheiro R, 2018, IEEE T AFFECT COMPUT, V9, P240, DOI 10.1109/TAFFC.2016.2598569
   McDuff D, 2017, IEEE INT CONF AUTOMA, P339, DOI 10.1109/FG.2017.49
   McFee B., 2015, P 14 PYTH SCI C, P18, DOI [DOI 10.25080/MAJORA-7B98E3ED-003, 10.25080/Majora-7b98e3ed-003]
   Merler M, 2019, IEEE T MULTIMEDIA, V21, P1147, DOI 10.1109/TMM.2018.2876046
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Mo SS, 2018, NEUROCOMPUTING, V291, P11, DOI 10.1016/j.neucom.2018.02.052
   Morreale F., 2013, Sound and Music Computing Conference, P207
   Muszynski M, 2021, IEEE T AFFECT COMPUT, V12, P36, DOI 10.1109/TAFFC.2019.2902091
   Nemati S, 2017, INTELL DATA ANAL, V21, P427, DOI 10.3233/IDA-160029
   Nemati S, 2016, J INF SCI, V42, P524, DOI 10.1177/0165551515593689
   Nicolaou MA, 2011, IEEE T AFFECT COMPUT, V2, P92, DOI 10.1109/T-AFFC.2011.9
   Niu JW, 2017, IEEE T EMERG TOP COM, V5, P412, DOI 10.1109/TETC.2017.2705341
   Niu JW, 2017, LECT NOTES COMPUT SC, V10133, P15, DOI 10.1007/978-3-319-51814-5_2
   Niu JW, 2016, NEUROCOMPUTING, V173, P339, DOI 10.1016/j.neucom.2015.01.104
   Ortony A., 1988, COGNITIVE STRUCTURE
   Panda Renato Eduardo Silva, 2019, THESIS
   Pandeya Yagya Raj, 2019, DESTECH T COMPUT SCI
   Pang B., 2008, INFORM RETRIEVAL, V2, P1, DOI [10.1561/1500000011, DOI 10.1561/1500000011, https://doi.org/10.1561/1500000011]
   Pang L, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P619, DOI 10.1145/2671188.2749400
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   PENG KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Poria S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P527
   Rajagopalan SS, 2016, LECT NOTES COMPUT SC, V9911, P338, DOI 10.1007/978-3-319-46478-7_21
   Rao TR, 2016, IEEE IMAGE PROC, P634, DOI 10.1109/ICIP.2016.7532434
   Sander D, 2005, NEURAL NETWORKS, V18, P317, DOI 10.1016/j.neunet.2005.03.001
   Sartori A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P311, DOI 10.1145/2733373.2806250
   SCHLOSBERG H, 1954, PSYCHOL REV, V61, P81, DOI 10.1037/h0054570
   She Dongyu, 2019, IEEE T MULTIM
   Shen GY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3838
   Shukla A., 2018, THESIS
   Shukla A., 2017, Proceedings of the 19th ACM International Conference on Multimodal Interaction, P402
   Shukla A, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1148, DOI 10.1145/3123266.3123444
   Shukla A, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P210, DOI 10.1145/3242969.3242988
   Sivaprasad S, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P413, DOI 10.1145/3206025.3206076
   Sjoberg M, MEDIAEVAL 2015 AFFEC, P3
   Smith JR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1799, DOI 10.1145/3123266.3127906
   Soleymani M., 2009, Affective Computing and Intelligent Interaction and Workshops, P1
   Soleymani M., 2013, P 2 ACM INT WORKSH C, P1, DOI DOI 10.1145/2506364.2506365
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Soleymani M, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P919, DOI 10.1145/2733373.2806364
   Soleymani M, 2014, IEEE T MULTIMEDIA, V16, P1075, DOI 10.1109/TMM.2014.2305573
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Speck JacquelinA., 2011, 12th International Society for Music Information Retrieval Conference (ISMIR 2011), number Ismir, P549
   Staiano J, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P427
   Subramanian Ramanathan, 2018, IEEE Transactions on Affective Computing, V9, P147, DOI 10.1109/TAFFC.2016.2625250
   Sun K, 2009, IEEE INT CON MULTI, P566, DOI 10.1109/ICME.2009.5202559
   Tarvainen Jussi, 2018, IEEE T AFFECT COMPUT
   Teixeira RMA, 2012, MULTIMED TOOLS APPL, V61, P21, DOI 10.1007/s11042-010-0702-0
   TORRALBA A, 2011, PROC CVPR IEEE, P1521, DOI DOI 10.1109/CVPR.2011.5995347
   Turnbull Douglas, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P439, DOI 10.1145/1277741.1277817
   Tzanetakis George., 2000, ORGAN SOUND, V4, P2000
   Wang SF, 2019, IEEE T AFFECT COMPUT, V10, P155, DOI 10.1109/TAFFC.2017.2702749
   Wang SF, 2015, IEEE T AFFECT COMPUT, V6, P410, DOI 10.1109/TAFFC.2015.2432791
   Wang SF, 2015, IEEE T AUTON MENT DE, V7, P189, DOI 10.1109/TAMD.2015.2463113
   Wang XH, 2013, IEEE IMAGE PROC, P3230, DOI 10.1109/ICIP.2013.6738665
   Warriner AB, 2013, BEHAV RES METHODS, V45, P1191, DOI 10.3758/s13428-012-0314-x
   Wu RZ, 2013, IEEE INT SOC CONF, P368, DOI 10.1109/SOCC.2013.6749717
   Xing BX, 2019, IEEE ACCESS, V7, P59844, DOI 10.1109/ACCESS.2019.2914872
   Xu BH, 2018, IEEE T AFFECT COMPUT, V9, P255, DOI 10.1109/TAFFC.2016.2622690
   Xu C., 2014, Visual sentiment prediction with deep convolutional neural networks
   Yang JT, 2017, PROCEEDINGS OF THE 10TH INTERNATIONAL JOINT CONFERENCE ON BIOMEDICAL ENGINEERING SYSTEMS AND TECHNOLOGIES, VOL 5: HEALTHINF, P326, DOI 10.5220/0006149503260333
   Yang JF, 2018, PROC CVPR IEEE, P7584, DOI 10.1109/CVPR.2018.00791
   Yang JF, 2017, AAAI CONF ARTIF INTE, P224
   Yang Jufeng, 2018, P AAAI C ART INT
   Yang P, 2010, PROC CVPR IEEE, P2638, DOI 10.1109/CVPR.2010.5539978
   Yang XY, 2018, MULTIMEDIA SYST, V24, P365, DOI 10.1007/s00530-017-0559-4
   Yang Y, 2014, AAAI CONF ARTIF INTE, P306
   Yang YH, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168754
   Yao Xingxu, 2019, P IEEE INT C COMP VI
   Yi Yun, 2018, MULTIMED TOOLS APPL, P1, DOI [DOI 10.1156/2014/752103, 10.1156/2014/752103]
   Yingying Zhu, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9916, P170, DOI 10.1007/978-3-319-48890-5_17
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Zadeh A, 2018, P AAAI C ART INT
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zentner M, 2008, EMOTION, V8, P494, DOI 10.1037/1528-3542.8.4.494
   Zhan Chi, 2019, P IEEE INT C COMP VI
   Zhang KJ, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P135, DOI 10.1145/3206025.3206037
   Zhao SB, 2018, INT C PAR DISTRIB SY, P553, DOI [10.1109/PADSW.2018.8644652, 10.1109/ICPADS.2018.00078]
   Zhao SC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5534
   Zhao SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1319, DOI 10.1145/3240508.3240591
   Zhao SC, 2019, AAAI CONF ARTIF INTE, P2620
   Zhao SC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3233184
   Zhao SC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P369, DOI 10.1145/3123266.3130858
   Zhao SC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4669
   Zhao SC, 2020, IEEE T AFFECT COMPUT, V11, P574, DOI [10.1109/TAFFC.2018.2818685, 10.1109/TAFFC.2016.2628787]
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhao SC, 2015, IEEE IMAGE PROC, P2459, DOI 10.1109/ICIP.2015.7351244
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1025, DOI 10.1145/2647868.2655035
   Zhao Sicheng, 2016, P 24 ACM INT C MULT, P1385, DOI DOI 10.1145/2964284.2964289
   Zhao Sicheng, 2019, P ACM INT C MULT
   Zhong SH, 2019, NEUROCOMPUTING, V332, P224, DOI 10.1016/j.neucom.2018.12.040
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
   Zhu YY, 2019, EXPERT SYST APPL, V128, P316, DOI 10.1016/j.eswa.2019.03.017
   Zlatintsi A, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0194-1
NR 177
TC 38
Z9 38
U1 1
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2019
VL 15
IS 3
SU S
SI SI
AR 93
DI 10.1145/3363560
PG 32
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA LR5EU
UT WOS:000535718800009
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Zhou, C
   Li, ZH
   Osgood, J
   Liu, Y
AF Zhou, Chao
   Li, Zhenhua
   Osgood, Joe
   Liu, Yao
TI On the Effectiveness of Offset Projections for 360-Degree Video
   Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 360-degree video streaming; offset projection; visual quality; adaptive
   streaming
AB A new generation of video streaming technology, 360-degree video, promises greater immersiveness than standard video streams. This level of immersiveness is similar to that produced by virtual reality devices-users can control the field of view using head movements rather than needing to manipulate external devices. Although 360-degree video could revolutionize the streaming experience, its large-scale adoption is hindered by a number of factors: 360-degree video streams have larger bandwidth requirements and require faster responsiveness to user inputs, and users may be more sensitive to lower quality streams.
   In this article, we review standard approaches toward 360-degree video encoding and compare these to families of approaches that distort the spherical surface to allow oriented concentrations of the 360-degree view. We refer to these distorted projections as offset projections. Our measurement studies show that most types of offset projections produce rendered views with better quality than their nonoffset equivalents when view orientations are within 40 or 50 degrees of the offset orientation. Offset projections complicate adaptive 360-degree video streaming because they require a combination of bitrate and view orientation adaptations. We estimate that this combination of streaming adaptation in two dimensions can cause over 57% extra segments to be downloaded compared to an ideal downloading strategy, wasting 20% of the total downloading bandwidth.
C1 [Zhou, Chao; Osgood, Joe; Liu, Yao] SUNY Binghamton, Dept Comp Sci, POB 6000, Binghamton, NY 13902 USA.
   [Li, Zhenhua] Tsinghua Univ, East Main Bldg,Area 11,Room 212, Beijing 100084, Peoples R China.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Binghamton; Tsinghua University
RP Zhou, C (corresponding author), SUNY Binghamton, Dept Comp Sci, POB 6000, Binghamton, NY 13902 USA.
EM czhou5@binghamton.edu; lizhenhua1983@tsinghua.edu.cn;
   josgood2@binghamton.edu; yaoliu@binghamton.edu
RI Li, Zhenhua/AAS-8358-2020
FU Direct For Computer & Info Scie & Enginr; Division Of Computer and
   Network Systems [1618931] Funding Source: National Science Foundation
CR Akamai, 2016, AK STAT INT Q1 2016
   [Anonymous], Internet connection speed recommendations
   [Anonymous], 2007, Transformations and Projections in Computer Graphics
   [Anonymous], IEEE INT C COMM ICC
   [Anonymous], P ACM MULT C
   Corbillon Xavier, 2017, P ACM MULT MM 17
   Gotz D, 2004, MULTIMEDIA 04, P612
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   ISO/ IEC, 2014, 26300912014 ISOIEC
   Krishnan S., 2007, MULTIMEDIA 07, P1026
   Kuzyakov E., 2017, ENHANCING HIGH RESOL
   Kuzyakov E., 2015, Under the hood: Building 360 video
   Kuzyakov E., 2016, Next-generation video encoding techniques for 360 video and VR-Engineering at Meta
   Matt C. Yu, 2015, SPHERE 655362
   Ochi D, 2015, P IEEE VIRT REAL ANN, P349, DOI 10.1109/VR.2015.7223439
   OneRepublic, 2016, ONEREPUBLIC KIDS
   OpenCV, OPENCV RES
   Patney A., 2016, ACM SIGGRAPH 2016 emerging technologies, P1, DOI [10.1145/2929464.2929472, DOI 10.1145/2929464.2929472]
   Qian Feng, 2007, P 5 WORKSH ALL THING, P1
   RobotShop, LYNXM PAN TILT KIT
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weisstein Eric W., EQUIRECTANGULAR PROJ
   Yang Z, 2006, NAS: 2006 INTERNATIONAL WORKSHOP ON NETWORKING, ARCHITECTURE, AND STORAGES, PROCEEDINGS, P73
   Zare A., 2016, P 24 ACM INT C MULT, P601, DOI DOI 10.1145/2964284.2967292
   Zhou Chao, 2017, P 8 INT C MULT SYST
NR 25
TC 9
Z9 10
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
SU S
AR 62
DI 10.1145/3209660
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG9GD
UT WOS:000455314500010
OA Bronze
DA 2024-07-18
ER

PT J
AU Burger, V
   Zinner, T
   Lam, DX
   Wamser, F
   Phuoc, TG
AF Burger, Valentin
   Zinner, Thomas
   Lam Dinh-Xuan
   Wamser, Florian
   Phuoc Tran-Gia
TI A Generic Approach to Video Buffer Modeling Using Discrete-Time Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adaptive video streaming; quality of experience; buffering policy;
   band-width variation; performance analysis
ID EXPERIENCE; QUALITY; ADAPTATION; SERVICE; POLICY; QUEUE
AB The large share of traffic in the Internet generated by video streaming services puts high loads on access and aggregation networks, resulting in high costs for the content delivery infrastructure. To reduce the bandwidth consumed while maintaining a high playback quality, video players use policies that control and limit the buffer level by using thresholds for pausing and continuing the video download. This allows shaping the bandwidth consumed by video streams and limiting the traffic wasted in case of playback abortion. Especially in mobile scenarios, where the throughput can be highly variant, the buffer policy can have a high impact on the probability of interruptions during video playback. To find the optimal setting for the buffer policy in each network condition, the relationship between the parameters of the buffer policy, the network throughput dynamics, and the corresponding video playback behavior needs to be understood. To this end, we model the video buffer as GI/GI/1 queue with pq-policy using discrete-time analysis. By studying the stochastic properties of the buffer-level distribution, we are able to accurately evaluate the impact of network and video bitrate dynamics on the video playback quality based on the buffer policy. We find a fundamental relationship between the bandwidth variation and the expected interarrival time of segments, meaning that overproportionately more bandwidth is necessary to prevent stalling events for high bandwidth variation. The proposed model further allows to optimize the trade-off between the traffic wasted in case of video abortion and video streaming quality experienced by the user.
C1 [Burger, Valentin; Zinner, Thomas; Lam Dinh-Xuan; Wamser, Florian; Phuoc Tran-Gia] Univ Wurzburg, Inst Comp Sci, Wurzburg, Germany.
C3 University of Wurzburg
RP Burger, V (corresponding author), Univ Wurzburg, Inst Comp Sci, Wurzburg, Germany.
EM valentin.burger@informatik.uni-wuerzburg.de;
   zinner@informatik.uni-wuerzburg.de;
   lam.dinh-xuan@informatik.uni-wuerzburg.de;
   florian.wamser@informatik.uni-wuerzburg.de;
   trangia@informatik.uni-wuerzburg.de
RI Dinh-Xuan, Lam/JFS-5687-2023; Zinner, Thomas/AAA-6004-2019; Wamser,
   Florian/AAM-7320-2021
OI Dinh-Xuan, Lam/0000-0001-5550-6189; Zinner, Thomas/0000-0002-4179-4105;
   Wamser, Florian/0000-0002-0356-6291
FU Deutsche Forschungsgemeinschaft (DFG) [ZI 1334/2-1, TR 257/43-1]
FX This work was partly funded by Deutsche Forschungsgemeinschaft (DFG)
   under grants ZI 1334/2-1 and TR 257/43-1. The authors alone are
   responsible for the content of the article.
CR [Anonymous], SPIE OPT ENG APPL IN
   [Anonymous], 2014, P 2014 WORKSHOP DESI
   [Anonymous], 1999, P910 ITUT
   [Anonymous], P 2015 IFIP IEEE INT
   BALACHANDRAN KR, 1975, MANAGE SCI, V21, P1073
   Bao W, 2015, IEEE ICC, P3076, DOI 10.1109/ICC.2015.7248796
   Benaroya H., 2005, PROBABILITY MODELS E, V193
   BOHM W, 1993, J STAT PLAN INFER, V34, P23, DOI 10.1016/0378-3758(93)90031-Z
   BOXMA OJ, 1976, MANAGE SCI, V22, P916, DOI 10.1287/mnsc.22.8.916
   Cisco, 2017, CISC VIS NETW IND GL
   Fiedler M, 2010, IEEE NETWORK, V24, P36, DOI 10.1109/MNET.2010.5430142
   Hemmiger Stephen, 2005, P LIN C
   HEYMAN DP, 1977, MANAGE SCI, V23, P775, DOI 10.1287/mnsc.23.7.775
   HINKLEY DV, 1969, BIOMETRIKA, V56, P635, DOI 10.1093/biomet/56.3.635
   Hobfeld T., 2016, P 5 ISCA DEGA WORKSH
   Hossfeld T., 2013, DATA TRAFFIC MONITOR
   Hossfeld Tobias, 2012, P 2012 4 INT WORKSH
   Hossfeld Tobias, 2014, P 2014 6 INT WORKSH
   Hossfeld Tobias, 2013, P 5 WORKSH MOB VID
   Hossfeld Tobias, 2011, P 2011 IEEE INT S MU
   James C, 2016, I S MOD ANAL SIM COM, P331, DOI 10.1109/MASCOTS.2016.75
   Le Callet P., 2012, 1003 COST IC EUR NET
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liu Y, 2015, IEEE T BROADCAST, V61, P651, DOI 10.1109/TBC.2015.2460611
   Luan TH, 2010, IEEE T MULTIMEDIA, V12, P64, DOI 10.1109/TMM.2009.2036294
   MEKKI Sami, 2017, ARXIV170508809
   Moldovan Christian, 2016, P 2016 28 INT TEL C
   Ramos-Munoz JJ, 2014, IEEE WIREL COMMUN, V21, P18, DOI 10.1109/MWC.2014.6757893
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Stohr D, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1113, DOI 10.1145/3123266.3123426
   TRANGIA P, 1993, P 8 AUSTR TEL RES SE
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wamser F, 2016, COMPUT NETW, V109, P211, DOI 10.1016/j.comnet.2016.03.020
   Xie XF, 2015, MOBICOM '15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P413, DOI 10.1145/2789168.2790118
   YADIN M, 1967, NAV RES LOGIST Q, V14, P43, DOI 10.1002/nav.3800140105
   Yang J, 2016, IEEE T VEH TECHNOL, V65, P643, DOI 10.1109/TVT.2015.2398515
   Yang J, 2011, IEEE T MULTIMEDIA, V13, P1141, DOI 10.1109/TMM.2011.2160158
   Zhang Runtong., 2005, Fuzzy Control of Queuing Systems
NR 39
TC 15
Z9 16
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 33
DI 10.1145/3183511
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GI6TD
UT WOS:000434634700006
DA 2024-07-18
ER

PT J
AU Motamedi, M
   Gysel, P
   Ghiasi, S
AF Motamedi, Mohammad
   Gysel, Philipp
   Ghiasi, Soheil
TI PLACID: A Platform for FPGA-Based Accelerator Creation for DCNNs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Convolutional neural networks; deep learning; accelerator design; design
   automation
ID COPROCESSOR
AB Deep Convolutional Neural Networks (DCNNs) exhibit remarkable performance in a number of pattern recognition and classification tasks. Modern DCNNs involve many millions of parameters and billions of operations. Inference using such DCNNs, if implemented as software running on an embedded processor, results in considerable execution time and energy consumption, which is prohibitive in many mobile applications. Field-programmable gate array (FPGA)-based acceleration of DCNN inference is a promising approach to improve both energy consumption and classification throughput. However, the engineering effort required for development and verification of an optimized FPGA-based architecture is significant.
   In this article, we present PLACID, an automated PLatform for Accelerator CreatIon for DCNNs. PLACID uses an analytical approach to characterization and exploration of the implementation space. PLACID enables generation of an accelerator with the highest throughput for a given DCNN on a specific target FPGA platform. Subsequently, it generates an RTL level architecture in Verilog, which can be passed onto commercial tools for FPGA implementation. PLACID is fully automated, and reduces the accelerator design time from a few months down to a few hours. Experimental results show that architectures synthesized by PLACID yield 2x higher throughput density than the best competing approach.
C1 [Motamedi, Mohammad; Gysel, Philipp; Ghiasi, Soheil] Univ Calif Davis, Elect & Comp Engn Dept, One Shields Ave, Davis, CA 95616 USA.
C3 University of California System; University of California Davis
RP Motamedi, M (corresponding author), Univ Calif Davis, Elect & Comp Engn Dept, One Shields Ave, Davis, CA 95616 USA.
EM mmotamedi@ucdavis.edu; pmgysel@ucdavis.edu; ghi-asi@ucdavis.edu
FU National Science Foundation (NSF) [CCF-1346812]; Seattle Foundation;
   Division of Computing and Communication Foundations; Direct For Computer
   & Info Scie & Enginr [1346812] Funding Source: National Science
   Foundation
FX This work is partly funded by National Science Foundation (NSF) under
   Grant No. CCF-1346812 and the Seattle Foundation. We thank Altera for
   providing the FPGAs that made this research possible. Moreover, we thank
   NVIDIA for donating GPUs, which were used for convnet compression. We
   are also grateful to Mr. Terry O'Neil for his insightful comments.
CR Abadi M, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467
   [Anonymous], P 2015 ACM SIGDA INT
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2015, CORR
   [Anonymous], 2014, TRAINING DEEP NEURAL
   [Anonymous], 2011, P BIGLEARNING WORKSH
   [Anonymous], 2016, ARXIV160305279
   [Anonymous], ARXIV00000000
   [Anonymous], GOOGLE BLOG 0518
   [Anonymous], 2015, CORR
   Cadambi S, 2010, PACT 2010: PROCEEDINGS OF THE NINETEENTH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P273, DOI 10.1145/1854273.1854309
   Chakradhar S, 2010, CONF PROC INT SYMP C, P247, DOI 10.1145/1816038.1815993
   Chen TS, 2014, ACM SIGPLAN NOTICES, V49, P269, DOI 10.1145/2541940.2541967
   Chen YJ, 2014, INT SYMP MICROARCH, P609, DOI 10.1109/MICRO.2014.58
   Collobert R, 2011, BIGL NIPS WORKSH
   Cong Jason, 2014, Artificial Neural Networks and Machine Learning - ICANN 2014. 24th International Conference on Artificial Neural Networks. Proceedings: LNCS 8681, P281, DOI 10.1007/978-3-319-11179-7_36
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Iandola F.N., 2016, PROC INT C LEARN
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Moore E.F., 1956, Automata studies, P129
   Motamedi M, 2016, ASIA S PACIF DES AUT, P575, DOI 10.1109/ASPDAC.2016.7428073
   Peemen M, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P13, DOI 10.1109/ICCD.2013.6657019
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sankaradas M, 2009, IEEE INT CONF ASAP, P53, DOI 10.1109/ASAP.2009.25
   Simonyan K., 2014, 14091556 ARXIV
   Suda N, 2016, PROCEEDINGS OF THE 2016 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS (FPGA'16), P16, DOI 10.1145/2847263.2847276
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Zhang C, 2015, PROCEEDINGS OF THE 2015 SYMPOSIUM ON PIEZOELECTRICITY, ACOUSTIC WAVES AND DEVICE APPLICATIONS, P161, DOI 10.1109/SPAWDA.2015.7364463
NR 30
TC 19
Z9 23
U1 1
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 62
DI 10.1145/3131289
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300016
DA 2024-07-18
ER

PT J
AU Zhang, C
   Liu, JC
   Wang, HY
AF Zhang, Cong
   Liu, Jiangchuan
   Wang, Haiyang
TI Cloud-Assisted Crowdsourced Livecast
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Crowdsourced livecast; public clouds; workload migration; resource
   allocation
AB The past two years have witnessed an explosion of a new generation of livecast services, represented by Twitch. tv, GamingLive, and Dailymotion, to name but a few. With such a livecast service, geo-distributed Internet users can broadcast any event in real-time, for example, game, cooking, drawing, and so on, to viewers of interest. Its crowdsourced nature enables rich interactions among broadcasters and viewers but also introduces great challenges to accommodate their great scales and dynamics. To fulfill the demands from a large number of heterogeneous broadcasters and geo-distributed viewers, expensive server clusters have been deployed to ingest and transcode live streams. Yet our Twitch-based measurement shows that a significant portion of the unpopular and dynamic broadcasters are consuming considerable system resources; in particular, 25% of bandwidth resources and 30% of computational capacity are used by the broadcasters who do not have any viewers at all. In this article, through the real-world measurement and data analysis, we show that the public cloud has great potentials to address these scalability challenges. We accordingly present the design of Cloud-assisted Crowdsourced Livecast (CACL) and propose a comprehensive set of solutions for broadcaster partitioning. Our trace-driven evaluations show that our CACL design can smartly assign ingesting and transcoding tasks to the elastic cloud virtual machines, providing flexible and cost-effective system deployment.
C1 [Zhang, Cong; Liu, Jiangchuan] Simon Fraser Univ, Sch Comp Sci, 8888 Univ Dr, Burnaby, BC V5A 1S6, Canada.
   [Wang, Haiyang] Univ Minnesota, Dept Comp Sci, 1049 Univ Dr, Duluth, MN 55812 USA.
C3 Simon Fraser University; University of Minnesota System; University of
   Minnesota Duluth
RP Zhang, C (corresponding author), Simon Fraser Univ, Sch Comp Sci, 8888 Univ Dr, Burnaby, BC V5A 1S6, Canada.
EM congz@cs.sfu.ca; jcliu@cs.sfu.ca; haiyang@d.umn.edu
FU NPRP grant from the Qatar National Research Fund (a member of Qatar
   Foundation) [8-519-1-108]; Natural Sciences and Engineering Research
   Natural Sciences and Engineering Research Council (NSERC) of Canada;
   University of Minnesota
FX This publication was made possible by NPRP grant #[8-519-1-108] from the
   Qatar National Research Fund (a member of Qatar Foundation). The
   findings achieved herein are solely the responsibility of the authors.
   Dr. J. Liu's work is also supported by the Natural Sciences and
   Engineering Research Natural Sciences and Engineering Research Council
   (NSERC) of Canada. H. Wang's work was supported by Chancellor's Small
   Grant and Grant-in-aid programs from the University of Minnesota.
CR Adhikari V. K., 2012, P IEEE INFOCOM
   [Anonymous], 1998, J OPERATIONAL RES SO
   Aparicio-Pardo R., 2015, P ACM MMSYS
   Cheng X, 2013, IEEE T MULTIMEDIA, V15, P1184, DOI 10.1109/TMM.2013.2265531
   Cotta C., 1998, HYBRID GENETIC ALGOR, P250
   DREXL A, 1988, COMPUTING, V40, P1, DOI 10.1007/BF02242185
   El Essaili A, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2801124
   Hajjat Mohammad, 2015, P IEEE INFOCOM
   Jia AL, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2957750
   Kaytoue Mehdi, 2012, P ACM WWW
   Li Chen, 2016, P IEEE IC2E
   Li Z., 2012, P ACM NOSSDAV
   Liu ZM, 2009, LECT NOTES COMPUT SC, V5550, P274
   Ma H., 2014, P ACM MMSYS
   Niu YP, 2015, IEEE INFOCOM SER
   Shea Ryan, 2015, P ACM NOSSDAV
   Simoens Pieter, 2013, P ACM MOBISYS
   Wang FX, 2016, NANOSCALE HORIZ, V1, P272, DOI 10.1039/c5nh00116a
   Wu Yu, 2013, P ACM SIGCOMM WORKSH
   Xu F, 2014, IEEE T COMPUT, V63, P3012, DOI 10.1109/TC.2013.185
   Zhang C, 2015, P ACM NOSSDAV
   Zhang Cong, 2016, P ACM NOSSDAV
NR 22
TC 12
Z9 12
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
SU S
AR 46
DI 10.1145/3095755
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FP1UO
UT WOS:000417400400014
DA 2024-07-18
ER

PT J
AU Wang, K
   Mi, J
   Xu, CH
   Zhu, QQ
   Shu, L
   Deng, DJ
AF Wang, Kun
   Mi, Jun
   Xu, Chenhan
   Zhu, Qingquan
   Shu, Lei
   Deng, Der-Jiunn
TI Real-Time Load Reduction in Multimedia Big Data for Mobile Internet
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimedia; big data; mobile Internet; real-time; load reduction;
   networking
ID SEGMENTATION; NETWORKS; MOTION; QOE
AB In the age of multimedia big data, the popularity of mobile devices has been in an unprecedented growth, the speed of data increasing is faster than ever before, and Internet traffic is rapidly increasing, not only in volume but also in heterogeneity. Therefore, data processing and network overload have become two urgent problems. To address these problems, extensive papers have been published on image analysis using deep learning, but only a few works have exploited this approach for video analysis. In this article, a hybrid-stream model is proposed to solve these problems for video analysis. Functionality of this model covers Data Preprocessing, Data Classification, and Data-Load-Reduction Processing. Specifically, an improved Convolutional Neural Networks (CNN) classification algorithm is designed to evaluate the importance of each video frame and video clip to enhance classification precision. Then, a reliable keyframe extraction mechanism will recognize the importance of each frame or clip, and decide whether to abandon it automatically by a series of correlation operations. The model will reduce data load to a dynamic threshold changed by sigma, control the input size of the video in mobile Internet, and thus reduce network overload. Through experimental simulations, we find that the size of processed video has been effectively reduced and the quality of experience (QoE) has not been lowered due to a suitably selected parameter.. The simulation also shows that the model has a steady performance and is powerful enough for continuously growing multimedia big data.
C1 [Wang, Kun; Mi, Jun; Xu, Chenhan; Zhu, Qingquan] Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Networ, Nanjing 210003, Jiangsu, Peoples R China.
   [Shu, Lei] Guangdong Univ Petrochem Technol, Guangdong Prov Key Lab Petrochem Equipment Fault, Maoming 525000, Guangdong, Peoples R China.
   [Deng, Der-Jiunn] Natl Changhua Univ Educ, Changhua 500, Taiwan.
C3 Nanjing University of Posts & Telecommunications; Guangdong University
   of Petrochemical Technology; National Changhua University of Education
RP Wang, K (corresponding author), Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Networ, Nanjing 210003, Jiangsu, Peoples R China.
EM kwang@njupt.edu.cn; junmiia@163.com; xchank@outlook.com;
   njuptzqq@163.com; lei.shu@ieee.org; djdeng@cc.ncue.edu.tw
RI Shu, Lei/JQW-2386-2023
OI Shu, Lei/0000-0002-6700-9347
FU NSFC [61572262, 61100213, 61571233, 61373135, 61572172]; SFDPH
   [20113223120007]; NSF of Jiangsu Province [BK20141427]; NUPT [NY214097];
   Priority Academic Program Development of Jiangsu Higher Education
   Institutions; Open Research Fund of Key Lab of Broadband Wireless
   Communication and Sensor Network Technology (NUPT), Ministry of
   Education [NYKL201507]; Jiangsu Qing Lan Project; Educational Commission
   of Guangdong Province [2013KJCX0131]; Guangdong High-Tech Development
   Fund [2013B010401035]; International and Hong Kong, Macao & Taiwan
   collaborative innovation platform and major international cooperation
   projects of colleges in Guangdong Province [2015KGJHZ026]; Guangdong
   Province Outstanding Young Professor Project; Top Level Talents Project
   in the Sailing Plan of Guangdong Province
FX Support was provided by NSFC (61572262, 61100213, 61571233, 61373135,
   61572172); SFDPH (20113223120007); NSF of Jiangsu Province (BK20141427),
   NUPT (NY214097); Priority Academic Program Development of Jiangsu Higher
   Education Institutions; Open Research Fund of Key Lab of Broadband
   Wireless Communication and Sensor Network Technology (NUPT), Ministry of
   Education (NYKL201507); Jiangsu Qing Lan Project; Educational Commission
   of Guangdong Province (2013KJCX0131); Guangdong High-Tech Development
   Fund (2013B010401035); International and Hong Kong, Macao & Taiwan
   collaborative innovation platform and major international cooperation
   projects of colleges in Guangdong Province (No. 2015KGJHZ026); and the
   2014 Guangdong Province Outstanding Young Professor Project and 2013 Top
   Level Talents Project in the Sailing Plan of Guangdong Province.
CR Al-Fares M., 2010, NSDI, P19
   Alizadeh M, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P503, DOI 10.1145/2619239.2626316
   Baktashmotlagh M, 2014, IEEE T PATTERN ANAL, V36, P2353, DOI 10.1109/TPAMI.2014.2339851
   Chen BJ, 2015, J MATH IMAGING VIS, V51, P124, DOI 10.1007/s10851-014-0511-6
   Chiang DJ, 2015, IEEE INT CONF MO, P383, DOI [10.1109/MobServ.2015.59, 10.1109/MS.2015.59]
   De Rango F, 2009, WORKSHOP ON BIO-INSPIRED ALGORITHMS FOR DISTRIBUTED SYSTEMS - BADS 2009, P77
   Dong MX, 2015, IEEE WIREL COMMUN, V22, P50, DOI 10.1109/MWC.2015.7224727
   Dong MX, 2015, IEEE NETWORK, V29, P40, DOI 10.1109/MNET.2015.7166189
   Dong MX, 2014, IEICE T INF SYST, VE97D, P2606, DOI 10.1587/transinf.2013THP0011
   Fu ZJ, 2016, IEEE T PARALL DISTR, V27, P2546, DOI 10.1109/TPDS.2015.2506573
   Gu B, 2017, IEEE T NEUR NET LEAR, V28, P1646, DOI 10.1109/TNNLS.2016.2544779
   Han GJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16020246
   Hava A, 2015, IEEE T BROADCAST, V61, P238, DOI 10.1109/TBC.2015.2400811
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He KQ, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P465, DOI 10.1145/2785956.2787507
   Hernandez P., 2010, THESIS
   Hussein SH, 2005, IEEE J SEL AREA COMM, V23, P2248, DOI 10.1109/JSAC.2005.857205
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang Y., 2011, P ACM INT C MULT RET
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kroon R., 2002, THESIS
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lakhina A, 2005, ACM SIGCOMM COMP COM, V35, P217, DOI 10.1145/1090191.1080118
   Lee SK, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2771438
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   Morris BT, 2008, IEEE T INTELL TRANSP, V9, P425, DOI 10.1109/TITS.2008.922970
   Mrázová I, 2008, IEEE INTL CONF IND I, P439
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Qin T., 2009, P INT C COMM DRESD G, P1
   Qin T, 2010, 2010 8TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P147, DOI 10.1109/WCICA.2010.5553902
   Rao WX, 2015, IEEE T MOBILE COMPUT, V14, P755, DOI 10.1109/TMC.2014.2330296
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Song RN, 2014, INT CONF CLOUD COMPU, P187, DOI 10.1109/CCIS.2014.7175727
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Wang H, 2015, EURASIP J WIREL COMM, DOI 10.1186/s13638-015-0270-0
   Wang K, 2016, IEEE T IND INFORM, V12, P2091, DOI 10.1109/TII.2016.2537788
   Wang K, 2016, IEEE SENS J, V16, P4051, DOI 10.1109/JSEN.2015.2428712
   Wang K, 2016, IEEE NETWORK, V30, P36, DOI 10.1109/MNET.2016.7389829
   Wang K, 2015, IEEE COMMUN MAG, V53, P56, DOI 10.1109/MCOM.2015.7010516
   Wang K, 2013, INT J AD HOC UBIQ CO, V13, P197, DOI 10.1504/IJAHUC.2013.055453
   Wang KW, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/910260
   Xia F, 2012, INT J COMMUN SYST, V25, P1101, DOI 10.1002/dac.2417
   Xia ZH, 2016, MULTIMED TOOLS APPL, V75, P1947, DOI 10.1007/s11042-014-2381-8
   Ye H, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P435, DOI 10.1145/2671188.2749406
   Zeb K, 2014, 2014 FOURTH INTERNATIONAL CONFERENCE ON INNOVATIVE COMPUTING TECHNOLOGY (INTECH), P52, DOI 10.1109/INTECH.2014.6927746
   Zhang Y, 2011, IEEE COMMUN MAG, V49, P44, DOI 10.1109/MCOM.2011.5741145
   Zheng YH, 2015, J INTELL FUZZY SYST, V28, P961, DOI 10.3233/IFS-141378
NR 48
TC 38
Z9 38
U1 0
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2016
VL 12
IS 5
SU S
AR 76
DI 10.1145/2990473
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EJ0VT
UT WOS:000392929700007
DA 2024-07-18
ER

PT J
AU Cheung, M
   She, J
AF Cheung, Ming
   She, James
TI Evaluating the Privacy Risk of User-Shared Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Big data; privacy; de-anonymization; user-shared images; social network
   analysis
ID SOCIAL NETWORKS
AB User-shared images are shared on social media about a user's life and interests that are widely accessible to others due to their sharing nature. Unlike for online profiles and social graphs, most users are unaware of the privacy risks relating to shared images, as they do not directly disclose characteristics such as gender and origin. Recently, however, user-shared images have been proven to be an accessible alternative to social graphs for online friendship recommendation and gender identification. This article evaluates 1.6M user-shared images from an image-oriented social network, Fotolog, and concludes how they can create privacy risks by proposing a system for de-anonymization, as well as inferring information on online profiles with the user-shared images. It is concluded that given user-shared images, using social graphs is 2 and 2.5 times more effective in de-anonymization than using origins or genders. With two showcases, it is also proven that using user-shared images is effective in online friendship recommendation, gender identification, and origin inference. To the best of our knowledge, this is the first article to evaluate the privacy issue qualitatively with big multimedia data from a real social network.
C1 [Cheung, Ming; She, James] Hong Kong Univ Sci & Technol, Elect & Comp Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Cheung, M (corresponding author), Hong Kong Univ Sci & Technol, Elect & Comp Engn, Hong Kong, Hong Kong, Peoples R China.
EM cpming@ust.hk; eejames@ust.hk
OI Cheung, Ming/0000-0003-4646-1980
FU HKUST-NIE Social Media Lab
FX This work was supported by the HKUST-NIE Social Media Lab.
CR Ahern S, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P357
   [Anonymous], 2011, INT C WORLD WIDE WEB, DOI DOI 10.1145/1963405.1963481
   [Anonymous], 2005, P 2005 ACM SIGMOD IN
   Balduzzi M, 2010, LECT NOTES COMPUT SC, V6307, P422, DOI 10.1007/978-3-642-15512-3_22
   Calandrino JA, 2011, P IEEE S SECUR PRIV, P231, DOI 10.1109/SP.2011.40
   Campisi P., 2009, P 16 INT C DIG SIGN, P1
   Cheung M., 2014, P 4 INT C ADV INF MI, P83
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Gilbert E, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P211
   Golder Scott A., 2010, Proceedings of the 2010 IEEE Second International Conference on Social Computing (SocialCom 2010). the Second IEEE International Conference on Privacy, Security, Risk and Trust (PASSAT 2010), P88, DOI 10.1109/SocialCom.2010.22
   Guo B, 2015, IEEE T MOBILE COMPUT, V14, DOI 10.1109/TMC.2014.2385097
   Guy I., 2009, P 3 ACM C REC SYST, P53
   Hsu W., 2006, AAAI SPRING S SERIES
   Jain P, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P1259
   Jie ZM, 2015, SYMP NETW CLOUD, P103, DOI 10.1109/NCCA.2015.25
   Jin EM, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.046132
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Kleinberg J.M., 2007, Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, P4
   Korayem Mohammed., 2013, ICWSM
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   Liu Wendy, 2013, P AAAI SPRING S AN M
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lü LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027
   McCallum A., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P169, DOI 10.1145/347090.347123
   Ming Cheung, 2015, P IEEE INT C CYB PHY
   Mislove A, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P29
   Moxley E, 2009, IEEE INT CON MULTI, P1452, DOI 10.1109/ICME.2009.5202776
   Narayanan A, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1825, DOI 10.1109/IJCNN.2011.6033446
   Narayanan A, 2009, P IEEE S SECUR PRIV, P173, DOI 10.1109/SP.2009.22
   Parimi R, 2011, LECT NOTES ARTIF INT, V6635, P75, DOI 10.1007/978-3-642-20847-8_7
   Song Yi, 2011, P 13 INT C INF INT W, P246
   Thomas K, 2010, LECT NOTES COMPUT SC, V6205, P236, DOI 10.1007/978-3-642-14527-8_14
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wondracek G, 2010, P IEEE S SECUR PRIV, P223, DOI 10.1109/SP.2010.21
   Xing Xie, 2010, Proceedings of the 2010 IEEE/ACM Int'l Conference on Green Computing and Communications (GreenCom) and Int'l Conference on Cyber, Physical and Social Computing (CPSCom), P831, DOI 10.1109/GreenCom-CPSCom.2010.28
   You QZ, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOP (ICDMW), P1026, DOI 10.1109/ICDMW.2014.93
   You Quanzeng, 2015, SIGN PROCESS
   Yu ZW, 2016, IEEE T HUM-MACH SYST, V46, P151, DOI 10.1109/THMS.2015.2446953
   Zhang K, 2014, IEEE COMMUN MAG, V52, P58, DOI 10.1109/MCOM.2014.6766086
   Zhang XM, 2013, SIGNAL PROCESS, V93, P2178, DOI 10.1016/j.sigpro.2012.05.021
   Zheleva E., 2009, P 18 INT C WORLD WID, P531, DOI [10.1145/1526709.1526781, DOI 10.1145/1526709.1526781]
NR 41
TC 6
Z9 8
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2016
VL 12
IS 4
SU S
AR 58
DI 10.1145/2978568
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8DS
UT WOS:000392736100002
DA 2024-07-18
ER

PT J
AU Ouyang, JL
   Wen, XZ
   Liu, JX
   Chen, JJ
AF Ouyang, Junlin
   Wen, Xingzi
   Liu, Jianxun
   Chen, Jinjun
TI Robust Hashing Based on Quaternion Zernike Moments for Image
   Authentication
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Robust image hashing; quaternion Zernike moments; multimedia
   authentication; image forensics
ID TRANSFORM
AB The reliability and security of multimedia contents in transmission, communications, storage, and usage have attracted special attention. Robust image hashing, also referred to as perceptual image hashing, is widely applied in multimedia authentication and forensics, image retrieval, image indexing, and digital image watermarking. In this work, a novel robust image hashing method based on quaternion Zernike moments (QZMs) is proposed. QZMs offer a sound way to jointly deal with the three channels of color images without discarding chrominance information; the generated hash is thus shorter than the hash of three channels separately processing. The proposed approach's performance was evaluated on the color images database of UCID and compared with several recent and efficient methods. These experiments show that the proposed scheme provides a short hash in length that is robust to most common image content-preserving manipulations like JPEG compression, filtering, noise, scaling, and large angle rotation operations.
C1 [Ouyang, Junlin; Wen, Xingzi; Liu, Jianxun; Chen, Jinjun] Hunan Univ Sci & Technol, Coll Hunan Prov, Key Lab Knowledge Proc & Networked Mfg, Xiangtan 411201, Peoples R China.
   [Ouyang, Junlin; Chen, Jinjun] Univ Technol Sydney, Fac Engn & IT, Sydney, NSW 2007, Australia.
C3 Hunan University of Science & Technology; University of Technology
   Sydney
RP Ouyang, JL (corresponding author), Hunan Univ Sci & Technol, Coll Hunan Prov, Key Lab Knowledge Proc & Networked Mfg, Xiangtan 411201, Peoples R China.; Ouyang, JL (corresponding author), Univ Technol Sydney, Fac Engn & IT, Sydney, NSW 2007, Australia.
EM yangjunlin0732@163.com; wenxingzi1980@aliyun.com; ljx529@gmail.com;
   jinjun.chen@gmail.com
RI Chen, Jinjun/AAP-2361-2020
OI Ouyang, Junlin/0000-0001-7155-2732; Chen, Jinjun/0000-0003-1677-9525
FU National Natural Science Foundation of China [61272063, 61202111,
   61572187]; China Science and Technology Pillar Program [2015BAF32B01];
   Research Fund for the Hunan Provincial Natural Science Foundation of
   china [2015JJ2056]; Hunan Provincial University Innovation Platform Open
   Fund Project of China [14K037]
FX This article is supported by the National Natural Science Foundation of
   China under grant numbers 61272063, 61202111, and 61572187; the China
   Science and Technology Pillar Program under grant number 2015BAF32B01;
   the Research Fund for the Hunan Provincial Natural Science Foundation of
   china under grant number 2015JJ2056; and the Hunan Provincial University
   Innovation Platform Open Fund Project of China under grant number
   14K037.
CR [Anonymous], 2004, ELECT IMAGING
   Battiato S, 2011, LECT NOTES COMPUT SC, V6978, P473, DOI 10.1007/978-3-642-24085-0_49
   Beijing Chen, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P625, DOI 10.1109/ICPR.2010.158
   Chen BJ, 2012, SIGNAL PROCESS, V92, P308, DOI 10.1016/j.sigpro.2011.07.018
   Coatrieux G, 2013, IEEE J BIOMED HEALTH, V17, P1057, DOI 10.1109/JBHI.2013.2263533
   De Roover C, 2005, IEEE T SIGNAL PROCES, V53, P4020, DOI 10.1109/TSP.2005.855414
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Fridrich J., 2000, Proceedings International Conference on Information Technology: Coding and Computing (Cat. No.PR00540), P178, DOI 10.1109/ITCC.2000.844203
   Hamilton W., 1866, ELEMENTS QUATERNIONS
   Hsieh SL, 2015, MULTIMED TOOLS APPL, V74, P4947, DOI 10.1007/s11042-014-1857-x
   Hu DH, 2015, INT J SENS NETW, V17, P146, DOI 10.1504/IJSNET.2015.068178
   Kozat SS, 2004, IEEE IMAGE PROC, P3443, DOI 10.1109/ICIP.2004.1421855
   Lei YQ, 2011, SIGNAL PROCESS-IMAGE, V26, P280, DOI 10.1016/j.image.2011.04.007
   Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975
   Liu XL, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540990
   Liu Y, 2012, IEEE T IMAGE PROCESS, V21, P4480, DOI 10.1109/TIP.2012.2207394
   Lu WJ, 2010, IEEE IMAGE PROC, P989, DOI 10.1109/ICIP.2010.5650613
   Lv YM, 2015, IEEE T MULTIMEDIA, V17, P1225, DOI 10.1109/TMM.2015.2437712
   Ma J, 2013, J COMPUT SYST SCI, V79, P256, DOI 10.1016/j.jcss.2012.05.009
   Ma J, 2010, IEEE IMAGE PROC, P2157, DOI 10.1109/ICIP.2010.5651227
   Monga V, 2007, IEEE T INF FOREN SEC, V2, P376, DOI 10.1109/TIFS.2007.902670
   Mou LT, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2542205.2542208
   Ouyang JL, 2015, DIGIT SIGNAL PROCESS, V41, P98, DOI 10.1016/j.dsp.2015.03.006
   Qin C, 2013, DIGIT SIGNAL PROCESS, V23, P578, DOI 10.1016/j.dsp.2012.11.002
   Sangwine SJ, 1996, ELECTRON LETT, V32, P1979, DOI 10.1049/el:19961331
   Tang JH, 2015, IEEE T IMAGE PROCESS, V24, P2827, DOI 10.1109/TIP.2015.2421443
   Tang ZJ, 2014, LECT NOTES ARTIF INT, V8933, P112, DOI 10.1007/978-3-319-14717-8_9
   Tang ZJ, 2014, IEEE T KNOWL DATA EN, V26, P711, DOI 10.1109/TKDE.2013.45
   Tang ZJ, 2012, APPL MATH INFORM SCI, V6, p643S
   Venkatesan R, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P664, DOI 10.1109/ICIP.2000.899541
   Wang X., 2013, IEEE T INF FOREN SEC, V74, P4947
   Winter C, 2014, DIGIT INVEST, V11, pS27, DOI 10.1016/j.diin.2014.03.004
   Xiang S, 2007, MM&SEC'07: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2007, P121
   Yan LY, 2015, INT J COMPUT INT SYS, V8, P725, DOI 10.1080/18756891.2015.1046332
   Zhao Y, 2013, IEEE T INF FOREN SEC, V8, P55, DOI 10.1109/TIFS.2012.2223680
   Zhu XF, 2013, ACM T INFORM SYST, V31, DOI 10.1145/2457465.2457469
   Zou FH, 2013, NEUROCOMPUTING, V105, P81, DOI 10.1016/j.neucom.2012.06.042
NR 37
TC 35
Z9 37
U1 0
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2016
VL 12
IS 4
SU S
AR 63
DI 10.1145/2978572
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8DS
UT WOS:000392736100007
DA 2024-07-18
ER

PT J
AU Rawat, YS
   Kankanhalli, MS
AF Rawat, Yogesh Singh
   Kankanhalli, Mohan S.
TI Context-Aware Photography Learning for Smart Mobile Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Photography; context;
   aesthetics; composition learning; social media; camera parameters
AB In this work we have developed a photography model based on machine learning which can assist a user in capturing high quality photographs. As scene composition and camera parameters play a vital role in aesthetics of a captured image, the proposed method addresses the problem of learning photographic composition and camera parameters. Further, we observe that context is an important factor from a photography perspective, we therefore augment the learning with associated contextual information. The proposed method utilizes publicly available photographs along with social media cues and associated metainformation in photography learning. We define context features based on factors such as time, geolocation, environmental conditions and type of image, which have an impact on photography. We also propose the idea of computing the photographic composition basis, eigenrules and baserules, to support our composition learning. The proposed system can be used to provide feedback to the user regarding scene composition and camera parameters while the scene is being captured. It can also recommend position in the frame where people should stand for better composition. Moreover, it also provides camera motion guidance for pan, tilt and zoom to the user for improving scene composition.
C1 [Rawat, Yogesh Singh; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Dept Comp Sci, Singapore 117548, Singapore.
C3 National University of Singapore
RP Rawat, YS (corresponding author), Natl Univ Singapore, Sch Comp, Dept Comp Sci, Singapore 117548, Singapore.
EM yogesh@comp.nus.edu.sg; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
FU Singapore National Research Foundation under its International Research
   Centre @ Singapore Funding Initiative
FX This research was carried out at the NUS-ZJU Sensor-Enhanced Social
   Media (SeSaMe) Centre. It is supported by the Singapore National
   Research Foundation under its International Research Centre @ Singapore
   Funding Initiative and administered by the Interactive Digital Media
   Programme Office.
CR Abdullah R, 2011, LECT NOTES COMPUT SC, V6815, P13, DOI 10.1007/978-3-642-22571-0_2
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   [Anonymous], 2014, WEATHER FORECAST REP
   [Anonymous], 2011, INT JOINT C ARTIFICI
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], 2010, ACM MULTIMEDIA
   [Anonymous], 2014, SUNRISE SUNSET CALCU
   Bae S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805968
   Banerjee S, 2007, IEEE T IMAGE PROCESS, V16, P1807, DOI 10.1109/TIP.2007.898992
   Bares W, 2006, LECT NOTES COMPUT SC, V4073, P172
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bourke S., 2011, Proceedings of the International Conference of Intelligent User Interfaces (IUI), P13, DOI DOI 10.1145/1943403.1943408
   Butterfield D., 2006, US Patent App., Patent No. [11/350,981, 11350981]
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen Lujun, 2012, Advances in Multimedia Information Processing - PCM 2012. 13th Pacific-Rim Conference on Multimedia. Proceedings, P707, DOI 10.1007/978-3-642-34778-8_66
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Fu H., 2013, P SIGGRAPH AS C EM T, P7
   Goodale Mark, 2007, PHOTOGRAPHERS EYE CO, P1
   Huang J, 2011, IEEE T MULTIMEDIA, V13, P653, DOI 10.1109/TMM.2011.2127463
   Jacobson R.E., 2000, MANUAL PHOTOGRAPHY, V9th
   KELBY S, 2006, DIGITAL PHOTOGRAPHY
   Kim JG, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1171, DOI 10.1109/ICME.2000.871569
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Li CJ, 2010, PROCEEDINGS OF THE 16TH INTERNATIONAL SYMPOSIUM ON ELECTROMACHINING, 2010, P271
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131
   Ma S, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1053, DOI 10.1145/2647868.2655053
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mitarai H, 2013, LECT NOTES COMPUT SC, V7975, P348, DOI 10.1007/978-3-642-39640-3_26
   Ni BB, 2013, IEEE T MULTIMEDIA, V15, P1138, DOI 10.1109/TMM.2013.2241042
   Rawat YS, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P217, DOI 10.1145/2647868.2656409
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
   Wang PP, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P9
   Wang WY, 2014, LECT NOTES COMPUT SC, V8689, P756, DOI 10.1007/978-3-319-10590-1_49
   Wenyuan Yin, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P812, DOI 10.1109/ICME.2012.94
   Xu PF, 2014, MULTIMED TOOLS APPL, V69, P3, DOI 10.1007/s11042-012-1343-2
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Zheng L., 2014, IEEE T CIRCUITS SYST
NR 39
TC 16
Z9 19
U1 1
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 19
DI 10.1145/2808199
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100009
DA 2024-07-18
ER

PT J
AU Venkatagiri, SP
   Chan, MC
   Ooi, WT
AF Venkatagiri, Seshadri Padmanabha
   Chan, Mun Choon
   Ooi, Wei Tsang
TI Automated Link Generation for Sensor-Enriched Smartphone Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Smartphone; collaboration; hyperlinking; images
AB The ubiquity of the smartphones makes them ideal platforms for generating in-situ content. In well-attended events, photos captured by attendees have diverse views that could be subjected to occlusion and abnormal lighting effects that could obscure the view. Such unstructured photo collections also have significant redundancy. Thus, a scene that is partially occluded or has bad contrast in one photo may be captured in another photo, possibly with higher details. We propose an application called Autolink that automatically establishes content-based links between sensor-annotated photos in unstructured photo collections captured using smartphones, such that users could navigate between high-context and high-detail images. This hierarchically structured image collection facilitates the design of applications for navigation and discovery, analytics about user photography patterns, user taste, and content/event popularity. Autolink includes a framework that constructs this hierarchy efficiently and with little content-specific training data by combining photo content processing with associated sensor logs obtained from multiple participants. We evaluated the performance of Autolink on two real-world sensor tagged photo datasets. The result shows that Autolink is able to efficiently cluster photos at 20 times faster than candidate algorithms, into the appropriate hierarchy with at least 70% precision and 37% better recall than candidate algorithms.
C1 [Venkatagiri, Seshadri Padmanabha] Natl Univ Singapore, Interact Digital Media Inst, Singapore 117548, Singapore.
   [Chan, Mun Choon; Ooi, Wei Tsang] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
C3 National University of Singapore; National University of Singapore
RP Venkatagiri, SP (corresponding author), Natl Univ Singapore, Interact Digital Media Inst, Singapore 117548, Singapore.
EM padmanab@comp.nus.edu.sg; chanmc@comp.nus.edu.sg; ooiwt@comp.nus.edu.sg
RI Ooi, Wei Tsang/AAE-7810-2019; Ooi, Wei Tsang/HLW-5142-2023
OI Ooi, Wei Tsang/0000-0001-8994-1736; Ooi, Wei Tsang/0000-0001-8994-1736
FU Singapore National Research Foundation under its International Research
   Centre@Singapore Funding Initiative
FX This research was carried out at the NUS-ZJU Sensor Enhanced Social
   Media (SeSaMe) Centre. It is supported by the Singapore National
   Research Foundation under its International Research Centre@Singapore
   Funding Initiative and administered by the Interactive Digital Media
   Programme Office.
CR Alivand Majid., 2013, Proceedings of the Second ACM SIGSPATIAL International Workshop on Crowdsourced and Volunteered Geographic Information, P23
   [Anonymous], P IEEE INT C 3D VIS
   [Anonymous], 2006, P IEEE COMPUTER SOC, DOI DOI 10.1109/CVPR.2006.95
   [Anonymous], 2015, INT J COMPUT VISION
   [Anonymous], P ACM MM
   [Anonymous], MULTIMED TOOLS APPL
   [Anonymous], 2012, P 20 ACM INT C MULT
   ARPA A, 2013, P INT C 3D VIS 3DV 1, P422, DOI DOI 10.1109/3DV.2013.62
   Bao X, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P197, DOI 10.1145/2493432.2493440
   Bo C., 2013, International Conference on Mobile Computing and Networking (Mo- biCom), P195, DOI DOI 10.1145/2500423.2504574
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Brachmann Eric, 2013, P 3 ACM C INT C MULT, P25
   Carlier Axel, 2011, P 19 ACM INT C MULT, P43
   Changchang Wu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3057, DOI 10.1109/CVPR.2011.5995552
   Forssen P.-E., 2007, IEEE International Conference on Computer Vision (ICCV), P1, DOI DOI 10.1109/CVPR.2007.383120
   Gozali JP, 2012, IEEE INT CONF MULTI, P25, DOI 10.1109/ICMEW.2012.12
   Heath K, 2010, PROC CVPR IEEE, P3432, DOI 10.1109/CVPR.2010.5539991
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jiang YR, 2013, 2013 ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN), P289, DOI 10.1109/IPSN.2013.6917570
   Lacerda Y.A., 2012, Proceedings_of_the_18th_Brazilian_Symposium on_Multimedia_and_the_Web, WebMedia'12, P281
   Liu H., 2012, Proceedings_of_the_20th_ACM_ International_Conference_on_Multimedia, MM'12, P9
   Miller G, 2009, LECT NOTES COMPUT SC, V5709, P98, DOI 10.1007/978-3-642-04052-8_9
   Miyano Ruiko, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P545, DOI 10.1007/978-3-642-37484-5_44
   Park JS, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422961
   Qin Chuan., 2011, Pro- ceedings of the 9th international conference on Mobile systems, applications, and services, MobiSys '11, P1
   Radev Dragomir R., 2002, LREC
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Shipman F, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1413862.1413868
   Singh R, 2007, COMP MED SY, P153, DOI 10.1109/CBMS.2007.74
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Wang He, 2013, P 14 WORKSH MOB COMP
   Xie Xing., 2005, SIGCHI Conference on Human Factors in Computing Systems (CHI'05), P671
   Zhang L, 2013, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2013.240
   Zheng YT, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422959
NR 35
TC 0
Z9 0
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 13
DI 10.1145/2808209
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100003
DA 2024-07-18
ER

PT J
AU Chen, BH
   Huang, SC
AF Chen, Bo-Hao
   Huang, Shih-Chia
TI An Advanced Visibility Restoration Algorithm for Single Hazy Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; Performance; Image haze removal; dehaze; visibility
   restoration
ID QUALITY
AB Haze removal is the process by which horizontal obscuration is eliminated from hazy images captured during inclement weather. Images captured in natural environments with varied weather conditions frequently exhibit localized light sources or color-shift effects. The occurrence of these effects presents a difficult challenge for hazy image restoration, with which many traditional restoration methods cannot adequately contend. In this article, we present a new image haze removal approach based on Fisher's linear discriminant-based dual dark channel prior scheme in order to solve the problems associated with the presence of localized light sources and color shifts, and thereby achieve effective restoration. Experimental restoration results via qualitative and quantitative evaluations show that our proposed approach can provide higher haze-removal efficacy for images captured in varied weather conditions than can the other state-of-the-art approaches.
C1 [Chen, Bo-Hao; Huang, Shih-Chia] Natl Taipei Univ Technol, Taipei 106, Taiwan.
C3 National Taipei University of Technology
RP Huang, SC (corresponding author), Natl Taipei Univ Technol, Dept Elect Engn, Taipei 106, Taiwan.
EM schuang@ntut.edu.tw
FU Ministry of Science and Technology of the Republic of China [MOST
   103-2221-E-027-030-MY2, MOST 103-2221-E-027-031-MY2]
FX This work was supported by the Ministry of Science and Technology of the
   Republic of China under Grant Nos. MOST 103-2221-E-027-030-MY2 and MOST
   103-2221-E-027-031-MY2.
CR [Anonymous], IMAGE ANAL STEREOLOG
   [Anonymous], P NATL ACAD SCI US
   [Anonymous], 2003, INTERACTIVE DEWEATHE
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bin Xie, 2010, Proceedings 2010 International Conference on Intelligent System Design and Engineering Application (ISDEA 2010), P848, DOI 10.1109/ISDEA.2010.141
   Chen BH, 2013, IEEE INT SYM MULTIM, P267, DOI 10.1109/ISM.2013.51
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Haoran Xu, 2012, 2012 IEEE International Conference on Information Science and Technology, P663, DOI 10.1109/ICIST.2012.6221729
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hossain MA, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/1870121.1870124
   Huang SC, 2014, IEEE T CIRC SYST VID, V24, P1814, DOI 10.1109/TCSVT.2014.2317854
   Huang SC, 2014, IEEE T INTELL TRANSP, V15, P2321, DOI 10.1109/TITS.2014.2314696
   HURLBERT A, 1986, J OPT SOC AM A, V3, P1684, DOI 10.1364/JOSAA.3.001684
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Lam EY, 2005, I SYMP CONSUM ELECTR, P134, DOI 10.1109/ISCE.2005.1502356
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Liu XT, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1556134.1556141
   Mei T, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487269
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1
   Oakley JP, 1998, IEEE T IMAGE PROCESS, V7, P167, DOI 10.1109/83.660994
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Snidaro L, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071403
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Webster MA, 1996, NETWORK-COMP NEURAL, V7, P587, DOI 10.1088/0954-898X/7/4/002
   Wenbo Jin, 2012, Proceedings of the 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE), P791
   WEST G, 1982, J MATH BIOL, V15, P249, DOI 10.1007/BF00275077
   Wu JW, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1671962.1671964
   Yu J, 2011, INT CONF ACOUST SPEE, P1245
NR 30
TC 23
Z9 25
U1 0
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2015
VL 11
IS 4
AR 53
DI 10.1145/2726947
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CK0ZF
UT WOS:000355933700006
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Zhang, LM
   Zimmermann, R
AF Zhang, Ying
   Zhang, Luming
   Zimmermann, Roger
TI Aesthetics-Guided Summarization from Multiple User Generated Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Measurement; Performance; Quality assess; video
   quality; user generated videos; video summary
ID FRAMEWORK; RETRIEVAL
AB In recent years, with the rapid development of camera technology and portable devices, we have witnessed a flourish of user generated videos, which are gradually reshaping the traditional professional video oriented media market. The volume of user generated videos in repositories is increasing at a rapid rate. In today's video retrieval systems, a simple query will return many videos which seriously increase the viewing burden. To manage these video retrievals and provide viewers with an efficient way to browse, we introduce a system to automatically generate a summarization from multiple user generated videos and present their salience to viewers in an enjoyable manner. Among multiple consumer videos, we find their qualities to be highly diverse due to various factors such as a photographer's experience or environmental conditions at the time of capture. Such quality inspires us to include a video quality evaluation component into the video summarization since videos with poor qualities can seriously degrade the viewing experience. We first propose a probabilistic model to evaluate the aesthetic quality of each user generated video. This model compares the rich aesthetics information from several well-known photo databases with generic unlabeled consumer videos, under a human perception component indicating the correlation between a video and its constituting frames. Subjective studies were carried out with the results indicating that our method is reliable. Then a novel graph-based formulation is proposed for the multi-video summarization task. Desirable summarization criteria is incorporated as the graph attributes and the problem is solved through a dynamic programming framework. Comparisons with several state-of-the-art methods demonstrate that our algorithm performs better than other methods in generating a skimming video in preserving the essential scenes from the original multiple input videos, with smooth transitions among consecutive segments and appealing aesthetics overall.
C1 [Zhang, Ying; Zhang, Luming; Zimmermann, Roger] Natl Univ Singapore, Singapore 117548, Singapore.
C3 National University of Singapore
RP Zhang, LM (corresponding author), Natl Univ Singapore, Singapore 117548, Singapore.
EM yingz118@comp.nus.edu.sg; zglumg@nus.edu.sg; rogerz@comp.nus.edu.sg
RI Lei, Ming/JAD-1050-2023; zhang, lu/GRO-2969-2022; Zimmermann,
   Roger/D-7944-2015
OI Zimmermann, Roger/0000-0002-7410-2590
FU Singapore National Research Foundation under its International Research
   Centre @ Singapore Funding Initiative
FX This research has been supported by the Singapore National Research
   Foundation under its International Research Centre @ Singapore Funding
   Initiative and administered by the IDM Programme Office through the
   Centre of Social Media Innovations for Communities (COSMIC).
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2013, P IEEE C COMP VIS PA
   [Anonymous], P INT C MULT MOD MMM
   [Anonymous], P ACM MULT SYST C
   [Anonymous], 2009, P IEEE INT C COMP VI
   Bhattacharya Subhabrata, 2010, P ACM INT C MULT
   Chen YM, 2011, IEEE T CIRC SYST VID, V21, P1316, DOI 10.1109/TCSVT.2011.2148490
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Dalal N., CVPR, P886, DOI [DOI 10.1109/CVPR.2005.177, 10.1109/CVPR.2005.177]
   Datta R., 2006, P EUR C COMP VIS
   Datta R., 2008, P IEEE INT C IM PROC
   Dhar S., 2011, P IEEE C COMP VIS PA
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Gao GW, 2013, IEEE T IMAGE PROCESS, V22, P5050, DOI 10.1109/TIP.2013.2281429
   Hao Jia, 2011, P ACM INT C MULT
   Harel J., 2007, P ADV NEUR INF PROC
   He Liwei, 1999, P ACM INT C MULT
   Hori Tetsuro, 2003, P ACM SIGMM INT WORK
   Kim JG, 2003, INT J IMAG SYST TECH, V13, P267, DOI 10.1002/ima.10067
   Lee C, 2006, OPT ENG, V45, DOI 10.1117/1.2160515
   Lee J., 2007, Proceedings ACM SIGMOD Int. Conf. on Management of Data, p593 604
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li Yingbo, 2011, P INT WORKSH CONT BA
   Li Yingbo, 2010, P INT WORKSH CONT BA
   Luo Y, 2008, P EUR C COMP VIS
   Marchesotti L., 2011, P IEEE INT C COMP VI
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Moorthy Anush K., 2010, P EUR C COMP VIS
   Otsuka I, 2005, IEEE T CONSUM ELECTR, V51, P112, DOI 10.1109/TCE.2005.1405707
   Saini Mukesh Kumar, 2012, P ACM INT C MULT
   Shao J, 2010, COMPUT SCI INF SYST, V7, P85, DOI 10.2298/CSIS1001085S
   Shao Xi, 2006, ACM T MULTIM COMPUT, V2, P2, DOI DOI 10.1145/1142020.1142023
   Shipman F., 2003, P IEEE INT C MULT EX
   Su H., 2011, P ACM INT C MULT
   Sun Xiaoshuai, 2009, P ACM INT C MULT
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Wang Feng, 2009, P IEEE INT C MULT EX
   Wang Yanran, 2013, P ACM INT C MULT
   Wang Z, 2007, J OPT SOC AM A, V24, pB61, DOI 10.1364/JOSAA.24.000B61
   Wilk Stefan, 2013, P ACM INT WORKSH CRO
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P421, DOI 10.1109/TMM.2008.917346
   Xu Changsheng, 2006, P ACM INT C MULT
   Yang Chun-Yu, 2011, PROCEEDINGS OF THE I
   Yang Y, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457456
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhang SL, 2013, IEEE T IMAGE PROCESS, V22, P2889, DOI 10.1109/TIP.2013.2251650
NR 46
TC 9
Z9 9
U1 2
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2014
VL 11
IS 2
AR 24
DI 10.1145/2659520
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6DM
UT WOS:000348308800001
DA 2024-07-18
ER

PT J
AU Tan, S
   Jiang, YG
   Ngo, CW
AF Tan, Song
   Jiang, Yu-Gang
   Ngo, Chong-Wah
TI Placing Videos on a Semantic Hierarchy for Search Result Navigation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Web video browsing; video
   organization; visualization
AB Organizing video search results in a list view is widely adopted by current commercial search engines, which cannot support efficient browsing for complex search topics that have multiple semantic facets. In this article, we propose to organize video search results in a highly structured way. Specifically, videos are placed on a semantic hierarchy that accurately organizes various facets of a given search topic. To pick the most suitable videos for each node of the hierarchy, we define and utilize three important criteria: relevance, uniqueness, and diversity. Extensive evaluations on a large YouTube video dataset demonstrate the effectiveness of our approach.
C1 [Tan, Song] City Univ Hong Kong, Dept Comp Sci, Kowloon Town, Hong Kong, Peoples R China.
   [Jiang, Yu-Gang] Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Sch Comp Sci, Shanghai 201203, Peoples R China.
   [Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Creat Media Ctr CMC5003, Kowloon Town, Hong Kong, Peoples R China.
C3 City University of Hong Kong; Fudan University; City University of Hong
   Kong
RP Tan, S (corresponding author), City Univ Hong Kong, Dept Comp Sci, 83 Tat Chee Ave, Kowloon Town, Hong Kong, Peoples R China.
EM songtan-c@my.cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261
FU Research Grants Council of Hong Kong Special Administrative Region,
   China [CityU 119610]; National Natural Science Foundation of China
   [61228205]
FX This work was fully supported by a grant from the Research Grants
   Council of Hong Kong Special Administrative Region, China (CityU 119610)
   and a grant from the National Natural Science Foundation of China
   (61228205).
CR Ammar Anis Ben, 2013, OAIR, P53
   [Anonymous], ACM T MULTIMEDIA C S
   [Anonymous], 2008, P 31 ANN INT ACM SIG
   [Anonymous], 2004, Proceedings of the 12th ACM International Conference on Multimedia, DOI DOI 10.1145/1027527.1027747
   [Anonymous], INT C MULT
   [Anonymous], 2008, SIGIR
   Carmel D, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P139, DOI 10.1145/1571941.1571967
   Chen T., 2012, P 20 ACM INT C MULT, P781
   de Rooij O, 2010, IEEE T MULTIMEDIA, V12, P121, DOI 10.1109/TMM.2009.2037388
   Fung GPC, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P300
   Hsu WinstonH., 2007, ACM MM
   Huang C.-C., 2004, Proceedings of the 13th International Conference on World Wide Web, P184
   Ide Ichiro., 2003, Proceedings of the Fifth ACM SIGMM International Workshop on Multimedia Information Retrieval, P239
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jing F., 2006, PROC MM 06, P377, DOI DOI 10.1145/1180639.1180720
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Liu L., 2008, P 17 INT C WORLD WID, P1009
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ming ZY, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P2
   Nister David, 2006, CVPR
   ROMANO JP, 1990, J AM STAT ASSOC, V85, P686, DOI 10.2307/2290003
   Tan S, 2011, P 19 ACM INT C MULT, P243
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Wu XA, 2011, IEEE MULTIMEDIA, V18, P38, DOI 10.1109/MMUL.2011.12
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zhao WL, 2009, IEEE T IMAGE PROCESS, V18, P412, DOI 10.1109/TIP.2008.2008900
NR 27
TC 1
Z9 2
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2014
VL 10
IS 4
AR 37
DI 10.1145/2578394
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO4FY
UT WOS:000341292900007
OA Green Published
DA 2024-07-18
ER

PT J
AU Tang, JH
   Chen, Q
   Wang, M
   Yan, SC
   Chua, TS
   Jain, R
AF Tang, Jinhui
   Chen, Qiang
   Wang, Meng
   Yan, Shuicheng
   Chua, Tat-Seng
   Jain, Ramesh
TI Towards Optimizing Human Labeling for Interactive Image Tagging
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Interactive image tagging; multiscale
   cluster labeling
ID ANNOTATION
AB Interactive tagging is an approach that combines human and computer to assign descriptive keywords to image contents in a semi-automatic way. It can avoid the problems in automatic tagging and pure manual tagging by achieving a compromise between tagging performance and manual cost. However, conventional research efforts on interactive tagging mainly focus on sample selection and models for tag prediction. In this work, we investigate interactive tagging from a different aspect. We introduce an interactive image tagging framework that can more fully make use of human's labeling efforts. That means, it can achieve a specified tagging performance by taking less manual labeling effort or achieve better tagging performance with a specified labeling cost. In the framework, hashing is used to enable a quick clustering of image regions and a dynamic multiscale clustering labeling strategy is proposed such that users can label a large group of similar regions each time. We also employ a tag refinement method such that several inappropriate tags can be automatically corrected. Experiments on a large dataset demonstrate the effectiveness of our approach
C1 [Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci, Nanjing 210094, Jiangsu, Peoples R China.
   [Chen, Qiang; Yan, Shuicheng] Natl Univ Singapore, Singapore 117576, Singapore.
   [Wang, Meng; Chua, Tat-Seng] Natl Univ Singapore, Singapore 117417, Singapore.
   [Jain, Ramesh] Univ Calif Irvine, Irvine, CA 92697 USA.
C3 Nanjing University of Science & Technology; National University of
   Singapore; National University of Singapore; University of California
   System; University of California Irvine
RP Tang, JH (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci, Nanjing 210094, Jiangsu, Peoples R China.
EM jinhuitang@mail.njust.edu.cn
RI Yan, Shuicheng/HCI-1431-2022; chen, qiang/HGU-5418-2022; Tang,
   Jinhui/KBR-0891-2024; chen, qiang/GWZ-7308-2022
OI Tang, Jinhui/0000-0001-9008-222X
FU Natural Science Foundation of China (NSFC) [61103059]; National Basic
   Research Program of China (973 Program) [2012CB316304]; Program for New
   Century Excellent Talents in University [NCET-12-0632]; Natural Science
   Foundation of Jiangsu Province [BK2012033, BK2011700]; Research Fund for
   the Doctoral Program of Higher Education of China (RFDP)
   [20113219120022]
FX This work was partially supported by the Natural Science Foundation of
   China (NSFC) under grant 61103059, National Basic Research Program of
   China (973 Program) under grant 2012CB316304, Program for New Century
   Excellent Talents in University under grant NCET-12-0632, Natural
   Science Foundation of Jiangsu Province under grant BK2012033 and
   BK2011700, Research Fund for the Doctoral Program of Higher Education of
   China (RFDP) under grant 20113219120022.
CR ANDONI A., 2008, COMMUN ACM, V51, P1
   [Anonymous], P EUR C COMP VIS
   [Anonymous], P 11 ACM INT C MULT
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI [10.1145/1899412.1899418, DOI 10.1145/1899412.1899418]
   BISSOL S., 2003, P INT WORKSH CONT BA
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   CUI J., 2007, P ACM SIGCHI C HUM F
   Deng Yining., 2001, IEEE T PATTERN ANAL
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   GIRGENSOHN A, 2004, P 6 ACM SIGMM INT WO
   Hauptmann A. G., 2006, P ACM INT C MULT
   Huang TS, 2008, P IEEE, V96, P648, DOI 10.1109/JPROC.2008.916364
   Jeon J., 2003, P 26 ANN INT ACM SIG
   Joshi A. J., 2009, P IEEE C COMP VIS PA
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   KUCHINSKY A, 1999, P SIGCHI C HUM FACT
   LEE S., 2010, P IEEE INT C MULT EX
   Li T, 2011, IEEE T IMAGE PROCESS, V20, P2301, DOI 10.1109/TIP.2010.2103081
   Liu D., 2009, P ACM INT C MULT
   LIU W., 2001, P IFIP TC 13 INT C H
   LIU W., 2000, P ACM INT C MULT
   Makadia A., 2008, P 10 EUR C COMP VIS
   Mu Y., 2010, P IEEE C COMP VIS PA
   NAKAMURAA E., 1998, PATTERN RECOGNIT LET, V19, P14
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Rother C, 2006, ACM T GRAPHIC, V25, P847, DOI 10.1145/1141911.1141965
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   SUH B., 2004, HCIL200415 U MAR COM
   Suh B, 2007, INTERACT COMPUT, V19, P524, DOI 10.1016/j.intcom.2007.02.002
   TANG J., 2010, P ACM INT C MULT
   Tang J., 2009, P ACM INT C MULT
   Tang JH, 2012, IEEE T IMAGE PROCESS, V21, P2354, DOI 10.1109/TIP.2011.2180916
   TIAN Y., 2007, P IEEE INT C COMP VI
   TREC, TREC 10 P APP COMM E
   TUFFIELD M. M, 2006, P WORLD WID WEB C
   Wang X., 2008, IEEE Transactions on Pattern Analysis and Machine Intelligence
   XU H., 2009, P ACM INT C MULT
   YAN R., 2009, IEEE MULTIMEDIA MAG
   YANG K., 2009, P IEEE INT C MULT EX
   ZHU G., 2010, P ACM INT C MULT
NR 40
TC 16
Z9 16
U1 1
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2013
VL 9
IS 4
AR 29
DI 10.1145/2501643.2501651
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 206EW
UT WOS:000323501800007
DA 2024-07-18
ER

PT J
AU Yang, LJ
   Geng, B
   Hanjalic, A
   Hua, XS
AF Yang, Linjun
   Geng, Bo
   Hanjalic, Alan
   Hua, Xian-Sheng
TI A Unified Context Model for Web Image Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Image retrieval; context-based web image
   retrieval
ID ANNOTATION
AB Content-based web image retrieval based on the query-by-example (QBE) principle remains a challenging problem due to the semantic gap as well as the gap between a user's intent and the representativeness of a typical image query. In this article, we propose to address this problem by integrating query-related contextual information into an advanced query model to improve the performance of QBE-based web image retrieval. We consider both the local and global context of the query image. The local context can be inferred from the web pages and the click-through log associated with the query image, while the global context is derived from the entire corpus comprising all web images and the associated web pages. To effectively incorporate the local query context we propose a language modeling based approach to deal with the combined structured query representation from the contextual and visual information. The global query context is integrated by the multi-modal relevance model to "reconstruct" the query from the document models indexed in the corpus. In this way, the global query context is employed to address the noise or missing information in the query and its local context, so that a comprehensive and robust query model can be obtained. We evaluated the proposed approach on a representative product image dataset collected from the web and demonstrated that the inclusion of the local and global query contexts significantly improves the performance of QBE-based web image retrieval.
C1 [Yang, Linjun; Hua, Xian-Sheng] Microsoft Res Asia, Beijing 100190, Peoples R China.
   [Geng, Bo] Peking Univ, Beijing 100871, Peoples R China.
   [Hanjalic, Alan] Delft Univ Technol, Delft, Netherlands.
C3 Microsoft; Microsoft Research Asia; Peking University; Delft University
   of Technology
RP Yang, LJ (corresponding author), Microsoft Res Asia, Beijing 100190, Peoples R China.
EM linjuny@microsoft.com
CR [Anonymous], 1998, STAT LEARNING THEORY
   Belkin Nicholas J., 2008, Advances in Information Retrieval, Lecture Notes in Computer Science, P1
   Cao LL, 2009, IEEE T MULTIMEDIA, V11, P208, DOI 10.1109/TMM.2008.2009693
   CHANG S, 2006, P NIST TRECVID WORKS
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Dou ZC, 2009, IEEE T KNOWL DATA EN, V21, P1178, DOI 10.1109/TKDE.2008.172
   Geng B, 2009, INT CONF DAT MIN WOR, P158, DOI 10.1109/ICDMW.2009.114
   Heng Tao Shen, 2000, Proceedings ACM Multimedia 2000, P39, DOI 10.1145/354384.376098
   HUA X.-S., 2008, P TRECVID
   Hua XS, 2009, SIGNALS COMMUN TECHN, P353, DOI 10.1007/978-0-387-76569-3_13
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   JING F., 2005, IEEE T IMAGE PROCESS
   Lafferty J., 2001, SIGIR, P111, DOI DOI 10.1145/383952.383970
   Lavrenko V., 2001, SIGIR Forum, P120
   Lavrenko V., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P175
   Lawrence S., 2000, IEEE DATA ENG B, V23, P25
   Luo JB, 2009, IEEE T MULTIMEDIA, V11, P193, DOI 10.1109/TMM.2008.2009179
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Mei T., 2008, P 16 ACM INT C MULT, P439, DOI [https://doi.org/10.1145/1459359.1459418, DOI 10.1145/1459359.1459418]
   Page L., 1999, PAGERANK CITATION RA
   Robertson Stephen, 2009, Foundations and Trends in Information Retrieval, V3, P333, DOI 10.1561/1500000019
   Robertson S., 2004, P 13 ACM INT C INF K, P42
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Scholer F., 2002, Proceedings of the Eleventh International Conference on Information and Knowledge Management. CIKM 2002, P324, DOI 10.1145/584792.584846
   Scholer F, 2004, J AM SOC INF SCI TEC, V55, P637, DOI 10.1002/asi.20011
   Sinha Pinaki, 2008, 2008 Second IEEE International Conference on Semantic Computing (ICSC), P58, DOI 10.1109/ICSC.2008.87
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   TIAN X., 2008, P 16 ACM INT C MULT
   Wang X., 2004, ACM INT C MULTIMEDIA, P944
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   Westerveld T, 2003, EURASIP J APPL SIG P, V2003, P186, DOI 10.1155/S111086570321101X
   Wilkinson R., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P311
   Xue G.-R., 2004, CIKM, P118
   YAN R, 2003, P INT C IM VID RETR
   YANG L., 2010, P 18 ACM INT C MULT
   Yang L., 2010, Proceedings of the ACM International Conference on Image and Video Retrieval, P406
   YANG L., 2010, P 18 ACM INT C MULT
   Yang YH, 2008, P 16 ACM INT C MULT
   Yang Yi., 2009, Proceedings of the 17th ACM international conference on Multimedia, P175
   Zha ZJ, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823747
   Zhai C., 2009, Statistical Language Models for Information Retrieval
   Zhai CX, 2004, ACM T INFORM SYST, V22, P179, DOI 10.1145/984321.984322
   Zhang RF, 2005, IEEE I CONF COMP VIS, P846
   Zhao R, 2002, IEEE T MULTIMEDIA, V4, P189, DOI 10.1109/TMM.2002.1017733
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 48
TC 6
Z9 6
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2012
VL 8
IS 3
AR 28
DI 10.1145/2240136.2240141
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 986AM
UT WOS:000307311700005
DA 2024-07-18
ER

PT J
AU Santini, S
AF Santini, Simone
TI Efficient Computation of Queries on Feature Streams
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Theory; Multimedia databases; query optimization
AB This article introduces the notion of virtual feature stream, a feature stream defined from a primary data stream, in which at any time only the features that are needed to compute the queries that are currently running in the system are computed.
   Virtual feature streams are, in general, impossible to determine a priori, but the paper introduces an algorithm that stops the computation of features as soon as it can be proved that they are no longer needed thus generating, albeit in a roundabout and more expensive than the ideal way, a feature stream that is less expensive than the complete one to compute and safe: the queries that accept the virtual feature stream are those (and only those) that would accept the original feature stream.
C1 Univ Autonoma Madrid, Escuela Politecn Super, E-28049 Madrid, Spain.
C3 Autonomous University of Madrid
RP Santini, S (corresponding author), Univ Autonoma Madrid, Escuela Politecn Super, E-28049 Madrid, Spain.
EM simone.santini@uam.es
RI Santini, Simone/B-7219-2014
OI Santini, Simone/0000-0002-1869-5301
FU Consejeria de Educacion de la Comunidad Autonoma de Madrid
   [CCG08-UAM/TIC/4303]; Busqueda basada en contexto como alternativa
   semantica al modelo ontologico; Ministerio de Educacion y Ciencia
FX This work was supported in part by Consejeria de Educacion de la
   Comunidad Autonoma de Madrid, under the grant CCG08-UAM/TIC/4303,
   Busqueda basada en contexto como alternativa semantica al modelo
   ontologico, and by the program Ramon y Cajal of the Ministerio de
   Educacion y Ciencia.
CR ARASU A, 2003, P INT WORKSH DAT PRO
   AYAD A, 2004, P ACM SIGACT SIGART
   BABCOCK B, 2002, P ACM SIGACT SIGART
   Babu S, 2004, ACM T DATABASE SYST, V29, P545, DOI 10.1145/1016028.1016032
   Babu S, 2001, SIGMOD REC, V30, P109, DOI 10.1145/603867.603884
   Carney Don, 2003, Proceedings of the 29th International Conference on Very Large Data Bases (VLDB), V29, P838
   Chandrasekaran S, 2003, VLDB J, V12, P140, DOI 10.1007/s00778-003-0096-y
   Corman T.H., 2001, INTRO ALGORITHMS, V2nd
   DIAO Y, 2003, AMC T DATAB SYST, V8, P467
   Golab L, 2003, SIGMOD REC, V32, P5, DOI 10.1145/776985.776986
   GREEN TJ, 2003, P 9 INT C DAT THEOR
   GYLISTROM D, 2007, P 3 BIENN C INN DAT
   Hellerstein JM, 1998, ACM T DATABASE SYST, V23, P113, DOI 10.1145/292481.277627
   Hjelsvold Rune., 1994, VLDB 94 P 20 INT C V, P686
   Koudas N., 2005, P 21 INT C DAT ENG
   LIU B, 2005, P INT C DAT ENG
   PELEG S, 1990, P 10 INT C PATT REC
   Srivastava U., 2005, P 24 ACM SIGMOD SIGA
   Thomas W., 1997, HDB FORMAL LANGUAGES
   VIGLAS S, 2003, P 29 INT C VER LARG
   2011, ACM T MULTIMEDIA COM, V7
NR 21
TC 0
Z9 0
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2011
VL 7
IS 4
AR 38
DI 10.1145/2043612.2043616
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 856ZS
UT WOS:000297684000004
DA 2024-07-18
ER

PT J
AU Feng, WC
   Dang, T
   Kassebaum, J
   Bauman, T
AF Feng, Wu-Chi
   Thanh, Dang
   Kassebaum, John
   Bauman, Tim
TI Supporting Region-of-Interest Cropping Through Constrained Compression
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Management; Measurement; Performance; Design;
   Experimentation; Human Factors; Standardization; Video adaptation;
   region-of-interest; video coding; streaming
AB The ability to create super high-resolution video is becoming relative easy to do either through a single high-definition video camera or panoramic video that automatically stitches multiple views together. As an example of the former, the motion picture industry now has 6000 x 4000 pixel full-rate video cameras available. This means that supporting region-of-interest cropping will become more important in the future. In this article, we propose a mechanism to support region-of-interest adaptation of stored video. The proposed approach creates a compression-compliant stream (e.g., MPEG-2), while still allowing it to be cropped. Fortunately, video standards like MPEG-2 specify the format of a compliant stream, and not the algorithm to get there. As a result, there is an opportunity to allow system researchers and implementers ways to optimize for applications. We show various fundamental tradeoffs that are made in order to support region-of-interest cropping with super high-resolution video which we received from a local motion-picture firm.
C1 [Feng, Wu-Chi; Thanh, Dang; Kassebaum, John; Bauman, Tim] Portland State Univ, Dept Comp Sci, Portland, OR 97207 USA.
C3 Portland State University
RP Feng, WC (corresponding author), Portland State Univ, Dept Comp Sci, POB 751, Portland, OR 97207 USA.
FU National Science Foundation [CNS-0722063]
FX This research was supported by the National Science Foundation under
   grant CNS-0722063.
CR AGARWAL A, 2000, P SPIE VOIC VID DAT
   Ahmad I, 2005, IEEE T MULTIMEDIA, V7, P793, DOI 10.1109/TMM.2005.854472
   [Anonymous], P 15 EUR SIGN PROC C
   [Anonymous], 2007, MM
   Augustine J, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P559, DOI 10.1109/ICME.2004.1394253
   Bae TM, 2006, ETRI J, V28, P239, DOI 10.4218/etrij.06.0205.0126
   DUGAD R, 2003, IEEE T CIRCUITS SYST, V13
   Fan Xin., 2003, P ACM MULTIMEDIA, P247
   FENG W, 2008, P ACM MULT ACM NEW Y
   HASKELL B, 1996, DIGITAL VIDEO COMPRE
   Huang J, 2006, MULTIMEDIA SYST, V11, P513, DOI 10.1007/s00530-006-0036-y
   Lambert P, 2006, LECT NOTES COMPUT SC, V4179, P442
   Le Gall D., 1991, Communications of the ACM, V34, P46, DOI 10.1145/103085.103090
   Lin CW, 2003, IEEE T CIRC SYST VID, V13, P982, DOI 10.1109/TCSVT.2003.816505
   Rehan M, 2007, IEEE PACIF, P569
   Sinha A, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P161
   Sun XD, 2005, IEEE T MULTIMEDIA, V7, P981, DOI 10.1109/TMM.2005.854388
   WANG H, 2006, P IEEE INT C AC SPEE
NR 18
TC 1
Z9 1
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2011
VL 7
IS 3
AR 17
DI 10.1145/2000486.2000491
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 814DB
UT WOS:000294425100005
DA 2024-07-18
ER

PT J
AU Chetty, G
   White, M
AF Chetty, Girija
   White, Matthew
TI Multimedia Sensor Fusion for Retrieving Identity in Biometric Access
   Control Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Performance
ID SPEECH; RECOGNITION; MOTION
AB In this article, we propose a novel multimedia sensor fusion approach based on heterogeneous sensors for biometric access control applications. The proposed fusion technique uses multiple acoustic and visual sensors for extracting dominant biometric cues, and combines them with nondominant cues. The performance evaluation of the proposed fusion protocol and a novel cascaded authentication approach using a 3D stereovision database shows a significant improvement in performance and robustness, with equal error rates of 42.9% (audio only), 32% (audio + 3D face+ 2D lip features), 15% (audio + 3D face+ 2D eye features), and 7.3% (audio-3D face+2D lip +2D eye-eyebrows) respectively.
C1 [Chetty, Girija] Univ Canberra, Fac Informat Sci & Engn, Canberra, ACT 2601, Australia.
C3 University of Canberra
RP Chetty, G (corresponding author), Univ Canberra, Fac Informat Sci & Engn, Canberra, ACT 2601, Australia.
EM girija.chetty@canberra.edu.au
RI CHETTY, Girija/C-2221-2008
OI CHETTY, Girija/0000-0001-6264-8644
CR Bowyer KW, 2006, COMPUT VIS IMAGE UND, V101, P1, DOI 10.1016/j.cviu.2005.05.005
   BRUNELLI R, 1995, IEEE T PATTERN ANAL, V17, P955, DOI 10.1109/34.464560
   Callan DE, 2003, NEUROREPORT, V14, P2213, DOI 10.1097/00001756-200312020-00016
   Chelubishi C.C., 2002, IEEE T MULTIMEDIA, V4, P23
   CHETTY G, 2007, P INT C SPOK LANG PR
   Chetty Girija., 2004, Proc. Image and Vision Computing, P17
   Dasarathy BV, 1997, P IEEE, V85, P24, DOI 10.1109/5.554206
   DUTAGACI H, 2006, P SPIE C EL IM SEC S
   Goecke R., 2004, Proc. of the International Conference on Spoken Language Processing (ICSLP), V3, P2525, DOI 10.21437/Interspeech.2004-433
   Gokberk B., 2006, IMAGE VISIO IN PRESS
   HALLD L, 1997, P IEEE, V85, P6
   Hani C.Y., 2002, J PHONETICS, V30, P555
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   KAHRAMAN F, 2006, P IEEE NORD SIGN PRO
   Kroos C, 2002, J PHONETICS, V30, P569, DOI 10.1006/jpho.2002.0164
   ORTEGAGARCIA J, 2003, IEE P VIS IM SIGN PR
   Pigeon S, 1998, SIGNAL PROCESS, V69, P59, DOI 10.1016/S0165-1684(98)00087-5
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   QUATIERI TF, 2002, SIGNAL PROCESSING SE
   Santi A, 2003, J COGNITIVE NEUROSCI, V15, P800, DOI 10.1162/089892903322370726
NR 20
TC 3
Z9 4
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2010
VL 6
IS 4
AR 26
DI 10.1145/1865106.1865110
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 688VA
UT WOS:000284880900004
DA 2024-07-18
ER

PT J
AU Deng, RH
   Yang, YJ
AF Deng, Robert H.
   Yang, Yanjiang
TI A Study of Content Authentication in Proxy-Enabled Multimedia Delivery
   Systems: Model, Techniques, and Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Reliability; Multimedia content delivery; security; end-to-end
   authentication
ID ROBUST
AB Compared with the direct server-user approach, the server-proxy-user architecture for multimedia delivery promises significantly improved system scalability. The introduction of the intermediary transcoding proxies between content servers and end users in this architecture, however, brings unprecedented challenges to content security. In this article, we present a systematic study on the end-to-end content authentication problem in the server-proxy-user context, where intermediary proxies transcode multimedia content dynamically. We present a formal model for the authentication problem, propose a concrete construction for authenticating generic data modality and formally prove its security. We then apply the generic construction to authenticating specific multimedia formats, for example, JPEG2000 code-streams and MPEG-4 video streams. The prototype implementation shows that our scheme is suitable for practical applications.
C1 [Deng, Robert H.] Singapore Management Univ, Sch Informat Syst, Singapore, Singapore.
   [Yang, Yanjiang] Inst Infocomm Res, Singapore 138632, Singapore.
C3 Singapore Management University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Deng, RH (corresponding author), Singapore Management Univ, Sch Informat Syst, Singapore, Singapore.
EM robertdeng@smu.edu.sg; yyang@i2r.a-star.edu.sg
RI DENG, Robert H./E-8547-2012
OI Deng, Robert/0000-0003-3491-8146
FU Office of Research, Singapore Management University
FX This work is partly supported by the Office of Research, Singapore
   Management University.
CR [Anonymous], P 4 ACM INT WORKSH C
   [Anonymous], 2002, Information Security and Cryptology-ICISC 2001, DOI DOI 10.1007/3-540-45861-1
   [Anonymous], 2001, PROC 8 ACM C COMPUT
   [Anonymous], 2002, LNCS
   [Anonymous], P 1 ACM INT WORKSH M
   [Anonymous], 2003, NDSS
   [Anonymous], P ACM INT C MULT MM
   Ateniese G, 2005, LECT NOTES COMPUT SC, V3679, P159
   Bellare M., 1993, P 1 ACM C COMP COMM, P62
   BERGADANO F, 2000, P 7 ANN WORKSH SEL A, P144
   Bhattacharjee S, 1998, IEEE INFOCOM SER, P600, DOI 10.1109/INFCOM.1998.665080
   Bjork N., 2000, Proceedings of the 2000 ACM Workshops on Multimedia, P75
   BRISCOE B, 2000, BT RES
   Cardellini V., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P520, DOI 10.1145/354756.354861
   Contini S, 2006, LECT NOTES COMPUT SC, V4004, P165
   Deng RH, 2005, MULTIMEDIA SYST, V11, P60, DOI 10.1007/s00530-005-0190-7
   DENG RH, 2004, SECURITY 21 CENTURY, P229
   Dimitrov VS, 2000, IEEE T COMPUT, V49, P141, DOI 10.1109/12.833110
   Fridrich J, 2004, PROC SPIE, V5306, P354, DOI 10.1117/12.525418
   Furht B., 2006, MULTIMEDIA ENCRYPTIO
   Gentry C, 2005, IEEE J SEL AREA COMM, V23, P464, DOI 10.1109/JSAC.2004.839391
   GOLLE P, 2001, P ANN S NETW DISTR S, P12
   GUO H, 2002, P 10 ACM INT C MULT, P362
   HALLE MW, 2004, P 12 ACM MULT OCT, P768
   Huang JL, 2004, IEEE INFOCOM SER, P2050
   *ISO IEC, 2001, ISOIEC144961
   Jung Min Park, 2003, ACM Transactions on Information and Systems Security, V6, P258, DOI 10.1145/762476.762480
   Kirovski D, 2002, ACM INT C MULT, P372
   Krohn MN, 2004, P IEEE S SECUR PRIV, P226
   Li KQ, 2005, ACM T MULTIM COMPUT, V1
   LI T, 2005, P INT C INF COMM SEC, P389
   LIBSIE M, 2002, P 10 ACM INT C MULT, P644
   Lin CY, 2001, IEEE T CIRC SYST VID, V11, P153, DOI 10.1109/76.905982
   Liu X., 2003, Proceedings of the Second IASTED International Conference on Communications, Internet, and Information Technology, P527
   Lysyanskaya A, 2004, P IEEE S SECUR PRIV, P241
   Lysyanskaya A, 2004, LECT NOTES COMPUT SC, V3027, P74
   MERKLE RC, 1990, LECT NOTES COMPUT SC, V435, P218, DOI 10.1007/0-387-34805-0_21
   Miner S, 2001, P IEEE S SECUR PRIV, P232, DOI 10.1109/SECPRI.2001.924301
   Miyazaki Kunihiko., 2006, Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security, ASIACCS '06, P343, DOI DOI 10.1145/1128817.1128868
   MOHAMMED S, 2005, P IEEE 3 ANN COMM NE
   OOI WT, 2001, P ACM MULT SEPT 30 S, P159
   Parmanto B., 2005, P 38 HAW INT C SYST
   PENG C, 2000, P ACM INT C MULT MM, P433
   Perrig A, 2000, P IEEE S SECUR PRIV, P56, DOI 10.1109/SECPRI.2000.848446
   PERRIG A, 2001, P ANN S NETW DISTR S
   Perrig A., 2001, P 8 ACM C COMP COMM, P28, DOI DOI 10.1145/501983.501988
   Rohatgi P, 1999, 6TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P93, DOI 10.1145/319709.319722
   Schneider M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P227, DOI 10.1109/ICIP.1996.560425
   SHEN B, 2003, STREAMING MEDIA CACH
   Shi C., 1998, P INT C MULTIMEDIA, P55
   Smith JR, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P7, DOI 10.1109/ICIP.1998.998987
   SUZUKI T, 2004, P IFIP C COMM MULT S
   Swaminathan A, 2006, IEEE T INF FOREN SEC, V1, P215, DOI 10.1109/TIFS.2006.873601
   TAUBMAN DS, 2000, JPEG2000 IMAGE COMPR
   Wagner Matthias., 2004, Proceedings of the 12th annual ACM international conference on Multimedia, MULTIMEDIA '04, P104
   Wu YD, 2006, IEEE T MULTIMEDIA, V8, P152, DOI 10.1109/TMM.2005.861283
   YEUNG SF, 2002, P ACM INT C MULT MM, P75
   YUUICHI T, 2003, IEICE T COMMUN, V428, P1463
   Zheng Y., 1991, PROC ASIACRYPT, P124
NR 59
TC 4
Z9 5
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2009
VL 5
IS 4
AR 28
DI 10.1145/1596990.1596992
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 529WO
UT WOS:000272549900002
OA Green Published
DA 2024-07-18
ER

PT J
AU Wang, J
   Yin, LJ
   Moore, J
AF Wang, Jun
   Yin, Lijun
   Moore, Jason
TI Using geometric properties of topographic manifold to detect and track
   eyes for human-computer interaction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; eye detection; eye tracking; topographic manifold;
   Bhattacharyya affinity; mutual information
ID PRIMAL SKETCH; RECOGNITION
AB Automatic eye detection and tracking is an important component for advanced human-computer interface design. Accurate eye localization can help develop a successful system for face recognition and emotion identification. In this article, we propose a novel approach to detect and track eyes using geometric surface features on topographic manifold of eye images. First, in the joint spatial-intensity domain, a facial image is treated as a 3D terrain surface or image topographic manifold. In particular, eye regions exhibit certain intrinsic geometric traits on this topographic manifold, namely, the pit-labeled center and hillside-like surround regions. Applying a terrain classification procedure on the topographic manifold of facial images, each location of the manifold can be labeled to generate a terrain map. We use the distribution of terrain labels to represent the eye terrain pattern. The Bhattacharyya affinity is employed to measure the distribution similarity between two topographic manifolds. Based on the Bhattacharyya kernel, a support vector machine is applied for selecting proper eye pairs from the pit-labeled candidates. Second, given detected eyes on the first frame of a video sequence, a mutual-information-based fitting function is defined to describe the similarity between two terrain surfaces of neighboring frames. By optimizing the fitting function, eye locations are updated for subsequent frames. The distinction of the proposed approach lies in that both eye detection and eye tracking are performed on the derived topographic manifold, rather than on an original-intensity image domain. The robustness of the approach is demonstrated under various imaging conditions and with different facial appearances, using both static images and video sequences without background constraints.
C1 [Wang, Jun] Columbia Univ, Dept Engn, New York, NY 10027 USA.
   [Yin, Lijun] SUNY Binghamton, Dept Comp Sci, Binghamton, NY 13902 USA.
   [Moore, Jason] AF Res Lab, Informat Syst Res Branch, Rome, NY 13441 USA.
C3 Columbia University; State University of New York (SUNY) System; State
   University of New York (SUNY) Binghamton; United States Department of
   Defense; United States Air Force
RP Wang, J (corresponding author), Columbia Univ, Dept Engn, New York, NY 10027 USA.
EM jw2494@columbia.edu; lijun@cs.binghamton.edu; Jason.Moore@rl.af.mil
CR [Anonymous], 1994, IEEE C COMP VIS PATT
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P IEEE WORKSH APPL C
   BESL PJ, 1988, IEEE T PATTERN ANAL, V10, P167, DOI 10.1109/34.3881
   Bhattacharyya Anil, 1943, B CALCUTTA MATH SOC, V35, P99
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Comaniciu D., P IEEE C COMP VIS PA
   Comaniciu D., 1999, P 7 IEEE INT C COMP
   Cross ADJ, 1999, IMAGE VISION COMPUT, V17, P337, DOI 10.1016/S0262-8856(98)00133-4
   DJERABA C, 2006, P 14 ANN INT C ACM M
   DJERABA C, 2005, STATE ART BYE TRACKI
   Gu JW, 2004, PATTERN RECOGN, V37, P543, DOI 10.1016/S0031-3203(03)00178-X
   GULLIVER S, 2005, P C HUM COMP INT HCI
   Gulliver SR, 2004, IEEE T SYST MAN CY A, V34, P472, DOI 10.1109/TSMCA.2004.826309
   HARALICK RM, 1983, INT J ROBOT RES, V2, P50, DOI 10.1177/027836498300200105
   HARO A, 2000, P IEEE INT C COMP VI
   Huang J, 2000, IEEE T EVOLUT COMPUT, V4, P73, DOI 10.1109/4235.843496
   Jebara T, 2004, J MACH LEARN RES, V5, P819
   JESORSKY O, 2001, P 3 INT C AUD VID BA, P90
   KARTIKEYAN B, 1995, SIGNAL PROCESS, V42, P71, DOI 10.1016/0165-1684(94)00117-I
   KONDOR R, 2003, P 10 INT C MACH LEAR
   LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346
   Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413
   MAGEE J, 2004, P IEEE CVPR WORKSH R
   MEER P, 1992, J VIS COMMUN IMAGE R, V3, P1
   METH R, 1996, P SPIE
   MORIMOTO C, 1998, WORKSH PERCEPTURAL U
   Morimoto CH, 2005, COMPUT VIS IMAGE UND, V98, P4, DOI 10.1016/j.cviu.2004.07.010
   *NIELS NORM GROUP, 2006, EY RES
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   PONG TC, 1985, PATTERN RECOGN, V18, P333, DOI 10.1016/0031-3203(85)90024-X
   RUDDARRAJU R, 2002, 5 INT C MULT INT
   SCHEIDERMAN H, 2004, IEEE C COMP VIS PATT
   Sukanya P, 1998, INT C PATT RECOG, P334, DOI 10.1109/ICPR.1998.711148
   TRIER O, 1995, P INT C DOC AN REC I
   WANG J, 2005, IEEE CVPR WORKSH VIS
   WANG J, 2007, IN PRESS COMPUT VISI
   WANG L, 1993, IEEE T PATTERN ANAL, V15, P1053, DOI 10.1109/34.254062
   Yin LJ, 2001, COMPUT VIS IMAGE UND, V84, P201, DOI 10.1006/cviu.2001.0949
   YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169
   ZHU Z, 2002, ACM SIGCHI S EY TRAC
   Zhu ZW, 2005, COMPUT VIS IMAGE UND, V98, P124, DOI 10.1016/j.cviu.2004.07.012
NR 42
TC 15
Z9 19
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 4
AR 21
DI 10.1145/1314303.1314306
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 250QU
UT WOS:000252315900003
DA 2024-07-18
ER

PT J
AU Cai, JF
   Li, XJ
   Chen, CW
AF Cai, Jianfei
   Li, Xiangjun
   Chen, Chang Wen
TI Layered Unequal Loss Protection with Pre-Interleaving for Fast
   Progressive Image Transmission Over Packet-Loss Channels
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Progressive image transmission; unequal loss protection;
   joint source-channel coding; forward error correction; packet loss
ID SET
AB Most existing unequal loss protection (ULP) schemes do not consider the minimum quality requirement and usually have high computation complexity. In this research, we propose a layered ULP (L-ULP) scheme to solve these problems. In particular, we use the rate-based optimal solution with a local search to find the average forward error correction (FEC) allocation and use the gradient search to find the FEC solution for each layer. Experimental results show that the executing time of L-ULP is much faster than the traditional ULP scheme but the average distortion is worse. Therefore, we further propose to combine the L-ULP with the pre-interleaving to have an improved L-ULP (IL-ULP) system. By using the pre-interleaving, we are able to delay the occurrence of the first unrecoverable loss in the source bitstream and thus improve the loss resilience performance. With the better loss resilience performance in the source bitstream, our proposed IL-ULP scheme is allowed to have a weaker FEC protection and allocate more bits to the source coding which leads to the improvement of overall performance. Experimental results show that our proposed IL-ULP scheme even outperforms the global optimal result obtained by any traditional ULP scheme while the complexity of IL-ULP is almost the same as L-ULP.
C1 [Cai, Jianfei; Li, Xiangjun] Nanyang Technol Univ, Ctr Multimedia & Network Technol, Sch Comp Engn, Singapore 639798, Singapore.
   [Chen, Chang Wen] Florida Inst Technol, Dept Elect & Comp Engn, Melbourne, FL 32901 USA.
C3 Nanyang Technological University; Florida Institute of Technology
RP Cai, JF (corresponding author), Nanyang Technol Univ, Ctr Multimedia & Network Technol, Sch Comp Engn, Singapore 639798, Singapore.
EM asjfcai@ntu.edu.sg; pg05304684@ntu.edu.sg; cchen@fit.edu
RI Cai, Jianfei/A-3691-2011
CR Chande V, 2000, IEEE J SEL AREA COMM, V18, P850, DOI 10.1109/49.848239
   Dumitrescu S, 2004, IEEE T MULTIMEDIA, V6, P230, DOI 10.1109/TMM.2003.822793
   GRANGETTO M, 2002, P INT C MULT EXP SWI, V2, P469
   HAGENAUER J, 2000, P 3 ITG C SOURC CHAN, P123
   HAMZAOUI R, 2002, P DCC 02 SNOWB UT AP, P67
   Kim J, 2003, IEEE T IMAGE PROCESS, V12, P121, DOI 10.1109/TIP.2003.809006
   Li ZG, 2003, IEEE T CIRC SYST VID, V13, P472, DOI 10.1109/TCSVT.2003.813420
   Mohr AE, 2000, IEEE J SEL AREA COMM, V18, P819, DOI 10.1109/49.848236
   Mohr AE, 2000, IEEE IMAGE PROC, P367, DOI 10.1109/ICIP.2000.900971
   Nosratinia A, 2003, IEEE T COMMUN, V51, P186, DOI 10.1109/TCOMM.2003.809256
   Puri R., 1999, Proceedings of the 33rd Asilomar Confe. Signals, Systems, V1, P342
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   SCHAAR M, 2001, IEEE T MULTIMEDIA, P381
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Stankovic V, 2003, IEEE WCNC, P317
   STANKOVIC V, 2002, P IEEE ICIP SNOWB UT
   Stockhammer T, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P169
   STOCKHAMMER T, 2001, P 11 INT PACK VID WO
   STUHLMULLER KW, 1999, PACK VID WORKSH
   Taubman DS, 2002, P IEEE, V90, P1336, DOI 10.1109/JPROC.2002.800725
   WESTERINK PH, 1988, P ICASSP 88, P757
   WICKER SB, 1994, REED SOLOMON CODES
   Wu ZY, 2002, IEEE IMAGE PROC, P213
   Zhang XM, 2003, IEEE T CIRC SYST VID, V13, P121, DOI 10.1109/TCSVT.2002.808437
NR 24
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2005
VL 1
IS 4
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DX
UT WOS:000205012500002
DA 2024-07-18
ER

PT J
AU Lan, X
   Yuan, Y
   Wang, X
   Chen, L
   Wang, Z
   Ma, L
   Zhu, W
AF Lan, Xiaohan
   Yuan, Yitian
   Wang, Xin
   Chen, Long
   Wang, Zhi
   Ma, Lin
   Zhu, Wenwu
TI A Closer Look at Debiased Temporal Sentence Grounding in Videos:
   Dataset, Metric, and Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Temporal sentence grounding in videos; dataset bias; evaluation metric;
   dataset re-splitting; out-of-distribution test
AB Temporal Sentence Grounding in Videos (TSGV), which aims to ground a natural language sentence that indicates complex human activities in an untrimmed video, has drawn widespread attention over the past few years. However, recent studies have found that current benchmark datasets may have obvious moment annotation biases, enabling several simple baselines even without training to achieve state-of-the-art (SOTA) performance. In this paper, we take a closer look at existing evaluation protocols for TSGV, and find that both the prevailing dataset splits and evaluation metrics are the devils that lead to untrustworthy benchmarking. Therefore, we propose to re-organize the two widely-used datasets, making the ground-truth moment distributions different in the training and test splits, i.e., out-of-distribution (OOD) test. Meanwhile, we introduce a new evaluation metric "dR@n,IoU=m" that discounts the basic recall scores especially with small IoU thresholds, so as to alleviate the inflating evaluation caused by biased datasets with a large proportion of long ground-truth moments. New benchmarking results indicate that our proposed evaluation protocols can better monitor the research progress in TSGV. Furthermore, we propose a novel causality-based Multi-branch Deconfounding Debiasing (MDD) framework for unbiased moment prediction. Specifically, we design a multi-branch deconfounder to eliminate the effects caused by multiple confounders with causal intervention. In order to help the model better align the semantics between sentence queries and video moments, we enhance the representations during feature encoding. Specifically, for textual information, the query is parsed into several verb-centered phrases to obtain a more fine-grained textual feature. For visual information, the positional information has been decomposed from the moment features to enhance the representations of moments with diverse locations. Extensive experiments demonstrate that our proposed approach can achieve competitive results among existing SOTA approaches and outperform the base model with great gains.
C1 [Lan, Xiaohan; Wang, Zhi] Univ Town Shenzhen, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Yuan, Yitian; Ma, Lin] Meituan, Tower C,Hengjiweiye Bldg,4 Wangjing East Rd, Beijing 100102, Peoples R China.
   [Wang, Xin; Zhu, Wenwu] Tsinghua Univ, 30 Shuangqing Rd, Beijing 100084, Peoples R China.
   [Chen, Long] Columbia Univ, 500 W 120th St Mudd 510, New York, NY 10027 USA.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University;
   Columbia University
RP Zhu, W (corresponding author), Tsinghua Univ, 30 Shuangqing Rd, Beijing 100084, Peoples R China.; Chen, L (corresponding author), Columbia Univ, 500 W 120th St Mudd 510, New York, NY 10027 USA.
EM lanxh20@mails.tsinghua.edu.cn; yuanyitian@foxmail.com;
   xin_wang@tsinghua.edu.cn; zjuchenlong@gmail.com;
   wangzhi@sz.tsinghua.edu.cn; forest.linma@gmail.com;
   wwzhu@tsinghua.edu.cn
RI wang, nan/KHW-4897-2024; li, lan/KCJ-5061-2024; zhang,
   zheng/KHY-8870-2024; Wang, Fei/KEH-6292-2024; ZHANG, JING/KHY-1073-2024;
   Chen, Nuo/JZD-0344-2024; li, qing/KHU-6871-2024; li, jing/KHC-8303-2024;
   ZHOU, YUE/KCJ-8790-2024; Wang, Zhi/GZB-2713-2022; Lu, Xin/KHW-8570-2024;
   li, li/KHE-5750-2024; li, yf/KHX-1148-2024; Chen, Long/HZJ-7271-2023;
   zhang, yan/KHC-3163-2024; Wang, Chen/JZE-6385-2024; Wang,
   Jiachen/KFT-0161-2024; zhang, yuanyuan/KHV-4459-2024; yang,
   ying/KHW-9378-2024; lin, lin/KFB-9548-2024; wang, jin/KHD-7243-2024;
   jing, wang/KCZ-2144-2024; Zhang, Yi/KHW-2039-2024; yang,
   le/KFB-5420-2024
OI Wang, Zhi/0000-0001-6952-8848; Lu, Xin/0000-0001-9885-6031; Chen,
   Long/0000-0001-6148-9709; Wang, Zhi/0000-0002-5462-6178; Wang,
   Xin/0000-0002-0351-2939; Lan, Xiaohan/0000-0001-5382-6699
FU National Key Research and Development Program of China [2020AAA0106300];
   National Natural Science Foundation of China [62250008, 62222209,
   62102222, 61872215]
FX This work was supported in part by the National Key Research and
   Development Program of China No. 2020AAA0106300 and National Natural
   Science Foundation of China (No. 62250008, 62222209, 62102222,
   61872215).
CR Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cadene R., 2019, P INT C NEUR INF PRO, P839
   Cao M, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9810
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen L, 2023, Arxiv, DOI arXiv:2110.01013
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Clark C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4069
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan X, 2018, ADV NEUR IN, V31
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao Junyu, 2021, P IEEECVF INT C COMP
   Gao MF, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1481
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Gokhale T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P878
   Grand Gabriel, 2019, P 2 WORKSH SHORTC VI, P1
   Hahn Meera, 2020, BRIT MACHINE VISION
   He DL, 2019, AAAI CONF ARTIF INTE, P8393
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang Jiabo, 2021, P IEEE CVF INT C COM, P7199
   Jiang B, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P217, DOI 10.1145/3323873.3325019
   Kingma Diederik P., 2015, P 3 INT C LEARN
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu W., 2019, Neural Inf. Process. Syst., P536
   Long Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10797, DOI 10.1109/CVPR42600.2020.01081
   Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Niu YL, 2021, PROC CVPR IEEE, P12695, DOI 10.1109/CVPR46437.2021.01251
   Otani Mayu, 2020, BRIT MACHINE VISION
   Peng YX, 2017, FRONT INFORM TECH EL, V18, P44, DOI 10.1631/FITEE.1601787
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Ramakrishnan S., 2018, INT C NEURAL INF PRO, V31, P1541
   Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Song XM, 2018, LECT NOTES COMPUT SC, V11165, P340, DOI 10.1007/978-3-030-00767-6_32
   Song YJ, 2020, Arxiv, DOI arXiv:2003.07048
   Tan Reuben, 2019, ARXIV
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wu J, 2020, AAAI CONF ARTIF INTE, V34, P12386
   Wu ZZ, 2022, IEEE T NEUR NET LEAR, V33, P6249, DOI 10.1109/TNNLS.2021.3073016
   Xiao SN, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P4008
   Xiao SN, 2021, AAAI CONF ARTIF INTE, V35, P2986
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Yang X, 2023, IEEE T PATTERN ANAL, V45, P12996, DOI 10.1109/TPAMI.2021.3121705
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Yuan Yitian, 2021, HUMA'21: Proceedings of the 2nd International Workshop on Human-centric Multimedia Analysis, P13, DOI 10.1145/3475723.3484247
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, CVPR
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhou H, 2021, PROC CVPR IEEE, P8441, DOI 10.1109/CVPR46437.2021.00834
   Zhu WW, 2015, IEEE MULTIMEDIA, V22, P96, DOI 10.1109/MMUL.2015.66
NR 60
TC 3
Z9 3
U1 1
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 218
DI 10.1145/3565573
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200040
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, BZ
   Lei, JJ
   Peng, B
   Yu, CB
   Li, WQ
   Ling, N
AF Liu, Bingzheng
   Lei, Jianjun
   Peng, Bo
   Yu, Chuanbo
   Li, Wanqing
   Ling, Nam
TI Novel View Synthesis from a Single Unposed Image via Unsupervised
   Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimedia applications; 3D display; unsupervised single-view synthesis;
   token transformation module; view generation module
ID NETWORK
AB Novel view synthesis aims to generate novel views from one or more given source views. Although existing methods have achieved promising performance, they usually require paired views with different poses to learn a pixel transformation. This article proposes an unsupervised network to learn such a pixel transformation from a single source image. In particular, the network consists of a token transformation module that facilities the transformation of the features extracted froma source image into an intrinsic representation with respect to a pre-defined reference pose and a view generation module that synthesizes an arbitrary view from the representation. The learned transformation allows us to synthesize a novel view from any single source image of an unknown pose. Experiments on the widely used view synthesis datasets have demonstrated that the proposed network is able to produce comparable results to the state-of-the-art methods despite the fact that learning is unsupervised and only a single source image is required for generating a novel view. The code will be available upon the acceptance of the article.
C1 [Liu, Bingzheng; Lei, Jianjun] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Li, Wanqing] Univ Wollongong, Adv Multimedia Res Lab, Wollongong, Australia.
   [Ling, Nam] Santa Clara Univ, Dept Comp Sci & Engn, Santa Clara, CA 95053 USA.
C3 Tianjin University; University of Wollongong; Santa Clara University
RP Lei, JJ (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM bzliu@tju.edu.cn; jjlei@tju.edu.cn; bpeng@tju.edu.cn; cbyu@tju.edu.cn;
   wanqing@uow.edu.au; nling@scu.edu
OI Liu, Bingzheng/0000-0002-6949-4147; Li, Wanqing/0000-0002-4427-2687
FU National Key R&D Program of China [2021YFB2802300]; National Natural
   Science Foundation of China [62125110, 62101379, 61931014]; Natural
   Science Foundation of Tianjin [21JCQNJC01520]; China Postdoctoral
   Science Foundation [2022M712371, 2021TQ0244]
FX The work of Jianjun Lei and Bo Peng was supported in part by the
   National Key R&D Program of China (No. 2021YFB2802300), National Natural
   Science Foundation of China (No. 62125110, 62101379, 61931014), Natural
   Science Foundation of Tianjin under Grant (No. 21JCQNJC01520), and China
   Postdoctoral Science Foundation under Grant (No. 2022M712371,
   2021TQ0244).
CR Atrey PK, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3312574
   Bekhet S, 2018, ACM T INFORM SYST, V36, DOI 10.1145/3190784
   Carballeira P, 2022, IEEE T MULTIMEDIA, V24, P2378, DOI 10.1109/TMM.2021.3079711
   Chen B, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3366371
   Chen D, 2021, OPT EXPRESS, V29, P7866, DOI 10.1364/OE.419069
   Chen S, 2022, IEEE T CIRC SYST VID, V32, P1328, DOI 10.1109/TCSVT.2021.3068834
   Chen X, 2019, IEEE I CONF COMP VIS, P4089, DOI 10.1109/ICCV.2019.00419
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761
   Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P2625, DOI 10.1109/TIP.2014.2305100
   Gao W, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3459098
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo JX, 2022, PROC CVPR IEEE, P18228, DOI 10.1109/CVPR52688.2022.01771
   Guo PS, 2022, IEEE WINT CONF APPL, P11, DOI 10.1109/WACV51458.2022.00009
   Habtegebrial T, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5, P792, DOI 10.5220/0007360107920799
   Hani Nicolai, 2020, NEURIPS, P6086
   Henderson P, 2020, INT J COMPUT VISION, V128, P835, DOI 10.1007/s11263-019-01219-8
   Hou YX, 2021, IEEE WINT CONF APPL, P3118, DOI 10.1109/WACV48630.2021.00316
   Hu Ronghang, 2021, INT C COMPUTER VISIO, P12528
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Jiang L, 2022, NEUROCOMPUTING, V496, P35, DOI 10.1016/j.neucom.2022.04.123
   Kar A., 2017, P ADV NEUR INF PROC, P364
   Kingma Diederik P., 2015, P 3 INT C LEARN
   Koutaki G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925949
   Lata Kusam, 2019, 2019 3rd International Conference on Electronics, Communication and Aerospace Technology (ICECA). Proceedings, P186, DOI 10.1109/ICECA.2019.8822195
   Lei JJ, 2022, IEEE T IMAGE PROCESS, V31, P6707, DOI 10.1109/TIP.2022.3203213
   Lin CH, 2018, AAAI CONF ARTIF INTE, P7114
   Lin Chen-Hsuan, 2020, Advances in Neural Information Processing Systems, P11453
   Liu C., 2022, Asian Soc Sci, V18, P1
   Liu MM, 2018, PROC CVPR IEEE, P4616, DOI 10.1109/CVPR.2018.00485
   Liu Xiaofeng, 2020, P ECCV, P52
   Mingyu Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P87, DOI 10.1007/978-3-030-58604-1_6
   Olszewski K, 2019, IEEE I CONF COMP VIS, P7647, DOI 10.1109/ICCV.2019.00774
   Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82
   Paszke Adam, 2015, ADV NEURAL INFORM PR
   Peng B, 2023, IEEE T CIRC SYST VID, V33, P4041, DOI 10.1109/TCSVT.2023.3238580
   Peng B, 2022, IEEE T CIRC SYST VID, V32, P8342, DOI 10.1109/TCSVT.2022.3190916
   Pontes JK, 2019, LECT NOTES COMPUT SC, V11361, P365, DOI 10.1007/978-3-030-20887-5_23
   Ramirez Pierluigi Zama, 2021, arXiv
   Rematas K, 2017, IEEE T PATTERN ANAL, V39, P1576, DOI 10.1109/TPAMI.2016.2601093
   SHEPARD RN, 1971, SCIENCE, V171, P701, DOI 10.1126/science.171.3972.701
   Sun SH, 2018, LECT NOTES COMPUT SC, V11207, P162, DOI 10.1007/978-3-030-01219-9_10
   TatarchenkoMaxim Alexey Dosovitskiy, 2015, Knowl. Inf. Syst., V38, P231
   Thu NP, 2018, ADV NEUR IN, V31
   Tian FZ, 2022, IEEE T CIRC SYST VID, V32, P1751, DOI 10.1109/TCSVT.2021.3080928
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Tulsiani S, 2018, PROC CVPR IEEE, P302, DOI 10.1109/CVPR.2018.00039
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   van der Hooft J, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3362101
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiles Olivia, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7465, DOI 10.1109/CVPR42600.2020.00749
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xie JY, 2016, LECT NOTES COMPUT SC, V9908, P842, DOI 10.1007/978-3-319-46493-0_51
   Xu XG, 2019, IEEE I CONF COMP VIS, P7790, DOI 10.1109/ICCV.2019.00788
   Xu ZX, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323007
   Yan JB, 2022, IEEE T IMAGE PROCESS, V31, P3896, DOI 10.1109/TIP.2022.3177127
   Yan XC, 2016, ADV NEUR IN, V29
   Youngjoong Kwon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P387, DOI 10.1007/978-3-030-58548-8_23
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Zheng ZQ, 2023, IEEE T MULTIMEDIA, V25, P2474, DOI 10.1109/TMM.2022.3147425
   Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zhu H, 2018, PROC CVPR IEEE, P4450, DOI 10.1109/CVPR.2018.00468
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 66
TC 0
Z9 0
U1 5
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 186
DI 10.1145/3587467
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200009
DA 2024-07-18
ER

PT J
AU Xu, BQ
   Liang, J
   He, LX
   Wu, JL
   Fan, C
   Sun, ZN
AF Xu, Boqiang
   Liang, Jian
   He, Lingxiao
   Wu, Jinlin
   Fan, Chao
   Sun, Zhenan
TI Color-Unrelated Head-Shoulder Networks for Fine-Grained Person
   Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; fine-grained matching; visual surveillance
AB Person re-identification (re-id) attempts to match pedestrian images with the same identity across non-overlapping cameras. Existing methods usually study person re-id by learning discriminative features based on the clothing attributes (e.g., color, texture). However, the clothing appearance is not sufficient to distinguish different persons especially when they are in similar clothes, which is known as the fine-grained (FG) person re-id problem. By contrast, this paper proposes to exploit the color-unrelated feature along with the head-shoulder feature for FG person re-id. Specifically, a color-unrelated head-shoulder network (CUHS) is developed, which is featured in three aspects: (1) It consists of a lightweight head-shoulder segmentation layer for localizing the head-shoulder region and learning the corresponding feature. (2) It exploits instance normalization (IN) for learning color-unrelated features. (3) As IN inevitably reduces inter-class differences, we propose to explore richer visual cues for IN by an attention exploration mechanism to ensure high discrimination. We evaluate our model on the FG-reID, Market1501, and DukeMTMC-reID datasets, and the results show that CUHS surpasses previous methods on both the FG and conventional person re-id problems.
C1 [Xu, Boqiang] Univ Chinese Acad Sci, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Liang, Jian; Sun, Zhenan] Chinese Acad Sci, Inst Automat, CRIPAC, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Liang, Jian; Wu, Jinlin; Sun, Zhenan] Chinese Acad Sci, Inst Automat, MAIS, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [He, Lingxiao] AI Res JD, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Wu, Jinlin] Chinese Acad Sci, Ctr Artificial Intelligence & Robot, HKISI, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Fan, Chao] Chengdu Discaray Technol Co Ltd, 95 Zhongguancun East Rd, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS; Chinese Academy of
   Sciences
RP Liang, J (corresponding author), Chinese Acad Sci, Inst Automat, CRIPAC, 95 Zhongguancun East Rd, Beijing, Peoples R China.; Liang, J (corresponding author), Chinese Acad Sci, Inst Automat, MAIS, 95 Zhongguancun East Rd, Beijing, Peoples R China.
EM boqiang.xu@cripac.ia.ac.cn; helingxiao3@jd.com
RI YANG, DAN/KCL-5217-2024; chen, shuo bing/KHV-7129-2024; Guo,
   Li/KCK-9540-2024; Wu, Jinlin/ABH-1288-2021; WEI, ZHEN/KHU-7176-2024
OI Wu, Jinlin/0000-0001-6805-1822; liang, jian/0000-0003-3890-1894
FU National Natural Science Foundation of China [62276256]; Beijing Nova
   435 Program [Z211100002121108]
FX This work is supported by the National Natural Science Foundation of
   China (Grant No. 434 U1836217), National Natural Science Foundation of
   China (Grant No. 62276256) and Beijing Nova 435 Program under Grant
   Z211100002121108.
CR Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Chen GY, 2021, IEEE T IMAGE PROCESS, V30, P7663, DOI 10.1109/TIP.2021.3107211
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen Xiaodong, 2021, P IEEECVF INT C COMP, P11813
   Dai YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11844, DOI 10.1109/ICCV48922.2021.01165
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Dumoulin V, 2017, Arxiv, DOI [arXiv:1610.07629, DOI 10.48550/ARXIV.1610.07629]
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2021, Arxiv, DOI arXiv:2108.05045
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   Hermans Alexander, 2017, ARXIV170307737
   Huang Y, 2020, IEEE T CIRC SYST VID, V30, P3459, DOI 10.1109/TCSVT.2019.2948093
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Isobe T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8506, DOI 10.1109/ICCV48922.2021.00841
   Jaderberg M, 2015, ADV NEUR IN, V28
   Ji HXY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3641, DOI 10.1109/ICCV48922.2021.00364
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Kingma D. P., 2014, arXiv
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Kviatkovsky Igor, 2013, IEEE Trans Pattern Anal Mach Intell, V35, P1622, DOI 10.1109/TPAMI.2012.246
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li SQ, 2016, INT CONF CLOUD COMPU, P480, DOI 10.1109/CCIS.2016.7790306
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Meng Jingke, 2021, IEEE T PATTERN ANAL
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899
   Qi L, 2018, ARXIV
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Quispe R, 2021, INT C PATT RECOG, P2980, DOI 10.1109/ICPR48806.2021.9412017
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Ruan WJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P284, DOI 10.1145/3343031.3350984
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Somers V, 2023, IEEE WINT CONF APPL, P1613, DOI 10.1109/WACV56688.2023.00166
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Suh Yumin, 2018, ARXIV
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang HC, 2022, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR52688.2022.00715
   Wang XP, 2021, IEEE T IMAGE PROCESS, V30, P3017, DOI 10.1109/TIP.2021.3056223
   Wang YN, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3422, DOI 10.1145/3394171.3413815
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu JL, 2023, NEUROCOMPUTING, V518, P155, DOI 10.1016/j.neucom.2022.11.009
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P4651, DOI 10.1109/TIP.2022.3186759
   Xu BQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P673, DOI 10.1145/3394171.3414056
   Xu Boqiang, 2022, ECCV
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yang JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11865, DOI 10.1109/ICCV48922.2021.01167
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Yang Z., 2022, P IEEECVF C COMPUTER
   Yin J, 2020, INT J COMPUT VISION, V128, P1654, DOI 10.1007/s11263-019-01259-0
   Zhang TY, 2021, PROC CVPR IEEE, P11501, DOI 10.1109/CVPR46437.2021.01134
   Zhang X, 2017, ARXIV
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8351, DOI 10.1109/ICCV48922.2021.00826
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 84
TC 0
Z9 0
U1 4
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 210
DI 10.1145/3599730
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200033
DA 2024-07-18
ER

PT J
AU Hui, SX
   Guo, Q
   Geng, XY
   Zhang, CM
AF Hui, Shuaixiong
   Guo, Qiang
   Geng, Xiaoyu
   Zhang, Caiming
TI Multi-Guidance CNNs for Salient Object Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Salient object detection; self-guidance; cross-guidance; multi-level
   feature aggregation; pixelwise contrast loss
ID NETWORK
AB Feature refinement and feature fusion are two key steps in convolutional neural networks-based salient object detection (SOD). In this article, we investigate how to utilize multiple guidance mechanisms to better refine and fuse extracted multi-level features and propose a novel multi-guidance SOD model dubbed as MGuid-Net. Since boundary information is beneficial for locating and sharpening salient objects, edge features are utilized in our network together with saliency features for SOD. Specifically, a self-guidance module is applied to multi-level saliency features and edge features, respectively, which aims to gradually guide the refinement of lower-level features by higher-level features. After that, a cross-guidance module is devised to mutually refine saliency features and edge features via the complementarity between them. Moreover, to better integrate refined multi-level features, we also present an accumulative guidance module, which exploits multiple high-level features to guide the fusion of different features in a hierarchical manner. Finally, a pixelwise contrast loss function is adopted as an implicit guidance to help our network retain more details in salient objects. Extensive experiments on five benchmark datasets demonstrate our model can identify salient regions of an image more effectively compared to most of state-of-the-art models.
C1 [Hui, Shuaixiong; Guo, Qiang; Geng, Xiaoyu] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, 7366 East Erhuan Rd, Jinan 250014, Peoples R China.
   [Hui, Shuaixiong; Guo, Qiang; Geng, Xiaoyu] Shandong Prov Key Lab Digital Media Technol, 7366 East Erhuan Rd, Jinan 250014, Peoples R China.
   [Zhang, Caiming] Shandong Univ, Sch Software, 1500 Shunhua Rd, Jinan 250101, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong University
RP Guo, Q (corresponding author), Shandong Univ Finance & Econ, Sch Comp Sci & Technol, 7366 East Erhuan Rd, Jinan 250014, Peoples R China.; Guo, Q (corresponding author), Shandong Prov Key Lab Digital Media Technol, 7366 East Erhuan Rd, Jinan 250014, Peoples R China.
EM huishuaixiong@mail.sdufe.edu.cn; guoqiang@sdufe.edu.cn;
   gengxiaoyu@mail.sdufe.edu.cn; czhang@sdu.edu.cn
RI Hui, Shuaixiong/KII-3891-2024; Zhang, Caiming/AHD-6558-2022; Guo,
   Qiang/I-2949-2019
OI Hui, Shuaixiong/0000-0002-0382-3112; Zhang, Caiming/0000-0002-6365-6221;
   Guo, Qiang/0000-0003-4219-3528; zhang, caiming/0000-0003-0217-1543
FU National Natural Science Foundation of China [61873145]; Natural Science
   Foundation of Shandong Province for Excellent Young Scholars
   [ZR2017JL029]; Science and Technology Innovation Program for
   Distinguished Young Scholars of Shandong Province Higher Education
   Institutions [2019KJN045]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61873145, in part by the Natural Science
   Foundation of Shandong Province for Excellent Young Scholars under Grant
   ZR2017JL029, and in part by the Science and Technology Innovation
   Program for Distinguished Young Scholars of Shandong Province Higher
   Education Institutions under Grant 2019KJN045.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3763, DOI 10.1109/TIP.2020.2965989
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong B, 2021, NEUROCOMPUTING, V437, P58, DOI 10.1016/j.neucom.2021.01.034
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Guan WL, 2019, IEEE SIGNAL PROC LET, V26, P114, DOI 10.1109/LSP.2018.2881835
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu XW, 2018, AAAI CONF ARTIF INTE, P6943
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li KP, 2020, IEEE T PATTERN ANAL, V42, P2996, DOI 10.1109/TPAMI.2019.2921543
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liao GB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2436, DOI 10.1145/3394171.3413523
   Lin F, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3440694
   Liu JJ, 2021, IEEE T IMAGE PROCESS, V30, P9030, DOI 10.1109/TIP.2021.3122093
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu Y, 2022, IEEE T CYBERNETICS, V52, P6131, DOI 10.1109/TCYB.2021.3051350
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu Shanmei, 2020, IEEE ACCESS, V8
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Shi R, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491229
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LZ, 2019, IEEE T PATTERN ANAL, V41, P1734, DOI 10.1109/TPAMI.2018.2846598
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang YP, 2019, IEEE T IMAGE PROCESS, V28, P2813, DOI 10.1109/TIP.2019.2891055
   Wen HF, 2021, IEEE T IMAGE PROCESS, V30, P9179, DOI 10.1109/TIP.2021.3123548
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang S, 2021, IEEE T IMAGE PROCESS, V30, P8426, DOI 10.1109/TIP.2021.3113794
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107956
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhao ZR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4967, DOI 10.1145/3474085.3475494
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhuge YZ, 2018, IEEE SIGNAL PROC LET, V25, P1800, DOI 10.1109/LSP.2018.2875586
NR 60
TC 6
Z9 6
U1 5
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 117
DI 10.1145/3570507
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300017
OA Bronze
DA 2024-07-18
ER

PT J
AU Liu, ZM
   Niu, K
   He, ZQ
AF Liu, Zhiming
   Niu, Kai
   He, Zhiqiang
TI ML-CookGAN: Multi-Label Generative Adversarial Network for Food Image
   Generation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Food image; fine-grained image generation; generative adversarial
   network
AB Generating food images from recipe and ingredient information can be applied to many tasks such as food recommendation, recipe development, and health management. For the characteristics of food images, this paper proposes ML-CookGAN, a novel CGAN. This network enables the generation of food images based on recipe and ingredient labels. The generator of ML-CookGAN, Multi-Label Fusion Generator, converts recipe and ingredient labels into different granularity features and generates corresponding food images. The discriminator of ML-CookGAN, Multi-Branch Discriminator, implements discrimination and classification with a multi-branch structure. In addition, we propose two training strategies, Region-Wise Pooling and Image Style Distillation, to better the network performance. Region-Wise Pooling handles region-wise features with the discriminator. Image Style Distillation aims at extracting image latent features to assist image generation by an unsupervised method. The experiments conducted on VIREO Food-172 databases validate the proposed method to generate high-quality Chinese food images. And Region-Wise Pooling and Image Style Distillation are proven to enhance the diversity and realism of generated food images.
C1 [Liu, Zhiming; Niu, Kai; He, Zhiqiang] Beijing Univ Posts & Telecommun, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP He, ZQ (corresponding author), Beijing Univ Posts & Telecommun, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
EM Liu_zhiming@bupt.edu.cn; niukai@bupt.edu.cn; hezq@bupt.edu.cn
RI liu, zhiming/HPF-9707-2023
OI liu, zhiming/0000-0002-6133-4632
FU National Key Research and Development Program of China [2021YFE0205300];
   Capital's Funds for Health Improvement and Research [2020-2-4079];
   Fundamental Research Funds for the Central Universities [2020XD-A02-3]
FX This work was supported by the National Key Research and Development
   Program of China (2021YFE0205300), Capital's Funds for Health
   Improvement and Research (2020-2-4079), and Fundamental Research Funds
   for the Central Universities (2020XD-A02-3).
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Barratt S, 2018, Arxiv, DOI [arXiv:1801.01973, DOI 10.48550/ARXIV.1801.01973]
   Bolaños M, 2017, LECT NOTES COMPUT SC, V10590, P394, DOI 10.1007/978-3-319-70742-6_37
   Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Chen JJ, 2021, IEEE T IMAGE PROCESS, V30, P1514, DOI 10.1109/TIP.2020.3045639
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Cho J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2217, DOI 10.1145/3343031.3350604
   Dash A., 2021, arXiv, DOI DOI 10.48550/ARXIV.2110.01442
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gui J, 2020, Arxiv, DOI [arXiv:2001.06937, DOI 10.1109/TKDE.2021.3130191]
   Gulrajani I, 2017, Arxiv, DOI arXiv:1704.00028
   Han FD, 2021, Arxiv, DOI arXiv:2012.02821
   Han FD, 2020, IEEE WINT CONF APPL, P1439, DOI [10.1109/WACV45572.2020.9093463, 10.1109/wacv45572.2020.9093463]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liang Haozan, 2020, IEEE T MULTIMEDIA
   Liu CX, 2021, IEEE T CIRC SYST VID, V31, P2480, DOI 10.1109/TCSVT.2020.3020079
   Liu Steven, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14274, DOI 10.1109/CVPR42600.2020.01429
   Mariani G, 2018, Arxiv, DOI arXiv:1803.09655
   Marin J, 2021, IEEE T PATTERN ANAL, V43, P187, DOI 10.1109/TPAMI.2019.2927476
   Min WQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1331, DOI 10.1145/3343031.3350948
   Min WQ, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329168
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Miyato T, 2018, Arxiv, DOI arXiv:1802.05957
   Odena A, 2017, PR MACH LEARN RES, V70
   Pan SY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4244, DOI 10.1145/3394171.3413636
   Papadopoulos DP, 2019, PROC CVPR IEEE, P7994, DOI 10.1109/CVPR.2019.00819
   Salimans T, 2016, ADV NEUR IN, V29
   Salvador A, 2019, PROC CVPR IEEE, P10445, DOI 10.1109/CVPR.2019.01070
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Theis L, 2016, Arxiv, DOI arXiv:1511.01844
   Trattner C, 2017, Arxiv, DOI arXiv:1711.02760
   Zheng H, 2015, IEEE IJCNN
   Zhu B, 2020, PROC CVPR IEEE, P5518, DOI 10.1109/CVPR42600.2020.00556
   Zhu B, 2019, PROC CVPR IEEE, P11469, DOI 10.1109/CVPR.2019.01174
NR 39
TC 2
Z9 2
U1 3
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 85
DI 10.1145/3554738
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300010
DA 2024-07-18
ER

PT J
AU De Divitiis, L
   Becattini, F
   Baecchi, C
   Del Bimbo, A
AF De Divitiis, Lavinia
   Becattini, Federico
   Baecchi, Claudio
   Del Bimbo, Alberto
TI Disentangling Features for Fashion Recommendation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Garment recommendation; memory augmented neural networks; recommendation
   systems
AB Online stores have become fundamental for the fashion industry, revolving around recommendation systems to suggest appropriate items to customers. Such recommendations often suffer from a lack of diversity and propose items that are similar to previous purchases of a user. Recently, a novel kind of approach based on Memory Augmented Neural Networks (MANNs) has been proposed, aimed at recommending a variety of garments to create an outfit by complementing a given fashion item. In this article we address the task of compatible garment recommendation developing a MANN architecture by taking into account the co-occurrence of clothing attributes, such as shape and color, to compose an outfit. To this end we obtain disentangled representations of fashion items and store them in external memory modules, used to guide recommendations at inference time. We show that our disentangled representations are able to achieve significantly better performance compared to the state of the art and also provide interpretable latent spaces, giving a qualitative explanation of the recommendations.
C1 [De Divitiis, Lavinia; Becattini, Federico; Baecchi, Claudio; Del Bimbo, Alberto] Univ Florence, Florence, Italy.
C3 University of Florence
RP De Divitiis, L (corresponding author), Univ Florence, Florence, Italy.
EM lavinia.dedivitiis@unifi.it; federico.becattini@unifi.it;
   claudio.baecchi@unifi.it; alberto.delbimbo@unifi.it
RI Becattini, Federico/AAE-8554-2021
OI Becattini, Federico/0000-0003-2537-2700; DEL BIMBO,
   ALBERTO/0000-0002-1052-8322
FU Italian MIUR within PRIN 2017 [20172BH297]
FX This work was partially supported by the Italian MIUR within PRIN 2017,
   Project Grant 20172BH297: I-MALL - improving the customer experience in
   stores by intelligent computer vision.
CR Balntas V., 2016, Bmvc, DOI DOI 10.5244/C.30.119
   Becattini F., 2021, ACM MULTIMEDIA ASIA, P1
   Bigi Wolmer, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P2472, DOI 10.1145/3394171.3413530
   Cheng W.-H., 2020, ARXIV
   Cucurull G, 2019, PROC CVPR IEEE, P12609, DOI 10.1109/CVPR.2019.01290
   De Divitiis L, 2021, INT WORK CONTENT MUL, P1, DOI 10.1109/CBMI50038.2021.9461912
   De Divitiis Lavinia, 2020, PATTERN RECOGN, V12662, P282, DOI [10.1007/978-3-030-68790-8_23, DOI 10.1007/978-3-030-68790-8_23]
   Feng ZL, 2018, Arxiv, DOI arXiv:1806.04845
   Gao GY, 2019, MULTIMEDIA SYST, V25, P593, DOI 10.1007/s00530-019-00617-9
   Graves A, 2014, Arxiv, DOI arXiv:1410.5401
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   Heinz S, 2017, Arxiv, DOI arXiv:1708.07347
   Hou YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12127, DOI 10.1109/ICCV48922.2021.01193
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Jagadeesh V, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1925, DOI 10.1145/2623330.2623332
   Kang W., 2017, ARXIV
   Liu JH, 2019, NEUROCOMPUTING, V359, P249, DOI 10.1016/j.neucom.2019.05.081
   Lu Z, 2019, PROC CVPR IEEE, P10554, DOI 10.1109/CVPR.2019.01081
   Marchetti F., 2020, IEEE T PATTERN ANAL
   Marchetti F, 2024, Arxiv, DOI arXiv:2203.12446
   Marchetti F, 2020, PROC CVPR IEEE, P7141, DOI 10.1109/CVPR42600.2020.00717
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Pernici Federico, 2020, COMPUT VIS IMAGE UND, V2020, P197
   Rendle S, 2012, Arxiv, DOI arXiv:1205.2618
   Sagar D, 2020, 2020 IEEE SIXTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2020), P221, DOI 10.1109/BigMM50055.2020.00039
   Song XM, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P320, DOI 10.1145/3343031.3350956
   Song XM, 2018, ACM/SIGIR PROCEEDINGS 2018, P5, DOI 10.1145/3209978.3209996
   Song XM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P753, DOI 10.1145/3123266.3123314
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Wang X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P329, DOI 10.1145/3343031.3350909
   Weston J, 2015, Arxiv, DOI arXiv:1410.3916
   Yu WH, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P649, DOI 10.1145/3178876.3186146
   Zhang JJ, 2020, J TEXT I, V111, P1324, DOI 10.1080/00405000.2019.1694351
NR 37
TC 13
Z9 13
U1 5
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
AR 39
DI 10.1145/3531017
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800013
OA Green Published
DA 2024-07-18
ER

PT J
AU Wang, Q
   Li, S
   Zhang, XP
   Feng, GR
AF Wang, Quan
   Li, Sheng
   Zhang, Xinpeng
   Feng, Guorui
TI Multi-granularity Brushstrokes Network for Universal Style Transfer
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Neural style transfer; deep feature; brushstrokes
AB Neural style transfer has been developed in recent years, where both performance and efficiency have been greatly improved. However, most existing methods do not transfer the brushstrokes information of style images well. In this article, we address this issue by training a multi-granularity brushstrokes network based on a parallel coding structure. Specifically, we first adopt the content parsing module to obtain the spatial distribution of content image and the smoothness of different regions. Then, different brushstrokes features are transformed by a multi-granularity style-swap module guided by the region content map. Finally, the stylized features of the two branches are fused to enhance the stylized results. Themulti-granularity brushstrokes network is jointly supervised by a new multi-layer brushstroke loss and pre-existing loss. The proposed method is close to the artistic drawing process. In addition, we can control whether the color of the stylized results tend to be the style image or the content image. Experimental results demonstrate the advantage of our proposed method compare with the existing schemes.
C1 [Wang, Quan; Zhang, Xinpeng; Feng, Guorui] Shanghai Univ, Sch Commun & Informat Engn, 381 Shangda Rd, Shanghai, Peoples R China.
   [Li, Sheng] Shanghai Inst Intelligent Elect & Syst, Sch Comp Sci, Shanghai, Peoples R China.
C3 Shanghai University
RP Feng, GR (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, 381 Shangda Rd, Shanghai, Peoples R China.
EM quan_w@shu.edu.cn; lisheng@fudan.edu.cn; xzhang@shu.edu.cn;
   grfeng@shu.edu.cn
FU science and technology planning project of Zhejiang Province
   [2022C01090]; Natural Science Foundation of China [62072295, 62072114,
   U20A20178, U1936214]
FX This work was supported in part by the science and technology planning
   project of Zhejiang Province under Grant No. 2022C01090 and the Natural
   Science Foundation of China under Grants No. 62072295, No. 62072114, No.
   U20A20178, and No. U1936214.
CR Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623
   Deng YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2719, DOI 10.1145/3394171.3414015
   Frigo O, 2016, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2016.66
   Fu YF, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3429742
   Fu YF, 2019, IEEE T VIS COMPUT GR, V25, P2763, DOI 10.1109/TVCG.2018.2860004
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jing YC, 2018, LECT NOTES COMPUT SC, V11217, P244, DOI 10.1007/978-3-030-01261-8_15
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Lee H, 2011, COMPUT GRAPH-UK, V35, P81, DOI 10.1016/j.cag.2010.11.008
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li SH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1716, DOI 10.1145/3123266.3123425
   Li XT, 2019, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR.2019.00393
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Li YJ, 2017, ADV NEUR IN, V30
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Xiao-Chang, 2017, P S NONPH AN REND LO
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Phillips F., 2011, Issues Accounting Educ., V26, P593
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Siarohin A, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311781
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang X, 2017, PROC CVPR IEEE, P7178, DOI 10.1109/CVPR.2017.759
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang LC, 2018, COMPUT GRAPH FORUM, V37, P97, DOI 10.1111/cgf.13551
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Zhang YL, 2019, IEEE I CONF COMP VIS, P5942, DOI 10.1109/ICCV.2019.00604
   Zhizhong Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7786, DOI 10.1109/CVPR42600.2020.00781
NR 35
TC 5
Z9 5
U1 2
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 107
DI 10.1145/3506710
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600018
DA 2024-07-18
ER

PT J
AU Alaya, B
   Sellami, L
AF Alaya, Bechir
   Sellami, Lamaa
TI Multilayer Video Encoding for QoS Managing of Video Streaming in VANET
   Environment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE VANET; Video Streaming; QoS; limited resources; adaptive broadcasting;
   double rotating buffer; superimposable images
AB Efficient delivery and maintenance of the quality of service (QoS) of audio/video streams transmitted over VANETs for mobile and heterogeneous nodes are one of the major challenges in the convergence of this network type and these services. In this context, we propose an inter-layer approach for multimedia stream transmission in a VANET environment (VSMENET). The main idea of our work is based on the dynamic adaptation of the transmission rate according to the physical rate available in the VANET. VSMENET is all about eliminating downtime during video playback by vehicle users. This involves adapting the quality of the video to the actual performance of the VANETs, intelligent encoding of video on the Road Side Units (RSU) side, and finally continuous maintenance of the calculation tasks on the RSU side and sufficient video data on the vehicle node side. Thus, we are interested in the process of evaluating the strict parameters of the VANETs, influencing the video transmission. For example, we propose, on the one hand, an architecture for intelligent data selection and good clock synchronization, and, on the other hand, efficient management of the availability and consumption of video data. We used the NetSim simulator to test the proposed approach performance. To this end, several algorithms such as OCLFEC, MAC, ShieldHEVC, and AntArmour have been implemented for such a performance comparison. Our work suggests that VSMENET is well concerning the average lifetime of the video packets and their delivery rate (more than 9% gain compared with other approaches).
C1 [Alaya, Bechir; Sellami, Lamaa] Qassim Univ, Coll Business & Econ, Dept Management Informat Syst & Prod Management, Buraydah 51452, Saudi Arabia.
   [Alaya, Bechir] Univ Gabes, IResCoMath Lab, Gabes, Tunisia.
   [Sellami, Lamaa] Univ Gabes, CONPRI Lab, Gabes, Tunisia.
C3 Qassim University; Universite de Gabes; Universite de Gabes
RP Alaya, B (corresponding author), Qassim Univ, Coll Business & Econ, Dept Management Informat Syst & Prod Management, Buraydah 51452, Saudi Arabia.; Alaya, B (corresponding author), Univ Gabes, IResCoMath Lab, Gabes, Tunisia.
EM b.alaya@qu.edu.sa; l.sellami@qu.edu.sa
RI Alaya, Bechir/HCI-1812-2022
FU Qassim University
FX This work was supported by Qassim University.
CR Alaya B, 2021, WIRELESS PERS COMMUN, V118, P2175, DOI 10.1007/s11277-021-08118-7
   Alustwani, 2009, THESIS U FRANCHE COM
   Asefi M, 2012, IEEE T WIREL COMMUN, V11, P1817, DOI 10.1109/TWC.2012.030812.111064
   Battista S, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9101579
   Chen Y, 2017, MULTIMED TOOLS APPL, V76, P7595, DOI 10.1007/s11042-016-3387-1
   Duan P., 2015, INT J DISTRIB SENS N, V11, P1
   Flierl M, 2004, SIGNAL PROCESS-IMAGE, V19, P561, DOI 10.1016/j.image.2004.05.002
   Frisch D., 2019, P 22 INT C INF FUS D, P1
   Garg, 2019, INT J ENG ADV TECHNO, V8, P866
   Ghadiyaram D, 2019, IEEE T CIRC SYST VID, V29, P183, DOI 10.1109/TCSVT.2017.2768542
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Hadka D, 2017, BEGINNERS GUIDE MOEA
   Hu M, 2017, IEEE ACCESS, V5, P4140, DOI 10.1109/ACCESS.2017.2683640
   Huang C. F., 2020, ADV TECHNOL INNOV, V5, P56, DOI DOI 10.46604/AITI.2020.4080
   Huang CJ, 2013, APPL SOFT COMPUT, V13, P4508, DOI 10.1016/j.asoc.2013.07.025
   Huo YK, 2021, NEUROCOMPUTING, V431, P34, DOI 10.1016/j.neucom.2020.12.019
   Immich R., 2016, 2016 Mediterranean Ad Hoc Networking Workshop (Med-Hoc-Net), P1, DOI 10.1109/MedHocNet.2016.7528427
   Immich R, 2019, WIREL NETW, V25, P2587, DOI 10.1007/s11276-018-1687-2
   Joash, 2019, INT J ADV RES COMPUT, V8, P1
   Kazemi B, 2013, INT CONF CONNECT VEH, P926, DOI [10.1109/ICCVE.2013.6799929, 10.1109/ICCVE.2013.97]
   Khan S, 2018, COMPUT ELECTR ENG, V68, P447, DOI 10.1016/j.compeleceng.2018.04.017
   Khayam, 2003, ECE802602 MICH STAT
   Lyamin N, 2018, IEEE NETWORK, V32, P15, DOI 10.1109/MNET.2018.1800074
   MIHAELA VDS, 2001, IEEE T CIRC SYST VID, V11, P318
   Mireille, 2010, MEMOIRE DE MASTER, V2
   Mohan C. B., 2020, INT J ADV TRENDS COM, V9, P2065
   Nzouonta J, 2009, IEEE T VEH TECHNOL, V58, P3609, DOI 10.1109/TVT.2009.2014455
   Oche M, 2020, WIREL NETW, V26, P1685, DOI 10.1007/s11276-018-1860-7
   Panayides AS, 2020, IEEE ACCESS, V8, P11469, DOI 10.1109/ACCESS.2020.2965325
   Rezende C, 2015, COMPUT NETW, V81, P43, DOI 10.1016/j.comnet.2014.12.010
   SCHAFER R, 1995, P IEEE, V83, P907, DOI 10.1109/5.387092
   Schwarz H., 2003, EMERGING H264 AVC ST
   Shen LQ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3313185
   SHIPENG LY, 2001, ACOUST SPEECH SIG PR, V2, P1801
   Talari A, 2013, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2013-206
   TAUBMAN D, 1994, IEEE T IMAGE PROCESS, V3, P572, DOI 10.1109/83.334984
   Tetcos.com, 2019, NETSIM NETW SIM EM H
   TOBIAS T, 2006, P 2006 IEEE WORKSH M, P54
   Udousoro, 2018, SOFTWARE ENG, V6, P116, DOI DOI 10.11648/J.SE.20180604.12
   Vemireddy S, 2020, VEH COMMUN, V25, DOI 10.1016/j.vehcom.2020.100251
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
   Xu, 2020, EURASIP J WIREL COMM, V93, P1
   Xu CQ, 2015, IEEE T VEH TECHNOL, V64, P1201, DOI 10.1109/TVT.2014.2329696
   Xu CQ, 2013, IEEE T VEH TECHNOL, V62, P2273, DOI 10.1109/TVT.2012.2228682
   Zhang J., 2020, IMPROVED ENCODING SC
NR 45
TC 4
Z9 4
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 82
DI 10.1145/3491433
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600016
DA 2024-07-18
ER

PT J
AU Guo, KH
   Hu, M
   Ren, S
   Li, FF
   Zhang, J
   Guo, HF
   Kui, XY
AF Guo, Kehua
   Hu, Min
   Ren, Sheng
   Li, Fangfang
   Zhang, Jian
   Guo, Haifu
   Kui, Xiaoyan
TI Deep Illumination-Enhanced Face Super-Resolution Network for Low-Light
   Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face super-resolution; low-light face images; spectrum normalization;
   generative adversarial network
ID RESOLUTION; RETINEX
AB Face images are typically a key component in the fields of security and criminal investigation. However, due to lighting and shooting angles, faces taken under low-light conditions are often difficult to recognize. Face super-resolution (FSR) technology can restore high-resolution faces based on low-resolution inputs. However, existing face super-resolution methods typically rely on prior knowledge of inaccurate faces estimated from low-resolution images. Faces restored by low-light inputs may suffer from problems such as low brightness and many missing details. In this article, we proposed an Illumination-Enhanced Face Super-Resolution (IEFSR) model that can progressively super-resolve low-light faces of 32 x 32 pixels by an upscaling factor of 8. While reconstructing the low-light low-resolution face into a clear and high-quality face, we introduce a coarse low-resolution (LR) restoration network to recover the LR face details hidden in the dark. In the generator, we use a series of style blocks with noise to make the generated faces appear to have a more realistic visual aesthetic. Additionally, we introduce spectrum normalization in the discriminator to improve training stability. Extensive experimental evaluations show that the proposed IEFSR yields visually and metrically more attractive results than existing state-of-the-art FSR methods.
C1 [Guo, Kehua; Hu, Min; Ren, Sheng; Li, Fangfang; Zhang, Jian; Guo, Haifu; Kui, Xiaoyan] Cent South Univ, 932 Lu Shan South Rd, Changsha 410083, Hunan, Peoples R China.
C3 Central South University
RP Li, FF (corresponding author), Cent South Univ, 932 Lu Shan South Rd, Changsha 410083, Hunan, Peoples R China.
EM guokehua@csu.edu.cn; mhu1515@csu.edu.cn; rensheng@csu.edu.cn;
   lifangfang@csu.edu.cn; jianzhang@csu.edu.cn; HaiFu_Guo@163.com;
   xykui@csu.edu.cn
RI chen, minghui/KFR-8832-2024; li, fangyu/KCY-0521-2024
OI li, fangyu/0009-0009-8303-9157
FU National Natural Science Foundation of China [62076255, 62177047,
   62172449]; Hunan Provincial Science and Technology Plan Project
   [2020SK2059]; National Science Foundation of Hunan Province
   [2019JJ20025, 2019JJ40406, 2021JJ30870]; National Social Science Fund of
   China [20ZD120]; Postgraduate Scientific Research Innovation Project of
   Hunan Province [CX20200208]; Fundamental Research Funds for the Central
   Universities of Central South University [2020zzts588]; Changsha
   Municipal Natural Science Foundation [kq2014134]
FX This work was supported by the National Natural Science Foundation of
   China (62076255, 62177047, 62172449), the Hunan Provincial Science and
   Technology Plan Project (2020SK2059), the National Science Foundation of
   Hunan Province (2019JJ20025, 2019JJ40406, 2021JJ30870), the National
   Social Science Fund of China (No. 20&ZD120), the Postgraduate Scientific
   Research Innovation Project of Hunan Province (CX20200208), the
   Fundamental Research Funds for the Central Universities of Central South
   University (2020zzts588), Changsha Municipal Natural Science Foundation
   (kq2014134).
CR Bach JR, 1996, P SOC PHOTO-OPT INS, V2670, P76, DOI 10.1117/12.234785
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chen CF, 2021, IEEE T IMAGE PROCESS, V30, P1219, DOI 10.1109/TIP.2020.3043093
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5568, DOI 10.1109/CVPR42600.2020.00561
   Dong X, 2011, IEEE INT CON MULTI
   Gao HH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3398382
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guan WL, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3424115
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hensel M, 2017, ADV NEUR IN, V30
   Huang H, 2010, PATTERN RECOGN, V43, P2532, DOI 10.1016/j.patcog.2010.02.007
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Jia K, 2008, IEEE T IMAGE PROCESS, V17, P873, DOI 10.1109/TIP.2008.922421
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kuo WK, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2043612.2043614
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee CH, 2018, IEEE COMPUT SOC CONF, P834, DOI 10.1109/CVPRW.2018.00115
   Lee Jongyeon, 2020, FACE SUPER RESOLUTIO
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lv F., 2018, P BMVC, V220, P4
   Miyato T, 2018, INT C LEARN REPR
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Pisano ED, 1998, J DIGIT IMAGING, V11, P193, DOI 10.1007/BF03178082
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Tao L, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14
   Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhu NJ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3352573
   Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37
NR 49
TC 9
Z9 9
U1 6
U2 44
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 87
DI 10.1145/3495258
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600021
DA 2024-07-18
ER

PT J
AU Yang, KW
   Huang, YY
   Huang, JW
   Hsu, YR
   Wan, CL
   Shuai, HH
   Wang, LC
   Cheng, WH
AF Yang, Kai-Wei
   Huang, Yen-Yun
   Huang, Jen-Wei
   Hsu, Ya-Rou
   Wan, Chang-Lin
   Shuai, Hong-Han
   Wang, Li-Chun
   Cheng, Wen-Huang
TI Improving Crowd Density Estimation by Fusing Aerial Images and Radio
   Signals
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Crowd density estimation; unmanned aerial vehicles; data fusion;
   datasets
ID NETWORKS
AB A recent line of research focuses on crowd density estimation from RGB images for a variety of applications, for example, surveillance and traffic flow control. The performance drops dramatically for low-quality images, such as occlusion, or poor light conditions. However, people are equipped with various wireless devices, allowing the received signals to be easily collected at the base station. As such, another line of research utilizes received signals for crowd counting. Nevertheless, received signals offer only information regarding the number of people, while an accurate density map cannot be derived. As unmanned aerial vehicles (UAVs) are now treated as flying base stations and equipped with cameras, we make the first attempt to leverage both RGB images and received signals for crowd density estimation on UAVs. Specifically, we propose a novel network to effectively fuse the RGB images and received signal strength (RSS) information. Moreover, we design a new loss function that considers the uncertainty from RSS and makes the prediction consistent with the received signals. Experimental results show that the proposed method successfully helps break the limit of traditional crowd density estimation methods and achieves state-of-the-art performance. The proposed dataset is released as a public download for future research.
C1 [Yang, Kai-Wei; Huang, Yen-Yun; Huang, Jen-Wei; Hsu, Ya-Rou; Wan, Chang-Lin; Shuai, Hong-Han; Wang, Li-Chun; Cheng, Wen-Huang] Natl Yang Ming Chiao Tung Univ, 1001 Univ Rd, Hsinchu, Taiwan.
   [Cheng, Wen-Huang] Natl Chung Hsing Univ, 1001 Univ Rd, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University; National Chung Hsing
   University
RP Yang, KW (corresponding author), Natl Yang Ming Chiao Tung Univ, 1001 Univ Rd, Hsinchu, Taiwan.
EM andyst.eed06@nctu.edu.tw; milu0970488651.eed06@nctu.edu.tw;
   admsd.eed06@nctu.edu.tw; st000320.eed06@g2.nctu.edu.tw;
   lichun@cc.nctu.edu.tw; hhshuai@nctu.edu.tw;
   wanchunglin.eed06@g2.nctu.edu.tw; whcheng@nctu.edu.tw
RI huang, jw/KVY-9917-2024; Wang, Li-Chun/AAD-7164-2022
OI Wang, Li-Chun/0000-0002-7883-6217
FU Ministry of Science and Technology (MOST) of Taiwan
   [MOST-109-2223-E-009-002-MY3, MOST-110-2634-F-007-015,
   MOST-110-2634-F-009-021, MOST-109-2218-E-002-015,
   MOST-109-2221-E-009-114-MY3, MOST-110-2218-E-A49-018,
   MOST-109-2221-E-009-097, MOST-109-2221-E-001-015]
FX This work was supported in part by the Ministry of Science and
   Technology (MOST) of Taiwan under the grants
   MOST-109-2223-E-009-002-MY3, MOST-110-2634-F-007-015,
   MOST-110-2634-F-009-021, MOST-109-2218-E-002-015,
   MOST-109-2221-E-009-114-MY3, MOST-110-2218-E-A49-018,
   MOST-109-2221-E-009-097 and MOST-109-2221-E-001-015.
CR Andrews JG, 2014, IEEE J SEL AREA COMM, V32, P1065, DOI 10.1109/JSAC.2014.2328098
   Basalamah Anas, 2016, US Patent, Patent No. [9,401,086, 9401086]
   BRESENHAM JE, 1965, IBM SYST J, V4, P25, DOI 10.1147/sj.41.0025
   Cao Xinkun, 2020, P EUROPEAN C COMPUTE, P757
   Chan AB, 2012, IEEE T IMAGE PROCESS, V21, P2160, DOI 10.1109/TIP.2011.2172800
   Chen JW, 2020, NEUROCOMPUTING, V382, P210, DOI 10.1016/j.neucom.2019.11.064
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   Cheng ZQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1897, DOI 10.1145/3343031.3350898
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Di Domenico S., 2016, P 3 INT WORKSH PHYS, P37
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Fu HY, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P877, DOI 10.1145/2647868.2655040
   Gao JY, 2019, NEUROCOMPUTING, V363, P1, DOI 10.1016/j.neucom.2019.08.018
   Guo D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1823, DOI 10.1145/3343031.3350881
   Handte M., 2014, EDBT/ICDT Workshops, P315
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hu YC, 2016, J VIS COMMUN IMAGE R, V38, P530, DOI 10.1016/j.jvcir.2016.03.021
   Idrees H, 2015, IEEE T PATTERN ANAL, V37, P1986, DOI 10.1109/TPAMI.2015.2396051
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Khawaja W, 2019, IEEE COMMUN SURV TUT, V21, P2361, DOI 10.1109/COMST.2019.2915069
   Kocamaz Mehmet Kemal, 2016, IEEE WINT CONF APPL, P1, DOI 10.1109/WACV.2016.7477685
   Lai CC, 2019, IEEE GLOB COMM CONF, DOI 10.1109/globecom38437.2019.9014210
   Lai WC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P202, DOI 10.1145/3394171.3413602
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192
   Liu CB, 2020, IEEE T MULTIMEDIA, V22, P1785, DOI 10.1109/TMM.2019.2954747
   Liu QH, 2019, JOINT URB REMOTE SEN, DOI 10.1109/jurse.2019.8809046
   Liu WZ, 2019, IEEE INT C INT ROBOT, P244, DOI [10.1109/iros40897.2019.8967852, 10.1109/IROS40897.2019.8967852]
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma YJ, 2022, IEEE T MULTIMEDIA, V24, P261, DOI 10.1109/TMM.2021.3050059
   Ma Z, 2013, PROC CVPR IEEE, P2539, DOI 10.1109/CVPR.2013.328
   Pan XG, 2018, AAAI CONF ARTIF INTE, P7276
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Shi MJ, 2019, PROC CVPR IEEE, P7271, DOI 10.1109/CVPR.2019.00745
   Shibata K, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE IN INFORMATION AND COMMUNICATION (ICAIIC 2019), P143, DOI [10.1109/icaiic.2019.8669071, 10.1109/ICAIIC.2019.8669071]
   Sindagi Vishwanath A., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P212, DOI 10.1007/978-3-030-58621-8_13
   Sio CH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1948, DOI 10.1145/3394171.3413611
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   Stuber G. L., 2017, Principles of Mobile Communication, V4th
   Tan X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1907, DOI 10.1145/3343031.3350914
   Tian YK, 2020, IEEE T IMAGE PROCESS, V29, P2714, DOI 10.1109/TIP.2019.2952083
   Wang HJ, 2019, IEEE INTERNET THINGS, V6, P10009, DOI 10.1109/JIOT.2019.2935105
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang Shuheng, 2019, P ACM MULTIMEDIA ASI, P1
   Xia ZX, 2021, IEEE IND ELECTRON M, V15, P6, DOI 10.1109/MIE.2020.2970790
   Xiyang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P241, DOI 10.1007/978-3-030-58586-0_15
   Yaik OB., 2016, J TELECOMMUN ELECT C, V8, P79
   Yan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P242, DOI 10.1007/978-3-030-58555-6_15
   Yu P, 2018, IEEE WIREL COMMUN, V25, P58, DOI 10.1109/MWC.2018.1700393
   Zhang AR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3356019
   Zhang H, 2020, WIRELESS NETW-GER, P1, DOI 10.1007/978-3-030-33039-2
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhang ZX, 2015, NEUROCOMPUTING, V166, P151, DOI 10.1016/j.neucom.2015.03.083
   Zhou R, 2020, WIREL NETW, V26, P3495, DOI 10.1007/s11276-020-02274-7
   Zhu Pengfei, 2020, ARXIV
NR 58
TC 4
Z9 4
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 84
DI 10.1145/3492346
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600018
DA 2024-07-18
ER

PT J
AU Masud, M
   Alhamid, MF
   Zhang, Y
AF Masud, Mehedi
   Alhamid, Mohammed F.
   Zhang, Yin
TI A Convolutional Neural Network Model Using Weighted Loss Function to
   Detect Diabetic Retinopathy
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; diabetic retinopathy; computer vision; convolutional
   neural network
ID COMPUTER-AIDED DIAGNOSIS; AUTOMATED DETECTION
AB Nowadays, artificial intelligence (AI) provides tremendous prospects for driving future healthcare while empowering patients and service providers. The extensive use of digital healthcare produces a massive amount of multimedia healthcare data continuously (e.g., MRI, X-Ray, ultrasound images, etc.). Hence, it needs special data analytics techniques to provide a smart diagnosis to the patients. Recent advancements in artificial intelligence and machine learning techniques, particularly Deep learning (DL) methods, have demonstrated tremendous medical diagnosis progress and achievements. Diabetic Retinopathy (DR), cataract, macular degeneration, and glaucoma are the most conunon eye problems due to diabetes. Numerous models have been proposed using deep learning models to diagnose diabetic retinopathy, but no model is perfect for detecting DR diseases. This article presents a deep learning model to analyze diabetic retinopathy images to classify DR patients' severity levels. The model applies a custom-weighted loss function in the model's training and achieves 92.49% accuracy and a 0.945 Cohen Kappa score on test data. The model's weighted average precision was 93%, recall 92%, and f1 score 93%. The model is compared with several state-of-the-art pre-trained models. We observe that the proposed model performs better in accuracy results and Cohen Kappa score.
C1 [Masud, Mehedi] Taif Univ, Coll Comp & Informat Technol, Dept Comp Sci, At Taif 21944, Saudi Arabia.
   [Alhamid, Mohammed F.] King Saud Univ, Coll Comp & Informat Sci, Res Chair Smart Technol, Riyadh 11543, Saudi Arabia.
   [Zhang, Yin] Univ Elect Sci & Technol China, Chengdu, Peoples R China.
C3 Taif University; King Saud University; University of Electronic Science
   & Technology of China
RP Masud, M (corresponding author), Taif Univ, Coll Comp & Informat Technol, Dept Comp Sci, At Taif 21944, Saudi Arabia.; Alhamid, MF (corresponding author), King Saud Univ, Coll Comp & Informat Sci, Res Chair Smart Technol, Riyadh 11543, Saudi Arabia.
EM mmasud@tu.edu.sa; mohalhamid@ksu.edu.sa; yin.zhang.cn@ieee.org
RI Zhang, Yin/O-2149-2015; Masud, Mehedi/AAZ-7022-2020
OI Zhang, Yin/0000-0002-1772-0763; Masud, Mehedi/0000-0001-6019-7245
FU Deanship of Scientific Research at King Saud University, Riyadh, Saudi
   Arabia
FX The authors are grateful to the Deanship of Scientific Research at King
   Saud University, Riyadh, Saudi Arabia, for funding this work through the
   Vice Deanship of Scientific Research Chairs: Research Chair of Smart
   Technologies.
CR Amin J, 2016, SCIENTIFICA, V2016, DOI 10.1155/2016/6838976
   Amin SU, 2019, IEEE ACCESS, V7, P10745, DOI 10.1109/ACCESS.2019.2891390
   Bhaskaranand M., 2016, J DIABETES SCI TECHN, V10, P254, DOI [DOI 10.1177/1932296816628546, 10.1177/1932296816628546]
   Byrd Jonathon, 2018, ABS181203372 CORR, P1
   Chernenko, GAUSSIAN FILTER GAUS
   Choi JY, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0187336
   DeHoog E, 2009, APPL OPTICS, V48, P221, DOI 10.1364/AO.48.000221
   Hammes HP, 2014, EXP CLIN ENDOCR DIAB, V122, P387, DOI 10.1055/s-0034-1366292
   Hermann B, 2004, OPT LETT, V29, P2142, DOI 10.1364/OL.29.002142
   Hossain MS, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3241056
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Hossain MS, 2019, MULTIMEDIA SYST, V25, P565, DOI 10.1007/s00530-017-0561-x
   Hossain MS, 2017, IEEE SYST J, V11, P118, DOI 10.1109/JSYST.2015.2470644
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   kag, 2019, APTOS 2019 BLINDN DE
   Kalyani G, 2023, COMPLEX INTELL SYST, V9, P2651, DOI 10.1007/s40747-021-00318-9
   Kandhasamy JP, 2020, MULTIMED TOOLS APPL, V79, P10581, DOI 10.1007/s11042-019-7485-8
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li BX, 2013, CURR DIABETES REP, V13, P453, DOI 10.1007/s11892-013-0393-9
   Manivannan A, 1998, BRIT J OPHTHALMOL, V82, P342, DOI 10.1136/bjo.82.4.342
   Miao Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311747
   Mishra PK, 2014, BIOINFORMATION, V10, P556, DOI 10.6026/97320630010556
   Mookiah MRK, 2013, J MED IMAG HEALTH IN, V3, P598, DOI 10.1166/jmihi.2013.1210
   Mookiah MRK, 2013, COMPUT BIOL MED, V43, P2136, DOI 10.1016/j.compbiomed.2013.10.007
   Muhammad G, 2021, IEEE J SEL AREA COMM, V39, P603, DOI 10.1109/JSAC.2020.3020654
   Murray V, 2012, IEEE IMAGE PROC, P525, DOI 10.1109/ICIP.2012.6466912
   Niemeijer M, 2007, INVEST OPHTH VIS SCI, V48, P2260, DOI 10.1167/iovs.06-0996
   Prakash Nattanmai Balasubramanian, 2014, INT J ELECT ENG INFO, V6, P598
   Ruder S, 2021, OVERVIEW GRADIENT DE
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sakaguchi, 2019, FUNDUS IMAGE CLASSIF, DOI [10.1145/3326172.3326198, DOI 10.1145/3326172.3326198]
   Saleh E, 2018, ARTIF INTELL MED, V85, P50, DOI 10.1016/j.artmed.2017.09.006
   Shankar K, 2020, PATTERN RECOGN LETT, V133, P210, DOI 10.1016/j.patrec.2020.02.026
   Shorfuzzaman M, 2021, SUSTAIN CITIES SOC, V64, DOI 10.1016/j.scs.2020.102582
   Shorfuzzaman M, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107700
   Sikder N, 2021, SYMMETRY-BASEL, V13, DOI 10.3390/sym13040670
   Simó R, 2009, DIABETES CARE, V32, P1556, DOI 10.2337/dc09-0565
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Trivino M. C. A., 2018, ABS180709232 CORR
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379
   Yun WL, 2008, INFORM SCIENCES, V178, P106, DOI 10.1016/j.ins.2007.07.020
   Zhou YF, 2020, IEEE ACCESS, V8, P69273, DOI 10.1109/ACCESS.2020.2987281
NR 46
TC 6
Z9 6
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 40
DI 10.1145/3470976
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300017
DA 2024-07-18
ER

PT J
AU Tan, M
   Yuan, F
   Yu, J
   Wang, GJ
   Gu, XL
AF Tan, Min
   Yuan, Fu
   Yu, Jun
   Wang, Guijun
   Gu, Xiaoling
TI Fine-grained Image Classification via Multi-scale Selective Hierarchical
   Biquadratic Pooling
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fine-grained classification; feature interaction; biquadratic pooling;
   multi-scale feature; interaction selection
AB How to extract distinctive features greatly challenges the fine-grained image classification tasks. In previous models, bilinear pooling has been frequently adopted to address this problem. However, most bilinear pooling models neglect either intra or inter layer feature interaction. This insufficient interaction brings in the loss of discriminative information. In this article, we devise a novel fine-grained image classification approach named Multi-scale Selective Hierarchical biQuadratic Pooling (MSHQP). The proposed biquadratic pooling simultaneously models intra and inter layer feature interactions and enhances part response by integrating multi-layer features. The subsequent coarse-to-fine multi-scale interaction structure captures the complementary information within features. Finally, the active interaction selection module adaptively learns the optimal interaction subset for a specific dataset. Consequently, we obtain a robust image representation with coarse-to-fine semantics. We conduct experiments on five benchmark datasets. The experimental results demonstrate that MSHQP achieves competitive or even match the state-of-the-art methods in terms of both accuracy and computational efficiency, with 89.0%, 94.9%, 93.4%, 90.4%, and 91.5% top-1 classification accuracy on CUB-200-2011, Stanford-Cars, FGVC-Aircraft, Stanford-Dog, and VegFru, respectively.
C1 [Tan, Min; Yuan, Fu; Yu, Jun; Wang, Guijun; Gu, Xiaoling] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, 1158,2nd Ave, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Hangzhou Dianzi University
RP Yu, J (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, 1158,2nd Ave, Hangzhou 310018, Zhejiang, Peoples R China.
EM tanmin@hdu.edu.cn; yuanfu@hdu.edu.cn; yujun@hdu.edu.cn;
   guijunHDU@hdu.edu.cn; guxl@hdu.edu.cn
FU Zhejiang Provincial Natural Science Foundation of China [LY19F020038];
   National Natural Science Foundation of China [61972119, 61836002,
   62020106007, 61802100, 61602136]
FX This work was supported in part by the Zhejiang Provincial Natural
   Science Foundation of China (No. LY19F020038); and by the National
   Natural Science Foundation of China (No. 61972119, No. 61836002, No.
   62020106007, No. 61802100, No. 61602136).
CR Amin S, 2020, PR MACH LEARN RES, V119
   Cai SJ, 2017, IEEE I CONF COMP VIS, P511, DOI 10.1109/ICCV.2017.63
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Chen ZN, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231742
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Dubey A, 2018, LECT NOTES COMPUT SC, V11216, P71, DOI 10.1007/978-3-030-01258-8_5
   Engin M, 2018, LECT NOTES COMPUT SC, V11206, P629, DOI 10.1007/978-3-030-01216-8_38
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Gao Y, 2020, AAAI CONF ARTIF INTE, V34, P10818
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Han K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2040, DOI 10.1145/3240508.3240550
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   He XT, 2017, PROC CVPR IEEE, P7332, DOI 10.1109/CVPR.2017.775
   Hou SH, 2017, IEEE I CONF COMP VIS, P541, DOI 10.1109/ICCV.2017.66
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Jing YC, 2020, AAAI CONF ARTIF INTE, V34, P4369
   Khosla A., 2011, P CVPR WORKSH FIN GR
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li J, 2020, AAAI CONF ARTIF INTE, V34, P8204
   Li L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3359753
   Li L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1092, DOI 10.1145/3240508.3240649
   Li LD, 2022, IEEE T NEUR NET LEAR, V33, P3400, DOI 10.1109/TNNLS.2021.3052829
   Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105
   Li Xuelu, 2020, ARXIV200401817
   Li YY, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3412384
   Li ZC, 2017, IEEE INT CONF COMP V, P1199, DOI 10.1109/ICCVW.2017.145
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Luo XF, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3381086
   Min Shaobo, 2019, P ACM MULT, P1
   Moghimi Mohammad, 2016, BMVC, V5, P6
   Munro J, 2020, PROC CVPR IEEE, P119, DOI 10.1109/CVPR42600.2020.00020
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shroff P, 2020, IEEE COMPUT SOC CONF, P3782, DOI 10.1109/CVPRW50498.2020.00442
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun QL, 2018, NEUROCOMPUTING, V282, P174, DOI 10.1016/j.neucom.2017.12.020
   Tan M, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.102186
   Tan M, 2019, IEEE ACCESS, V7, P117944, DOI 10.1109/ACCESS.2019.2936118
   Tan M, 2019, IEEE T IMAGE PROCESS, V28, P6047, DOI 10.1109/TIP.2019.2921861
   Tan M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3209666
   Tan M, 2018, MULTIMED TOOLS APPL, V77, P22145, DOI 10.1007/s11042-018-5703-4
   Tan M, 2016, IEEE T INTELL TRANSP, V17, P1415, DOI 10.1109/TITS.2015.2506182
   Tan M, 2016, NEUROCOMPUTING, V181, P96, DOI 10.1016/j.neucom.2015.04.123
   Do TT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3314051
   Vedaldi A., 2013, Technical report
   Wah Catherine, 2011, Technical report
   Wang QL, 2017, PROC CVPR IEEE, P6507, DOI 10.1109/CVPR.2017.689
   Wang Q, 2018, ADV NEUR IN, V31
   Wang ZH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1851, DOI 10.1145/3343031.3350976
   Wei X, 2018, LECT NOTES COMPUT SC, V11207, P365, DOI 10.1007/978-3-030-01219-9_22
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Xu Q, 2022, IEEE T MULTIMEDIA, V24, P567, DOI 10.1109/TMM.2021.3055362
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Yu J, 2020, IEEE T NEUR NET LEAR, V31, P661, DOI 10.1109/TNNLS.2019.2908982
   Yu J, 2022, IEEE T PATTERN ANAL, V44, P563, DOI 10.1109/TPAMI.2019.2932058
   Yu J, 2015, IEEE T CYBERNETICS, V45, P767, DOI 10.1109/TCYB.2014.2336697
   Yu KC, 2018, LECT NOTES COMPUT SC, V11211, P621, DOI 10.1007/978-3-030-01234-2_37
   Yu T, 2021, AAAI CONF ARTIF INTE, V35, P3243
   Zhang Y, 2019, PROC CVPR IEEE, P11997, DOI 10.1109/CVPR.2019.01228
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zheng Heliang, 2019, ADV NEUR IN ADV NEURAL INFORM PR, P4277
   Zheng XT, 2021, IEEE T MULTIMEDIA, V23, P1187, DOI 10.1109/TMM.2020.2993960
   Zhihui Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9746, DOI 10.1109/CVPR42600.2020.00977
   Zhipu Liu, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P4289, DOI 10.1145/3394171.3413689
NR 66
TC 14
Z9 15
U1 0
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 31
DI 10.1145/3492221
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300008
DA 2024-07-18
ER

PT J
AU Xu, X
   Wang, YF
   He, YX
   Yang, Y
   Hanjalic, A
   Shen, HT
AF Xu, Xing
   Wang, Yifan
   He, Yixuan
   Yang, Yang
   Hanjalic, Alan
   Shen, Heng Tao
TI Cross-Modal Hybrid Feature Fusion for Image-Sentence Matching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image-sentence matching; multimodal feature fusion; cross-modal
   retrieval; attention mechanism
ID ATTENTION
AB Image-sentence matching is a challenging task in the field of language and vision, which aims at measuring the similarities between images and sentence descriptions. Most existing methods independently map the global features of images and sentences into a common space to calculate the image-sentence similarity. However, the image-sentence similarity obtained by these methods may be coarse as (1) an intermediate common space is introduced to implicitly match the heterogeneous features of images and sentences in a global level, and (2) only the inter-modality relations of images and sentences are captured while the intra-modality relations are ignored. To overcome the limitations, we propose a novel Cross-Modal Hybrid Feature Fusion (CMHF) framework for directly learning the image-sentence similarity by fusing multimodal features with inter- and intra-modality relations incorporated. It can robustly capture the high-level interactions between visual regions in images and words in sentences, where flexible attention mechanisms are utilized to generate effective attention flows within and across the modalities of images and sentences. A structured objective with ranking loss constraint is formed in CMHF to learn the image-sentence similarity based on the fused fine-grained features of different modalities bypassing the usage of intermediate common space. Extensive experiments and comprehensive analysis performed on two widely used datasets-Microsoft COCO and Flickr30K-show the effectiveness of the hybrid feature fusion framework in CMHF, in which the state-of-the-art matching performance is achieved by our proposed CMHF method.
C1 [Xu, Xing; Wang, Yifan; He, Yixuan; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, 2006 Xiyuan Ave, Chengdu 611731, Peoples R China.
   [Hanjalic, Alan] Delft Univ Technol, Postbus 5, NL-2600 AA Delft, Netherlands.
   [Shen, Heng Tao] Pengcheng Lab, Xingke St, Shenzhen 518055, Peoples R China.
C3 University of Electronic Science & Technology of China; Delft University
   of Technology
RP Shen, HT (corresponding author), Univ Elect Sci & Technol China, 2006 Xiyuan Ave, Chengdu 611731, Peoples R China.; Shen, HT (corresponding author), Pengcheng Lab, Xingke St, Shenzhen 518055, Peoples R China.
EM xing.xu@uestc.edu.cn; wyffgz@163.com; abcqmars@gmail.com;
   dlyyang@gmail.com; A.Hanjalic@tudelft.nl; shenhengtao@hotmail.com
RI yang, yang/GVT-5210-2022; He, Yixuan/HCJ-0099-2022; Shen, Heng
   Tao/ABD-5331-2021; yang, yang/HGT-7999-2022; Lang, Ming/HIK-0758-2022
OI He, Yixuan/0000-0002-5990-0658; Hanjalic, Alan/0000-0002-5771-2549
FU National Natural Science Foundation of China [61976049, U20B2063];
   Fundamental Research Funds for the Central Universities [ZYGX2019Z015];
   Sichuan Science and Technology Program, China [2019ZDZX0008,
   2019YFG0003, 2019YFG0533, 2020YJ0038, 2020YFS0057]
FX This work is partially supported by the National Natural Science
   Foundation of China (61976049 and U20B2063); the Fundamental Research
   Funds for the Central Universities (ZYGX2019Z015); and the Sichuan
   Science and Technology Program, China (2019ZDZX0008, 2019YFG0003,
   2019YFG0533, 2020YJ0038, and 2020YFS0057).
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2020, IEEE WINT CONF APPL
   Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Buschman TJ, 2007, SCIENCE, V315, P1860, DOI 10.1126/science.1138071
   Choi H, 2018, NEUROCOMPUTING, V284, P171, DOI 10.1016/j.neucom.2018.01.007
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Clinchant S., 2011, ICMR, P44
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Fang ZW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282469
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Guthrie Guthrie David David, LREC, V6 6, P1222
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   Hou M, 2019, ADV NEUR IN, V32
   Huang FR, 2019, IEEE T IMAGE PROCESS, V28, P2008, DOI 10.1109/TIP.2018.2882225
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huang Y, 2020, IEEE T PATTERN ANAL, V42, P636, DOI 10.1109/TPAMI.2018.2883466
   Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Kim J.-H., 2016, arXiv
   King DB, 2015, ACS SYM SER, V1214, P1
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P970, DOI 10.1145/3123266.3123341
   Liu RY, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3300939
   Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Peng X, 2020, INT J ORAL SCI, V12, DOI 10.1038/s41368-020-0075-9
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HR, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2020.107359
   Wang K., 2016, A comprehensive survey on cross-modal retrieval
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang Liwei, 2017, ARXIV170403470
   Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535
   Wang S, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3314577
   Wang T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P12, DOI 10.1145/3343031.3350875
   Wang X., 2018, ARXIV180405448
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wei YW, 2020, IEEE T IMAGE PROCESS, V29, P1, DOI 10.1109/TIP.2019.2923608
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Westphal C, 2009, IEEE INFOCOM SER, P2826, DOI 10.1109/INFCOM.2009.5062240
   Wu H, 2019, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2019.00677
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Xu X, 2020, IEEE T CYBERNETICS, V50, P2400, DOI 10.1109/TCYB.2019.2928180
   Yang ZG, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3374754
   Ye ZD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356338
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yuan J, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3394955
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zhang DX, 2019, INFORM FUSION, V52, P268, DOI 10.1016/j.inffus.2019.03.005
   Zhang SS, 2018, PROC CVPR IEEE, P6995, DOI 10.1109/CVPR.2018.00731
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
NR 67
TC 7
Z9 7
U1 2
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 127
DI 10.1145/3458281
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800011
DA 2024-07-18
ER

PT J
AU Chen, BZ
   Liu, YS
   Zhang, Z
   Li, YJ
   Zhang, Z
   Lu, GM
   Yu, HB
AF Chen, Bingzhi
   Liu, Yishu
   Zhang, Zheng
   Li, Yingjian
   Zhang, Zhao
   Lu, Guangming
   Yu, Hongbing
TI Deep Active Context Estimation for Automated COVID-19 Diagnosis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Automated COVID-19 diagnosis; deep active context estimation;
   short-range channel interactions; long-range spatial dependencies;
   K-nearest neighbors
ID DATA FUSION; CLASSIFICATION; PNEUMONIA; NETWORK; SYSTEM; NET; CT
AB Many studies on automated COVID-19 diagnosis have advanced rapidly with the increasing availability of large-scale CT annotated datasets. Inevitably, there are still a large number of unlabeled CT slices in the existing data sources since it requires considerable consuming labor efforts. Notably, cinical experience indicates that the neighboring CT slices may present similar symptoms and signs. Inspired by such wisdom, we propose DACE, a novel CNN-based deep active context estimation framework, which leverages the unlabeled neighbors to progressively learn more robust feature representations and generate a well-performed classifier for COVID-19 diagnosis. Specifically, the backbone of the proposed DACE framework is constructed by a well-designed Long-Short Hierarchical Attention Network (LSHAN), which effectively incorporates two complementary attention mechanisms, i.e., short-range channel interactions (SCI) module and long-range spatial dependencies (LSD) module, to learn the most discriminative features from CT slices. To make full use of such available data, we design an efficient context estimation criterion to carefully assign the additional labels to these neighbors. Benefiting from two complementary types of informative annotations from K-nearest neighbors, i.e., the majority of high-confidence samples with pseudo labels and the minority of low-confidence samples with hand-annotated labels, the proposed LSHAN can be fine-tuned and optimized in an incremental learning manner. Extensive experiments on the Clean-CC-CCII dataset demonstrate the superior performance of our method compared with the state-of-the-art baselines.
C1 [Chen, Bingzhi; Liu, Yishu; Li, Yingjian; Lu, Guangming] Harbin Inst Technol, Shenzhen Med Biometr Percept & Anal Engn Lab, Shenzhen 518055, Peoples R China.
   [Zhang, Zheng] Harbin Inst Technol, Shenzhen, Peoples R China.
   [Zhang, Zheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Zhang, Zhao] Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230009, Peoples R China.
   [Zhang, Zhao] Hefei Univ Technol, Intelligent Interconnected Syst Lab Anhui Prov, Hefei 230009, Peoples R China.
   [Yu, Hongbing] Nanshan Dist Chron Dis Prevent & Control Hosp, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Harbin Institute of Technology; Peng
   Cheng Laboratory; Hefei University of Technology; Hefei University of
   Technology
RP Lu, GM (corresponding author), Harbin Inst Technol, Shenzhen Med Biometr Percept & Anal Engn Lab, Shenzhen 518055, Peoples R China.; Zhang, Z (corresponding author), Harbin Inst Technol, Shenzhen, Peoples R China.; Zhang, Z (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.
EM chenbingzhi@stu.hit.edu.cn; liuyishu.smile@gmail.com;
   darrenzz219@gmail.com; hit_lyj@126.com; cszzhang@gmail.com;
   luguangm@hit.edu.cn; pean2468@sina.com
RI Zhang, Zhao/B-5136-2010; LI, Yingjian/HDN-1425-2022; Zhang,
   Zhang/JAX-2097-2023; Zhang, Zheng/M-6325-2014; zhang,
   zhang/GQZ-6804-2022
OI Zhang, Zhao/0000-0002-5703-7969; Zhang, Zheng/0000-0003-1470-6998; Li,
   Yingjian/0000-0002-0653-4535
FU National Natural Science Foundation of China [62002085]; Guangdong Basic
   and Applied Basic Research Foundation [2019A1515110475,
   2019Bl515120055]; Shenzhen Fundamental Research Fund
   [JCYJ20180306172023949]; Key Project of Shenzhen Municipal Technology
   Research [JSGG20200103103401723]; Shenzhen Institute of Artificial
   Intelligence and Robotics for Societ [AC01202005018]
FX This work was supported in part by the National Natural Science
   Foundation of China (no. 62002085), the Guangdong Basic and Applied
   Basic Research Foundation (Grant nos. 2019A1515110475 and
   2019Bl515120055), the Shenzhen Fundamental Research Fund (no.
   JCYJ20180306172023949), the Key Project of Shenzhen Municipal Technology
   Research (no. JSGG20200103103401723), and in part by the Open Project
   Fund (no. AC01202005018) from Shenzhen Institute of Artificial
   Intelligence and Robotics for Society.
CR Alom MZ, 2020, COVID MTNET COVID 19
   Ardakani AA, 2020, COMPUT BIOL MED, V121, DOI 10.1016/j.compbiomed.2020.103795
   Chen BZ, 2020, IEEE J BIOMED HEALTH, V24, P2292, DOI 10.1109/JBHI.2020.2967084
   Chen BZ, 2020, IEEE J BIOMED HEALTH, V24, P2016, DOI 10.1109/JBHI.2019.2952597
   Chen BZ, 2019, BIOMED SIGNAL PROCES, V53, DOI 10.1016/j.bspc.2019.04.031
   Chen WT, 2018, IEEE DATA MINING, P917, DOI 10.1109/ICDM.2018.00111
   Chikontwe P, 2021, MED IMAGE ANAL, V72, DOI 10.1016/j.media.2021.102105
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Diaz-Pinto A, 2019, IEEE T MED IMAGING, V38, P2211, DOI 10.1109/TMI.2019.2903434
   Dongfeng Gu, 2017, THESIS U OTTAWA
   Fan DP, 2020, IEEE T MED IMAGING, V39, P2626, DOI 10.1109/TMI.2020.2996645
   Fang YC, 2020, RADIOLOGY, V296, pE115, DOI 10.1148/radiol.2020200432
   Feng Zhichao, 2020, LANCET
   Figueroa RL, 2012, J AM MED INFORM ASSN, V19, P809, DOI 10.1136/amiajnl-2011-000648
   Gu Mengwei, 2020, MACHINE LEARNING MET
   Han ZY, 2020, IEEE T MED IMAGING, V39, P2584, DOI 10.1109/TMI.2020.2996256
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Hecht-Nielsen R., 1992, Neural Networks for Perception, P65, DOI DOI 10.1016/B978-0-12-741252-8.50010-8
   Huang CL, 2020, LANCET, V395, P497, DOI [10.1016/S0140-6736(20)30183-5, 10.1016/S0140-6736(20)30211-7]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang Y., 2020, IEEE J BIOMED HEALTH, V25, P441, DOI [10.1109/JBHI.2020.3042523, DOI 10.1109/JBHI.2020.3042523]
   Jin C, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-18685-1
   Kang HY, 2020, IEEE T MED IMAGING, V39, P2606, DOI 10.1109/TMI.2020.2992546
   Li L, 2020, RADIOLOGY, V296, pE65, DOI 10.1148/radiol.2020200905
   Li Mingjie, 2020, ARXIV200603744
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1109/PLASMA.2013.6634954, 10.1017/S1368980013002176]
   Liu An-An, 2020, IEEE T MULTIMEDIA
   Loshchilov I, 2016, ARXIV
   Mahase E, 2020, BMJ-BRIT MED J, V368, DOI 10.1136/bmj.m641
   Mittal Sudhanshu, 2019, ARXIV191205361
   Ouyang X, 2020, IEEE T MED IMAGING, V39, P2595, DOI 10.1109/TMI.2020.2995508
   Paszke A, 2019, ADV NEUR IN, V32
   Peng XP, 2019, IEEE DATA MINING, P498, DOI 10.1109/ICDM.2019.00060
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sethy PK, 2020, INT J MATH ENG MANAG, V5, P643, DOI 10.33889/IJMEMS.2020.5.4.052
   Shannon C. E., 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X
   Shen HT, 2021, INFORM FUSION, V66, P54, DOI 10.1016/j.inffus.2020.08.023
   Shi F, 2021, PHYS MED BIOL, V66, DOI 10.1088/1361-6560/abe838
   Song YZ, 2022, IEEE T INTELL TRANSP, V23, P12287, DOI 10.1109/TITS.2021.3112458
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang DW, 2020, JAMA-J AM MED ASSOC, V323, P1061, DOI 10.1001/jama.2020.1585
   Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879
   Wang Linda, 2020, Sci Rep, V10, P19549, DOI 10.1038/s41598-020-76550-z
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang S, 2016, IEEE T KNOWL DATA EN, V28, P3191, DOI 10.1109/TKDE.2016.2605687
   Wang S, 2021, EUR RADIOL, V31, P6096, DOI [10.1080/1064119X.2021.1966557, 10.1079/9781789246070.0001, 10.1007/s00330-021-07715-1]
   Wang SH, 2021, INFORM FUSION, V68, P131, DOI 10.1016/j.inffus.2020.11.005
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XG, 2020, IEEE T MED IMAGING, V39, P2615, DOI 10.1109/TMI.2020.2995965
   Wu YH, 2021, IEEE T IMAGE PROCESS, V30, P3113, DOI 10.1109/TIP.2021.3058783
   Xie XZ, 2020, RADIOLOGY, V296, pE41, DOI 10.1148/radiol.2020200343
   Xu Z, 2020, GASNET WEAKLY SUPERV
   Xuehai He, 2020, SAMPLE EFFICIENT DEE
   Yan Chenggang, 2020, BENCHMARKING DEEP LE
   Zhang K, 2020, CELL, V181, P1423, DOI 10.1016/j.cell.2020.04.045
   Zhang YD, 2020, INFORM FUSION, V64, P149, DOI 10.1016/j.inffus.2020.07.006
   Zhang YD, 2022, IEEE SENS J, V22, P17573, DOI 10.1109/JSEN.2020.3025855
   Zhang Z, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3442204
   Zhang Z, 2021, IEEE T NEUR NET LEAR, V32, P4514, DOI 10.1109/TNNLS.2020.3018790
   Zheng CM, 2021, IEEE T MULTIMEDIA, V23, P2520, DOI 10.1109/TMM.2020.3013398
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu XF, 2021, IEEE T KNOWL DATA EN, V33, P2425, DOI 10.1109/TKDE.2019.2956530
   Zou LR, 2020, NEW ENGL J MED, V382, P1177, DOI [10.1056/NEJMc2001737, 10.1148/radiol.2020200463]
NR 63
TC 4
Z9 4
U1 2
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 101
DI 10.1145/3457124
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600003
DA 2024-07-18
ER

PT J
AU Xin, Q
   Hu, SH
   Liu, SQ
   Zhao, L
   Wang, SH
AF Xin, Qi
   Hu, Shaohao
   Liu, Shuaiqi
   Zhao, Ling
   Wang, Shuihua
TI WTRPNet: An Explainable Graph Feature Convolutional Neural Network for
   Epileptic EEG Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE EEG classification; CNN; wavelet transform; recurrence plot
ID WAVELET TRANSFORM; SEIZURE DETECTION; SIGNALS
AB As one of the important tools of epilepsy diagnosis, the electroencephalogram (EEG) is noninvasive and presents no traumatic injury to patients. It contains a lot of physiological and pathological information that is easy to obtain. The automatic classification of epileptic EEG is important in the diagnosis and therapeutic efficacy of epileptics. In this article, an explainable graph feature convolutional neural network named WTRPNet is proposed for epileptic EEG classification. Since WTRPNet is constructed by a recurrence plot in the wavelet domain, it can fully obtain the graph feature of the EEG signal, which is established by an explainable graph features extracted layer called WTRP block. The proposed method shows superior performance over state-of-the-art methods. Experimental results show that our algorithm has achieved an accuracy of 99.67% in classification of focal and nonfocal epileptic EEG, which proves the effectiveness of the classification and detection of epileptic EEG.
C1 [Xin, Qi; Hu, Shaohao] Beijing Jiaotong Univ, Inst Informat Sci, 3 Shangyuan Village, Beijing, Peoples R China.
   [Liu, Shuaiqi; Zhao, Ling] Heibei Univ, Coll Elect & Informat Engn, 2666 Qiyi East Rd, Baoding, Hebei, Peoples R China.
   [Wang, Shuihua] Univ Leicester, Dept Math, Univ Rd, Leicester LE1 7RH, Leics, England.
C3 Beijing Jiaotong University; Hebei University; University of Leicester
RP Hu, SH (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, 3 Shangyuan Village, Beijing, Peoples R China.; Liu, SQ (corresponding author), Heibei Univ, Coll Elect & Informat Engn, 2666 Qiyi East Rd, Baoding, Hebei, Peoples R China.; Wang, SH (corresponding author), Univ Leicester, Dept Math, Univ Rd, Leicester LE1 7RH, Leics, England.
EM qi.xin@bjtu.edu.cn; shhu@bjtu.edu.cn; shdkj-1918@163.com;
   lingzhao_hbu@163.com; shuihuawang@ieee.org
RI Wang, Shuihua/G-7326-2016
OI Liu, Shuaiqi/0000-0001-7520-8226
FU National Natural Science Foundation of China [62172139, 62172030];
   Natural Science Foundation of Hebei Province [F2020201025, F2019201151,
   F2018210148]; Science Research Project of Hebei Province [BJ2020030,
   QN2017306]; Open Foundation of Guangdong Key Laboratory of Digital
   Signal and Image Processing Technology [2020GDDSIPL-04]; HighPerformance
   Computing Center of Hebei University
FX Y This work was supported in part by National Natural Science Foundation
   of China under grants 62172139 and 62172030, Natural Science Foundation
   of Hebei Province under grants F2020201025, F2019201151, and
   F2018210148, Science Research Project of Hebei Province under grants
   BJ2020030 and QN2017306, and Open Foundation of Guangdong Key Laboratory
   of Digital Signal and Image Processing Technology (2020GDDSIPL-04). This
   work was also supported by the HighPerformance Computing Center of Hebei
   University.
CR Acharya UR, 2011, INT J NEURAL SYST, V21, P199, DOI 10.1142/S0129065711002808
   Akin M, 2002, J Med Syst, V26, P241, DOI 10.1023/A:1015075101937
   Andrzejak RG, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.046206
   Begum SAV, 2020, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS 2020), P332, DOI [10.1109/ICICCS48265.2020.9120920, 10.1109/iciccs48265.2020.9120920]
   Bhattacharyya A, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7040385
   Bhattacharyya A, 2018, NEURAL COMPUT APPL, V29, P47, DOI 10.1007/s00521-016-2646-4
   Carey HJ, 2017, C HUM SYST INTERACT, P137, DOI 10.1109/HSI.2017.8005015
   Chen ST, 2014, AMR, V1044-1045, P1251, DOI [10.4028/www.scientific.net/AMR.1044-1045.1251, DOI 10.4028/WWW.SCIENTIFIC.NET/AMR.1044-1045.1251]
   Chen YW, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107624
   Chen YW, 2021, IEEE T SYST MAN CY-S, V51, P3939, DOI 10.1109/TSMC.2019.2956527
   Chen YW, 2020, KNOWL-BASED SYST, V187, DOI 10.1016/j.knosys.2019.06.032
   Eckmann J.-P., 1995, WORLD SCI SERIES N A, V16, P441
   Fisher RS, 2014, EPILEPSIA, V55, P475, DOI 10.1111/epi.12550
   Geng SJ, 2012, ADV MAT RES, V532-533, P988, DOI DOI 10.4028/WWW.SCIENTIFIC.NET/AMR.532-533.988
   GOTMAN J, 1982, ELECTROEN CLIN NEURO, V54, P530, DOI 10.1016/0013-4694(82)90038-4
   Hanias M. P, 2012, APPL CHAOS NONLINEAR, V2, P59, DOI [10.1007/978-3-642-29329-0_4, DOI 10.1007/978-3-642-29329-0_4]
   Hassan AR, 2016, COMPUT METH PROG BIO, V137, P247, DOI 10.1016/j.cmpb.2016.09.008
   Hekmati Rasoul, 2020, Brain Inform, V7, P13, DOI 10.1186/s40708-020-00114-0
   Heravi MAY, 2020, BIOMED ENG-APP BAS C, V32, DOI 10.4015/S1016237220500295
   Liu SQ, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11060702
   Liu SQ, 2017, IEEE T GEOSCI REMOTE, V55, P2985, DOI 10.1109/TGRS.2017.2657602
   Lu D., 2019, P 6 AS PAC C SYNTH A, P1, DOI DOI 10.1109/CISPBMEI48845
   Marwan N, 2007, PHYS REP, V438, P237, DOI 10.1016/j.physrep.2006.11.001
   Marwan N, 2011, INT J BIFURCAT CHAOS, V21, P1003, DOI 10.1142/S0218127411029008
   Moctezuma LA, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00593
   Murugappan M, 2020, 2020 16TH IEEE INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING & ITS APPLICATIONS (CSPA 2020), P290, DOI [10.1109/CSPA48992.2020.9068709, 10.1109/cspa48992.2020.9068709]
   Ngamga EJ, 2016, PHYS LETT A, V380, P1419, DOI 10.1016/j.physleta.2016.02.024
   Orhan U, 2011, EXPERT SYST APPL, V38, P13475, DOI 10.1016/j.eswa.2011.04.149
   Ouyang GX, 2005, P ANN INT IEEE EMBS, P153
   Salvino LW, 2005, INTERD MATH SCI, V5, P227
   Sharma M, 2017, PATTERN RECOGN LETT, V94, P172, DOI 10.1016/j.patrec.2017.03.023
   Sharmila A, 2016, IEEE ACCESS, V4, P7716, DOI 10.1109/ACCESS.2016.2585661
   Shoka A. A. E., 2021, BRAIN INFORM, V8, P16, DOI [10.1186/s40708-021-00123-7, DOI 10.1186/S40708-021-00123-7]
   Sriraam N, 2017, J MED SYST, V41, DOI 10.1007/s10916-017-0800-x
   Swami P, 2016, EXPERT SYST APPL, V56, P116, DOI 10.1016/j.eswa.2016.02.040
   Wei XY, 2018, BMC MED INFORM DECIS, V18, DOI 10.1186/s12911-018-0693-8
   Wu MC, 2009, ADVANCED BIOSIGNAL PROCESSING, P335, DOI 10.1007/978-3-540-89506-0_16
   Xin Q, 2021, J MED IMAG HEALTH IN, V11, P25, DOI 10.1166/jmihi.2021.3259
   Zhang T, 2017, BIOMED SIGNAL PROCES, V31, P550, DOI 10.1016/j.bspc.2016.10.001
   Zhao XY, 2020, INT CONF ACOUST SPEE, P926, DOI [10.1109/ICASSP40776.2020.9052948, 10.1109/icassp40776.2020.9052948]
   Zhou MN, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00095
   Zhu JD, 2015, J MED SYST, V39, DOI 10.1007/s10916-014-0170-6
NR 42
TC 5
Z9 5
U1 11
U2 43
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 107
DI 10.1145/3460522
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600009
DA 2024-07-18
ER

PT J
AU Hu, HZ
   Zhou, WG
   Pu, JF
   Li, HQ
AF Hu, Hezhen
   Zhou, Wengang
   Pu, Junfu
   Li, Houqiang
TI Global-Local Enhancement Network for NMF-Aware Sign Language Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Non-manual features; global-local enhancement network; NMFs-CSL dataset;
   sign language recognition
ID FRAMEWORK
AB Sign language recognition (SLR) is a challenging problem, involving complex manual features (i.e., hand gestures) and fine-grained non-manual features (NMFs) (i.e., facial expression, mouth shapes, etc.). Although manual features are dominant, non-manual features also play an important role in the expression of a sign word. Specifically, many signwords convey different meanings due to non-manual features, even though they share the same hand gestures. This ambiguity introduces great challenges in the recognition of sign words. To tackle the above issue, we propose a simple yet effective architecture called Global-Local Enhancement Network (GLE-Net), including two mutually promoted streams toward different crucial aspects of SLR. Of the two streams, one captures the global contextual relationship, while the other stream captures the discriminative fine-grained cues. Moreover, due to the lack of datasets explicitly focusing on this kind of feature, we introduce the first non-manual-feature-aware isolated Chinese sign language dataset (NMFs-CSL) with a total vocabulary size of 1,067 sign words in daily life. Extensive experiments on NMFs-CSL and SLR500 datasets demonstrate the effectiveness of our method.
C1 [Hu, Hezhen; Pu, Junfu] Univ Sci & Technol China, Hefei 230027, Peoples R China.
   [Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Hu, HZ (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.
EM alexhu@mail.ustc.edu; zhwg@ustc.edu.cn; pjh@mail.ustc.edu.cn;
   lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
FU NSFC [61632019, 61836006, 61836011]; Youth Innovation Promotion
   Association CAS [2018497]; GPU cluster built by MCC Lab of Information
   Science and Technology Institution, USTC
FX This work was supported by NSFC under contract No. 61632019, 61836006
   and 61836011, and Youth Innovation Promotion Association CAS (No.
   2018497). It was also supported by the GPU cluster built by MCC Lab of
   Information Science and Technology Institution, USTC.
CR [Anonymous], 2018, ECCV
   [Anonymous], 2018, ICML
   [Anonymous], 2018, IJCAI
   Avola D, 2019, IEEE T MULTIMEDIA, V21, P234, DOI 10.1109/TMM.2018.2856094
   Buehler P, 2009, PROC CVPR IEEE, P2953, DOI 10.1109/CVPRW.2009.5206523
   Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI 10.1109/ICCV.2017.332
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chai X, 2014, VIPLTR14SLR001
   Chen YP, 2018, LECT NOTES COMPUT SC, V11205, P364, DOI 10.1007/978-3-030-01246-5_22
   Chuan CH, 2014, 2014 13TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P541, DOI 10.1109/ICMLA.2014.110
   Cooper H, 2012, J MACH LEARN RES, V13, P2205
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Eun-jung Holden, 2001, Multi-Image Analysis. 10th International Workshop on Theoretical Foundations of Computer Vision. Revised Papers (Lecture Notes in Computer Science Vol.2032), P270
   Evangelidis GD, 2015, LECT NOTES COMPUT SC, V8925, P595, DOI 10.1007/978-3-319-16178-5_42
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   FELS SS, 1993, IEEE T NEURAL NETWOR, V4, P2, DOI 10.1109/72.182690
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P1080, DOI 10.1109/ICCV.2015.129
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Guo D, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152121
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Huang J, 2019, IEEE T CIRC SYST VID, V29, P2822, DOI 10.1109/TCSVT.2018.2870740
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Huang J, 2015, 2015 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING, P166, DOI 10.1109/ChinaSIP.2015.7230384
   Jaderberg M, 2015, ADV NEUR IN, V28
   Joze Hamid Vaezi, 2019, BMVC
   Kapuscinski T, 2015, INT J ADV ROBOT SYST, V12, DOI 10.5772/60091
   Kay W., 2017, CORR ABS170506950
   Kim JS, 1996, IEEE T SYST MAN CY B, V26, P354, DOI 10.1109/3477.485888
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2018, INT J COMPUT VISION, V126, P1311, DOI 10.1007/s11263-018-1121-3
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Koller Oscar., 2016, P BMVC, P1, DOI 10.5244/C.30.136
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu Z, 2017, IEEE INT CONF COMP V, P3056, DOI 10.1109/ICCVW.2017.361
   Liwicki Stephan, 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P50, DOI 10.1109/CVPR.2009.5204291
   Ma WY, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON SMART CITY/SOCIALCOM/SUSTAINCOM (SMARTCITY), P1, DOI [10.1109/NEBEC.2015.7117114, 10.1109/SmartCity.2015.38]
   Mallya A, 2016, LECT NOTES COMPUT SC, V9905, P414, DOI 10.1007/978-3-319-46448-0_25
   Mnih V, 2014, ADV NEUR IN, V27
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Oszust M, 2013, C HUM SYST INTERACT, P219, DOI 10.1109/HSI.2013.6577826
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Qiu ZF, 2019, PROC CVPR IEEE, P12048, DOI 10.1109/CVPR.2019.01233
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ronchetti F., 2016, P 22 C ARG CIENC COM
   Sharma S., 2015, NEURAL INFORM PROCES
   Shi B., 2019, P IEEE INT C COMP VI, P5400
   Simonyan K, 2014, ADV NEUR IN, V27
   Starner T.E., 1995, VISUAL RECOGNITION A
   Stefanidis Kiriakos, 2020, IGI Global, P50, DOI DOI 10.4018/978-1-5225-5294-9.CH009
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang A, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2735952
   Tharwat A, 2015, ADV INTELL SYST, V334, P359, DOI 10.1007/978-3-319-13572-4_30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CL, 2001, LECT NOTES COMPUT SC, V2195, P150
   Wang HJ, 2019, IEEE T MULTIMEDIA, V21, P2806, DOI 10.1109/TMM.2019.2915032
   Wang L., 2016, P ECCV
   Wang LC, 2014, IEEE T MULTIMEDIA, V16, P751, DOI 10.1109/TMM.2014.2298382
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yasir F, 2015, 2015 IEEE 8TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND APPLICATIONS (IWCIA) PROCEEDINGS, P35, DOI 10.1109/IWCIA.2015.7449458
   Yin F, 2016, LECT NOTES COMPUT SC, V9911, P434, DOI 10.1007/978-3-319-46478-7_27
   Zahedi M, 2005, LECT NOTES COMPUT SC, V3663, P401
   Zhang JH, 2016, IEEE INT CON MULTI, DOI 10.1109/ICME.2016.7552950
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zhou H, 2019, IEEE INT CON MULTI, P1282, DOI 10.1109/ICME.2019.00223
   Zhou YZ, 2018, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2018.00054
NR 77
TC 15
Z9 16
U1 2
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 80
DI 10.1145/3436754
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lin, F
   Zhou, WG
   Deng, JJ
   Li, B
   Lu, Y
   Li, HQ
AF Lin, Feng
   Zhou, Wengang
   Deng, Jiajun
   Li, Bin
   Lu, Yan
   Li, Houqiang
TI Residual Refinement Network with Attribute Guidance for Precise Saliency
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Salient object detection; residual learning; deep intermediate
   supervision; attribute encoding
ID OBJECT DETECTION; MODEL
AB As an important topic in the multimedia and computer vision fields, salient object detection has been researched for years. Recently, state-of-the-art performance has been witnessed with the aid of the fully convolutional networks (FCNs) and the various pyramid-like encoder-decoder frameworks. Starting from a common encoder-decoder architecture, we enhance a residual refinement network with feature purification for better saliency estimation. To this end, we improve the global knowledge streams with intermediate supervisions for global saliency estimation and design a specific feature subtraction module for residual learning, respectively. On the basis of the strengthened network, we also introduce an attribute encoding sub-network (AENet) with a grid aggregation block (GAB) to guide the final saliency predictor to obtain more accurate saliency maps. Furthermore, the network is trained with a novel constraint loss besides the traditional cross-entropy loss to yield the finer results. Extensive experiments on five public benchmarks show our method achieves better or comparable performance compared with previous state-of-the-art methods.
C1 [Lin, Feng; Zhou, Wengang; Deng, Jiajun; Li, Houqiang] Univ Sci & Technol China, 96 JinZhai Rd, Hefei, Peoples R China.
   [Li, Bin; Lu, Yan] Microsoft Res Asia, 5 Dan Ling St, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, 96 JinZhai Rd, Hefei, Peoples R China.
EM lin1993@mail.ustc.edu.cn; zhwg@ustc.edu.cn; dengjj@mail.ustc.edu.cn;
   libin@microsoft.com; yanlu@microsoft.com; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013; Deng, Jiajun/KIK-3592-2024
OI Lin, Feng/0000-0001-8541-7400
FU NSFC [61836011]; National Natural Science Foundation of China [61822208,
   61632019]; Youth Innovation Promotion Association CAS [2018497]
FX The work of H. Li was supported by NSFC under contract 61836011. The
   work of W. Zhou was supported in part by the National Natural Science
   Foundation of China under contracts 61822208 and 61632019, and in part
   by Youth Innovation Promotion Association CAS (No. 2018497).
CR Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Borji A, 2012, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2012.6247711
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Ding YY, 2011, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2011.5995445
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fourure D., 2017, ARXIV170707958, P181
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Guan GL, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2632267
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kim J, 2014, PROC CVPR IEEE, P883, DOI 10.1109/CVPR.2014.118
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li ZH, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5708
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Y, 2018, INT C LEARN REPR
   Liu Y, 2019, IEEE I CONF COMP VIS, P1232, DOI 10.1109/ICCV.2019.00132
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Min XK, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996463
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Quan R, 2018, IEEE T MULTIMEDIA, V20, P1101, DOI 10.1109/TMM.2017.2763780
   Sharma G, 2012, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR.2012.6248093
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K., 2014, 14091556 ARXIV
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang L, 2018, IEEE IPCCC
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Wu Zhe, 2019, CVPR, P3907
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu YY, 2019, IEEE I CONF COMP VIS, P3788, DOI 10.1109/ICCV.2019.00389
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Zhang J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107956
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
   Zhuge YZ, 2019, AAAI CONF ARTIF INTE, P9340
NR 73
TC 8
Z9 8
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 81
DI 10.1145/3440694
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400005
DA 2024-07-18
ER

PT J
AU Lokoc, J
   Vesely, P
   Mejzlík, F
   Kovalcík, G
   Soucek, T
   Rossetto, L
   Schoeffmann, K
   Bailer, W
   Gurrin, C
   Sauter, L
   Song, J
   Vrochidis, S
   Wu, JX
   Jónsson, BD
AF Lokoc, Jakub
   Vesely, Patrik
   Mejzlik, Frantisek
   Kovalcik, Gregor
   Soucek, Tomas
   Rossetto, Luca
   Schoeffmann, Klaus
   Bailer, Werner
   Gurrin, Cathal
   Sauter, Loris
   Song, Jaeyub
   Vrochidis, Stefanos
   Wu, Jiaxin
   Jonsson, Bjorn Dor
TI Is the Reign of Interactive Search Eternal? Findings from the Video
   Browser Showdown 2020
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Interactive video retrieval; deep learning; interactive search
   evaluation
ID RETRIEVAL; SYSTEM
AB Comprehensive and fair performance evaluation of information retrieval systems represents an essential task for the current information age. Whereas Cranfield-based evaluations with benchmark datasets support development of retrieval models, significant evaluation efforts are required also for user-oriented systems that try to boost performance with an interactive search approach. This article presents findings from the 9th Video Browser Showdown, a competition that focuses on a legitimate comparison of interactive search systems designed for challenging known-item search tasks over a large video collection. During previous installments of the competition, the interactive nature of participating systems was a key feature to satisfy known-item search needs, and this article continues to support this hypothesis. Despite the fact that top-performing systems integrate the most recent deep learning models into their retrieval process, interactive searching remains a necessary component of successful strategies for known-item search tasks. Alongside the description of competition settings, evaluated tasks, participating teams, and overall results, this article presents a detailed analysis of query logs collected by the top three performing systems, SOMHunter, VIRET, and vitrivr. The analysis provides a quantitative insight to the observed performance of the systems and constitutes a new baseline methodology for future events. The results reveal that the top two systems mostly relied on temporal queries before a correct frame was identified. An interaction log analysis complements the result log findings and points to the importance of result set and video browsing approaches. Finally, various outlooks are discussed in order to improve the Video Browser Showdown challenge in the future.
C1 [Lokoc, Jakub; Vesely, Patrik; Mejzlik, Frantisek; Kovalcik, Gregor; Soucek, Tomas] Charles Univ Prague, Fac Math & Phys, Malostranske Nam 25, Prague 11800, Czech Republic.
   [Rossetto, Luca] Univ Zurich, Dept Informat, Binzmuhlestr 14, CH-8050 Zurich, Switzerland.
   [Schoeffmann, Klaus] Klagenfurt Univ, Klagenfurt, Austria.
   [Bailer, Werner] Joanneum Res, Steyrergasse 17, A-8010 Graz, Austria.
   [Gurrin, Cathal] Dublin City Univ, Dublin, Ireland.
   [Sauter, Loris] Univ Basel, Dept Math & Comp Sci, Spiegelgasse 1, CH-4051 Basel, Switzerland.
   [Song, Jaeyub] Korea Adv Inst Sci & Technol, 291 Daehak Ro, Daejeon 34141, South Korea.
   [Vrochidis, Stefanos] CERTH, Informat Technol Inst, 6th Km Charilaou Thermi Rd, Thessaloniki 57001, Greece.
   [Wu, Jiaxin] City Univ Hong Kong, Run Run Shaw Creat Media Ctr, Kowloon Tong, Hong Kong, Peoples R China.
   [Jonsson, Bjorn Dor] IT Univ Copenhagen, Copenhagen, Denmark.
   [Jonsson, Bjorn Dor] Rued Langgaards Vej 7, DK-2300 Copenhagen S, Denmark.
C3 Charles University Prague; University of Zurich; University of
   Klagenfurt; Dublin City University; University of Basel; Korea Advanced
   Institute of Science & Technology (KAIST); Centre for Research &
   Technology Hellas; City University of Hong Kong; IT University
   Copenhagen
RP Lokoc, J (corresponding author), Charles Univ Prague, Fac Math & Phys, Malostranske Nam 25, Prague 11800, Czech Republic.
EM lokoc@ksi.mff.cuni.cz; prtrikvesely@gmail.com; frankmejzlik@gmail.com;
   gregor.kovalcik@gmail.com; tomas.soucek1@gmail.com; rossetto@ifi.uzh.ch;
   ks@itec.aau.at; werner.bailer@joanneum.at; cathal.gurrin@dcu.ie;
   loris.sauter@unibas.ch; jsong0327@kaist.ac.kr; stefanos@iti.gr;
   jiaxin.wu@my.cityu.edu.hk; bjth@itu.dk
RI Wu, Jiaxin/GVT-3486-2022; Lokoč, Jakub/P-1216-2017; Gurrin,
   Cathal/Q-4442-2019; Rossetto, Luca/AAI-8684-2020
OI Gurrin, Cathal/0000-0003-2903-3968; Rossetto, Luca/0000-0002-5389-9465;
   Vrochidis, Stefanos/0000-0002-2505-9178; Song,
   Jaeyub/0000-0002-8714-3308; Sauter, Loris/0000-0001-8046-0362; wu,
   jiaxin/0000-0003-4074-3442; Bailer, Werner/0000-0003-2442-4900
FU Czech Science Foundation (GACR) project [19-22071Y]; European Union's
   Horizon 2020 research and innovation programmes [761802, 779962]; H2020
   - Industrial Leadership [779962, 761802] Funding Source: H2020 -
   Industrial Leadership
FX This article has been supported by Czech Science Foundation (GACR)
   project 19-22071Y. Part of this work has received funding from European
   Union's Horizon 2020 research and innovation programmes under Grant No.
   761802 MARCONI and 779962 V4Design.
CR Andreadis S, 2020, LECT NOTES COMPUT SC, V11962, P778, DOI 10.1007/978-3-030-37734-2_69
   [Anonymous], 2002, Introduction to MPEG- 7: Multimedia content description interface
   [Anonymous], 2018, COMPUTER VISION PATT
   [Anonymous], 2014, P NIPS
   [Anonymous], 2019, ABS190603363 CORR
   Awad G., 2019, P TRECVID 2019
   Berns F, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P334, DOI 10.1145/3323873.3325051
   Carreira J., 2018, arXiv
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen L.-C., 2018, ASIA-PAC J ATMOS SCI, DOI DOI 10.1007/s13143-018-0064-5
   Cobârzan C, 2017, MULTIMED TOOLS APPL, V76, P5539, DOI 10.1007/s11042-016-3661-2
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Duan LY, 2019, IEEE MULTIMEDIA, V26, P44, DOI 10.1109/MMUL.2018.2873844
   Duan LY, 2014, IEEE MULTIMEDIA, V21, P30, DOI 10.1109/MMUL.2013.66
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Galanopoulos Damianos, 2020, P 2020 ACM INT C MUL
   Gasser R, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P391, DOI 10.1145/3323873.3326921
   GeorgeAwad Asad Butt, 2017, TRECVID 2017
   Gialampoukidis I, 2016, 2016 11TH INTERNATIONAL WORKSHOP ON SEMANTIC AND SOCIAL MEDIA ADAPTATION AND PERSONALIZATION (SMAP), P1, DOI 10.1109/SMAP.2016.7753375
   Gkountakos K, 2019, INT ICE CONF ENG
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jónsson BP, 2020, LECT NOTES COMPUT SC, V11962, P796, DOI 10.1007/978-3-030-37734-2_72
   Khan Omar Shahbaz, 2020, P EUR C INF RETR ECI, P16
   Kratochvíl M, 2020, LECT NOTES COMPUT SC, V11962, P790, DOI 10.1007/978-3-030-37734-2_71
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Leibetseder A, 2020, LECT NOTES COMPUT SC, V11962, P753, DOI 10.1007/978-3-030-37734-2_65
   Li XR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1786, DOI 10.1145/3343031.3350906
   Li YC, 2016, PROC CVPR IEEE, P4641, DOI 10.1109/CVPR.2016.502
   Lokoc J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2553, DOI 10.1145/3394171.3414002
   Lokoc J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1777, DOI 10.1145/3343031.3351046
   Lokoc J, 2020, LECT NOTES COMPUT SC, V11962, P784, DOI 10.1007/978-3-030-37734-2_70
   Lokoc J, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P177, DOI 10.1145/3323873.3325034
   Lokoc J, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3295663
   Lokoc J, 2018, IEEE T MULTIMEDIA, V20, P3361, DOI 10.1109/TMM.2018.2830110
   Markatopoulou Foteini, 2018, IEEE T CIRC SYST VID
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   Mettes Pascal, 2020, ACM T MULTIMEDIA COM, V16, P1
   Nguyen Phuong Anh, 2019, NIST TRECVID WORKSH
   Le NK, 2020, LECT NOTES COMPUT SC, V11962, P766, DOI 10.1007/978-3-030-37734-2_67
   Park S, 2020, LECT NOTES COMPUT SC, V11962, P809, DOI 10.1007/978-3-030-37734-2_74
   Nguyen PA, 2020, LECT NOTES COMPUT SC, V11962, P772, DOI 10.1007/978-3-030-37734-2_68
   Nguyen PA, 2018, LECT NOTES COMPUT SC, V10705, P407, DOI 10.1007/978-3-319-73600-6_42
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rossetto L, 2021, IEEE T MULTIMEDIA, V23, P243, DOI 10.1109/TMM.2020.2980944
   Rossetto L, 2019, LECT NOTES COMPUT SC, V11295, P349, DOI 10.1007/978-3-030-05710-7_29
   Rossetto L, 2019, LECT NOTES COMPUT SC, V11296, P616, DOI 10.1007/978-3-030-05716-9_55
   Rossetto L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1183, DOI 10.1145/2964284.2973797
   Rossetto L, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P18, DOI 10.1109/ISM.2014.38
   Rossetto Luca, 2021, P 27 INT C MULT MOD
   Rossetto Luca, 2021, P 27 INT C MULT MOD
   Sauter L, 2020, LECT NOTES COMPUT SC, V11962, P760, DOI 10.1007/978-3-030-37734-2_66
   Schoeffmann K, 2019, INT WORK CONTENT MUL
   Schoeffmann K, 2019, LECT NOTES COMPUT SC, V11296, P585, DOI 10.1007/978-3-030-05716-9_50
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Smith R, 2007, PROC INT CONF DOC, P629, DOI 10.1109/icdar.2007.4376991
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xun Yang, 2020, SIGIR '20: Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, P1339, DOI 10.1145/3397271.3401151
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
NR 71
TC 25
Z9 25
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 91
DI 10.1145/3445031
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400015
OA Green Published
DA 2024-07-18
ER

PT J
AU Liang, W
   Long, J
   Li, KC
   Xu, JB
   Ma, NJ
   Lei, X
AF Liang, Wei
   Long, Jing
   Li, Kuan-Ching
   Xu, Jianbo
   Ma, Nanjun
   Lei, Xia
TI A Fast Defogging Image Recognition Algorithm Based on Bilateral Hybrid
   Filtering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE IoT; defogging image; bilateral hybrid filtering; robustness
AB With the rapid advancement of video and image processing technologies in the Internet of Things, it is urgent to address the issues in real-time performance, clarity, and reliability of image recognition technology for a monitoring system in foggy weather conditions. In this work, a fast defogging image recognition algorithm is proposed based on bilateral hybrid filtering. First, the mathematical model based on bilateral hybrid filtering is established. The dark channel is used for filtering and denoising the defogging image. Next, a bilateral hybrid filtering method is proposed by using a combination of guided filtering and median filtering, as it can effectively improve the robustness and transmittance of defogging images. On this basis, the proposed algorithm dramatically decreases the computation complexity of defogging image recognition and reduces the image execution time. Experimental results show that the defogging effect and speed are promising, with the image recognition rate reaching to 98.8% after defogging.
C1 [Liang, Wei] Hunan Univ, Changsha, Peoples R China.
   [Long, Jing] Hunan Normal Univ, Changsha, Peoples R China.
   [Long, Jing] Minjiang Univ, Fujian Prov Key Lab Informat Proc & Intelligent C, Fuzhou, Peoples R China.
   [Li, Kuan-Ching] Providence Univ, Taichung, Taiwan.
   [Xu, Jianbo; Ma, Nanjun] Hunan Univ Sci & Technol, Xiangtan, Peoples R China.
   [Lei, Xia] China Univ Petr, Beijing, Peoples R China.
C3 Hunan University; Hunan Normal University; Minjiang University;
   Providence University - Taiwan; Hunan University of Science &
   Technology; China University of Petroleum
RP Long, J (corresponding author), Hunan Normal Univ, Changsha, Peoples R China.; Long, J (corresponding author), Minjiang Univ, Fujian Prov Key Lab Informat Proc & Intelligent C, Fuzhou, Peoples R China.
EM weiliang99@hnu.edu.cn; jlong@hunnu.edu.cn; kuancli@pu.edu.tw;
   jbxu@hnust.cn; njma@hnust.cn; leixia2008530059@163.com
RI lei, xia/T-5940-2017; Li, K/S-4073-2019
OI Li, K/0000-0003-1381-4364
FU National Natural Science Foundation of China [61872138, 62072170]; Hunan
   Provincial Science & Technology Project Foundation [2018TP1018,
   2020JJ5369]; Scientifc Research Fund of the Hunan Provincial Education
   Department [19C1157]; Start-Up Funds of Hunan Normal University
   [531120-3812]; Fujian Provincial Natural Science Foundation of China
   [2018J01570]; Guangxi Key Laboratory of Crytography and Information
   Security [GCIS201920]; Open Fund Project of Fujian Provincial Key
   Laboratory of Information Processing and Intelligent Control (Minjiang
   University) [MJUKF-IPIC202008]
FX This research was supported by the National Natural Science Foundation
   of China (Nos. 61872138, 62072170), the Hunan Provincial Science &
   Technology Project Foundation (Nos. 2018TP1018,2020JJ5369), the
   Scientifc Research Fund of the Hunan Provincial Education Department
   (No.19C1157), the Start-Up Funds of Hunan Normal University (No.
   531120-3812), and Fujian Provincial Natural Science Foundation of China
   (No. 2018J01570), Guangxi Key Laboratory of Crytography and Information
   Security (No. GCIS201920) and Open Fund Project of Fujian Provincial Key
   Laboratory of Information Processing and Intelligent Control (Minjiang
   University) (No. MJUKF-IPIC202008).
CR Chen MY, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT, PROCEEDINGS, P675, DOI 10.1109/ICNIDC.2009.5360871
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Dong LM, 2015, NEUROCOMPUTING, V159, P268, DOI 10.1016/j.neucom.2015.01.050
   Fan TH, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P410, DOI 10.1109/ICIVC.2017.7984588
   Gibson KB, 2012, IEEE T IMAGE PROCESS, V21, P662, DOI 10.1109/TIP.2011.2166968
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Jiang XP, 2018, IEEE T IND INFORM, V14, P3281, DOI 10.1109/TII.2018.2810188
   Kaur Mandeep, 2018, P INT C FUT TRENDS N, P45, DOI [10.1007/978-981-13-3804-5_4, DOI 10.1007/978-981-13-3804-5_4]
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Liang W, 2020, IEEE T IND INFORM, V16, P6543, DOI 10.1109/TII.2020.2966069
   Liang W, 2020, IEEE T IND INFORM, V16, P2063, DOI 10.1109/TII.2019.2946791
   Liang W, 2019, IEEE T IND INFORM, V15, P3582, DOI 10.1109/TII.2019.2907092
   Ma RQ, 2019, OPTIK, V180, P997, DOI 10.1016/j.ijleo.2018.12.020
   Ma ZL, 2016, NEUROCOMPUTING, V173, P1257, DOI 10.1016/j.neucom.2015.08.084
   Mastriani Mario, 2018, ARXIV180711571, DOI [10.1016/j.neucom.2015.08.084, DOI 10.1016/J.NEUCOM.2015.08.084]
   Mutimbu L, 2018, PATTERN RECOGN, V82, P56, DOI 10.1016/j.patcog.2018.04.023
   Paul A, 2013, IETE TECH REV, V30, P157, DOI 10.4103/0256-4602.110555
   Rahmatov N, 2019, INT J DISTRIB SENS N, V15, DOI 10.1177/1550147719883551
   Rathore MMU, 2016, J SYST ARCHITECT, V64, P122, DOI 10.1016/j.sysarc.2015.11.006
   Shi HY, 2018, PROC SPIE, V10615, DOI 10.1117/12.2302669
   Shi YK, 2019, IEEE ACCESS, V7, P39660, DOI 10.1109/ACCESS.2019.2906936
   Sivakumar R., 2018, INT J APPL ENG RES, V13, P9811
   Starck JL, 2002, IEEE T IMAGE PROCESS, V11, P670, DOI [10.1109/TIP.2002.1014998, 10.1117/12.408568]
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tan ZM, 2014, FUJITSU SCI TECH J, V50, P60
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang YK, 2014, IEEE T IMAGE PROCESS, V23, P4826, DOI 10.1109/TIP.2014.2358076
   Wu JJ, 2010, SIGNAL PROCESS-IMAGE, V25, P717, DOI 10.1016/j.image.2010.10.003
   Xu Y, 2016, IEEE ACCESS, V4, P165, DOI 10.1109/ACCESS.2015.2511558
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
NR 31
TC 73
Z9 75
U1 5
U2 119
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 42
DI 10.1145/3391297
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000004
DA 2024-07-18
ER

PT J
AU Wang, Y
AF Wang, Yang
TI Survey on Deep Multi-modal Data Analytics: Collaboration, Rivalry, and
   Fusion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-modal data; deep neural networks
ID MULTIVIEW; REPRESENTATIONS; RECOGNITION; NETWORK
AB With the development of web technology, multi-modal or multi-view data has surged as a major stream for big data, where each modal/view encodes individual property of data objects. Often, different modalities are complementary to each other. This fact motivated a lot of research attention on fusing the multi-modal feature spaces to comprehensively characterize the data objects. Most of the existing state-of-the-arts focused on how to fuse the energy or information from multi-modal spaces to deliver a superior performance over their counterparts with single modal. Recently, deep neural networks have been exhibited as a powerful architecture to well capture the nonlinear distribution of high-dimensional multimedia data, so naturally does for multi-modal data. Substantial empirical studies are carried out to demonstrate its advantages that are benefited from deep multi-modal methods, which can essentially deepen the fusion from multi-modal deep feature spaces. In this article, we provide a substantial overview of the existing state-of-the-arts in the field of multi-modal data analytics from shallow to deep spaces. Throughout this survey, we further indicate that the critical components for this field go to collaboration, adversarial competition, and fusion over multi-modal spaces. Finally, we share our viewpoints regarding some future directions in this field.
C1 [Wang, Yang] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei, Peoples R China.
   [Wang, Yang] Hefei Univ Technol, Intelligent Interconnected Syst Lab Anhui Prov, Hefei, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology
RP Wang, Y (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei, Peoples R China.; Wang, Y (corresponding author), Hefei Univ Technol, Intelligent Interconnected Syst Lab Anhui Prov, Hefei, Peoples R China.
EM yangwang@hfut.edu.cn
FU National Natural Science Foundation of China; NSFC [61806035]
FX This research is supported by National Natural Science Foundation of
   China, under Grants No. NSFC U1936217 and No. NSFC 61806035. All content
   represents the opinion of the authors, which is not necessarily shared
   or endorsed by their respective employers and/or sponsors.
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2017, P ADV NEUR INF PROC
   [Anonymous], 1998, The mnist database of handwritten digits
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   Aytar Y, 2018, IEEE T PATTERN ANAL, V40, P2303, DOI 10.1109/TPAMI.2017.2753232
   Baumgartner CF, 2018, PROC CVPR IEEE, P8309, DOI 10.1109/CVPR.2018.00867
   Bickel S, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P19, DOI 10.1109/ICDM.2004.10095
   Cai X, 2012, BIOINFORMATICS, V28, pI16, DOI 10.1093/bioinformatics/bts220
   Cao Guanqun, 2018, ABS180110402 CORR
   CAO J, 2018, ARXIV180207447
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Cao Y, 2017, AAAI CONF ARTIF INTE, P3974
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   Chen Mickael, 2016, ABS161102019 CORR
   Chen TF, 2017, IEEE INT CON MULTI, P955, DOI 10.1109/ICME.2017.8019322
   Chen X, 2005, COMPUT VIS IMAGE UND, V99, P332, DOI 10.1016/j.cviu.2005.03.001
   Chen X, 2016, ADV NEUR IN, V29
   Chen ZD, 2018, AAAI CONF ARTIF INTE, P274
   Chi JJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2165
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dizaji KG, 2019, PROC CVPR IEEE, P4386, DOI 10.1109/CVPR.2019.00452
   Dizaji KG, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1435, DOI 10.1145/3219819.3220114
   Dizaji KG, 2018, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR.2018.00386
   Doinychko Anastasiia, 2020, Advances in Information Retrieval, 42nd European Conference on IR Research, ECIR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12035), P807, DOI 10.1007/978-3-030-45439-5_53
   Du CY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1348, DOI 10.1145/3219819.3219957
   Dumoulin V., 2016, INT C LEARN REPR
   Espinosa-Duró V, 2013, COGN COMPUT, V5, P119, DOI 10.1007/s12559-012-9163-2
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Forsyth David, 2008, P EUR C COMP VIS ECC, P262
   Gao JY, 2019, AAAI CONF ARTIF INTE, P3622
   Gonçalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021
   Gong Yihong, 2020, P AAAI C ART INT
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goutte Cyril, 2009, P 23 ANN C NEUR INF
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Guo HY, 2016, IEEE T IMAGE PROCESS, V25, P5526, DOI 10.1109/TIP.2016.2609814
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   He L, 2017, IEEE INT CON MULTI, P1153, DOI 10.1109/ICME.2017.8019549
   Hong CQ, 2015, IEEE T IMAGE PROCESS, V24, P5659, DOI 10.1109/TIP.2015.2487860
   Hu JL, 2018, IEEE T PATTERN ANAL, V40, P2281, DOI 10.1109/TPAMI.2017.2749576
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Hu P, 2019, IEEE T IMAGE PROCESS, V28, P5352, DOI 10.1109/TIP.2019.2913511
   Hu SW, 2015, J OPT SOC AM A, V32, P431, DOI 10.1364/JOSAA.32.000431
   Hu ZX, 2020, INFORM FUSION, V55, P251, DOI 10.1016/j.inffus.2019.09.005
   Huang Heyan, 2019, ABS190712490 CORR
   Huang HC, 2012, PROC CVPR IEEE, P773, DOI 10.1109/CVPR.2012.6247748
   Huang SW, 2018, LECT NOTES COMPUT SC, V11213, P731, DOI 10.1007/978-3-030-01240-3_44
   Huang X, 2020, IEEE T CYBERNETICS, V50, P1047, DOI 10.1109/TCYB.2018.2879846
   Huang ZY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2563
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Jia K, 2019, IEEE T IMAGE PROCESS, V28, P5121, DOI 10.1109/TIP.2019.2912356
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jiang Y., 2019, NEURIPS, P5880
   Jiang Y. -G., 2011, P 1 ACM INT C MULT R, P29
   Jin L, 2019, IEEE T NEUR NET LEAR, V30, P1429, DOI 10.1109/TNNLS.2018.2869601
   John Shepherd, 2013, P ACM C MULT
   Jungong Han, 2018, P INT JOINT C ART IN
   Kan MN, 2016, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2016.524
   Kanojia Gagan, 2020, P NAT C COMM NCC 20, P1
   Kim S, 2016, LECT NOTES COMPUT SC, V9912, P679, DOI 10.1007/978-3-319-46484-8_41
   Kumar Abhishek, 2011, P C NEUR INF PROC SY
   Lai YH, 2018, IEEE INT CONF AUTOMA, P263, DOI 10.1109/FG.2018.00046
   Li C, 2019, AAAI CONF ARTIF INTE, P176
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li CX, 2017, ADV NEUR IN, V30
   Li D, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107085
   Li JX, 2018, AAAI CONF ARTIF INTE, P3498
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li XM, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1373
   Li XL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1398, DOI 10.1145/3123266.3123355
   Li Xuelong, 2019, IEEE T CYBERNET
   Li ZY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2952
   Lin CH, 2018, PATTERN RECOGN, V83, P314, DOI 10.1016/j.patcog.2018.05.004
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2020, IEEE T PATTERN ANAL, V42, P580, DOI 10.1109/TPAMI.2018.2882816
   Liong VE, 2017, IEEE I CONF COMP VIS, P4097, DOI 10.1109/ICCV.2017.439
   Liu X, 2021, IEEE T PATTERN ANAL, V43, P964, DOI 10.1109/TPAMI.2019.2940446
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   Liu XW, 2017, AAAI CONF ARTIF INTE, P2259
   Liu XW, 2019, AAAI CONF ARTIF INTE, P4400
   Luo Yi, 2018, P 32 AAAI C ART INT
   Ma YH, 2017, AAAI CONF ARTIF INTE, P38
   Madani O, 2013, MACH LEARN, V92, P457, DOI 10.1007/s10994-013-5377-0
   Mahmud T, 2018, IEEE GLOB CONF SIG, P1164, DOI 10.1109/GlobalSIP.2018.8646380
   Marin J, 2021, IEEE T PATTERN ANAL, V43, P187, DOI 10.1109/TPAMI.2019.2927476
   McLachlan G. J., 1997, EM ALGORTIHM ITS EXT
   Mukherjee S, 2019, AAAI CONF ARTIF INTE, P4610
   Nie FP, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107207
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Nie Feiping, 2016, P INT JOINT C ART IN
   Niu YL, 2019, IEEE T IMAGE PROCESS, V28, P1720, DOI 10.1109/TIP.2018.2881928
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qing Zhang, 2014, P ACM C MULT
   Rashtchian Cyrus, 2010, P NAACL HLT WORKSH C
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Sage A, 2018, PROC CVPR IEEE, P5879, DOI 10.1109/CVPR.2018.00616
   Saito K, 2017, IEEE INT CONF COMP V, P2623, DOI 10.1109/ICCVW.2017.311
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Sarfraz MS, 2017, INT J COMPUT VISION, V122, P426, DOI 10.1007/s11263-016-0933-2
   Shang C, 2017, IEEE INT CONF BIG DA, P766, DOI 10.1109/BigData.2017.8257992
   Shang F, 2019, NEUROCOMPUTING, V355, P93, DOI 10.1016/j.neucom.2019.04.041
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song LY, 2018, IEEE T IMAGE PROCESS, V27, P6025, DOI 10.1109/TIP.2018.2864920
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Sun YW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3527
   Tao ZQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3562
   Tian Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P942
   Wang T, 2020, NEUROCOMPUTING, V386, P84, DOI 10.1016/j.neucom.2019.12.058
   van Breukelen M, 1998, KYBERNETIKA, V34, P381
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang C, 2015, PROC INT C TOOLS ART, P234, DOI 10.1109/ICTAI.2015.45
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang LC, 2019, IEEE I CONF COMP VIS, P6221, DOI 10.1109/ICCV.2019.00631
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wang QQ, 2018, IEEE DATA MINING, P1290, DOI 10.1109/ICDM.2018.00174
   Wang X, 2019, KNOWL-BASED SYST, V168, P109, DOI 10.1016/j.knosys.2019.01.017
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wang Y, 2018, NEURAL NETWORKS, V103, P1, DOI 10.1016/j.neunet.2018.03.006
   Wang Y, 2017, IEEE T IMAGE PROCESS, V26, P1393, DOI 10.1109/TIP.2017.2655449
   Wang Y, 2017, IEEE T NEUR NET LEAR, V28, P57, DOI 10.1109/TNNLS.2015.2498149
   Wang Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P79, DOI 10.1145/2733373.2806233
   Wang Y, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P999, DOI 10.1145/2766462.2767825
   Wang Y, 2016, KNOWL INF SYST, V46, P515, DOI 10.1007/s10115-015-0833-8
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wang Y, 2013, INFORM SCIENCES, V220, P292, DOI 10.1016/j.ins.2012.07.009
   Wang Yang, 2016, P INT JOINT C ART IN
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wen X, 2019, IEEE INT CON MULTI, P478, DOI 10.1109/ICME.2019.00089
   Wu L, 2021, IEEE T NEUR NET LEAR, V32, P722, DOI 10.1109/TNNLS.2020.2979190
   Wu L, 2020, IEEE T CIRC SYST VID, V30, P2081, DOI 10.1109/TCSVT.2019.2909549
   Wu L, 2019, IEEE T NEUR NET LEAR, V30, P3347, DOI 10.1109/TNNLS.2019.2891244
   Wu L, 2020, IEEE T IMAGE PROCESS, V29, P1233, DOI 10.1109/TIP.2019.2940684
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu L, 2019, IEEE T IMAGE PROCESS, V28, P1602, DOI 10.1109/TIP.2018.2878970
   Wu L, 2019, IEEE T CYBERNETICS, V49, P1791, DOI 10.1109/TCYB.2018.2813971
   Wu L, 2017, IEEE T CYBERNETICS, V47, P4497, DOI 10.1109/TCYB.2016.2612686
   Wu L, 2017, IMAGE VISION COMPUT, V57, P58, DOI 10.1016/j.imavis.2016.11.008
   Wu X, 2018, AAAI CONF ARTIF INTE, P1679
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xianping Fu, 2020, IEEE T MULTIMEDIA
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xu C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3933
   Xu C, 2014, IEEE T PATTERN ANAL, V36, P1559, DOI 10.1109/TPAMI.2013.2296528
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Xuan Q, 2019, IEEE T IND ELECTRON, V66, P8244, DOI 10.1109/TIE.2018.2885684
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yang LX, 2019, IEEE I CONF COMP VIS, P6449, DOI 10.1109/ICCV.2019.00654
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Yang Y, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2594, DOI 10.1145/3219819.3220012
   Yann LeCun, 2016, 4 INT C LEARN REPR I
   Yao HX, 2018, AAAI CONF ARTIF INTE, P2588
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yu J, 2018, LECT NOTES COMPUT SC, V11164, P223, DOI 10.1007/978-3-030-00776-8_21
   Yu J, 2017, IEEE T CYBERNETICS, V47, P4014, DOI 10.1109/TCYB.2016.2591583
   Yuan Y, 2019, AAAI CONF ARTIF INTE, P9176
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang Changqing, 2019, P ADV NEUR INF PROC, P557
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zheng F, 2018, IEEE T PATTERN ANAL, V40, P1059, DOI 10.1109/TPAMI.2016.2645565
   Zhou P, 2018, PROC CVPR IEEE, P1596, DOI 10.1109/CVPR.2018.00172
NR 175
TC 109
Z9 111
U1 9
U2 50
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 10
DI 10.1145/3408317
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gati, NJ
   Yang, LT
   Feng, J
   Mo, YJ
   Alazab, M
AF Gati, Nicholaus J.
   Yang, Laurence T.
   Feng, Jun
   Mo, Yijun
   Alazab, Mamoun
TI Differentially Private Tensor Train Deep Computation for Internet of
   Multimedia Things
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Tensor train auto-encoder; epsilon-differential privacy;
   cyber-physical-social systems; deep learning; privacy; deep computation;
   IoMT
AB The significant growth of the Internet of Things (IoT) takes a key and active role in healthcare. smart homes, smart manufacturing, and wearable gadgets. Due to complexness and difficulty in processing multimedia data, the IoT based scheme, namely Internet of Multimedia Things (IoMT) exists that is specialized for services and applications based on multimedia data. However, IoMT generated data are facing major processing and privacy issues. Therefore, tensor-based deep computation models proved a better platform to process IoMT generated data. A differentially private deep computation method working in the tensor space can attest to its efficacy for IoMT. Nevertheless, the deep computation model comprises a multitude of parameters; thus, it requires large units of memory and expensive computing units with higher performance levels, which hinders its performance for IoMT. Motivated by this, therefore, the paper proposes a deep private tensor train autoencoder (dPTTAE) technique to deal with IoMT generated data. Notably, the compression of weight tensors to manageable tensor train format is achieved through Tensor Train (TT) network. Moreover, TT format parameters are trained through higher-order back-propagation and gradient descent. We applied dPTTAE on three representative datasets. Comprehensive experimental evaluations and theoretical analysis show that dPTTAE enhances training time efficiency, and greatly improve memory utilization efficiency, attesting its potential for IoMT.
C1 [Gati, Nicholaus J.; Yang, Laurence T.; Mo, Yijun] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
   [Yang, Laurence T.] St Francis Xavier Univ, Dept Math Stat & Comp Sci, Antigonish, NS, Canada.
   [Feng, Jun] Huazhong Univ Sci & Technol, Sch Cyber Sci & Engn, Wuhan, Peoples R China.
   [Alazab, Mamoun] Charles Darwin Univ, Coll Engn IT & Environm, Darwin, NT, Australia.
C3 Huazhong University of Science & Technology; Saint Francis Xavier
   University - Canada; Huazhong University of Science & Technology;
   Charles Darwin University
RP Yang, LT (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.; Yang, LT (corresponding author), St Francis Xavier Univ, Dept Math Stat & Comp Sci, Antigonish, NS, Canada.
EM ltyang@gmail.com
RI Laurence T. Yang, FCAE/AAA-1898-2019; Alazab, Mamoun/AAG-6684-2021;
   Feng, Jun/GNM-8320-2022
OI Laurence T. Yang, FCAE/0000-0002-7986-4244; Alazab,
   Mamoun/0000-0002-1928-3704; Feng, Jun/0000-0001-9917-1819
CR Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Agrawal R, 2000, SIGMOD REC, V29, P439, DOI 10.1145/335191.335438
   Alvi SA, 2015, AD HOC NETW, V33, P87, DOI 10.1016/j.adhoc.2015.04.006
   Aslam A, 2018, IEEE ACCESS, V6, P25573, DOI 10.1109/ACCESS.2018.2823590
   Atzori L, 2010, COMPUT NETW, V54, P2787, DOI 10.1016/j.comnet.2010.05.010
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chen XW, 2014, IEEE ACCESS, V2, P514, DOI 10.1109/ACCESS.2014.2325029
   Cichocki A., 2014, Era of big data processing: a new approach via tensor networks and tensor decompositions. arXiv, P1
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14
   Feng J, 2022, IEEE INTERNET THINGS, V9, P8387, DOI 10.1109/JIOT.2020.3004826
   Feng J, 2021, IEEE T IND INFORM, V17, P4904, DOI 10.1109/TII.2020.2968923
   Feng J, 2022, IEEE T IND INFORM, V18, P7009, DOI 10.1109/TII.2020.2998086
   Feng J, 2019, ACM T INTERNET TECHN, V19, DOI 10.1145/3230641
   Feng J, 2020, IEEE T DEPEND SECURE, V17, P857, DOI 10.1109/TDSC.2018.2881452
   Feng J, 2018, IEEE WIREL COMMUN, V25, P98, DOI 10.1109/MWC.2017.1800097
   He DJ, 2013, IEEE T IND ELECTRON, V60, P5348, DOI 10.1109/TIE.2012.2218562
   Kumari A, 2018, J NETW COMPUT APPL, V124, P169, DOI 10.1016/j.jnca.2018.09.014
   Lindell Y, 2000, LECT NOTES COMPUT SC, V1880, P36
   Liu Y, 2010, IEEE T NEURAL NETWOR, V21, P1848, DOI 10.1109/TNN.2010.2066574
   Luo CQ, 2014, IEEE T PARALL DISTR, V25, P3211, DOI 10.1109/TPDS.2013.2297922
   Lyu HL, 2015, CHIN CONT DECIS CONF, P2885
   McSherry F, 2010, ACM SIGCOMM COMP COM, V40, P123, DOI 10.1145/1851275.1851199
   Phan N, 2016, AAAI CONF ARTIF INTE, P1309
   Novikov A, 2015, ADV NEUR IN, V28
   Patterson Eric K., 2002, 2002 IEEE INT C AC S, V2
   Rani S, 2017, IEEE INTERNET THINGS, V4, P832, DOI 10.1109/JIOT.2017.2671460
   Shokri R, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1310, DOI 10.1145/2810103.2813687
   Zhang J, 2012, PROC VLDB ENDOW, V5, P1364, DOI 10.14778/2350229.2350253
   Zhang Q., 2017, Math Probl. Eng, V2017, P1
   Zhang QC, 2018, IEEE T IND INFORM, V14, P3197, DOI 10.1109/TII.2018.2791423
NR 33
TC 6
Z9 7
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 3
SU S
AR 95
DI 10.1145/3421276
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2RB
UT WOS:000612586300003
DA 2024-07-18
ER

PT J
AU Namasudra, S
   Chakraborty, R
   Majumder, A
   Moparthi, NR
AF Namasudra, Suyel
   Chakraborty, Rupak
   Majumder, Abhishek
   Moparthi, Nageswara Rao
TI Securing Multimedia by Using DNA-Based Encryption in the Cloud Computing
   Environment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cloud computing; DNA computing; complementary rule; American Standard
   Code for Information Interchange; decimal encoding rule; CloudSim
ID ACCESS-CONTROL; HIDING MESSAGES; ALGORITHM; PROTOCOL
AB Today, the size of a multimedia file is increasing day by day from gigabytes to terabytes or even petabytes, mainly because of the evolution of a large amount of real-time data. As most of the multimedia files are transmitted through the internet, hackers and attackers try to access the users' personal and confidential data without any authorization. Thus, maintaining a strong security technique has become a significant concerned to protect the personal information. Deoxyribonucleic Acid (DNA) computing is an advanced field for improving security, which is based on the biological concept of DNA. A novel DNA-based encryption scheme is proposed in this article for protecting multimedia files in the cloud computing environment. Here, a 1024-bit secret key is generated based on DNA computing and the user's attributes and password to encrypt any multimedia file. To generate the secret key, the decimal encoding rule, American Standard Code for Information Interchange value, DNA reference key, and complementary rule are used, which enable the system to protect the multimedia file against many security attacks. Experimental results, as well as theoretical analyses, show the efficiency of the proposed scheme over some well-known existing schemes.
C1 [Namasudra, Suyel] Natl Inst Technol Patna, Dept Comp Sci & Engn, Patna 800005, Bihar, India.
   [Chakraborty, Rupak] Guru Nanak Inst Technol, Dept Comp Sci & Engn, Kolkata 700114, W Bengal, India.
   [Majumder, Abhishek] Tripura Univ, Dept Comp Sci & Engn, Suryamaninagar 799022, Tripura, India.
   [Moparthi, Nageswara Rao] Koneru Lakshmaiah Educ Fdn, Dept Comp Sci & Engn, Vaddeswaram 522502, AP, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Patna; Tripura University; Koneru Lakshmaiah Education
   Foundation (K L Deemed to be University)
RP Namasudra, S (corresponding author), Natl Inst Technol Patna, Dept Comp Sci & Engn, Patna 800005, Bihar, India.
EM suyelnamasudra@gmail.com; rupak.jis@gmail.com; abhi2012@gmail.com;
   mnrphd@gmail.com
RI Chakraborty, Rupak/AAA-4734-2020; Majumder, Abhishek/AAV-3041-2020;
   Moparthi, Nageswara Rao/V-8130-2017; Chakraborty, Rupak/GLN-8156-2022;
   Majumder, Abhishek/ABC-3221-2021; Majumder, Abhishek/GQO-9495-2022;
   Namasudra, Suyel/AAV-1723-2020
OI Majumder, Abhishek/0000-0001-8451-0451; Moparthi, Nageswara
   Rao/0000-0001-6406-4554; Chakraborty, Rupak/0000-0002-2900-5863;
   Majumder, Abhishek/0000-0001-8451-0451; 
CR Alguliyev RM, 2020, CAAI T INTELL TECHNO, V5, P9, DOI 10.1049/trit.2019.0048
   [Anonymous], 2017, 2017 INT C INN INF
   [Anonymous], 2015, INT J CURR ENG TECHN
   [Anonymous], 1992, 15 NAT COMP SEC C
   [Anonymous], 2017, P 2 INT C REC TRENDS
   [Anonymous], 2014, 2014 INT C GREEN
   Ben Salem M, 2011, LECT NOTES COMPUT SC, V6739, P35, DOI 10.1007/978-3-642-22424-9_3
   Bethencourt J, 2007, P IEEE S SECUR PRIV, P321, DOI 10.1109/sp.2007.11
   Calheiros RN, 2011, SOFTWARE PRACT EXPER, V41, P23, DOI 10.1002/spe.995
   Clelland CT, 1999, NATURE, V399, P533, DOI 10.1038/21092
   [崔光照 CUI Guangzhao], 2006, [计算机工程与应用, Computer Engineering and Application], V42, P29
   Deka G. C., 2013, P INT C COMP INT COM
   Devi D, 2020, INT J DATA WAREHOUS, V16, P60, DOI 10.4018/IJDWM.2020070104
   Enayatifar R, 2014, OPT LASER ENG, V56, P83, DOI 10.1016/j.optlaseng.2013.12.003
   Goyal V., 2006, P 2006 INT C PRIVACY, P1
   Huth A., 2011, BASICS CLOUD COMPUTI
   Isawa R., 2011, ONE TIME PASSWORD AU, P225
   Jagatic TN, 2007, COMMUN ACM, V50, P94, DOI 10.1145/1290958.1290968
   Leier A, 2000, BIOSYSTEMS, V57, P13, DOI 10.1016/S0303-2647(00)00083-6
   Li Gong, 1995, Proceedings. The Eighth IEEE Computer Security Foundations Workshop (Cat. No.95TB8076), P24, DOI 10.1109/CSFW.1995.518549
   Li S, 2019, CAAI T INTELL TECHNO, V4, P223, DOI 10.1049/trit.2019.0021
   Lu M, 2007, SCI CHINA SER F, V50, P324, DOI 10.1007/s11432-007-0025-6
   Mian R, 2013, FUTURE GENER COMP SY, V29, P1452, DOI 10.1016/j.future.2012.01.008
   Namasudra S., 2018, Advances of DNA computing in cryptography
   Namasudra S, 2018, ADV DNA COMPUTING CR, P153
   Namasudra S, 2020, INT J APPL RES BIOIN
   Namasudra S., 2018, Advances of DNA Computing in Cryptography, P181, DOI DOI 10.1201/9781351011419-9X
   Namasudra S, 2018, ADV DNA COMPUTING CR, P27
   Namasudra S, 2018, Advances of DNA Computing in Cryptography, P37, DOI DOI 10.1007/S12652-021-02942-2
   Namasudra S., 2018, Journal of Fundamental and Applied Sciences
   Namasudra S, 2021, ARCH COMPUT METHOD E, V28, P1497, DOI 10.1007/s11831-020-09426-0
   Namasudra S, 2020, COMPUT COMMUN, V151, P539, DOI 10.1016/j.comcom.2019.12.041
   Namasudra S, 2019, CONCURR COMP-PRACT E, V31, DOI 10.1002/cpe.4364
   Namasudra S, 2018, J ORGAN END USER COM, V30, P14, DOI 10.4018/JOEUC.2018100102
   Namasudra S, 2017, IET COMMUN, V11, P1558, DOI 10.1049/iet-com.2016.0777
   Namasudra S, 2017, J INF SCI ENG, V33, P585, DOI 10.6688/JISE.2017.33.3.1
   Namasudra S, 2016, MULTIAGENT GRID SYST, V12, P69, DOI 10.3233/MGS-160244
   Raghavendra S, 2016, PROCEDIA COMPUT SCI, V89, P293, DOI 10.1016/j.procs.2016.06.062
   Sarkar S., 2015, SSRG INT J COMPUT SC, V2, P18, DOI DOI 10.14445/23488387/IJCSE-V2I8P104
   Schramm K, 2003, LECT NOTES COMPUT SC, V2887, P206
   Shamir A., 1985, Advances in Cryptology, V84 4, P47, DOI 10.1007/3-540-39568-7_5
   Tanaka K, 2005, BIOSYSTEMS, V81, P25, DOI 10.1016/j.biosystems.2005.01.004
   Wang B, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7276084
   Wang YF, 2019, IEEE T NANOTECHNOL, V18, P299, DOI 10.1109/TNANO.2019.2904842
   Zhao XC, 2019, CAAI T INTELL TECHNO, V4, P159, DOI 10.1049/trit.2019.0018
NR 45
TC 51
Z9 51
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 3
SU S
AR 99
DI 10.1145/3392665
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2RB
UT WOS:000612586300007
DA 2024-07-18
ER

PT J
AU Huang, FR
   Wei, KM
   Weng, J
   Li, ZJ
AF Huang, Feiran
   Wei, Kaimin
   Weng, Jian
   Li, Zhoujun
TI Attention-Based Modality-Gated Networks for Image-Text Sentiment
   Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Sentiment analysis; attention; modality-gated; multimodal; deep learning
ID FUSION
AB Sentiment analysis of social multimedia data has attracted extensive research interest and has been applied to many tasks, such as election prediction and products evaluation. Sentiment analysis of one modality (e.g., text or image) has been broadly studied. However, not much attention has been paid to the sentiment analysis of multimodal data. Different modalities usually have information that is complementary. Thus, it is necessary to learn the overall sentiment by combining the visual content with text description. In this article, we propose a novel method-Attention-Based Modality-Gated Networks (AMGN)-to exploit the correlation between the modalities of images and texts and extract the discriminative features for multimodal sentiment analysis. Specifically, a visual-semantic attention model is proposed to learn attended visual features for each word. To effectively combine the sentiment information on the two modalities of image and text, a modalitygated LSTM is proposed to learn the multimodal features by adaptively selecting the modality that presents stronger sentiment information. Then a semantic self-attention model is proposed to automatically focus on the discriminative features for sentiment classification. Extensive experiments have been conducted on both manually annotated and machine weakly labeled datasets. The results demonstrate the superiority of our approach through comparison with state-of-the-art models.
C1 [Huang, Feiran; Wei, Kaimin; Weng, Jian] Jinan Univ, Coll Informat Sci & Technol, Coll Cyber Secur, 601 West Huangpu Ave, Guangzhou 510632, Peoples R China.
   [Li, Zhoujun] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Software Dev Environm, XueYuan Rd 37, Beijing 100191, Peoples R China.
C3 Jinan University; Beihang University
RP Wei, KM (corresponding author), Jinan Univ, Coll Informat Sci & Technol, Coll Cyber Secur, 601 West Huangpu Ave, Guangzhou 510632, Peoples R China.
EM huangfr@jnu.edu.cn; cswei@jnu.edu.cn; cryptjweng@gmail.com;
   lizj@buaa.edu.cn
OI Weng, Jian/0000-0003-4067-8230; Li, Zhoujun/0000-0002-9603-9713
FU National Key R&D Program of China [2018YFB1402600]; National Natural
   Science Foundation of China [61906075, 61932010, 61972178, 61825203,
   U1736203]; Natural Science Foundation of Guangdong Province, China
   [2019A1515011920, 2017A030313334, 2019A1515011753]; Guangdong Provincial
   Special Funds for Applied Technology Research and Development and
   Transformation of Important Scientific and Technological Achieve
   [2017B010124002]; Science and Technology Program of Guangzhou of China
   [201802010061]
FX This work was supported in part by the National Key R&D Program of China
   (2018YFB1402600), the National Natural Science Foundation of China
   (61906075, 61932010, 61972178, 61825203, U1736203), Natural Science
   Foundation of Guangdong Province, China (2019A1515011920,
   2017A030313334, 2019A1515011753), Guangdong Provincial Special Funds for
   Applied Technology Research and Development and Transformation of
   Important Scientific and Technological Achieve (2017B010124002), and
   Science and Technology Program of Guangzhou of China (201802010061).
CR Ahmad K, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3199668
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   [Anonymous], 2007, ICWSM 2007 INT C WEB
   Asghar MZ, 2018, EXPERT SYST, V35, DOI 10.1111/exsy.12233
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Cai Zheng, 2015, ARXIV150600765
   Cai ZL, 2014, INTERNATIONAL CONFERENCE ON COMPUTATIONAL AND INFORMATION SCIENCES (ICCIS 2014), P1246
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI 10.1145/3136755.3136801
   Chen YX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P117, DOI 10.1145/3240508.3240533
   Chiong R., 2018, P GENETIC EVOLUTIONA, P278, DOI DOI 10.1145/3205651.3205682
   Chung Jessica Elan, 2011, P 25 AAAI C ART INT
   Cui H., 2006, Proceedings of the AAAI conference on artificial intelligence, V6, P1265
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P350, DOI 10.1145/3219819.3219853
   Huang FR, 2019, KNOWL-BASED SYST, V167, P26, DOI 10.1016/j.knosys.2019.01.019
   Hutto C., 2014, P INT AAAI C WEB SOC, V8, P216, DOI [DOI 10.1609/ICWSM.V8I1.14550, 10.1609/icwsm.v8i1.14550]
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Jin Xin, 2010, P 18 ACM INT C MULT, P1235, DOI [10.1145/1873951.1874196, DOI 10.1145/1873951.1874196]
   Kim J, 2017, J SEMICOND TECH SCI, V17, P1, DOI 10.5573/JSTS.2017.17.1.001
   Kim Y, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808204
   Kingma D. P., 2014, arXiv
   Krishnamoorthy S, 2018, KNOWL INF SYST, V56, P373, DOI 10.1007/s10115-017-1134-1
   Li CZ, 2019, AAAI CONF ARTIF INTE, P996
   Li LH, 2017, AAAI CONF ARTIF INTE, P4133
   Li XD, 2014, KNOWL-BASED SYST, V69, P14, DOI 10.1016/j.knosys.2014.04.022
   Liu NN, 2013, COMPUT VIS IMAGE UND, V117, P493, DOI 10.1016/j.cviu.2012.10.009
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mao QR, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5139
   Mazloom M., 2016, P ACM MULT, P197, DOI [10.1145/2964284.2967210, 10]
   Morency L.-P., 2011, P 13 INT C MULT INT, P169, DOI DOI 10.1145/2070481.2070509
   Mun J, 2017, AAAI CONF ARTIF INTE, P4233
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Perez-Rosas V., 2013, P ANN M ASS COMP LIN, P973
   PORIA S, 2016, IEEE DATA MINING, P439, DOI [DOI 10.1109/ICDM.2016.178, DOI 10.1109/ICDM.2016.0055]
   Poria S., 2015, P 2015 C EMP METH NA, P2539, DOI DOI 10.18653/V1/D15-1303
   Poria S, 2017, IEEE DATA MINING, P1033, DOI 10.1109/ICDM.2017.134
   Qiao TT, 2018, AAAI CONF ARTIF INTE, P7300
   Truong QT, 2019, AAAI CONF ARTIF INTE, P305
   Truong QT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1274, DOI 10.1145/3123266.3123374
   Rao YH, 2014, WORLD WIDE WEB, V17, P723, DOI 10.1007/s11280-013-0221-9
   Remus Robert., 2013, Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), P450
   Severyn A, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P959, DOI 10.1145/2766462.2767830
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang Duyu., 2014, International Workshop on Semantic Evaluation, P208
   Tumasjan A, 2010, 4 INT AAAI C WEBL SO, V10, P178, DOI 10.1074/jbc.M501708200
   Verma S, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3627
   Wang H., 2012, P ACL 2012 SYST DEM, P115, DOI DOI 10.1145/1935826.1935854
   Wang X., 2011, Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM'11, P1031
   Wei W, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P404
   Wu J, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3271485
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xu C., 2014, Visual sentiment prediction with deep convolutional neural networks
   Xu J, 2019, KNOWL-BASED SYST, V178, P61, DOI 10.1016/j.knosys.2019.04.018
   Xu J, 2019, APPL SOFT COMPUT, V80, P387, DOI 10.1016/j.asoc.2019.04.010
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu N, 2019, AAAI CONF ARTIF INTE, P371
   Xu N, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2399, DOI 10.1145/3132847.3133142
   You QZ, 2017, AAAI CONF ARTIF INTE, P231
   You QZ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1008, DOI 10.1145/2964284.2964288
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zhang D, 2019, IEEE INT CON MULTI, P730, DOI 10.1109/ICME.2019.00131
   Zhang YX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4595
NR 65
TC 55
Z9 59
U1 8
U2 81
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 79
DI 10.1145/3388861
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200005
DA 2024-07-18
ER

PT J
AU Yang, L
   Hu, HF
   Xing, SL
   Lu, XL
AF Yang, Liang
   Hu, Haifeng
   Xing, Songlong
   Lu, Xinlong
TI Constrained LSTM and Residual Attention for Image Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image captioning; visual attention; visual skeleton; object detection;
   LSTM
AB Visual structure and syntactic structure are essential in images and texts, respectively. Visual structure depicts both entities in an image and their interactions, whereas syntactic structure in texts can reflect the part-of-speech constraints between adjacent words. Most existing methods either use visual global representation to guide the language model or generate captions without considering the relationships of different entities or adjacent words. Thus, their language models lack relevance in both visual and syntactic structure. To solve this problem, we propose a model that aligns the language model to certain visual structure and also constrains it with a specific part-of-speech template. In addition, most methods exploit the latent relationship betweenwords in a sentence and pre-extracted visual regions in an image yet ignore the effects of unextracted regions on predicted words. We develop a residual attention mechanism to simultaneously focus on the preextracted visual objects and unextracted regions in an image. Residual attention is capable of capturing precise regions of an image corresponding to the predicted words considering both the effects of visual objects and unextracted regions. The effectiveness of our entire framework and each proposed module are verified on two classical datasets: MSCOCO and Flickr30k. Our framework is on par with or even better than the stateof-the-art methods and achieves superior performance on COCO captioning Leaderboard.
C1 [Yang, Liang; Hu, Haifeng; Xing, Songlong; Lu, Xinlong] Sun Yat Sen Univ, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Guangzhou 510006, Guangdong, Peoples R China.
EM yangliang5@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn;
   xingslong@mail2.sysu.edu.cn; luxlong@mail2.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; National Key R&D Program of China [2018YFB1601101]; Natural
   Science Foundation of Guangdong [2017A030311029]; Science and Technology
   Program of Guangzhou [201704020180]; Fundamental Research Funds for the
   Central Universities of China [17lgzd08]
FX This work was supported in part by the National Natural Science
   Foundation of China under grants 61673402, 61273270, and 60802069; in
   part by the National Key R&D Program of China under Grant
   2018YFB1601101; in part by the Natural Science Foundation of Guangdong
   under grants 2017A030311029; in part by the Science and Technology
   Program of Guangzhou under grants 201704020180; and in part by the
   Fundamental Research Funds for the Central Universities of China under
   grant 17lgzd08.
CR Anderson Peter, 2018, P 2018 IEEE CVF C CO
   [Anonymous], 2014, Transactions of the Association for Computational Linguistics
   [Anonymous], ARXIV14114952
   [Anonymous], 2012, P C EUR CHAPT ASS CO
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bengio S., 2015, NIPS, DOI DOI 10.5555/2969239.2969370
   Chen D., 2014, P 2014 C EMPIRICAL M, P740, DOI DOI 10.3115/V1/D14-1082
   Chen Fuhai, 2018, P IEEE C COMP VIS PA
   Chen Shi, 2018, P EUR C COMP VIS ECC
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Choi Y, 2011, P 15 C COMPUTATIONAL, P220
   Defferrard Michael, 2016, ADV NEURAL INFORM PR, P3837, DOI DOI 10.5555/3157382.3157527
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Kipf TN, 2016, ARXIV
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kuznetsova Polina, 2012, P M ASS COMP LING LO
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XQ, 2018, LECT NOTES COMPUT SC, V11211, P381, DOI 10.1007/978-3-030-01234-2_23
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mao J, 2014, CELL DEATH DIS, V5, DOI 10.1038/cddis.2013.515
   Qin Y, 2019, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2019.00856
   Qiu ZF, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P225, DOI 10.1145/3077136.3080842
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang AQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3226037
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang L, 2019, NEURAL PROCESS LETT, V50, P549, DOI 10.1007/s11063-019-10045-5
   Yang L, 2017, ELECTRON LETT, V53, P1471, DOI 10.1049/el.2017.2351
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yang Y., 2011, P C EMP METH NAT LAN, P444
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Zheng Yue, 2019, P IEEE C COMP VIS PA
NR 45
TC 25
Z9 28
U1 0
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 75
DI 10.1145/3386725
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200001
DA 2024-07-18
ER

PT J
AU Zeng, DH
   Yu, Y
   Oyama, K
AF Zeng, Donghuo
   Yu, Yi
   Oyama, Keizo
TI Deep Triplet Neural Networks with Cluster-CCA for Audio-Visual
   Cross-Modal Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep triplet neural networks; cluster-CCA; cross-modal retrieval;
   triplet loss
AB Cross-modal retrieval aims to retrieve data in one modality by a query in another modality, which has been a very interesting research issue in the field of multimedia, information retrieval, and computer vision, and database. Most existing works focus on cross-modal retrieval between text-image, text-video, and lyricsaudio. Little research addresses cross-modal retrieval between audio and video due to limited audio-video paired datasets and semantic information. The main challenge of the audio-visual cross-modal retrieval task focuses on learning joint embeddings from a shared subspace for computing the similarity across different modalities, where generating new representations is to maximize the correlation between audio and visual modalities space. In this work, we propose TNN-C-CCA, a novel deep triplet neural network with cluster canonical correlation analysis, which is an end-to-end supervised learning architecture with an audio branch and a video branch. We not only consider the matching pairs in the common space but also compute the mismatching pairs when maximizing the correlation. In particular, two significant contributions are made. First, a better representation by constructing a deep triplet neural network with triplet loss for optimal projections can be generated to maximize correlation in the shared subspace. Second, positive examples and negative examples are used in the learning stage to improve the capability of embedding learning between audio and video. Our experiment is run over fivefold cross validation, where average performance is applied to demonstrate the performance of audio-video cross-modal retrieval. The experimental results achieved on two different audio-visual datasets show that the proposed learning architecture with two branches outperforms existing six canonical correlation analysis-based methods and four state-of-the-art-based cross-modal retrieval methods.
C1 [Zeng, Donghuo; Yu, Yi; Oyama, Keizo] SOKENDAI, Natl Inst Informat, 2 Chome 1-2 Hitotsubashi, Tokyo 1018430, Japan.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; Graduate University for Advanced
   Studies - Japan
RP Zeng, DH (corresponding author), SOKENDAI, Natl Inst Informat, 2 Chome 1-2 Hitotsubashi, Tokyo 1018430, Japan.
EM zengdonghuo@nii.ac.jp; yiyu@nii.ac.jp; oyama@nii.ac.jp
RI Zeng, Donghuo/GYJ-5363-2022
OI Zeng, Donghuo/0000-0002-6425-6270
FU JSPS [19K11987]; Grants-in-Aid for Scientific Research [19K11987]
   Funding Source: KAKEN
FX This work was supported by JSPS Grant-in-Aid for Scientific Research (C)
   under grant 19K11987.
CR Abu-El-Haija Sami, 2016, arXiv
   Andrew G., 2013, ICML, P1247
   [Anonymous], 2011, NIPS
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247923
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Feng FX, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808205
   Gu W, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/3323873.3325045
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   He L, 2017, IEEE INT CON MULTI, P1153, DOI 10.1109/ICME.2017.8019549
   Hermans Alexander, 2017, ARXIV170307737
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Irie G, 2015, IEEE I CONF COMP VIS, P1886, DOI 10.1109/ICCV.2015.219
   Ji YL, 2019, NEUROCOMPUTING, V333, P231, DOI 10.1016/j.neucom.2018.12.037
   Karpathy Andrej, 2014, Advances in neural information processing systems, P1889
   Lai PL, 2000, IEEE IJCNN, P614
   Li D., 2003, P 11 ACM INT C MULTI, P604
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P5585, DOI 10.1109/TIP.2018.2852503
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rasiwasia N, 2014, JMLR WORKSH CONF PRO, V33, P823
   Rastegari Mohammad, 2013, Book Predictable Dual-View Hashing
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shah RR, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P607, DOI 10.1145/2647868.2654919
   Shen C, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3309881
   Shi XX, 2012, IEEE DATA MINING, P635, DOI 10.1109/ICDM.2012.30
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   SOCHER R, 2010, PROC CVPR IEEE, P966, DOI DOI 10.1109/CVPR.2010.5540112
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   van der Spoel E, 2015, ICML DEEP LEARNING W, V7, P956, DOI 10.1017/CBO9781107415324
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang C, 2015, PROC INT C TOOLS ART, P234, DOI 10.1109/ICTAI.2015.45
   Wang D, 2016, IEEE T IMAGE PROCESS, V25, P4540, DOI 10.1109/TIP.2016.2592800
   Wang J, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P347, DOI 10.1145/2671188.2749341
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang ZC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3340262
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Xu C., 2013, Neural Computing and Applications, V23, P2031, DOI DOI 10.1007/S00521-013-1362-6
   XU X, 2016, NEUROCOMPUTING, V213, P191, DOI DOI 10.1016/J.NEUC0M.2015.11.133
   Xu XL, 2020, IEEE T IND INFORM, V16, P6172, DOI 10.1109/TII.2019.2959258
   Xu X, 2019, WORLD WIDE WEB, V22, P657, DOI 10.1007/s11280-018-0541-x
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yao T, 2015, IEEE I CONF COMP VIS, P28, DOI 10.1109/ICCV.2015.12
   YU Y, 2018, IEEE T NEURAL NETWOR, V3, P1, DOI DOI 10.1016/J.GEE.2018.01.002
   Yu Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3281746
   Zeng DH, 2018, IEEE INT SYM MULTIM, P143, DOI 10.1109/ISM.2018.00-21
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zheng Y, 2014, PROC CVPR IEEE, P1370, DOI 10.1109/CVPR.2014.178
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
   Zhu LG, 2019, IRONMAK STEELMAK, V46, P499, DOI 10.1080/03019233.2017.1405153
NR 60
TC 22
Z9 24
U1 3
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 76
DI 10.1145/3387164
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, YG
   Liu, CP
   Ji, Y
   Gong, SR
   Xu, HB
AF Li, Yonggang
   Liu, Chunping
   Ji, Yi
   Gong, Shengrong
   Xu, Haibao
TI Spatio-Temporal Deep Residual Network with Hierarchical Attentions for
   Video Event Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Event recognition; hierarchical attention; surveillance video; deep
   residual recurrent network; spatio-temporal
ID REPRESENTATION
AB Event recognition in surveillance video has gained extensive attention from the computer vision community. This process still faces enormous challenges due to the tiny inter-class variations that are caused by various facets, such as severe occlusion, cluttered backgrounds, and so forth. To address these issues, we propose a spatio-temporal deep residual network with hierarchical attentions (STDRN-HA) for video event recognition. In the first attention layer, the ResNet fully connected feature guides the Faster R-CNN feature to generate object-based attention (O-attention) for target objects. In the second attention layer, the O-attention further guides the ResNet convolutional feature to yield the holistic attention (H-attention) in order to perceive more details of the occluded objects and the global background. In the third attention layer, the attention maps use the deep features to obtain the attention-enhanced features. Then, the attention-enhanced features are input into a deep residual recurrent network, which is used to mine more event clues from videos. Furthermore, an optimized loss function named softmax-RC is designed, which embeds the residual block regularization and center loss to solve the vanishing gradient in a deep network and enlarge the distance between inter-classes. We also build a temporal branch to exploit the long- and short-term motion information. The final results are obtained by fusing the outputs of the spatial and temporal streams. Experiments on the four realistic video datasets, CCV, VIRAT 1.0, VIRAT 2.0, and HMDB51, demonstrate that the proposed method has good performance and achieves state-of-the-art results.
C1 [Li, Yonggang] Jiaxing Univ, Coll Math Phys & Informat Engn, 118 Pahang Rd, Jiaxing 314001, Peoples R China.
   [Liu, Chunping; Ji, Yi; Gong, Shengrong] Soochow Univ, Sch Comp Sci & Technol, 1 Shizi St, Suzhou 215009, Peoples R China.
   [Gong, Shengrong] Changshu Inst Sci & Technol, Sch Comp Sci & Engn, 99 Hushan Rd, Suzhou 215500, Peoples R China.
   [Gong, Shengrong] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing, Peoples R China.
   [Xu, Haibao] Zhejiang Univ, 269 Shixiang Rd, Hangzhou 310015, Peoples R China.
C3 Jiaxing University; Soochow University - China; Beijing Jiaotong
   University; Zhejiang University
RP Li, YG (corresponding author), Jiaxing Univ, Coll Math Phys & Informat Engn, 118 Pahang Rd, Jiaxing 314001, Peoples R China.
EM lyg_gang@163.com; cpliu@suda.edu.cn; jiyi@suda.edu.cn;
   shrgong@cslg.edu.cn; 21860635@zju.edu.cn
OI Li, Yonggang/0000-0002-7269-0368
FU National Natural Science Foundation of China (NSFC) [61972059, 61702055,
   61773272]; Provincial Natural Science Foundation of Zhejiang
   [Y19F020071, Y18F020084]; Natural Science Foundation of Jiangsu Province
   [BK20191474]; Natural Science Foundation of the Jiangsu Higher Education
   Institutions of China [19KJA230001]; Key Laboratory of Symbolic
   Computation and Knowledge Engineering of Ministry of Education, Jilin
   University [93K172016K08]; Jiaxing Science and Technology Project
   [2020AY10024]; Priority Academic Program Development of Jiangsu Higher
   Education Institutions
FX This work was partially supported by the National Natural Science
   Foundation of China (NSFC Grants No. 61972059, No. 61702055, and No.
   61773272), Provincial Natural Science Foundation of Zhejiang (Grants No.
   Y19F020071 and Ni, Y18F020084), Natural Science Foundation of Jiangsu
   Province (Grant No. BK20191474), the Natural Science Foundation of the
   Jiangsu Higher Education Institutions of China (Grant No. 19KJA230001),
   Key Laboratory of Symbolic Computation and Knowledge Engineering of
   Ministry of Education, Jilin University (Grant No. 93K172016K08),
   Jiaxing Science and Technology Project (Grant No. 2020AY10024), and the
   Priority Academic Program Development of Jiangsu Higher Education
   Institutions.
CR Amer MR, 2012, PROC CVPR IEEE, P1314, DOI 10.1109/CVPR.2012.6247816
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2017, IEEE C COMP VIS PATT
   [Anonymous], 2018, AAAI
   [Anonymous], 2007, CIVR '07
   [Anonymous], 2015, COMPUTER VISION PATT
   [Anonymous], 2017, IEEE C COMP VIS PATT
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90
   [Anonymous], 2017, CVPR
   [Anonymous], 2017, IEEE C COMP VIS PATT
   [Anonymous], 2017, NEW MODEL KINETICS D
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Cong JQ, 2017, ASIA-PAC CONF COMMUN, P236
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C., 2016, P INT C NEUR INF PRO, P3468
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Feichtenhofer Christoph, 2017, P IEEE C COMP VIS PA, P4768
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Guan XJ, 2018, IEEE CONF COMPUT
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Hori C, 2019, INT CONF ACOUST SPEE, P2352, DOI 10.1109/ICASSP.2019.8682583
   Hou JY, 2016, IEEE INT CON MULTI
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Ji Z, 2020, IEEE T CIRC SYST VID, V30, P1709, DOI 10.1109/TCSVT.2019.2904996
   Jiang Y., 2011, P ACM INT C MULT RET
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Kumaran N, 2018, MULTIMED TOOLS APPL, V77, P23115, DOI 10.1007/s11042-017-5591-z
   Lee I., 2017, ICCV
   Li C, 2017, IEEE I CONF COMP VIS, P3667, DOI 10.1109/ICCV.2017.394
   Li C, 2017, IEEE T IMAGE PROCESS, V26, P2149, DOI 10.1109/TIP.2017.2670782
   Li YN, 2018, DISCRETE DYN NAT SOC, V2018, DOI 10.1155/2018/4945728
   Li YK, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P1
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Liu Jun, 2017, IEEE C COMP VIS PATT
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Lu P, 2018, AAAI CONF ARTIF INTE, P7218
   Mahmud T, 2017, IEEE I CONF COMP VIS, P5784, DOI 10.1109/ICCV.2017.616
   Mnih V., 2014, Neural Information Processing Systems, P2204
   Oh SM, 2011, PROC CVPR IEEE
   Paul Sujoy, 2017, IEEE C COMP VIS PATT, P3138
   Peng YX, 2019, IEEE T CIRC SYST VID, V29, P773, DOI 10.1109/TCSVT.2018.2808685
   Ramanathan V, 2015, IEEE I CONF COMP VIS, P4471, DOI 10.1109/ICCV.2015.508
   Ren S, 2015, NIPS, P91, DOI DOI 10.1109/TPAMI.2016.2577031
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sharma Shikhar, 2016, 2016 ACTION RECOGNIT
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Simonyan Karen., 2014, Advances in neural information processing systems, P568
   Soltanian M, 2019, IEEE T MULTIMEDIA, V21, P157, DOI 10.1109/TMM.2018.2844101
   Tan ZX, 2018, AAAI CONF ARTIF INTE, P4929
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P ECCV
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang XY, 2017, IEEE T PATTERN ANAL, V39, P1770, DOI 10.1109/TPAMI.2016.2616308
   Wang XY, 2014, PROC CVPR IEEE, P2561, DOI 10.1109/CVPR.2014.328
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Xian Y, 2017, IEEE T CIRC SYST VID, V27, P624, DOI 10.1109/TCSVT.2016.2589838
   Xie WL, 2019, IEEE T MULTIMEDIA, V21, P1425, DOI 10.1109/TMM.2018.2879749
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yi Zhu, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P668, DOI 10.1007/978-3-319-46604-0_47
   Yu TZ, 2019, IEEE T MULTIMEDIA, V21, P2504, DOI 10.1109/TMM.2019.2907060
   Zhang H, 2019, IEEE T MULTIMEDIA, V21, P1450, DOI 10.1109/TMM.2018.2884478
   Zhang SY, 2020, IEEE T CIRC SYST VID, V30, P1051, DOI 10.1109/TCSVT.2019.2902268
   Zhu YY, 2015, IEEE T PATTERN ANAL, V37, P1360, DOI 10.1109/TPAMI.2014.2369044
   Zhu YY, 2013, PROC CVPR IEEE, P2491, DOI 10.1109/CVPR.2013.322
NR 68
TC 12
Z9 14
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 62
DI 10.1145/3378026
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FH
UT WOS:000583712600005
DA 2024-07-18
ER

PT J
AU Yadav, S
   Ramteke, P
   Ekbal, A
   Saha, S
   Bhattacharyya, P
AF Yadav, Shweta
   Ramteke, Pralay
   Ekbal, Asif
   Saha, Sriparna
   Bhattacharyya, Pushpak
TI Exploring Disorder-Aware Attention for Clinical Event Extraction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Neural networks; event extraction; temporal event extraction; attention;
   clinical event extraction; social media
ID TEMPORAL INFORMATION; HEALTH; SUPPORT; TEXT
AB Event extraction is one of the crucial tasks in biomedical text mining that aims to extract specific information concerning incidents embedded in the texts. In this article, we propose a deep learning framework that aims to identify the attributes (severity, course, temporal expression, and document creation time) associated with the medical concepts extracted from electronic medical records. The bi-directional long short-term memory network assisted by the attention mechanism is utilized to uncover the important aspects of the patient's medical conditions. The attention mechanism specific to the medical disorder mention can focus on various parts of the sentence when different disorders are considered as input. The proposed methodology is evaluated on benchmark ShARe/CLEF eHealth Evaluation Lab 2014 shared task 2 datasets. In addition to the CLEF dataset, we also used the social media text, especially the medical blog posts. Experimental results of the proposed approach illustrate that our proposed approach achieves significant performance improvements over the state-of-the-art techniques and the highly competitive deep learning-based baseline methods.
C1 [Yadav, Shweta] Wright State Univ, Dept Comp Sci & Engn, 379 Joshi Res Ctr, Dayton, OH 45435 USA.
   [Ramteke, Pralay; Ekbal, Asif; Saha, Sriparna; Bhattacharyya, Pushpak] Indian Inst Technol Patna, Dept Comp Sci & Engn, 510 AI NLP Ml Grp, Bihta 801103, Bihar, India.
C3 University System of Ohio; Wright State University Dayton; Indian
   Institute of Technology (IIT) - Patna
RP Yadav, S (corresponding author), Wright State Univ, Dept Comp Sci & Engn, 379 Joshi Res Ctr, Dayton, OH 45435 USA.
EM shweta@knoesis.org; pralay.cs14@iitp.ac.in; asif@iitp.ac.in;
   sriparna@iitp.ac.in; pb@iitp.ac.in
RI Yadav, Shweta/HWQ-6638-2023; Ekbal, Asif/JKI-7638-2023
OI Yadav, Shweta/0000-0001-7699-8098; 
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Amir Silvio, 2019, P 6 WORKSH COMP LING, P114
   [Anonymous], 2013, PREPRINT ARXIV 1308
   [Anonymous], 2015, Reasoning about entailment with neural attention
   [Anonymous], 2011, SOCIAL LIFE HLTH INF
   [Anonymous], 2014, EMNLP
   [Anonymous], 2019, ARXIV190108746
   [Anonymous], 2017, C HUMAN FACTORS COMP, DOI [DOI 10.1145/3027063.3051141, 10.1145/3027063.3051141]
   [Anonymous], 2017, J MED INTERNET RES
   Azar Mahmood Yousefi, 2015, P AUSTRALASIAN LANGU, P2
   Bahdanau Dzmitry, 2014, NEURAL MACHINE TRANS
   Bethard Steven., 2015, P 9 INT WORKSHOP SEM, P806
   Bethard Steven, 2016, P 10 INT WORKSH SEM, P1052
   Biswas A, 2018, PROCEEDINGS OF IN SITU INFRASTRUCTURES FOR ENABLING EXTREME-SCALE ANALYSIS AND VISUALIZATION (ISAV 2018), P13, DOI 10.1145/3281464.3281467
   Chen Q, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P993, DOI 10.1145/3077136.3080699
   Chiu J.P., 2016, Named entity recognition with bidirectional lstm-cnns, V4, P357, DOI 10.1162/tacl_a_00104
   Combi C, 1997, COMPUT BIOL MED, V27, P353, DOI 10.1016/S0010-4825(96)00010-8
   Coppersmith  G., 2015, P 2 WORKSH COMP LING, P31, DOI [10.3115/v1/W15-1204, DOI 10.3115/V1/W15-1204]
   D'Souza J, 2014, DATABASE-OXFORD, DOI 10.1093/database/bau109
   De Choudhury M, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P353, DOI 10.1145/2998181.2998220
   De Choudhury M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P2098, DOI 10.1145/2858036.2858207
   Devlin J., 2018, BERT PRE TRAINING DE
   Dredze M, 2014, IEEE INTELL SYST, V29, P64
   Elhadad N., 2015, P 9 INT WORKSH SEM E, P303, DOI [10.18653/v1/S15-2051, DOI 10.18653/V1/S15-2051]
   Ernala Sindhu Kiranmai, 2018, P INT AAAI C WEB SOC
   Fernandez-Luque L, 2011, J MED INTERNET RES, V13, P138, DOI 10.2196/jmir.1432
   Gupta D., 2018, P 22 C COMPUTATIONAL, P119, DOI DOI 10.18653/V1/K18-1012
   Gupta Deepak, 2018, ARXIV180801650
   Gupta Deepak, 2018, LREC
   Gupta Deepak, 2018, COLING 2018, P499
   Hamon Thierry, 2014, CLEF 2014 C, P79
   Herbst Konrad, 2014, CLEF 2014 C, P91
   Hripcsak G, 2009, J AM MED INFORM ASSN, V16, P354, DOI 10.1197/jamia.M2922
   Johan Hasselqvist Niklas, 2017, ARXIV171206100
   Johri Nishikant, 2014, P C LABS EV FOR CLEF
   Kingma D. P., 2014, arXiv
   Kummervold PE, 2002, NORD J PSYCHIAT, V56, P59, DOI 10.1080/08039480252803945
   Lin C, 2016, J AM MED INFORM ASSN, V23, P387, DOI 10.1093/jamia/ocv113
   Liu Yang, 2014, AMIA Annu Symp Proc, V2014, P1825
   Long Y, 2017, EUR J INORG CHEM, P146, DOI 10.1002/ejic.201601034
   MacAvaney Sean, 2017, P 11 INT WORKSHOP SE, P1024
   Maitra A, 2018, LECT NOTES ARTIF INT, V11107, P267, DOI 10.1007/978-3-030-00794-2_29
   Mkrtchyan Tigran, 2014, CEUR WORKSHOP P, V1180, P138
   Mowery Danielle L., 2014, P C LABS EV FOR CLEF
   Nema P, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1063, DOI 10.18653/v1/P17-1098
   Nikfarjam A, 2015, J AM MED INFORM ASSN, V22, P671, DOI 10.1093/jamia/ocu041
   Ningthoujam Dhanachandra, 2019, ARXIV190309941
   Pathak P., 2015, Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), P412, DOI DOI 10.18653/V1/S15-2071
   Pike J, 2014, MORAL GEOGRAPHIES OF CHILDREN, YOUNG PEOPLE AND FOOD BEYOND: JAMIE'S SCHOOL DINNERS, P172
   Pustejovsky James, 2011, Proceedings of the 5th Linguistic Annotation Workshop, P152
   Ramanan S. V., 2014, CLEF 2014 C
   Saeed M, 2002, COMPUT CARDIOL, V29, P641, DOI 10.1109/CIC.2002.1166854
   Seo M., 2016, Bidirectional attention flow for machine comprehension
   Sharma Eva, 2018, P 2018 CHI C HUM FAC, P641
   Sun WY, 2013, J BIOMED INFORM, V46, pS5, DOI 10.1016/j.jbi.2013.07.004
   Sun WY, 2013, J AM MED INFORM ASSN, V20, P814, DOI 10.1136/amiajnl-2013-001760
   Sun WY, 2013, J AM MED INFORM ASSN, V20, P806, DOI 10.1136/amiajnl-2013-001628
   Sutskever I, 2014, ADV NEUR IN, V27
   Tan Chuanqi, 2017, CoRR
   Tan Ming, 2015, Lstm-based deep learning models for non-factoid answer selection
   Tang BZ, 2013, J AM MED INFORM ASSN, V20, P828, DOI 10.1136/amiajnl-2013-001635
   Wang Y., 2016, P 2016 C EMPIRICAL M, P606, DOI 10.18653/v1/D16-1058
   Xu J, 2015, P 9 INT WORKSH SEM E, P311, DOI DOI 10.18653/V1/S15-2052
   Yadav S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5234
   Yadav S, 2019, KNOWL-BASED SYST, V166, P18, DOI 10.1016/j.knosys.2018.11.020
   Yadav Shweta., 2018, P 2018 C N AM CHAPT, V2, P271
   Yadav Shweta, 2018, P 11 INT C LANG RES
   Yadav Shweta, 2018, ARXIV180711172
   Zhou Li, 2005, AMIA Annu Symp Proc, P869
   Zhou Peng., 2016, Text Classification Improved by Integrating Bidirectional LSTM with Two-Dimensional Max Pooling
NR 70
TC 5
Z9 5
U1 2
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 31
DI 10.1145/3372328
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA OJ1EL
UT WOS:000583710300013
DA 2024-07-18
ER

PT J
AU Zhuang, YT
   Xu, DJ
   Yan, X
   Cheng, WZ
   Zhao, Z
   Pu, SL
   Xiao, J
AF Zhuang, Yueting
   Xu, Dejing
   Yan, Xin
   Cheng, Wenzhuo
   Zhao, Zhou
   Pu, Shiliang
   Xiao, Jun
TI Multichannel Attention Refinement for Video Question Answering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video question answering; attention mechanism; multichannel
AB Video Question Answering (VideoQA) is the extension of image question answering (ImageQA) in the video domain. Methods are required to give the correct answer after analyzing the provided video and question in this task. Comparing to ImageQA, the most distinctive part is the media type. Both tasks require the understanding of visual media, but VideoQA is much more challenging, mainly because of the complexity and diversity of videos. Particularly, working with the video needs to model its inherent temporal structure and analyze the diverse information it contains. In this article, we propose to tackle the task from a multichannel perspective. Appearance, motion, and audio features are extracted from the video, and question-guided attentions are refined to generate the expressive clues that support the correct answer. We also incorporate the relevant text information acquired from Wikipedia as an attempt to extend the capability of the method. Experiments on TGIF-QA and ActivityNet-QA datasets show the advantages of our method compared to existing methods. We also demonstrate the effectiveness and interpretability of our method by analyzing the refined attention weights during the question-answering procedure.
C1 [Zhuang, Yueting; Xu, Dejing; Yan, Xin; Cheng, Wenzhuo; Zhao, Zhou; Xiao, Jun] Zhejiang Univ, Hangzhou, Peoples R China.
   [Pu, Shiliang] Hikvis Res Inst, Hangzhou, Peoples R China.
C3 Zhejiang University
RP Xiao, J (corresponding author), Zhejiang Univ, Hangzhou, Peoples R China.
EM yzhuang@zju.edu.cn; xudejing@zju.edu.cn; yan_xin@zju.edu.cn;
   chengwenzhuo@zju.edu.cn; zhaozhou@zju.edu.cn; pushiliang@hikvision.com;
   junx@zju.edu.cn
RI zhao, zhao/JAC-1686-2023; Zhao, zhuo/JYO-7894-2024; Xu,
   Dejing/AAA-2793-2021
OI Xu, Dejing/0000-0003-3404-1305
FU National Key Research and Development Program of China [2018AAA0101903];
   Zhejiang Natural Science Foundation [LR19F020002, LZ17F020001]; National
   Natural Science Foundation of China [61976185, 61572431, U19B200023];
   Fundamental Research Funds for the Central Universities, Chinese
   Knowledge Center for Engineering Sciences and Technology; Joint Research
   Program of ZJU & Hikvision Research Institute
FX This work was supported by the National Key Research and Development
   Program of China (2018AAA0101903), Zhejiang Natural Science Foundation
   (LR19F020002, LZ17F020001), National Natural Science Foundation of China
   (61976185, 61572431, U19B200023), the Fundamental Research Funds for the
   Central Universities, Chinese Knowledge Center for Engineering Sciences
   and Technology, and Joint Research Program of ZJU & Hikvision Research
   Institute.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2017, P 31 AAAI C ART INT
   [Anonymous], 2013, P INT ACM C MULTIMED, DOI DOI 10.1145/2502081.2502093
   [Anonymous], COMPUT VIS PATTERN R
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Jang Y, 2017, IEEE INT CONF COMP V, P1581, DOI 10.1109/ICCVW.2017.186
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   LAU JH, 2016, P 1 WORKSH REPR LEAR, P78, DOI DOI 10.18653/V1/W16-1609
   Le QV, 2011, PROC CVPR IEEE
   Li YC, 2016, PROC CVPR IEEE, P4641, DOI 10.1109/CVPR.2016.502
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Le Q, 2014, PR MACH LEARN RES, V32, P1188
   Ren Mengye, 2015, NIPS, P2953
   Roach MJ, 2001, INT CONF ACOUST SPEE, P1557, DOI 10.1109/ICASSP.2001.941230
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Shrivastava A, 2016, PROCEEDINGS OF THE 10TH INDIACOM - 2016 3RD INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT, P2378
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang M, 2017, IEEE T KNOWL DATA EN, V29, P1101, DOI 10.1109/TKDE.2017.2654445
   Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xu D., 2019, P IEEE C COMP VIS PA
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xue HY, 2018, IEEE T IMAGE PROCESS, V27, P5563, DOI 10.1109/TIP.2018.2859820
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P5656, DOI 10.1109/TIP.2017.2746267
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Yu Zhou, 2019, P 33 AAAI C ART INT
   Zanfir M, 2016, P AS C COMP VIS
   Zhang H., 2017, Journal of Ophthalmology, V2017, P1, DOI DOI 10.1155/2017/4513410
   Zhang SP, 2014, INFORM SCIENCES, V281, P635, DOI 10.1016/j.ins.2013.12.052
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhao Z, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3518
   Zhu LG, 2019, IRONMAK STEELMAK, V46, P499, DOI 10.1080/03019233.2017.1405153
NR 67
TC 12
Z9 12
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 24
DI 10.1145/3366710
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300006
DA 2024-07-18
ER

PT J
AU Shen, L
   Hong, R
   Zhang, HR
   Tian, XM
   Wang, M
AF Shen, Ling
   Hong, Richang
   Zhang, Haoran
   Tian, Xinmei
   Wang, Meng
TI Video Retrieval with Similarity-Preserving Deep Temporal Hashing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Content-based video retrieval; video hashing; convolutional neural
   network; recurrent neural network
ID QUANTIZATION
AB Despite the fact that remarkable progress has been made in recent years, Content-based Video Retrieval (CBVR) is still an appealing research topic due to increasing search demands in the Internet era of big data. This article aims to explore an efficient CBVR system by discriminately hashing videos into short binary codes. Existing video hashing methods usually encounter two weaknesses originating from the following sources: (1) Most works adopt the separated stages method or the frame-pooling based end-to-end architecture. However, the spatial-temporal properties of videos cannot be fully explored or kept well in the follow-up hashing step. (2) Discriminative learning based on pairwise or triplet constraints often suffers from slow convergence and poor local optimization, mainly because of the limited samples for each update. To alleviate these problems, we propose an end-to-end video retrieval framework called the Similarity-Preserving Deep Temporal Hashing (SPDTH) network. Specifically, we equip the model with the ability to capture spatial-temporal properties of videos and to generate binary codes by stacked Gated Recurrent Units (GRUs). It unifies video temporal modeling and learning to hash into one step to allow for maximum retention of information. We also introduce a deep metric learning objective called l(2)All_loss for network training by preserving intraclass similarity and inter-class separability, and a quantization loss between the real-valued outputs and the binary codes is minimized. Extensive experiments on several challenging datasets demonstrate that SPDTH can consistently outperform state-of-the-art methods.
C1 [Shen, Ling; Hong, Richang; Zhang, Haoran; Wang, Meng] Hefei Univ Technol, 485 Danxia Rd, Hefei 230601, Anhui, Peoples R China.
   [Tian, Xinmei] Univ Sci & Technol China, 96 Jinzhai Rd, Hefei, Anhui, Peoples R China.
C3 Hefei University of Technology; Chinese Academy of Sciences; University
   of Science & Technology of China, CAS
RP Hong, R (corresponding author), Hefei Univ Technol, 485 Danxia Rd, Hefei 230601, Anhui, Peoples R China.
EM sammiling315@gmail.com; hongrc.hfut@gmail.com; lmc.haoran.edu@gmail.com;
   xinmei@ustc.edu.cn; eric.mengwang@gmail.com
RI Zhang, Haoran/M-2665-2019; Wang, Meng/ITR-8699-2023
OI Zhang, Haoran/0000-0002-4641-0641; 
FU National Natural Science Foundation of China [61722204, 61732007,
   61632007]
FX The work is supported by the National Natural Science Foundation of
   China under grant nos. 61722204, 61732007 and 61632007.
CR [Anonymous], 2015, P CVPR BOST MA US
   [Anonymous], 2011, P 1 INT C MULT RETR
   [Anonymous], 2010, P 18 ACM INT C MULTI, DOI [DOI 10.1145/1873951.1874033, 10.1145/1873951.1874033]
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2016, IEEE INT CON MULTI
   [Anonymous], 2017, ARXIV170200758
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2009, P SIGMM WORKSH SOC M
   [Anonymous], 2012, ARXIV12120402
   Cao Liangliang., 2012, Proceedings of the 20th ACM International Conference on Multimedia, P299
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Gupta A, 1997, COMMUN ACM, V40, P70, DOI 10.1145/253769.253798
   Hao YB, 2017, IEEE T IMAGE PROCESS, V26, P5531, DOI 10.1109/TIP.2017.2737329
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Jin ZM, 2014, IEEE T CYBERNETICS, V44, P1362, DOI 10.1109/TCYB.2013.2283497
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Li Q, 2017, ADV NEUR IN, V30
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang X., 2016, AS C COMP VIS ACCV
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yang X, 2019, INT J NEUROSCI, V129, P101, DOI 10.1080/00207454.2018.1486305
   Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282
   Zhang HW, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P781, DOI 10.1145/2964284.2964308
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 43
TC 18
Z9 21
U1 2
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 109
DI 10.1145/3356316
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800009
DA 2024-07-18
ER

PT J
AU Yue, GH
   Hou, CP
   Zhou, TW
AF Yue, Guanghui
   Hou, Chunping
   Zhou, Tianwei
TI Subtitle Region Selection of S3D Images in Consideration of Visual
   Discomfort and Viewing Habit
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Stereoscopic 3D; visual discomfort; subtitle; horizontal disparity;
   evaluation; just noticeable depth difference (JNDD)
ID DEPTH SENSATION ENHANCEMENT; QUALITY ASSESSMENT; COMFORT; VIDEO
AB Subtitles, serving as a linguistic approximation of the visual content, are an essential element in stereoscopic advertisement and the film industry. Due to the vergence accommodation conflict, the stereoscopic 3D (S3D) subtitle inevitably causes visual discomfort. To meet the viewing experience, the subtitle region should be carefully arranged. Unfortunately, very few works have been dedicated to this area. In this article, we propose a method for S3D subtitle region selection in consideration of visual discomfort and viewing habit. First, we divide the disparity map into multiple depth layers according to the disparity value. The preferential processed depth layer is determined by considering the disparity value of the foremost object. Second, the optimal region and coarse disparity value for S3D subtitle insertion are chosen by convolving the selective depth layer with the mean filter. Specifically, the viewing habit is considered during the region selection. Finally, after region selection, the disparity value of the subtitle is further modified by using the just noticeable depth difference (JNDD) model. Given that there is no public database reported for the evaluation of S3D subtitle insertion, we collect 120 S3D images as the test platform. Both objective and subjective experiments are conducted to evaluate the comfort degree of the inserted subtitle. Experimental results demonstrate that the proposed method can obtain promising performance in improving the viewing experience of the inserted subtitle.
C1 [Yue, Guanghui] Shenzhen Univ, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasoun, Sch Biomed Engn,Hlth Sci Ctr, Shenzhen 518060, Peoples R China.
   [Hou, Chunping] Tianjin Univ, Weijin Rd, Tianjin 300072, Peoples R China.
   [Zhou, Tianwei] Shenzhen Univ, Coll Management, Shenzhen 518060, Peoples R China.
C3 Shenzhen University; Tianjin University; Shenzhen University
RP Hou, CP (corresponding author), Tianjin Univ, Weijin Rd, Tianjin 300072, Peoples R China.; Zhou, TW (corresponding author), Shenzhen Univ, Coll Management, Shenzhen 518060, Peoples R China.
EM guanghuiyue.doctor@gmail.com; hcp@tju.edu.cn; tianweizhoudr@gmail.com
RI Zhou, Tianwei/GSI-8460-2022
FU National Natural Science Foundation of China [61731003, 61520106002,
   61871274, 61801305, 81571758]; National Natural Science Foundation of
   Guangdong Province [2017A030313377, 2016A030313047]; Shenzhen Peacock
   Plan; Shenzhen Key Basic Research Project [JCYJ20170818142347251,
   JCYJ20170818094109846]
FX This work was supported by the National Natural Science Foundation of
   China under Grants (Nos. 61731003, 61520106002, 61871274, 61801305, and
   81571758), National Natural Science Foundation of Guangdong Province
   (Nos. 2017A030313377 and 2016A030313047), Shenzhen Peacock Plan
   (No.KQTD2016053112051497), and Shenzhen Key Basic Research Project (Nos.
   JCYJ20170818142347251 and JCYJ20170818094109846).
CR Akeley K., 2004, ACM Trans. Graph, V23, P3
   [Anonymous], IEEE SIGNAL PROCESS
   [Anonymous], 2002, BT.500-11,
   [Anonymous], 2000, BT1438 ITUR
   [Anonymous], 2010, P INT WORKSH VID PRO
   [Anonymous], ACM T GRAPH
   [Anonymous], 2010, ACM T GRAPH P SIGGRA
   Chen ZY, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/819324
   Chu CH, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808211
   De Silva DVSX, 2011, IEEE J-STSP, V5, P335, DOI 10.1109/JSTSP.2011.2108113
   De Silva V, 2011, IEEE T MULTIMEDIA, V13, P498, DOI 10.1109/TMM.2011.2129500
   Ding P, 2013, CHEMOSPHERE, V91, P28, DOI 10.1016/j.chemosphere.2012.11.038
   Fang YM, 2017, IEEE T IMAGE PROCESS, V26, P4684, DOI 10.1109/TIP.2017.2721112
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P2625, DOI 10.1109/TIP.2014.2305100
   Feng S, 2016, OPT EXPRESS, V24, P2166, DOI 10.1364/OE.24.012166
   Holliman NS, 2011, IEEE T BROADCAST, V57, P362, DOI 10.1109/TBC.2011.2130930
   Jiang QP, 2017, NEUROCOMPUTING, V252, P77, DOI 10.1016/j.neucom.2016.02.089
   Jung CK, 2015, DISPLAYS, V40, P17, DOI 10.1016/j.displa.2015.05.006
   Jung SW, 2012, IEEE T IMAGE PROCESS, V21, P3624, DOI 10.1109/TIP.2012.2191569
   Jung YJ, 2013, IEEE T CIRC SYST VID, V23, P2077, DOI 10.1109/TCSVT.2013.2270394
   Kan BC, 2018, OPT EXPRESS, V26, P11418, DOI 10.1364/OE.26.011418
   Kim T, 2015, IEEE T IMAGE PROCESS, V24, P4335, DOI 10.1109/TIP.2015.2462026
   Lambooij M, 2013, DISPLAYS, V34, P8, DOI 10.1016/j.displa.2012.09.002
   Lambooij MTM, 2007, PROC SPIE, V6490, DOI 10.1117/12.705527
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   Liu XG, 2017, IEEE SYST J, V11, P2829, DOI 10.1109/JSYST.2015.2478119
   Oh H, 2016, IEEE T IMAGE PROCESS, V25, P3358, DOI 10.1109/TIP.2016.2567099
   Park J, 2014, IEEE J-STSP, V8, P415, DOI 10.1109/JSTSP.2014.2311885
   Park J, 2015, IEEE T ROBOT, V31, P1101, DOI 10.1109/TRO.2015.2459373
   Perrin J, 1998, P SOC PHOTO-OPT INS, V3387, P124, DOI 10.1117/12.316401
   Read J, 2005, PROG BIOPHYS MOL BIO, V87, P77, DOI 10.1016/j.pbiomolbio.2004.06.005
   Shao F, 2017, IEEE T CYBERNETICS, V47, P4521, DOI 10.1109/TCYB.2016.2615856
   Shao F, 2017, OPT EXPRESS, V25, P12478, DOI 10.1364/OE.25.012478
   Shao F, 2015, DISPLAYS, V39, P125, DOI 10.1016/j.displa.2015.10.001
   Sohn H, 2014, IEEE T CIRC SYST VID, V24, P745, DOI 10.1109/TCSVT.2013.2291281
   Tam WJ, 2011, IEEE T BROADCAST, V57, P335, DOI 10.1109/TBC.2011.2125070
   Wan S, 2013, INT WORK QUAL MULTIM, P170
   Wu Y, 2016, 2016 3RD INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE), P659, DOI 10.1109/ICISCE.2016.146
   Yue GH, 2019, IEEE T IMAGE PROCESS, V28, P2075, DOI 10.1109/TIP.2018.2875913
   Yue GH, 2018, SIGNAL PROCESS, V150, P204, DOI 10.1016/j.sigpro.2018.04.019
   Yue GH, 2016, INT CONF COMP SCI ED, P733, DOI 10.1109/ICCSE.2016.7581671
NR 41
TC 5
Z9 5
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2019
VL 15
IS 3
AR 77
DI 10.1145/3325197
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ6YR
UT WOS:000494301200008
DA 2024-07-18
ER

PT J
AU Mesfin, G
   Hussain, N
   Covaci, A
   Ghinea, G
AF Mesfin, Gebremariam
   Hussain, Nadia
   Covaci, Alexandra
   Ghinea, Gheorghita
TI Using Eye Tracking and Heart-Rate Activity to Examine Crossmodal
   Correspondences QoE in Mulsemedia
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Mulsemedia; video; audio; haptic; cross-modal correspondence; quality of
   experience; gaze tracking; heart-rate variability
ID MULTIMEDIA; EXPERIENCE
AB Different senses provide us with information of various levels of precision and enable us to construct a more precise representation of the world. Rich multisensory simulations are thus beneficial for comprehension, memory reinforcement, or retention of information. Crossmodal mappings refer to the systematic associations often made between different sensory modalities (e.g., high pitch is matched with angular shapes) and govern multisensory processing. A great deal of research effort has been put into exploring cross-modal correspondences in the field of cognitive science. However, the possibilities they open in the digital world have been relatively unexplored. Multiple sensorial media (mulsemedia) provides a highly inunersive experience to the users and enhances their Quality of Experience (QoE) in the digital world. Thus, we consider that studying the plasticity and the effects of cross-modal correspondences in a mulsemedia setup can bring interesting insights about improving the human computer dialogue and experience. In our experiments, we exposed users to videos with certain visual dimensions (brightness, color, and shape), and we investigated whether the pairing with a cross-modal matching sound (high and low pitch) and the corresponding auto-generated vibrotactile effects (produced by a haptic vest) lead to an enhanced QoE. For this, we captured the eye gaze and the heart rate of users while experiencing mulsemedia, and we asked them to fill in a set of questions targeting their enjoyment and perception at the end of the experiment. Results showed differences in eye-gaze patterns and heart rate between the experimental and the control group, indicating changes in participants' engagement when videos were accompanied by matching cross-modal sounds (this effect was the strongest for the video displaying angular shapes and high-pitch audio) and transitively generated cross-modal vibrotactile effects.
C1 [Mesfin, Gebremariam; Hussain, Nadia; Ghinea, Gheorghita] Brunel Univ London, London, England.
   [Covaci, Alexandra] Univ Kent, Sch Engn & Digital Arts, Canterbury CT2 7NT, Kent, England.
   [Mesfin, Gebremariam; Hussain, Nadia; Ghinea, Gheorghita] Brunel Univ, Dept Comp Sci, Uxbridge UB8 3PH, Middx, England.
C3 Brunel University; University of Kent; Brunel University
RP Mesfin, G (corresponding author), Brunel Univ London, London, England.; Mesfin, G (corresponding author), Brunel Univ, Dept Comp Sci, Uxbridge UB8 3PH, Middx, England.
EM gebremariam.assres@brunel.ac.uk; nadia.hussain@brunel.ac.uk;
   a.covaci@kent.ac.uk; george.ghinea@brunel.ac.uk
RI Assres, Gebremariam Mesfin/AFM-0811-2022; Hussain, Nadia/GXH-7350-2022;
   Ghinea, Gheorghita/AAG-6770-2020
OI Assres, Gebremariam Mesfin/0000-0002-6760-690X; Ghinea,
   Gheorghita/0000-0003-2578-5580; Covaci, Alexandra/0000-0002-3205-2273
FU European Union [688503]
FX This article was funded by the European Union's Horizon 2020 Research
   and Innovation programme under Grant Agreement no. 688503.
CR Ademoye OA, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2957753
   Ademoye OA, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487270
   [Anonymous], 2007, Cross-Modal Influences on Gustatory Perception
   [Anonymous], P DIGRA 2005 CHANG V
   [Anonymous], SIMPLE STAT LIB INFO
   [Anonymous], 2005, P 3 INT C COMPUTER G, DOI DOI 10.1145/1101389.1101462
   [Anonymous], P HAID 2006 1 INT WO
   [Anonymous], 2016, IEEE, DOI DOI 10.1109/QOMEX.2016.7498964
   Brkic Belma R., 2009, P 25 SPRING C COMP G, P161, DOI DOI 10.1145/1980462.1980494
   Brunnstrom K., 2013, Qualinet White Paper on Definitions of Quality of Experience
   Cingel D, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P505, DOI 10.1145/2998181.2998240
   Crisinel AS, 2009, NEUROSCI LETT, V464, P39, DOI 10.1016/j.neulet.2009.08.016
   Dalmaijer E., 2014, PEERJ PREPRINTS, DOI [10.7287/peerj.preprints.585v1, DOI 10.7287/PEERJ.PREPRINTS.585V1]
   Danieau F, 2013, IEEE T HAPTICS, V6, P193, DOI [10.1109/TOH.2012.70, 10.1109/ToH.2012.70]
   Drachen A., 2010, Proceedings of the 5th ACM SIGGRAPH Symposium on Video Games, P49, DOI DOI 10.1145/1836135.1836143
   Dumais SusanT., 2010, Proceedings of IIiX, P185, DOI [DOI 10.1145/1840784.1840812, 10.1145/1840784.1840812]
   Eid M, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON VIRTUAL ENVIRONMENTS, HUMAN-COMPUTER INTERFACES AND MEASUREMENT SYSTEMS, P5, DOI 10.1109/VECIMS.2008.4592743
   Engelke U, 2017, IEEE J-STSP, V11, P6, DOI 10.1109/JSTSP.2016.2609843
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   Ghinea G, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2661333
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2379790.2379794
   Gulliver SR, 2004, IEEE T SYST MAN CY A, V34, P472, DOI 10.1109/TSMCA.2004.826309
   Hagtvedt H, 2016, J MARKETING RES, V53, P551, DOI 10.1509/jmr.14.0414
   Huisman G, 2016, 13TH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER ENTERTAINMENT TECHNOLOGY (ACE 2016)
   Hulusic Vedad, 2010, 2010 2nd International Conference on Games and Virtual Worlds for Serious Applications (VS-GAMES 2010), P126, DOI 10.1109/VS-GAMES.2010.20
   Hulusic Vedad., 2009, SCCG, P151
   Keighrev C., 2017, Quality of Multimedia Experience (QoMEX), 2017 Ninth International Conference on, P1
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Koizumi N, 2011, PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER ENTERTAINMENT TECHNOLOGY (ACE 2011)
   Litscher D, 2013, EVID-BASED COMPL ALT, V2013, DOI 10.1155/2013/674183
   MARKS LE, 1987, J EXP PSYCHOL HUMAN, V13, P384, DOI 10.1037/0096-1523.13.3.384
   MARKS LE, 1974, AM J PSYCHOL, V87, P173, DOI 10.2307/1422011
   Mastoropoulou Georgia, 2007, THESIS
   Munster G., 2015, gene, V612, P303
   Murray N, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3108243
   Ramic-Brkic Belma, 2013, P 29 SPRING C COMP G, P91, DOI DOI 10.1145/2508244.2508256
   Ranasinghe N., 2014, Proceedings of the 8th International Conference on Tangible, Embedded and Embodied Interaction, P133, DOI [DOI 10.1145/2540930, DOI 10.1145/2540930.2540939, 10.1145/2540930.2540939]
   Ranasinghe N, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174151
   Ranasinghe N, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1139, DOI 10.1145/3123266.3123440
   Rehman S., 2014, P 51 ANN DES AUT C D, P1, DOI DOI 10.7873/DATE.2014.119
   Seo HS, 2010, NEUROSCI LETT, V478, P175, DOI 10.1016/j.neulet.2010.05.011
   Shams L, 2000, NATURE, V408, P788, DOI 10.1038/35048669
   Simner J., 2009, P 3 INT C SYN ART
   SIMPSON R H, 1956, J Genet Psychol, V89, P95
   Slocombe BG, 2016, NEUROPSYCHOLOGIA, V88, P58, DOI 10.1016/j.neuropsychologia.2015.07.011
   Song Guanghan, 2012, P 13 INT WORKSH IM A
   Spence C., 2010, The World of Fine Wine, V28, P122
   Spence C, 2011, ATTEN PERCEPT PSYCHO, V73, P971, DOI 10.3758/s13414-010-0073-7
   Striner Alina, 2018, ARXIV180400229
   Sulema Y, 2016, INT CONF SYST SIGNAL, P19
   Sun XW, 2018, PEERJ, V6, DOI 10.7717/peerj.4443
   Tag B, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2017 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC '17 ADJUNCT), P289, DOI 10.1145/3123024.3123190
   Tanaka A, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P2150, DOI 10.1145/2858036.2858304
   Tortell R., 2007, Virtual Reality, V11, P61, DOI 10.1007/s10055-006-0056-0
   Tsiros A, 2017, ELECT VISUALISATION, P175
   Valenti G, 2013, IEEE ICCE, P195, DOI 10.1109/ICCE.2013.6486856
   Wang JJ, 2017, PSYCHON B REV, V24, P393, DOI 10.3758/s13423-016-1097-3
   WELCH RB, 1980, PSYCHOL BULL, V88, P638, DOI 10.1037/0033-2909.88.3.638
   WELCH RB, 1986, PERCEPT PSYCHOPHYS, V39, P294, DOI 10.3758/BF03204939
   Yau JM, 2009, CURR BIOL, V19, P561, DOI 10.1016/j.cub.2009.02.013
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
   Yuan ZH, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2661329
   Zou LH, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P315, DOI 10.1145/3083187.3084014
NR 63
TC 11
Z9 11
U1 2
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 34
DI 10.1145/3303080
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA IM3ZV
UT WOS:000477935400005
OA Green Published, Green Accepted
DA 2024-07-18
ER

PT J
AU Fan, HH
   Zheng, L
   Yan, CG
   Yang, Y
AF Fan, Hehe
   Zheng, Liang
   Yan, Chenggang
   Yang, Yi
TI Unsupervised Person Re-identification: Clustering and Fine-tuning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Large-scale person re-identification; unsupervised learning;
   convolutional neural network; clustering
ID ENSEMBLE; NETWORK
AB The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised- Person- Re- identification- Clustering- and- Fine- tuning.
C1 [Fan, Hehe; Yan, Chenggang] Hangzhou Dianzi Univ, Inst Informat & Control, 1158 Baiyang Rd, Zhejiang 310018, Peoples R China.
   [Fan, Hehe; Zheng, Liang; Yang, Yi] Univ Technol Sydney, Ctr Artificial Intelligence, 15 Broadway, Sydney, NSW 2007, Australia.
C3 Hangzhou Dianzi University; University of Technology Sydney
RP Yan, CG (corresponding author), Hangzhou Dianzi Univ, Inst Informat & Control, 1158 Baiyang Rd, Zhejiang 310018, Peoples R China.
EM hehe.fan@student.uts.edu.au; liang.zheng@uts.edu.au; cgyan@hdu.edu.cn;
   yi.yang@uts.edu.au
RI yang, yang/HGT-7999-2022; Lang, Ming/HIK-0758-2022; yang,
   yang/GWB-9426-2022; Yang, Yi/B-9273-2017; yang, yang/GVT-5210-2022
OI Yang, Yi/0000-0002-0512-880X; Zheng, Liang/0000-0002-1464-9500
FU National Nature Science Foundation of China [61671196, 61525206,
   61701149]; Zhejiang Province Nature Science Foundation of China
   [LR17F030006]; National Key Research and Development Program of China
   [2017YFC0820600]; 111 Project [D17019]; Data to Decisions CRC (D2D CRC);
   Cooperative Research Centres Programme; SIEF STEM+ Business fellowship
FX Part of this work was done when Hehe Fan was visiting Hangzhou Dianzi
   University. This work is in part supported by National Nature Science
   Foundation of China (Grants No. 61671196, No. 61525206, and No.
   61701149), in part supported by Zhejiang Province Nature Science
   Foundation of China LR17F030006, in part supported by National Key
   Research and Development Program of China (Grant No. 2017YFC0820600),
   111 Project, No. D17019, in part supported by Data to Decisions CRC (D2D
   CRC) and in part supported by Cooperative Research Centres Programme.
   Liang Zheng is the recipient of a SIEF STEM+ Business fellowship.
CR [Anonymous], 2007, SOC IND APPL MATH
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bai S, 2019, IEEE T PATTERN ANAL, V41, P1213, DOI 10.1109/TPAMI.2018.2828815
   Bai S, 2017, AAAI CONF ARTIF INTE, P1281
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Dong XY, 2019, IEEE T PATTERN ANAL, V41, P1641, DOI 10.1109/TPAMI.2018.2844853
   Fan HH, 2017, IEEE I CONF COMP VIS, P736, DOI 10.1109/ICCV.2017.86
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Geng Mengyue, 2016, ABS161105244 ARXIV
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Jiang L, 2014, ADV NEUR IN, V27
   Kodirov E, 2016, LECT NOTES COMPUT SC, V9905, P178, DOI 10.1007/978-3-319-46448-0_11
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin Yutian, 2017, ABS170307220 ARXIV
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu Jiawei, 2016, P 24 ACM INT C MULT, P192
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Ma F, 2017, PR MACH LEARN RES, V70
   Ma XL, 2017, PATTERN RECOGN, V65, P197, DOI 10.1016/j.patcog.2016.11.018
   Ma ZG, 2017, IEEE T MULTIMEDIA, V19, P1558, DOI 10.1109/TMM.2017.2659221
   Pawan Kumar M., 2010, NIPS
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Song Bai, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P774, DOI 10.1109/ICCV.2017.90
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang HX, 2016, LECT NOTES COMPUT SC, V9908, P405, DOI 10.1007/978-3-319-46493-0_25
   Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45
   Wei Longhui, 2017, ABS171108565 ARXIV
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P220, DOI 10.1109/TITS.2017.2749977
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P284, DOI 10.1109/TITS.2017.2749965
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Yang Y, 2017, AAAI CONF ARTIF INTE, P4306
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang Y, 2016, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2016.143
   Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   ZHENG L, 2015, PROC CVPR IEEE, P1741, DOI DOI 10.1109/CVPR.2015.7298783
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2016, ABS161002984 ARXIV
   Zheng Liang, 2017, ABS170107732 ARXIV
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
NR 65
TC 482
Z9 534
U1 13
U2 134
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2018
VL 14
IS 4
AR 83
DI 10.1145/3243316
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JB
UT WOS:000457139200005
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Slivar, I
   Suznjevic, M
   Skorin-Kapov, L
AF Slivar, Ivan
   Suznjevic, Mirko
   Skorin-Kapov, Lea
TI Game Categorization for Deriving QoE-Driven Video Encoding Configuration
   Strategies for Cloud Gaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cloud gaming; Quality of Experience; game categorization; video codec
   configuration strategies
AB Cloud gaming has been recognized as a promising shift in the online game industry, with the aim of implementing the "on demand" service concept that has achieved market success in other areas of digital entertainment such as movies and TV shows. The concepts of cloud computing are leveraged to render the game scene as a video stream that is then delivered to players in real-time. The main advantage of this approach is the capability of delivering high-quality graphics games to any type of end user device; however, at the cost of high bandwidth consumption and strict latency requirements. A key challenge faced by cloud game providers lies in configuring the video encoding parameters so as to maximize player Quality of Experience (QoE) while meeting bandwidth availability constraints. In this article, we tackle one aspect of this problem by addressing the following research question: Is it possible to improve service adaptation based on information about the characteristics of the game being streamed? To answer this question, two main challenges need to be addressed: the need for different QoE-driven video encoding (re-)configuration strategies for different categories of games, and how to determine a relevant game categorization to be used for assigning appropriate configuration strategies. We investigate these problems by conducting two subjective laboratory studies with a total of 80 players and three different games. Results indicate that different strategies should likely be applied for different types of games, and show that existing game classifications are not necessarily suitable for differentiating game types in this context. We thus further analyze objective video metrics of collected game play video traces as well as player actions per minute and use this as input data for clustering of games into two clusters. Subjective results verify that different video encoding configuration strategies may be applied to games belonging to different clusters.
C1 [Slivar, Ivan; Suznjevic, Mirko; Skorin-Kapov, Lea] Univ Zagreb, Fac Elect Engn & Comp, Unska 3, Zagreb, Croatia.
C3 University of Zagreb
RP Slivar, I (corresponding author), Univ Zagreb, Fac Elect Engn & Comp, Unska 3, Zagreb, Croatia.
EM ivan.slivar@fer.hr; mirko.suznjevic@fer.hr; lea.skorin-kapov@fer.hr
RI Slivar, Ivan/AAH-1173-2021; Skorin-Kapov, Lea/AAJ-4737-2021
FU Croatian Science Foundation [UIP-2014-09-5605]; project "Information and
   communication technology for generic and energy-efficient communication
   solutions with application in e-/m-health (ICTGEN))" - EU from the
   European Regional Development Fund
FX This work has been supported in part by the Croatian Science Foundation
   under the project UIP-2014-09-5605 (Q-MANIC) and the project
   "Information and communication technology for generic and
   energy-efficient communication solutions with application in e-/m-health
   (ICTGEN))" co-financed by the EU from the European Regional Development
   Fund.
CR Ahmadi H, 2014, MULTIMEDIA SYST, V20, P485, DOI 10.1007/s00530-014-0381-1
   [Anonymous], 2015, IEEE T CIRCUITS SYST
   [Anonymous], 2013, PROC 12 ANN WORKSHOP
   [Anonymous], 2013, P 4 INT WORKSHOP PER
   [Anonymous], P910 SUBJ VID QUAL A
   [Anonymous], 2015, P INT WORKSH NETW SY, DOI DOI 10.1109/NETGAMES.2015.7383002
   [Anonymous], P 14 ANN WORKSH NETW
   [Anonymous], 2015, PROC 7 INT WORKSHOP
   Cai W, 2016, IEEE ACCESS, V4, P7605, DOI 10.1109/ACCESS.2016.2590500
   Claypool Mark, 2015, 2015 IEEE Games Entertainment Media Conference (GEM), P1, DOI 10.1109/GEM.2015.7377234
   Claypool M., 2014, Network and Systems Support for Games (NetGames), 2014 13th Annual Workshop on, P1, DOI DOI 10.1109/NETGAMES.2014.7008964
   Claypool M., 2009, P 4 INT C FDN DIGITA, P42, DOI DOI 10.1145/1536513.1536530
   Claypool M, 2006, COMMUN ACM, V49, P40, DOI 10.1145/1167838.1167860
   Claypool Mark., 2009, Proceedings of the 4th International Conference on Foundations of Digital Games, P34, DOI DOI 10.1145/1536513.1536529
   Clincy V, 2013, PROCEEDINGS OF THE 2013 10TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: NEW GENERATIONS, P473, DOI 10.1109/ITNG.2013.79
   Dick M., 2005, NetGames'05, P1
   Hossfeld T., 2016, Quality and User Experience, V1, P1, DOI [10.1007/S41233-016-0002-1, DOI 10.1007/S41233-016-0002-1, 10.1007/s41233-016-0002-1]
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   Jarschel M, 2013, MATH COMPUT MODEL, V57, P2883, DOI 10.1016/j.mcm.2011.12.014
   Kaufman L., 2009, FINDING GROUPS DATA
   Lee Y.-T., 2012, Network and Systems Support for Games (NetGames), 2012 11th Annual Workshop on, P1
   Liu Y, 2014, TSINGHUA SCI TECHNOL, V19, P33, DOI 10.1109/TST.2014.6733206
   Quax P, 2013, INT IEEE CONSUM ELEC, P216, DOI 10.1109/IGIC.2013.6659141
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Slivar Ivan., 2016, Proceedings of the 7th international conference on multimedia systems, P1
   Suznjevic M, 2009, MULTIMED TOOLS APPL, V45, P191, DOI 10.1007/s11042-009-0300-1
   Wang SQ, 2009, PROCEEDINGS OF THE 2009 CHINESE CONFERENCE ON PATTERN RECOGNITION AND THE FIRST CJK JOINT WORKSHOP ON PATTERN RECOGNITION, VOLS 1 AND 2, P1, DOI 10.1109/PLASMA.2009.5227540
   Wen ZY, 2014, IEEE INT WORKSH MULT
NR 28
TC 14
Z9 14
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
SU S
AR 56
DI 10.1145/3132041
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG9GD
UT WOS:000455314500004
DA 2024-07-18
ER

PT J
AU Bruneau-Queyreix, J
   Batalla, JM
   Lacaud, M
   Negru, D
AF Bruneau-Queyreix, Joachim
   Batalla, Jordi Mongay
   Lacaud, Mathias
   Negru, Daniel
TI PMS: A Novel Scale-Adaptive and Quality-Adaptive Hybrid P2P/Multisource
   Solution for Live Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE DASH; multiple-source streaming; P2P streaming; adaptive streaming
ID VIDEO SYSTEMS; LAYERP2P
AB Single-source HTTP adaptive streaming solutions (HAS) have become the de facto solutions to deliver live video over the Internet. By avoiding video stalling events that are mainly caused by the lack of throughput at client or at server side, HAS solutions increase the end users' quality of experience (QoE). We propose to pragmatically extend HAS with our MS-Stream solution that simultaneously utilizes several servers. MS-Stream aims at offering high QoE for live content delivery by exploiting expanded bandwidth and link diversity in distributed heterogeneous infrastructures. By leveraging end users' connectivity capacities, we further extend the QoE and scalability capabilities of our proposal by exposing a hybrid P2P/multisource live-streaming solution (P2P/MS-Stream (PMS)), achieving trade-offs between the system's scale and the end users' QoE. We propose a distributed quality adaptation algorithm run by every peer, along with a local optimization method of the usage of the server infrastructure made available. Large-scale evaluations conducted with 300 peers located in France permits validating our approach and algorithms over flash crowd events and allow us to conclude that PMS can reach the optimal trade-offs between QoE and system scale.
C1 [Bruneau-Queyreix, Joachim; Batalla, Jordi Mongay] Natl Inst Telecommun, Warsaw, Poland.
   [Bruneau-Queyreix, Joachim; Lacaud, Mathias] Joada, Bordeaux, France.
   [Batalla, Jordi Mongay] Warsaw Univ Technol, Warsaw, Poland.
   [Lacaud, Mathias; Negru, Daniel] Univ Bordeaux, UMR 5800, F-5800 UMR, France.
   [Bruneau-Queyreix, Joachim; Lacaud, Mathias] Joada Bordeaux, Bordeaux, France.
C3 National Institute of Telecommunications - Poland; Warsaw University of
   Technology; Universite de Bordeaux; Centre National de la Recherche
   Scientifique (CNRS)
RP Bruneau-Queyreix, J (corresponding author), Natl Inst Telecommun, Warsaw, Poland.; Bruneau-Queyreix, J (corresponding author), Joada, Bordeaux, France.; Bruneau-Queyreix, J (corresponding author), Joada Bordeaux, Bordeaux, France.
EM jbruneauqueyreix@joada.net; jordim@interfree.it;
   mathias.lacaud@labri.fr; daniel.negru@labri.fr
RI Batalla, Jordi/AAL-9056-2021
OI Batalla, Jordi/0000-0002-1489-5138
CR Adhikari V., 2012, P IEEE INFOCOM IEEE
   [Anonymous], 2012, P 8 INT C EM NETW EX
   [Anonymous], 2013, 2013 20 INT PACK VID
   [Anonymous], 2013, P 5 WORKSH MOB VID M
   Bruneau-Queyreix J., 2017, P ACM INT C MULT MM
   Bruneau-Queyreix J., 2017, P IEEE CONS COMM NET
   Bruneau-Queyreix J., 2017, P IEEE INT C MULT ME
   Bruneau-Queyreix J., 2017, P IEEE COMM NETW C C
   Brunson JeremyL., 2018, INTERPRETING POLITIC, P99, DOI [10.4324/9781315619224-6, DOI 10.4324/9781315619224-6]
   Chellouche S. A., 2012, P IEEE S COMP COMM I
   Cisco, 2016, CISC VIS NETW IND
   Ganjam A., 2015, P USENIX C NETW SYST
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Hossfeld T, 2012, INT WORK QUAL MULTIM, P1, DOI 10.1109/QoMEX.2012.6263849
   Hossfeld T., 2011, P IEEE S MULT
   Huang C., 2008, P 18 INT WORKSH NETW, P75, DOI [10.1145/1496046.1496064., DOI 10.1145/1496046.1496064]
   Lederer S., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P161, DOI 10.1109/PV.2012.6229730
   Liu YT, 2013, IEEE INT CONF COMM, P682, DOI 10.1109/ICCW.2013.6649320
   Liu ZY, 2009, IEEE T MULTIMEDIA, V11, P1340, DOI 10.1109/TMM.2009.2030656
   Medjiah S, 2014, IEEE J SEL AREA COMM, V32, P734, DOI 10.1109/JSAC.2014.140406
   Merani ML, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2912123
   Moon YH, 2013, J SUPERCOMPUT, V66, P700, DOI 10.1007/s11227-012-0858-7
   MS-Stream, 2017, MS STREAM DEMONSTRAT
   Ni P., 2011, P 19 ACM INT C MULT
   Pu W., 2011, P IEEE GLOB TEL C GL
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Watson M., 2014, HTTP ADAPTIVE STREAM
   Wu D, 2009, IEEE INFOCOM SER, P2726, DOI 10.1109/INFCOM.2009.5062220
   Xiao X, 2009, IEEE INFOCOM SER, P603, DOI 10.1109/INFCOM.2009.5061967
   Xu DY, 2006, MULTIMEDIA SYST, V11, P383, DOI 10.1007/s00530-006-0015-3
   Yin H., 2009, Proceedings of ACM International Conference on Multimedia, P25
   Yin H, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823750
   Zhang G, 2015, IEEE T MULTIMEDIA, V17, P229, DOI 10.1109/TMM.2014.2383617
   Zhang S., 2015, P IEEE INT C COMM IC
   ZHANG X, 2005, P IEEE INFOCOM
   Zhihui Lu, 2012, Journal of Communications, V7, P232, DOI 10.4304/jcm.7.3.232-245
   Zhou C, 2014, IEEE T CIRC SYST VID, V24, P681, DOI 10.1109/TCSVT.2013.2290580
NR 37
TC 2
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 35
DI 10.1145/3183515
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TD
UT WOS:000434634700008
DA 2024-07-18
ER

PT J
AU Hussein, F
   Piccardi, M
AF Hussein, Fairouz
   Piccardi, Massimo
TI V-JAUNE: A Framework for Joint Action Recognition and Video
   Summarization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Action recognition; video summarization; latent structural SVM;
   submodular inference
AB Video summarization and action recognition are two important areas of multimedia video analysis. While these two areas have been tackled separately to date, in this article, we present a latent structural SVM framework to recognize the action and derive the summary of a video in a joint, simultaneous fashion. Efficient inference is provided by a submodular score function that accounts for the action and summary jointly. In this article, we also define a novel measure to evaluate the quality of a predicted video summary against the annotations of multiple annotators. Quantitative and qualitative results over two challenging action datasets-the ACE and MSR DailyActivity3D datasets-show that the proposed joint approach leads to higher action recognition accuracy and equivalent or better summary quality than comparable approaches that perform these tasks separately.
C1 [Hussein, Fairouz; Piccardi, Massimo] Univ Technol Sydney, Fac Engn & IT, Global Big Data Technol Ctr, POB 123, Broadway, NSW 2007, Australia.
C3 University of Technology Sydney
RP Hussein, F (corresponding author), Univ Technol Sydney, Fac Engn & IT, Global Big Data Technol Ctr, POB 123, Broadway, NSW 2007, Australia.
EM Fairouz.Hussein@student.uts.edu.au; Massimo.Piccardi@uts.edu.au
RI Piccardi, Massimo/AAY-1323-2020
OI Piccardi, Massimo/0000-0001-9250-6604; Hussein,
   Fairouz/0000-0001-5290-515X
CR Almeida J, 2012, PATTERN RECOGN LETT, V33, P397, DOI 10.1016/j.patrec.2011.08.007
   Altun Yasemin., 2005, Advances in neural information processing systems, P33
   [Anonymous], 2016, DEEP LEARNING
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Brendel W, 2010, LECT NOTES COMPUT SC, V6312, P721, DOI 10.1007/978-3-642-15552-9_52
   Chen BW, 2009, IEEE T MULTIMEDIA, V11, P295, DOI 10.1109/TMM.2008.2009703
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Duan Huizhong., 2012, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL'12, P1511
   Ejaz N, 2013, SIGNAL PROCESS-IMAGE, V28, P34, DOI 10.1016/j.image.2012.10.002
   Ejaz N, 2012, J VIS COMMUN IMAGE R, V23, P1031, DOI 10.1016/j.jvcir.2012.06.013
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Guan GL, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2632267
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hu Y, 2011, COMM COM INF SC, V202, P535
   Hussein F, 2016, INT CONF ACOUST SPEE, P2697, DOI 10.1109/ICASSP.2016.7472167
   Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31
   Jaffe Alexander., 2006, P INT C WORLD WIDE W, P853
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kim G, 2015, PROC CVPR IEEE, P1993, DOI 10.1109/CVPR.2015.7298810
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Li L., 2011, Proc. 20th Int. Conf. World Wide Web, P287
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin H., 2011, P 49 ANN M ASS COMP, P510
   Liu Y., 2010, ACM Multimedia, P751
   Lu G., 2016, Multimedia Tools and Applications, P1
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   Maybury M., 1997, ACM Multimedia, P102
   Mundur P, 2006, INT J DIGIT LIBRARIE, V6, P219, DOI 10.1007/s00799-005-0129-9
   Negin F., 2016, HUMAN ACTION RECOGNI
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Ni BB, 2015, INT J COMPUT VISION, V111, P229, DOI 10.1007/s11263-014-0742-4
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342
   Sachan M, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P239
   Schindler K., 2008, P 2008 IEEE C COMPUT, P1
   Shimada Atsushi, 2013, Advances in Depth Image Analysis and Applications. International Workshop, WDIA 2012. Selected and Invited Papers: LNCS 7854, P168, DOI 10.1007/978-3-642-40303-3_18
   Sipos Ruben, 2012, EACL, P224
   Slama R, 2015, PATTERN RECOGN, V48, P556, DOI 10.1016/j.patcog.2014.08.011
   Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Turaga P, 2009, COMPUT VIS IMAGE UND, V113, P353, DOI 10.1016/j.cviu.2008.08.009
   Vedaldi A., 2011, A MATLAB wrapper of SVMstruct
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang HW, 2009, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE AND ENGINEERING MANAGEMENT, P124
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   [王元清 Wang Yuanqing], 2011, [土木建筑与环境工程, Journal of Civil, Architectural & Environmental Engineering], V33, P7
   Wu XX, 2013, IEEE T CIRC SYST VID, V23, P1422, DOI 10.1109/TCSVT.2013.2244794
   Xiong Z., 2006, A unified framework for video summarization, browsing, and retrieval with applications to consumer and surveillance video
   Yang CL, 2013, PATTERN RECOGN, V46, P948, DOI 10.1016/j.patcog.2012.07.011
   Yu T., 2009, P 26 ANN INT C MACH, P1169, DOI DOI 10.1145/1553374.1553523
   Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPRW.2009.5206671, 10.1109/CVPR.2009.5206671]
   Zhu L, 2010, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2010.5540096
NR 61
TC 13
Z9 14
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2017
VL 13
IS 2
AR 20
DI 10.1145/3063532
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EV1VK
UT WOS:000401537300008
DA 2024-07-18
ER

PT J
AU Zhou, YP
   Chen, L
   Jing, M
   Zou, SL
   Ma, RT
AF Zhou, Yipeng
   Chen, Liang
   Jing, Mi
   Zou, Shenglong
   Ma, Richard Tianbai
TI Design, Implementation, and Measurement of a Crowdsourcing-Based Content
   Distribution Platform
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 6th ACM International Conference on Multimedia Systems (MMSys)
   Co-Located with 25th ACM Workshop on Network and Operating Systems
   Support for Digital Audio and Video (NOSSDAV)
CY MAR 18-20, 2015
CL Portland, OR
SP ACM
DE Crowdsourcing; video distribution; agent; CDN
AB Content distribution, especially the distribution of video content, unavoidably consumes bandwidth resources heavily. Internet content providers invest heavily in purchasing content distribution network (CDN) services. By deploying tens of thousands of edge servers close to end users, CDN companies are able to distribute content efficiently and effectively, but at considerable cost. Thus, it is of great importance to develop a new system that distributes content at a lower cost but comparable service quality. In lieu of expensive CDN systems, we implement a crowdsourcing-based content distribution system, Thunder Crystal, by renting bandwidth for content upload/download and storage for content cache from agents. This is a large-scale system with tens of thousands of agents, whose resources significantly amplify Thunder Crystal's content distribution capacity. The involved agents are either from ordinary Internet users or enterprises. Monetary rewards are paid to agents based on their upload traffic so as to motivate them to keep contributing resources. As far as we know, this is a novel system that has not been studied or implemented before. This article introduces the design principles and implementation details before presenting the measurement study. In summary, with the help of agent devices, Thunder Crystal is able to reduce the content distribution cost by one half and amplify the content distribution capacity by 11 to 15 times.
C1 [Zhou, Yipeng] Shenzhen Univ, Coll Comp Sci & Software Engn, Room 622-3,Nanhai Ave 3688, Shenzhen 518060, Peoples R China.
   [Chen, Liang] Shenzhen Univ, Coll Informat Engn, N910,Nanhai Ave 3688, Shenzhen 518060, Peoples R China.
   [Jing, Mi; Zou, Shenglong] Xunlei Networking Technol Ltd, Floor 8,Software Pk 2,Keji Mid 2nd Rd, Shenzhen 518060, Peoples R China.
   [Ma, Richard Tianbai] Natl Univ Singapore, Sch Comp, 04-27,15 Comp Dr,COM2 Bldg, Singapore 117418, Singapore.
C3 Shenzhen University; Shenzhen University; National University of
   Singapore
RP Chen, L (corresponding author), Shenzhen Univ, Coll Informat Engn, N910,Nanhai Ave 3688, Shenzhen 518060, Peoples R China.
EM ypzhou@szu.edu.cn; lchen@szu.edu.cn; jingmi@xunlei.com; sean@xunlei.com;
   tbma@comp.nus.edu.sg
RI Ma, Richard/AAR-9233-2020
OI Ma, Richard/0000-0002-9883-5844; Zhou, Yipeng/0000-0003-1533-0865
FU Foundation of Shenzhen City [KQCX20140519103756206]; Natural Science
   Foundation of China [61402297, 61502315]; Natural Science Foundation of
   Guangdong province [2015A030310366]; Shenzhen Science and Technology
   Foundation [JCYJ20150529164656096]
FX The research was partially supported by the Foundation of Shenzhen City
   (no. KQCX20140519103756206), the Natural Science Foundation of China
   (nos. 61402297 and 61502315), the Natural Science Foundation of
   Guangdong province (no. 2015A030310366), and the Shenzhen Science and
   Technology Foundation (grant JCYJ20150529164656096).
CR Abrahamsson H., 2012, P 2012 INT MEAS C IM, P199
   Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   [Anonymous], 2003, P WORKSH EC PEER PEE
   [Anonymous], 2011, P SIGCOMM W MUST WOR
   [Anonymous], 2012, CISC VIS NETW IND GL
   [Anonymous], 2010, J NANOMATER
   Balachandran A., 2013, Proceedings of the 2013 conference on Internet measurement conference, P43
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chen L, 2015, AER ADV ENG RES, V31, P43
   Chen L, 2014, IEEE IJCNN, P1, DOI 10.1109/IJCNN.2014.6889506
   Fan B, 2006, PROCEEDINGS OF THE 2006 IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, P233
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Huang C, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Huang Y, 2008, ACM SIGCOMM COMP COM, V38, P375, DOI 10.1145/1402946.1403001
   Jia AL, 2013, IEEE T PARALL DISTR, V24, P2503, DOI 10.1109/TPDS.2012.332
   Keller Lorenzo., 2012, ACM MOBISYS, P57
   Krishnan R, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P190
   Liu Zhong-Hua, 2011, International Journal of Neuropsychopharmacology, V14, P618, DOI 10.1017/S1461145710000520
   Pallis G, 2006, COMMUN ACM, V49, P101, DOI 10.1145/1107458.1107462
   Passarella A, 2012, COMPUT COMMUN, V35, P1, DOI 10.1016/j.comcom.2011.10.005
   Rahman R, 2011, ACM SIGCOMM COMP COM, V41, P182, DOI 10.1145/2043164.2018458
   Tan B, 2013, IEEE ACM T NETWORK, V21, P566, DOI 10.1109/TNET.2012.2208199
   Traverso S, 2013, ACM SIGCOMM COMP COM, V43, P6
   Vecchioy Alessio., 2013, P SENSYS NOV, P1, DOI 10.1145/2517351.2517397
   Wu CC, 2013, IEEE T MULTIMEDIA, V15, P1121, DOI 10.1109/TMM.2013.2241043
   Wu D, 2009, IEEE INFOCOM SER, P2726, DOI 10.1109/INFCOM.2009.5062220
   Wu WJ, 2014, IEEE T PARALL DISTR, V25, P612, DOI 10.1109/TPDS.2013.94
   Yang L.X., 2012, DISCRETE DYN NAT SOC, V2012, P1, DOI DOI 10.1186/1471-2229-13-53
   Yin H., 2009, Proceedings of ACM International Conference on Multimedia, P25
   Zhang C., 2015, ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, P55, DOI DOI 10.1145/2736084.2736091
   Zhao BQ, 2012, IEEE ACM T NETWORK, V20, P367, DOI 10.1109/TNET.2011.2161770
   Zhengye Liu, 2010, Proceedings of the 2010 IEEE 30th International Conference on Distributed Computing Systems. ICDCS 2010, P610, DOI 10.1109/ICDCS.2010.90
   Zhou YP, 2015, IEEE ACM T NETWORK, V23, P1163, DOI 10.1109/TNET.2014.2321422
   Zhou YP, 2015, IEEE T MULTIMEDIA, V17, P1273, DOI 10.1109/TMM.2015.2447277
   Zhou YP, 2013, IEEE ACM T NETWORK, V21, P233, DOI 10.1109/TNET.2012.2196444
NR 35
TC 5
Z9 5
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2016
VL 12
IS 5
SU S
AR 80
DI 10.1145/2978655
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EJ0VT
UT WOS:000392929700011
DA 2024-07-18
ER

PT J
AU Shen, SQ
   Hu, SY
   Iosup, A
   Epema, D
AF Shen, Siqi
   Hu, Shun-Yun
   Iosup, Alexandru
   Epema, Dick
TI Area of Simulation: Mechanism and Architecture for Multi-Avatar Virtual
   Environments
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Distributed virtual environments; area of interest; real time
   strategy games
ID SYNCHRONIZATION; LATENCY
AB Although Multi-Avatar Distributed Virtual Environments (MAVEs) such as Real-Time Strategy (RTS) games entertain daily hundreds of millions of online players, their current designs do not scale. For example, even popular RTS games such as the StarCraft series support in a single game instance only up to 16 players and only a few hundreds of avatars loosely controlled by these players, which is a consequence of the Event-Based Lockstep Simulation (EBLS) scalability mechanism they employ. Through empirical analysis, we show that a single Area of Interest (AoI), which is a scalability mechanism that is sufficient for single-avatar virtual environments (such as Role-Playing Games), also cannot meet the scalability demands of MAVEs. To enable scalable MAVEs, in this work we propose Area of Simulation (AoS), a new scalability mechanism, which combines and extends the mechanisms of AoI and EBLS. Unlike traditional AoI approaches, which employ only update-based operational models, our AoS mechanism uses both event-based and update-based operational models to manage not single, but multiple areas of interest. Unlike EBLS, which is traditionally used to synchronize the entire virtual world, our AoS mechanism synchronizes only selected areas of the virtual world. We further design an AoS-based architecture, which is able to use both our AoS and traditional AoI mechanisms simultaneously, dynamically trading-off consistency guarantees for scalability. We implement and deploy this architecture and we demonstrate that it can operate with an order of magnitude more avatars and a larger virtual world without exceeding the resource capacity of players' computers.
C1 [Shen, Siqi; Iosup, Alexandru; Epema, Dick] Delft Univ Technol, NL-2600 AA Delft, Netherlands.
   [Hu, Shun-Yun] Acad Sinica, Taipei 115, Taiwan.
C3 Delft University of Technology; Academia Sinica - Taiwan
RP Shen, SQ (corresponding author), Delft Univ Technol, NL-2600 AA Delft, Netherlands.
EM s.shen@tudelft.nl
RI Iosup, Alexandru/G-4069-2012
OI Iosup, Alexandru/0000-0001-8030-9398
FU National Basic Research Program of China [2011CB302603]; Dutch STW/NWO
   Veni personal grant [11881]; Dutch national program COMMIT; Dutch KIEM
   project KIESA; Dutch national program COMMissioner
FX This work is supported by the National Basic Research Program of China
   under grant No. 2011CB302603, by the Dutch STW/NWO Veni personal grant
   @large(#11881), by the Dutch national program COMMIT and COMMissioner,
   and by the Dutch KIEM project KIESA.
CR A. D. team, 2014, FREE OP SOURC GAM AN
   Ahmed D., 2009, HDB MULTIMEDIA DIGIT
   [Anonymous], 2012, Essential facts about the computer and video game industry
   [Anonymous], 2002, RFC3284
   Baughman NE, 2001, IEEE INFOCOM SER, P104, DOI 10.1109/INFCOM.2001.916692
   BERNIER Y, 2001, P GAM DEV C
   Bharambe A, 2008, ACM SIGCOMM COMP COM, V38, P389, DOI 10.1145/1402946.1403002
   Boulanger J.S., 2006, Proceedings of 5th ACM SIGCOMM workshop on Network and system support for games, P1
   Brewer E, 2012, COMPUTER, V45, P23, DOI 10.1109/MC.2012.37
   Buro M, 2012, AI MAG, V33, P106, DOI 10.1609/aimag.v33i3.2419
   Chen T., 2010, P ANN WORKSH NETW SY, P1
   Clark C, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND SYMPOSIUM ON NETWORKED SYSTEMS DESIGN & IMPLEMENTATION (NSDI '05), P273
   Claypool M, 2005, COMPUT NETW, V49, P52, DOI 10.1016/j.comnet.2005.04.008
   Cronin E, 2004, MULTIMED TOOLS APPL, V23, P7, DOI 10.1023/B:MTAP.0000026839.31028.9f
   Deng YH, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2499906
   Diot C, 1999, IEEE NETWORK, V13, P6, DOI 10.1109/65.777437
   Ferretti S., 2008, P 2 INT C DISTRIBUTE, P83
   Fiedler G, 2010, WHAT EVERY PROGRAMME
   Frey D., 2008, INT WORKSHOP MASSIVE, P29
   Gilmore JS, 2012, IEEE T PARALL DISTR, V23, P818, DOI 10.1109/TPDS.2011.210
   Granberg C., 2006, PROGRAMMING RTS GAME
   Gregory Jason., 2009, GAME ENGINE ARCHITEC
   Huang Chun-Ying, 2013, P 4 ACM MULT SYST C, P36, DOI DOI 10.1145/2483977.2483981
   Iosup A, 2014, P INT WORKSH MASS MU, P1
   Keller J, 2003, PDPTA'03: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED PROCESSING TECHNIQUES AND APPLICATIONS, VOLS 1-4, P262
   Knutsson B, 2004, IEEE INFOCOM SER, P96
   Krammer L, 2012, INT C PAR DISTRIB SY, P604, DOI 10.1109/ICPADS.2012.87
   Lake D., 2010, PROC 9 ANN WORKSHOP, P1
   Liu HY, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2333112.2333116
   Lu F., 2006, P 5 ACM SIGCOMM WORK, P1
   Lui JCS, 2002, IEEE T PARALL DISTR, V13, P193, DOI 10.1109/71.993202
   LUPEI D, 2010, P EUR C COMP SYST, P41
   Mauve M, 2004, IEEE T MULTIMEDIA, V6, P47, DOI 10.1109/TMM.2003.819751
   McGee J., 2011, PROS CONS COLLISION
   Miller P., 2011, Professional gamers: a day in the life
   Morillo P, 2007, IEEE T PARALL DISTR, V18, P1215, DOI 10.1109/TPDS.2007.1055
   Muller J., 2005, P INT C ADV COMP ENT, P125
   Najaran MahdiTayarani., 2014, ACM Multimedia Systems Conference (MMSys), P127
   ROSEDALE P, 2003, GAMASUTRA RESOURCE G
   Shao S., 2011, 2011 IEEE Power and Energy Society General Meeting, P1
   Shun-Yun Hu, 2011, 2011 5th IEEE International Conference on Self-Adaptive and Self-Organizing Systems (SASO), P21, DOI 10.1109/SASO.2011.13
   Tang XY, 2010, IEEE T PARALL DISTR, V21, P765, DOI 10.1109/TPDS.2009.113
   Terrano M., 2001, P GAM DEV C
   Waldo J., 2008, ACM QUEUE, V51, P38
   Yahyavi A, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2522968.2522977
   Yahyavi A, 2013, MULTIMEDIA SYST, V19, P255, DOI 10.1007/s00530-012-0275-z
   Yu A., 2005, Proceedings of the 15th International Workshop on Network and Operating Systems Support for Digital Audio and Video. NOSSDAV 2005, P99, DOI 10.1145/1065983.1066007
   Zhang KW, 2011, SYM REL DIST SYST, P31, DOI 10.1109/SRDS.2011.13
   Zhang L, 2011, INT CON DISTR COMP S, P203, DOI 10.1109/ICDCS.2011.24
NR 49
TC 6
Z9 6
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2015
VL 12
IS 1
AR 8
DI 10.1145/2764463
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CP9LG
UT WOS:000360215200008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Baik, E
   Pande, A
   Mohapatra, P
AF Baik, Eilwoo
   Pande, Amit
   Mohapatra, Prasant
TI Efficient MAC for Real-Time Video Streaming over Wireless LAN
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Performance; IEEE 802.11 standards; video transmission; wireless
   LAN
ID TRANSMISSION; 802.11E
AB Wireless communication systems are highly prone to channel errors. With video being a major player in Internet traffic and undergoing exponential growth in wireless domain, we argue for the need of a Video-aware MAC (VMAC) to significantly improve the throughput and delay performance of real-time video streaming service. VMAC makes two changes to optimize wireless LAN for video traffic: (a) It incorporates a Perceptual-Error-Tolerance (PET) to the MAC frames by reducing MAC retransmissions while minimizing any impact on perceptual video quality; and (b) It uses a group NACK-based Adaptive Window (NAW) of MAC frames to improve both throughput and delay performance in varying channel conditions. Through simulations and experiments, we observe 56-89% improvement in throughput and 34-48% improvement in delay performance over legacy DCF and 802.11e schemes. VMAC also shows 15-78% improvement over legacy schemes with multiple clients.
C1 [Baik, Eilwoo; Pande, Amit; Mohapatra, Prasant] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.
C3 University of California System; University of California Davis
RP Baik, E (corresponding author), Univ Calif Davis, Dept Comp Sci, One Shields Ave, Davis, CA 95616 USA.
EM ebaik@ucdavis.edu
CR [Anonymous], 2003, H.264 and MPEG-4 video compression: video coding for next generation multimedia
   [Anonymous], 2000, BUT CON MUS
   [Anonymous], P INT C INF NETW ICO
   BALAN A, 2003, P 10 IEEE INT C IM P
   Borella MS, 1998, PROCEEDINGS OF THE 1998 ICPP WORKSHOPS ON ARCHITECTURAL AND OS SUPPORT FOR MULTIMEDIA APPLICATIONS - FLEXIBLE COMMUNICATION SYSTEMS - WIRELESS NETWORKS AND MOBILE COMPUTING, P3, DOI 10.1109/ICPPW.1998.721868
   Bucciol P, 2007, ADV MULTIMED, V2007, DOI 10.1155/2007/13969
   Cabral Orlando, 2009, IAENG International Journal of Computer Science, V36, P85
   Chan A., 2010, 2010 Proceedings IEEE INFOCOM, P1, DOI DOI 10.1109/INFCOM.2010.5461979
   CiscoSystem, 2011, CISC VIS NETW IND GL
   Doufexi A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, P1196
   Dua A, 2007, GLOB TELECOMM CONF, P5226
   Feamster Nick, 2002, P 12 INT PACK VID WO, P9
   Gangadharan D, 2011, IEEE INT CONF EMBED, P319, DOI 10.1109/RTCSA.2011.49
   Hiertz G., 2005, Wireless Conference 2005-Next Generation Wireless and Mobile Communications and Services (European Wireless), 11th European, P1
   Huszak A., 2010, COMMUNICATIONS ICC 2, P1, DOI DOI 10.1109/ISCCSP.2010.5463469
   Inan I, 2006, IEEE ICC, P5263
   James She, 2011, GES J COMPUTER SCI T, V2, P31
   Klaue J, 2003, LECT NOTES COMPUT SC, V2794, P255, DOI 10.1007/978-3-540-45232-4_16
   Ksentini A, 2006, IEEE COMMUN MAG, V44, P107, DOI 10.1109/MCOM.2006.1580940
   Li TJ, 2006, WIREL COMMUN MOB COM, V6, P877, DOI 10.1002/wcm.447
   MACKENZIE R, 2009, P IEEE 20 INT S PERS, P1173
   Majkowski J, 2006, SOFTCOM 2006: INTERNATIONAL CONFERENCE ON SOFTWARE, TELECOMMUNICATIONS AND COMPUTER NETWORKS, P66
   Moorthy A., 2012, IEEE J-STSP, V6, P6
   Pavon JD, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, P1108
   Roshan P., 2004, 802.11 wireless LAN fundamentals
   Sen S, 2010, ACM SIGCOMM COMP COM, V40, P15, DOI 10.1145/1851275.1851187
   Sendra S., 2011, International Journal on Advances in Networks and Services, V4, P209
   Shujun Bi, 2012, 2012 Fourth International Conference on Computational and Information Sciences (ICCIS), P849, DOI 10.1109/ICCIS.2012.278
   Vasilios A, 2006, 4 INT S MOD OPT MOB, P1
NR 29
TC 2
Z9 2
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2015
VL 11
IS 4
AR 50
DI 10.1145/2744412
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CK0ZF
UT WOS:000355933700003
DA 2024-07-18
ER

PT J
AU Fang, Q
   Sang, JT
   Xu, CS
AF Fang, Quan
   Sang, Jitao
   Xu, Changsheng
TI Discovering Geo-Informative Attributes for Location Recognition and
   Exploration
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Geo-informative attributes;
   location recognition; latent model
ID LANDMARK; PLACE
AB This article considers the problem of automatically discovering geo-informative attributes for location recognition and exploration. The attributes are expected to be both discriminative and representative, which correspond to certain distinctive visual patterns and associate with semantic interpretations. For our solution, we analyze the attribute at the region level. Each segmented region in the training set is assigned a binary latent variable indicating its discriminative capability. A latent learning framework is proposed for discriminative region detection and geo-informative attribute discovery. Moreover, we use user-generated content to obtain the semantic interpretation for the discovered visual attributes. Discriminative and search-based attribute annotation methods are developed for geo-informative attribute interpretation. The proposed approach is evaluated on one challenging dataset including GoogleStreetView and Flickr photos. Experimental results show that (1) geo-informative attributes are discriminative and useful for location recognition; (2) the discovered semantic interpretation is meaningful and can be exploited for further location exploration.
C1 Chinese Acad Sci, Beijing 100190, Peoples R China.
   China Singapore Inst Digital Media, Singapore, Singapore.
C3 Chinese Academy of Sciences
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM qfang@nlpr.ia.ac.cn; jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61303176, 61272256];
   Beijing Natural Science Foundation [4131004]; Singapore National
   Research Foundation under its International Research Centre @ Singapore
   Funding Initiative
FX This work is supported in part by the National Basic Research Program of
   China (No. 2012CB316304), National Natural Science Foundation of China
   (No. 61225009, 61303176, 61272256), and Beijing Natural Science
   Foundation (No. 4131004). This work is also supported by the Singapore
   National Research Foundation under its International Research Centre @
   Singapore Funding Initiative and administered by the IDM Programme
   Office.
CR [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], P C ACM MULT
   [Anonymous], P 19 INT C MULT SCOT
   [Anonymous], 2009, P 17 ACM INT C MULTI
   [Anonymous], 2013, PROC THE 21 ACM INT
   [Anonymous], RR7665 INRIA
   [Anonymous], 2008, P 2008 IEEE C COMP V
   [Anonymous], P INT C MULTIMEDIA M
   [Anonymous], 2012, ACM T GRAPHIC, DOI DOI 10.1145/2185520.2185597
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], CTUCMP201116 CZECH T
   [Anonymous], P IEEE COMP VIS PATT
   Chen CY, 2011, PROC CVPR IEEE, P1569, DOI 10.1109/CVPR.2011.5995412
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Do Trinh-Minh-Tri., 2009, ICML '09 Proceedings of the 26th Annual International Conference on Machine Learning, P265
   Duan K, 2012, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2012.6248089
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Friedland Gerald., 2010, P 18 ACM INT C MULTI, P1245, DOI DOI 10.1145/1873951.1874197
   Hao Qiang., 2009, Proceedings of the ACM International Conference on Multimedia, P801
   Hays J, 2008, PROC CVPR IEEE, P3436
   Hollenstein L, 2010, J SPAT INT SCI, P21, DOI 10.5311/JOSIS.2010.1.3
   Jaffe A., 2006, MULTIMEDIA INFORM RE, P89
   Kalogerakis E, 2009, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2009.5459259
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   Li XW, 2008, LECT NOTES COMPUT SC, V5302, P427
   Li YP, 2009, IEEE I CONF COMP VIS, P1957, DOI 10.1109/ICCV.2009.5459432
   Lin TY, 2013, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2013.120
   Liu H., 2012, Proceedings_of_the_20th_ACM_ International_Conference_on_Multimedia, MM'12, P9
   Luo JB, 2011, MULTIMED TOOLS APPL, V51, P187, DOI 10.1007/s11042-010-0623-y
   Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467
   Papadopoulos S., 2010, Proceedings of the International Conference on Multimedia, P1617
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Parizi SN, 2012, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2012.6248001
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Rattenbury T, 2009, ACM T WEB, V3, DOI 10.1145/1462148.1462149
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Serdyukov P, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P484, DOI 10.1145/1571941.1572025
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Xiao X, 2012, IEEE T MULTIMEDIA, V14, P1246, DOI 10.1109/TMM.2012.2190384
   Yu T., 2009, P 26 ANN INT C MACH, P1169, DOI DOI 10.1145/1553374.1553523
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zha ZJ, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823747
   Zheng YT, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168770
   Zheng YT, 2011, MULTIMED TOOLS APPL, V51, P77, DOI 10.1007/s11042-010-0630-z
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
   Zhu Jianke., 2008, P 16 ACM INT C MULTI, P41
NR 47
TC 7
Z9 8
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2014
VL 11
IS 1
SU S
SI SI
AR 19
DI 10.1145/2648581
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS0RG
UT WOS:000343984800011
DA 2024-07-18
ER

PT J
AU Bhatnagar, G
   Wu, QMJ
   Atrey, PK
AF Bhatnagar, Gaurav
   Wu, Q. M. Jonathan
   Atrey, Pradeep K.
TI Secure Randomized Image Watermarking Based on Singular Value
   Decomposition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Verification; Digital watermarking; reversible random
   extension transform; chaotic map; wavelet frame transform; automatic
   thresholding; singular value decomposition
ID DISCRETE WAVELET TRANSFORM; SCHEME
AB In this article, a novel logo watermarking scheme is proposed based on wavelet frame transform, singular value decomposition and automatic thresholding. The proposed scheme essentially rectifies the ambiguity problem in the SVD-based watermarking. The core idea is to randomly upscale the size of host image using reversible random extension transform followed by the embedding of logo watermark in the wavelet frame domain. After embedding, a verification phase is casted with the help of a binary watermark and toral automorphism. At the extraction end, the binary watermark is first extracted followed by the verification of watermarked image. The logo watermark is extracted if and only if the watermarked image is verified. The security, attack and comparative analysis confirm high security, efficiency and robustness of the proposed watermarking system.
   Categories and Subject Descriptors: H.4.3 [Information Systems Applications]: Communications Applications
C1 [Bhatnagar, Gaurav] Indian Inst Technol, Jodhpur, Rajasthan, India.
   [Wu, Q. M. Jonathan] Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
   [Atrey, Pradeep K.] Univ Winnipeg, Dept Appl Comp Sci, Winnipeg, MB R3B 2E9, Canada.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Jodhpur; University of Windsor; University of
   Winnipeg
RP Bhatnagar, G (corresponding author), Indian Inst Technol, Jodhpur, Rajasthan, India.
EM goravb@iitj.ac.in
RI Wu, Q.M.Jonathan/O-3234-2017; Bhatnagar, Gaurav/O-5817-2019
OI Bhatnagar, Gaurav/0000-0002-0282-3372; Wu, Q.M.
   Jonathan/0000-0002-5208-7975
FU Canada Chair Research Program; Natural Sciences and Engineering Research
   Council of Canada
FX This work was supported in part by the Canada Chair Research Program and
   the Natural Sciences and Engineering Research Council of Canada.
CR [Anonymous], P IEEE INT C IM PROC
   [Anonymous], 1883, J SCI MILITAIRES
   Awrangjeb Mohammad, 2008, 2008 IEEE 10th Workshop on Multimedia Signal Processing (MMSP), P713, DOI 10.1109/MMSP.2008.4665168
   Awrangjeb M, 2012, ADV MULTIMED, V2012, DOI 10.1155/2012/875759
   Bas P, 2002, IEEE T IMAGE PROCESS, V11, P1014, DOI 10.1109/TIP.2002.801587
   Bazi Y, 2007, PATTERN RECOGN, V40, P619, DOI 10.1016/j.patcog.2006.05.006
   Bhatnagar G, 2010, INT J WAVELETS MULTI, V8, P225, DOI 10.1142/S021969131000347X
   Bhatnagar G, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P1, DOI 10.1109/ICVGIP.2008.15
   Bhatnagar G, 2009, COMPUT STAND INTER, V31, P1002, DOI 10.1016/j.csi.2008.09.031
   Briassouli A, 2004, IEEE T CIRC SYST VID, V14, P1308, DOI 10.1109/TCSVT.2004.836753
   Cui LH, 2011, IEEE T IMAGE PROCESS, V20, P1047, DOI 10.1109/TIP.2010.2079551
   de Albuquerque MP, 2004, PATTERN RECOGN LETT, V25, P1059, DOI 10.1016/j.patrec.2004.03.003
   Ganic E, 2005, J ELECTRON IMAGING, V14, DOI 10.1117/1.2137650
   Jain C., 2008, RELIABLE SVD BASED W, P1
   Lai CC, 2010, IEEE T INSTRUM MEAS, V59, P3060, DOI 10.1109/TIM.2010.2066770
   Lee WB, 2002, J SYST SOFTWARE, V62, P195, DOI 10.1016/S0164-1212(01)00142-X
   Lin CY, 2001, IEEE T IMAGE PROCESS, V10, P767, DOI 10.1109/83.918569
   Lin PY, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000489
   Lin PY, 2009, IEEE T CIRC SYST VID, V19, P1169, DOI 10.1109/TCSVT.2009.2020263
   Liu RZ, 2002, IEEE T MULTIMEDIA, V4, P121, DOI 10.1109/6046.985560
   Mohanty SP, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1413862.1413865
   Sahoo PK, 2006, PATTERN RECOGN LETT, V27, P520, DOI 10.1016/j.patrec.2005.09.017
   Sang J, 2008, IEEE T INSTRUM MEAS, V57, P595, DOI 10.1109/TIM.2007.911585
   Wu YD, 2005, IEEE T MULTIMEDIA, V7, P624, DOI 10.1109/TMM.2005.846774
   Yavuz E, 2007, APPLIED COMPUTING 2007, VOL 1 AND 2, P1051, DOI 10.1145/1244002.1244232
   You XG, 2010, IEEE T IMAGE PROCESS, V19, P3271, DOI 10.1109/TIP.2010.2055570
   Zhang XP, 2005, IEEE T MULTIMEDIA, V7, P593, DOI 10.1109/TMM.2005.843357
   Zheng D, 2003, IEEE T CIRC SYST VID, V13, P753, DOI 10.1109/TCSVT.2003.815959
   Zhu XL, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240143
NR 29
TC 22
Z9 22
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2013
VL 10
IS 1
AR 4
DI 10.1145/2542205.2542207
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 280JP
UT WOS:000329025400004
DA 2024-07-18
ER

PT J
AU Armitage, G
   Heyde, A
AF Armitage, Grenville
   Heyde, Amiel
TI REED: Optimizing First Person Shooter Game Server Discovery using
   Network Coordinates
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurement; Performance; internet protocol; home networks; server
   discovery; network coordinates; latency estimation; search optimization;
   online games; first person shooter
AB Online First Person Shooter (FPS) games typically use a client-server communication model, with thousands of enthusiast-hosted game servers active at any time. Traditional FPS server discovery may take minutes, as clients create thousands of short-lived packet flows while probing all available servers to find a selection of game servers with tolerable round trip time (RTT). REED reduces a client's probing time and network traffic to 1% of traditional server discovery. REED game servers participate in a centralized, incremental calculation of their network coordinates, and clients use these coordinates to expedite the discovery of servers with low RTTs.
C1 [Armitage, Grenville; Heyde, Amiel] Swinburne Univ Technol, Ctr Adv Internet Architectures, Hawthorn, Vic 3122, Australia.
C3 Swinburne University of Technology
RP Armitage, G (corresponding author), Swinburne Univ Technol, Ctr Adv Internet Architectures, Hawthorn, Vic 3122, Australia.
EM garmitage@swin.edu.au
CR AGARWAL S, 2009, P ACM SIGCOMM C DAT, P315
   ARMITAGE G., 2008, P INT FED INF PROC C
   ARMITAGE G., 2006, P AUSTR TEL NETW APP
   ARMITAGE G., 2008, P ACM INT WORKSH NET
   Armitage Grenville.J., 2003, P 11 IEEE INT C NETW
   BEIGBEDER T, 2003, P ACM SIGCOMM 3 WORK, P144
   CHAMBERS C., 2003, P ACM MULT
   CLAYPOOL M., 2008, P ANN MULT COMP NETW
   DABEK F, 2004, P 2004 C APPL TECHN, P15
   FENG W.-C., 2003, P 2 WORKSH NETW SYST
   Francis P, 2001, IEEE ACM T NETWORK, V9, P525, DOI 10.1109/90.958323
   GRIWODZ C., 2009, P ANN MULT COMP NETW
   Gummadi KP, 2002, IMW 2002: PROCEEDINGS OF THE SECOND INTERNET MEASUREMENT WORKSHOP, P5, DOI 10.1145/637201.637203
   Henderson T, 2002, P 1 WORKSH NETW SYST, P47
   HOBEROCK J., 2010, THRUST C TEMPLATE LI
   Ledlie Jonathan., 2006, ICDCS '06, P74
   Lee Y-S, 2009, P 9 PASS ACT NETW ME, P41
   Lua EngKeong., 2005, IMC '05, P11
   Ng ISE, 2002, IEEE INFOCOM SER, P170, DOI 10.1109/INFCOM.2002.1019258
   Ng TSE, 2001, IMW 2001: PROCEEDINGS OF THE FIRST ACM SIGCOMM INTERNET MEASUREMENT WORKSHOP, P25
   PLANETLAB, 2008, PLAN OP PLATF DEV DE
   VALVE CORPORATION, 2009, COUNT SOURC
   VALVE CORPORATION, 2009, SERV QUER
   VALVE CORPORATION, 2009, WELC STEAM
   VALVECORPORATION, 2009, MAST SERV QUER PROT
   WONG B, 2005, P SIGCOMM, P85
   ZANDER S., 2005, P 4 WORKSH NETW SYST
NR 27
TC 7
Z9 7
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2012
VL 8
IS 2
AR 20
DI 10.1145/2168996.2169000
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 945BX
UT WOS:000304248900004
DA 2024-07-18
ER

PT J
AU Hsu, CH
   Hefeeda, M
AF Hsu, Cheng-Hsin
   Hefeeda, Mohamed
TI A Framework for Cross-Layer Optimization of Video Streaming in Wireless
   Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Wireless networks; video optimization; cross layer design;
   effective airtime; WLAN; WiMAX
ID RATE-DISTORTION ANALYSIS; PERFORMANCE ANALYSIS; RESOURCE-ALLOCATION;
   TRANSMISSION; COMMUNICATION; CONSTRAINTS; CHANNELS; QUALITY; SERVICE;
   SUPPORT
AB We present a general framework for optimizing the quality of video streaming in wireless networks that are composed of multiple wireless stations. The framework is general because: (i) it can be applied to different wireless networks, such as IEEE 802.11e WLAN and IEEE 802.16 WiMAX, (ii) it can employ different objective functions for the optimization, and (iii) it can adopt various models for the wireless channel, the link layer, and the distortion of the video streams in the application layer. The optimization framework controls parameters in different layers to optimally allocate the wireless network resources among all stations. More specifically, we address this video optimization problem in two steps. First, we formulate an abstract optimization problem for video streaming in wireless networks in general. This formulation exposes the important interaction between parameters belonging to different layers in the network stack. Then, we instantiate and solve the general problem for the recent IEEE 802.11e WLANs, which support prioritized traffic classes. We show how the calculated optimal solutions can efficiently be implemented in the distributed mode of the IEEE 802.11e standard. We evaluate our proposed solution using extensive simulations in the OPNET simulator, which captures most features of realistic wireless networks. In addition, to show the practicality of our solution, we have implemented it in the driver of an off-the-shelf wireless adapter that complies with the IEEE 802.11e standard. Our experimental and simulation results show that significant quality improvement in video streams can be achieved using our solution, without incurring any significant communication or computational overhead. We also explain how the general video optimization problem can be applied to other wireless networks, in particular, to the IEEE 802.16 WiMAX networks, which are becoming very popular.
C1 [Hsu, Cheng-Hsin; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Hsu, CH (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM mhefeeda@cs.sfu.ca
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada and in part by the British Columbia
   Innovation Council.
CR [Anonymous], 1996, WIRELESS COMMUNICATI
   [Anonymous], 2016, IEEE Standard 802.11-2020
   [Anonymous], 2005, 80211 IEEE
   CHENG W, 2006, P IEEE INT C IM PROC
   CHOU C, 2005, P IEEE INFOCOM
   Cicconetti C, 2006, IEEE NETWORK, V20, P50, DOI 10.1109/MNET.2006.1607896
   DANI J, 2005, P IEEE GLOB TEL C GL
   Eisenberg Y, 2002, IEEE T CIRC SYST VID, V12, P411, DOI 10.1109/TCSVT.2002.800309
   Gao DY, 2005, IEEE NETWORK, V19, P6, DOI 10.1109/MNET.2005.1470677
   Ge Y, 2007, COMPUT NETW, V51, P1955, DOI 10.1016/j.comnet.2006.07.018
   Ghosh A, 2005, IEEE COMMUN MAG, V43, P129, DOI 10.1109/MCOM.2005.1391513
   Haratcherev I, 2006, IEEE COMMUN MAG, V44, P115, DOI 10.1109/MCOM.2006.1580941
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   He ZH, 2005, IEEE T CIRC SYST VID, V15, P645, DOI 10.1109/TCSVT.2005.846433
   He ZH, 2006, IEEE T CIRC SYST VID, V16, P590, DOI 10.1109/TCSVT.2006.873154
   Hsu C, 2009, P ACM SPIE MULT COMP
   HUANG J, 2006, P INT PACK VID WORKS
   Hui J, 2005, IEEE T COMMUN, V53, P1498, DOI 10.1109/TCOMM.2005.855013
   Katsaggelos AK, 2005, P IEEE, V93, P135, DOI 10.1109/JPROC.2004.839621
   Khan S, 2006, IEEE COMMUN MAG, V44, P122, DOI 10.1109/MCOM.2006.1580942
   Kim H, 2007, COMPUT NETW, V51, P1922, DOI 10.1016/j.comnet.2006.07.017
   Lu XA, 2003, IEEE J SEL AREA COMM, V21, P1738, DOI 10.1109/JSAC.2003.815682
   Ma Z, 2008, INT CONF ACOUST SPEE, P1125
   N SS, 2007, IEEE T VEH TECHNOL, V56, P2346, DOI 10.1109/TVT.2007.897646
   Ni Q, 2005, IEEE NETWORK, V19, P21, DOI 10.1109/MNET.2005.1470679
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   van der Schaar M, 2006, IEEE T MOBILE COMPUT, V5, P755, DOI 10.1109/TMC.2006.81
   WANG Y, 2001, VIDEO PROCESSINGS CO
   WU D, 2007, P SPIE INT C VIS COM
   Wu DP, 2003, IEEE T WIREL COMMUN, V2, P630, DOI 10.1109/TWC.2003.814353
   Zhai F, 2005, IEEE T MULTIMEDIA, V7, P716, DOI 10.1109/TMM.2005.850989
NR 33
TC 10
Z9 14
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2011
VL 7
IS 1
AR 5
DI 10.1145/1870121.1870126
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 712GI
UT WOS:000286653800005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Friedland, G
   Yeo, C
   Hung, H
AF Friedland, Gerald
   Yeo, Chuohao
   Hung, Hayley
TI Dialocalization: Acoustic Speaker Diarization and Visual Localization as
   Joint Optimization Problem
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Speech; visual localization; speaker diarization;
   multimodal integration
ID SPEECH; TIME
AB The following article presents a novel audio-visual approach for unsupervised speaker localization in both time and space and systematically analyzes its unique properties. Using recordings from a single, low-resolution room overview camera and a single far-field microphone, a state-of-the-art audio-only speaker diarization system (speaker localization in time) is extended so that both acoustic and visual models are estimated as part of a joint unsupervised optimization problem. The speaker diarization system first automatically determines the speech regions and estimates "who spoke when," then, in a second step, the visual models are used to infer the location of the speakers in the video. We call this process "dialocalization." The experiments were performed on real-world meetings using 4.5 hours of the publicly available AMI meeting corpus. The proposed system is able to exploit audio-visual integration to not only improve the accuracy of a state-of-the-art (audio-only) speaker diarization, but also adds visual speaker localization at little incremental engineering and computation costs. The combined algorithm has different properties, such as increased robustness, that cannot be observed in algorithms based on single modalities. The article describes the algorithm, presents benchmarking results, explains its properties, and systematically discusses the contributions of each modality.
C1 [Friedland, Gerald] Int Comp Sci Inst, Berkeley, CA 94704 USA.
   [Yeo, Chuohao] Inst Infocomm Res, Singapore 138632, Singapore.
   [Hung, Hayley] IDIAP Res Inst, CH-1920 Martigny, Switzerland.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R)
RP Friedland, G (corresponding author), Int Comp Sci Inst, 1947 Ctr St,Suite 600, Berkeley, CA 94704 USA.
EM fractor@icsi.berkeley.edu
RI Hung, Hayley/AAS-2215-2021
OI Hung, Hayley/0000-0003-0719-8948
FU Swiss IM-2; EU; A*STAR
FX H. Hung and G. Friedland were supported by the Swiss IM-2 and the
   EU-funded AMIDA project. C. Yeo was supported by an A*STAR fellowship.
CR ADAMI A, 2002, P 7 INT C SPOK LANG, P4
   [Anonymous], 2003, H.264 and MPEG-4 video compression: video coding for next generation multimedia
   Ba SO, 2009, IEEE T SYST MAN CY B, V39, P16, DOI 10.1109/TSMCB.2008.927274
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   BEYMER D, 1997, P IEEE INT C COMP VI
   Boakye K, 2008, INT CONF ACOUST SPEE, P4353, DOI 10.1109/ICASSP.2008.4518619
   CAMPBELL N, 2006, WORKING VERY SPARSE
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   CHEN SS, 1998, P DARPA SPEECH REC W
   CHIEN SY, 2001, P IEEE INT C MULT EX, P239
   Fisher J.W., 2000, Neural Information Processing Systems (NIPS), V13, P772
   Fisher JW, 2004, IEEE T MULTIMEDIA, V6, P406, DOI 10.1109/TMM.2004.827503
   Friedland G, 2009, P 17 ACM INT C MULT, P195
   Friedland G, 2007, INT J SEMANT COMPUT, V1, P221, DOI 10.1142/S1793351X07000123
   Friedland G, 2009, IEEE T AUDIO SPEECH, V17, P985, DOI 10.1109/TASL.2009.2015089
   Friedland G, 2009, INT CONF ACOUST SPEE, P4069, DOI 10.1109/ICASSP.2009.4960522
   GARAU G, 2009, P ACM INT C MULT ACM, P681
   Gatica-Perez D, 2007, IEEE T AUDIO SPEECH, V15, P601, DOI 10.1109/TASL.2006.881678
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   HERSHENSON M, 1962, J EXP PSYCHOL, V63, P289, DOI 10.1037/h0039516
   HOSPEDALES T, 2008, THESIS U EDINBURGH
   HUIJBREGTS M, 2008, SEGMENTATION DIARIZA
   HUNG H, 2008, P WORKSH MULT MULT S
   HUNG H, 2010, IEEE T AUDI IN PRESS
   HUNG H, 2008, P IEEE INT C AC SPEE, P835
   HUYNH BL, 2008, THESIS ECOLE POLYTEC
   Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   McKenna SJ, 1998, PATTERN RECOGN, V31, P1883, DOI 10.1016/S0031-3203(98)00066-1
   McNeill D., 2000, LANGUAGE GESTURE
   Mermelstein P., 1976, 1976 Joint Workshop on Pattern Recognition and Artificial Intelligence
   MISRA H, 2003, P IEEE INT C AC SPEE, P1
   NOCK HJ, 2003, P INT C IM VID RETR, P488
   Noulas AK, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P350
   Pardo JM, 2007, IEEE T COMPUT, V56, P1212, DOI 10.1109/TC.2007.1077
   Patterson EK, 2002, INT CONF ACOUST SPEE, P2017
   RAO R, 1996, P INT PICT COD S
   Rao RR, 1996, INT CONF ACOUST SPEE, P2056, DOI 10.1109/ICASSP.1996.545722
   REIDSMA D, 2008, THESIS U TWENTE
   Reynolds DA, 2005, INT CONF ACOUST SPEE, P953
   SIMON M, 2001, ROBOCUP 2000 ROBOT S, V4, P239
   SIRACUSA M, 2007, P IEEE INT C AC SPEE, V2, P457
   TAMURA S, 2004, REAL WORLD SPEECH PR
   Vajaria H, 2008, IEEE T CIRC SYST VID, V18, P1608, DOI 10.1109/TCSVT.2008.2005602
   Vajaria H, 2006, INT C PATT RECOG, P1150
   WOOTERS C, 2007, P RICH TRANSCR M REC
   YEO C, 2008, UCBEECS200879
   ZHANG C, 2006, P IEEE INT WORKSH MU
NR 48
TC 11
Z9 13
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2010
VL 6
IS 4
AR 27
DI 10.1145/1865106.1865111
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 688VA
UT WOS:000284880900005
DA 2024-07-18
ER

PT J
AU Cesar, P
   Bulterman, DCA
   Jansen, J
   Geerts, D
   Knoche, H
   Seager, W
AF Cesar, Pablo
   Bulterman, Dick C. A.
   Jansen, Jack
   Geerts, David
   Knoche, Hendrik
   Seager, William
TI Fragment, Tag, Enrich, and Send: Enhancing Social Sharing of Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Documentation; Experimentation; Languages; Asynchronous media
   sharing; differentiated content enrichment; secondary screens
AB The migration of media consumption to personal computers retains distributed social viewing, but only via nonsocial, strictly personal interfaces. This article presents an architecture, and implementation for media sharing that allows for enhanced social interactions among users. Using a mixed-device model, our work allows targeted, personalized enrichment of content. All recipients see common content, while differentiated content is delivered to individuals via their personal secondary screens. We describe the goals, architecture, and implementation of our system in this article. In order to validate our results, we also present results from two user studies involving disjoint sets of test participants.
C1 [Cesar, Pablo; Bulterman, Dick C. A.; Jansen, Jack] Ctr Wiskunde & Informat, Amsterdam, Netherlands.
   [Geerts, David] Ctr Usabil Res, Louvain, Belgium.
   [Knoche, Hendrik; Seager, William] UCL, London, England.
C3 University of London; University College London
RP Cesar, P (corresponding author), Ctr Wiskunde & Informat, Amsterdam, Netherlands.
EM p.s.cesar@cwi.nl
RI Jansen, Jack/KHZ-0382-2024; Knoche, Hendrik/AAD-4754-2019
OI Jansen, Jack/0000-0002-7006-2560; Knoche, Hendrik/0000-0003-3950-8453;
   Geerts, David/0000-0003-3933-9266; Cesar, Pablo/0000-0003-1752-6837
CR Abowd G.D., 2003, Proceedings of the 5th ACM SIGMM international workshop on Multimedia information retrieval, P1, DOI DOI 10.1145/973264.973266
   Adams MJ, 2005, PROTEIN REV, V1, P3
   [Anonymous], P ACM S DOC ENG
   [Anonymous], P SIGCHI C HUM FACT
   [Anonymous], 2007, IMC 07 P 2007 ACM SI, DOI DOI 10.1145/1298306.1298310
   Bultennan DCA, 2008, SMIL 3 0 INTERACTIVE
   Bulterman DCA, 2005, ACM T MULTIM COMPUT, V1, P89, DOI 10.1145/1047936.1047943
   Cattelan RG, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1412196.1412201
   Cesar P., 2008, Proceedings of the 16th ACM international conference on Multimedia (MM '08), P11, DOI DOI 10.1145/1459359.1459362
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chorianopoulos K, 2007, J COMMUN NETW-UK, V6, P23
   COSTA RMR, 2006, P ACM S DOC ENG, P165
   DEARMAN D, 2008, P SIGCHI C HUM FACT, P767
   Halvey M.J., 2007, WWW 07, P1273, DOI DOI 10.1145/1242572.1242804
   HESSELMAN C, 2008, P INT C AMB MED SYST
   HUA XS, 2006, P 1 ACM INT WORKSH H, P65
   HUANG EM, 2009, P 27 INT C HUM FACT, P585
   Jokela T, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P63
   KINDBERG T, 2005, HUM FACT COMP SYST, P1545
   KIRK D, 2007, C HUM FACT COMP, P61
   Kosch H, 2005, IEEE MULTIMEDIA, V12, P80, DOI 10.1109/MMUL.2005.13
   Liu J., 2007, Proceedings of the 15th international conference on Multimedia, MULTIMEDIA '07, P541
   Marlow C., 2006, P 17 C HYP HYPERTEXT, P31, DOI [https://doi.org/10.1145/1149941.1149949, DOI 10.1145/1149941.1149949]
   Metcalf C, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1412196.1412200
   Nathan M., 2008, P UXTV 2008, P85
   Pea R, 2004, IEEE MULTIMEDIA, V11, P54, DOI 10.1109/MMUL.2004.1261108
   PFEIFFER S, 2003, P 5 ACM SIGMM INT WO, P87
   RUTLEDGE L, 2001, P INT C MULT MOD 200, P147
   SGOUROS NM, 2007, P ACM SIGMM WORKSH H, P41
   Shamma DA., 2007, P INT WORKSHOP WORKS, P275, DOI [10.1145/1290082.1290120, DOI 10.1145/1290082.1290120]
   SHAW R, 2006, P 1 ACM INT WORKSH H, P89
   Shipman F, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1413862.1413868
   Stamou G, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.15
   Taylor A.S., 2002, P SIGCHI C HUMAN FAC, P439
   VANHOUSE N, 2005, HUM FACT COMP SYST, P1853
NR 35
TC 14
Z9 16
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2009
VL 5
IS 3
AR 19
DI 10.1145/1556134.1556136
PG 27
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 504DV
UT WOS:000270595600002
DA 2024-07-18
ER

PT J
AU Liu, TC
   Kender, JR
AF Liu, Tiecheng
   Kender, John R.
TI Computational approaches to temporal sampling of video sequences
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; video summarization; key frame selection; video content
   analysis; ubiquitous media access; temporal video sampling
ID EXTRACTION; RETRIEVAL; ALGORITHM; SCHEME
AB Video key frame extraction is one of the most important research problems for video summarization, indexing, and retrieval. For a variety of applications such as ubiquitous media access and video streaming, the temporal boundaries between video key frames are required for synchronizing visual content with audio. In this article, we define temporal video sampling as a unified process of extracting video key frames and computing their temporal boundaries, and formulate it as an optimization problem. We first provide an optimal approach that minimizes temporal video sampling error using a dynamic programming process. The optimal approach retrieves a key frame hierarchy and all temporal boundaries in 0(n(4)) time and O(n(2)) space. To further reduce computational complexity, we also provide a suboptimal greedy algorithm that exploits the data structure of a binary heap and uses a novel "look-ahead" computational technique, enabling all levels of key frames to be extracted with an average-case computational time of O(n log n) and memory usage of 0 (n). Both the optimal and the greedy methods are free of parameters, thus avoiding the threshold-selection problem that exists in other approaches. We empirically compare the proposed optimal and greedy methods with several existing methods in terms of video sampling error, computational cost, and subjective quality. An evaluation of eight videos of different genres shows that the greedy approach achieves performance very close to that of the optimal approach while drastically reducing computational cost, making it suitable for processing long video sequences in large video databases.
C1 Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
   Columbia Univ, New York, NY 10027 USA.
C3 University of South Carolina System; University of South Carolina
   Columbia; Columbia University
RP Liu, TC (corresponding author), Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
EM tiecheng@cse.sc.edu; jrk@cs.columbia.edu
CR Aner-Wolf A, 2004, COMPUT VIS IMAGE UND, V95, P201, DOI 10.1016/j.cviu.2004.03.005
   [Anonymous], 1995, PROC ICJAI, DOI DOI 10.1145/217279.215068
   Ardizzone E, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P158, DOI 10.1109/MMCS.1999.778213
   Boreczky JS, 1996, P SOC PHOTO-OPT INS, V2670, P170, DOI 10.1117/12.234794
   Chang HS, 1999, IEEE T CIRC SYST VID, V9, P1269, DOI 10.1109/76.809161
   Chang SF, 2003, 12TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P494
   Chang SF, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P313, DOI 10.1145/266180.266382
   Chiu P, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1329, DOI 10.1109/ICME.2000.871011
   CHUA TS, 1995, ACM T INFORM SYST, V13, P373, DOI 10.1145/211430.211431
   DeMenthon D., 1998, Proceedings ACM Multimedia 98, P211, DOI 10.1145/290747.290773
   Dimitrova N., 1997, Proceedings of the Sixth International Conference on Information and Knowledge Management. CIKM'97, P113, DOI 10.1145/266714.266876
   Divakaran A, 2002, IEEE IMAGE PROC, P932
   Fan JP, 2004, IEEE T IMAGE PROCESS, V13, P974, DOI 10.1109/TIP.2004.827232
   Fauvet B, 2004, LECT NOTES COMPUT SC, V3115, P419
   Ferman AM, 2003, IEEE T MULTIMEDIA, V5, P244, DOI 10.1109/TMM.2003.811617
   Girgensohn A, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P756, DOI 10.1109/MMCS.1999.779294
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   Ho YH, 2004, IEEE IMAGE PROC, P613
   Idris F, 1997, J VIS COMMUN IMAGE R, V8, P146, DOI 10.1006/jvci.1997.0355
   KENDER JR, 2000, P AS C COMP VIS
   Koh JL, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P472, DOI 10.1109/MMCS.1999.778508
   Lee HC, 2002, ELECTRON LETT, V38, P217, DOI 10.1049/el:20020112
   LEE SH, 2004, IEEE INT C MULT EXP, V2, P1099
   LIU T, 2003, INT C IM PROC, V1, P921
   LIU T, 2002, P ICIP, V1, P601
   LIU T, 2001, P IEEE C COMP VIS PA, V2, P531
   Liu TM, 2003, IEEE T CIRC SYST VID, V13, P1006, DOI 10.1109/TCSVT.2003.816521
   Ma YF, 2002, IEEE IMAGE PROC, P129
   Mandal MK, 1999, IMAGE VISION COMPUT, V17, P513, DOI 10.1016/S0262-8856(98)00143-7
   Peker KA, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P2055, DOI 10.1109/ICME.2004.1394669
   Robles-Kelly A, 2005, IEEE T PATTERN ANAL, V27, P365, DOI 10.1109/TPAMI.2005.56
   Rong JW, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P571, DOI 10.1109/ICME.2004.1394256
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   Smith MA, 1997, PROC CVPR IEEE, P775, DOI 10.1109/CVPR.1997.609414
   Smith MA, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P61, DOI 10.1109/CAIVD.1998.646034
   Sun XD, 2000, REAL-TIME IMAGING, V6, P449, DOI 10.1006/rtim.1999.0197
   Sundaram H, 2001, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P124, DOI 10.1109/IVL.2001.990866
   Teodosio L, 2005, ACM T MULTIM COMPUT, V1, P16, DOI 10.1145/1047936.1047940
   Wolf W, 1996, INT CONF ACOUST SPEE, P1228, DOI 10.1109/ICASSP.1996.543588
   YEUNG M, 1996, P INT C PATT REC ICP, VC, P375
   Yeung MM, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pA338
   ZHANG D.-Q., 2004, PROC ACM INT C MULTI, P877, DOI DOI 10.1145/1027527.1027730
   Zhang LH, 2003, PATTERN RECOGN LETT, V24, P9, DOI 10.1016/S0167-8655(02)00160-5
   Zhou XS, 2002, IEEE T CIRC SYST VID, V12, P535, DOI 10.1109/TCSVT.2002.800324
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 45
TC 23
Z9 26
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 2
AR 7
DI 10.1145/1230812.1230813
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230JA
UT WOS:000250871600001
DA 2024-07-18
ER

PT J
AU Del Bimbo, A
   Pala, P
AF Del Bimbo, Alberto
   Pala, Pietro
TI Content-based retrieval of 3D models
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE theory; design; performance; 3D shape description; retrieval by content
   of 3D models; comparative analysis
ID OBJECT RECOGNITION
AB In the past few years, there has been an increasing availability of technologies for the acquisition of digital 3D models of real objects and the consequent use of these models in a variety of applications, in medicine, engineering, and cultural heritage. In this framework, content-based retrieval of 3D objects is becoming an important subject of research, and finding adequate descriptors to capture global or local characteristics of the shape has become one of the main investigation goals. In this article, we present a comparative analysis of a few different solutions for description and retrieval by similarity of 3D models that are representative of the principal classes of approaches proposed. We have developed an experimental analysis by comparing these methods according to their robustness to deformations, the ability to capture an object's structural complexity, and the resolution at which models are considered.
C1 Univ Florence, Dipartimento Sistemi & Informat, I-50139 Florence, Italy.
C3 University of Florence
RP Del Bimbo, A (corresponding author), Univ Florence, Dipartimento Sistemi & Informat, Via S Marta 3, I-50139 Florence, Italy.
EM delbimbo@dsi.unifi.it; pala@dsi.unifi.it
OI PALA, PIETRO/0000-0001-5670-3774; DEL BIMBO, ALBERTO/0000-0002-1052-8322
CR [Anonymous], The digital Michelangelo project
   [Anonymous], 1979, COMPUT INTRACTABILIT
   [Anonymous], National Design Repository
   ANTINI G, 2005, P INT C MULT EXP ICM
   Assfalg J, 2004, INT C PATT RECOG, P906, DOI 10.1109/ICPR.2004.1334675
   ASSFALG J, 2003, P INT C MULT EXP ICM
   BELYAEV AG, 2000, P GEOM MOD PROC 2000, P229
   Berretti S, 2001, IEEE T PATTERN ANAL, V23, P1089, DOI 10.1109/34.954600
   BESL PJ, 1985, COMPUT SURV, V17, P75, DOI 10.1145/4078.4081
   Bezdek J., 1999, FUZZY MODELS ALGORIT
   Campbell RJ, 2001, COMPUT VIS IMAGE UND, V81, P166, DOI 10.1006/cviu.2000.0889
   CHEN DY, 2003, P EUR COMP GRAPH FOR, V22, P3
   Colombo C, 2005, IEEE T PATTERN ANAL, V27, P99, DOI 10.1109/TPAMI.2005.14
   Del Bimbo A, 1998, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES - PROCEEDINGS, P35, DOI 10.1109/IVL.1998.694485
   Desbrun Mathieu., 2000, Discrete differential-geometry operators in nd. preprint
   Elad M., 2001, PROC EG MULTIMEDIA, P97, DOI DOI 10.2312/EGMM/EGMM01/107-118
   ESHERA MA, 1984, IEEE T SYST MAN CYB, V14, P398, DOI 10.1109/TSMC.1984.6313232
   GARLAND M, 1999, P EUR 99 SEPT
   GOSHTASBY A, 1985, IEEE T PATTERN ANAL, V7, P738, DOI 10.1109/TPAMI.1985.4767734
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kim DJ, 2001, IEICE T INF SYST, VE84D, P281
   KOLANIAS I, 2001, P INT WORKSH CONT BA, P19
   Kriegel H.-P., 1998, GeoInformatica, V2, P113, DOI 10.1023/A:1009760031965
   Mahmoudi S, 2002, INT C PATT RECOG, P457, DOI 10.1109/ICPR.2002.1048337
   Mokhtarian F, 2001, IMAGE VISION COMPUT, V19, P271, DOI 10.1016/S0262-8856(00)00076-7
   NOVOTNI M, 2003, SOLID MODELING
   Ohbuchi R, 2003, THEORY AND PRACTICE OF COMPUTER GRAPHICS, PROCEEDINGS, P97, DOI 10.1109/TPCG.2003.1206936
   Ohbuchi R., 2003, P 5 ACM SIGMM INT WO, P39, DOI DOI 10.1145/973264.973272
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Paquet E, 1999, IMAGE VISION COMPUT, V17, P157, DOI 10.1016/S0262-8856(98)00119-X
   ROSSL C, 2000, SMART GRAPH P 2000 A
   SHILANE P, 2004, P SHAP MOD INT JUN G
   SNYDER JP, 1995, MAP PROJECTIONS REFE
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   TAUBIN G, 1995, COMPUTER GRAPHICS, V29, P351
   VANDEBORRE JP, 2002, P 1 INT S 3D DAT PRO
   VRANIC DV, 2001, P IEEE WORKSH MULT S
   Zhang C., 2001, Proc. ACM international conference on Multimedia, P615
NR 38
TC 94
Z9 100
U1 2
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2006
VL 2
IS 1
BP 20
EP 43
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IW
UT WOS:000250871200002
DA 2024-07-18
ER

PT J
AU Chen, Z
   Yang, M
   Zhang, SL
AF Chen, Zhen
   Yang, Ming
   Zhang, Shiliang
TI Complementary Coarse-to-Fine Matching for Video Object Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video object segmentation; coarse-to-fine matching; label propagation
AB Semi-supervised Video Object Segmentation (VOS) needs to establish pixel-level correspondences between a video frame and preceding segmented frames to leverage their segmentation clues. Most works rely on features at a single scale to establish those correspondences, e.g., perform densematching with Convolutional Neural Network (CNN) features from a deep layer. Differently, this work explores complementary features at different scales to pursue more robust feature matching. A coarse feature from a deep layer is first adopted to get coarse pixel-level correspondences. We hence evaluate the quality of those correspondences, and select pixels with low-quality correspondences for fine-scale feature matching. Segmentation clues of previous frames are propagated by both coarse and fine-scale correspondences, which are fused with appearance features for object segmentation. Compared with previous works, this coarse-to-fine matching scheme is more robust to distractions by similar objects and better preserves object details. The sparse fine-scale matching also ensures a fast inference speed. On popular VOS datasets including DAVIS and YouTube-VOS, the proposed method shows promising performance compared with recent works.
C1 [Chen, Zhen; Zhang, Shiliang] Peking Univ, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
   [Yang, Ming] Ant Grp, Multimodal Cognit, 525 Almanor Ave, Sunnyvale, CA 94085 USA.
C3 Peking University
RP Chen, Z (corresponding author), Peking Univ, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
EM zhen.chen@pku.edu.cn; m-yang4@u.northwestern.edu; slzhang.jdl@pku.edu.cn
RI Yang, Ming-Hsuan/T-9533-2019; Zhang, ShiLiang/AAA-4638-2020
OI Yang, Ming-Hsuan/0000-0003-4848-2304; Yang, Ming/0000-0003-1691-6817
FU Natural Science Foundation of China [U20B2052, 61936011]; National Key
   Research and Development Program of China [2018YFE0118400]
FX This work is supported in part by Natural Science Foundation of China
   under Grant No. U20B2052, 61936011, in part by The National Key Research
   and Development Program of China under Grant No. 2018YFE0118400.
CR Anguelov D., 2015, P INT C LEARN REPR W
   Bao Linchao, 2018, P IEEE C COMPUTER VI
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P777, DOI 10.1007/978-3-030-58536-5_46
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen K, 2018, PROC CVPR IEEE, P7814, DOI 10.1109/CVPR.2018.00815
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen Xi, 2020, P IEEE C COMPUTER VI
   Chen YY, 2019, ACM T SENSOR NETWORK, V15, DOI 10.1145/3342515
   Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130
   Cheng H.K., 2021, Advances in Neural Information Processing Systems, P11781
   Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981
   Duke B, 2021, PROC CVPR IEEE, P5908, DOI 10.1109/CVPR46437.2021.00585
   Ge WB, 2021, PROC CVPR IEEE, P16831, DOI 10.1109/CVPR46437.2021.01656
   Ho Kei Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8887, DOI 10.1109/CVPR42600.2020.00891
   Hu L, 2021, PROC CVPR IEEE, P4142, DOI 10.1109/CVPR46437.2021.00413
   Kaihua Zhang, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P1515, DOI 10.1145/3394171.3413942
   Lan M, 2022, AAAI CONF ARTIF INTE, P1228
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li X, 2020, PROC CVPR IEEE, P2866, DOI 10.1109/CVPR42600.2020.00294
   Li XX, 2018, LECT NOTES COMPUT SC, V11207, P93, DOI 10.1007/978-3-030-01219-9_6
   Liang Y., 2020, Adv. Neural Inf. Proces. Syst., V33, P3430
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin Zhihui, 2022, P IEEE C COMPUTER VI
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35
   Mao YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9650, DOI 10.1109/ICCV48922.2021.00953
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Park Kwanyong, 2022, P IEEE C COMPUTER VI
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Pont-Tuset J, 2018, Arxiv, DOI arXiv:1704.00675
   Robinson Andreas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7404, DOI 10.1109/CVPR42600.2020.00743
   Seong H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12869, DOI 10.1109/ICCV48922.2021.01265
   Seoung Wug Oh, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P9225, DOI 10.1109/ICCV.2019.00932
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang Haidong, 2023, ACM T MULTIM COMPUT, V19, P1
   Wang HC, 2021, PROC CVPR IEEE, P1296, DOI 10.1109/CVPR46437.2021.00135
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wu Jingjing, 2022, ACM T MULTIM COMPUT, V18, P1
   Xiankai Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P661, DOI 10.1007/978-3-030-58580-8_39
   Xie HZ, 2021, PROC CVPR IEEE, P1286, DOI 10.1109/CVPR46437.2021.00134
   Xu K, 2022, PROC CVPR IEEE, P1332, DOI 10.1109/CVPR52688.2022.00140
   Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36
   Xuhua Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8876, DOI 10.1109/CVPR42600.2020.00890
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Yang Linjie, 2019, P IEEECVF INT C COMP
   Yang ZZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446618
   Yang ZX, 2021, ADV NEUR IN, V34
   Yang ZX, 2022, IEEE T PATTERN ANAL, V44, P4701, DOI 10.1109/TPAMI.2021.3081597
   Yu F., 2015, ARXIV
   Yuan Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321512
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7233, DOI 10.1109/ICCV.2019.00733
   Zhao Dongyang, 2021, P IEEECVF INT C COMP
NR 56
TC 1
Z9 1
U1 2
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 203
DI 10.1145/3596496
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200026
DA 2024-07-18
ER

PT J
AU Fontanini, T
   Donati, L
   Bertozzi, M
   Prati, A
AF Fontanini, Tomaso
   Donati, Luca
   Bertozzi, Massimo
   Prati, Andrea
TI Unsupervised Discovery and Manipulation of Continuous Disentangled
   Factors of Variation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; unsupervised learning; mutual information maximization;
   image manipulation
ID IMAGE SYNTHESIS
AB Learning a disentangled representation of a distribution in a completely unsupervised way is a challenging task that has drawn attention recently. In particular, much focus has been put in separating factors of variation (i.e., attributes) within the latent code of a Generative Adversarial Network (GAN). Achieving that permits control of the presence or absence of those factors in the generated samples by simply editing a small portion of the latent code. Nevertheless, existing methods that perform very well in a noise-to-image setting often fail when dealing with a real data distribution, i.e., when the discovered attributes need to be applied to real images. However, some methods are able to extract and apply a style to a sample but struggle to maintain its content and identity, while others are not able to locally apply attributes and end up achieving only a global manipulation of the original image.
   In this article, we propose a completely (i.e., truly) unsupervised method that is able to extract a disentangled set of attributes from a data distribution and apply them to new samples from the same distribution by preserving their content. This is achieved by using an image-to-image GAN that maps an image and a random set of continuous attributes to a new image that includes those attributes. Indeed, these attributes are initially unknown and they are discovered during training by maximizing the mutual information between the generated samples and the attributes' vector. Finally, the obtained disentangled set of continuous attributes can be used to freely manipulate the input samples. We prove the effectiveness of our method over a series of datasets and show its application on various tasks, such as attribute editing, data augmentation, and style transfer.
C1 [Fontanini, Tomaso; Donati, Luca; Bertozzi, Massimo; Prati, Andrea] Univ Parma, Dept Engn & Architecture, Via Scienze 181-a, I-43124 Parma, Italy.
C3 University of Parma
RP Fontanini, T (corresponding author), Univ Parma, Dept Engn & Architecture, Via Scienze 181-a, I-43124 Parma, Italy.
EM tomaso.fontanini@unipr.it; luca.donati@unipr.it;
   massimo.bertozzi@unipr.it; andrea.prati@unipr.it
RI Fontanini, Tomaso/JVZ-6748-2024; Prati, Andrea/B-7440-2014
OI Fontanini, Tomaso/0000-0001-6595-4874; Prati,
   Andrea/0000-0002-1211-529X; Donati, Luca/0000-0003-0974-1211; Bertozzi,
   Massimo/0000-0003-1463-5384
FU Programme "FIL-Quota Incentivante" of University of Parma; Fondazione
   Cariparma; NVIDIA Corporation
FX This research has been financially supported by the Programme "FIL-Quota
   Incentivante" of University of Parma and co-sponsored by Fondazione
   Cariparma. We gratefully acknowledge the support of NVIDIA Corporation
   with the donation of the Quadro RTX 6000 GPU used for this research.
CR Alharbi Y, 2020, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR42600.2020.00518
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Baek K., 2021, P IEEE CVF INT C COM, P14154
   Baek K, 2021, Arxiv, DOI arXiv:2006.06500
   Brock A, 2019, Arxiv, DOI [arXiv:1809.11096, DOI 10.48550/ARXIV.1809.11096]
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chao Brian, 2019, ANIME FACE DATASET
   Chen X, 2016, Arxiv, DOI [arXiv:1606.03657, DOI 10.48550/ARXIV.1606.03657]
   Cheng JP, 2016, Arxiv, DOI [arXiv:1601.06733, DOI 10.48550/ARXIV.1601.06733]
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Fontanini T, 2020, NEURAL NETWORKS, V131, P185, DOI 10.1016/j.neunet.2020.07.031
   FRECHET M, 1957, CR HEBD ACAD SCI, V244, P689
   Goodfellow I., 2014, arXiv, DOI 10.48550/arXiv.1406.2661
   Gulrajani I, 2017, Arxiv, DOI arXiv:1704.00028
   Harkonen E., 2020, Advances in Neural Information Processing Systems, V33, P9841
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Hu QY, 2018, PROC CVPR IEEE, P3399, DOI 10.1109/CVPR.2018.00358
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ji X, 2019, IEEE I CONF COMP VIS, P9864, DOI 10.1109/ICCV.2019.00996
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu YH, 2021, PROC CVPR IEEE, P10780, DOI 10.1109/CVPR46437.2021.01064
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Luo P, 2019, Arxiv, DOI [arXiv:1806.10779, 10.48550/arXiv.1806.10779]
   Mathieu M, 2016, Arxiv, DOI arXiv:1611.03383
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mukherjee S, 2019, AAAI CONF ARTIF INTE, P4610
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Nitzan Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417826
   Parikh A.P., 2016, arXiv, DOI [10.48550/arXiv.1606.01933, 10.48550/ARXIV.1606.01933, DOI 10.48550/ARXIV.1606.01933]
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Saito K, 2020, Arxiv, DOI arXiv:2007.07431
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Szab¢ A, 2017, Arxiv, DOI arXiv:1711.02245
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wu PW, 2019, IEEE I CONF COMP VIS, P5913, DOI 10.1109/ICCV.2019.00601
   Xu YH, 2021, Arxiv, DOI arXiv:2007.10379
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zareapoor M, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3458280
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 54
TC 1
Z9 1
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
DI 10.1145/3591358
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200012
DA 2024-07-18
ER

PT J
AU Wang, ZM
   Xu, YW
   Wu, LF
   Han, H
   Ma, YK
   Li, Z
AF Wang, Zhuming
   Xu, Yaowen
   Wu, Lifang
   Han, Hu
   Ma, Yukun
   Li, Zun
TI Improving Face Anti-spoofing via Advanced Multi-perspective Feature
   Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face anti-spoofing; multi-perspective; universal cues
ID IMAGE
AB Face anti-spoofing (FAS) plays a vital role in securing face recognition systems. Previous approaches usually learn spoofing features from a single perspective, in which only universal cues shared by all attack types are explored. However, such single-perspective-based approaches ignore the differences among various attacks and commonness between certain attacks and bona fides, thus tending to neglect some non-universal cues that contain strong discernibility against certain types. As a result, when dealing with multiple types of attacks, the above approaches may suffer from the uncomprehensive representation of bona fides and spoof faces. In this work, we propose a novel Advanced Multi-Perspective Feature Learning network (AMPFL), in which multiple perspectives are adopted to learn discriminative features, to improve the performance of FAS. Specifically, the proposed network first learns universal cues and several perspective-specific cues from multiple perspectives, then aggregates the above features and further enhances them to perform face anti-spoofing. In this way, AMPFL obtains features that are difficult to be captured by single-perspective-based methods and provides more comprehensive information on bona fides and spoof faces, thus achieving better performance for FAS. Experimental results show that our AMPFL achieves promising results in public databases, and it effectively solves the issues of single-perspective-based approaches.
C1 [Wang, Zhuming; Xu, Yaowen; Wu, Lifang; Li, Zun] Beijing Univ Technol, Beijing 100124, Peoples R China.
   [Wu, Lifang] Beijing Key Lab Computat Intelligence & Intellige, Beijing, Peoples R China.
   [Han, Hu] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Han, Hu] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Han, Hu] Pengcheng Lab, Shenzhen 518055, Peoples R China.
   [Ma, Yukun] Henan Inst Sci & Technol, Xinxiang, Henan, Peoples R China.
C3 Beijing University of Technology; Chinese Academy of Sciences; Institute
   of Computing Technology, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Henan Institute of Science &
   Technology
RP Wu, LF (corresponding author), Beijing Univ Technol, Beijing 100124, Peoples R China.; Wu, LF (corresponding author), Beijing Key Lab Computat Intelligence & Intellige, Beijing, Peoples R China.
EM wzm1030@126.com; xuyao_wen@126.com; lfwu@bjut.edu.cn; hanhu@ict.ac.cn;
   yukuner@126.com; zunli@bjut.edu.cn
RI liu, zhen/KFS-0275-2024; Yang, han/KFS-2671-2024; yang,
   xiao/KHT-9445-2024; Sun, Yue/KHU-8159-2024; Chen, Yang/KHD-8849-2024;
   li, li/KHE-5750-2024; wang, nan/KHW-4897-2024; Zhang, Lu/KHE-5879-2024;
   Zhang, Yi/KHW-2039-2024; Wang, Zhuming/KFB-4433-2024; Liu,
   Zhen/KFS-2748-2024; Han, Liang/KFR-6745-2024; li, qing/KHU-6871-2024;
   li, feiyang/KHW-5210-2024; wang, jin/KHD-7243-2024; Lin,
   Fan/JZT-1441-2024
OI Lin, Fan/0000-0002-7330-3833; Wang, Zhuming/0000-0002-2230-5716
FU Natural Science Foundation of China [61976010,61732004, 62176249,
   62106010, 62176011]; China Postdoctoral Science Foundation
   [2022M720318]; Beijing Postdoctoral Science Foundation [2022-zz-077]
FX This research was supported in part by Natural Science Foundation of
   China (grants 61976010,61732004, 62176249, 62106010 and 62176011), China
   Postdoctoral Science Foundation (grant 2022M720318), and Beijing
   Postdoctoral Science Foundation (grant 2022-zz-077).
CR Agarwal A., 2016, INT CONF BIOMETR THE, P1
   [Anonymous], 2016, ISO/IEC 30107-1:2016 Information Technology-Biometric Presentation Attack Detection-Part 1: Framework
   Atoum Y, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P319, DOI 10.1109/BTAS.2017.8272713
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Boulkenafet Z, 2017, IEEE INT CONF AUTOMA, P612, DOI 10.1109/FG.2017.77
   Chingovska I., 2012, 2012 BIOSIG P INT C, P1
   de Freitas Pereira Tiago, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P121, DOI 10.1007/978-3-642-37410-4_11
   Dong Xin, 2021, OPEN SET FACE ANTISP, P4082, DOI [10.1145/3474085.3475538, DOI 10.1145/3474085.3475538]
   Erdogmus N, 2014, IEEE T INF FOREN SEC, V9, P1084, DOI 10.1109/TIFS.2014.2322255
   Feng LT, 2016, J VIS COMMUN IMAGE R, V38, P451, DOI 10.1016/j.jvcir.2016.03.019
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   George Anjith, 2019, INT C BIOM ICB, DOI DOI 10.1109/ICB45273.2019.8987370
   Guo JZ, 2020, PROC CVPR IEEE, P6162, DOI 10.1109/CVPR42600.2020.00620
   Jingtian Xia, 2020, ICCPR 2020: Proceedings of the 2020 9th International Conference on Computing and Pattern Recognition, P200, DOI 10.1145/3436369.3437404
   Jourabloo A, 2018, LECT NOTES COMPUT SC, V11217, P297, DOI 10.1007/978-3-030-01261-8_18
   Kollreider K, 2007, IEEE T INF FOREN SEC, V2, P548, DOI 10.1109/TIFS.2007.902037
   Komulainen Jukka, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P146, DOI 10.1007/978-3-642-37410-4_13
   Komulainen J, 2013, 2013 IEEE SIXTH INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS)
   Li L, 2016, INT CONF IMAG PROC
   Li Sheng, 2021, DIFFUSING LIVENESS C, P1636, DOI [10.1145/3474085.3475305, DOI 10.1145/3474085.3475305]
   Li XB, 2016, INT C PATT RECOG, P4244, DOI 10.1109/ICPR.2016.7900300
   Li YD, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3432817
   Liu SQ, 2018, LECT NOTES COMPUT SC, V11220, P577, DOI 10.1007/978-3-030-01270-0_34
   Liu SQ, 2016, IEEE COMPUT SOC CONF, P1551, DOI 10.1109/CVPRW.2016.193
   Liu SQ, 2016, LECT NOTES COMPUT SC, V9911, P85, DOI 10.1007/978-3-319-46478-7_6
   Liu YJ, 2019, PROC CVPR IEEE, P4675, DOI 10.1109/CVPR.2019.00481
   Liu YJ, 2018, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2018.00048
   Niu XS, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P642, DOI 10.1109/BTAS.2017.8272752
   Pan G, 2007, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2007.4409068
   Patel K, 2016, IEEE T INF FOREN SEC, V11, P2268, DOI 10.1109/TIFS.2016.2578288
   Pereira TD, 2013, INT CONF BIOMETR
   Shao R, 2019, IEEE T INF FOREN SEC, V14, P923, DOI 10.1109/TIFS.2018.2868230
   Shao R, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P748, DOI 10.1109/BTAS.2017.8272765
   Siddiqui TA, 2016, INT C PATT RECOG, P1035, DOI 10.1109/ICPR.2016.7899772
   Sun WY, 2020, IEEE T INF FOREN SEC, V15, P3181, DOI 10.1109/TIFS.2020.2985530
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504, DOI 10.1007/978-3-642-15567-3_37
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang ZZ, 2019, Arxiv, DOI arXiv:1811.05118
   Wang ZZ, 2020, PROC CVPR IEEE, P5041, DOI 10.1109/CVPR42600.2020.00509
   Wang ZM, 2021, IEEE INT CONF COMP V, P4099, DOI 10.1109/ICCVW54120.2021.00457
   Xu YW, 2021, 2021 INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2021), DOI 10.1109/IJCB52358.2021.9484389
   Xu ZQ, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P141, DOI 10.1109/ACPR.2015.7486482
   Yang X, 2019, PROC CVPR IEEE, P3502, DOI 10.1109/CVPR.2019.00362
   Yao CL, 2021, IEEE IMAGE PROC, P3872, DOI 10.1109/ICIP42928.2021.9506276
   Yu ZT, 2021, IEEE T PATTERN ANAL, V43, P3005, DOI 10.1109/TPAMI.2020.3036338
   Yu ZT, 2020, PROC CVPR IEEE, P5294, DOI 10.1109/CVPR42600.2020.00534
   Zhang K., 2020, ARXIV PREPRINT ARXIV
   Zhiwei Zhang, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P26, DOI 10.1109/ICB.2012.6199754
NR 48
TC 0
Z9 0
U1 3
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 212
DI 10.1145/3575660
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200035
DA 2024-07-18
ER

PT J
AU Yang, X
   Li, HR
   Li, XC
   Li, T
AF Yang, Xin
   Li, Hengrui
   Li, Xiaochuan
   Li, Tao
TI HIFGAN: A High-Frequency Information-Based Generative Adversarial
   Network for Image Super-Resolution
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Super-resolution; generative adversarial networks; high-frequency
   information; information fusion
ID CONVOLUTIONAL NETWORK
AB Since the neural network was introduced into the super-resolution (SR) field, many SR deep models have been proposed and have achieved excellent results. However, there are two main drawbacks: one is that the methods based on the best peak-signal-to-noise ratio (PSNR) do not have enough comfortable visual quality; the other is that although the SR models based on generative adversarial network (GAN) have satisfactory visual quality, the structure of the reconstructed image has apparent defects. Therefore, according to the characteristics that human eyes are sensitive to high-frequency components in images, this article proposes an improved image SRGAN model based on high-frequency information fusion (HIFGAN). It builds a feature extraction network for high-frequency information fusion by designing a lightweight spatial attention module and improving the network architecture of enhanced super-resolution GAN (ESRGAN). It makes the generator in the GAN network have better feature recovery ability, reduces the dependence of the later training on the decider and loss function, and makes the generated image structure more consistent with the real situation. In addition, we build a high-frequency loss function to optimize the training of the generator network. Detailed experimental results show that HIFGAN performs excellently in both objective criterion evaluation and subjective visual effect. Compared with the state-of-the-art GAN-based SR networks, the reconstructed image by our model is more precise and complete in texture details.
C1 [Yang, Xin; Li, Hengrui; Li, Xiaochuan; Li, Tao] Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 210016, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Yang, X (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Automat Engn, Nanjing 210016, Peoples R China.
EM yangxin@nuaa.edu.cn; hrli_1997@163.com; xiaochuan_le@163.com;
   autolitao@nuaa.edu.cn
RI Li, Wenjuan/KDN-8450-2024; Zhang, Yuan/JUF-7293-2023; LI,
   HAO/KBD-0866-2024; Liu, Yang/JVD-6777-2023; Zhou, Xinyi/KGM-6689-2024;
   LI, LI/KCJ-5600-2024; yang, ying/KHW-9378-2024; Li, YU/JQV-2716-2023;
   Sun, Yang/KHY-5117-2024; luo, Jing/KFT-0288-2024; li, Li/JPA-0218-2023;
   li, yan/KFQ-3850-2024; WANG, YING/JLM-9219-2023; Liu, Zhe/KEJ-5299-2024;
   Liu, Yan/KFQ-1417-2024; zhang, zheng/KHY-8870-2024; wang,
   jin/KHD-7243-2024; Liu, Chang/KGL-6678-2024; wang, wei/JYP-7819-2024;
   wei, wang/KHY-7669-2024; song, yu/KCZ-2003-2024; xiang,
   wei/JXL-3308-2024; Liu, Zhiyuan/KDP-2606-2024; yang, liu/JXX-5043-2024;
   Yang, Fan/JMA-9594-2023; zhong, jing/KBP-7800-2024; Li,
   Yao/JJC-2927-2023; Li, Yang/KFB-5350-2024; LI, yue/KHC-6771-2024; WANG,
   YONGJIA/KFQ-4823-2024; lin, yuan/JXL-9592-2024; Li, Wen/JQI-4757-2023;
   Zhang, Bo/JVD-9890-2024; yang, xiao/JLL-7721-2023; li,
   fang/KDO-8841-2024; Wang, Xinyi/KHV-4909-2024; Yan, Xin/KGL-5903-2024;
   you, li/KHW-2201-2024; liu, yang/JMB-9083-2023; Chen,
   Liang/JXX-7887-2024; li, li/JVP-2971-2024; Liu, Junjie/KHV-6949-2024;
   li, tao/JVO-9006-2024; li, li/KHE-5750-2024; wang,
   xiaoqiang/JMT-2783-2023; zhao, lin/JJF-0406-2023; zhang,
   yingying/KGM-8162-2024; LI, WEI/JUE-9796-2023; li, jing/KHY-5337-2024;
   Wang, Yu/KGL-3101-2024; zhang, xiao/JCN-8822-2023; wang,
   yue/KDO-9209-2024; Li, Xintong/KHD-6915-2024; Yang, Fan/JVO-8611-2024;
   li, yifan/JHU-9272-2023; Li, Yan/KFQ-9244-2024; Wang,
   Jing/JRW-1512-2023; Liu, Zhihao/JUF-7651-2023; Zhang, Yi/KHW-2039-2024;
   Li, Fan/KBB-8931-2024; liu, yuhao/JWP-0475-2024; YANG,
   DAN/KCL-5217-2024; li, yuan/KBQ-4200-2024; Li, Lei/JPE-6543-2023; zhang,
   wen/JXN-0191-2024; Zhang, Xiaoyu/JXR-6386-2024; Lu, Yi/KEJ-2560-2024;
   Chen, Zheng/KCY-2338-2024; wang, liangyu/KHD-1769-2024; Wang,
   Xin/KGK-9099-2024; zhang, yuanyuan/KHV-4459-2024; wang,
   jun/JPY-3635-2023; yang, yunfeng/KHT-9566-2024; chang, yu/KFB-2822-2024;
   chen, chen/KHW-7024-2024; Zhang, Lu/KHE-5879-2024; Wang,
   Jin/KAM-5595-2024; zhang, ying/JQX-1479-2023; Lin, Wei/KFQ-5381-2024;
   Li, siqi/KDN-4520-2024; Zhang, Youyou/KCY-0810-2024; Liu,
   Jie/JCP-1070-2023; Jing, Jing/JSK-6237-2023; wang,
   wenjing/KEH-0575-2024; Li, Kunpeng/KFS-6306-2024; li, lin/KEJ-1056-2024;
   Zhang, Wei/JKI-3565-2023; wang, Xiaoming/KBB-8854-2024; Wang,
   Yuhan/KGL-5855-2024; su, hang/KEH-2976-2024; Li, J N/JXL-5833-2024;
   zhang, lu/KGL-6144-2024; yuanyuan, Li/JEZ-6497-2023; yang,
   rui/JHI-3328-2023; Liu, Chenyu/KBQ-8899-2024; Wang, Fei/KEH-6292-2024;
   liu, xiao/JLL-2119-2023; ZHOU, YUE/KCJ-8790-2024; wang,
   xin/JWA-3772-2024; PENG, CHENG/KCL-2506-2024; Zhang, Kai/KBD-3312-2024;
   wang, xiaoxuan/JMP-6531-2023; zhang, xinyu/JKI-8403-2023; zhang,
   hao/KEJ-2291-2024; liu, feng/KCL-0778-2024; Wang, Bo/KEH-0105-2024; li,
   cheng/KCZ-0615-2024; liu, qi/KHC-7509-2024; Wang, Xuechun/JRX-6509-2023;
   Wang, Yifan/KDO-8319-2024; lu, Li/KBA-2603-2024; Zhou,
   Yue/JHS-8791-2023; YI, J/JJE-7713-2023; Li, Yan/JUU-5189-2023; ZHANG,
   JING/KHY-1073-2024; liu, yang/KFA-8402-2024; Lu, Xin/KHW-8570-2024;
   Wang, YuHan/KGY-2933-2024; Wang, Xin/JVE-0200-2024; li,
   xiang/JCN-9316-2023; Zhang, jin/KFT-0762-2024; zheng, yan/JKJ-3632-2023;
   Li, Wei/JLL-4365-2023; li, lan/KCJ-5061-2024; han, yang/KHX-8947-2024;
   chen, zhuo/JXX-1337-2024; Yun, Ji/KAL-9759-2024; Wang,
   Yue/JRY-8962-2023; li, jing/KHC-8303-2024; Wang, Zhen/KCL-5193-2024;
   zhang, yan/KHC-3163-2024; yang, le/KFB-5420-2024; Zhang,
   Yun/JCN-7026-2023; Liu, Xiaohan/KBB-4246-2024; wang, zhe/JNE-3510-2023;
   zhao, yan/JNT-6961-2023; Yuan, Ye/KBC-9835-2024; Liu,
   Shaobo/JUU-5767-2023; Ling, Li/JYO-7043-2024; Li, Bo/KHX-7246-2024; Li,
   Kexin/KAO-2519-2024; wang, jiaqi/KHC-5900-2024; Wang, yl/JNR-4963-2023;
   yang, xiao/KHT-9445-2024; Liu, Zhen/KFS-2748-2024; Wen,
   Jing/KCL-6614-2024; wang, xi/JNT-5162-2023; Li, Yan/JRW-0176-2023; wang,
   yi/JYO-8193-2024; Zhang, Tianxi/KEH-5921-2024; liu, zhen/KFS-0275-2024;
   lin, lin/KCZ-0185-2024; Yin, Jing/KDO-6274-2024; peng,
   yan/JCO-1763-2023; Yu, Xiaohan/KCK-5462-2024; liu, jingwen/JQW-9270-2023
OI zhang, yingying/0000-0001-7479-3398; Wang, Xin/0009-0004-7428-3071; Lu,
   Xin/0000-0001-9885-6031; Wang, Yue/0000-0001-8673-6358; Liu,
   Xiaohan/0009-0009-5291-2494; Yuan, Ye/0009-0008-1640-7047; Yang,
   Xin/0000-0003-0445-6497
FU National Natural Science Foundation of China [61573182]; Fundamental
   Research Funds for the Central Universities [NS2020025]
FX This research was supported by the National Natural Science Foundation
   of China (61573182), and by the Fundamental Research Funds for the
   Central Universities (NS2020025).
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Haris M, 2018, Arxiv, DOI arXiv:1803.02735
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2018, INT C PATT RECOG, P2670, DOI 10.1109/ICPR.2018.8545648
   Hwang JW, 2004, IEEE SIGNAL PROC LET, V11, P359, DOI 10.1109/LSP.2003.821718
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Muqeet Abdul, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P103, DOI 10.1007/978-3-030-67070-2_6
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Xintao, 2018, P EUROPEAN C COMPUTE, DOI [10.48550/arXiv.1809.00219, DOI 10.48550/ARXIV.1809.00219]
   Yang X, 2022, CLUSTER COMPUT, V25, P3977, DOI 10.1007/s10586-022-03631-1
   Yang X, 2021, J VIS COMMUN IMAGE R, V75, DOI 10.1016/j.jvcir.2021.103019
   Yang X, 2021, MULTIMED TOOLS APPL, V80, P7063, DOI 10.1007/s11042-020-09958-4
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 32
TC 5
Z9 5
U1 6
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 159
DI 10.1145/3578934
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300008
DA 2024-07-18
ER

PT J
AU Zhao, MY
   Tang, H
   Xie, P
   Dai, SL
   Sebe, N
   Wang, W
AF Zhao, Mengyi
   Tang, Hao
   Xie, Pan
   Dai, Shuling
   Sebe, Nicu
   Wang, Wei
TI Bidirectional Transformer GAN for Long-term Human Motion Prediction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Long-term human motion prediction; bidirectional generation;
   Transformer; GAN; DTW
AB The mainstream motion prediction methods usually focus on short-term prediction, and their predicted long-term motions often fall into an average pose, i.e., the freezing forecasting problem [27]. To mitigate this problem, we propose a novel Bidirectional Transformer-based Generative Adversarial Network (BiTGAN) for long-term human motion prediction. The bidirectional setup leads to consistent and smooth generation in both forward and backward directions. Besides, to make full use of the history motions, we split them into two parts. The first part is fed to the Transformer encoder in our BiTGAN while the second part is used as the decoder input. This strategy can alleviate the exposure problem [37]. Additionally, to better maintain both the local (i.e., frame-level pose) and global (i.e., video-level semantic) similarities between the predicted motion sequence and the real one, the soft dynamic time warping (Soft-DTW) loss is introduced into the generator. Finally, we utilize a dual-discriminator to distinguish the predicted sequence at both frame and sequence levels. Extensive experiments on the public Human3.6M dataset demonstrate that our proposed BiTGAN achieves state-of-the-art performance on long-term (4s) human motion prediction, and reduces the average error of all actions by 4%.
C1 [Zhao, Mengyi] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Tang, Hao] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
   [Xie, Pan] Beihang Univ, Beijing 100191, Peoples R China.
   [Dai, Shuling] Beihang Univ, Jiangxi Res Inst, Beijing, Jiangxi, Peoples R China.
   [Sebe, Nicu; Wang, Wei] Univ Trento, I-38122 Trento, Italy.
C3 Beihang University; Swiss Federal Institutes of Technology Domain; ETH
   Zurich; Beihang University; Beihang University; University of Trento
RP Zhao, MY (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM zhaomengyi@buaa.edu.cn; hao.tang@vision.ee.ethz.ch; panxie@buaa.edu.cn;
   sldai@buaa.edu.cn; niculae.sebe@unitn.it; wei.wang@unitn.it
RI Wang, Wei/AAK-5521-2021; Sebe, Niculae/KEC-2000-2024
OI Wang, Wei/0000-0002-5477-1017; Sebe, Niculae/0000-0002-6597-7248; Tang,
   Hao/0000-0002-2077-1246
FU National Key Research & Development Program of China [2018AAA0102902]
FX This work is supported by the National Key Research & Development
   Program of China (No. 2018AAA0102902).
CR Arjovsky M., 2017, PREPRINT, DOI DOI 10.48550/ARXIV.1701.04862
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Bütepage J, 2017, PROC CVPR IEEE, P1591, DOI 10.1109/CVPR.2017.173
   Cao Z., 2020, ECCV, P387
   Cuturi M, 2017, PR MACH LEARN RES, V70
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Dang LW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11447, DOI 10.1109/ICCV48922.2021.01127
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2021, PROCEEDING 9 INT C L
   Duan Y., 2021, ARXIV
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Gao K, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4833, DOI 10.1145/3474085.3479231
   Gopalakrishnan A, 2019, PROC CVPR IEEE, P12108, DOI 10.1109/CVPR.2019.01239
   Gui LY, 2018, LECT NOTES COMPUT SC, V11208, P823, DOI 10.1007/978-3-030-01225-0_48
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   Ingram JN, 2008, EXP BRAIN RES, V188, P223, DOI 10.1007/s00221-008-1355-3
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kingma D. P., 2014, arXiv
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Kundu JN, 2019, AAAI CONF ARTIF INTE, P8553
   Li C, 2018, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2018.00548
   Li J, 2020, ARXIV
   Li JC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16076, DOI 10.1109/ICCV48922.2021.01579
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li RL, 2021, Arxiv, DOI arXiv:2101.08779
   Liu HY, 2017, J MANUF SYST, V44, P287, DOI 10.1016/j.jmsy.2017.04.009
   Liu YC, 2021, PROC CVPR IEEE, P7573, DOI 10.1109/CVPR46437.2021.00749
   Liu ZG, 2019, PROC CVPR IEEE, P9996, DOI 10.1109/CVPR.2019.01024
   Lyu KD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4976, DOI 10.1145/3474085.3475630
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Mao W, 2019, IEEE I CONF COMP VIS, P9488, DOI 10.1109/ICCV.2019.00958
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Messina N, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451390
   Ranzato MarcAurelio, 2016, PROCEEDING 4 INT C L
   Ren XC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P46, DOI 10.1145/3394171.3413932
   Sofianos Theodoros, 2021, P IEEE CVF INT C COM, P11209
   Su PX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P713, DOI 10.1145/3474085.3475237
   Tang H., 2018, P AS C COMP VIS ACCV, P3
   Tang YY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P935
   Vaswani A, 2017, ADV NEUR IN, V30
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Wang BR, 2019, IEEE I CONF COMP VIS, P7123, DOI 10.1109/ICCV.2019.00722
   Wei Mao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P474, DOI 10.1007/978-3-030-58568-6_28
   Yang ZL, 2020, Arxiv, DOI arXiv:1906.08237
   Yujun Cai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P226, DOI 10.1007/978-3-030-58571-6_14
   Zhang B, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3449359
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 49
TC 7
Z9 7
U1 8
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 163
DI 10.1145/3579359
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300012
OA Green Published
DA 2024-07-18
ER

PT J
AU Wanyan, YY
   Yang, XS
   Ma, X
   Xu, CS
AF Wanyan, Yuyang
   Yang, Xiaoshan
   Ma, Xuan
   Xu, Changsheng
TI Dual Scene Graph Convolutional Network for Motivation Prediction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Motivation prediction; scene graph; graph convolutional network;
   multi-modalities
ID INTENTS
AB Humans can easily infer the motivations behind human actions from only visual data by comprehensively analyzing the complex context information and utilizing abundant life experiences. Inspired by humans' reasoning ability, existing motivation prediction methods have improved image-based deep classification models using the commonsense knowledge learned by pre-trained language models. However, the knowledge learned from public text corpora is probably incompatible with the task-specific data of the motivation prediction, which may impact the model performance. To address this problem, this paper proposes a dual scene graph convolutional network (dual-SGCN) to comprehensively explore the complex visual information and semantic context prior from the image data for motivation prediction. The proposed dual-SGCN has a visual branch and a semantic branch. For the visual branch, we build a visual graph based on scene graph where object nodes and relation edges are represented by visual features. For the semantic branch, we build a semantic graph where nodes and edges are directly represented by the word embeddings of the object and relation labels. In each branch, node-oriented and edge-oriented message passing is adopted to propagate interaction information between different nodes and edges. Besides, a multi-modal interactive attention mechanism is adopted to cooperatively attend and fuse the visual and semantic information. The proposed dual-SGCN is learned in an end-to-end form by a multi-task co-training scheme. In the inference stage, Total Direct Effect is adopted to alleviate the bias caused by the semantic context prior. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance.
C1 [Wanyan, Yuyang; Yang, Xiaoshan; Ma, Xuan; Xu, Changsheng] Univ Chinese Acad Sci UCAS, Inst Automat, Chinese Acad Sci CASIA, Natl Lab Pattern Recognit, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
   [Wanyan, Yuyang; Yang, Xiaoshan; Ma, Xuan; Xu, Changsheng] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
   [Yang, Xiaoshan; Xu, Changsheng] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Peng Cheng
   Laboratory
RP Wanyan, YY (corresponding author), Univ Chinese Acad Sci UCAS, Inst Automat, Chinese Acad Sci CASIA, Natl Lab Pattern Recognit, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.; Wanyan, YY (corresponding author), Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
EM wanyanyuyang2021@ia.ac.cn; xiaoshan.yang@nlpr.ia.ac.cn;
   maxuan2018@ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; yang, xiaoshan/HSE-6093-2023
OI xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2018AAA0100604];
   National Natural Science Foundation of China [61720106006, 62036012,
   62072455, 61721004, U1836220, 61872424]; Beijing Natural Science
   Foundation [L201001]
FX This work was supported by National Key Research and Development Program
   of China (No. 2018AAA0100604), National Natural Science Foundation of
   China (No. 61720106006, 62036012, 62072455, 61721004, U1836220,
   61872424), Beijing Natural Science Foundation (L201001).
CR Abu-El-Haija S, 2020, PR MACH LEARN RES, V115, P841
   Phan AV, 2018, NEURAL NETWORKS, V108, P533, DOI 10.1016/j.neunet.2018.09.001
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Burnaev E, 2015, PROC SPIE, V9875, DOI 10.1117/12.2228523
   de Ruiter J., 2012, Proceedings of SemDial 2012 (SeineDial) ? 16th Workshop on the Semantics and Pragmatics of Dialogue, P149
   Defferrard M, 2016, ADV NEUR IN, V29
   Feng G, 2021, PROC CVPR IEEE, P15501, DOI 10.1109/CVPR46437.2021.01525
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Grunde-McLaughlin M, 2021, PROC CVPR IEEE, P11282, DOI 10.1109/CVPR46437.2021.01113
   Gu Y, 2017, LECT NOTES ARTIF INT, V10233, P260, DOI 10.1007/978-3-319-57351-9_30
   Hamilton WL, 2017, ADV NEUR IN, V30
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henaff M, 2015, Arxiv, DOI arXiv:1506.05163
   Huang XY, 2016, IEEE COMPUT SOC CONF, P778, DOI 10.1109/CVPRW.2016.102
   Jia ML, 2021, PROC CVPR IEEE, P12981, DOI 10.1109/CVPR46437.2021.01279
   Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Joo J, 2014, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2014.35
   Kim JH, 2018, ADV NEUR IN, V31
   Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010
   Kim UH, 2020, IEEE T CYBERNETICS, V50, P4921, DOI 10.1109/TCYB.2019.2931042
   Kim W., 2021, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar A, 2016, PR MACH LEARN RES, V48
   Li JW, 2015, Arxiv, DOI arXiv:1506.01057
   Li X J., 2020, P EUR C COMP VIS, P121, DOI DOI 10.1007/978-3-030-58577-8_8
   Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142
   Li YJ, 2017, Arxiv, DOI arXiv:1511.05493
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu JS, 2016, ADV NEUR IN, V29
   Rush AM, 2015, Arxiv, DOI [arXiv:1509.00685, 10.18653/v1/d15-1044, DOI 10.18653/V1/D15-1044]
   Mnih V, 2014, ADV NEUR IN, V27
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Newell A., 2017, ADV NEUR IN, P2171
   Nguyen K, 2021, Arxiv, DOI arXiv:2102.04990
   Niepert M, 2016, PR MACH LEARN RES, V48
   Ramanan D, 2014, Arxiv, DOI arXiv:1312.1743
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Safaei M, 2019, IEEE WINT CONF APPL, P111, DOI 10.1109/WACV.2019.00019
   Sekhon J, 2021, Arxiv, DOI arXiv:2102.00109
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stollenga Marijn, 2014, Deep networks with internal selective attention through feedback connections
   Sylvain Tristan, 2020, P AAAI C ARTIFICIAL, V35, P2647
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tong ZK, 2020, Arxiv, DOI arXiv:2004.13970
   Uleman JS, 1996, ADV EXP SOC PSYCHOL, V28, P211, DOI 10.1016/S0065-2601(08)60239-7
   Vaswani A, 2017, ADV NEUR IN, V30
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Vondrick C, 2016, PROC CVPR IEEE, P2997, DOI 10.1109/CVPR.2016.327
   Wang S, 2019, IEEE INT CONF BIG DA, P5401, DOI 10.1109/BigData47090.2019.9006464
   Wiles O, 2021, PROC CVPR IEEE, P15915, DOI 10.1109/CVPR46437.2021.01566
   WIMMER H, 1983, COGNITION, V13, P103, DOI 10.1016/0010-0277(83)90004-5
   Wu TY, 2017, IEEE I CONF COMP VIS, P48, DOI 10.1109/ICCV.2017.15
   Xi Nan, 2020, P INT AAAI C WEB SOC, V14, P726
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yoon JS, 2021, PROC CVPR IEEE, P15034, DOI 10.1109/CVPR46437.2021.01479
   Yoon S, 2021, AAAI CONF ARTIF INTE, V35, P10718
   Yu F, 2021, AAAI CONF ARTIF INTE, V35, P3208
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang HL, 2021, AAAI CONF ARTIF INTE, V35, P14374
   Zhang HL, 2021, AAAI CONF ARTIF INTE, V35, P14365
   Zhao B, 2020, INT J COMPUT VISION, V128, P2418, DOI 10.1007/s11263-020-01300-7
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 71
TC 1
Z9 1
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 137
DI 10.1145/3572914
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700012
OA Bronze
DA 2024-07-18
ER

PT J
AU Huang, BJ
   Wang, ZY
   Wang, GC
   Han, Z
   Jiang, K
AF Huang, Baojin
   Wang, Zhongyuan
   Wang, Guangcheng
   Han, Zhen
   Jiang, Kui
TI Local Eyebrow Feature Attention Network for Masked Face Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Masked face dataset; masked face recognition; eyebrows; graph
   convolutional network
AB During the COVID-19 coronavirus epidemic, wearing masks has become increasingly popular. Traditional occlusion face recognition algorithms are almost ineffective for such heavy mask occlusion. Therefore, it is urgent to improve the recognition performance of the existing face recognition technology on masked faces. Due to the limited visible feature points of the masked face image relative to the normal face image, we have to exploit the identification potential of eyebrow (referring to eyes and brows) features. This article proposes a local eyebrow feature attention network for masked face recognition, which consists of feature extraction, eyebrow region pooling, and feature fusion. To highlight the eyebrow region, we first use the eyebrow region pooling to separate the local features of eyebrows from the learned overall facial features. We then make full use of the symmetry of left and right eyebrows to emphasize their discriminant ability, due to the inadequate fine information of the low-resolution eyebrows. In particular, in view of the symmetrical similarity between eyebrow pairs and the subordinate relationship between facial components and the whole, we propose a feature fusion model based on graph convolutional network (GCN) to learn the feature association structure of eye features, brow features, and global facial features. We construct the benchmark datasets for masked face recognition to validate our approach, including real-world masked face recognition dataset (RMFRD) and synthetic masked face recognition dataset (SMFRD). Extensive experimental results on both public datasets and our built masked face datasets show that our approach significantly outperforms the state-of-the-arts.
C1 [Huang, Baojin; Wang, Zhongyuan; Wang, Guangcheng; Han, Zhen; Jiang, Kui] Wuhan Univ, Sch Comp, Wuhan, Peoples R China.
C3 Wuhan University
RP Wang, ZY (corresponding author), Wuhan Univ, Sch Comp, Wuhan, Peoples R China.
EM huangbaoin@whu.edu.cn; wzy_hope@163.com; wangguangcheng0428@163.com;
   hanzhen_1980@163.com; kuijiang_1994@163.com
RI Jiang, Kui/Z-2573-2019
OI Jiang, Kui/0000-0002-4055-7503; Huang, Baojin/0000-0002-4882-5787; Wang,
   Zhongyuan/0000-0002-9796-488X; Han, Zhen/0000-0002-1862-4781; Wang,
   Guangcheng/0000-0001-8277-797X
FU National Key Research and Development Program of China [2021YFF0602102];
   National Natural Science Foundation of China [U1903214, 62071339,
   62072347]
FX This work was supported by National Key Research and Development Program
   of China (2021YFF0602102) and National Natural Science Foundation of
   China (U1903214, 62071339, 62072347). The numerical calculations in this
   article have been done on the supercomputing system in the
   Supercomputing Center of Wuhan University.
CR Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chakraborty S, 2018, IEEE T CIRC SYST VID, V28, P171, DOI 10.1109/TCSVT.2016.2603535
   Deng J., 2019, P COMP VIS PATT REC, P5203, DOI 10.1109/CVPR42600.2020.00525
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Y, 2011, IEEE T IMAGE PROCESS, V20, P2329, DOI 10.1109/TIP.2011.2109729
   Ding FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2281, DOI 10.1145/3394171.3413731
   Geng MY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2246, DOI 10.1145/3394171.3413723
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu JL, 2018, IEEE T CIRC SYST VID, V28, P1875, DOI 10.1109/TCSVT.2017.2691801
   Huang BJ, 2023, IEEE T NEUR NET LEAR, V34, P10875, DOI 10.1109/TNNLS.2022.3171604
   Huang G.B., 2008, PROC WORKSHOP FACES
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Lin YS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3469288
   Liu BY, 2019, IEEE I CONF COMP VIS, P10051, DOI 10.1109/ICCV.2019.01015
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Low CY, 2019, IEEE T CIRC SYST VID, V29, P115, DOI 10.1109/TCSVT.2017.2761829
   Martínez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Ngan, 2020, NIST
   Qian JJ, 2014, IEEE COMPUT SOC CONF, P21, DOI 10.1109/CVPRW.2014.9
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sengupta S, 2016, IEEE WINT CONF APPL
   Shao CB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P666
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh A, 2017, IEEE INT CONF COMP V, P1648, DOI 10.1109/ICCVW.2017.193
   Song LX, 2019, IEEE I CONF COMP VIS, P773, DOI 10.1109/ICCV.2019.00086
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wada Kentaro, 2021, Zenodo
   Wan WT, 2017, IEEE IMAGE PROC, P3795, DOI 10.1109/ICIP.2017.8296992
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang YT, 2018, LECT NOTES COMPUT SC, V11219, P764, DOI 10.1007/978-3-030-01267-0_45
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Yang M, 2011, PROC CVPR IEEE, P625, DOI 10.1109/CVPR.2011.5995393
   Yi D, 2014, Arxiv, DOI arXiv:1411.7923
   Zhang S, 2018, IEEE T INF FOREN SEC, V13, P637, DOI 10.1109/TIFS.2017.2763119
   Zhao J, 2019, IEEE T PATTERN ANAL, V41, P2380, DOI 10.1109/TPAMI.2018.2858819
   Zhao ZJ, 2018, IEEE T INF FOREN SEC, V13, P2937, DOI 10.1109/TIFS.2018.2833018
NR 41
TC 6
Z9 6
U1 4
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 114
DI 10.1145/3569943
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300014
DA 2024-07-18
ER

PT J
AU Qiao, ZT
   Shi, DX
   Yi, XD
   Shi, YY
   Zhang, YH
   Liu, YY
AF Qiao, Ziteng
   Shi, Dianxi
   Yi, Xiaodong
   Shi, Yanyan
   Zhang, Yuhui
   Liu, Yangyang
TI UEFPN: Unified and Enhanced Feature Pyramid Networks for Small Object
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Channel attention; multi-scale feature; small object detection
AB Object detection models based on feature pyramid networks have made significant progress in general object detection. However, small object detection is still a challenge for the existing models. In this paper, we think that two factors in the existing feature pyramid networks inhibit the performance of small object detection. The first one is that the different feature domains of shallow and deep layer features inhibit the model performance. The second one is that the accumulation of upper layer features leads to feature aliasing effect on the lower layer features, which interferes with the representations of small object features. Therefore, we propose Unified and Enhanced Feature Pyramid Networks (UEFPN) to improve the APs and ARs of small object detection. It has the following three characteristics: (1) Using the deep features of high-resolution image and original image to form the multi-scale features of unified domain. (2) In multi-scale features fusion, we learn the importance of upper layer features with the Channel Attention Fusion module (CAF), to optimize feature aliasing effect and enhance the context information of shallow layer features. (3) UEFPN can be quickly applied to different models. The results of many experiments show that the models with UEFPN achieve significant performance improvement in small object detection compared with the baseline models.
C1 [Qiao, Ziteng; Shi, Dianxi; Yi, Xiaodong] Def Technol Innovat Inst, Beijing 100166, Peoples R China.
   [Shi, Yanyan; Liu, Yangyang] Natl Univ Def Technol, Changsha 410015, Hunan, Peoples R China.
   [Zhang, Yuhui] Tianjin Artificial Intelligence Innovat Ctr, Tianjin 300457, Peoples R China.
C3 National University of Defense Technology - China
RP Qiao, ZT (corresponding author), Def Technol Innovat Inst, Beijing 100166, Peoples R China.
EM ztqiao99@163.com; dxshi@nudt.edu.cn; yixiaodong@nudt.edu.cn;
   yany_shi@163.com; yuhui0116@163.com; liuyangyang@nudt.edu.cn
RI zhang, xinyu/JKI-8403-2023; Yang, Lili/JTT-5215-2023; Zhang,
   Wei/JKI-3565-2023; Hui, Yang/KGL-7041-2024; chen, bin/KBQ-8114-2024;
   Chen, Fang/JZE-4446-2024; lei, lei/JSL-3106-2023; yan,
   yan/JVN-1800-2024; Li, Yuanxiang/KCX-8706-2024; Zhang,
   Yuting/JZE-2800-2024; zhang, lm/JWP-8874-2024; lan, lan/JWO-3679-2024;
   LIU, JIALIN/JXN-8034-2024; Shi, Yaolin/JXN-8322-2024; Wang,
   Han/JJF-2614-2023; Shi, Dianxi/JXM-7411-2024; Chen, Jin/KBQ-0163-2024;
   Zhang, Yuting/JRW-3937-2023; lu, yuan/JZD-0832-2024
OI Yang, Lili/0009-0008-2926-484X; chen, bin/0000-0002-3398-1314; Shi,
   Dianxi/0000-0002-8112-371X; Chen, Jin/0009-0005-5844-635X; Qiao,
   Ziteng/0000-0002-2414-0062; shi, yanyan/0000-0002-8822-1558
CR Abdar M, 2022, Arxiv, DOI arXiv:2105.08590
   Abdar M, 2021, INFORM SCIENCES, V577, P353, DOI 10.1016/j.ins.2021.07.024
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2968, DOI 10.1109/ICCV48922.2021.00298
   Dai XY, 2021, PROC CVPR IEEE, P7369, DOI 10.1109/CVPR46437.2021.00729
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong YQ, 2021, IEEE WINT CONF APPL, P1159, DOI 10.1109/WACV48630.2021.00120
   Hassan MR, 2022, INFORM FUSION, V77, P70, DOI 10.1016/j.inffus.2021.07.010
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZM, 2020, IEEE COMPUT SOC CONF, P4422, DOI 10.1109/CVPRW50498.2020.00521
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Najibi M, 2019, IEEE I CONF COMP VIS, P9744, DOI 10.1109/ICCV.2019.00984
   Nascita A, 2021, IEEE T NETW SERV MAN, V18, P4225, DOI 10.1109/TNSM.2021.3098157
   Niu ZY, 2021, NEUROCOMPUTING, V452, P48, DOI 10.1016/j.neucom.2021.03.091
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Rabbi J, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091432
   Rahaman MM, 2021, COMPUT BIOL MED, V136, DOI 10.1016/j.compbiomed.2021.104649
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shrivastava A, 2017, Arxiv, DOI arXiv:1612.06851
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh B, 2018, 32 C NEURAL INFORM P
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yu XH, 2020, IEEE WINT CONF APPL, P1246, DOI 10.1109/WACV45572.2020.9093394
   Yuhang Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11580, DOI 10.1109/CVPR42600.2020.01160
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhao Min, 2022, Med Image Anal, V78, P102413, DOI 10.1016/j.media.2022.102413
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
   Zhu YS, 2019, IEEE T IMAGE PROCESS, V28, P113, DOI 10.1109/TIP.2018.2865280
NR 49
TC 2
Z9 2
U1 10
U2 36
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 95
DI 10.1145/3561824
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300020
DA 2024-07-18
ER

PT J
AU Peng, F
   Jiang, WY
   Long, M
AF Peng, Fei
   Jiang, Wenyan
   Long, Min
TI A Low Distortion and Steganalysis-resistant Reversible Data Hiding for
   2D Engineering Graphics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Steganalysis-resistant data hiding; quantization index modulation;
   reversible data hiding; 2D engineering graphics
ID QUANTIZATION INDEX MODULATION; WATERMARKING SCHEME; HIGH-CAPACITY; VIDEO
   STEGANALYSIS
AB To reduce the distortion resulting from the large number of crossing quantization cells and resist steganalysis, a reversible data hiding scheme for 2D engineering graphics is put forward based on reversible dual-direction quantization index modulation (RDQIM). The quantization cell index of the host data is first computed, and its distances to the embedding cells in both the left and the right directions are calculated. After that, the data hiding is performed by modifying the data to the nearest embedding cell. To guarantee the reversibility, each quantization cell is further subdivided into three sub-cells, and the source quantization interval of the host data is marked by the index of the located sub-cell. The data extraction is accomplished by calculating the index of the quantization cell where the stego data is in. Meanwhile, the lossless recovery of the stego data is realized by combining the index of the located sub-cell and the relative distance within the sub-cell. Besides, different embedding strategies are adopted for different types of entities to achieve steganalysis-resistant ability. Experimental results and analysis show that the proposed scheme can strike a good balance among imperceptibility, semi-fragility, and steganalysis-resistant ability. Moreover, under the same conditions, the average imperceptibility and the average capacity are, respectively, improved by at least 7.487% and 41.045% compared with the existing methods.
C1 [Peng, Fei] Guangzhou Univ, Guangzhou Higher Educ Mega Ctr, 230 Wai Huan Xi Rd, Guangzhou 510006, Guangdong, Peoples R China.
   [Jiang, Wenyan] Hunan Univ, Lushan Nan Rd, Changsha 410082, Hunan, Peoples R China.
   [Long, Min] Changsha Univ Sci & Technol, 960 Wanjiali South Rd, Changsha 410114, Peoples R China.
C3 Guangzhou University; Hunan University; Changsha University of Science &
   Technology
RP Long, M (corresponding author), Changsha Univ Sci & Technol, 960 Wanjiali South Rd, Changsha 410114, Peoples R China.
EM eepengf@gmail.com; jwy1996@hnu.edu.cn; caslongm@aliyun.com
RI Long, Min/AGW-6059-2022; Peng, Fei/H-6951-2017
OI Peng, Fei/0000-0001-8053-4587
FU National Natural Science Foundation of China [92067104, U1936115,
   62072055]
FX This work was supported in part by project supported by National Natural
   Science Foundation of China (Grant Nos. 92067104, U1936115, 62072055).
CR Avcibas I, 2003, IEEE T IMAGE PROCESS, V12, P221, DOI 10.1109/TIP.2002.807363
   AWRANJEB M, 2003, INT C COMP INF TECHN, P75
   Budhia U, 2006, IEEE T INF FOREN SEC, V1, P502, DOI 10.1109/TIFS.2006.885020
   Cao Y, 2012, IEEE SIGNAL PROC LET, V19, P35, DOI 10.1109/LSP.2011.2176116
   Chen B, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P273, DOI 10.1109/MMSP.1998.738946
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Chen B, 2000, P SOC PHOTO-OPT INS, V3971, P48, DOI 10.1117/12.384995
   Deng L. P., 2010, COMPUT KNOWL TECHNOL, V4
   Fei P, 2013, SECUR COMMUN NETW, V6, P1117, DOI 10.1002/sec.680
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Gao GY, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107817
   Geng MQ, 2012, PROCEEDINGS OF THE 3RD IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT (IEEE IC-NIDC 2012), P521, DOI 10.1109/ICNIDC.2012.6418808
   Geng Mingqin, 2013, EMERGING TECHNOLOGIE, P443
   Hu RW, 2021, IEEE SIGNAL PROC LET, V28, P464, DOI 10.1109/LSP.2021.3059202
   Jana Biswapati, 2016, International Journal of Network Security, V18, P633
   Jana B, 2018, MULTIMED TOOLS APPL, V77, P8805, DOI 10.1007/s11042-017-4775-x
   Jana B, 2018, MULTIMED TOOLS APPL, V77, P763, DOI 10.1007/s11042-016-4230-4
   Jana B, 2017, MULTIMED TOOLS APPL, V76, P21691, DOI 10.1007/s11042-016-3990-1
   Jana B, 2016, OPTIK, V127, P3347, DOI 10.1016/j.ijleo.2015.12.055
   Li XL, 2014, 2014 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (CHINASIP), P426, DOI 10.1109/ChinaSIP.2014.6889278
   Li ZY, 2017, INFORM SCIENCES, V415, P85, DOI 10.1016/j.ins.2017.06.011
   Lin ZX, 2018, IEEE T INF FOREN SEC, V13, P2372, DOI 10.1109/TIFS.2018.2819122
   Liu QZ, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000492
   Liu QZ, 2009, IEEE T INF FOREN SEC, V4, P359, DOI 10.1109/TIFS.2009.2024718
   Lyu SW, 2006, IEEE T INF FOREN SEC, V1, P111, DOI 10.1109/TIFS.2005.863485
   Meikap S, 2018, MULTIMED TOOLS APPL, V77, P31281, DOI 10.1007/s11042-018-6203-2
   Men Chaoguang, 2010, CHINESE HIGH TECHNOL, V4
   Mohanty SP, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1413862.1413865
   Narawade Navnath, 2011, INT J COMPUT SCI TEL, V2, P46
   Özer H, 2003, PROC SPIE, V5020, P55, DOI 10.1117/12.477313
   Peng F, 2019, MULTIMED TOOLS APPL, V78, P26885, DOI 10.1007/s11042-017-4362-1
   Peng F, 2011, INT J DIGIT CRIME FO, V3, P35, DOI 10.4018/jdcf.2011040103
   Peng F, 2011, INT J DIGIT CRIME FO, V3, P53, DOI 10.4018/jdcf.2011010104
   Peng F, 2011, COMPUT AIDED DESIGN, V43, P1018, DOI 10.1016/j.cad.2011.03.011
   Peng Fei, 2008, P INT C INT INF HIND, P1516
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Singh AK, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3382772
   Tong DY, 2019, KSII T INTERNET INF, V13, P6190, DOI 10.3837/tiis.2019.12.022
   Voigt M, 2004, P 2004 MULT SEC WORK, P160, DOI DOI 10.1145/1022431.1022459
   Wang KR, 2014, IEEE T INF FOREN SEC, V9, P741, DOI 10.1109/TIFS.2014.2308633
   Wang NN, 2017, MULTIMED TOOLS APPL, V76, P20935, DOI 10.1007/s11042-016-3970-5
   Wang NN, 2014, COMPUT AIDED DESIGN, V47, P108, DOI 10.1016/j.cad.2013.10.005
   Wang X, 2007, IEEE T INF FOREN SEC, V2, P311, DOI 10.1109/TIFS.2007.902677
   Wang X, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103203
   Xiao D, 2015, MULTIMED TOOLS APPL, V74, P2109, DOI 10.1007/s11042-013-1744-x
   Yang Y, 2017, IEEE T VIS COMPUT GR, V23, P1002, DOI 10.1109/TVCG.2016.2525771
   Yang Y, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2535555
   Yao H, 2021, INFORM SCIENCES, V563, P130, DOI 10.1016/j.ins.2021.02.015
NR 48
TC 0
Z9 0
U1 6
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 53
DI 10.1145/3539661
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000003
DA 2024-07-18
ER

PT J
AU Li, J
   Han, L
   Zhang, C
   Li, QY
   Liu, Z
AF Li, Jie
   Han, Ling
   Zhang, Chong
   Li, Qiyue
   Liu, Zhi
TI Spherical Convolution Empowered Viewport Prediction in 360 Video
   Multicast with Limited FoV Feedback
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 360-degree video; video multicast; FoV prediction; saliency detection;
   spherical convolution
ID SALIENCY PREDICTION; VR VIDEO
AB Field of view (FoV) prediction is critical in 360-degree video multicast, which is a key component of the emerging virtual reality and augmented reality applications. Most of the current prediction methods combining saliency detection and FoV information neither take into account that the distortion of projected 360-degree videos can invalidate the weight sharing of traditional convolutional networks nor do they adequately consider the difficulty of obtaining complete multi-user FoV information, which degrades the prediction performance. This article proposes a spherical convolution-empowered FoV prediction method, which is a multi-source prediction framework combining salient features extracted from 360-degree video with limited FoV feedback information. A spherical convolutional neural network is used instead of a traditional two-dimensional convolutional neural network to eliminate the problem of weight sharing failure caused by video projection distortion. Specifically, salient spatial-temporal features are extracted through a spherical convolution-based saliency detection model, after which the limited feedback FoV information is represented as a time-series model based on a spherical convolution-empowered gated recurrent unit network. Finally, the extracted salient video features are combined to predict future user FoVs. The experimental results show that the performance of the proposed method is better than other prediction methods.
C1 [Li, Jie; Han, Ling; Zhang, Chong] Hefei Univ Technol, Sch Comp & Informat, Hefei 230000, Anhui, Peoples R China.
   [Li, Qiyue] Hefei Univ Technol, Sch Elect Engn & Automat, Hefei 230000, Anhui, Peoples R China.
   [Liu, Zhi] Univ Electrocommun, Chofu, Tokyo, Japan.
C3 Hefei University of Technology; Hefei University of Technology;
   University of Electro-Communications - Japan
RP Liu, Z (corresponding author), Univ Electrocommun, Chofu, Tokyo, Japan.
EM lijie@hfut.edu.cn; hanling@mail.hfut.edu.cn; zhangcong@mail.hfut.edu.cn;
   liqiyue@mail.ustc.edu.cn; liu@ieee.org
RI Liu, Zhi/AAE-5698-2020; Li, Qiyue/GON-3433-2022; Wu,
   Celimuge/P-1232-2019
OI Li, Qiyue/0000-0002-7990-0671; Wu, Celimuge/0000-0001-6853-5878; Liu,
   Zhi/0000-0003-0537-4522; LI, Jie/0000-0001-8483-6240
FU National Natural Science Foundation of China [52077049]; Anhui
   Provincial Natural Science Foundation [2008085UD04]; Fundamental
   Research Funds for the Central Universities [PA2020GDJQ0027];
   Grants-in-Aid for Scientific Research [20H04174] Funding Source: KAKEN
FX This work was supported in part by grants from the National Natural
   Science Foundation of China (52077049), Anhui Provincial Natural Science
   Foundation (2008085UD04), and Fundamental Research Funds for the Central
   Universities (PA2020GDJQ0027).
CR Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chao FY, 2018, IEEE INT CONF MULTI
   Che ZH, 2020, IEEE T IMAGE PROCESS, V29, P2287, DOI 10.1109/TIP.2019.2945857
   Chen JY, 2021, IEEE T MULTIMEDIA, V23, P3853, DOI 10.1109/TMM.2020.3033127
   Chopra L, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2379, DOI 10.1145/3442381.3450070
   Cohen T. S., 2018, P INT C LEARNING REP
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   Dedhia B, 2019, INT CONF ACOUST SPEE, P2142, DOI [10.1109/icassp.2019.8683125, 10.1109/ICASSP.2019.8683125]
   DRISCOLL JR, 1994, ADV APPL MATH, V15, P202, DOI 10.1006/aama.1994.1008
   Eder Marc, 2019, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P1
   Eltobgy O, 2020, IEEE T MULTIMEDIA, V22, P3139, DOI 10.1109/TMM.2020.2973855
   Fan CL, 2020, IEEE T MULTIMEDIA, V22, P744, DOI 10.1109/TMM.2019.2931807
   Fan CL, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329119
   Feng XL, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P800, DOI [10.1109/VR46266.2020.00005, 10.1109/VR46266.2020.1584727730619]
   Guo CJ, 2021, IEEE T WIREL COMMUN, V20, P5408, DOI 10.1109/TWC.2021.3067803
   Guo CJ, 2019, IEEE WIREL COMMUN LE, V8, P145, DOI 10.1109/LWC.2018.2864151
   Jamali M., 2020, IEEE INT SYMP CIRC S, DOI DOI 10.1109/iscas45731.2020.9180528
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Lee Kyunghan, 2017, P SMPTE 2017 ANN TEC, P1
   Li J, 2020, IEEE COMMUN LETT, V24, P863, DOI 10.1109/LCOMM.2020.2966193
   Li Jinlong, 2022, IEEE T MULTIMEDIA
   Liang QL, 2020, Arxiv, DOI arXiv:2003.12488
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu Z, 2021, IEEE NETWORK, V35, P202, DOI 10.1109/MNET.101.2000364
   Min XK, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996463
   Nasrabadi AT, 2020, NOSSDAV '20: PROCEEDINGS OF THE 2020 WORKSHOP ON NETWORK AND OPERATING SYSTEM SUPPORT FOR DIGITAL AUDIO AND VIDEO, P34, DOI 10.1145/3386290.3396934
   Park S, 2021, IEEE T NETW SERV MAN, V18, P1000, DOI 10.1109/TNSM.2021.3053183
   Petrangeli S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P157, DOI 10.1109/AIVR.2018.00033
   Petrangeli S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P306, DOI 10.1145/3123266.3123453
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Qiao ML, 2021, IEEE T MULTIMEDIA, V23, P748, DOI 10.1109/TMM.2020.2987682
   Rumney M, 2013, LTE AND THE EVOLUTION TO 4G WIRELESS: DESIGN AND MEASUREMENT CHALLENGES, 2ND EDITION, P1, DOI 10.1002/9781118799475
   Su YC, 2017, ADV NEUR IN, V30
   Sun LY, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P26, DOI 10.1145/3339825.3391856
   Tang JT, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207562
   Vielhaben J, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P74, DOI 10.1109/AIVR46125.2019.00020
   Vo Chuong H., 2020, 2020 5th International Conference on Green Technology and Sustainable Development (GTSD), P442, DOI 10.1109/GTSD50082.2020.9303135
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xianglong Feng, 2019, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V3, DOI 10.1145/3328914
   Xinwei Chen, 2020, IEEE Networking Letters, V2, P81, DOI 10.1109/LNET.2020.2977124
   Xu AY, 2019, IEEE INT CONF MULTI, P336, DOI 10.1109/ICMEW.2019.00064
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Yadav PK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3724, DOI 10.1145/3394171.3413550
   Yang Q, 2019, IEEE INT SYMP CIRC S
   Yang ZB, 2020, PROC CVPR IEEE, P190, DOI [10.1109/CVPR42600.2020.00027, 10.1109/cvpr42600.2020.00027]
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang J, 2020, INT CONF UTIL CLOUD, P402, DOI 10.1109/UCC48980.2020.00063
   Zhang YQ, 2020, IEEE J-STSP, V14, P27, DOI 10.1109/JSTSP.2019.2955824
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhao PY, 2019, INT CONF ACOUST SPEE, P1657, DOI 10.1109/ICASSP.2019.8682776
   Zhu C., 2019, P IEEE INT S BROADBA, P1
   Zhu Dandan, 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME51207.2021.9428420
   Zhu YC, 2020, IEEE T MULTIMEDIA, V22, P2331, DOI 10.1109/TMM.2019.2957986
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
   Zhu Yucheng, 2020, IEEE WCNC, V16
NR 59
TC 40
Z9 41
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 3
DI 10.1145/3511603
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400003
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Lin, JZ
   Zhang, Y
   Li, N
   Jiang, HL
AF Lin, Jinzhi
   Zhang, Yun
   Li, Na
   Jiang, Hongling
TI Joint Source-Channel Decoding of Polar Codes for HEVC-Based Video
   Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Joint source-channel decoding; HEVC; polar code; video streaming
ID POLARIZATION; OPTIMIZATION; 360-DEGREES; DELIVERY; DESIGN
AB Ultra High-Definition (UHD) and Virtual Reality (VR) video streaming over 5G networks are emerging, in which High-Efficiency Video Coding (HEVC) is used as source coding to compress videos more efficiently and polar code is used as channel coding to transmit bitstream reliably over an error-prone channel. In this article, a novel Joint Source-Channel Decoding ( JSCD) of polar codes for HEVC-based video streaming is presented to improve the streaming reliability and visual quality. Firstly, a Kernel Density Estimation (KDE) fitting approach is proposed to estimate the positions of error channel decoded bits. Secondly, a modified polar decoder called R-SCFlip is designed to improve the channel decoding accuracy. Finally, to combine the KDE estimator and the R-SCFlip decoder together, the JSCD scheme is implemented in an iterative process. Extensive experimental results reveal that, compared to the conventional methods without JSCD, the error data-frame correction ratios are increased. Averagely, 1.07% and 1.11% Frame Error Ratio (FER) improvements have been achieved for Additive White Gaussian Noise (AWGN) and Rayleigh fading channels, respectively. Meanwhile, the qualities of the recovered videos are significantly improved. For the 2D videos, the average Peak Signal-to-Noise Ratio (PSNR) and Structural SIMilarity (SSIM) gains reach 14% and 34%, respectively. For the 360. videos, the average improvements in terms of Weighted-to-Spherically-uniform PSNR (WS-PSNR) and Voronoi-based Video Multimethod Assessment Fusion (VI-VMAF) reach 21% and 7%, respectively.
C1 [Lin, Jinzhi] Shenzhen Inst Informat Technol, Shenzhen 518172, Peoples R China.
   [Zhang, Yun; Li, Na] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Jiang, Hongling] Beijing Informat Sci & Technol Univ, Beijing 10092, Peoples R China.
C3 Shenzhen Institute of Information Technology; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS; Beijing
   Information Science & Technology University
RP Zhang, Y (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM linjz@sziit.edu.cn; yun.zhang@siat.ac.cn; na.li1@siat.ac.cn;
   jhl@bistu.edu.cn
RI Zhang, Yun/V-7261-2019; Jiang, Hongling/IAP-7553-2023; Jiang,
   Hongling/IAP-7395-2023
OI Zhang, Yun/0000-0001-9457-7801; Lin, Jinzhi/0000-0002-0704-7579; ,
   Na/0000-0003-3888-7893; Hongling, Jiang/0000-0002-5955-672X
FU National Natural Science Foundation of China [62172400, 61902389];
   Shenzhen Science and Technology Program [JCYJ20180507183823045,
   JCYJ20200109110410133]; Guangdong International Science and Technology
   Cooperative Research Project [2018A050506063]; Youth Innovation
   Promotion Association, Chinese Academy of Sciences [2018392]; Beijing
   Municipal Education Commission Applied Basic Research Project
   [KM202011232022]
FX This work was supported in part by the National Natural Science
   Foundation of China under grants 62172400 and 61902389, in part by the
   Shenzhen Science and Technology Program under grants
   JCYJ20180507183823045 and JCYJ20200109110410133, in part by Guangdong
   International Science and Technology Cooperative Research Project under
   grant 2018A050506063, in part by Membership of Youth Innovation
   Promotion Association, Chinese Academy of Sciences under grant 2018392,
   in part by the Beijing Municipal Education Commission Applied Basic
   Research Project under grant KM202011232022.
CR Afisiadis O, 2014, CONF REC ASILOMAR C, P2116, DOI 10.1109/ACSSC.2014.7094848
   [Anonymous], 2020, SG16WP3 JCTVC ITUT
   [Anonymous], 2010, P IEEE 23 CAN C EL C
   [Anonymous], 2018, Multiplexing and channel coding (Release 10)
   [Anonymous], 2016, VIEW 5G ARCH
   Arikan E, 2009, IEEE T INFORM THEORY, V55, P3051, DOI 10.1109/TIT.2009.2021379
   Beeharry Yogesh, 2019, ECTI T ELECT ENG ELE, V17, P1, DOI [10.37936/ectieec.2019171.215363, DOI 10.37936/ECTIEEC.2019171.215363]
   Ben Abdessalem M, 2020, PHYS COMMUN-AMST, V38, DOI 10.1016/j.phycom.2019.100947
   Bossen F., 2013, JCTVCM1010
   Bourtsoulatze E, 2019, IEEE T COGN COMMUN, V5, P567, DOI 10.1109/TCCN.2019.2919300
   Cassagne A, 2019, SOFTWAREX, V10, DOI 10.1016/j.softx.2019.100345
   Chen QW, 2019, IEEE COMMUN SURV TUT, V21, P2977, DOI 10.1109/COMST.2019.2894154
   Colonnese S, 2018, IEEE T MOBILE COMPUT, V17, P1497, DOI 10.1109/TMC.2017.2774298
   Croci S., 2020, QUAL USER EXPER, V5, P1, DOI 10.1007/s41233-020-00032-3
   Duhamel P., 2009, JOINT SOURCE CHANNEL
   Duhamel Pierre, 2009, JOINT SOURCE CHANNEL, P13
   Dumitrescu S, 2010, IEEE T COMMUN, V58, P128, DOI 10.1109/TCOMM.2010.01.080091
   EPANECHN.VA, 1969, THEOR PROBAB APPL+, V14, P153, DOI 10.1137/1114019
   Ercan F, 2019, IEEE T COMMUN, V67, P61, DOI 10.1109/TCOMM.2018.2873322
   Fan S, 2018, CHINA COMMUN, V15, P215, DOI 10.1109/CC.2018.8456464
   Fresia M, 2010, IEEE SIGNAL PROC MAG, V27, P104, DOI 10.1109/MSP.2010.938080
   Gong SP, 2017, IEEE T WIREL COMMUN, V16, P7560, DOI 10.1109/TWC.2017.2750688
   Hadi A, 2018, IET COMMUN, V12, P956, DOI 10.1049/iet-com.2017.1195
   He Yuwen, 2017, 360LIB SOFTWARE MANU
   HoangVan X, 2019, IEEE T BROADCAST, V65, P504, DOI 10.1109/TBC.2018.2881355
   Huo YK, 2013, IEEE T VEH TECHNOL, V62, P1597, DOI 10.1109/TVT.2012.2227072
   ITU Telecommunication Standardization Sector, 2019, REC ITU T H 265 HIGH
   Jin LQ, 2018, IEEE ACCESS, V6, P7340, DOI 10.1109/ACCESS.2017.2788887
   Jin LQ, 2018, IEEE COMMUN LETT, V22, P49, DOI 10.1109/LCOMM.2017.2768036
   Khas M, 2018, IET COMMUN, V12, P1003, DOI 10.1049/iet-com.2017.1084
   Khattak HA, 2019, COMPUT SCI INF SYST, V16, P1, DOI 10.2298/CSIS181115004K
   Kourkchi H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1768, DOI 10.1109/ICASSP.2018.8462054
   Kurka David Burth, 2020, IEEE Journal on Selected Areas in Information Theory, V1, P178, DOI 10.1109/JSAIT.2020.2987203
   Levine D, 2010, SIGNAL PROCESS-IMAGE, V25, P75, DOI 10.1016/j.image.2009.12.006
   Li PH, 2020, IEEE ACCESS, V8, P85128, DOI 10.1109/ACCESS.2020.2990198
   Lin JZ, 2020, NEUROCOMPUTING, V382, P52, DOI 10.1016/j.neucom.2019.11.066
   Liu Z, 2018, SIGNAL PROCESS, V147, P154, DOI 10.1016/j.sigpro.2018.01.009
   Luo L, 2019, IEEE T MULTIMEDIA, V21, P2973, DOI 10.1109/TMM.2019.2919474
   Nasruminallah, 2012, IEEE COMMUN SURV TUT, V14, P538, DOI 10.1109/SURV.2011.032211.00118
   Nguyen DV, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3373359
   Park J., 2018, 2018 IEEE GLOB COMM, P206, DOI DOI 10.1109/GLOCOM.2018.8647389
   Perera R, 2016, IEEE ACCESS, V4, P7186, DOI 10.1109/ACCESS.2016.2619259
   Perfecto C, 2020, IEEE T COMMUN, V68, P2491, DOI 10.1109/TCOMM.2020.2965527
   Scott DW, 2015, WILEY SER PROBAB ST, P1, DOI 10.1002/9781118575574
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Wang Y, 2005, IEEE T CONSUM ELECTR, V51, P1273, DOI 10.1109/TCE.2005.1561855
   Wang Y, 2016, IEEE INT SYMP CIRC S, P1, DOI 10.1109/ISCAS.2016.7527155
   Watanabe Y, 2016, IEEE T BROADCAST, V62, P598, DOI 10.1109/TBC.2016.2576599
   Xiang L, 2017, IEEE T VEH TECHNOL, V66, P11366, DOI 10.1109/TVT.2017.2720481
   Zare A, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3335053
   Zheng Yuan, 2012, 2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet), P3288, DOI 10.1109/CECNet.2012.6201904
   Zhu XL, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240143
   Zink M, 2019, P IEEE, V107, P639, DOI 10.1109/JPROC.2019.2894817
NR 53
TC 1
Z9 1
U1 2
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 100
DI 10.1145/3502208
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600011
OA Bronze
DA 2024-07-18
ER

PT J
AU Zheng, Y
   Zhou, Y
   Zhao, JQ
   Chen, Y
   Yao, R
   Liu, B
   El Saddik, A
AF Zheng, Yi
   Zhou, Yong
   Zhao, Jiaqi
   Chen, Ying
   Yao, Rui
   Liu, Bing
   El Saddik, Abdulmotaleb
TI Clustering Matters: Sphere Feature for Fully Unsupervised Person
   Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; deep learning; unsupervised learning; feature
   mapping; hierarchical clustering; sphere feature
AB In person re-identification (Re-ID), the data annotation cost of supervised learning, is huge and it cannot adapt well to complex situations. Therefore, compared with supervised deep learning methods, unsupervised methods are more in line with actual needs. In unsupervised learning, a key to solving Re-ID is to find a standard that can effectively distinguish the difference (distance) between the features of images belonging to different pedestrian identities. However, there are some differences in the images captured by different cameras (such as brightness, angle, etc.). It is well known that the training of neural networks is mainly based on the distance between features, while in unsupervised learning, especially in unsupervised learning methods based on hierarchical clustering, the distance between features plays a more important role in the clustering phase. We improve the accuracy of a deep learning method based on hierarchical clustering under fully unsupervised conditions, starting from both feature and distance metrics. First, we propose to use spherical features, by normalizing the images in the feature space, to weaken the structural differences (length) between features, while saving the feature differences (direction) between different identities. Then, we use the sum of squared errors (SSE) as a regularization term to balance different cluster states. We evaluate our method on four large-scale Re-ID datasets, and experiments show that our method achieves better results than the state-of-the-art unsupervised methods.
C1 [Zheng, Yi; Zhou, Yong; Zhao, Jiaqi; Chen, Ying; Yao, Rui; Liu, Bing] China Univ Min & Technol, Engn Res Ctr Mine Digitizat, Minist Educ Peoples Republ China, Sch Comp Sci & Technol, Xuzhou, Jiangsu, Peoples R China.
   [El Saddik, Abdulmotaleb] Univ Ottawa, Multimedia Commun Res Lab MCRLab, Sch Elect Engn & Comp Sci, 800 King Edward, Ottawa, ON K1N 6N5, Canada.
C3 China University of Mining & Technology; University of Ottawa
RP Zheng, Y (corresponding author), China Univ Min & Technol, Engn Res Ctr Mine Digitizat, Minist Educ Peoples Republ China, Sch Comp Sci & Technol, Xuzhou, Jiangsu, Peoples R China.
EM yizheng@cumt.edu.cn; yzhou@cumt.edu.cn; jiaqizhao@cumt.edu.cn;
   cheny@cumt.edu.cn; ruiyao@cumt.edu.cn; liubing@cumt.edu.cn;
   elsaddik@uottawa.ca
FU National Natural Science Foundation of China [61806206, 62172417];
   Natural Science Foundation of Jiangsu Province [BK20180639, BK20201346];
   Six Talent Peaks Project in Jiangsu Province [2015-DZXX-010,
   2018-XYDXX-044]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 61806206, 62172417), Natural Science Foundation of
   Jiangsu Province (Grant Nos. BK20180639, BK20201346) and Six Talent
   Peaks Project in Jiangsu Province (Grant Nos. 2015-DZXX-010,
   2018-XYDXX-044).
CR Ainam JP, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377352
   Bai ZC, 2021, PROC CVPR IEEE, P12909, DOI 10.1109/CVPR46437.2021.01272
   Chen YC, 2018, PR MACH LEARN RES, V80
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Chuanchen Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P224, DOI 10.1007/978-3-030-58555-6_14
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding G., 2019, ARXIV190601308
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fan X, 2019, J VIS COMMUN IMAGE R, V60, P51, DOI 10.1016/j.jvcir.2019.01.010
   Grigorev A, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3360050
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YY, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3412384
   Li ZJ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3362988
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Qi L, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419439
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Ruan WJ, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3402666
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tian MQ, 2018, PROC CVPR IEEE, P5794, DOI 10.1109/CVPR.2018.00607
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang J, 2018, PATTERN RECOGN, V74, P38, DOI 10.1016/j.patcog.2017.09.014
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xing E.P., Advances in neural information processing systems, 2003, P521
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Ye M, 2019, IEEE T IMAGE PROCESS, V28, P2976, DOI 10.1109/TIP.2019.2893066
   Ye M, 2018, LECT NOTES COMPUT SC, V11211, P176, DOI 10.1007/978-3-030-01234-2_11
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yi Zheng, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11857), P409, DOI 10.1007/978-3-030-31654-9_35
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Yu TY, 2019, IEEE I CONF COMP VIS, P552, DOI 10.1109/ICCV.2019.00064
   Zhang AG, 2021, PROC CVPR IEEE, P598, DOI 10.1109/CVPR46437.2021.00066
   Zhang Xucong, 2017, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), V41, P162
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
NR 66
TC 11
Z9 11
U1 0
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 97
DI 10.1145/3501404
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600008
DA 2024-07-18
ER

PT J
AU Yao, LX
   Kusakunniran, W
   Wu, Q
   Xu, JS
   Zhang, J
AF Yao, Lingxiang
   Kusakunniran, Worapan
   Wu, Qiang
   Xu, Jingsong
   Zhang, Jian
TI Recognizing Gaits Across Walking and Running Speeds
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-mode gait recognition; walking mode; running mode
ID RECOGNITION
AB For decades, very few methods were proposed for cross-mode (i.e., walking vs. running) gait recognition. Thus, it remains largely unexplored regarding how to recognize persons by the way they walk and run. Existing cross-mode methods handle the walking-versus-running problem in two ways, either by exploring the generic mapping relation between walking and running modes or by extracting gait features which are non-/less vulnerable to the changes across these two modes. However, for the first approach, a mapping relation fit for one person may not be applicable to another person. There is no generic mapping relation given that walking and running are two highly self-related motions. The second approach does not give more attention to the disparity between walking and running modes, since mode labels are not involved in their feature learning processes. Distinct from these existing cross-mode methods, in our method, mode labels are used in the feature learning process, and a mode-invariant gait descriptor is hybridized for cross-mode gait recognition to handle this walking-versus-running problem. Further research is organized in this article to investigate the disparity between walking and running. Running is different from walking not only in the speed variances but also, more significantly, in prominent gesture/motion changes. According to these rationales, in our proposed method, we give more attention to the differences between walking and running modes, and a robust gait descriptor is developed to hybridize the mode-invariant spatial and temporal features. Two multi-task learning-based networks are proposed in this method to explore these mode-invariant features. Spatial features describe the body parts non-/less affected by mode changes, and temporal features depict the instinct motion relation of each person. Mode labels are also adopted in the training phase to guide the network to give more attention to the disparity across walking and running modes. In addition, relevant experiments on OU-ISIR Treadmill Dataset A have affirmed the effectiveness and feasibility of the proposed method. A state-of-the-art result can be achieved by our proposed method on this dataset.
C1 [Yao, Lingxiang; Wu, Qiang; Xu, Jingsong; Zhang, Jian] Univ Technol Sydney, Sch Elect & Data Engn, 15 Broadway, Ultimo, NSW 2007, Australia.
   [Kusakunniran, Worapan] Mahidol Univ, Fac Informat & Commun Technol, 999 Phuttamonthon 4 Rd, Salaya 73170, Nakhon Pathom, Thailand.
C3 University of Technology Sydney; Mahidol University
RP Kusakunniran, W (corresponding author), Mahidol Univ, Fac Informat & Commun Technol, 999 Phuttamonthon 4 Rd, Salaya 73170, Nakhon Pathom, Thailand.
EM Lingxiang.Yao@student.uts.edu.au; worapan.kun@mahidol.edu;
   Qiang.Wu@uts.edu.au; Jingsong.Xu@uts.edu.au; Jian.Zhang@uts.edu.au
OI Zhang, Jian/0000-0002-7240-3541; Yao, Lingxiang/0000-0002-6570-985X;
   Kusakunniran, Worapan/0000-0002-2896-611X; Wu, Qiang/0000-0001-5641-2483
CR [Anonymous], 2005, Establishing Pedestrian Walking Speeds
   [Anonymous], 1999, GAIT RECOGNITION DAT
   Chao Fan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14213, DOI 10.1109/CVPR42600.2020.01423
   Chao H., 2018, ARXIV181106186
   Chi Xu, IEEE INT JOINT C BIO, P1
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feng Y, 2016, INT C PATT RECOG, P325, DOI 10.1109/ICPR.2016.7899654
   Guan Y, 2013, INT CONF BIOMETR
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hermans Alexander, 2017, ARXIV170307737
   Huang HX, 2019, IEEE INT CON MULTI, P91, DOI 10.1109/ICME.2019.00024
   Huang Y, 2019, IEEE I CONF COMP VIS, P9526, DOI 10.1109/ICCV.2019.00962
   Huang Y, 2019, IEEE T IMAGE PROCESS, V28, P1391, DOI 10.1109/TIP.2018.2874715
   King DB, 2015, ACS SYM SER, V1214, P1
   Liu ZY, 2006, IEEE T PATTERN ANAL, V28, P863, DOI 10.1109/TPAMI.2006.122
   Makihara Yasushi, 2012, IPSJ Transactions on Computer Vision and Applications, V4, P42, DOI 10.2197/ipsjtcva.4.53
   Makihara Y, 2012, INT C PATT RECOG, P3276
   Mansur A, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014)
   Shiraga K, 2016, INT CONF BIOMETR
   Simonyan K, 2014, ADV NEUR IN, V27
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sundaresan A, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P93
   Takemura N, 2019, IEEE T CIRC SYST VID, V29, P2708, DOI 10.1109/TCSVT.2017.2760835
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   TranSafety Inc, 1997, STUD COMP OLD YOUNG
   Wang YT, 2018, LECT NOTES COMPUT SC, V11219, P764, DOI 10.1007/978-3-030-01267-0_45
   Winter DA, 2009, BIOMECHANICS MOTOR C, DOI [10.1002/9780470549148, DOI 10.1002/9780470549148]
   Wolf T, 2016, IEEE IMAGE PROC, P4165, DOI 10.1109/ICIP.2016.7533144
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Wu ZF, 2015, IEEE T MULTIMEDIA, V17, P1960, DOI 10.1109/TMM.2015.2477681
   Xiang Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13306, DOI 10.1109/CVPR42600.2020.01332
   Xu C, 2021, IEEE T CIRC SYST VID, V31, P260, DOI 10.1109/TCSVT.2020.2975671
   Xu C, 2019, MULTIMED TOOLS APPL, V78, P26509, DOI 10.1007/s11042-019-7712-3
   Yam C.-Y., 2002, Gait recognition by walking and running: A model-based approach
   Yam CY, 2001, LECT NOTES COMPUT SC, V2091, P278
   Yam CY, 2002, INT C PATT RECOG, P287, DOI 10.1109/ICPR.2002.1044691
   Yang K, 2016, J VIS COMMUN IMAGE R, V39, P209, DOI 10.1016/j.jvcir.2016.05.020
   Yao L., 2017, 2017 IEEE Conf on Environment and Electrical Engineering and 2017 IEEE Industrial and Commerical Power Systems Europe(EEEIC/ICPS Europe), P1
   Yao LX, 2021, PATTERN RECOGN LETT, V150, P289, DOI 10.1016/j.patrec.2019.05.012
   Yao LX, 2018, 2018 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P297
   Zhang C, 2016, INT CONF ACOUST SPEE, P2832, DOI 10.1109/ICASSP.2016.7472194
   Zhang P, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8852258
   Zhang YQ, 2020, IEEE T IMAGE PROCESS, V29, P1001, DOI 10.1109/TIP.2019.2926208
NR 44
TC 0
Z9 0
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 75
DI 10.1145/3488715
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600009
DA 2024-07-18
ER

PT J
AU He, J
   Hong, RC
   Liu, XL
   Xu, ML
   Sun, QR
AF He, Jun
   Hong, Richang
   Liu, Xueliang
   Xu, Mingliang
   Sun, Qianru
TI Revisiting Local Descriptor for Improved Few-Shot Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Few-shot learning; image classification; visual recognition;
   meta-learning; attention networks
AB Few-shot classification studies the problem of quickly adapting a deep learner to understanding novel classes based on few support images. In this context, recent research efforts have been aimed at designing more and more complex classifiers that measure similarities between query and support images but left the importance of feature embeddings seldom explored. We show that the reliance on sophisticated classifiers is not necessary, and a simple classifier applied directly to improved feature embeddings can instead outperform most of the leading methods in the literature. To this end, we present a new method, named DCAP, for few-shot classification, in which we investigate how one can improve the quality of embeddings by leveraging Dense Classification and Attentive Pooling (DCAP). Specifically, we propose to train a learner on base classes with abundant samples to solve dense classification problem first and then meta-train the learner on plenty of randomly sampled few-shot tasks to adapt it to few-shot scenario or the test time scenario. During meta-training, we suggest to pool feature maps by applying attentive pooling instead of the widely used global average pooling to prepare embeddings for few-shot classification. Attentive pooling learns to reweight local descriptors, explaining what the learner is looking for as evidence for decision making. Experiments on two benchmark datasets show the proposed method to be superior in multiple few-shot settings while being simpler and more explainable. Code is publicly available at https://github.com/Ukeyboard/dcap/.
C1 [He, Jun; Hong, Richang; Liu, Xueliang] Hefei Univ Technol, Intelligent Interconnected Syst Lab Anhui Prov, 193 Tunxi Rd, Hefei 230009, Anhui, Peoples R China.
   [He, Jun; Hong, Richang; Liu, Xueliang] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, 5089 Wangjiang West Rd, Hefei 230026, Anhui, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, 100 Sci Ave, Zhengzhou 450001, Henan, Peoples R China.
   [Sun, Qianru] Singapore Management Univ, 80 Stamford Rd, Singapore 178902, Singapore.
C3 Hefei University of Technology; Zhengzhou University; Singapore
   Management University
RP Hong, RC (corresponding author), Hefei Univ Technol, Intelligent Interconnected Syst Lab Anhui Prov, 193 Tunxi Rd, Hefei 230009, Anhui, Peoples R China.; Hong, RC (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, 5089 Wangjiang West Rd, Hefei 230026, Anhui, Peoples R China.
EM hyj.hfut.mail@gmail.com; hongrc.hfut@gmail.com;
   liuxueliang1982@gmail.com; iexumingliang@zzu.edu.cn;
   qianrusun@smu.edu.sg
RI Jiang, Yuan/JED-3759-2023; zhang, xueying/JMB-7808-2023; Zhou,
   heng/JCN-6493-2023; Zhang, Jinfan/JPK-7588-2023; Yu, ZH/KBC-6889-2024;
   wu, meng/JPK-1930-2023; wang, wenjuan/JGD-0428-2023
OI wang, wenjuan/0000-0002-4220-8817
CR [Anonymous], 2011, The Caltech-UCSD Birds-200-2011 Dataset
   Aoxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12573, DOI 10.1109/CVPR42600.2020.01259
   Baik Sungyong, 2020, P 33TH INT C NEURAL
   Bendre N, 2020, arXiv
   Bin Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1345, DOI 10.1145/3123266.3123391
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen WY., 2019, P 7 INT C LEARNING R
   Chen YB, 2021, Arxiv, DOI [arXiv:2003.04390, 10.48550/arXiv.2003.04390]
   Dan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10052, DOI 10.1109/CVPR42600.2020.01007
   Dhillon G. S., 2020, INT C LEARN REPR
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Doersch Carl, 2020, P 34 ANN C NEURAL IN
   Finn C, 2017, PR MACH LEARN RES, V70
   Furlanello T., 2018, P MACHINE LEARNING R, V80, P1607, DOI DOI 10.48550/ARXIV.1805.04770
   Gidaris Spyros, 2018, P 6 INT C LEARNING R
   Guo D, 2022, IEEE T PATTERN ANAL, V44, P6056, DOI 10.1109/TPAMI.2021.3085755
   Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   He J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1236, DOI 10.1145/3394171.3413811
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hospedales T, 2020, Arxiv, DOI arXiv:2004.05439
   Hou RB, 2019, ADV NEUR IN, V32
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jamal MA, 2019, PROC CVPR IEEE, P11711, DOI 10.1109/CVPR.2019.01199
   Jiang ZH, 2020, Arxiv, DOI arXiv:2008.02465
   Jong-Chyi Su, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P645, DOI 10.1007/978-3-030-58571-6_38
   Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010
   Krizhevsky A., 2012, Advances in Neural Information Processing Systems, V25, P1106
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Li SC, 2019, IEEE I CONF COMP VIS, P6101, DOI 10.1109/ICCV.2019.00620
   Li WB, 2019, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR.2019.00743
   Li YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4091, DOI 10.1145/3474085.3475540
   Li ZG, 2017, Arxiv, DOI arXiv:1707.09835
   Lifchitz Y, 2019, PROC CVPR IEEE, P9250, DOI 10.1109/CVPR.2019.00948
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu XL, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3375787
   Liu Y., 2019, 7 INT C LEARNING REP
   Mangla P, 2020, IEEE WINT CONF APPL, P2207, DOI [10.1109/wacv45572.2020.9093338, 10.1109/WACV45572.2020.9093338]
   Meng L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P557, DOI 10.1145/3343031.3350870
   Mishra Nikhil, 2018, P 6 INT C LEARNING R
   Nichol A, 2018, Arxiv, DOI [arXiv:1803.02999, DOI 10.48550/ARXIV.1803.02999]
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Raghu A, 2020, Arxiv, DOI arXiv:1909.09157
   Ravi S, 2016, OPTIMIZATION MODEL F
   Ren Mengye, 2018, P 5 INT C LEARNING R
   Rodriguez Pau, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P121, DOI 10.1007/978-3-030-58574-7_8
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Rusu Andrei A., 2019, P INT C LEARNING REP
   Santoro A, 2016, PR MACH LEARN RES, V48
   Satorras V.G., 2018, ICLR
   Shang XD, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P279, DOI 10.1145/3323873.3325056
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Simon C, 2020, PROC CVPR IEEE, P4135, DOI 10.1109/CVPR42600.2020.00419
   Snell J, 2017, ADV NEUR IN, V30
   Snell Jake, 2021, P 9 INT C LEARNING R
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tan Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P592, DOI 10.1145/3474085.3475218
   Tian YL, 2020, Arxiv, DOI arXiv:2003.11539
   Tseng H. Y., 2020, ICLR
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wertheimer D, 2021, PROC CVPR IEEE, P8008, DOI 10.1109/CVPR46437.2021.00792
   Wu ZY, 2019, IEEE I CONF COMP VIS, P6658, DOI 10.1109/ICCV.2019.00676
   Xu H, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3153
   Xu H, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107951
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Yang X, 2022, IEEE T IMAGE PROCESS, V31, P1204, DOI 10.1109/TIP.2022.3140611
   Yang X, 2019, IEEE T NEUR NET LEAR, V30, P2987, DOI [10.1109/TNNLS.2018.2861991, 10.1109/TNNLS.2018.2790479]
   Yoon J, 2018, ADV NEUR IN, V31
   Zhang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6933, DOI 10.1109/ICCV48922.2021.00687
   Zhang ZZ, 2021, Arxiv, DOI arXiv:2010.04525
   Zhou YAN, 2020, PROC CVPR IEEE, P4776, DOI 10.1109/CVPR42600.2020.00483
NR 78
TC 13
Z9 14
U1 4
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 127
DI 10.1145/3511917
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000014
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Yang, CY
   Xu, XL
   Zhou, XK
   Qi, LY
AF Yang, Chenyi
   Xu, Xiaolong
   Zhou, Xiaokang
   Qi, Lianyong
TI Deep Q Network-Driven Task Offloading for Efficient Multimedia Data
   Analysis in Edge Computing-Assisted IoV
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep reinforcement learning; Fuzzy C-means; Deep Q Network; multimedia
   data; edge computing; Industry 4.0; Internet of Vehicles
ID INTERNET
AB With the prosperity of Industry 4.0, numerous emerging industries continue to gain popularity and their market scales are expanding ceaselessly. The Internet of Vehicles (IoV), one of the thriving intelligent industries, enjoys bright development prospects. However, at the same time, the reliability and availability of IoV applications are confronted with two major bottlenecks of time delay and energy consumption. To make matters worse, massive heterogeneous and multi-dimensional multimedia data generated on the IoV present a huge obstacle to effective data analysis. Fortunately, the advent of edge computing technology enables tasks to be offloaded to edge servers, which significantly reduces total overhead of IoV systems. Deep reinforcement learning (DRL), equipped with its excellent perception and decision-making capability, is undoubtedly a dominant technology to solve task offloading problems. In this article, we first employ an optimized Fuzzy C-means algorithm to cluster vehicles and other edge devices according to their respective service quality requirements. Then, we employ an election algorithm to assist in maintaining the stability of the IoV. Last, we propose a task-offloading algorithm based on the Deep Q Network (DQN) to acquire an optimal task offloading scheme. Massive simulation experiments demonstrate the superiority of our method in minimizing time delay and energy consumption.
C1 [Yang, Chenyi; Xu, Xiaolong] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing, Peoples R China.
   [Xu, Xiaolong] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
   [Zhou, Xiaokang] Shiga Univ, Fac Data Sci, Hikone, Shiga, Japan.
   [Zhou, Xiaokang] RIKEN Ctr Adv Intelligence Project, Tokyo, Japan.
   [Qi, Lianyong] Qufu Normal Univ, Sch Comp Sci, Jining, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University; Shiga University; RIKEN; Qufu Normal University
RP Zhou, XK (corresponding author), Shiga Univ, Fac Data Sci, Hikone, Shiga, Japan.; Zhou, XK (corresponding author), RIKEN Ctr Adv Intelligence Project, Tokyo, Japan.
EM yangchenyi@nuist.edu.cn; xlxu@nuist.edu.cn; zhou@biwako.shiga-u.ac.jp;
   lianyongqi@gmail.com
RI Xu, Xiaolong/U-2547-2019; Qi, Lianyong/AAO-2681-2020
OI Xu, Xiaolong/0000-0003-4879-9803; 
CR Ale L, 2021, IEEE T COGN COMMUN, V7, P881, DOI 10.1109/TCCN.2021.3066619
   BEZDEK JC, 1984, COMPUT GEOSCI, V10, P191, DOI 10.1016/0098-3004(84)90020-7
   Bi R, 2021, TSINGHUA SCI TECHNOL, V26, P239, DOI 10.26599/TST.2019.9010062
   Cai T, 2022, IEEE T NETW SCI ENG, V9, P3197, DOI 10.1109/TNSE.2021.3121690
   Chang Z, 2018, IEEE T VEH TECHNOL, V67, P1570, DOI 10.1109/TVT.2017.2762745
   Chen XF, 2019, IEEE INTERNET THINGS, V6, P4005, DOI 10.1109/JIOT.2018.2876279
   Dalenogare LS, 2018, INT J PROD ECON, V204, P383, DOI 10.1016/j.ijpe.2018.08.019
   Ganaie M. A., 2021, ARXIV
   He ZL, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3447878
   Hoppner Frank, 1999, FUZZY CLUSTER ANAL M, V51, P769
   Huang HX, 2021, 2021 IEEE CONFERENCE ON DEPENDABLE AND SECURE COMPUTING (DSC), DOI 10.1109/DSC49826.2021.9346233
   Islam S, 2019, IEEE SYS MAN CYBERN, P4341, DOI 10.1109/SMC.2019.8914581
   Ke HC, 2020, IEEE T VEH TECHNOL, V69, P7916, DOI 10.1109/TVT.2020.2993849
   Kizilkaya B, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473037
   Li J, 2018, IEEE WCNC, DOI 10.1109/WCNC.2018.8377343
   Li Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3529107
   Liang W, 2022, IEEE T IND INFORM, V18, P5087, DOI 10.1109/TII.2021.3116085
   Mnih V, 2013, ARXIV
   Qi LY, 2023, IEEE T KNOWL DATA EN, V35, P5444, DOI 10.1109/TKDE.2022.3168611
   Qi Q, 2019, IEEE T VEH TECHNOL, V68, P4192, DOI 10.1109/TVT.2019.2894437
   Qu GJ, 2021, IEEE T NETW SERV MAN, V18, P3448, DOI 10.1109/TNSM.2021.3087258
   Ren JY, 2022, TSINGHUA SCI TECHNOL, V27, P760, DOI 10.26599/TST.2021.9010046
   Sahu AK, 2021, COMPUT COMMUN, V176, P146, DOI 10.1016/j.comcom.2021.05.024
   Sandhu AK, 2022, BIG DATA MIN ANAL, V5, P32, DOI 10.26599/BDMA.2021.9020016
   Shi JM, 2020, IEEE T VEH TECHNOL, V69, P16067, DOI 10.1109/TVT.2020.3041929
   Shi WS, 2016, IEEE INTERNET THINGS, V3, P637, DOI 10.1109/JIOT.2016.2579198
   Tang M, 2022, IEEE T MOBILE COMPUT, V21, P1985, DOI 10.1109/TMC.2020.3036871
   Tanveer M, 2022, IEEE J BIOMED HEALTH, V26, P1453, DOI 10.1109/JBHI.2021.3083274
   Tong Z, 2021, BIG DATA MIN ANAL, V4, P155, DOI 10.26599/BDMA.2020.9020029
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Wang J, 2021, IEEE T PARALL DISTR, V32, P242, DOI 10.1109/TPDS.2020.3014896
   Wang K, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2990473
   Wu HM, 2018, IEEE T MOBILE COMPUT, V17, P461, DOI 10.1109/TMC.2017.2711014
   Xu XL, 2022, IEEE T FUZZY SYST, V30, P4593, DOI 10.1109/TFUZZ.2022.3158000
   Xu XL, 2022, IEEE T SERV COMPUT, V15, P1206, DOI 10.1109/TSC.2022.3142265
   Xu XL, 2021, ACM T SENSOR NETWORK, V17, DOI 10.1145/3447032
   Xu XL, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3401979
   Xu XL, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3408319
   Yang L, 2022, IEEE T IND INFORM, V18, P8864, DOI 10.1109/TII.2021.3128954
   YIN X, 2021, IEEE T INTELL TRANSP, P1, DOI DOI 10.1109/TITS.2021.3118701
   Zhang FQ, 2021, IEEE T VEH TECHNOL, V70, P237, DOI 10.1109/TVT.2020.3045271
   Zhang W, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3279952
   Zhou XK, 2022, IEEE INTERNET THINGS, V9, P9310, DOI 10.1109/JIOT.2021.3130434
   Zhou XK, 2022, IEEE INTERNET THINGS, V9, P14988, DOI 10.1109/JIOT.2021.3077937
   Zhou XK, 2021, IEEE INTERNET THINGS, V8, P12588, DOI 10.1109/JIOT.2021.3077449
   Zhou XK, 2021, IEEE T COMPUT SOC SY, V8, P171, DOI 10.1109/TCSS.2020.2987846
   Zhu HY, 2021, IEEE T VEH TECHNOL, V70, P9787, DOI 10.1109/TVT.2021.3105478
   Zimmermann HJ, 2010, WIRES COMPUT STAT, V2, P317, DOI 10.1002/wics.82
NR 48
TC 7
Z9 7
U1 0
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 124
DI 10.1145/3548687
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000011
DA 2024-07-18
ER

PT J
AU Cetinic, E
   She, J
AF Cetinic, Eva
   She, James
TI Understanding and Creating Art with AI: Review and Outlook
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Review
DE AI Art; deep learning; generative art; computational creativity;
   generative adversarial networks; convolutional neural networks; visual
   arts; image understanding
ID CLASSIFICATION; PAINTINGS; HISTORY; STYLOMETRY; STATISTICS
AB Technologies related to artificial intelligence (AI) have a strong impact on the changes of research and creative practices in visual arts. The growing number of research initiatives and creative applications that emerge in the intersection of AI and art motivates us to examine and discuss the creative and explorative potentials of AI technologies in the context of art. This article provides an integrated review of two facets of AI and art: (1) AI is used for art analysis and employed on digitized artwork collections, or (2) AI is used for creative purposes and generating novel artworks. In the context of AI-related research for art understanding, we present a comprehensive overview of artwork datasets and recent works that address a variety of tasks such as classification, object detection, similarity retrieval, multimodal representations, and computational aesthetics, among others. In relation to the role of AI in creating art, we address various practical and theoretical aspects of AI Art and consolidate related works that deal with those topics in detail. Finally, we provide a concise outlook on the future progression and potential impact of AI technologies on our understanding and creation of art.
C1 [Cetinic, Eva] Rudjer Boskovic Inst, Zagreb, Croatia.
   [Cetinic, Eva] Univ Durham, Durham, England.
   [She, James] Hamad Bin Khalifa Univ HBKU, Doha, Qatar.
   [She, James] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Cetinic, Eva] Inst Rudjer Boskovic, Ctr Informat & Comp, Bijenicka Cesta 54, Zagreb 10000, Croatia.
   [She, James] Hamad Bin Khalifa Univ, Coll Sci & Engn, Div Informat & Comp Technol, Doha, Qatar.
   [She, James] Hong Kong Univ Sci & Technol, HKUST NIE Social Media Lab, Kowloon, Clear Water Bay, Hong Kong, Peoples R China.
C3 Rudjer Boskovic Institute; Durham University; Qatar Foundation (QF);
   Hamad Bin Khalifa University-Qatar; Hong Kong University of Science &
   Technology; Rudjer Boskovic Institute; Qatar Foundation (QF); Hamad Bin
   Khalifa University-Qatar; Hong Kong University of Science & Technology
RP Cetinic, E (corresponding author), Inst Rudjer Boskovic, Ctr Informat & Comp, Bijenicka Cesta 54, Zagreb 10000, Croatia.
EM ecetinic@irb.hr; pshe@hbku.edu.qa
OI Cetinic, Eva/0000-0002-5330-1259
CR Abry P, 2013, SIGNAL PROCESS, V93, P554, DOI 10.1016/j.sigpro.2012.01.016
   Achlioptas Panos, 2021, P IEEE CVF C COMP VI
   Agarwal Siddharth, 2015, 2015 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P588, DOI 10.1109/WACV.2015.84
   Alameda-Pineda X, 2016, PROC CVPR IEEE, P5240, DOI 10.1109/CVPR.2016.566
   Amirshahi SA, 2015, LECT NOTES COMPUT SC, V8925, P3, DOI 10.1007/978-3-319-16178-5_1
   [Anonymous], 2012, J. Comput. Cult. Herit., DOI DOI 10.1145/2307723.2307726
   Bar Y, 2015, LECT NOTES COMPUT SC, V8925, P71, DOI 10.1007/978-3-319-16178-5_5
   Baraldi L, 2018, INT C PATT RECOG, P1097, DOI 10.1109/ICPR.2018.8545064
   Bell Peter, 2019, Das Mittelalter, V24, P31
   Bianco S, 2019, EXPERT SYST APPL, V135, P90, DOI 10.1016/j.eswa.2019.05.036
   Boden MA, 2009, DIGIT CREAT, V20, P21, DOI 10.1080/14626260902867915
   Boden Margaret A., 2010, Creativity and Art: Three Roads to Surprise
   Bongini Pietro, 2020, ARXIV PREPRINT ARXIV
   Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877
   Bsteh S, 2021, From painting to pixel: Understanding NFT artworks
   Carneiro G, 2012, LECT NOTES COMPUT SC, V7575, P143, DOI 10.1007/978-3-642-33765-9_11
   Castellano G, 2020, COMM COM INF SC, V1177, P105, DOI 10.1007/978-3-030-39905-4_11
   Castellano G, 2021, MULTIMED TOOLS APPL, V80, P6599, DOI 10.1007/s11042-020-09995-z
   Cetinic Eva, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P19
   Cetinic E., 2020, PATTERN RECOGN, V12663, P502, DOI 10.1007/978-3-030-68796- 0_36
   Cetinic E, 2020, PATTERN RECOGN LETT, V129, P56, DOI 10.1016/j.patrec.2019.11.008
   Cetinic E, 2019, IEEE ACCESS, V7, P73694, DOI 10.1109/ACCESS.2019.2921101
   Cetinic E, 2018, EXPERT SYST APPL, V114, P107, DOI 10.1016/j.eswa.2018.07.026
   Cetinic E, 2016, ELMAR PROC, P201, DOI 10.1109/ELMAR.2016.7731786
   Ch'ng E, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3326338
   Chamberlain R, 2018, PSYCHOL AESTHET CREA, V12, P177, DOI 10.1037/aca0000136
   Christie's, 2018, Is artificial intelligence set to become art's next medium?
   Christie's, 2021, MONUMENTAL COLLAGE B
   Coeckelbergh M., 2017, PHILOS TECHNOLOGY, V30, P285, DOI [DOI 10.1007/S13347-016-0231-5, https://doi.org/10.1007/s13347-016-0231-5, 10.1007/s13347-016-0231-5]
   Colton S., 2012, COMPUTERS CREATIVITY, P3, DOI DOI 10.1007/978-3-642-31727-9_1
   Colton Simon, 2018, P 9 INT C COMPUTATIO
   Crowley Elliot J., 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P721, DOI 10.1007/978-3-319-46604-0_50
   Crowley E. J., 2015, P BRIT MACHINE VISIO
   Crowley EJ, 2015, LECT NOTES COMPUT SC, V8925, P54, DOI 10.1007/978-3-319-16178-5_4
   Crowley Elliot J, 2014, P 2014 BRIT MACHINE
   Daniele A., 2019, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES'19, P155, DOI DOI 10.1145/3306618.3314233
   David OE, 2016, LECT NOTES COMPUT SC, V9887, P20, DOI 10.1007/978-3-319-44781-0_3
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng YY, 2021, IEEE T MULTIMEDIA, V23, P2794, DOI 10.1109/TMM.2020.3016887
   Dorin A., 2013, P VIRT REAL INT C LA, P19
   Dorin A, 2012, DIGIT CREAT, V23, P239, DOI 10.1080/14626268.2012.709940
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Efthymiou Athanasios, 2021, ARXIV PREPRINT ARXIV
   Elgammal A., 2017, ARXIV170607068
   Elgammal A, 2018, AAAI CONF ARTIF INTE, P2183
   Elgammal A, 2019, AM SCI, V107, P18
   Epstein Z, 2020, ISCIENCE, V23, DOI 10.1016/j.isci.2020.101515
   Eshraghian JK, 2020, NAT MACH INTELL, V2, P157, DOI 10.1038/s42256-020-0161-x
   Florea C, 2016, EUR SIGNAL PR CONF, P918, DOI 10.1109/EUSIPCO.2016.7760382
   Franceschet M, 2021, LEONARDO, V54, P402, DOI 10.1162/leon_a_02003
   Galanter Philip., 2003, P 2003 6 GENERATIVE
   Garcia Noa, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12536), P92, DOI 10.1007/978-3-030-66096-3_8
   Garcia N, 2019, LECT NOTES COMPUT SC, V11130, P676, DOI 10.1007/978-3-030-11012-3_52
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gillotte J.L., 2019, UC Davis L. Rev., V53, P2655
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goh G., 2021, DALLE CREATING IMAGE
   Gonthier N, 2019, LECT NOTES COMPUT SC, V11130, P692, DOI 10.1007/978-3-030-11012-3_53
   Gonthier Nicolas, 2020, PATTERN RECOGN, V12663, P546
   Gooch B., 2001, Non-photorealistic rendering
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graham DJ, 2012, WIRES COMPUT STAT, V4, P115, DOI 10.1002/wics.197
   Guadamuz Andres, 2017, Intellectual Property Quarterly
   Hayn-Leichsenring GU, 2017, I-PERCEPTION, V8, DOI 10.1177/2041669517715474
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Hertzmann A, 2020, COMMUN ACM, V63, P45, DOI 10.1145/3347092
   Hertzmann A, 2020, LEONARDO, V53, P424, DOI [10.1162/LEON_a_01930, 10.1145/3386567.3388574]
   Hertzmann A, 2018, ARTS, V7, DOI 10.3390/arts7020018
   Hong JW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3326337
   Hong JW, 2018, LECT NOTES COMPUT SC, V10902, P290, DOI 10.1007/978-3-319-91244-8_24
   Jacobsen CR, 2013, SIGNAL PROCESS, V93, P579, DOI 10.1016/j.sigpro.2012.09.019
   Jenicek Tomas, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1338, DOI 10.1109/ICDAR.2019.00216
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson CG, 2019, COMPLEXITY, DOI 10.1155/2019/3495962
   Karayev Sergey, 2014, P 2014 BRIT MACHINE
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Keren D, 2002, INT C PATT RECOG, P474, DOI 10.1109/ICPR.2002.1048341
   Khalili A, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23020153
   Khan FS, 2014, MACH VISION APPL, V25, P1385, DOI 10.1007/s00138-014-0621-6
   Kim D, 2014, SCI REP-UK, V4, DOI 10.1038/srep07370
   Kim Diana, 2019, P 10 INT C COMPUTATI, P33
   Kim DS, 2018, IEEE INT C SEMANT CO, P156, DOI 10.1109/ICSC.2018.00030
   Klinke Harald., 2020, The Routledge Companion to Digital Humanities and Art History, P32
   Lang S, 2019, LECT NOTES COMPUT SC, V11130, P647, DOI 10.1007/978-3-030-11012-3_49
   Lang S, 2018, DIGIT SCHOLARSH HUM, V33, P845, DOI 10.1093/llc/fqy006
   Lecoutre A., 2017, ASIAN C MACHINE LEAR, P327
   Lee B, 2020, P NATL ACAD SCI USA, V117, P26580, DOI 10.1073/pnas.2011927117
   Lemercier Joanie, 2021, GUIDE ECOFRIENDLY CR
   Lin Hubert, 2020, PATTERN RECOGN, V12663, P531
   Madhu Prathmesh, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12536), P109, DOI 10.1007/978-3-030-66096-3_9
   Madhu P, 2019, SUMAC'19: PROCEEDINGS OF THE 1ST WORKSHOP ON STRUCTURING AND UNDERSTANDING OF MULTIMEDIA HERITAGE CONTENTS, P15, DOI 10.1145/3347317.3357242
   Mao H, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1183, DOI 10.1145/3123266.3123405
   Mather G, 2018, ART PERCEPT, V6, P97, DOI 10.1163/22134913-20181092
   Mazzone M, 2019, ARTS, V8, DOI 10.3390/arts8010026
   McCormack Jon, 2019, Computational Intelligence in Music, Sound, Art and Design. 8th International Conference, EvoMUSART 2019. Held as Part of EvoStar 2019. Proceedings: Lecture Notes in Computer Science (LNCS 11453), P35, DOI 10.1007/978-3-030-16667-0_3
   McCormack J, 2014, LEONARDO, V47, P135, DOI 10.1162/LEON_a_00533
   Menis-Mastromichalakis O, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207645
   Mensink T, 2014, P INT C MULT RETR, P451, DOI DOI 10.1145/2578726.2578791
   Mermet Alexis, 2020, SUMAC'20: Proceedings of the 2nd Workshop on Structuring and Understanding of Multimedia heritAge Contents, P23, DOI 10.1145/3423323.3423412
   Milani Federico, 2020, ARXIV PREPRINT ARXIV
   Mohammad S., 2018, Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC 2018)
   Mordvintsev Alexander, 2015, GOOGLE RES BLOG, V2015, P3
   Mzoughi O, 2018, LECT NOTES COMPUT SC, V11182, P333, DOI 10.1007/978-3-030-01449-0_28
   Notaro A., 2020, P EV LOND 2020 EL VI, P322, DOI [10.14236/ewic/EVA2020.58, DOI 10.14236/EWIC/EVA2020.58]
   Pease Alison, 2020, 11 INT C COMPUTATION, P334
   Posthumus Etienne., 2020, BRILL ICONCLASS TEST
   Qi HC, 2011, INT CONF ACOUST SPEE, P2036
   Radford Alec, 2021, ARXIV ABS210300020
   Ragot Martin, 2020, CHI EA '20: Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, DOI 10.1145/3334480.3382892
   Rea N., 2018, HAS ARTIFICIAL INTEL
   Redies C, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00593
   Reed S, 2016, PR MACH LEARN RES, V48
   RISSET JC, 1982, LECT NOTES PHYS, V173, P281
   Sabatelli M, 2019, LECT NOTES COMPUT SC, V11130, P631, DOI 10.1007/978-3-030-11012-3_48
   Sandoval C, 2019, IEEE ACCESS, V7, P41770, DOI 10.1109/ACCESS.2019.2907986
   Sargentis GF, 2020, HERITAGE-BASEL, V3, P283, DOI 10.3390/heritage3020017
   Seguin Benoit, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P753, DOI 10.1007/978-3-319-46604-0_52
   Shamir L, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670672
   Shen X, 2019, PROC CVPR IEEE, P9270, DOI 10.1109/CVPR.2019.00950
   Sheng SR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2478, DOI 10.1145/3343031.3350972
   Sidorova E, 2019, ARTS, V8, DOI 10.3390/arts8030084
   Sigaki HYD, 2018, P NATL ACAD SCI USA, V115, pE8585, DOI 10.1073/pnas.1800083115
   Srinivasan R, 2021, PROCEEDINGS OF THE 2021 ACM CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, FACCT 2021, P41, DOI 10.1145/3442188.3445869
   Stefanini M., 2019, LECT NOTES COMPUT SC, P729, DOI [10.1007/978-3-030-30645-8_66, DOI 10.1007/978-3-030-30645-8_66]
   Stephensen Jan Lohmann, 2019, SCIENCEOPEN, P21, DOI DOI 10.14236/EWIC/POM19.4
   Strezoski G, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3273022
   Todorov Paul., 2019, ARXIV PREPRINT ARXIV
   van Noord N, 2017, PATTERN RECOGN, V61, P583, DOI 10.1016/j.patcog.2016.06.005
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q., 2021, ARXIV PREPRINT ARXIV
   Wechsler H, 2019, PATTERN RECOGN LETT, V126, P3, DOI 10.1016/j.patrec.2018.02.014
   Westlake Nicholas, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P825, DOI 10.1007/978-3-319-46604-0_57
   Wu YH, 2020, COMPUT HUM BEHAV, V104, DOI 10.1016/j.chb.2019.106186
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang H, 2020, VISUAL COMPUT, V36, P559, DOI 10.1007/s00371-019-01641-6
   Yanisky-Ravid Shlomit., 2018, Minn JL Sci Tech, V19, P1
   Yanulevskaya V, 2012, P 20 ACM INT C MULT, P349, DOI [10.1145/2393347.2393399, DOI 10.1145/2393347.2393399]
   Zhang JJ, 2021, IEEE ACCESS, V9, P77164, DOI 10.1109/ACCESS.2021.3083075
   Zhao L, 2020, COMPUT VIS IMAGE UND, V199, DOI 10.1016/j.cviu.2020.103024
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 141
TC 76
Z9 81
U1 104
U2 520
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 66
DI 10.1145/3475799
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Feng, SM
   Nong, XZ
   Hu, HF
AF Feng, Shenming
   Nong, Xingzhong
   Hu, Haifeng
TI Cascaded Structure-Learning Network with Using Adversarial Training for
   Robust Facial Landmark Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 2D Facial landmark detection; adversarial training; convolutional neural
   networks; keypoints structure learning
ID FACE ALIGNMENT; FEATURES
AB Recently, great progress has been achieved on facial landmark detection based on convolutional neural network, while it is still challenging due to partial occlusion and extreme head pose. In this paper, we propose a Cascaded Structure-Learning Network (CSLN) with using adversarial training to improve the performance of 2D facial landmark detection by taking the structure of facial landmarks into account. In the first stage, we improve the original stacked hourglass network, which applies a multi-branch module to capture different scales of features, a progressive convolution structure to compensate for the missing structural features in hourglass networks, and a pyramid inception structure to expand the receptive field. Specially, by introducing a discriminator, we use the adversarial training strategy to urge the improved hourglass network for generating more accurate heatmaps. The second stage, which is based on attention mechanism, optimizes the spatial correlations between different facial landmarks by reusing the structural features. Moreover, we propose a novel region loss, which can adaptively allocate proper weights to different regions. In this way, the network can focus more on those occluded landmarks. The experimental results on several datasets, i.e 300W, COFW, and AFLW, show that our proposed method achieves superior performance compared with the state-of-the-art methods.
C1 [Feng, Shenming; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
   [Nong, Xingzhong] Guangzhou Metro Design & Res Inst Co Ltd, Guangzhou 510010, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM fengshm3@mail2.sysu.edu.cn; nongxingzhong@dts.jy.com;
   huhaif@mail.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [62076262, 61673402,
   61273270, 60802069]; Natural Science Foundation of Guangdong Province
   [2017A030311029]; Science and Technology Program of Huizhou of China
   [2020SC0702002]
FX This work was supported in part by the National Natural Science
   Foundation of China (62076262, 61673402, 61273270, 60802069), the
   Natural Science Foundation of Guangdong Province (2017A030311029), the
   Science and Technology Program of Huizhou of China (2020SC0702002).
CR Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Berthelot David, 2017, CoRR
   Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Chen D, 2014, LECT NOTES COMPUT SC, V8694, P109, DOI 10.1007/978-3-319-10599-4_8
   Chen WL, 2018, IEEE IMAGE PROC, P196, DOI 10.1109/ICIP.2018.8451574
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chen Y, 2017, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2017.137
   Chou CJ, 2018, ASIAPAC SIGN INFO PR, P17, DOI 10.23919/APSIPA.2018.8659538
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Deng Jiankang, 2017, ABS170806023 CORR
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047
   Ghiasi G, 2014, PROC CVPR IEEE, P1899, DOI 10.1109/CVPR.2014.306
   Ghiasi Golnaz, 2015, ARXIV PREPRINT ARXIV
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Hassner T, 2015, PROC CVPR IEEE, P4295, DOI 10.1109/CVPR.2015.7299058
   Honari S, 2018, PROC CVPR IEEE, P1546, DOI 10.1109/CVPR.2018.00167
   Jain Arjun, 2013, ABS13127302 CORR
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Ke LP, 2018, LECT NOTES COMPUT SC, V11206, P731, DOI 10.1007/978-3-030-01216-8_44
   Kittler J, 2016, LECT NOTES COMPUT SC, V9756, P185, DOI 10.1007/978-3-319-41778-3_19
   Köstinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Kumar A, 2018, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2018.00052
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee HJ, 2020, IEEE T CIRC SYST VID, V30, P771, DOI 10.1109/TCSVT.2019.2897243
   Liu QS, 2017, IEEE T IMAGE PROCESS, V26, P797, DOI 10.1109/TIP.2016.2633939
   Liu YJ, 2017, IEEE INT CONF COMP V, P1619, DOI 10.1109/ICCVW.2017.190
   Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393
   Melekhov I, 2017, IEEE INT CONF COMP V, P870, DOI 10.1109/ICCVW.2017.107
   Miao X, 2018, PROC CVPR IEEE, P5040, DOI 10.1109/CVPR.2018.00529
   Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37
   Moniz J. R. A., 2018, P C NEUR INF PROC SY, P9736
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Ren Shaoqing, 2016, IEEE Trans Image Process, V25, P1233, DOI 10.1109/TIP.2016.2518867
   Roth J, 2017, IEEE T PATTERN ANAL, V39, P2127, DOI 10.1109/TPAMI.2016.2636829
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Saragih J, 2007, IEEE I CONF COMP VIS, P2173
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tang ZQ, 2018, LECT NOTES COMPUT SC, V11207, P348, DOI 10.1007/978-3-030-01219-9_21
   Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453
   Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79
   Valle R, 2018, LECT NOTES COMPUT SC, V11218, P609, DOI 10.1007/978-3-030-01264-9_36
   Wang W, 2018, IEEE T PATTERN ANAL, V40, P2569, DOI 10.1109/TPAMI.2018.2810881
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu WY, 2017, IEEE COMPUT SOC CONF, P2096, DOI 10.1109/CVPRW.2017.261
   Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227
   Wu Y, 2018, IEEE T PATTERN ANAL, V40, P3067, DOI 10.1109/TPAMI.2017.2787130
   Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yang JL, 2017, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2017.554
   Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253
   Zafeiriou S, 2017, IEEE COMPUT SOC CONF, P2116, DOI 10.1109/CVPRW.2017.263
   Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   ZhiaoHuang Erjin Zhou, 2015, ABS151104901 CORR
   Zhu ML, 2019, PROC CVPR IEEE, P3481, DOI 10.1109/CVPR.2019.00360
   Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zou X, 2019, IEEE I CONF COMP VIS, P141, DOI 10.1109/ICCV.2019.00023
NR 69
TC 0
Z9 0
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 49
DI 10.1145/3474595
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400006
DA 2024-07-18
ER

PT J
AU Yang, CW
   Phung, TH
   Shuai, HH
   Cheng, WH
AF Yang, Chun-Wei
   Thanh Hai Phung
   Shuai, Hong-Han
   Cheng, Wen-Huang
TI Mask or Non-Mask? Robust Face Mask Detector via Triplet-Consistency
   Representation Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face detection; object detection; deep learning; face occlusion
AB In the absence of vaccines or medicines to stop COVID-19, one of the effective methods to slow the spread of the coronavirus and reduce the overloading of healthcare is to wear a face mask. Nevertheless, to mandate the use of face masks or coverings in public areas, additional human resources are required, which is tedious and attention-intensive. To automate the monitoring process, one of the promising solutions is to leverage existing object detection models to detect the faces with or without masks. As such, security officers do not have to stare at the monitoring devices or crowds, and only have to deal with the alerts triggered by the detection of faces without masks. Existing object detection models usually focus on designing the CNN-based network architectures for extracting discriminative features. However, the size of training datasets of face mask detection is small, while the difference between faces with and without masks is subtle. Therefore, in this article, we propose a face mask detection framework that uses the context attention module to enable the effective attention of the feed-forward convolution neural network by adapting their attention maps' feature refinement. Moreover, we further propose an anchor-free detector with Triplet-Consistency Representation Learning by integrating the consistency loss and the triplet loss to deal with the small-scale training data and the similarity between masks and occlusions. Extensive experimental results show that our method outperforms the other state-of-the-art methods. The source code is released as a public download to improve public health at https://github.com/wei-1006/MaskFaceDetection.
C1 [Yang, Chun-Wei; Thanh Hai Phung; Shuai, Hong-Han; Cheng, Wen-Huang] Natl Yang Ming Chiao Tung Univ, 1001 Univ Rd, Hsinchu, Taiwan.
   [Cheng, Wen-Huang] Natl Chung Hsing Univ, 1001 Univ Rd, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University; National Chung Hsing
   University
RP Yang, CW (corresponding author), Natl Yang Ming Chiao Tung Univ, 1001 Univ Rd, Hsinchu, Taiwan.
EM wei1997.ee08g@nctu.edu.tw; haipt.eed08g@nctu.edu.tw;
   hhshuai@nctu.edu.tw; whcheng@nctu.edu.tw
FU Ministry of Science and Technology (MOST) of Taiwan
   [MOST-109-2223-E-009-002-MY3, MOST-110-2634-F-007-015,
   MOST-109-2218-E-002-015, MOST-109-2221-E-009-114-MY3,
   MOST-110-2218-E-A49-018, MOST-109-2327-B-010-005,
   MOST-109-2221-E-009-097, MOST-109-2221-E-001-015]; Higher Education
   Sprout Project of the National Yang Ming Chiao Tung University; Ministry
   of Education (MOE), Taiwan
FX This work is supported in part by the Ministry of Science and Technology
   (MOST) of Taiwan under the grants MOST-109-2223-E-009-002-MY3,
   MOST-110-2634-F-007-015, MOST-109-2218-E-002-015,
   MOST-109-2221-E-009-114-MY3, MOST-110-2218-E-A49-018,
   MOST-109-2327-B-010-005, MOST-109-2221-E-009-097 and
   MOST-109-2221-E-001-015, and the Higher Education Sprout Project of the
   National Yang Ming Chiao Tung University and Ministry of Education
   (MOE), Taiwan.
CR [Anonymous], 2014, ARXIV PREPRINT ARXIV
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bochkovskiy A., 2020, PREPRINT
   Borkar NR, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTING METHODOLOGIES AND COMMUNICATION (ICCMC), P249, DOI 10.1109/ICCMC.2017.8282685
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Chiang Daniell, 2021, AIZOO TECH FACE MASK
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Elsayed Gamaleldin, 2019, Advances in Neural Information Processing Systems, P702
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu C., 2019, NEURIPS, P2674
   Gathani Jenil, 2020, 2020 IEEE 15th International Conference on Industrial and Information Systems (ICIIS), P156, DOI 10.1109/ICIIS51140.2020.9342737
   Ge SM, 2017, PROC CVPR IEEE, P426, DOI 10.1109/CVPR.2017.53
   Gidaris S., 2018, P 6 INT C LEARNING R
   Girshick R. B., 2011, Advances in Neural Information Processing Systems, P442
   Girshick R.B., 2012, Discriminatively trained deformable part models, release 5
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Grill J.-B., 2020, P ADV NEUR INF PROC, V33, P21271
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jeong J, 2019, ADV NEUR IN, V32
   Jiale Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11482, DOI 10.1109/CVPR42600.2020.01150
   Jiang Mingjie, 2020, ARXIVCSCV200503950
   Jignesh Chowdary G., 2020, Big Data Analytics. 8th International Conference, BDA 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12581), P81, DOI 10.1007/978-3-030-66665-1_6
   Joshi Aniruddha Srinivas, 2020, 2020 12th International Conference on Computational Intelligence and Communication Networks (CICN), P435, DOI 10.1109/CICN49253.2020.9242625
   King DB, 2015, ACS SYM SER, V1214, P1
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Kumar A, 2019, ARTIF INTELL REV, V52, P927, DOI 10.1007/s10462-018-9650-2
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Lee Y, 2020, P IEEE CVF C COMP VI
   Leung NHL, 2020, NAT MED, V26, P676, DOI 10.1038/s41591-020-0843-2
   Lin SH, 2016, NEUROCOMPUTING, V218, P197, DOI 10.1016/j.neucom.2016.08.056
   Lin Stephen, 2020, NEURAL INFORM PROCES
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu CB, 2020, IEEE T MULTIMEDIA, V22, P1785, DOI 10.1109/TMM.2019.2954747
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Locatello F., 2020, Adv. Neural Inform. Process. Syst., V33, P11525
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mahmoud HAH, 2021, PERS UBIQUIT COMPUT, V25, P129, DOI 10.1007/s00779-020-01419-x
   Min SB, 2020, IEEE T IMAGE PROCESS, V29, P4996, DOI 10.1109/TIP.2020.2977457
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Neubeck Alexander, 2006, P IEEE INT C PATT RE
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J., 2018, COMPUTER VISION PATT
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Sun X, 2017, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2017.284
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456
   Vaswani A, 2017, ADV NEUR IN, V30
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang TC, 2020, PROC CVPR IEEE, P4115, DOI 10.1109/CVPR42600.2020.00417
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xu Guodong, 2020, P EUR C COMP VIS ECC
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Zhang JC, 2010, CHIN CONT DECIS CONF, P1735, DOI 10.1109/ICCSE.2010.5593578
   Zhang SJ, 2020, I S MOD ANAL SIM COM, P1, DOI 10.1109/mascots50786.2020.9285955
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
   Zou Z., 2019, ARXIV PREPRINT ARXIV
NR 85
TC 11
Z9 11
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 42
DI 10.1145/3472623
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Mawalim, CO
   Okada, S
   Nakano, YI
AF Mawalim, Candy Olivia
   Okada, Shogo
   Nakano, Yukiko, I
TI Task-independent Recognition of Communication Skills in Group
   Interaction Using Time-series Modeling
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodal analysis; time-series modeling; task independent;
   communication skills; group discussion
ID FACIAL EXPRESSION
AB Case studies of group discussions are considered an effective way to assess communication skills (CS). This method can help researchers evaluate participants' engagement with each other in a specific realistic context. In this article, multimodal analysis was performed to estimate CS indices using a three-task-type group discussion dataset, the MATRICS corpus. The current research investigated the effectiveness of engaging both static and time-series modeling, especially in task-independent settings. This investigation aimed to understand three main points: first, the effectiveness of time-series modeling compared to nonsequential modeling; second, multimodal analysis in a task-independent setting; and third, important differences to consider when dealing with task-dependent and task-independent settings, specifically in terms of modalities and prediction models. Several modalities were extracted (e.g., acoustics, speaking turns, linguistic-related movement, dialog tags, head motions, and face feature sets) for inferring the CS indices as a regression task. Three predictive models, including support vector regression (SVR), long short-term memory (LSTM), and an enhanced time-series model (an LSTM model with a combination of static and time-series features), were taken into account in this study. Our evaluation was conducted by using the R-2 score in a cross-validation scheme. The experimental results suggested that time-series modeling can improve the performance of multimodal analysis significantly in the task-dependent setting (with the best R-2 = 0.797 for the total CS index), with word2vec being the most prominent feature. Unfortunately, highly context-related features did not fit well with the task-independent setting. Thus, we propose an enhanced LSTM model for dealing with task-independent settings, and we successfully obtained better performance with the enhanced model than with the conventional SVR and LSTM models (the best R-2 = 0.602 for the total CS index). In other words, our study shows that a particular time-series modeling can outperform traditional nonsequential modeling for automatically estimating the CS indices of a participant in a group discussion with regard to task dependency.
C1 [Mawalim, Candy Olivia; Okada, Shogo] Japan Adv Inst Sci & Technol, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.
   [Nakano, Yukiko, I] Seikei Univ, Musashino, Tokyo, Japan.
C3 Japan Advanced Institute of Science & Technology (JAIST); Seikei
   University
RP Mawalim, CO (corresponding author), Japan Adv Inst Sci & Technol, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.
EM candyolivia@jaist.ac.jp; okada-s@jaist.ac.jp; y.nakano@st.seikei.ac.jp
OI Okada, Shogo/0000-0002-9260-0403; Mawalim, Candy
   Olivia/0000-0001-9853-8893
FU Japan Society for the Promotion of Science (JSPS) KAKENHI, Japan
   [19H01120, 19H01719, 20J20580]; JST AIP Trilateral AI Research Grant,
   Japan [JPMJCR20G6]; Grants-in-Aid for Scientific Research [20J20580,
   19H01719] Funding Source: KAKEN
FX This work was partially supported by the Japan Society for the Promotion
   of Science (JSPS) KAKENHI Grants No. 19H01120, No. 19H01719, and No.
   20J20580 and JST AIP Trilateral AI Research Grant No. JPMJCR20G6, Japan.
CR ADLER F, 1965, SOC FORCES, V44, P126, DOI 10.2307/2574840
   Alam Firoj, 2017, ARXIV170504839
   [Anonymous], 1997, NEURAL COMPUT
   Aran O, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P11, DOI 10.1145/2522848.2522859
   Bachman L. F., 1990, Fundamental considerations in language testing
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Bartlett MS, 2005, PROC CVPR IEEE, P568
   Beyan C, 2019, IEEE T MULTIMEDIA, V21, P2107, DOI 10.1109/TMM.2019.2895505
   Biel JI, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P53
   Bull Peter., 2002, COMMUNICATION MICROS, DOI [10.4324/9780203408025, DOI 10.4324/9780203408025]
   Core M. G., 1997, Proceedings of the AAAI Fall Symposium on Communicative Action in Humans and Machines, P28
   Drucker H, 1997, ADV NEUR IN, V9, P155
   Edwards R., 2011, International Journal of Listening, V25, P47, DOI DOI 10.1080/10904018.2011.536471
   Eyben Florian, 2010, P 18 ACM INT C MULT, P1459
   Greene Jennifer C., 2003, HDB COMMUNICATION SO, DOI [10.4324/9781410607133, DOI 10.4324/9781410607133]
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Hargie Owen., 2019, HDB COMMUNICATION SK
   Hymes D., 1972, COMMUNICATIVE COMPET
   Kudo T, 2004, P 2004 C EMP METH NA, P230
   Nguyen LS, 2014, IEEE T MULTIMEDIA, V16, P1018, DOI 10.1109/TMM.2014.2307169
   Lin YS, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P163, DOI 10.1145/3242969.3243001
   Lu Maggie., 2002, HARVARD BUSINESS SCH
   Mawalim CO, 2019, LECT NOTES COMPUT SC, V11578, P370, DOI 10.1007/978-3-030-21902-4_27
   Mehta Y, 2020, ARTIF INTELL REV, V53, P2313, DOI 10.1007/s10462-019-09770-z
   Mikolov T., 2013, INT C LEARN REPR SCO, DOI 10.48550/ARXIV.1301.3781
   Naim I, 2018, IEEE T AFFECT COMPUT, V9, P191, DOI 10.1109/TAFFC.2016.2614299
   Nihei F, 2014, Proceedings of the 16th International Conference on Multimodal Interaction, ICMI'14, page, P136
   Okada S, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P169, DOI 10.1145/2993148.2993154
   Okada Shogo, 2016, T JAPAN SOC ARTIFIC, V31, DOI [10.1527/tjsai.AI30-E, DOI 10.1527/TJSAI.AI30-E]
   Park S., 2014, International Conference on Multimodal Interaction, P50, DOI DOI 10.1145/2663204.2663260
   Le Q, 2014, PR MACH LEARN RES, V32, P1188
   Ramanarayanan V, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P23, DOI 10.1145/2818346.2820765
   Rasipuram S, 2018, MULTIMED TOOLS APPL, V77, P18709, DOI 10.1007/s11042-018-5654-9
   Sabanovic S, 2006, 9TH IEEE INTERNATIONAL WORKSHOP ON ADVANCED MOTION CONTROL, VOLS 1 AND 2, PROCEEDINGS, P756
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Sapru A, 2015, IEEE T MULTIMEDIA, V17, P746, DOI 10.1109/TMM.2015.2408437
   Scherer S., 2012, Proceedings of the 1st International Workshop on Multimodal Learning Analytics - MLA, P1, DOI [DOI 10.1145/2389268.2389269, 10.1145/2389268.2389269]
   Schuller B., 2014, COMPUT SPEECH LANG, V29
   Schuller B, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P254
   Shriberg E., 2004, The ICSI meeting recorder dialog act (MRDA) corpus: Technical Report
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Valente F, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P1182
   Wöllmer M, 2012, INT CONF ACOUST SPEE, P4157, DOI 10.1109/ICASSP.2012.6288834
   Wood E, 2015, IEEE I CONF COMP VIS, P3756, DOI 10.1109/ICCV.2015.428
   Zadeh A, 2017, IEEE COMPUT SOC CONF, P2051, DOI 10.1109/CVPRW.2017.256
NR 45
TC 0
Z9 0
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 122
DI 10.1145/3450283
PG 27
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800006
DA 2024-07-18
ER

PT J
AU Wu, JY
   Huang, Y
   Wu, Q
   Gao, ZP
   Zhao, JQ
   Huang, LQ
AF Wu, Junyi
   Huang, Yan
   Wu, Qiang
   Gao, Zhipeng
   Zhao, Jianqiang
   Huang, Liqin
TI Dual-Stream Guided-Learning via <i>a Priori</i> Optimization for Person
   Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; a priori optimization; guided-learning;
   foreground images
ID NETWORK
AB The task of person re-identification (re-ID) is to find the same pedestrian across non-overlapping camera views. Generally, the performance of person re-ID can be affected by background clutter. However, existing segmentation algorithms cannot obtain perfect foreground masks to cover the background information clearly. In addition, if the background is completely removed, some discriminative ID-related cues (i.e., back-pack or companion) may be lost. In this article, we design a dual-stream network consisting of a Provider Stream (P-Stream) and a Receiver Stream (R-Stream). The R-Stream performs an a priori optimization operation on foreground information. The P-Stream acts as a pusher to guide the R-Stream to concentrate on foreground information and some useful ID-related cues in the background. The proposed dual-stream network can make full use of the a priori optimization and guided-learning strategy to learn encouraging foreground information and some useful ID-related information in the background. Our method achieves Rank-1 accuracy of 95.4% on Market-1501, 89.0% on DukeMTMC-reID, 78.9% on CUHK03 (labeled), and 75.4% on CUHK03 (detected), outperforming state-of-the-art methods.
C1 [Wu, Junyi; Gao, Zhipeng; Zhao, Jianqiang] Xiamen Meiya Pico Informat Co Ltd, AI Res Ctr, Xiamen, Fujian, Peoples R China.
   [Huang, Yan; Wu, Qiang] Univ Technol Sydney, Sch Elect & Data Engn, Sydney, NSW, Australia.
   [Huang, Liqin] Fuzhou Univ, Coll Phys & Informat, Fuzhou, Fujian, Peoples R China.
C3 University of Technology Sydney; Fuzhou University
RP Huang, Y (corresponding author), Univ Technol Sydney, Sch Elect & Data Engn, Sydney, NSW, Australia.; Huang, LQ (corresponding author), Fuzhou Univ, Coll Phys & Informat, Fuzhou, Fujian, Peoples R China.
EM Junyi.Wu-1@outlook.com; yan.huang-3@student.uts.edu.au; hlq@fzu.edu.cn
OI Wu, Qiang/0000-0001-5641-2483; Wu, Junyi/0000-0002-2509-1223
FU Major Science and Technology Projects in Fujian, China [2018H0018,
   JAT160093]
FX This work was supported by Major Science and Technology Projects in
   Fujian, China (2018H0018, JAT160093). Authors' addresses: J. Wu, Z. Gao,
   and J. Zhao, AI Research Center, Xiamen Meiya Pico Information Co.,
   Ltd., Xiamen, Fujian Province, China;
   emails:Junyi.Wu-1@outlook.com;Y.Huang (corresponding author) and Q. Wu,
   School of Electrical and Data Engineering, University of Technology
   Sydney, NewSouthWales, Australia;
   emails:yan.huang-3@student.uts.edu.au;L.Huang (corresponding author),
   College of Physics and Information, Fuzhou University, Fuzhou, Fujian
   Province, China; email: hlq@fzu.edu.cn.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 2016, 19th International Conference on OFDM and Frequency Domain Techniques ICOF
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devito Z., 2017, NEURIPS WORKSHOPS, P1
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gheissari N., 2006, 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06), V2, P1528
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374
   Guo YQ, 2017, IEEE INT C INTELL TR
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hermans Alexander, 2017, ARXIV170307737
   Hinton G., 2015, COMPUT SCI, V2
   Huang Y, 2019, IEEE I CONF COMP VIS, P9526, DOI 10.1109/ICCV.2019.00962
   Huang Y, 2020, IEEE T CIRC SYST VID, V30, P3459, DOI 10.1109/TCSVT.2019.2948093
   Huang Y, 2019, IEEE T IMAGE PROCESS, V28, P1391, DOI 10.1109/TIP.2018.2874715
   Huang Y, 2017, NEUROCOMPUTING, V241, P191, DOI 10.1016/j.neucom.2017.02.055
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   King DB, 2015, ACS SYM SER, V1214, P1
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu Y., 2019, IEEE T IMAGE PROCESS, V30, P2060
   Qi L., 2018, ARXIV180403864
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Ren P., 2018, ARXIV181108073
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shen YT, 2018, PROC CVPR IEEE, P2265, DOI 10.1109/CVPR.2018.00241
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Nguyen TB, 2016, INT CONF KNOWL SYS, P339, DOI 10.1109/KSE.2016.7758077
   Tian MQ, 2018, PROC CVPR IEEE, P5794, DOI 10.1109/CVPR.2018.00607
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144
   Wei H, 2011, NAT COMMUN, V2, DOI 10.1038/ncomms1388
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yu R., 2017, BMVC
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2016, ARXIV
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou SP, 2019, IEEE T IMAGE PROCESS, V28, P4671, DOI 10.1109/TIP.2019.2908065
   Zhu K., 2020, ECCV, P346, DOI [10.1007/978-3-030-58580-8_21, DOI 10.1007/978-3-030-58580-8_21]
NR 66
TC 2
Z9 2
U1 2
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 117
DI 10.1145/3447715
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800001
DA 2024-07-18
ER

PT J
AU Li, HL
   Mao, XY
   Xu, MD
   Jin, XG
AF Li, Honglin
   Mao, Xiaoyang
   Xu, Mengdi
   Jin, Xiaogang
TI Deep-based Self-refined Face-top Coordination
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fashion analysis; personalized face-top coordination; deep cross-modal
   learning; canonical correlation analysis; relevance feedback;
   optimum-path forest
ID FACIAL ATTRACTIVENESS; BEAUTY
AB Face-top coordination, which exists in most clothes-fitting scenarios, is challenging due to varieties of attributes, implicit correlations, and tradeoffs between general preferences and individual preferences. We present a Deep-Based Self-Refined (DBSR) system to simulate face-top coordination based on intuition evaluation. To this end, we first establish a well-coordinated face-top (WCFT) dataset from fashion databases and communities. Then, we use a jointly trained CNN Deep Canonical Correlation Analysis (DCCA) method to bridge the semantic face-top gap based on the WCFT dataset to deal with general preferences. Subsequently, an irrelevance-based Optimum-path Forest (OPF) method is developed to adapt the results to individual preferences iteratively. Experimental results and user study demonstrate the effectiveness of our method.
C1 [Li, Honglin] Quanzhou Med Coll, Quanzhou 362010, Fujian, Peoples R China.
   [Mao, Xiaoyang] Univ Yamanashi, 400-8510U, Kofu, Yamanashi, Japan.
   [Xu, Mengdi; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
C3 University of Yamanashi; Zhejiang University
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM lihonglin79@qq.com; mao@yamanashi.ac.jp; 21821045@zju.edu.com;
   jin@cad.zju.edu.com
RI Mao, Xiaoyang/AAG-1294-2020; Xu, Meng/HJY-7139-2023
OI Mao, Xiaoyang/0000-0001-5010-6952; 
FU National Key R&D Program of China [2017YFB1002600]; Ningbo Major Special
   Projects of the "Science and Technology Innovation 2025" [2020Z007];
   National Natural Science Foundation of China [61972344, 61732015]
FX This work was supported by the National Key R&D Program of China (Grant
   No. 2017YFB1002600), the Ningbo Major Special Projects of the "Science
   and Technology Innovation 2025" (Grant No. 2020Z007), and the National
   Natural Science Foundation of China (Grant Nos. 61972344, 61732015).
CR Abe Kaori, 2017, ARXIV170307920
   Ajmani S, 2013, 2013 IEEE/WIC/ACM INTERNATIONAL JOINT CONFERENCE ON WEB INTELLIGENCE AND INTELLIGENT AGENT TECHNOLOGY - WORKSHOPS (WI-IAT), VOL 3, P17, DOI 10.1109/WI-IAT.2013.143
   Al-Halah Z, 2017, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2017.50
   Andrew G., 2013, P ICML, P1247
   [Anonymous], 2012, P 20 ACM INT C MULT
   Baudouin JY, 2004, ACTA PSYCHOL, V117, P313, DOI 10.1016/j.actpsy.2004.07.002
   Bottino A, 2010, LECT NOTES COMPUT SC, V6111, P425, DOI 10.1007/978-3-642-13772-3_43
   Chen W, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2662, DOI 10.1145/3292500.3330652
   Chen X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P765, DOI 10.1145/3331184.3331254
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   da Silva AT, 2011, PATTERN RECOGN, V44, P2971, DOI 10.1016/j.patcog.2011.04.026
   Eisenthal Y, 2006, NEURAL COMPUT, V18, P119, DOI 10.1162/089976606774841602
   He R., 2016, P 25 INT JOINT C ART, P3740
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He RN, 2016, IEEE DATA MINING, P937, DOI [10.1109/ICDM.2016.65, 10.1109/ICDM.2016.0116]
   He Tong, 2018, ARXIV PREPRINT ARXIV
   He Wanjia, 2017, 5 INT C LEARN REPR C
   Hidayati SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P438, DOI 10.1145/3240508.3240546
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hou M, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4681
   Hsiao WL, 2018, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR.2018.00748
   Hsiao WL, 2017, IEEE I CONF COMP VIS, P4213, DOI 10.1109/ICCV.2017.451
   Hsieh CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P275, DOI 10.1145/3343031.3351075
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Iwata Tomoharu, 2011, P 22 INT JOINT C ON, V3, P2262
   Jagadeesh V, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1925, DOI 10.1145/2623330.2623332
   Kang WC, 2019, PROC CVPR IEEE, P10524, DOI 10.1109/CVPR.2019.01078
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kornilov Dmitry, 2019, US Patent App, Patent No. [15/826,533, 15826533]
   Kumar Sudhir, 2019, ARXIV PREPRINT ARXIV
   Li HL, 2016, VISUAL COMPUT, V32, P1351, DOI 10.1007/s00371-016-1232-1
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liao LZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1571, DOI 10.1145/3240508.3240646
   Lin YJ, 2020, IEEE T KNOWL DATA EN, V32, P1502, DOI 10.1109/TKDE.2019.2906190
   Liu Luoqi, 2014, ACM T MULTIM COMPUT, V11, P1, DOI DOI 10.1145/2659234
   Lu Z, 2019, PROC CVPR IEEE, P10554, DOI 10.1109/CVPR.2019.01081
   Ma YH, 2017, AAAI CONF ARTIF INTE, P38
   Ma YS, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P257, DOI 10.1145/3343031.3350889
   Mall U, 2019, IEEE I CONF COMP VIS, P411, DOI 10.1109/ICCV.2019.00050
   Matzen Kevin, 2017, ARXIV170601869
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Mukkamala MC, 2017, PR MACH LEARN RES, V70
   Nguyen Tam V., 2012, PROC ACM INT C MULTI, P239
   Sattar H, 2019, IEEE WINT CONF APPL, P968, DOI 10.1109/WACV.2019.00108
   Shotaro Akaho, 2006, ARXIVCS0609071
   Simo-Serra E, 2015, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2015.7298688
   Simonyan K., 2014, 14091556 ARXIV
   Sun GL, 2018, MULTIMED TOOLS APPL, V77, P17731, DOI 10.1007/s11042-017-5245-1
   Takagi M, 2017, IEEE INT CONF COMP V, P2247, DOI 10.1109/ICCVW.2017.263
   Tangseng P, 2020, IEEE WINT CONF APPL, P2142, DOI 10.1109/WACV45572.2020.9093367
   Vaccaro K, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P777, DOI 10.1145/2984511.2984573
   Valenzano DR, 2006, VISION RES, V46, P1282, DOI 10.1016/j.visres.2005.10.024
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang S, 2015, INT CONF MACH LEARN, P883, DOI 10.1109/ICMLC.2015.7340670
   Wu ZH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P293, DOI 10.1145/3343031.3351083
   Xie DR, 2015, IEEE SYS MAN CYBERN, P1821, DOI 10.1109/SMC.2015.319
   Yamaguchi K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P773, DOI 10.1145/2647868.2654958
   Yang W, 2012, LECT NOTES COMPUT SC, V7131, P277
   Yang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P775, DOI 10.1145/3331184.3331242
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
   Yu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366153
   Yu WH, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P649, DOI 10.1145/3178876.3186146
   Yu Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3281746
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zhou ZZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1119, DOI 10.1145/3240508.3240596
   Zhuang Fuzheng, 2019, ARXIV PREPRINT ARXIV
NR 67
TC 0
Z9 0
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 95
DI 10.1145/3446970
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400019
DA 2024-07-18
ER

PT J
AU Nie, J
   Wei, ZQ
   Nie, WZ
   Liu, AA
AF Nie, Jie
   Wei, Zhi-Qiang
   Nie, Weizhi
   Liu, An-An
TI PGNet: Progressive Feature Guide Learning Network for Three-dimensional
   Shape Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D model retrieval; multi-modal; 3D shape recognition
ID NEURAL-NETWORK; 3D
AB Three-dimensional (3D) shape recognition is a popular topic and has potential application value in the field of computer vision. With the recent proliferation of deep learning, various deep learning models have achieved state-of-the-art performance. Among them, multiview-based 3D shape representation has received increased attention in recent years, and related approaches have shown significant improvement in 3D shape recognition. However, these methods focus on feature learning based on the design of the network and ignore the correlation among views. In this article, we propose a novel progressive feature guide learning network (PGNet) that focuses on the correlation among multiple views and integrates multiple modalities for 3D shape recognition. In particular, we propose two information fusion schemes from visual and feature aspects. The visual fusion scheme focuses on the view level and employs the soft-attention model to define the weights of views for visual information fusion. The feature fusion scheme focuses on the feature dimension information and employs the quantified feature as the mask to further optimize the feature. These two schemes jointly construct a PGNet for 3D shape representation. The classic ModelNet40 and ShapeNetCore55 datasets are applied to demonstrate the performance of our approach. The corresponding experiment also demonstrates the superiority of our approach.
C1 [Nie, Jie; Wei, Zhi-Qiang] Ocean Univ China, Coll Informat Sci & Engn, Qingdao, Peoples R China.
   [Nie, Weizhi; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
C3 Ocean University of China; Tianjin University
RP Nie, WZ; Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
EM niejie@ouc.edu.cn; weizhiqiang@ouc.edu.cn; weizhinie@tju.edu.cn;
   anan0422@gmail.com
RI Nie, Jie/ABG-9228-2021; wei, zhiqiang/M-8868-2013; Nie,
   Weizhi/ABF-5316-2021
OI Nie, Jie/0000-0003-4952-7666; nie, weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China [61872267, 61702471,
   61772359, 61572356]; Natural Science Foundation of Tianjin
   [19JCQNJC00500]
FX This work was supported in part by the National Natural Science
   Foundation of China (61872267, 61702471, 61772359, 61572356) and a grant
   of the 2019 Tianjin New Generation Artificial Intelligence Major
   Program. The Natural Science Foundation of Tianjin (19JCQNJC00500).
CR Allen M, 2008, 2008 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, PROCEEDINGS, P371, DOI 10.1109/IPSN.2008.45
   [Anonymous], 2018, ARXIV181111424
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bouksim Mohcine, 2018, INT J INTELL ENG SYS, V11, P98, DOI 10.22266/ijies2018.0131.01
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Grabner A, 2018, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2018.00319
   Han ZZ, 2019, IEEE T CYBERNETICS, V49, P481, DOI 10.1109/TCYB.2017.2778764
   Han ZZ, 2018, IEEE T IMAGE PROCESS, V27, P3049, DOI 10.1109/TIP.2018.2816821
   He XW, 2019, IEEE I CONF COMP VIS, P7514, DOI 10.1109/ICCV.2019.00761
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Jiang JW, 2019, AAAI CONF ARTIF INTE, P8513
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li YZ, 2018, ADV NEUR IN, V31
   Lim T., 2016, Proc. ICLR
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Malhotra P., 2015, ESANN, V89, P89
   Ng Andrew Y, 2012, P 26 ANN C NEUR PROC, P665, DOI DOI 10.1002/2014GB005021
   Nie WZ, 2019, IEEE T CIRC SYST VID, V29, P1619, DOI 10.1109/TCSVT.2018.2852310
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Savva M., 2016, P EUR WORKSH 3D OBJ, P89, DOI DOI 10.2312/3DOR.20161092
   Savva Manolis, 2017, P EUR WORKSH 3D OBJ, P5010
   Sfikas K., 2017, EXPLOITING PANORAMA, P1, DOI 10.2312/3dor.20171045
   Song R, 2019, COMPUT AIDED DESIGN, V115, P98, DOI 10.1016/j.cad.2019.05.006
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su YT, 2020, IEEE T CIRC SYST VID, V30, P3765, DOI 10.1109/TCSVT.2019.2942688
   Wang C., 2019, ARXIV190601592
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702
   Yu Q, 2020, NEURAL PROCESS LETT, V52, P581, DOI 10.1007/s11063-020-10268-x
   Yulan Guo, 2017, Chinese Conference on Image and Graphics Technologies, P199
   Zhang ZZ, 2018, IEEE T IMAGE PROCESS, V27, P5957, DOI 10.1109/TIP.2018.2862625
NR 42
TC 4
Z9 4
U1 5
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 87
DI 10.1145/3443708
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400011
DA 2024-07-18
ER

PT J
AU Hu, YT
   Liu, XH
   Zhang, BC
   Han, JG
   Cao, XB
AF Hu, Yutao
   Liu, Xuhui
   Zhang, Baochang
   Han, Jungong
   Cao, Xianbin
TI Alignment Enhancement Network for Fine-grained Visual Categorization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fine-grained visual categorization; image classification; feature
   aggregation
AB Fine-grained visual categorization (FGVC) aims to automatically recognize objects from different sub-ordinate categories. Despite attracting considerable attention from both academia and industry, it remains a challenging task due to subtle visual differences among different classes. Cross-layer feature aggregation and cross-image pairwise learning become prevailing in improving the performance of FGVC by extracting discriminative class-specific features. However, they are still inefficient to fully use the cross-layer information based on the simple aggregation strategy, while existing pairwise learning methods also fail to explore long-range interactions between different images. To address these problems, we propose a novel Alignment Enhancement Network (AENet), including two-level alignments, Cross-layer Alignment (CLA) and Cross-image Alignment (CIA). The CLA module exploits the cross-layer relationship between low-level spatial information and high-level semantic information, which contributes to cross-layer feature aggregation to improve the capacity of feature representation for input images. The new CIA module is further introduced to produce the aligned feature map, which can enhance the relevant information as well as suppress the irrelevant information across the whole spatial region. Our method is based on an underlying assumption that the aligned feature map should be closer to the inputs of CIA when they belong to the same category. Accordingly, we establish Semantic Affinity Loss to supervise the feature alignment within each CIA block. Experimental results on four challenging datasets show that the proposed AENet achieves the state-of-the-art results over prior arts.
C1 [Hu, Yutao; Liu, Xuhui; Cao, Xianbin] Beihang Univ, Sch Elect & Informat Engn, XueYuan Rd 37, Beijing, Peoples R China.
   [Zhang, Baochang] Beihang Univ, XueYuan Rd 37, Beijing, Peoples R China.
   [Han, Jungong] Aberystwyth Univ, Comp Sci Dept, Aberystwyth SY23 3FL, Dyfed, Wales.
   [Cao, Xianbin] Natl Engn Lab Big Data Applicat Technol Comprehen, XueYuan Rd 37, Beijing, Peoples R China.
C3 Beihang University; Beihang University; Aberystwyth University
RP Cao, XB (corresponding author), Beihang Univ, Sch Elect & Informat Engn, XueYuan Rd 37, Beijing, Peoples R China.; Cao, XB (corresponding author), Natl Engn Lab Big Data Applicat Technol Comprehen, XueYuan Rd 37, Beijing, Peoples R China.
EM huyutao@buaa.edu.cn; xuhui_cc@126.com; bczhang@buaa.edu.cn;
   jungonghan77@gmail.com; xbcao@buaa.edu.cn
RI Liu, Xu-Hui/HPB-9965-2023; Han, Jungong/ABE-6812-2020
FU National Natural Science Foundation of Chain (NSFC) [61827901, 62076016]
FX This article was supported by National Natural Science Foundation of
   Chain (NSFC) under Grants No. 61827901 and No. 62076016.
CR [Anonymous], 2017, ADV NEURAL INFORM PR
   Branson S., 2014, BIRD SPECIES CATEGOR, V1, P7
   Cai SJ, 2017, IEEE I CONF COMP VIS, P511, DOI 10.1109/ICCV.2017.63
   CAI Z, 2016, EUR C COMP VIS, P354, DOI DOI 10.1007/978-3-319-46493-0_22
   Chen KX, 2020, IEEE T NEUR NET LEAR, V31, P1747, DOI 10.1109/TNNLS.2019.2927224
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen ZN, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231742
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Ding GG, 2019, IEEE T IMAGE PROCESS, V28, P3752, DOI 10.1109/TIP.2019.2902115
   Ding ST, 2020, NEUROCOMPUTING, V398, P520, DOI 10.1016/j.neucom.2019.04.095
   Ding ST, 2019, FUTURE GENER COMP SY, V93, P583, DOI 10.1016/j.future.2018.10.054
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Dubey A, 2018, LECT NOTES COMPUT SC, V11216, P71, DOI 10.1007/978-3-030-01258-8_5
   Engin M, 2018, LECT NOTES COMPUT SC, V11206, P629, DOI 10.1007/978-3-030-01216-8_38
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Gao Z, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377876
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Gou MR, 2018, PROC CVPR IEEE, P3175, DOI 10.1109/CVPR.2018.00335
   Hanselmann H, 2020, IEEE WINT CONF APPL, P1236, DOI [10.1109/WACV45572.2020.9093601, 10.1109/wacv45572.2020.9093601]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Jingtao, 2019, P INT JOINT C NEURAL, P1
   Hu YT, 2021, IEEE T CIRC SYST VID, V31, P301, DOI 10.1109/TCSVT.2020.2978115
   Hu Yutao, 2020, NAS COUNT COUNTING B
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Huang YP, 2019, ADV NEUR IN, V32
   Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629
   Khosla A., 2011, P CVPR WORKSH FIN GR
   Korsch Dimitri, 2020, END TO END LEARNING
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li Hao, 2020, ATTRIBUTE MIX SEMANT
   Li JJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P663, DOI 10.1145/3240508.3240579
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Yizhao, 2020, MULTI BRANCH MULTISC
   Liuyu Xiang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P247, DOI 10.1007/978-3-030-58558-7_15
   Luan SZ, 2018, IEEE T IMAGE PROCESS, V27, P4357, DOI 10.1109/TIP.2018.2835143
   Luo W, 2019, IEEE I CONF COMP VIS, P8241, DOI 10.1109/ICCV.2019.00833
   Ma JQ, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107676
   Maji S., 2013, Technical report
   Meng L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P557, DOI 10.1145/3343031.3350870
   Min Shaobo, 2019, P ACM MULT, P1
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruyi Ji, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10465, DOI 10.1109/CVPR42600.2020.01048
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Tan M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3209666
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang DQ, 2015, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2015.276
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang N, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107448
   Wang QL, 2017, PROC CVPR IEEE, P6507, DOI 10.1109/CVPR.2017.689
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131
   Welinder P., 2010, Technical Report CNS-TR-2010-001
   Wu JF, 2019, PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON COMPUTER MODELING AND SIMULATION (ICCMS 2019) AND 8TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND APPLICATIONS (ICICA 2019), P133, DOI 10.1145/3307363.3307382
   Xiao LX, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511399
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zhang AR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3356019
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang LB, 2019, IEEE I CONF COMP VIS, P8330, DOI 10.1109/ICCV.2019.00842
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zheng HL, 2019, ADV NEUR IN, V32
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 75
TC 7
Z9 7
U1 1
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 12
DI 10.1145/3446208
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900012
DA 2024-07-18
ER

PT J
AU Liu, ZQ
   Zhu, GP
   Wang, YG
   Yang, JQ
   Kwong, S
AF Liu, Zuquan
   Zhu, Guopu
   Wang, Yuan-Gen
   Yang, Jianquan
   Kwong, Sam
TI A Novel (<i>t, s, k, n</i>) -Threshold Visual Secret Sharing Scheme
   Based on Access Structure Partition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Secret sharing; threshold visual secret sharing; essential participant;
   access structure partition
ID CRYPTOGRAPHY; CONTRAST
AB Visual secret sharing (VSS) is a new technique for sharing a binary image into multiple shadows. For VSS, the original image can he reconstructed from the shadows in any qualified set, but cannot be reconstructed from those in any forbidden set. In most traditional VSS schemes, the shadows held by participants have the same importance. However, in practice, a certain number of shadows are given a higher importance due to the privileges of their owners. In this article, a novel (t, s, k, n)-threshold VSS scheme is proposed based on access structure partition. First, we construct the basis matrix of the proposed (t, s, k, n)-threshold VSS scheme by utilizing a new access structure partition method and sub-access structure merging method. Then, the secret image is shared by the basis matrix as n shadows, which are divided into s essential shadows and n - s non-essential shadows. To reconstruct the secret image, k or more shadows should be collected, which include at least t essential shadows; otherwise, no information about the secret image can be obtained. Compared with related schemes, our scheme achieves a smaller shadow size and a higher visual quality of the reconstructed image. Theoretical analysis and experiments indicate the effectiveness of the proposed scheme.
C1 [Liu, Zuquan; Zhu, Guopu; Yang, Jianquan] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
   [Wang, Yuan-Gen] Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Guangzhou University; City University of Hong Kong
RP Zhu, GP (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
EM zq.liu2@siat.ac.cn; gp.zhu@siat.ac.cn; wangyg@gzhu.edu.cn;
   jq.yang@siat.ac.cn; cssamk@cityu.edu.hk
RI ouyang, jianquan/HTN-9999-2023; Kwong, Sam/C-9319-2012
OI ouyang, jianquan/0000-0002-7518-5156; Kwong, Sam/0000-0001-7484-7261;
   Zhu, Guopu/0000-0001-7956-5343
FU National Natural Science Foundation of China [61872350, 61872099,
   61802382, 61672443]; Hong Kong GRF-RGC General Research Fund [9042322
   (CityU 11200116), 9042489 (CityU 11206317), 9042816 (CityU 11209819)];
   Tip-top Scientific and Technical Innovative Youth Talents of Guangdong
   Special Support Program [2019TQ05X696]; Guangdong Basic and Applied
   Basic Research Foundation [2020A1515010640]; Science and Technology
   Program of Guangzhou [201904010478]; Basic Research Program of Shenzhen
   [JCYJ20170818163403748]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61872350, Grant 61872099, Grant
   61802382, and Grant 61672443, in part by Hong Kong GRF-RGC General
   Research Fund under Grant 9042322 (CityU 11200116), Grant 9042489 (CityU
   11206317), and Grant 9042816 (CityU 11209819), in part by the Tip-top
   Scientific and Technical Innovative Youth Talents of Guangdong Special
   Support Program under Grant 2019TQ05X696, in part by Guangdong Basic and
   Applied Basic Research Foundation under Grant 2020A1515010640, in part
   by the Science and Technology Program of Guangzhou under Grant
   201904010478, and in part by the Basic Research Program of Shenzhen
   under Grant JCYJ20170818163403748.
CR Ateniese G, 1996, INFORM COMPUT, V129, P86, DOI 10.1006/inco.1996.0076
   Ateniese G, 2001, THEOR COMPUT SCI, V250, P143, DOI 10.1016/S0304-3975(99)00127-9
   Blundo C, 1999, J CRYPTOL, V12, P261, DOI 10.1007/s001459900057
   Blundo C, 2000, INFORM PROCESS LETT, V75, P255, DOI 10.1016/S0020-0190(00)00108-3
   Blundo C, 2006, THEOR COMPUT SCI, V369, P169, DOI 10.1016/j.tcs.2006.08.008
   Chen CC, 2018, J VIS COMMUN IMAGE R, V52, P143, DOI 10.1016/j.jvcir.2018.02.006
   Chen CC, 2016, J VIS COMMUN IMAGE R, V38, P595, DOI 10.1016/j.jvcir.2016.04.004
   Cimato S, 2006, COMPUT J, V49, P97, DOI 10.1093/comjnl/bxh152
   Cimato S, 2005, DESIGN CODE CRYPTOGR, V35, P311, DOI 10.1007/s10623-003-6741-z
   Droste S., 1996, Advances in Cryptology - CRYPTO'96. 16th Annual International Cryptology Conference. Proceedings, P401
   Fu ZX, 2019, MULTIMED TOOLS APPL, V78, P2367, DOI 10.1007/s11042-018-6364-z
   Fu ZX, 2014, MULTIMED TOOLS APPL, V73, P1177, DOI 10.1007/s11042-013-1625-3
   Guo Teng, 2013, LECT NOTES COMPUTER, P56
   Han Y, 2017, CHIN CONTR CONF, P3652, DOI 10.23919/ChiCC.2017.8027926
   Hou YC, 2003, PATTERN RECOGN, V36, P1619, DOI 10.1016/S0031-3203(02)00258-3
   Hou YC, 2014, IEEE T CIRC SYST VID, V24, P733, DOI 10.1109/TCSVT.2013.2280097
   Hu H, 2019, MULTIMED TOOLS APPL, V78, P12055, DOI 10.1007/s11042-018-6738-2
   KAFRI O, 1987, OPT LETT, V12, P377, DOI 10.1364/OL.12.000377
   KURIHARA K, 2019, DESIGN CODE CRYPTOGR, V95, P21, DOI DOI 10.1016/J.JDERMSCI.2019.06.002
   Lee KH, 2012, IEEE T INF FOREN SEC, V7, P219, DOI 10.1109/TIFS.2011.2167611
   LI P, 2013, PATTERN RECOGN, V24, P1106, DOI DOI 10.1016/J.JVCIR.2013.07.005
   LI P, 2013, IEEE T CIRC SYST VID, V24, P1106, DOI DOI 10.1016/J.JVCIR.2013.07.005
   LI P, 2013, DESIGN CODE CRYPTOGR, V24, P1106, DOI DOI 10.1016/J.JVCIR.2013.07.005
   LI P, 2013, ACM T MULTIM COMPUT, V24, P1106, DOI DOI 10.1016/J.JVCIR.2013.07.005
   Li P, 2018, INT J DIGIT CRIME FO, V10, P78, DOI 10.4018/IJDCF.2018070107
   Li P, 2018, SIGNAL PROCESS-IMAGE, V65, P210, DOI 10.1016/j.image.2018.04.002
   Li P, 2016, DIGIT SIGNAL PROCESS, V50, P51, DOI 10.1016/j.dsp.2015.12.004
   Li P, 2013, J VIS COMMUN IMAGE R, V24, P1106, DOI 10.1016/j.jvcir.2013.07.005
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Wu X., 2013, P 1 ACM WORKSH INF H, P181, DOI [10.1177/1753193413497191, DOI 10.1145/2482513.2482515]
   Wu XT, 2013, IEEE T INF FOREN SEC, V8, P1541, DOI 10.1109/TIFS.2013.2274955
   Xu Y, 2004, CHINESE J ASTRON AST, V4, P481, DOI 10.1088/1009-9271/4/5/481
   Yan X, 2018, INT J POLYM SCI, V2018, DOI 10.1155/2018/6464051
   Yang CN, 2015, SIGNAL PROCESS-IMAGE, V31, P1, DOI 10.1016/j.image.2014.11.003
NR 34
TC 12
Z9 12
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 118
DI 10.1145/3418212
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800002
DA 2024-07-18
ER

PT J
AU Yang, X
   Qiao, Y
   Chen, SZ
   He, SF
   Yin, BC
   Zhang, Q
   Wei, XP
   Lau, RWH
AF Yang, Xin
   Qiao, Yu
   Chen, Shaozhe
   He, Shengfeng
   Yin, Baocai
   Zhang, Qiang
   Wei, Xiaopeng
   Lau, Rynson W. H.
TI Smart Scribbles for Image Matting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image matting; alpha matte; markov chain; deep learning; label
   propagation
ID SINGLE-IMAGE
AB Image matting is an ill-posed problem that usually requires additional user input, such as trimaps or scribbles. Drawing a fine trimap requires a large amount of user effort, while using scribbles can hardly obtain satisfactory alpha mattes for non-professional users. Some recent deep learning-based matting networks rely on large-scale composite datasets for training to improve performance, resulting in the occasional appearance of obvious artifacts when processing natural images. In this article, we explore the intrinsic relationship between user input and alpha mattes and strike a balance between user effort and the quality of alpha mattes. In particular, we propose an interactive framework, referred to as smart scribbles, to guide users to draw few scribbles on the input images to produce high-quality alpha mattes. It first infers the most informative regions of an image for drawing scribbles to indicate different categories (foreground, background, or unknown) and then spreads these scribbles (i.e., the category labels) to the rest of the image via our well-designed two-phase propagation. Both neighboring low-level affinities and high-level semantic features are considered during the propagation process. Our method can be optimized without large-scale matting datasets and exhibits more universality in real situations. Extensive experiments demonstrate that smart scribbles can produce more accurate alpha mattes with reduced additional input, compared to the state-of-the-art matting methods.
C1 [Yang, Xin; Qiao, Yu; Chen, Shaozhe; Yin, Baocai; Zhang, Qiang; Wei, Xiaopeng] Dalian Univ Technol, 2 Linggong Rd, Dalian 116024, Liaoning, Peoples R China.
   [Yang, Xin] Beijing Technol & Business Univ, Beijing, Peoples R China.
   [He, Shengfeng] South China Univ Technol, Guangzhou 510006, Guangdong, Peoples R China.
   [Yin, Baocai] Peng Cheng Lab, 2 Xingke First St, Shenzhen 518055, Guangdong, Peoples R China.
   [Lau, Rynson W. H.] City Univ Hong Kong, Kowloon Tong, 83 Tat Chee Rd, Hong Kong, Peoples R China.
C3 Dalian University of Technology; Beijing Technology & Business
   University; South China University of Technology; Peng Cheng Laboratory;
   City University of Hong Kong
RP Wei, XP (corresponding author), Dalian Univ Technol, 2 Linggong Rd, Dalian 116024, Liaoning, Peoples R China.; He, SF (corresponding author), South China Univ Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM xinyang@dlut.edu.cn; qiaoyu2017@mail.dlut.edu.cn; csz@mail.dlut.edu.cn;
   hesfe@scut.edu.cn; ybc@dlut.edu.cn; zhangq@dlut.edu.cn;
   weixp@dlut.edu.cn; rynson.lau@cityu.edu.hk
RI wei, xiao/ISB-6027-2023; Jiang, Tao/IWM-7503-2023; Zhang,
   Qiang/IWU-5000-2023; zhang, lin/IZQ-4870-2023; jiang, lei/IWE-1124-2023;
   Qiao, Yu/ABD-5787-2021; Zhang, Qiang/GXF-3105-2022; zhang,
   qiang/HZJ-9551-2023; He, Shengfeng/E-5682-2016
OI He, Shengfeng/0000-0002-3802-4644; Qiao, Yu/0000-0002-1889-2567; ,
   Xin/0000-0002-8046-722X; Zhang, Qiang/0000-0003-3776-9799
FU National Natural Science Foundation of China [91748104, 61972067,
   61632006, U1811463, U1908214, 61751203]; National Key Research and
   Development Program of China [2018AAA0102003, 2018YFC0910506]; Open
   Research Fund of Beijing Key Laboratory of Big Data Technology for Food
   Safety [BTBD-2018KF]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 91748104, Grant 61972067, Grant
   61632006, Grant U1811463, Grant U1908214, Grant 61751203, in part by the
   National Key Research and Development Program of China under Grant
   2018AAA0102003, Grant 2018YFC0910506, in part by the Open Research Fund
   of Beijing Key Laboratory of Big Data Technology for Food Safety
   (Project No. BTBD-2018KF).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32
   [Anonymous], 2019, P IEEECVF INT C COMP
   [Anonymous], 1997, TECHNOMETRICS, DOI DOI 10.1080/00401706.1997.10485132
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P618, DOI 10.1145/3240508.3240610
   Cho D, 2016, LECT NOTES COMPUT SC, V9906, P626, DOI 10.1007/978-3-319-46475-6_39
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Endo Y, 2016, COMPUT GRAPH FORUM, V35, P189, DOI 10.1111/cgf.12822
   Feng XX, 2016, LECT NOTES COMPUT SC, V9906, P204, DOI 10.1007/978-3-319-46475-6_13
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   Grady L, 2005, PROCEEDINGS OF THE FIFTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P423
   Guan Y, 2006, COMPUT GRAPH FORUM, V25, P567, DOI 10.1111/j.1467-8659.2006.00976.x
   Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423
   Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495
   Karacan L, 2015, IEEE I CONF COMP VIS, P424, DOI 10.1109/ICCV.2015.56
   Lee P., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2193, DOI 10.1109/CVPR.2011.5995665
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P1699, DOI 10.1109/TPAMI.2008.168
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li C, 2017, COMPUT VIS IMAGE UND, V162, P34, DOI 10.1016/j.cviu.2017.06.011
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336
   Lutz S., 2018, BMVC
   Qiao Y, 2020, P IEEE CVF C COMP VI, P13676
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shahrian E, 2013, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2013.88
   Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Sun JG, 2015, IEEE T IMAGE PROCESS, V24, P1639, DOI 10.1109/TIP.2015.2403241
   Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317
   Wang J, 2005, IEEE I CONF COMP VIS, P936
   Wang J., 2007, 2007 IEEE C COMP VIS, P1
   Wang YH, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P307, DOI 10.1145/2964284.2967232
   Xu K, 2018, VISUAL COMPUT, V34, P1065, DOI 10.1007/s00371-018-1554-2
   Xu N., 2017, PROC CVPR IEEE, P2970, DOI DOI 10.1109/CVPR.2017.41
   Yang X, 2018, ADV NEUR IN, V31
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zhang JS, 2021, J MED ENTOMOL, V58, P471, DOI 10.1093/jme/tjaa155
   Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765
   Zheng YJ, 2009, IEEE I CONF COMP VIS, P889, DOI 10.1109/ICCV.2009.5459326
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 44
TC 4
Z9 4
U1 1
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 121
DI 10.1145/3408323
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hu, XJ
   Shan, JM
   Liu, Y
   Zhang, L
   Shirmohammadi, S
AF Hu, Xinjue
   Shan, Jingming
   Liu, Yu
   Zhang, Lin
   Shirmohammadi, Shervin
TI An Adaptive Two-Layer Light Field Compression Scheme Using GNN-Based
   Reconstruction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Light field image; light field compression; light field reconstruction
AB As a new form of volumetric media, Light Field (LP) can provide users with a true six degrees of freedom immersive experience because LF captures the scene with photo-realism, including aperture-limited changes in viewpoint. But uncompressed LF data is too large for network transmission, which is the reason why LF compression has become an important research topic. One of the more recent approaches for LF compression is to reduce the angular resolution of the input LF during compression and to use LF reconstruction to recover the discarded viewpoints during decompression. Following this approach, we propose a new LF reconstruction algorithm based on Graph Neural Networks; we show that it can achieve higher compression and better quality compared to existing reconstruction methods, although suffering from the same problem as those methods-the inability to deal effectively with high-frequency image components. To solve this problem, we propose an adaptive two-layer compression architecture that separates high-frequency and low-frequency components and compresses each with a different strategy so that the performance can become robust and controllable. Experiments with multiple datasets' show that our proposed scheme is capable of providing a decompression quality of above 40 dB, and can significantly improve compression efficiency compared with similar LF reconstruction schemes.
C1 [Hu, Xinjue; Shan, Jingming; Liu, Yu; Zhang, Lin] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
   [Liu, Yu; Zhang, Lin] Peng Cheng Lab, Res Ctr Networks & Commun, Shenzhen, Peoples R China.
   [Shirmohammadi, Shervin] Univ Ottawa, 800 King Edward, Ottawa, ON K1N 6N5, Canada.
C3 Beijing University of Posts & Telecommunications; Peng Cheng Laboratory;
   University of Ottawa
RP Hu, XJ (corresponding author), Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
EM huxinjue@buptedu.cn; shanjm@buptedu.cn; liuy@bupt.edu.cn;
   zhanglin@bupt.edu.cn; shervin@discover.uottawa.ca
RI Hu, Xinjue/GOG-8517-2022; Zhang, Liqun/JDN-3523-2023; Shirmohammadi,
   Shervin/E-6945-2012; Zhang, Lin/HZH-4842-2023; Chen, Qi/GVU-3024-2022;
   zhang, lin/IZQ-4870-2023
OI Hu, Xinjue/0000-0001-8304-9720; Shirmohammadi,
   Shervin/0000-0002-3973-4445; Chen, Qi/0000-0002-6568-7267; 
FU BUPT Excellent Ph.D. Students Foundation [CX2019 102]; project "The
   Verification Platform of Multi-tier Coverage Communication Network for
   Oceans" [LZC0020]
FX The work is supported by the BUPT Excellent Ph.D. Students Foundation
   (CX2019 102). This work is also supported by the project "The
   Verification Platform of Multi-tier Coverage Communication Network for
   Oceans (LZC0020)".
CR Abu Basim NM, 2017, 2017 INTERNATIONAL CONFERENCE ON INNOVATIONS IN ELECTRICAL, ELECTRONICS, INSTRUMENTATION AND MEDIA TECHNOLOGY (ICIEEIMT), P1, DOI 10.1109/ICIEEIMT.2017.8116804
   Adelson Edward H., 1991, PLENOPTIC FUNCTION E, V2
   Aggoun A, 2011, J DISP TECHNOL, V7, P586, DOI 10.1109/JDT.2011.2159359
   Amirpour H, 2018, IEEE DATA COMPR CONF, P397, DOI 10.1109/DCC.2018.00050
   [Anonymous], 2016, INT C MACH LEARN
   Computer Graphics Laboratory Stanford University, 2008, LIGHT FIELD DAT
   Conti C, 2016, SIGNAL PROCESS-IMAGE, V42, P59, DOI 10.1016/j.image.2016.01.008
   Dansereau DG, 2013, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2013.137
   Gortler S.J., 1996, ACM T GRAPH, V23, P43
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hawary F, 2017, IEEE IMAGE PROC, P3250, DOI 10.1109/ICIP.2017.8296883
   Hu XJ, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P74, DOI 10.1145/3304109.3306228
   Jiang Xiaoran, 2017, IEEE J SELECTED TOPI, V99, P1
   Kauvar I, 2015, ACM T GRAPHIC, V34, DOI [10.1145/2682631, 10.1145/2816795.2818070]
   Kundu S, 2012, INT CONF ACOUST SPEE, P1349, DOI 10.1109/ICASSP.2012.6288140
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li L, 2017, IEEE J-STSP, V11, P1107, DOI 10.1109/JSTSP.2017.2725198
   Liu D., 2016, ARXIV PREPRINT ARXIV, P1, DOI DOI 10.1109/ICMEW.2016.7574674
   Lucas LER, 2014, EUR SIGNAL PR CONF, P11
   Monteiro R, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574670
   Ng R, 2005, RES REPORT, DOI DOI 10.1145/3097571
   Perra C, 2016, INT CONF IMAG PROC, DOI 10.1109/ICMEW.2016.7574671
   Perra C, 2017, IEEE ICCE, DOI 10.1109/ICCE.2017.7889217
   Rassool Reza, 2017, IEEE INT S BROADBAND
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Viola Irene, 2018, APPL DIGITAL IMAGE P, P12
   Wen F, 2018, ASIA COMMUN PHOTON
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Xu Shan., 2014, Asian Conference on Computer Vision, P3
   Yun Li, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P539, DOI 10.1109/ICASSP.2014.6853654
   Zhang Wei, 2018, VISUAL COMMUNICATION, P1
   Zhang XG, 2018, SHOCK VIB, V2018, DOI 10.1155/2018/4526970
NR 32
TC 16
Z9 16
U1 1
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 72
DI 10.1145/3395620
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FH
UT WOS:000583712600015
DA 2024-07-18
ER

PT J
AU Saini, N
   Saha, S
   Bhattacharyya, P
   Tuteja, H
AF Saini, Naveen
   Saha, Sriparna
   Bhattacharyya, Pushpak
   Tuteja, Himanshu
TI Textual Entailment-Based Figure Summarization for Biomedical Articles
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Figure-assisted text summarization; textual entailment; evolutionary
   computing; multi-objective optimization (MOO)
ID DIFFERENTIAL EVOLUTION; OPTIMIZATION; ALGORITHM
AB This article proposes a novel unsupervised approach (FigSum++) for automatic figure summarization in biomedical scientific articles using a multi-objective evolutionary algorithm. The problem is treated as an optimization problem where relevant sentences in the summary for a given figure are selected based on various sentence scoring features (or objective functions), such as the textual entailment score between sentences in the summary and a figure's caption, the number of sentences referring to that figure, semantic similarity between sentences and a figure's caption, and the number of overlapping words between sentences and a figure's caption. These objective functions are optimized simultaneously using multi-objective binary differential evolution (MBDE). MBDE consists of a set of solutions, and each solution represents a subset of sentences to be selected in the summary. MBDE generally uses a single differential evolution variant, but in the current study, an ensemble of two different differential evolution variants measuring diversity among solutions and convergence toward global optimal solution, respectively, is employed for efficient search. Usually, in any summarization system, diversity among sentences (called anti-redundancy) in the summary is a very critical feature, and it is calculated in terms of similarity (like cosine similarity) among sentences. In this article, a new way of measuring diversity in terms of textual entailment is proposed. To represent the sentences of the article in the form of numeric vectors, the recently proposed BioBERT pre-trained language model in biomedical text mining is utilized. An ablation study has also been presented to determine the importance of different objective functions. For evaluation of the proposed technique, two benchmark biomedical datasets containing 91 and 84 figures are considered. Our proposed system obtains 5% and 11% improvements in terms of the F-measure metric over two datasets, compared to the state-of-the-art unsupervised methods.
C1 [Saini, Naveen; Saha, Sriparna; Bhattacharyya, Pushpak] Indian Inst Technol Patna, Patna, Bihar, India.
   [Tuteja, Himanshu] Birla Inst Technol Mersa, Jaipur, Rajasthan, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Patna
RP Saini, N (corresponding author), Indian Inst Technol Patna, Patna, Bihar, India.
EM naveen.pcs16@iitp.ac.in; sriparna@iitp.ac.in; pb@iitp.ac.in;
   tuteja.himanshututs@gmail.com
RI SAINI, NAVEEN/AAP-6298-2021
OI SAINI, NAVEEN/0000-0002-2421-1457
FU Young Faculty Research Fellowship (YFRF) Award - Visvesvaraya Ph.D.
   scheme for Electronics and IT, Ministry of Electronics and Information
   Technology (MeitY), Government of India
FX This research was funded by the Young Faculty Research Fellowship (YFRF)
   Award, supported by the Visvesvaraya Ph.D. scheme for Electronics and
   IT, Ministry of Electronics and Information Technology (MeitY),
   Government of India, being implemented by Digital India Corporation
   (formerly Media Lab Asia).
CR Afantenos S. D., 2005, ARTIF INTELL, V2, P157
   Agarwal Shashank, 2009, Summit Transl Bioinform, V2009, P6
   Alguliev RM, 2013, EXPERT SYST APPL, V40, P1675, DOI 10.1016/j.eswa.2012.09.014
   Alguliev RM, 2012, KNOWL-BASED SYST, V36, P21, DOI 10.1016/j.knosys.2012.05.017
   Bandyopadhyay S, 2008, IEEE T EVOLUT COMPUT, V12, P269, DOI 10.1109/TEVC.2007.900837
   Bhatia S, 2012, ACM T INFORM SYST, V30, DOI 10.1145/2094072.2094075
   Das S, 2011, IEEE T EVOLUT COMPUT, V15, P4, DOI 10.1109/TEVC.2010.2059031
   Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017
   DunlaVy DM, 2007, INFORM PROCESS MANAG, V43, P1588, DOI 10.1016/j.ipm.2007.01.003
   Dutta S, 2018, IEEE INTELL SYST, V33, P4, DOI 10.1109/MIS.2018.033001411
   Ferreira R, 2013, EXPERT SYST APPL, V40, P5755, DOI 10.1016/j.eswa.2013.04.023
   Futrelle R.P., 2004, P WORKSH ANN M ASS C, P61
   Futrelle RP, 1999, ADVANCES IN AUTOMATIC TEXT SUMMARIZATION, P403
   Guglielmo EJ, 1996, ACM T INFORM SYST, V14, P237, DOI 10.1145/230538.230539
   He ZA, 2016, IEEE T EVOLUT COMPUT, V20, P386, DOI 10.1109/TEVC.2015.2472283
   Huang A., 2008, Proceedings of the sixth new zealand computer science research student conference (NZCSRSC2008), V4, P9
   Lee J., 2019, ARXIV190108746
   Lipo Wang, 2005, SUPPORT VECTOR MACHI, V177
   Mani Inderjeet, 2001, COMPUT LINGUIST, V28, P221
   Mohamed AW, 2013, EGYPT INFORM J, V14, P37, DOI 10.1016/j.eij.2013.01.001
   Nallapati R., 2017, AAAI, V31
   Narayan Shashi, 2017, ARXIV170404530
   Nomoto Tadashi., 2001, P 24 ANN INT ACM SIG, P26
   Oliveira H, 2017, PROC INT C TOOLS ART, P270, DOI 10.1109/ICTAI.2017.00051
   PASSONNEAU R., 1996, P INT C NAT LANG PRO, P204
   Ramesh BP, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0115671
   Ramos J, 2003, P 1 INSTRUCTIONAL C, V242, P29
   Rish I., 2001, IJCAI 2001 WORKSH EM, P41
   Romanov A, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1586
   Rush Alexander M., 2015, P 2015 C EMPIRICAL M, P379, DOI [10.48550/arXiv.1509.00685, DOI 10.18653/V1/D15-1044]
   Saha S, 2018, ACM T KNOWL DISCOV D, V12, DOI 10.1145/3182181
   Saini N, 2019, IEEE T COMPUT SOC SY, V6, P1219, DOI 10.1109/TCSS.2019.2945172
   Saini N, 2019, COGN COMPUT, V11, P271, DOI 10.1007/s12559-018-9611-8
   Saini N, 2019, APPL INTELL, V49, P1803, DOI 10.1007/s10489-018-1350-8
   Saini N, 2019, KNOWL-BASED SYST, V164, P45, DOI 10.1016/j.knosys.2018.10.021
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Sanchez-Gomez JM, 2018, KNOWL-BASED SYST, V159, P1, DOI 10.1016/j.knosys.2017.11.029
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Wan XJ, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2903
   Wang B, 2018, IEEE SENS J, V99, P1
   Wang LY, 2010, SYST CONTROL-FOUND A, P49, DOI 10.1007/978-0-8176-4956-2_4
   Wang Y, 2011, IEEE T EVOLUT COMPUT, V15, P55, DOI 10.1109/TEVC.2010.2087271
   Wu P., 2011, P WORKSHOP TEXT SUMM, P53
   Yeh JY, 2005, INFORM PROCESS MANAG, V41, P75, DOI 10.1016/j.ipm.2004.04.003
   Yu Hong, 2009, J Biomed Discov Collab, V4, P1, DOI 10.1186/1747-5333-4-1
   Zhang D, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (IEEE ICMA 2014), P239, DOI 10.1109/ICMA.2014.6885702
NR 46
TC 2
Z9 2
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 35
DI 10.1145/3357334
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300017
DA 2024-07-18
ER

PT J
AU Tanveer, M
   Richhariya, B
   Khan, RU
   Rashid, AH
   Khanna, P
   Prasad, M
   Lin, CT
AF Tanveer, M.
   Richhariya, B.
   Khan, R. U.
   Rashid, A. H.
   Khanna, P.
   Prasad, M.
   Lin, C. T.
TI Machine Learning Techniques for the Diagnosis of Alzheimer's Disease: A
   Review
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Review
DE Magnetic resonance imaging (MRI); positron emission tomography (PET);
   diffusion tensor imaging (DTI); mild cognitive impairment (MCI)
ID MILD COGNITIVE IMPAIRMENT; SUPPORT VECTOR MACHINE; INDEPENDENT COMPONENT
   ANALYSIS; ARTIFICIAL NEURAL-NETWORKS; COMPUTER-AIDED DIAGNOSIS;
   RESTING-STATE FMRI; FEATURE-SELECTION; BRAIN IMAGES; MULTIMODAL
   CLASSIFICATION; AUTOMATIC CLASSIFICATION
AB Alzheimer's disease is an incurable neurodegenerative disease primarily affecting the elderly population. Efficient automated techniques are needed for early diagnosis of Alzheimer's. Many novel approaches are proposed by researchers for classification of Alzheimer's disease. However, to develop more efficient learning techniques, better understanding of the work done on Alzheimer's is needed. Here, we provide a review on 165 papers from 2005 to 2019, using various feature extraction and machine learning techniques. The machine learning techniques are surveyed under three main categories: support vector machine (SVM), artificial neural network (ANN), and deep learning (DL) and ensemble methods. We present a detailed review on these three approaches for Alzheimer's with possible future directions.
C1 [Tanveer, M.; Richhariya, B.; Khan, R. U.; Rashid, A. H.] Indian Inst Technol Indore, Discipline Math, Indore 453552, India.
   [Rashid, A. H.] Natl Inst Sci & Technol, Sch Comp Sci & Engn, Berhampur 761008, Odisha, India.
   [Khanna, P.] PDPM Indian Inst Informat Technol Design & Mfg, Jabalpur 482005, India.
   [Prasad, M.; Lin, C. T.] Univ Technol Sydney, Ctr Artificial Intelligence, Sch Comp Sci, FEIT, Sydney, NSW, Australia.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Indore; National Institute of Science & Technology
   (NIST); Indian Institute of Information Technology Design &
   Manufacturing, Jabalpur; University of Technology Sydney
RP Tanveer, M (corresponding author), Indian Inst Technol Indore, Discipline Math, Indore 453552, India.
EM mtanveer@iiti.ac.in; phd1701241001@iiti.ac.in; riyaz@iiti.ac.in;
   ashrafrashid102@gmail.com; pkhanna@iiitdmj.ac.in;
   mukesh.prasad@uts.edu.au; chintenglin@gmail.com
RI Khanna, Pritee/V-5418-2019; Richhariya, Bharat/AAW-6994-2020; Tanveer,
   Mohammad/I-4585-2013; Lin, Chin-Teng (CT)/G-8129-2017
OI Khanna, Pritee/0000-0003-0518-2133; Richhariya,
   Bharat/0000-0001-6112-6799; Tanveer, Mohammad/0000-0002-5727-3697; Lin,
   Chin-Teng (CT)/0000-0001-8371-8197; Prasad, Mukesh/0000-0002-7745-9667
FU CSIR, New Delhi, India
FX We gratefully acknowledge the Indian Institute of Technology Indore for
   providing facilities and support. We are thankful to the Indian
   Institute of Technology Indore for providing Institute fellowship to Mr.
   Bharat Richhariya. We are also thankful to CSIR, New Delhi, India for
   providing research fellowship to Mr. Riyaj Uddin Khan.
CR Abdulkadir A, 2011, NEUROIMAGE, V58, P785, DOI 10.1016/j.neuroimage.2011.06.029
   Ahmadlou M, 2010, J NEURAL TRANSM, V117, P1099, DOI 10.1007/s00702-010-0450-3
   Al-Naami B., 2013, World Acad. Sci. Eng. Technol. Int. J. Biomed. Biol. Eng., V7, P5
   Alam S, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/8750506
   Alam S, 2017, INT J IMAG SYST TECH, V27, P133, DOI 10.1002/ima.22217
   Aljovic A, 2016, MEDD C EMBED COMPUT, P286, DOI 10.1109/MECO.2016.7525762
   [Anonymous], 2002, ARTIF INTELL MAG
   [Anonymous], 2008, P 14 ACM SIGKDD INT, DOI [10.1145, DOI 10.1145/1401890.1402012]
   Asgari Meysam, 2017, Alzheimers Dement (N Y), V3, P219, DOI 10.1016/j.trci.2017.01.006
   Basaia Silvia, 2018, NEUROIMAGE-CLIN
   Bateman RJ, 2011, ALZHEIMERS RES THER, V3, DOI 10.1186/alzrt59
   Beheshti I, 2017, J ALZHEIMERS DIS, V60, P295, DOI 10.3233/JAD-161080
   Beheshti I, 2017, COMPUT BIOL MED, V83, P109, DOI 10.1016/j.compbiomed.2017.02.011
   Bi XA, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00060
   Bi XA, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0194479
   Bisgin H, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24926-7
   Cabral C, 2015, COMPUT BIOL MED, V58, P101, DOI 10.1016/j.compbiomed.2015.01.003
   Casanova R, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0077949
   Chaplot S, 2006, BIOMED SIGNAL PROCES, V1, P86, DOI 10.1016/j.bspc.2006.05.002
   Chaves R, 2009, NEUROSCI LETT, V461, P293, DOI 10.1016/j.neulet.2009.06.052
   Chen G, 2011, RADIOLOGY, V259, P213, DOI 10.1148/radiol.10100734
   Chen YP, 2013, INT J PHOTOENERGY, V2013, DOI 10.1155/2013/510242
   Cheng B, 2019, BRAIN IMAGING BEHAV, V13, P138, DOI 10.1007/s11682-018-9846-8
   Cheng B, 2017, NEUROINFORMATICS, V15, P115, DOI 10.1007/s12021-016-9318-5
   Cheng B, 2015, BRAIN IMAGING BEHAV, V9, P913, DOI 10.1007/s11682-015-9356-x
   Chincarini A, 2011, NEUROIMAGE, V58, P469, DOI 10.1016/j.neuroimage.2011.05.083
   Cho Y, 2012, NEUROIMAGE, V59, P2217, DOI 10.1016/j.neuroimage.2011.09.085
   Chyzhyk D, 2014, NEUROCOMPUTING, V128, P73, DOI 10.1016/j.neucom.2013.01.065
   Clark David Glenn, 2016, Alzheimers Dement (Amst), V2, P113, DOI 10.1016/j.dadm.2016.02.001
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cui RX, 2018, I S BIOMED IMAGING, P1398, DOI 10.1109/ISBI.2018.8363833
   Cuingnet R, 2013, IEEE T PATTERN ANAL, V35, P682, DOI 10.1109/TPAMI.2012.142
   Cuingnet R, 2011, NEUROIMAGE, V56, P766, DOI 10.1016/j.neuroimage.2010.06.013
   Dai ZJ, 2012, NEUROIMAGE, V59, P2187, DOI 10.1016/j.neuroimage.2011.10.003
   Dale AM, 1999, NEUROIMAGE, V9, P179, DOI 10.1006/nimg.1998.0395
   Davatzikos C, 2008, NEUROIMAGE, V41, P1220, DOI 10.1016/j.neuroimage.2008.03.050
   Desikan RS, 2009, BRAIN, V132, P2048, DOI 10.1093/brain/awp123
   Doecke JD, 2012, ARCH NEUROL-CHICAGO, V69, P1318, DOI 10.1001/archneurol.2012.1282
   Dukart J, 2013, PSYCHIAT RES-NEUROIM, V212, P230, DOI 10.1016/j.pscychresns.2012.04.007
   Fan Y, 2008, NEUROIMAGE, V41, P277, DOI 10.1016/j.neuroimage.2008.02.043
   Fritsch J, 2019, INT CONF ACOUST SPEE, P5841, DOI 10.1109/ICASSP.2019.8682690
   Frozza RL, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00037
   Fujishima M, 2017, J ALZHEIMERS DIS, V56, P75, DOI 10.3233/JAD-160621
   Fung G, 2007, KNOWL INF SYST, V11, P243, DOI 10.1007/s10115-006-0043-5
   Gallego-Jutglà E, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/1/016018
   García-Floriano A, 2018, INFORM SOFTWARE TECH, V97, P99, DOI 10.1016/j.infsof.2018.01.003
   Gerardin E, 2009, NEUROIMAGE, V47, P1476, DOI 10.1016/j.neuroimage.2009.05.036
   Gorji HT, 2015, NEUROSCIENCE, V305, P361, DOI 10.1016/j.neuroscience.2015.08.013
   Gorriz J. M., 2008, P INT C NEUR INF PRO, P418
   Gorriz J. M., 2008, P INT C NEUR INF PRO, P410
   Gosztolya G, 2019, COMPUT SPEECH LANG, V53, P181, DOI 10.1016/j.csl.2018.07.007
   Gray KR, 2013, NEUROIMAGE, V65, P167, DOI 10.1016/j.neuroimage.2012.09.065
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Haller S, 2013, AM J NEURORADIOL, V34, P283, DOI 10.3174/ajnr.A3223
   Hernandez-Dominguez Laura, 2018, Alzheimers Dement (Amst), V10, P260, DOI 10.1016/j.dadm.2018.02.004
   Hett K, 2018, COMPUT MED IMAG GRAP, V70, P8, DOI 10.1016/j.compmedimag.2018.08.002
   Hidalgo-Muñoz AR, 2014, FRONT AGING NEUROSCI, V6, DOI 10.3389/fnagi.2014.00020
   Hinrichs C, 2009, NEUROIMAGE, V48, P138, DOI 10.1016/j.neuroimage.2009.05.056
   Hojjati SH, 2017, J NEUROSCI METH, V282, P69, DOI 10.1016/j.jneumeth.2017.03.006
   Hon M, 2017, IEEE INT C BIOINFORM, P1166, DOI 10.1109/BIBM.2017.8217822
   Hor S, 2016, MED IMAGE ANAL, V34, P30, DOI 10.1016/j.media.2016.07.012
   Horn JF, 2009, ARTIF INTELL MED, V47, P147, DOI 10.1016/j.artmed.2009.05.001
   Hosseini-Asl E., 2016, ARXIV160700556
   Huang CZ, 2008, INT CONF BIOMED, P250, DOI 10.1109/BMEI.2008.245
   Ieracitano C, 2019, NEUROCOMPUTING, V323, P96, DOI 10.1016/j.neucom.2018.09.071
   Illán IA, 2011, APPL SOFT COMPUT, V11, P2376, DOI 10.1016/j.asoc.2010.08.019
   Iwatsubo T, 2010, ALZHEIMERS DEMENT, V6, P297, DOI 10.1016/j.jalz.2010.03.011
   Jack CR, 2015, ALZHEIMERS DEMENT, V11, P740, DOI 10.1016/j.jalz.2015.05.002
   Jain Rachna, 2019, COGN SYST RES
   Jayadeva, 2007, IEEE T PATTERN ANAL, V29, P905, DOI 10.1109/TPAMI.2007.1068
   Jha D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/9060124
   Jiang QK, 2014, IEEE ENG MED BIO, P3366, DOI 10.1109/EMBC.2014.6944344
   Jie B, 2013, LECT NOTES COMPUT SC, V8149, P275, DOI 10.1007/978-3-642-40811-3_35
   Jie B, 2014, HUM BRAIN MAPP, V35, P2876, DOI 10.1002/hbm.22353
   Joshi Sandhya, 2010, Proceedings of the 2nd International Conference on Machine Learning and Computing (ICMLC 2010), P218, DOI 10.1109/ICMLC.2010.45
   Kamathe RS, 2018, BIOMED SIGNAL PROCES, V40, P41, DOI 10.1016/j.bspc.2017.09.005
   Kar S, 2019, J ALZHEIMERS DIS REP, V3, P1, DOI 10.3233/ADR-180082
   Kazemi K, 2014, J Biomed Phys Eng, V4, P13
   Khazaee A, 2016, BRAIN IMAGING BEHAV, V10, P799, DOI 10.1007/s11682-015-9448-7
   Khedher L, 2015, LECT NOTES COMPUT SC, V9107, P78, DOI 10.1007/978-3-319-18914-7_9
   Khedher L, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065716500507
   Kim J, 2018, HUM BRAIN MAPP, V39, P3728, DOI 10.1002/hbm.24207
   Klöppel S, 2008, BRAIN, V131, P681, DOI 10.1093/brain/awm319
   Korolev IO, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0138866
   Krishnakumar V., 2019, J NEUROSCI METHODS
   Kulkarni NN, 2017, IETE J RES, V63, P11, DOI 10.1080/03772063.2016.1241164
   Kumar MA, 2009, EXPERT SYST APPL, V36, P7535, DOI 10.1016/j.eswa.2008.09.066
   Lahmiri S, 2019, BIOMED SIGNAL PROCES, V52, P414, DOI 10.1016/j.bspc.2018.08.009
   Lahmiri S, 2014, HEALTHC TECHNOL LETT, V1, P32, DOI 10.1049/htl.2013.0022
   Lama RK, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/5485080
   Lebedev AV, 2014, NEUROIMAGE-CLIN, V6, P115, DOI 10.1016/j.nicl.2014.08.023
   Lee W, 2013, COMPUT BIOL MED, V43, P1313, DOI 10.1016/j.compbiomed.2013.07.004
   Li F, 2018, COMPUT MED IMAG GRAP, V70, P101, DOI 10.1016/j.compmedimag.2018.09.009
   Li H, 2014, CURR EYE RES, V9, P1, DOI DOI 10.1109/JSTARS.2014.2348173
   Li W, 2019, IEEE J BIOMED HEALTH, V23, P1234, DOI 10.1109/JBHI.2018.2839771
   Lillemark L, 2014, BMC MED IMAGING, V14, DOI 10.1186/1471-2342-14-21
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Liu F, 2014, NEUROIMAGE, V84, P466, DOI 10.1016/j.neuroimage.2013.09.015
   Liu J, 2018, IEEE ACM T COMPUT BI, V15, P624, DOI 10.1109/TCBB.2016.2635144
   Liu J, 2017, IEEE T NANOBIOSCI, V16, P428, DOI 10.1109/TNB.2017.2707139
   Liu MH, 2012, NEUROIMAGE, V60, P1106, DOI 10.1016/j.neuroimage.2012.01.055
   Liu MX, 2019, IEEE T BIO-MED ENG, V66, P1195, DOI 10.1109/TBME.2018.2869989
   Liu SQ, 2015, IEEE T BIO-MED ENG, V62, P1132, DOI 10.1109/TBME.2014.2372011
   Liu X, 2013, NEUROIMAGE, V83, P148, DOI 10.1016/j.neuroimage.2013.06.033
   Long XJ, 2010, PROC CVPR IEEE, P2910, DOI 10.1109/CVPR.2010.5540031
   Long Y, 2017, ACM TRANS SPAT ALGOR, V3, DOI 10.1145/3099471
   Lopes Helder Frederico da Silva, 2010, J MED SYST, V34, P1073
   López M, 2009, ELECTRON LETT, V45, P389, DOI 10.1049/el.2009.0176
   Lopez M., 2008, P INT C NEUR INF PRO, P402
   Lu DH, 2018, MED IMAGE ANAL, V46, P26, DOI 10.1016/j.media.2018.02.002
   Lu S, 2017, COMPUT MED IMAG GRAP, V60, P35, DOI 10.1016/j.compmedimag.2017.01.001
   Magnin B, 2009, NEURORADIOLOGY, V51, P73, DOI 10.1007/s00234-008-0463-x
   Mahanand BS, 2012, NEURAL NETWORKS, V32, P313, DOI 10.1016/j.neunet.2012.02.035
   Mahmood R, 2013, INT CONF SYST SIGNAL, P133, DOI 10.1109/IWSSIP.2013.6623471
   Mateos-Pérez JM, 2018, NEUROIMAGE-CLIN, V20, P506, DOI 10.1016/j.nicl.2018.08.019
   Mazaheri A, 2018, NEUROIMAGE-CLIN, V17, P188, DOI 10.1016/j.nicl.2017.10.009
   McEvoy LK, 2009, RADIOLOGY, V251, P195, DOI 10.1148/radiol.2511080924
   Mesrob L, 2008, LECT NOTES COMPUT SC, V5128, P124, DOI 10.1007/978-3-540-79982-5_14
   Min R, 2014, HUM BRAIN MAPP, V35, P5052, DOI 10.1002/hbm.22531
   Möller C, 2016, RADIOLOGY, V279, P838, DOI 10.1148/radiol.2015150220
   Moradi E, 2015, NEUROIMAGE, V104, P398, DOI 10.1016/j.neuroimage.2014.10.002
   O'Dwyer L, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0032441
   Ortiz A, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500258
   Ortiz A, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00132
   Ortiz A, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0093851
   Ortiz A, 2013, PATTERN RECOGN LETT, V34, P1725, DOI 10.1016/j.patrec.2013.04.014
   Padilla P, 2010, NEUROSCI LETT, V479, P192, DOI 10.1016/j.neulet.2010.05.047
   Paraskevaidi M, 2018, ACS CHEM NEUROSCI, V9, P2786, DOI 10.1021/acschemneuro.8b00198
   Patterson C, 2018, The state of the art of dementia research: New frontiers World Alzheimer Report 2018
   Payan A., 2015, ARXIV150202506, V2, P355
   Pellegrini Enrico, 2018, Alzheimers Dement (Amst), V10, P519, DOI 10.1016/j.dadm.2018.07.004
   Peng JL, 2019, PATTERN RECOGN, V88, P370, DOI 10.1016/j.patcog.2018.11.027
   Petrosian AA, 2001, CLIN NEUROPHYSIOL, V112, P1378, DOI 10.1016/S1388-2457(01)00579-X
   Plant C, 2010, NEUROIMAGE, V50, P162, DOI 10.1016/j.neuroimage.2009.11.046
   Plocharski M, 2016, COMPUT METH PROG BIO, V133, P35, DOI 10.1016/j.cmpb.2016.05.009
   Quintana M, 2012, J CLIN EXP NEUROPSYC, V34, P195, DOI 10.1080/13803395.2011.630651
   Ramírez J, 2010, NEUROSCI LETT, V472, P99, DOI 10.1016/j.neulet.2010.01.056
   Ramírez J, 2013, INFORM SCIENCES, V237, P59, DOI 10.1016/j.ins.2009.05.012
   Rao A, 2011, IEEE ENG MED BIO, P4499, DOI 10.1109/IEMBS.2011.6091115
   Rathore S, 2017, NEUROIMAGE, V155, P530, DOI 10.1016/j.neuroimage.2017.03.057
   Retico A, 2015, J NEUROIMAGING, V25, P552, DOI 10.1111/jon.12163
   Richhariya B, 2018, APPL SOFT COMPUT, V71, P418, DOI 10.1016/j.asoc.2018.07.003
   Richhariya B, 2018, EXPERT SYST APPL, V106, P169, DOI 10.1016/j.eswa.2018.03.053
   Rodrigues P, 2011, COMM COM INF SC, V221, P272
   Sankari Z, 2011, J NEUROSCI METH, V197, P165, DOI 10.1016/j.jneumeth.2011.01.027
   Sarraf S, 2016, DEEPAD ALZHEIMERS DI, DOI DOI 10.1101/070441
   Savio A, 2009, LECT NOTES COMPUT SC, V5788, P641, DOI 10.1007/978-3-642-04394-9_78
   Schmitter D, 2015, NEUROIMAGE-CLIN, V7, P7, DOI 10.1016/j.nicl.2014.11.001
   Schouten TM, 2016, NEUROIMAGE-CLIN, V11, P46, DOI 10.1016/j.nicl.2016.01.002
   Segovia F, 2013, EXPERT SYST APPL, V40, P677, DOI 10.1016/j.eswa.2012.07.071
   Segovia F, 2010, NEUROSCI LETT, V474, P58, DOI 10.1016/j.neulet.2010.03.010
   Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI [10.1146/annurev-bioeng-071516044442, 10.1146/annurev-bioeng-071516-044442]
   Sheng JH, 2019, BEHAV BRAIN RES, V365, P210, DOI 10.1016/j.bbr.2019.03.004
   Shi J, 2018, J Immunol Res, V2018, P1
   Shi Zhenghao, 2009, Int J Comput Sci, V3, P86
   Singh N, 2014, MED IMAGE ANAL, V18, P616, DOI 10.1016/j.media.2014.01.001
   Spasov S, 2019, NEUROIMAGE, V189, P276, DOI 10.1016/j.neuroimage.2019.01.031
   Spulber G, 2013, J INTERN MED, V273, P396, DOI 10.1111/joim.12028
   Stoeckel Jonathan, 2005, P 5 IEEE INT C DAT M
   Suk HI, 2017, MED IMAGE ANAL, V37, P101, DOI 10.1016/j.media.2017.01.008
   Suk HI, 2016, LECT NOTES COMPUT SC, V10019, P113, DOI 10.1007/978-3-319-47157-0_14
   Suk HI, 2016, NEUROIMAGE, V129, P292, DOI 10.1016/j.neuroimage.2016.01.005
   Suk HI, 2013, LECT NOTES COMPUT SC, V8150, P583, DOI 10.1007/978-3-642-40763-5_72
   Suk HI, 2015, BRAIN STRUCT FUNCT, V220, P841, DOI 10.1007/s00429-013-0687-3
   Suk HI, 2014, NEUROIMAGE, V101, P569, DOI 10.1016/j.neuroimage.2014.06.077
   Suk HI, 2014, FRONT AGING NEUROSCI, V6, DOI 10.3389/fnagi.2014.00168
   Sun Z, 2018, NEUROIMAGE, V178, P445, DOI 10.1016/j.neuroimage.2018.05.051
   Tangaro S, 2017, PHYS MEDICA, V38, P36, DOI 10.1016/j.ejmp.2017.04.027
   Tanveer M, 2016, APPL INTELL, V45, P174, DOI 10.1007/s10489-015-0751-1
   Termenon M, 2013, NEUROCOMPUTING, V114, P132, DOI 10.1016/j.neucom.2012.08.044
   Termenon M, 2012, NEURAL PROCESS LETT, V35, P1, DOI 10.1007/s11063-011-9200-2
   Tian BL, 2010, PROCEEDINGS OF THE 7TH CONFERENCE ON BIOLOGICAL DYNAMIC SYSTEM AND STABILITY OF DIFFERENTIAL EQUATION, VOLS I AND II, P699
   Tian Y., 2014, Annals of Data Science, V1, P253
   Tong T, 2017, PATTERN RECOGN, V63, P171, DOI 10.1016/j.patcog.2016.10.009
   van Veen Rick, 2018, P INT C APPL INT SYS
   Veitch DP, 2019, ALZHEIMERS DEMENT, V15, P106, DOI 10.1016/j.jalz.2018.08.005
   Vemuri P, 2008, NEUROIMAGE, V39, P1186, DOI 10.1016/j.neuroimage.2007.09.073
   Vemuri P, 2010, ALZHEIMERS RES THER, V2, DOI 10.1186/alzrt47
   Wang HF, 2019, NEUROCOMPUTING, V333, P145, DOI 10.1016/j.neucom.2018.12.018
   Wang K, 2007, HUM BRAIN MAPP, V28, P967, DOI 10.1002/hbm.20324
   Wang SH, 2015, INT J IMAG SYST TECH, V25, P153, DOI 10.1002/ima.22132
   Wang Tingyan, 2018, SCI REP, V8
   Weiner MW, 2017, ALZHEIMERS DEMENT, V13, pE1, DOI 10.1016/j.jalz.2016.11.007
   Westman E, 2011, NEUROIMAGE, V58, P818, DOI 10.1016/j.neuroimage.2011.06.065
   Xu YT, 2015, APPL INTELL, V42, P527, DOI 10.1007/s10489-014-0611-4
   Yang ST, 2013, COMPUT MATH METHOD M, V2013, DOI 10.1155/2013/253670
   Zeng NY, 2018, NEUROCOMPUTING, V320, P195, DOI 10.1016/j.neucom.2018.09.001
   Zhang DQ, 2011, I S BIOMED IMAGING, P1628, DOI 10.1109/ISBI.2011.5872715
   Zhang DQ, 2015, BIOMED RES INT, V2015, DOI 10.1155/2015/292683
   Zhang J, 2016, I S BIOMED IMAGING, P646, DOI 10.1109/ISBI.2016.7493350
   Zhang YT, 2018, CHINESE PHYS B, V27, DOI 10.1088/1674-1056/27/8/088702
   Zhang YD, 2015, PEERJ, V3, DOI 10.7717/peerj.1251
   Zhang YD, 2015, BIOMED SIGNAL PROCES, V21, P58, DOI 10.1016/j.bspc.2015.05.014
   Zhang YD, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00066
   Zheng WH, 2018, BIOL PSYCHIAT-COGN N, V3, P887, DOI 10.1016/j.bpsc.2018.06.004
   Zheng X, 2017, I S BIOMED IMAGING, P456, DOI 10.1109/ISBI.2017.7950559
   Zheng X, 2016, I S BIOMED IMAGING, P851, DOI 10.1109/ISBI.2016.7493399
   Zhou K, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8081372
   Zhu XF, 2016, IEEE T BIO-MED ENG, V63, P607, DOI 10.1109/TBME.2015.2466616
   Zhu Yingying, 2016, Med Image Comput Comput Assist Interv, V9900, P264, DOI 10.1007/978-3-319-46720-7_31
NR 200
TC 189
Z9 195
U1 32
U2 376
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 30
DI 10.1145/3344998
PG 35
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300012
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Punn, NS
   Agarwal, S
AF Punn, Narinder Singh
   Agarwal, Sonali
TI Inception U-Net Architecture for Semantic Segmentation to Identify
   Nuclei in Microscopy Cell Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Convolution neural networks; healthcare; medical image analysis;
   semantic segmentation
AB With the increasing applications of deep learning in biomedical image analysis, in this article we introduce an inception U-Net architecture for automating nuclei detection in microscopy cell images of varying size and modality to help unlock faster cures, inspired from Kaggle Data Science Bowl Challenge 2018 (KDSB18). This study follows from the fact that most of the analysis requires nuclei detection as the starting phase for getting an insight into the underlying biological process and further diagnosis. The proposed architecture consists of a switch normalization layer, convolution layers, and inception layers (concatenated 1x1, 3x3, and 5x5 convolution and the hybrid of a max and Hartley spectral pooling layer) connected in the U-Net fashion for generating the image masks. This article also illustrates the model perception of image masks using activation maximization and filter map visualization techniques. A novel objective function segmentation loss is proposed based on the binary cross entropy, dice coefficient, and intersection over union loss functions. The intersection over union score, loss value, and pixel accuracy metrics evaluate the model over the KDSB18 dataset. The proposed inception U-Net architecture exhibits quite significant results as compared to the original U-Net and recent U-Net++ architecture.
C1 [Punn, Narinder Singh; Agarwal, Sonali] Indian Inst Informat Technol Allahabad, Prayagraj, Uttar Pradesh, India.
C3 Indian Institute of Information Technology Allahabad
RP Punn, NS (corresponding author), Indian Inst Informat Technol Allahabad, Prayagraj, Uttar Pradesh, India.
EM pse2017002@iiita.ac.in; sonali@iiit.ac.in
RI Punn, Narinder Singh/ABA-3787-2020; Agarwal, Sonali/AAT-1740-2020
OI Punn, Narinder Singh/0000-0003-1175-1865; Agarwal,
   Sonali/0000-0001-9083-5033
CR Akhtar N, 2020, NEURAL COMPUT APPL, V32, P879, DOI 10.1007/s00521-019-04296-5
   [Anonymous], 2018, ARXIV181206499
   [Anonymous], 2018, ARXIV181004028
   [Anonymous], 2014, WORKSHOP INT C LEARN
   Anwar SM, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-1088-1
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bhardwaj R, 2017, P INT COMP SOFTW APP, P236, DOI 10.1109/COMPSAC.2017.164
   Dolz J, 2019, IEEE T MED IMAGING, V38, P1116, DOI 10.1109/TMI.2018.2878669
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   GitHub, 2018, UNETPLUSPLUS
   Gu Z, 2019, FRONT IMMUNOL, V10, DOI 10.3389/fimmu.2019.02288
   Havaei M, 2017, MED IMAGE ANAL, V35, P18, DOI 10.1016/j.media.2016.05.004
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jiang Hongda, 2018, BIORXIV, DOI 10.1101/382549v1.
   Kaggle, 2018, KDSB ANOTHER IOU MET
   Kaggle, 2018, KAGGLE DATA SCI BOWL
   Kaggle, 2018, KDSB CHALLENGE RANK
   Kaggle, 2018, KDSB CHALLENGERANK 4
   Kamnitsas K, 2017, MED IMAGE ANAL, V36, P61, DOI 10.1016/j.media.2016.10.004
   Ker J, 2018, IEEE ACCESS, V6, P9375, DOI 10.1109/ACCESS.2017.2788044
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Lin T., 2017, P IEEE INT C COMP VI
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Luo P., 2018, ARXIV
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI [10.1109/TPAMI.2015.2481406, 10.1109/TPAMI.2016.2537320]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruder S., 2016, ARXIV
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shickel Benjamin, 2018, IEEE J Biomed Health Inform, V22, P1589, DOI 10.1109/JBHI.2017.2767063
   Solovei I, 2013, CELL, V152, P584, DOI 10.1016/j.cell.2013.01.009
   Suzuki K, 2017, RADIOL PHYS TECHNOL, V10, P257, DOI 10.1007/s12194-017-0406-5
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Ulyanov Dmitry, 2016, arXiv
   Weiskopf NG, 2013, J AM MED INFORM ASSN, V20, P144, DOI 10.1136/amiajnl-2011-000681
   Zeng ZT, 2019, IEEE ACCESS, V7, P21420, DOI 10.1109/ACCESS.2019.2896920
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zwerger M, 2011, ANNU REV BIOMED ENG, V13, P397, DOI 10.1146/annurev-bioeng-071910-124736
NR 39
TC 65
Z9 66
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 12
DI 10.1145/3376922
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100011
DA 2024-07-18
ER

PT J
AU Fang, ZW
   Liu, J
   Liu, XL
   Tang, Q
   Li, Y
   Lu, HQ
AF Fang, Zhiwei
   Liu, Jing
   Liu, Xueliang
   Tang, Qu
   Li, Yong
   Lu, Hanqing
TI BTDP: Toward Sparse Fusion with Block Term Decomposition Pooling for
   Visual Question Answering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Sparse bilinear pooling; block term decomposition pooling; visual
   question answering
ID HIGHER-ORDER TENSOR
AB Bilinear models are very powerful in multimodal fusion tasks like Visual Question Answering. The predominant bilinear methods can all be seen as a kind of tensor-based decomposition operation that contains a key kernel called "core tensor." Current approaches usually focus on reducing the computation complexity by applying low-rank constraint on the core tensor. In this article, we propose a novel bilinear architecture called Block Term Decomposition Pooling (BTDP), which not only maintains the advantages of previous bilinear methods but also conducts sparse bilinear interactions between modalities. Our method is based on Block Term Decompositions theory of tensor, which will result in a sparse and learnable block-diagonal core tensor for multimodal fusion. We prove that using such a block-diagonal core tensor is equivalent to conducting many "tiny" bilinear operations in different feature spaces. Thus, introducing sparsity into the bilinear operation can significantly increase the performance of feature fusion and improve VQA models. What is more, our BTDP is very flexible in design. We develop several variants of BTDP and discuss the effects of the diagonal blocks of core tensor. Extensive experiments on two challenging VQA-v1 and VQA-v2 datasets show that our BTDP method outperforms current bilinear models, achieving state-of-the-art performance.
C1 [Fang, Zhiwei; Liu, Jing; Lu, Hanqing] Chinese Acad Sci, Inst Automat, 95 East Zhongguancun Rd, Beijing, Peoples R China.
   [Fang, Zhiwei; Liu, Jing; Lu, Hanqing] Univ Chinese Acad Sci, 95 East Zhongguancun Rd, Beijing, Peoples R China.
   [Liu, Xueliang] Hefei Univ Technol, 193 Tuixi Rd, Hefei 230009, Anhui, Peoples R China.
   [Tang, Qu] Southeast Univ Monash Univ Joint Grad Sch, Suzhou, Peoples R China.
   [Li, Yong] JD Com, Business Growth BU, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Hefei University of Technology; Southeast University - China
RP Fang, ZW; Liu, J (corresponding author), Chinese Acad Sci, Inst Automat, 95 East Zhongguancun Rd, Beijing, Peoples R China.; Fang, ZW; Liu, J (corresponding author), Univ Chinese Acad Sci, 95 East Zhongguancun Rd, Beijing, Peoples R China.
EM zhiwei.fang@nlpr.ia.ac.cn; jliu@nlpr.ia.ac.cn; eliang@hfut.edu.cn;
   tangquu@gmail.com; liyong5@jd.com; luhq@nlpr.ia.ac.cn
FU Beijing Natural Science Foundation [4192059]; National Natural Science
   Foundation of China [61872366, 61472422]
FX This work was supported by Beijing Natural Science Foundation (4192059)
   and National Natural Science Foundation of China (61872366 and
   61472422).
CR Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   [Anonymous], 2018, P INT JOINT C ART IN
   [Anonymous], 2017, IEEE INT C COMP VIS
   [Anonymous], P INT C LEARN REPR I
   [Anonymous], 2017, NIPS 17
   [Anonymous], 2016, P C EMP METH NAT LAN
   [Anonymous], PROC CVPR IEEE
   [Anonymous], Simple baseline for visual question answering
   [Anonymous], 2014, WORKSHOP INT C LEARN
   [Anonymous], 2017, INT JOINT C ART INT
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2014, EMNLP
   [Anonymous], 2016, ARXIV160603647
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 1995, CONVOLUTIONAL NETWOR
   [Anonymous], 2016, NAACL
   [Anonymous], IEEE T NEURAL NETW L
   [Anonymous], 2018, P IEEE C COMP VIS PA
   [Anonymous], 2014, ICLR, DOI DOI 10.1145/1830483.1830503
   [Anonymous], 2016, ARXIV160401485
   [Anonymous], 2017, CVPR
   [Anonymous], 2018, IEEE T PATTERN ANAL
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   De Lathauwer L, 2008, SIAM J MATRIX ANAL A, V30, P1067, DOI 10.1137/070690730
   De Lathauwer L, 2008, SIAM J MATRIX ANAL A, V30, P1033, DOI 10.1137/070690729
   De Lathauwer L, 2008, SIAM J MATRIX ANAL A, V30, P1022, DOI 10.1137/060661685
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong J, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2501643.2501646
   Gao H., 2015, INT C NEURAL INF PRO, P2296
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Harshman Richard A., 1970, FDN PARAFAC PROCEDUR
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P5814, DOI 10.1109/TIP.2016.2614132
   Hong RC, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2043612.2043613
   Hong RC, 2012, IEEE MULTIMEDIA, V19, P72, DOI 10.1109/MMUL.2011.53
   Houle ME, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2487736
   Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44
   Karpathy A, 2014, ADV NEUR IN, V27
   Kiros Ryan., 2015, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, P3294
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li ZC, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2624140
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu ZG, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3187011
   Malinowski M, 2014, ADV NEUR IN, V27
   Mei T, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071402
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591
   Pang L, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818711
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wang P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1290
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu LC, 2015, IEEE I CONF COMP VIS, P2461, DOI 10.1109/ICCV.2015.283
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang P, 2016, PROC CVPR IEEE, P5014, DOI 10.1109/CVPR.2016.542
   Zhao Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3683
   Zhao Z, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1050, DOI 10.1145/3123266.3123364
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 72
TC 6
Z9 6
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 50
DI 10.1145/3282469
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900003
DA 2024-07-18
ER

PT J
AU Pala, P
   Berretti, S
AF Pala, Pietro
   Berretti, Stefano
TI Reconstructing 3D Face Models by Incremental Aggregation and Refinement
   of Depth Frames
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Depth data; 3D reconstruction; 3D face recognition; anisotropic error
ID IMAGE SUPERRESOLUTION; RECOGNITION; NETWORK
AB Face recognition from two-dimensional (2D) still images and videos is quite successful even with "in the wild" conditions. Instead, less consolidated results are available for the cases in which face data come from non-conventional cameras, such as infrared or depth. In this article, we investigate this latter scenario assuming that a low-resolution depth camera is used to perform face recognition in an uncooperative context. To this end, we propose, first, to automatically select a set of frames from the depth sequence of the camera because they provide a good view of the face in terms of pose and distance. Then, we design a progressive refinement approach to reconstruct a higher-resolution model from the selected low-resolution frames. This process accounts for the anisotropic error of the existing points in the current 3D model and the points in a newly acquired frame so that the refinement step can progressively adjust the point positions in the model using a Kalman-like estimation. The quality of the reconstructed model is evaluated by considering the error between the reconstructed models and their corresponding high-resolution scans used as ground truth. In addition, we performed face recognition using the reconstructed models as probes against a gallery of reconstructed models and a gallery with high-resolution scans. The obtained results confirm the possibility to effectively use the reconstructed models for the face recognition task.
C1 [Pala, Pietro; Berretti, Stefano] Univ Florence, Via Santa Marta 3, I-50139 Florence, Italy.
C3 University of Florence
RP Pala, P (corresponding author), Univ Florence, Via Santa Marta 3, I-50139 Florence, Italy.
EM pietro.pala@unifi.it; stefano.berretti@unifi.it
RI Berretti, Stefano/U-9004-2019
OI Berretti, Stefano/0000-0003-1219-4386
CR Al Ismaeil Kassem, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P8, DOI 10.1109/CVPRW.2015.7301389
   Al Ismaeil K, 2017, IEEE T PATTERN ANAL, V39, P2045, DOI 10.1109/TPAMI.2016.2622698
   Al Ismaeil K, 2016, COMPUT VIS IMAGE UND, V147, P38, DOI 10.1016/j.cviu.2016.04.006
   [Anonymous], 2018, ACM T MULTIM COMPUT, DOI DOI 10.1145/3182179
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], 1989, Shape from shading
   Berretti S, 2014, IEEE T INF FOREN SEC, V9, P1436, DOI 10.1109/TIFS.2014.2337258
   Berretti S, 2013, COMPUT GRAPH-UK, V37, P509, DOI 10.1016/j.cag.2013.04.001
   Berretti S, 2013, IEEE T INF FOREN SEC, V8, P374, DOI 10.1109/TIFS.2012.2235833
   Berretti S, 2010, IEEE T PATTERN ANAL, V32, P2162, DOI 10.1109/TPAMI.2010.43
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bondi E, 2016, IEEE T INF FOREN SEC, V11, P2843, DOI 10.1109/TIFS.2016.2601059
   Cao QX, 2017, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2017.180
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Dimitrievski M, 2017, PROC SPIE, V10410, DOI 10.1117/12.2273959
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dou P, 2017, PROC CVPR IEEE, P1503, DOI 10.1109/CVPR.2017.164
   DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X
   Drira H, 2013, IEEE T PATTERN ANAL, V35, P2270, DOI 10.1109/TPAMI.2013.48
   Fankhauser P, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P388, DOI 10.1109/ICAR.2015.7251485
   Hernandez M, 2012, EUR SIGNAL PR CONF, P1995
   Huber P, 2017, IEEE SIGNAL PROC LET, V24, P437, DOI 10.1109/LSP.2016.2643284
   Izadi S., 2011, ACM SIGGRAPH 2011 Talks, P23, DOI 10.1145/2037826.2037857
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jongmoo Choi, 2013, 2013 IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), P19, DOI 10.1109/ROMAN.2013.6628525
   Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527
   Nguyen K, 2018, PATTERN RECOGN, V78, P23, DOI 10.1016/j.patcog.2018.01.002
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Liang Shu, 2014, Proc Int Conf 3D Vis, V2014, P31, DOI 10.1109/3DV.2014.67
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Nasrollahi K, 2014, MACH VISION APPL, V25, P1423, DOI 10.1007/s00138-014-0623-4
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Pan G, 2006, LECT NOTES COMPUT SC, V3952, P389
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Passalis G, 2011, IEEE T PATTERN ANAL, V33, P1938, DOI 10.1109/TPAMI.2011.49
   Ren JS., 2015, ADV NEURAL INFORM PR, V1, P901
   Sagawa R, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P559
   Schuon S, 2009, PROC CVPR IEEE, P343, DOI 10.1109/CVPRW.2009.5206804
   Sell J, 2014, IEEE MICRO, V34, P44, DOI 10.1109/MM.2014.9
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   van Ouwerkerk JD, 2006, IMAGE VISION COMPUT, V24, P1039, DOI 10.1016/j.imavis.2006.02.026
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9
   Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171
   Williams J, 2000, IEEE IMAGE PROC, P545, DOI 10.1109/ICIP.2000.901016
   WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479
   Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211
   Xu CH, 2006, PATTERN RECOGN LETT, V27, P1487, DOI 10.1016/j.patrec.2006.02.015
   Yang CY, 2013, PROC CVPR IEEE, P1099, DOI 10.1109/CVPR.2013.146
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yue LW, 2016, SIGNAL PROCESS, V128, P389, DOI 10.1016/j.sigpro.2016.05.002
   ZHANG ZY, 1992, INT J ROBOT RES, V11, P269, DOI 10.1177/027836499201100401
   Zhou EJ, 2015, AAAI CONF ARTIF INTE, P3871
   Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37
NR 57
TC 0
Z9 0
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
AR 23
DI 10.1145/3287309
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9JB
UT WOS:000459798800007
DA 2024-07-18
ER

PT J
AU Zheng, ZD
   Zheng, L
   Yang, Y
AF Zheng, Zhedong
   Zheng, Liang
   Yang, Yi
TI A Discriminatively Learned CNN Embedding for Person Reidentification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person reidentification; convolutional neural networks
AB In this article, we revisit two popular convolutional neural networks in person re-identification (re-ID): verification and identification models. The two models have their respective advantages and limitations due to different loss functions. Here, we shed light on how to combine the two models to learn more discriminative pedestrian descriptors. Specifically, we propose a Siamese network that simultaneously computes the identification loss and verification loss. Given a pair of training images, the network predicts the identities of the two input images and whether they belong to the same identity. Our network learns a discriminative embedding and a similarity measurement at the same time, thus taking full usage of the re-ID annotations. Our method can be easily applied on different pretrained networks. Albeit simple, the learned embedding improves the state-of-the-art performance on two public person re-ID benchmarks. Further, we show that our architecture can also be applied to image retrieval. The code is available at https://github.com/layumi/2016_person_re-ID.
C1 [Zheng, Zhedong; Zheng, Liang] Univ Technol Sydney, 15 Broadway, Ultimo, NSW 2007, Australia.
   [Yang, Yi] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, 4 South Fourth St, Beijing 100190, Peoples R China.
C3 University of Technology Sydney; Chinese Academy of Sciences; Institute
   of Software, CAS
RP Zheng, ZD (corresponding author), Univ Technol Sydney, 15 Broadway, Ultimo, NSW 2007, Australia.
EM zdzheng12@gmail.com; liangzheng06@gmail.com; yee.i.yang@gmail.com
RI yang, yang/HGT-7999-2022; Yang, Yi/B-9273-2017; Lang,
   Ming/HIK-0758-2022; Zheng, Zhedong/R-5314-2019; yang,
   yang/GWB-9426-2022; yang, yang/GVT-5210-2022
OI Yang, Yi/0000-0002-0512-880X; Zheng, Zhedong/0000-0002-2434-9050; 
CR [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2016, arXiv preprint arXiv:1611.05244
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247939
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Chang XJ, 2017, IEEE T NEUR NET LEAR, V28, P2294, DOI 10.1109/TNNLS.2016.2582746
   Chang XJ, 2017, IEEE T PATTERN ANAL, V39, P1617, DOI 10.1109/TPAMI.2016.2608901
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Houle ME, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3063595
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   LinWu Chunhua Shen, 2016, ARXIV160107255
   Lisanti G, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3038916
   Liu Hao, 2016, ARXIV160604404
   Liu WY, 2016, PR MACH LEARN RES, V48
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Mignon A, 2012, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2012.6247987
   Natarajan P, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2710128
   Radenovic F, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P587, DOI 10.1145/2671188.2749366
   Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun Y, 2014, LECT NOTES COMPUT SC, V8835, P279, DOI 10.1007/978-3-319-12640-1_34
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Tolias G., 2015, ARXIV151105879
   Ustinova E., 2015, ARXIV PREPRINT ARXIV
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   Varior Rahul Rama, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wu L, 2017, PATTERN RECOGN, V65, P238, DOI 10.1016/j.patcog.2016.12.022
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Yang YS, 2009, PROCEEDINGS OF 2009 CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE & SYSTEM DYNAMICS, VOL 8, P175, DOI 10.1145/1631272.1631298
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang Y, 2016, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2016.143
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zheng L, 2016, INT J COMPUT VISION, V120, P1, DOI 10.1007/s11263-016-0889-2
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng L, 2015, IEEE T MULTIMEDIA, V17, P648, DOI 10.1109/TMM.2015.2408563
   Zheng L, 2014, IEEE T IMAGE PROCESS, V23, P3368, DOI 10.1109/TIP.2014.2330763
   Zheng Liang, 2016, ARXIV
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
NR 59
TC 413
Z9 434
U1 6
U2 98
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2018
VL 14
IS 1
AR 13
DI 10.1145/3159171
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW9HO
UT WOS:000425646500013
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Cheung, M
   Li, XP
   She, J
AF Cheung, Ming
   Li, Xiaopeng
   She, James
TI An Efficient Computation Framework for Connection Discovery using Shared
   Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Social networks; connection discovery; bag-of-features tagging;
   user-shared images; computation framework
ID RECOMMENDATION
AB With the advent and popularity of the social network, social graphs become essential to improve services and information relevance to users for many social media applications to predict follower/followee relationship, community membership, and so on. However, the social graphs could be hidden by users due to privacy concerns or kept by social media. Recently, connections discovered from user-shared images using machine-generated labels are proved to be more accessible alternatives to social graphs. But real-time discovery is difficult due to high complexity, and many applications are not possible. This article proposes an efficient computation framework for connection discovery using user-shared images, which is suitable for any image processing and computer vision techniques for connection discovery on the fly. The framework includes the architecture of online computation to facilitate real-time processing, offline computation for a complete processing, and online/offline communication. The proposed framework is implemented to demonstrate its effectiveness by speeding up connection discovery through user-shared images. By studying 300K+ user-shared images from two popular social networks, it is proven that the proposed computation framework reduces 90% of runtime with a comparable accurate with existing frameworks.
C1 [Cheung, Ming; Li, Xiaopeng; She, James] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Clear Water Bay Rd, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Cheung, M (corresponding author), Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Clear Water Bay Rd, Kowloon, Hong Kong, Peoples R China.
EM cpming@ust.hk; xlibo@connect.ust.hk
FU HKUST-NIE Social Media Lab
FX This work is supported by HKUST-NIE Social Media Lab.
CR ACM Trans, 2017, MULTIMEDIA COMPUT CO, V13
   Agarwal D., 2010, Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '10, P703
   Agarwal V, 2013, SOC NETW ANAL MIN, V3, P359, DOI 10.1007/s13278-012-0083-7
   Amatriain Xavier, 2013, NETFLIX TECH BLOG
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2012, CSCW, DOI DOI 10.1145/2145204.2145360
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], P 2011 VIS INF COMM
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Buhler J, 2001, BIOINFORMATICS, V17, P419, DOI 10.1093/bioinformatics/17.5.419
   Cheung M., 2014, P 4 INT C ADV INF MI, P83
   Cheung Ming, 2015, P IEEE INT C CYB PHY
   Cheung Ming, 2015, IEEE T MULTIMEDIA
   Cohen E, 2001, IEEE T KNOWL DATA EN, V13, P64, DOI 10.1109/69.908981
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Das Abhinandan S, 2007, P 16 INT C WORLD WID, P271
   Diamantini C, 2014, PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON COLLABORATION TECHNOLOGIES AND SYSTEMS (CTS), P353, DOI 10.1109/CTS.2014.6867588
   Gilbert E, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P211
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Hsu W., 2006, AAAI SPRING S SERIES
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Ji JZ, 2003, IEEE/WIC INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, PROCEEDINGS, P68
   Jiang Shuhui., 2015, IEEE T MULTIMEDIA, V17, P907
   Jie Zhanming, 2015, P IEEE 4 S NETW CLOU
   Jones JJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0052168
   Kanellopoulos I, 1997, INT J REMOTE SENS, V18, P711, DOI 10.1080/014311697218719
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Leung IXY, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.066107
   Li Z, 2013, INT CONF CLOUD COMP, P1, DOI 10.1109/CloudCom.2013.7
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Qia XQ, 2014, CHINA COMMUN, V11, P109, DOI 10.1109/CC.2014.6821743
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Quijano-Sanchez L, 2011, PROC INT C TOOLS ART, P239, DOI 10.1109/ICTAI.2011.44
   Rogati M., 2010, Proceedings of the 19th international conference on World wide web, P981, DOI DOI 10.1145/1772690.1772790
   Si Xiance., 2009, Journal of Computational Information Systems, V6, P23
   Skeels MM, 2009, GROUP 2009 PROCEEDINGS, P95
   Slaney M, 2008, IEEE SIGNAL PROC MAG, V25, P128, DOI 10.1109/MSP.2007.914237
   Song Y, 2008, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE 2007, VOL 6, PTS A AND B, P515, DOI 10.1145/1390334.1390423
   StanleyWasserman Katherine, 1994, SOCIAL NETWORK ANAL, DOI 10.1017/CBO9780511815478
   Tsai D. T., 2010, P INT CARN C SEC TEC
   Wu Z., 2009, Proceedings of the 17th ACM international conference on Multimedia, P987
   Xing Xie, 2010, Proceedings of the 2010 IEEE/ACM Int'l Conference on Green Computing and Communications (GreenCom) and Int'l Conference on Cyber, Physical and Social Computing (CPSCom), P831, DOI 10.1109/GreenCom-CPSCom.2010.28
   Yang XW, 2013, IEEE T PARALL DISTR, V24, P642, DOI 10.1109/TPDS.2012.192
   Yao T., 2011, Proceedings of the 19th ACM International Conference on Multimedia', MM'11, P945
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
NR 46
TC 3
Z9 3
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 58
DI 10.1145/3115951
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300012
DA 2024-07-18
ER

PT J
AU Wei, Z
   Yan, Z
   Wu, YD
   Deng, RHJ
AF Wei, Zhuo
   Yan, Zheng
   Wu, Yongdong
   Deng, Robert Huijie
TI Trustworthy Authentication on Scalable Surveillance Video with
   Background Model Support
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE H.264/SVC; authentication; integrity; surveillance application;
   background model
ID IMAGE; SCHEME; ROBUST
AB H.264/SVC (Scalable Video Coding) codestreams, which consist of a single base layer and multiple enhancement layers, are designed for quality, spatial, and temporal scalabilities. They can be transmitted over networks of different bandwidths and seamlessly accessed by various terminal devices. With a huge amount of video surveillance and various devices becoming an integral part of the security infrastructure, the industry is currently starting to use the SVC standard to process digital video for surveillance applications such that clients with different network bandwidth connections and display capabilities can seamlessly access various SVC surveillance (sub) codestreams. In order to guarantee the trustworthiness and integrity of received SVC codestreams, engineers and researchers have proposed several authentication schemes to protect video data. However, existing algorithms cannot simultaneously satisfy both efficiency and robustness for SVC surveillance codestreams. Hence, in this article, a highly efficient and robust authentication scheme, named TrustSSV (Trust Scalable Surveillance Video), is proposed. Based on quality/spatial scalable characteristics of SVC codestreams, TrustSSV combines cryptographic and content-based authentication techniques to authenticate the base layer and enhancement layers, respectively. Based on temporal scalable characteristics of surveillance codestreams, TrustSSV extracts, updates, and authenticates foreground features for each access unit dynamically with background model support. Using SVC test sequences, our experimental results indicate that the scheme is able to distinguish between content-preserving and content-changing manipulations and to pinpoint tampered locations. Compared with existing schemes, the proposed scheme incurs very small computation and communication costs.
C1 [Wei, Zhuo] Teletech Pk Singapore Sci Pk II, 20 Sci Pk Rd 03-31-32, Singapore 117674, Singapore.
   [Deng, Robert Huijie] Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore.
   [Yan, Zheng] Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Wu, Yongdong] Agcy Sci Technol & Res, Inst Infocomm Res, Singapore 138632, Singapore.
   [Wei, Zhuo] Huaweis Shield Lab, Singapore, Singapore.
   [Yan, Zheng] Aalto Univ, Espoo 02150, Finland.
C3 Singapore Management University; Xidian University; Agency for Science
   Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research
   (I2R); Aalto University
RP Wei, Z (corresponding author), Teletech Pk Singapore Sci Pk II, 20 Sci Pk Rd 03-31-32, Singapore 117674, Singapore.
EM phdzwei@gmail.com; zheng.yan@aalto.fi; wydong@i2r.a-star.edu.sg;
   robertdeng@smu.edu.sg
RI zheng, yan/GQY-6668-2022; DENG, Robert H./E-8547-2012; yang,
   zheng/HGC-7753-2022; Yan, Zheng/AEV-7247-2022
OI Yan, Zheng/0000-0002-9697-2108; Deng, Robert/0000-0003-3491-8146
FU National Natural Science Funds of China [61402199]; Natural Science
   Funds of Guangdong [2015A030310017]
FX This work is supported by National Natural Science Funds of China (Grant
   No. 61402199) and Natural Science Funds of Guangdong (Grant No.
   2015A030310017).
CR Alghoniemy M, 2004, IEEE T IMAGE PROCESS, V13, P145, DOI 10.1109/TIP.2004.823831
   [Anonymous], 2008, Introduction to Modern Cryptography
   Bouwmans Thierry, 2011, Recent Patents on Computer Science, V4, P147, DOI 10.2174/1874479611104030147
   Brutzer S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1937, DOI 10.1109/CVPR.2011.5995508
   CAVIAR, 2004, CAVIAR TEST CAS SCEN
   Fridrich J., 2000, Proceedings International Conference on Information Technology: Coding and Computing (Cat. No.PR00540), P178, DOI 10.1109/ITCC.2000.844203
   Grois D., 2012, WATERMARKING
   Han SH, 2010, INT J INF SECUR, V9, P19, DOI 10.1007/s10207-009-0093-2
   Hefeeda M., 2011, HDB SECURITY NETWORK, P93
   JSVM Salzburg Austria, 2011, JOINT SCAL VID MOD S
   Kim HS, 2003, IEEE T CIRC SYST VID, V13, P766, DOI 10.1109/TCSVT.2003.815955
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lifei Wei, 2010, Proceedings 2010 30th International Conference on Distributed Computing Systems Workshops (ICDCS 2010 Workshops), P52, DOI 10.1109/ICDCSW.2010.36
   Lin CY, 2001, IEEE T CIRC SYST VID, V11, P153, DOI 10.1109/76.905982
   Meerwald P., 2010, INT WORKSH DIG WAT, P156
   Mokhtarian K, 2010, IEEE T MULTIMEDIA, V12, P730, DOI 10.1109/TMM.2010.2051410
   Monga V, 2007, IEEE T INF FOREN SEC, V2, P376, DOI 10.1109/TIFS.2007.902670
   Park SW, 2008, STUD COMPUT INTELL, V142, P351
   Park SW, 2011, J INF SCI ENG, V27, P129
   Schneider M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P227, DOI 10.1109/ICIP.1996.560425
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shi F, 2010, LECT NOTES COMPUT SC, V6297, P697
   Simitopoulos D, 2003, IEEE T CIRC SYST VID, V13, P732, DOI 10.1109/TCSVT.2003.815947
   SPEVI, 2005, SURV PERF EV IN SPEV
   Su PC, 2009, IEEE T CIRC SYST VID, V19, P668, DOI 10.1109/TCSVT.2009.2017404
   Wei LF, 2014, INFORM SCIENCES, V258, P371, DOI 10.1016/j.ins.2013.04.028
   Wei Z, 2013, IEEE INT CONF MULTI
   Wei Z, 2014, IEEE T INF FOREN SEC, V9, P543, DOI 10.1109/TIFS.2014.2301916
   Wu HJ, 2008, LECT NOTES COMPUT SC, V4986, P39
   Wu YD, 2006, IEEE T MULTIMEDIA, V8, P152, DOI 10.1109/TMM.2005.861283
   Yu HH, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1912, DOI 10.1109/ICC.2004.1312853
   Zhao Y. F., 2012, 6 INT C NETW SYST SE, V7645, P192
   Zhu BB, 2004, PROC SPIE, V5601, P157, DOI 10.1117/12.571869
   Zhuo Wei, 2012, Communications and Multimedia Security. 13th IFIP TC 6/TC 11 International Conference, CMS 2012. Proceedings, P72, DOI 10.1007/978-3-642-32805-3_6
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 35
TC 3
Z9 3
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2016
VL 12
IS 4
SU S
AR 64
DI 10.1145/2978573
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8DS
UT WOS:000392736100008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Ebrahim, M
   Chia, WC
AF Ebrahim, Mansoor
   Chia, Wai Chong
TI Multiview Image Block Compressive Sensing with Joint Multiphase Decoding
   for Visual Sensor Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Wireless sensor networks; image compression
ID RECONSTRUCTION
AB In this article, a multiview image compression framework, which involves the use of Block-based Compressive Sensing (BCS) and Joint Multiphase Decoding (JMD), is proposed for a Visual Sensor Network (VSN). In the proposed framework, one of the sensor nodes is configured to serve as the reference node, the others as nonreference nodes. The images are encoded independently using the BCS to produce two observed measurements that are transmitted to the host workstation. In this case, the nonreference nodes always encoded the images (I-NR) at a lower subrate when compared with the images from the reference nodes (I-R). The idea is to improve the reconstruction of I-NR using I-R. After the two observed measurements are received by the host workstation, they are first decoded independently, then image registration is applied to align I-R onto the same plane of I-NR. The aligned I-R is then fused with I-NR, using wavelets to produce the projected image I-P. Subsequently, the difference between the measurements of the I-P and I-NR is calculated. The difference is then decoded and added to I-P to produce the final reconstructed I-NR. The simulation results show that the proposed framework is able to improve the quality of I-NR on average by 2 dB to 3 dB at lower subrates when compared with other Compressive Sensing (CS)-based multiview image compression frameworks.
C1 [Ebrahim, Mansoor; Chia, Wai Chong] Sunway Univ, Fac Sci & Technol, 5 Jalan Univ Bandsr Sunway, Petaling Jaya 47500, Selangor, Malaysia.
C3 Sunway University
RP Ebrahim, M; Chia, WC (corresponding author), Sunway Univ, Fac Sci & Technol, 5 Jalan Univ Bandsr Sunway, Petaling Jaya 47500, Selangor, Malaysia.
EM 12032389@imail.sunway.edu.my; waichongc@sunway.edu.my
RI Chia, Wai Chong/GWC-1506-2022; Ebrahim, Mansoor/AAA-7362-2019
OI Ebrahim, Mansoor/0000-0002-2000-8398
FU Ministry of Science, Technology and Innovation (MOSTI) Malaysia
   [01-02-16-SF0026]
FX This work is supported by the Ministry of Science, Technology and
   Innovation (MOSTI) Malaysia, under grant Science Fund No:
   01-02-16-SF0026.
CR Ahmad N, 2013, INT J DISTRIB SENS N, DOI 10.1155/2013/490489
   [Anonymous], MATH MODELS COMPUTER
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candès E, 2007, INVERSE PROBL, V23, P969, DOI 10.1088/0266-5611/23/3/008
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258
   Chang K, 2013, IEEE INT SYMP CIRC S, P221, DOI 10.1109/ISCAS.2013.6571822
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Chen X, 2009, INT CONF ACOUST SPEE, P1005, DOI 10.1109/ICASSP.2009.4959756
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Ebrahim Mansoor, 2014, International Journal of Communication Networks and Information Security, V6, P104
   Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281
   Fitzpatrick M., 2000, HDB MED IMAGING, V2
   Fowler JE, 2010, FOUND TRENDS SIGNAL, V4, P297, DOI 10.1561/2000000033
   Fowler James E., 2013, BCS SPL BLOCK COMPR
   Frajka Tama's A., 2002, P INT C IM PROC, V2, P271, DOI [10.1109/ICIP.2002.1039926, DOI 10.1109/ICIP.2002.1039926]
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Haupt J, 2006, IEEE T INFORM THEORY, V52, P4036, DOI 10.1109/TIT.2006.880031
   Jacques L, 2011, IEEE T INFORM THEORY, V57, P559, DOI 10.1109/TIT.2010.2093310
   Jung H, 2010, INT J IMAG SYST TECH, V20, P81, DOI 10.1002/ima.20231
   Jung H, 2009, MAGN RESON MED, V61, P103, DOI 10.1002/mrm.21757
   Li Chengbo., 2009, Tval3: Tv minimization by augmented lagrangian and alternating direction algorithm
   Li Chengbo, 2010, THESIS RICE U HOUSTO
   Li Chengbo, 2013, THESIS RICE U HOUSTO
   Li X, 2010, ELECTRON LETT, V46, P1548, DOI 10.1049/el.2010.2325
   LU W, 2009, P IEEE INT C IM PROC, P3045, DOI DOI 10.1109/ICIP.2009.5414208
   Lu W., 2009, RECURSIVE RECONSTRUC
   Mathworks, 2014, AUT REG
   Misiti M, 2007, WAVELETS AND THEIR APPLICATIONS, P1, DOI 10.1002/9780470612491
   Misra S, 2012, ACM T SENSOR NETWORK, V8, DOI 10.1145/2240092.2240101
   MUN S, 2009, P INT C IM PROC CAIR, P3021, DOI DOI 10.1109/ICIP.2009.5414429
   Mun S, 2012, EUR SIGNAL PR CONF, P1424
   Park JY, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-37
   Rauhut Holger, 2010, THEORETICAL FDN NUME, DOI [10.1515/9783110226157, DOI 10.1515/9783110226157]
   Razzaque MA, 2013, ACM T SENSOR NETWORK, V10, DOI 10.1145/2528948
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Thirumalai V, 2013, J VIS COMMUN IMAGE R, V24, P649, DOI 10.1016/j.jvcir.2011.12.004
   Thirumalai V, 2012, IEEE T IMAGE PROCESS, V21, P3206, DOI 10.1109/TIP.2012.2188035
   Trocan M, 2010, IEEE IMAGE PROC, P3345, DOI 10.1109/ICIP.2010.5652767
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wakin M., 2009, PICT COD S, P1, DOI DOI 10.1109/PCS.2009.5167356
   Ye J. C., 2012, K T FOCUSS VERSION 1
   Zeeuw P., 1998, Wavelet and image fusion
   Zymnis A, 2010, IEEE SIGNAL PROC LET, V17, P149, DOI 10.1109/LSP.2009.2035667
NR 47
TC 5
Z9 5
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2016
VL 12
IS 2
AR 30
DI 10.1145/2818712
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PK
UT WOS:000373906200004
DA 2024-07-18
ER

PT J
AU Yang, XS
   Zhang, TZ
   Xu, CS
   Yang, MH
AF Yang, Xiaoshan
   Zhang, Tianzhu
   Xu, Changsheng
   Yang, Ming-Hsuan
TI Boosted Multifeature Learning for Cross-Domain Transfer
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Domain adaptation;
   multifeature; boosting; denoising auto-encoder
AB Conventional learning algorithm assumes that the training data and test data share a common distribution. However, this assumption will greatly hinder the practical application of the learned model for cross-domain data analysis in multimedia. To deal with this issue, transfer learning based technology should be adopted. As a typical version of transfer learning, domain adaption has been extensively studied recently due to its theoretical value and practical interest. In this article, we propose a boosted multifeature learning (BMFL) approach to iteratively learn multiple representations within a boosting procedure for unsupervised domain adaption. The proposed BMFL method has a number of properties. (1) It reuses all instances with different weights assigned by the previous boosting iteration and avoids discarding labeled instances as in conventional methods. (2) It models the instance weight distribution effectively by considering the classification error and the domain similarity, which facilitates learning new feature representation to correct the previously misclassified instances. (3) It learns multiple different feature representations to effectively bridge the source and target domains. We evaluate the BMFL by comparing its performance on three applications: image classification, sentiment classification and spam filtering. Extensive experimental results demonstrate that the proposed BMFL algorithm performs favorably against state-of-the-art domain adaption methods.
C1 [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] China Singapore Inst Digital Media, Singapore 119613, Singapore.
   [Yang, Ming-Hsuan] Univ Calif, Dept Elect Engn & Comp Sci, Merced, CA 95334 USA.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; University of
   California System; University of California Merced
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM xiaoshan.yang@nlpr.ia.ac.cn; tzzhang10@gmail.com; csxu@nlpr.ia.ac.cn;
   mhyang@ucmerced.edu
RI Yang, Ming-Hsuan/AAE-7350-2019; xu, cj/HJZ-3488-2023; Zhang,
   Tianzhu/AGY-9389-2022; Yang, Ming-Hsuan/T-9533-2019
OI Zhang, Tianzhu/0000-0003-0764-6106; Yang, Ming-Hsuan/0000-0003-4848-2304
FU National Program on Key Basic Research Project (973 Program)
   [2012CB316304]; National Natural Science Foundation of China [61225009,
   61303173, 61373122]; Singapore National Research Foundation under its
   International Research Centre@Singapore Funding Initiative; Direct For
   Computer & Info Scie & Enginr [1152576] Funding Source: National Science
   Foundation; Div Of Information & Intelligent Systems [1152576] Funding
   Source: National Science Foundation
FX This work is supported in part by the National Program on Key Basic
   Research Project (973 Program, Project No. 2012CB316304), and National
   Natural Science Foundation of China (61225009, 61303173). This work is
   also supported by the Singapore National Research Foundation under its
   International Research Centre@Singapore Funding Initiative and
   administered by the IDM Programme Office, and supported by National
   Natural Science Foundation of China (61373122).
CR Al-Stouhi S, 2011, LECT NOTES ARTIF INT, V6911, P60, DOI 10.1007/978-3-642-23780-5_14
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2012, ACM MM'12'
   [Anonymous], 2007, CALTECH 256 OBJECT C
   [Anonymous], 2012, P 2 ACM INT C MULTIM, DOI DOI 10.1145/2324796.2324825
   [Anonymous], P INT C MACH LEARN I
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], 2010, Proceedings of 2010 IEEE Conference on Computer Vision and Pattern Recognition, DOI DOI 10.1109/CVPR.2010.5539857
   [Anonymous], P INT C COMP VIS
   Bao B.-K., 2013, Proceedings of the International Conference on Multimedia Retrieval, P135
   Ben-David S., 2010, INT C ART INT STAT, P129
   Ben-David S., 2007, ADV NEURAL INFORM PR, V19
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bergamo Alessandro, 2010, ADV NEURAL INFORM PR, V23
   Blitzer J., 2006, PROC C EMPIRICAL MET, P120, DOI DOI 10.3115/1610075.1610094
   Blitzer J., 2011, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011, P173
   Blitzer John, 2007, P ANN C NEUR INF PRO
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Brenner M., 2012, P 2 ACM INT C MULT R
   Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57
   Chen M, 2012, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON APAC 2011
   Chen M., 2011, P INT C MACH LEARN B
   Dai Wenyuan, 2007, P 24 INT C MACHINE L, P193
   Daume III Hal, 2007, P ANN C NEUR INF PRO
   Duan LX, 2010, PROC CVPR IEEE, P1959, DOI 10.1109/CVPR.2010.5539870
   Effelsberg W, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2502434
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Glorot X., 2011, P 28 INT C INT C MAC, P513
   Gong B., 2013, P INT C MACH LEARN, P222
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Gretton A, 2009, NEURAL INF PROCESS S, P131
   Habrard Amaury, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P433, DOI 10.1007/978-3-642-40991-2_28
   Huang JY, 2007, Advances in Neural Information Processing Systems (NeurIPS), V19, P601
   Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924
   Jin Xin, 2010, P 18 ACM INT C MULT, P1235, DOI [10.1145/1873951.1874196, DOI 10.1145/1873951.1874196]
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   Lai K, 2010, INT J ROBOT RES, V29, P1019, DOI 10.1177/0278364910369190
   Liu S., 2012, P IEEE C COMP VIS PA
   Liu XC, 2013, PROCEEDINGS OF THE 1ST INTERNATIONAL JOINT SYMPOSIUM ON JOINING AND WELDING, P151
   Lu WT, 2013, IEEE T MULTIMEDIA, V15, P1920, DOI 10.1109/TMM.2013.2280895
   Ma G., 2013, P IEEE INT C MULT EX, P1
   Ma Z., 2012, MM 2012 - Proceedings of the 20th ACM International Conference on Multimedia, P469, DOI DOI 10.1145/2393347.2393414
   Naaman M, 2012, MULTIMED TOOLS APPL, V56, P9, DOI 10.1007/s11042-010-0538-7
   Ni J, 2013, PROC CVPR IEEE, P692, DOI 10.1109/CVPR.2013.95
   Orlando S, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P1285
   Pan SJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1187
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Qi G., 2011, Proc. ACM International Conference on World Wide Web, P297
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Qian Shengsheng, 2014, ACM T MULTIMEDIA COM
   Qian Shengsheng, 2014, P 22 INT C PATT REC
   Reuter T., 2012, P 2 ACM INT C MULT R
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Tan Chenhao., 2011, P 17 ACM SIGKDD INT, P1397, DOI DOI 10.1145/2020408.2020614
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang C, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1273
   Wang Y., 2012, Proceedings of the 20th ACM International Conference on Multimedia, P865, DOI [DOI 10.1145/2393347.239633216, 10.1145/2393347.239633216]
   Yang Xiaoshan, 2014, IEEE T MULTIMEDIA
   Yang X, 2007, INT J PATTERN RECOGN, V21, P961, DOI 10.1142/S0218001407005703
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Zhang TZ, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2602633
   Zhang TZ, 2011, IEEE T CIRC SYST VID, V21, P853, DOI 10.1109/TCSVT.2011.2133090
   Zhang Tianzhu, 2009, P IEEE 12 INT C COMP
   Zhu J, 2009, STAT INTERFACE, V2, P349
NR 65
TC 10
Z9 10
U1 2
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2015
VL 11
IS 3
AR 35
DI 10.1145/2700286
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CB8BF
UT WOS:000349852500003
DA 2024-07-18
ER

PT J
AU Hanjalic, A
AF Hanjalic, Alan
TI Multimedia Retrieval that Matters
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Theory; Performance; Multimedia information retrieval; multimedia
   search; multimedia indexing; utility by design
AB This article emphasizes the need to refocus multimedia information retrieval (MIR) research towards bridging the utility gap, the gap between the expected and defacto usefulness of MIR solutions. This requires us to revisit the notion of relevance, but also to consider other criteria for assessing MIR solutions, like the informativeness of the retrieved results and how helpful they are for the users. The article also states that this focus shift cannot be realized incrementally, but by revisiting the foundations of MIR solutions, that is, by a utility-by-design approach. In this respect, a number of research challenges are proposed.
C1 Delft Univ Technol, NL-2600 AA Delft, Netherlands.
C3 Delft University of Technology
RP Hanjalic, A (corresponding author), Delft Univ Technol, NL-2600 AA Delft, Netherlands.
EM A.Hanjalic@tudelft.nl
OI Hanjalic, Alan/0000-0002-5771-2549
FU Dutch national research program COMMIT
FX This publication was supported by the Dutch national research program
   COMMIT.
CR [Anonymous], 2012, P TRECVID
   BRODER A, 2010, ACM SIGIR FORUM, V36, P2
   BRODER A, 2010, ACM SIGIR FORUM, V36, P3
   Cremonesi P., 2010, P 4 ACM C REC SYST, P39, DOI [DOI 10.1145/1864708.1864721, 10.1145/1864708.1864721]
   Downie JS, 2008, ACOUST SCI TECHNOL, V29, P247, DOI 10.1250/ast.29.247
   Downie J. Stephen, 2008, ACOUST SCI TECH, V29, P4
   Hanjalic A, 2012, INT J MULTIMED INF R, V1, P139, DOI 10.1007/s13735-012-0019-z
   Hanjalic Alan., 2012, P 20 ACM INT C MULT, P1239
   Rudinac S, 2012, INT J MULTIMED INF R, V1, P263, DOI 10.1007/s13735-012-0018-0
   SHI Y., 2013, ACM T INTEL IN PRESS
   Wagstaff Kiri L, 2012, P 29 INT C MACH LEAR
NR 11
TC 5
Z9 5
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 44
DI 10.1145/2490827
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700014
DA 2024-07-18
ER

PT J
AU Lin, YC
   Yang, YH
   Chen, HH
AF Lin, Yu-Ching
   Yang, Yi-Hsuan
   Chen, Homer H.
TI Exploiting Online Music Tags for Music Emotion Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Performance; Human Factors; Music emotion classification;
   social media; online music tags; music genre; class imbalance;
   multi-label classification
ID GENRE CLASSIFICATION; RECOGNITION; FEATURES
AB The online repository of music tags provides a rich source of semantic descriptions useful for training emotion-based music classifier. However, the imbalance of the online tags affects the performance of emotion classification. In this paper, we present a novel data-sampling method that eliminates the imbalance but still takes the prior probability of each emotion class into account. In addition, a two-layer emotion classification structure is proposed to harness the genre information available in the online repository of music tags. We show that genre-based grouping as a precursor greatly improves the performance of emotion classification. On the average, the incorporation of online genre tags improves the performance of emotion classification by a factor of 55% over the conventional single-layer system. The performance of our algorithm for classifying 183 emotion classes reaches 0.36 in example-based f-score.
C1 [Lin, Yu-Ching; Chen, Homer H.] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   [Yang, Yi-Hsuan] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 11529, Taiwan.
C3 National Taiwan University; Academia Sinica - Taiwan
RP Lin, YC (corresponding author), Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
EM vagante@gmail.com; yang@citi.sinica.edu.tw; homer@cc.ee.ntu.edu.tw
OI Chen, Homer/0000-0002-8795-1911
FU National Science Council [97-2221-E-002-111-MY3,
   1-00-2221-E-002-198-MY3]
FX This work is supported by grants from National Science Council under
   Contracts 97-2221-E-002-111-MY3 and 1-00-2221-E-002198-MY3.
CR ANDREWS S, 2003, P NEUR INF PROC SYST
   [Anonymous], 2003, Proceedings of the International Symposium on Music Information Retrieval
   [Anonymous], IEEE T KNOWL DATA EN
   [Anonymous], 2010, P 10 ANN JOINT C DIG, DOI [DOI 10.1145/1816123.1816146, 10.1145/1816123.1816146, 10.1145/1816123.18161461,2,4,6,7]
   [Anonymous], 2007, P 9 INT C MUS INF RE
   [Anonymous], 2003, WORKSH LEARN IMB DAT
   Beard D., 2005, MUSICOLOGY KEY CONCE
   Bello J. P., 2005, P 6 INT C MUSIC INFO, P304, DOI 10.5281/zenodo.1417431
   Bertin-Mahieux T, 2008, J NEW MUSIC RES, V37, P115, DOI 10.1080/09298210802479250
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   DUAN Z, 2008, P 9 INT C MUS INF RE, P237
   Duda R., 1973, Pattern Classification and Scene Analysis
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Feng Y., 2003, P 26 ANN INT ACM SIG, P375, DOI [DOI 10.1145/860500.860508, 10.1145/860435, DOI 10.1145/860435]
   HANKS WF, 1987, AM ETHNOL, V14, P668, DOI 10.1525/ae.1987.14.4.02a00050
   Hu X., 2007, P 8 INT C MUSIC INFO, P67
   Huq A, 2010, J NEW MUSIC RES, V39, P227, DOI 10.1080/09298215.2010.513733
   Huron D., 2000, P INT C MUS INF RETR
   Joachims Thorsten., 2004, Learning to Classify Text Using Support Vector Machines: Methods, Theory, and Algorithms
   Juslin PN, 2004, J NEW MUSIC RES, V33, P217, DOI 10.1080/0929821042000317813
   KIM J, 2010, P AES CONV
   Kim Y.E., 2010, Proceedings of the 11th International Society for Music Information Retrieval Conference, P255
   Lamere P, 2008, J NEW MUSIC RES, V37, P101, DOI 10.1080/09298210802479284
   Laurier C, 2008, SEVENTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P688, DOI 10.1109/ICMLA.2008.96
   Lewis M., 2008, Handbook of emotions, V3rd
   Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853
   Lu L, 2006, IEEE T AUDIO SPEECH, V14, P5, DOI 10.1109/TSA.2005.860344
   Mandel M.I., 2008, P INT SOC MUSIC INFO, P577
   Mandel M, 2008, J NEW MUSIC RES, V37, P151, DOI 10.1080/09298210802479300
   Meyer L.B., 1956, Emotion and meaning in music
   Park I, 2004, PROCEEDINGS OF THE TENTH INTERNATIONAL CONFERENCE ON GROUND PENETRATING RADAR, VOLS 1 AND 2, P411
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Rentfrow PJ, 2003, J PERS SOC PSYCHOL, V84, P1236, DOI 10.1037/0022-3514.84.6.1236
   Scaringella N, 2006, IEEE SIGNAL PROC MAG, V23, P133, DOI 10.1109/MSP.2006.1598089
   Schuller B, 2010, J NEW MUSIC RES, V39, P13, DOI 10.1080/09298210903430475
   SHAH C, 2008, P IEEE ACM JOINT C D
   Slaney M, 2008, IEEE T AUDIO SPEECH, V16, P253, DOI 10.1109/TASL.2007.915221
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Trohidis Konstantinos., 2008, International Conference on Music Information Retrieval, P325
   Tsoumakas G, 2007, LECT NOTES ARTIF INT, V4701, P406
   TURNBULL D, 2007, P 8 INT C MUS INF RE, P535
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Tzanetakis G., 2000, Organised Sound, V4, P169, DOI DOI 10.1017/S1355771800003071
   Von Ahn L, 2004, P SIGCHI C HUM FACT, DOI DOI 10.1145/985692.985733
   Wieczorkowska A, 2006, ADV SOFT COMP, P307
   Yang Y.H., 2009, Proc. ISMIR, P147
   Yang Y.-H., 2006, P 14 ANN ACM INT C M, P81, DOI DOI 10.1145/1180639.1180665
   Yang YH, 2011, MULTIMEDIA COMPUT CO, P1
   Yang YH, 2008, IEEE T AUDIO SPEECH, V16, P448, DOI 10.1109/TASL.2007.911513
   Yang YH, 2011, IEEE T AUDIO SPEECH, V19, P762, DOI 10.1109/TASL.2010.2064164
NR 52
TC 20
Z9 21
U1 1
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 26
DI 10.1145/2037676.2037683
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 857NR
UT WOS:000297725800007
DA 2024-07-18
ER

PT J
AU Rabbath, M
   Sandhaus, P
   Boll, S
AF Rabbath, Mohamad
   Sandhaus, Philipp
   Boll, Susanne
TI Automatic Creation of Photo Books from Stories in Social Media
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Human Factors; Photo books; social media; social
   networks; expectation-maximization; multimedia retrieval; information
   retrieval; layout
AB Photos are a special way to tell stories of our best memories and moments. The representation of those photos in appealing physical photo books is highly appreciated by many people. Today, many photos are shared via social networking sites, where people upload their photos and share their stories with their friends. The members of social networks comment on each other's photos, add tags or descriptions and upload new photos of the same events to their albums. While the media of different personal events are available on the social network, there is no easy way to collect and bundle them into a story and print this story as a photo book. We propose an approach to automatically detect media elements that match a query (where, when, what, who) in the user's social network and intelligently arrange and compose them into a printable photo book. We combine content analysis of text and images to automatically and semi-automatically select photos of a specific story. We calculate the probabilities of each two photos to belong to the same event using an Expectation-Maximization algorithm that we propose in order to be able to retrieve them easily when receiving the user queries, and we address the differences between our model and other models that use similar proposed algorithms. People's tags and the interaction between the users and the photos as well as other semantic information are exploited to select important photos that are suitable to create the photo book. The selected photos and derived semantics are then employed to automatically create an appealing layout for the photo book.
C1 [Rabbath, Mohamad; Sandhaus, Philipp] Inst Informat Technol, OFFIS, D-26121 Oldenburg, Germany.
   [Boll, Susanne] Carl von Ossietzky Univ Oldenburg, D-2900 Oldenburg, Germany.
C3 Carl von Ossietzky Universitat Oldenburg
RP Rabbath, M (corresponding author), Inst Informat Technol, OFFIS, Escherweg2, D-26121 Oldenburg, Germany.
EM rabbath@offis.de; sandhaus@offis.de;
   susanne.boll@informatik.uni-oldenburg.de
OI Sandhaus, Philipp/0000-0002-9250-1704
CR Atkins C.B., 2008, P 16 ACM INT C MULTI, P821
   Boll S., 2011, P 17 INT C MULT MOD
   GROSS R, 2005, P ACM WORKSH PRIV EL
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   Kennedy Lyndon., 2007, P 15 INT C MULTIMEDI, P631
   Lidwell W., 2003, Universal principles of design
   Mei T, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1757, DOI 10.1109/ICME.2006.262891
   Miller AD, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P347
   Negoescu R.A., 2008, Proc. Content-based Image and Video Retrieval, P417, DOI DOI 10.1145/1386352.1386406
   *PMA, 2009, PHOT REP SUMM
   Rabbath M., 2010, P 18 INT C MULT
   Rattenbury T., 2007, Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, P103
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Shevade B, 2007, ACM-IEEE J CONF DIG, P127, DOI 10.1145/1255175.1255200
   VANHOUSE NA, 2007, P C HUM FACT COMP SY, P2717
   Westermann U, 2007, IEEE MULTIMEDIA, V14, P19, DOI 10.1109/MMUL.2007.23
   Zhao M, 2006, LECT NOTES COMPUT SC, V4071, P163
   Zhu C.-Z., 2005, P 7 ACM SIGMM INT WO, P113
NR 18
TC 7
Z9 7
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 27
DI 10.1145/2037676.2037684
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 857NR
UT WOS:000297725800008
DA 2024-07-18
ER

PT J
AU Hsu, CH
   Hefeeda, M
AF Hsu, Cheng-Hsin
   Hefeeda, Mohamed
TI Statistical Multiplexing of Variable-Bit-Rate Videos Streamed to Mobile
   Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Broadcast networks; variable-bit-rate streams; mobile TV; energy
   saving; goodput; DVB-H
ID DVB-H; BROADCAST; PERFORMANCE; LAYER
AB We address the problem of broadcasting multiple video streams over a broadcast network to many mobile devices, so that: (i) streaming quality of mobile devices is maximized, (ii) energy consumption of mobile devices is minimized, and (iii) goodput in the network is maximized. We consider two types of broadcast networks: closed-loop networks, in which all video streams are jointly encoded to ensure their total bit rate does not exceed the broadcast network bandwidth, and open-loop networks, in which videos are encoded using standalone coders, and thus must be carefully broadcast to avoid playout glitches. We first show that the problem of optimally broadcasting multiple videos is NP-complete. We then propose an approximation algorithm to construct burst schedules for multiple VBR (Variable-Bit-Rate) streams. The proposed algorithm frees network operators from the manual and error-prone bandwidth reservation process which is currently used in practice. We prove that the proposed algorithm achieves optimal goodput and near-optimal energy saving. We show that it produces glitch-free schedules in closed-loop networks, and it minimizes number of glitches in open-loop networks. We implement the proposed algorithm in a trace-driven simulator, and conduct extensive simulations for both open-and closed-loop networks. The simulation results show that the proposed algorithm outperforms the existing algorithms in many aspects, including number of late frames, number of concurrently broadcast video streams, and energy saving of mobile devices. To show the practicality and efficiency of the proposed algorithm, we also implement it in a real mobile TV testbed as a proof of concept. The results from the testbed confirm that the proposed algorithm: (i) does not result in playout glitches, (ii) achieves high energy saving, and (iii) runs in real time.
C1 [Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
C3 Simon Fraser University
RP Hefeeda, M (corresponding author), Simon Fraser Univ, Sch Comp Sci, 250-13450 102nd Ave, Surrey, BC V3T 0A3, Canada.
EM mhefeeda@cs.sfu.ca
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada and in part by the British Columbia
   Innovation Council.
CR [Anonymous], 102377 ETSI EN
   [Anonymous], 2004, 302304 ETSI EN
   [Anonymous], P ACM MULT 08 DEM SE
   [Anonymous], 2007, MULTIME DIA IP WIREL
   [Anonymous], 2009, ATSC mobile DTV standard
   *AT T, 2007, AT T SELLS WIR SPECT
   Chari MR, 2007, IEEE T BROADCAST, V53, P145, DOI 10.1109/TBC.2007.891696
   Faria G, 2006, P IEEE, V94, P194, DOI 10.1109/JPROC.2005.861011
   He ZH, 2008, IEEE T MULTIMEDIA, V10, P1237, DOI 10.1109/TMM.2008.2004903
   HEFEEDA M, 2008, P IEEE INN INF TECHN, P430
   Hefeeda M, 2010, IEEE ACM T NETWORK, V18, P610, DOI 10.1109/TNET.2009.2030326
   Hsu C.-H., 2009, P ACM MULT JAN, P411
   Hsu CH, 2010, IEEE ACM T NETWORK, V18, P681, DOI 10.1109/TNET.2009.2033058
   Hsu CH, 2009, IEEE INFOCOM SER, P2231, DOI 10.1109/INFCOM.2009.5062148
   Jacobs ME, 2008, INT TELECOM ENERGY, P1
   Kornfeld M, 2007, IEEE T BROADCAST, V53, P161, DOI 10.1109/TBC.2006.889210
   Lai HL, 2005, IEEE T CIRC SYST VID, V15, P221, DOI 10.1109/TCSVT.2004.841687
   Lakshman TV, 1998, P IEEE, V86, P952, DOI 10.1109/5.664282
   Lin JW, 2006, IEEE T MULTIMEDIA, V8, P996, DOI 10.1109/TMM.2006.879868
   Parkvall S, 2006, IEEE COMMUN MAG, V44, P68
   Pinedo M., 2012, Scheduling: Theory, algorithms, and systems, V29
   REZAEI M, 2009, INT J DIGITAL MULTIM
   Rezaei M, 2008, IEEE T MULTIMEDIA, V10, P1455, DOI 10.1109/TMM.2008.2007315
   Ribas-Corbera J, 2003, IEEE T CIRC SYST VID, V13, P674, DOI 10.1109/TCSVT.2003.814965
   Seeling P, 2005, IEEE POTENTIALS, V24, P21, DOI 10.1109/MP.2005.1549754
   Seeling P, 2004, IEEE COMMUN SURV TUT, V6, P58, DOI 10.1109/COMST.2004.5342293
   Striccoli D., 2006, P ACM INT MOB MULT C
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tagliasacchi M, 2008, IEEE T IMAGE PROCESS, V17, P1129, DOI 10.1109/TIP.2008.924278
   *UBS, 2009, UBS DVB H IP ENC DVE
   *UDCAST, 2008, UDCAST DVB H DVB SH
   Wang F, 2008, IEEE COMMUN MAG, V46, P41, DOI 10.1109/MCOM.2008.4644118
   Wang LM, 1996, IEEE T CONSUM ELECTR, V42, P300, DOI 10.1109/30.536124
   Yang XD, 2004, SYMPOTIC '04: JOINT IST WORKSHOP ON MOBILE FUTURE & SYMPOSIUM ON TRENDS IN COMMUNICATIONS, PROCEEDINGS, P183, DOI 10.1109/TIC.2004.1409529
NR 34
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2011
VL 7
IS 2
AR 12
DI 10.1145/1925101.1925107
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 729AT
UT WOS:000287919200006
DA 2024-07-18
ER

PT J
AU Maddage, NC
   Sim, KC
   Li, HZ
AF Maddage, Namunu C.
   Sim, Khe Chai
   Li, Haizhou
TI Word Level Automatic Alignment of Music and Lyrics Using Vocal Synthesis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Music-lyrics alignment; rhythm
   analysis; music indexing; music information modeling; singing voice
   synthesis
AB We propose a signal-based approach instead of the commonly used model-based approach, to automatically align vocal music with text lyrics at the word level. In this approach, we use a text-to-speech system to synthesize the singing voice according to the lyrics. In this way, aligning the music signal with the corresponding text lyrics becomes the alignment of two audio signals. This study uses the results of music information modeling and singing voice synthesis. In music information modeling, we study different music representation strategies for music segmentation, music region indexing and region content descriptions; in singing voice synthesis, we generate singing voice by making use of music knowledge to approximate the target vocal line in terms of tempo. The experimental results on a 20-song database show 26.3% and 36.1% word level alignment error rates at eighth note and sixteenth note alignment tolerances respectively. The proposed approach presents an alternative and effective solution to music-lyrics alignment which may require less training dataset.
C1 [Maddage, Namunu C.] RMIT Univ, Melbourne, Vic 3001, Australia.
   [Maddage, Namunu C.; Sim, Khe Chai; Li, Haizhou] Inst Infocomm Res, Singapore 138632, Singapore.
C3 Royal Melbourne Institute of Technology (RMIT); Agency for Science
   Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research
   (I2R)
RP Maddage, NC (corresponding author), RMIT Univ, Melbourne, Vic 3001, Australia.
EM namunu.maddage@rmit.edu.au; kcsim@i2r.a-star.edu.sg;
   hli@i2r.a-star.edu.sg
RI Li, Haizhou/Q-6438-2019
OI Li, Haizhou/0000-0001-9158-9401
CR [Anonymous], P IEEE WORKSH APPL S
   Bartsch MA, 2004, IEEE T SPEECH AUDI P, V12, P100, DOI 10.1109/TSA.2003.822637
   BERENZWEIG A, 2001, P IEEE WORKSH APPL S
   BROWN JC, 1992, J ACOUST SOC AM, V92, P1933
   CHEN K, 2006, J ACOUSTIC SOC AM
   Dannenberg R, 1984, P 1984 INT COMP MUS, P193
   DEUTSCH D, 1988, J ACOUST SOC AM, V80, P1346
   DUXBURG C, 2002, P INT C DIG AUD EFF
   ELLIS DPW, 2006, P INT C AC SPEECH SI
   FLETCHER H, 1931, J ACOUST SOC AM, V3, P1
   FUJIHARA H, 2006, P IEEE INT S MULT IS
   FURINI M, 2004, P IEEE CONS COMM NET
   Grubb L., 1997, P ICMC, P301
   HAMON C, 1989, P IEEE INT C AC SPEE, P238
   Inoue W., 1994, Proceedings of the 1994 International Computer Music Conference, P70
   Iskandar D., 2006, P 14 ACM INT C MULTI, P659
   JOHN RD, 1999, DISCRETE TIME PROCES
   Jourdain R., 1997, Music, the brain, and ecstasy: How music captures our imagination
   Katayose H., 1993, Proceedings of the International Computer Music Conference, P138
   Kim Y., 2003, Strategic advantages of information technology in construction
   KORST J, 2006, P PHIL S INT ALG
   LOSCOS A, 1999, P INT COMP MUS C ICM
   Maddage N. C., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P67, DOI 10.1145/1148170.1148185
   MAKHOUL J, 1975, P IEEE, V63, P561, DOI 10.1109/PROC.1975.9792
   NWE TL, 2004, P 5 INT S C MUS INF
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   *ROYAL SCH MUS, 1949, RUD THEOR MUS
   SAKEO H, 1978, IEEE T ACOUSTICS SPE, V26, P43
   SHEH A, 2003, P INT C MUS INF ISMI
   Stevens SS, 1937, J ACOUST SOC AM, V8, P185, DOI 10.1121/1.1915893
   Taylor P.A., 1998, P 3 INT WORKSH SPEEC
   TSAI WH, 2004, P INT S MUS INF RETR
   Wang Y., 2004, Proceedings of the 12th Annual ACM International Conference on Multimedia, P212
   Wijitha Senadeera Wijitha Senadeera, 2006, International Journal of Food Engineering, V2, P7
   WONG CH, 2006, MULTIMEDIA SYSTEMS
NR 35
TC 2
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2010
VL 6
IS 3
SI SI
AR 19
DI 10.1145/1823746.1823753
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 649UL
UT WOS:000281799300008
DA 2024-07-18
ER

PT J
AU Mohanty, SP
   Bhargava, BK
AF Mohanty, Saraju P.
   Bhargava, Bharat K.
TI Invisible Watermarking Based on Creation and Robust Insertion-Extraction
   of Image Adaptive Watermarks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Algorithms; Watermarking; invisible watermarking; image;
   content protection; copyright protection
ID INFORMATION; ATTACKS; PROTECTION; ALGORITHMS
AB This article presents a novel invisible robust watermarking scheme for embedding and extracting a digital watermark in an image. The novelty lies in determining a perceptually important subimage in the host image. Invisible insertion of the watermark is performed in the most significant region of the host image such that tampering of that portion with an intention to remove or destroy will degrade the esthetic quality and value of the image. One feature of the algorithm is that this subimage is used as a region of interest for the watermarking process and eliminates the chance of watermark removal. Another feature of the algorithm is the creation of a compound watermark using the input user watermark (logo) and attributes of the host image. This facilitates the homogeneous fusion of a watermark with the cover image, preserves the quality of the host image, and allows robust insertion-extraction. Watermark creation consists of two distinct phases. During the first phase, a statistical image is synthesized from a perceptually important subimage of the image. A compound watermark is created by embedding a watermark (logo) into the statistical synthetic image by using a visible watermarking technique. This compound watermark is invisibly embedded into the important block of the host image. The authentication process involves extraction of the perceptive logo as well statistical testing for two-layer evidence. Results of the experimentation using standard benchmarks demonstrates the robustness and efficacy of the proposed watermarking approach. Ownership proof could be established under various hostile attacks.
C1 [Mohanty, Saraju P.] Univ N Texas, Dept Comp Sci & Engn, Denton, TX 76207 USA.
   [Bhargava, Bharat K.] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
C3 University of North Texas System; University of North Texas Denton;
   Purdue University System; Purdue University
RP Mohanty, SP (corresponding author), Univ N Texas, Dept Comp Sci & Engn, Denton, TX 76207 USA.
EM smohanty@cse.unt.edu; bbshail@purdue.edu
OI Mohanty, Saraju/0000-0003-2959-6541
FU NSF [0242840, 0219110]; Direct For Computer & Info Scie & Enginr;
   Division Of Computer and Network Systems [0219110] Funding Source:
   National Science Foundation; Div Of Information & Intelligent Systems;
   Direct For Computer & Info Scie & Enginr [0242840] Funding Source:
   National Science Foundation
FX This research is partially supported by NSF Grants 0242840 and 0219110
   at Purdue University.
CR [Anonymous], THESIS INDIAN I SCI
   BARNETT R, 1999, IEE ELECT COMMUN AUG, P173
   Bender W, 2000, IBM SYST J, V39, P547, DOI 10.1147/sj.393.0547
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Cox IJ, 2002, EURASIP J APPL SIG P, V2002, P126, DOI 10.1155/S1110865702000525
   Cox IJ, 1997, P SOC PHOTO-OPT INS, V3016, P92, DOI 10.1117/12.274502
   Craver S, 1998, IEEE J SEL AREA COMM, V16, P573, DOI 10.1109/49.668979
   Eskicioglu AM, 2001, SIGNAL PROCESS-IMAGE, V16, P681, DOI 10.1016/S0923-5965(00)00050-3
   Fei CH, 2004, IEEE T IMAGE PROCESS, V13, P126, DOI 10.1109/TIP.2004.823830
   Guitart O, 2006, J ELECTRON IMAGING, V15, DOI 10.1117/1.2400067
   Heileman G. L, 1999, P 5 BAION WORKSH EM, P149
   Holliman M, 2000, IEEE T IMAGE PROCESS, V9, P432, DOI 10.1109/83.826780
   Hu YJ, 2001, ELECTRON LETT, V37, P1219, DOI 10.1049/el:20010838
   Hua X. S, 2001, P 6 INT C PATT REC I, P263
   Jiang GY, 2002, 2002 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS I AND II, P1580, DOI 10.1109/ICOSP.2002.1180099
   Kang HI, 2004, IEEE IMAGE PROC, P1553
   Khan A, 2007, INFORM FUSION, V8, P354, DOI 10.1016/j.inffus.2005.09.007
   Kundur D, 2004, IEEE T MULTIMEDIA, V6, P185, DOI 10.1109/TMM.2003.819747
   Kutter M, 1999, PROC SPIE, V3657, P226, DOI 10.1117/12.344672
   LANGELAAR G, 1999, P SPIE IS T, V3657
   Lu CS, 2000, LECT NOTES COMPUT SC, V1768, P333
   Lu ZM, 2005, IEEE T IMAGE PROCESS, V14, P822, DOI 10.1109/TIP.2005.847324
   Maes M, 2000, IEEE SIGNAL PROC MAG, V17, P47, DOI 10.1109/79.879338
   Memon N, 1998, COMMUN ACM, V41, P34
   Mintzer F, 1998, COMMUN ACM, V41, P56
   Mintzer F, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL III, P9, DOI 10.1109/ICIP.1997.631957
   Mintzer FC, 1996, IBM J RES DEV, V40, P139, DOI 10.1147/rd.402.0139
   Mohanty SP, 2006, IEEE INT SYM MULTIM, P153
   Mohanty SP, 2006, IEEE T CIRCUITS-II, V53, P394, DOI 10.1109/TCSII.2006.870216
   Mohanty SP, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1029, DOI 10.1109/ICME.2000.871535
   Mohanty SP, 1999, P 7 ACM INT MM C, V2, P49
   Osberger W, 1998, INT C PATT RECOG, P701, DOI 10.1109/ICPR.1998.711240
   PAI YT, 2005, LECT NOTES COMPUTER, P1219
   Petitcolas FAP, 1998, LECT NOTES COMPUT SC, V1525, P218
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Planitz B. M., 2005, P DIG IM COMP TECHN, P70, DOI [10.1109/DICTA.2005.7, DOI 10.1109/DICTA.2005.7]
   Podilchuk CI, 1998, IEEE J SEL AREA COMM, V16, P525, DOI 10.1109/49.668975
   Qi HY, 2008, SIGNAL PROCESS, V88, P174, DOI 10.1016/j.sigpro.2007.07.020
   REININGER RC, 1983, IEEE T COMMUN, V31, P835, DOI 10.1109/TCOM.1983.1095893
   SAXENA V, 2007, P 15 IEEE C SIGN PRO, P1
   Sequeira A, 2001, PROC SPIE, V4518, P216, DOI 10.1117/12.448206
   Servette S. D, 1998, P IEEE INT C IM PROC, V1, P445
   Shen B, 1996, P SOC PHOTO-OPT INS, V2670, P404, DOI 10.1117/12.234779
   Topkara M, 2005, LECT NOTES COMPUT SC, V3710, P470
   Tsai T., 2001, P IEEE INT WORKSH IN
   Voloshynovskiy S, 2001, IEEE COMMUN MAG, V39, P118, DOI 10.1109/35.940053
   Wolfgang RB, 1999, P IEEE, V87, P1108, DOI 10.1109/5.771067
   Wu YQ, 2001, PROCEEDINGS OF THE 6TH INTERNATIONAL SYMPOSIUM ON MOLTEN SALT CHEMISTRY AND TECHNOLOGY, P359, DOI 10.1109/CGI.2001.934699
   Xie LH, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P427, DOI 10.1109/ICIP.1998.723409
   Yeung MM, 1997, 1997 IEEE FIRST WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P357, DOI 10.1109/MMSP.1997.602661
   Zhao Y, 2004, IEEE T IMAGE PROCESS, V13, P428, DOI 10.1109/TIP.2003.821552
NR 52
TC 35
Z9 36
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2008
VL 5
IS 2
AR 12
DI 10.1145/1413862.1413865
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 376AU
UT WOS:000261155900003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Goel, A
   Krasic, C
   Walpole, J
AF Goel, Ashvin
   Krasic, Charles
   Walpole, Jonathan
TI Low-latency adaptive streaming over TCP
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE measurements; performance; TCP; low latency streaming; multimedia
   applications
AB Media streaming over TCP has become increasingly popular because TCP's congestion control provides remarkable stability to the Internet. Streaming over TCP requires adapting to bandwidth availability, but unforunately, TCP can introduce significant latency at the application level, which causes unresponsive and poor adaptation. This article shows that this latency is not inherent in TCP but occurs as a result of throughput-optimized TCP implementations. We show that this latency can be minimized by dynamically tuning TCP's send buffer. Our evaluation shows that this approach leads to better application-level adaptation and it allows supporting interactive and other low-latency applications over TCP.
C1 [Goel, Ashvin] Univ Toronto, Toronto, ON M5S 3G4, Canada.
   [Krasic, Charles] Univ British Columbia, Vancouver, BC V6T 1Z4, Canada.
   [Walpole, Jonathan] Portland State Univ, Portland, OR 97207 USA.
C3 University of Toronto; University of British Columbia; Portland State
   University
RP Goel, A (corresponding author), Univ Toronto, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
EM ashvin@eecg.toronto.edu; krasic@cs.ubc.ca; walpole@pdx.edu
CR Allman M., 1999, IETF RFC 2581
   ALLMAN M, 2003, 3465 RFC
   [Anonymous], 2000, RFC 2884
   [Anonymous], 2000, 2960 RFC
   BANSAL D, 2001, P ACM SIGCOMM ACM NE
   CLARK DD, 1990, P ACM S COMM ARCH PR, P200
   FENG W, 1997, CSETR34997 U MICH NO
   FENG W, 1999, P SPIE MULT COMP NET, P286
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Floyd S., 2002, PROBLEM STA IN PRESS
   Floyd S, 1993, IEEE ACM T NETWORK, V1, P397, DOI 10.1109/90.251892
   Goel A, 2002, INT WORKSH QUAL SERV, P193, DOI 10.1109/IWQoS.2002.1006587
   Huffaker B., 2001, P WORKSH PASS ACT ME
   Hurley P, 1999, INT WORKSH QUAL SERV, P132, DOI 10.1109/IWQOS.1999.766487
   Iannaccone G, 2001, ACM SIGCOMM COMP COM, V31, P4, DOI 10.1145/505659.505661
   Kohler E., 2006, Datagram Congestion Control Protocol (DCCP) (No. rfc4340)
   Kozuch M, 2002, FOURTH IEEE WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS, PROCEEDINGS, P40, DOI 10.1109/MCSA.2002.1017484
   Krasic C., 2003, Proceedings of the 13th International Workshop on Network and Operating Systems Support for Digital Audio and Video, P112
   Mathis M., 1996, TCP Selective Acknowledgment Options," RFC 2018
   MATHIS M, 1999, RATE HALVING ALGORIT
   McCann J., 1996, 1981 RFC
   *NETMEETING, WIND NETM
   *NISTNET, NIST NETW EM TOOL
   Nonnenmacher J, 1998, IEEE ACM T NETWORK, V6, P349, DOI 10.1109/90.720869
   Ramakrishnan K., 2001, The Addition of Explicit Congestion Notification (ECN) to IP
   *REALVNC LTC, 2002, REALVNC
   Rejaie R, 1999, COMP COMM R, V29, P189, DOI 10.1145/316194.316222
   Rizzo L., 1997, ACM COMPUTER COMMUNI, V27, P24, DOI DOI 10.1145/263876.263881
   Semke J., 1998, Computer Communication Review, V28, P315, DOI 10.1145/285243.285292
   SHENKER S, 1991, P ACM SIGCOMM ACM NE, P133
   YANG Y, 2000, TR200009 U TEX AUST
NR 31
TC 15
Z9 17
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2008
VL 4
IS 3
AR 20
DI 10.1145/1386109.1386113
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 351OI
UT WOS:000259433300004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Metcalf, C
   Harboe, G
   Tullio, J
   Massey, N
   Romano, G
   Huang, EM
   Bentley, F
AF Metcalf, Crysta
   Harboe, Gunnar
   Tullio, Joe
   Massey, Noel
   Romano, Guy
   Huang, Elaine M.
   Bentley, Frank
TI Examining Presence and Lightweight Messaging in a Social Television
   Experience
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Experimentation; Human Factors; Social television; ambient
   displays; computer-mediated communication; awareness displays
AB We report on a field evaluation of a prototype social television system (Social TV) that incorporates lightweight messaging as well as ambient awareness of user presence on the system. This evaluation was conducted over a two-week period and involved the participation of ten households. Participants appreciated the ability to see their buddies' presence on the system, the ability to see or suggest the programs they were currently watching, and the ability to send short messages to one another. The presence facilities available in Social TV also allowed participants to learn more about one another's TV viewing habits and preferences, and fostered a sense of connectedness between them. However, they also felt constrained by the limitations of the communication options available to them and demanded free-form text or voice chat to be able to fully express themselves.
C1 [Metcalf, Crysta; Harboe, Gunnar; Tullio, Joe; Massey, Noel; Romano, Guy; Huang, Elaine M.; Bentley, Frank] Motorola Inc, Schaumburg, IL 60195 USA.
RP Metcalf, C (corresponding author), Motorola Inc, Schaumburg, IL 60195 USA.
RI Bentley, Frank/S-6142-2019
OI Bentley, Frank/0000-0002-9150-1331
CR AIPPERSPACH R, 2006, P UB 2006
   [Anonymous], 1998, Handbook of methods in cultural anthropology
   Baillie L, 2007, ITI, P215
   BEYER H, 1998, CONTEXTUAL DESIGN DE
   Chorianopoulos K, 2008, INT J HUM-COMPUT INT, V24, P113, DOI 10.1080/10447310701821574
   COPPENS T, 2004, P EUR INT TEL C EURO
   Geerts D., 2006, P NORDICHI 2006, P461, DOI DOI 10.1145/1182475.1182537
   HARBOE G, 2008, P ACM C HUM FACT COM
   HARBOE G, 2007, EUR INT TEL C, P116
   LeCompte M.D., 2010, Designing and Conducting Ethnographic Research, V2nd
   LUYTEN K, 2006, ACM C HUM FACT COMP
   METCALF CJ, 2006, P C ETHN PRAX IND, P49
   Mori M., 1970, Energy, V7, P33, DOI [DOI 10.1109/MRA.2012.2192811, 10.1109/MRA.2012.2192811]
   Oehlberg L., 2006, P EUROLTV, P25
   Regan T., 2004, P NORDICHI 2004, P141
   Weisz J.D., 2007, P SIGCHI 2007, P877
NR 16
TC 15
Z9 16
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 4
AR 27
DI 10.1145/1412196.1412200
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 376AP
UT WOS:000261155300004
DA 2024-07-18
ER

PT J
AU Wang, B
   Kurose, J
   Shenoy, P
   Towsley, D
AF Wang, Bing
   Kurose, Jim
   Shenoy, Prashant
   Towsley, Don
TI Multimedia streaming via TCP: An analytic performance study
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE performance; performance modeling; multimedia streaming
AB TCP is widely used in commercial multimedia streaming systems, with recent measurement studies indicating that a significant fraction of Internet streaming media is currently delivered over HTTP/TCP. These observations motivate us to develop analytic performance models to systematically investigate the performance of TCP for both live and stored-media streaming. We validate our models via ns simulations and experiments conducted over the Internet. Our models provide guidelines indicating the circumstances under which TCP streaming leads to satisfactory performance, showing, for example, that TCP generally provides good streaming performance when the achievable TCP throughput is roughly twice the media bitrate, with only a few seconds of startup delay.
C1 [Wang, Bing] Univ Connecticut, Dept Comp Sci & Engn, Storrs, CT 06269 USA.
   [Kurose, Jim; Shenoy, Prashant; Towsley, Don] Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
C3 University of Connecticut; University of Massachusetts System;
   University of Massachusetts Amherst
RP Wang, B (corresponding author), Univ Connecticut, Dept Comp Sci & Engn, Storrs, CT 06269 USA.
EM bing@engr.uconn.edu; kurose@cs.umass.edu; shenoy@cs.umass.edu;
   towsley@cs.umass.edu
OI Towsley, Donald/0000-0002-7808-7375; Shenoy,
   Prashant/0000-0002-5435-1901
CR Altman E, 2000, ACM SIGCOMM COMP COM, V30, P231, DOI 10.1145/347057.347549
   [Anonymous], P ACM SIGCOMM 98
   [Anonymous], P 7 INT WEB CONT CAC
   Bendat J. S., 1986, RANDOM DATA ANAL MEA
   BOHACEK S, 2003, P IEEE INFOCOM
   BOUTREMANS C, 2003, P IEEE INFOCOM SAN F
   Cardwell N., 2000, INFOCOM 2000 19 ANN, V3, P1742, DOI [DOI 10.1109/INFCOM.2000.832574, 10.1109/INFCOM.2000.832574]
   DECUETOS P, 2002, INT C MULT EXP ICME0
   DECUETOS P, 2002, P ACM INT WORKSH NET
   ESARY JD, 1967, ANN MATH STAT, V38, P1466, DOI 10.1214/aoms/1177698701
   FIGUEIREDO DR, 2002, COMPUT NETW J
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   HUFFAKER B, 2001, WORKSH PASS ACT MEAS
   Kim T, 2006, PROC SPIE, V6077, DOI 10.1117/12.643583
   KRASIC C, 2001, P ACM MULT DOCT S OT
   LI M, 2005, ACM T INTERNET TECH, V5, P5
   Mathis M., 1997, COMPUT COMMUN, V27, P3
   Mellia M, 2002, IEEE COMMUN LETT, V6, P85, DOI 10.1109/4234.984705
   MISRA V, 1999, P PERFORMANCE IST TU
   PADHYE J, 1999, 9902 U MASS DEP COMP
   Rejaie R, 1999, COMP COMM R, V29, P189, DOI 10.1145/316194.316222
   SEELAM N, 2001, P MAN MULT NETW SERV
   SILVA ED, 1998, STOCH MODELS, V14, P509
   SILVA EDE, 2000, P 11 INT C MOD TOOLS
   Sripanidkulchai K., 2004, Proc. ACM Internet Measurement Conference, P41
   Stevens W, 1994, TCP/ IP Illustrated, V1
   VERSCHEURE O, 1998, P ACM INT WORKSH NET
   WANG B, 2004, 0421 U MASS DEP COMP
   WANG Y, 2001, P ACM SIGCOMM INT ME
NR 29
TC 103
Z9 114
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 2
AR 16
DI 10.1145/1352012.1352020
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 306RG
UT WOS:000256264900008
DA 2024-07-18
ER

PT J
AU Shao, X
   Xu, CS
   Maddage, NC
   Tian, Q
   Kankanhalli, MS
   Jin, JS
AF Shao, Xi
   Xu, Changsheng
   Maddage, Namunu C.
   Tian, Qi
   Kankanhalli, Mohan S.
   Jin, Jesse S.
TI Automatic summarization of music videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; experimentation; performance; music summarization; video
   summarization; music video
AB In this article, we propose a novel approach for automatic music video summarization. The proposed summarization scheme is different from the current methods used for video summarization. The music video is separated into the music track and video track. For the music track, a music summary is created by analyzing the music content using music features, an adaptive clustering algorithm, and music domain knowledge. Then, shots in the video track are detected and clustered. Finally, the music video summary is created by aligning the music summary and clustered video shots. Subjective studies by experienced users have been conducted to evaluate the quality of music summaries and effectiveness of the proposed summarization approach. Experiments are performed on different genres of music videos and comparisons are made with the summaries generated based on music track, video track, and manually. The evaluation results indicate that summaries generated using the proposed method are effective in helping realize users' expectations.
C1 Inst Infocomm Res, Singapore, Singapore.
   Natl Univ Singapore, Sch Comput, Singapore 117548, Singapore.
   Univ Newcastle, Sch Design Communicat & IT, Newcastle, NSW 2308, Australia.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); National University of Singapore;
   University of Newcastle
RP Shao, X (corresponding author), Inst Infocomm Res, 21 Heng Mui Keng Terrace, Singapore, Singapore.
EM xucs@i2r.a-star.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019; chen, yue/JXW-9556-2024; xu,
   cj/HJZ-3488-2023; shao, xi/ABE-3263-2021
OI Kankanhalli, Mohan/0000-0002-4846-2015; 
CR AGNIHOTRI L, 2004, P IEEE INT C MULT EX
   [Anonymous], 1988, Proceedings of the SIGCHI conference on Human factors in computing systems-CHI, DOI DOI 10.1145/57167.57203
   [Anonymous], P 11 ACM INT C MULT
   Assfalg J, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P825, DOI 10.1109/ICME.2002.1035909
   Bartsch MA, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P15, DOI 10.1109/ASPAA.2001.969531
   Chai Wei, 2003, P 11 ACM INT C MULT, P223
   Cooper M., 2002, P 3 INT S MUSIC INFO, P81
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Dannenberg R.B., 2002, Proceedings of the International Symposium on Music Information Retrieval, P63
   Deller J., 1999, Discrete-time processing of speech signals
   DeMenthon D., 1998, Proceedings ACM Multimedia 98, P211, DOI 10.1145/290747.290773
   Duda R., 1973, Pattern Classification and Scene Analysis
   EUGENE N, 1990, ANAL COGNITION BASIC
   Foote J., 2002, P ACM MULTIMEDIA 200, P553, DOI DOI 10.1145/641007.641119
   Gao S, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P1576
   GONG Y, 2001, P IEEE INT C MULT EX, P788
   Gong YH, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P285, DOI 10.1109/ICME.2002.1035774
   Gunsel B, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P128, DOI 10.1109/ICIP.1998.727150
   JOAMCHIMS T, 1998, P EUR C MACH LEARN
   KRAFT R, 2001, Patent No. 016225546
   LOGAN B, 2000, P IEEE INT C AUD SPE, V2, pII749
   Lu Lie., 2003, P 11 ACM INT C MULTI, P140, DOI [10.1145/957013.957043, DOI 10.1145/957013.957043]
   Nakamura Y, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P393, DOI 10.1145/266180.266391
   Papageorgiou CP, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P555, DOI 10.1109/ICCV.1998.710772
   PEETERS G, 2002, P 3 INT C MUS INF RE, P86
   Pfeiffer S, 1996, J VIS COMMUN IMAGE R, V7, P345, DOI 10.1006/jvci.1996.0030
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Scheirer ED, 1998, J ACOUST SOC AM, V103, P588, DOI 10.1121/1.421129
   SUGANO Y, 2000, MULTIMEDIA MODELING, P1
   SUN X, 2001, LECT NOTES COMPUTER, V2195
   Sundaram Hari., 2002, COMUNICA O APRESENTA, DOI [DOI 10.1145/641007.641042, 10.1145/641007.641042]
   XU C, 2002, P IEEE INT C MULT EX, V1, P117
   YOW D, 1995, P 2 AS C COMP VIS, V2, P499
   Zettl H., 1999, SIGHT SOUND MOTION
   ZHANG T, 2003, P IEEE C MULT EXP, V1
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 36
TC 6
Z9 10
U1 3
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2006
VL 2
IS 2
BP 127
EP 148
DI 10.1145/1142020.1142023
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IX
UT WOS:000250871300003
DA 2024-07-18
ER

PT J
AU Wu, HH
   Claypool, M
   Kinicki, R
AF Wu, Huahui
   Claypool, Mark
   Kinicki, Robert
TI Adjusting Forward Error Correction with Temporal Scaling for
   TCP-Friendly Streaming MPEG
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Design; Multimedia networking; MPEG; forward error
   correction; TCP-friendly
AB New TCP-friendly constraints require multimedia flows to reduce their data rates under packet loss to that of a conformant TCP flow. To reduce data rates while preserving real-time playout, temporal scaling can be used to discard the encoded multimedia frames that have the least impact on perceived video quality. To limit the impact of lost packets, Forward Error Correction (FEC) can be used to repair frames damaged by packet loss. However, adding FEC requires further reduction of multimedia data, making the decision of how much FEC to use of critical importance. Current approaches use either inflexible FEC patterns or adapt to packet loss on the network without regard to TCP-friendly data rate constraints. In this article, we analytically model the playable frame rate of a TCP-friendly MPEG stream with FEC and temporal scaling, capturing the impact of distributing FEC within MPEG frame types with interframe dependencies. For a given network condition and MPEG video encoding, we use our model to exhaustively search for the optimal combination of FEC and temporal scaling that yields the highest playable frame rate within TCP-friendly constraints. Analytic experiments over a range of network and application conditions indicate that adjustable FEC with temporal scaling can provide a significant performance improvement over current approaches. Extensive simulation experiments based on Internet traces show that our model can be effective as part of a streaming protocol that chooses FEC and temporal scaling patterns that meet dynamically-changing application and network conditions.
C1 [Wu, Huahui; Claypool, Mark; Kinicki, Robert] Worcester Polytech Inst, Dept Comp Sci, Worcester, MA 01609 USA.
C3 Worcester Polytechnic Institute
RP Wu, HH (corresponding author), Worcester Polytech Inst, Dept Comp Sci, 100 Inst Rd, Worcester, MA 01609 USA.
EM flashine@cs.wpi.edu; claypool@cs.wpi.edu; rek@cs.wpi.edu
RI Claypool, Mark/ABC-5316-2020
CR Acharya S., 1998, P ACM SPIE MULT COMP, P166
   Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   [Anonymous], P ACM SIGCOMM CAMBR
   BOCHECK P, 1999, P INT WORKSH NETW OP
   BOLOT JC, 1999, P IEEE INFOCOM NEW Y
   BOUTREMANS C, 2002, P NETW OP SYST SUPP
   Boyce J. M., 1998, Proceedings ACM Multimedia 98, P181, DOI 10.1145/290747.290770
   BRADEN B, 1998, 2309 IETF RFC
   CHUNG J, 2003, P PACK VID WORKSH PV
   DESANTIS T, 2003, TCP TRAFFIC ANAL
   FEAMSTER N, 2002, P 12 INT PACK VID WO
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   FLOYD S, 1994, COMPUT COMM REV
   Floyd S., 1999, IEEE ACM T NETW
   GARRET MW, 1994, P ACM SIGCOMM
   HARDMAN V, 1995, P INT SOC INT NETW C
   HARTANTO F, 1999, P 10 IEEE WORKSH LOC
   HEMY M, 1999, P PACK VID WORKSH
   *INT TEL UN, 1996, G114 INT TEL UN
   JAISWAL S, 2004, P IEEE INF HONG KONG
   KRUNZ M, 1995, IEEE INFOCOM SER, P455, DOI 10.1109/INFCOM.1995.515909
   LAMBRECHT CJV, 1996, P SPIE
   LIU YL, 2000, P IS T SPIE ACM MULT
   LOGUINOV D, 2001, P ACM SIGCOMM INT ME
   MAHAJAN R, 2001, P 9 INT C NETW PROT
   MAYERPATEL K, 2002, P ACM MULT
   Mena A., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P101, DOI 10.1109/INFCOM.2000.832178
   Mitchell J.L., 1996, MPEG VIDEO COMPRESSI
   Nguyen T., 2002, 12 INT PACK VID WORK
   PADHYE C, 2000, P IEEE INT PERF COMP
   PADHYE J, 1998, P ACM SIGCOMM VANC B
   Park KH, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P426, DOI 10.1109/MMCS.1999.778482
   PAXSON V, 1999, IEEE ACM T NETWOR
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   *REAL NETW INC, 2000, REALPRODUCER US GUID
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Rejaie R, 1999, IEEE INFOCOM SER, P1337, DOI 10.1109/INFCOM.1999.752152
   RHEE I, 1998, P ACM SIGCOMM C
   Rizzo L., 1997, FEASIBILITY SOFTWARE
   Rose O., 1995, Proceedings. 20th Conference on Local Computer Networks (Cat. No.95TB100005), P397, DOI 10.1109/LCN.1995.527368
   TRIPATHI A, 2002, WORKSH INT MULT COMP
   Wang YB, 2001, IMW 2001: PROCEEDINGS OF THE FIRST ACM SIGCOMM INTERNET MEASUREMENT WORKSHOP, P295
   WU H, 2003, WPICSTR0310 CS DEP
   WU H, 2003, P WORKSH NETW OP SYS
   WU H, 2004, P PACK VID WORKSH PV
NR 45
TC 5
Z9 6
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2005
VL 1
IS 4
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DX
UT WOS:000205012500001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Niu, TZ
   Dong, SS
   Chen, ZD
   Luo, X
   Guo, SQ
   Huang, Z
   Xu, XS
AF Niu, Tian-Zi
   Dong, Shan-Shan
   Chen, Zhen-Duo
   Luo, Xin
   Guo, Shanqing
   Huang, Zi
   Xu, Xin-Shun
TI Semantic Enhanced Video Captioning with Multi-feature Fusion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video captioning; semantic encoder; discrete selection; multi-feature
   fusion
ID NETWORK
AB Video captioning aims to automatically describe a video clip with informative sentences. At present, deep learning-based models have become the mainstream for this task and achieved competitive results on public datasets. Usually, these methods leverage different types of features to generate sentences, e.g., semantic information, 2D or 3D features. However, some methods only treat semantic information as a complement of visual representations and cannot fully exploit it; some of them ignore the relationship between different types of features. In addition, most of them select multiple frames of a video with an equally spaced sampling scheme, resulting in much redundant information. To address these issues, we present a novel video-captioning framework, Semantic Enhanced video captioning with Multi-feature Fusion, SEMF for short. It optimizes the use of different types of features from three aspects. First, a semantic encoder is designed to enhance meaningful semantic features through a semantic dictionary to boost performance. Second, a discrete selection module pays attention to important features and obtains different contexts at different steps to reduce feature redundancy. Finally, a multi-feature fusionmodule uses a novel relation-aware attentionmechanism to separate the common and complementary components of different features to provide more effective visual features for the next step. Moreover, the entire framework can be trained in an end-to-endmanner. Extensive experiments are conducted on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT) datasets. The results demonstrate that SEMF is able to achieve state-of-the-art results.
C1 [Niu, Tian-Zi; Dong, Shan-Shan; Chen, Zhen-Duo; Luo, Xin; Xu, Xin-Shun] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Guo, Shanqing] Shandong Univ, Sch Cyber Sci & Technol, Qingdao 266237, Peoples R China.
   [Huang, Zi] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Australia.
C3 Shandong University; Shandong University; University of Queensland
RP Xu, XS (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
EM 201920560@mail.sdu.edu.cn; 201914779@mail.sdu.edu.cn;
   chenzd.sdu@gmail.com; luoxin.lxin@gmail.com; guoshanqing@sdu.edu.cn;
   huang@itee.uq.edu.au; xuxinshun@sdu.edu.cn
RI Luo, Xin/HNR-3191-2023
OI Luo, Xin/0000-0002-6901-5476; Chen, Zhen-Duo/0000-0002-3481-4892; HUANG,
   ZI/0000-0002-9738-4949
FU National Natural Science Foundation of China [62172256, 62202278,
   62202272]; Natural Science Foundation of Shandong Province [ZR2019ZD06];
   Major Program of the National Natural Science Foundation of China
   [61991411]; Quan Cheng Laboratory, Jinan, China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant Nos. 62172256, 62202278, and 62202272,
   in part by Natural Science Foundation of Shandong Province under Grant
   ZR2019ZD06, and in part by the Major Program of the National Natural
   Science Foundation of China under Grant 61991411, and in part by the
   Quan Cheng Laboratory, Jinan, China.
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen HR, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.475767
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Chen YY, 2018, LECT NOTES COMPUT SC, V11217, P367, DOI 10.1007/978-3-030-01261-8_22
   Cho K., 2014, ARXIV14061078
   Das P, 2013, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2013.340
   Deng JC, 2022, IEEE T CIRC SYST VID, V32, P880, DOI 10.1109/TCSVT.2021.3063423
   Dong SS, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3550276
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Jang E., 2017, P ICLR, P1
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin K, 2021, AAAI CONF ARTIF INTE, V35, P2047
   Liu S, 2021, IEEE T PATTERN ANAL, V43, P3259, DOI 10.1109/TPAMI.2019.2940007
   Liu XX, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409388
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Song WP, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1161, DOI 10.1145/3357384.3357925
   Song YQ, 2021, PROC CVPR IEEE, P11240, DOI 10.1109/CVPR46437.2021.01109
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan GC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P745
   Tang PJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303083
   Tang ZN, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P2415
   Thomason J., 2014, COLING, P1218
   Vaidya J, 2022, IEEE WINT CONF APPL, P2442, DOI 10.1109/WACV51458.2022.00250
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Wei YW, 2020, IEEE T IMAGE PROCESS, V29, P1, DOI 10.1109/TIP.2019.2923608
   Wu Hanjie, 2022, ACM Trans. Multimedia Comput. Commun. Appl., V18
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Yang B, 2021, AAAI CONF ARTIF INTE, V35, P3119
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Yuan J, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zheng Q., 2020, 2020 CVPR, P13093, DOI 10.1109/CVPR42600.2020.01311
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 61
TC 0
Z9 0
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 182
DI 10.1145/3588572
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200005
DA 2024-07-18
ER

PT J
AU Tang, ZJ
   Chen, ZY
   Li, ZX
   Zhong, BN
   Zhang, XQ
AF Tang, Zhenjun
   Chen, Zhiyuan
   Li, Zhixin
   Zhong, Bineng
   Zhang, Xianquan
TI Unifying Dual-Attention and Siamese Transformer Network for
   Full-Reference Image Quality Assessment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Transformer; siamese network; dual-attention; image quality assessment
   (IQA)
ID STATISTICAL EVALUATION; INFORMATION; SIMILARITY; INDEX
AB Image Quality Assessment (IQA) is a critical task of computer vision. Most Full-Reference (FR) IQA methods have limitation in the accurate prediction of perceptual qualities of the traditional distorted images and the Generative Adversarial Networks (GANs) based distorted images. To address this issue, we propose a novel method by Unifying Dual-Attention and Siamese Transformer Network ( UniDASTN) for FR-IQA. An important contribution is the spatial attention module composed of a Siamese Transformer Network and a feature fusion block. It can focus on significant regions and effectively maps the perceptual differences between the reference and distorted images to a latent distance for distortion evaluation. Another contribution is the dual-attention strategy that exploits channel attention and spatial attention to aggregate features for enhancing distortion sensitivity. In addition, a novel loss function is designed by jointly exploiting Mean Square Error (MSE), bidirectional Kullback-Leibler divergence, and rank order of quality scores. The designed loss function can offer stable training and thus enables the proposed UniDASTN to effectively learn visual perceptual image quality. Extensive experiments on standard IQA databases are conducted to validate the effectiveness of the proposed UniDASTN. The IQA results demonstrate that the proposed UniDASTN outperforms some state-of-the-art FR-IQA methods on the LIVE, CSIQ, TID2013, and PIPAL databases.
C1 [Tang, Zhenjun; Chen, Zhiyuan; Li, Zhixin; Zhong, Bineng; Zhang, Xianquan] Guangxi Normal Univ, Minist Educ, Key Lab Educ Blockchain & Intelligent Technol, Guilin 541004, Peoples R China.
   [Tang, Zhenjun; Chen, Zhiyuan; Li, Zhixin; Zhong, Bineng; Zhang, Xianquan] Guangxi Normal Univ, Guangxi Key Lab MultiSource Informat Mining & Se, Guilin 541004, Peoples R China.
   [Zhang, Xianquan] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 Guangxi Normal University; Guangxi Normal University; Fudan University
RP Tang, ZJ (corresponding author), Guangxi Normal Univ, Minist Educ, Key Lab Educ Blockchain & Intelligent Technol, Guilin 541004, Peoples R China.; Tang, ZJ (corresponding author), Guangxi Normal Univ, Guangxi Key Lab MultiSource Informat Mining & Se, Guilin 541004, Peoples R China.
EM tangzj230@163.com; 354898119@qq.com; lizx@gxnu.edu.cn;
   bnzhong@gxnu.edu.cn; zxq6622@163.com
OI Tang, Zhenjun/0000-0003-3664-1363; Zhang, Xianquan/0000-0003-3359-117X;
   Li, Zhixin/0000-0002-5313-6134; Chen, Zhiyuan/0000-0001-6888-4859
FU Guangxi Natural Science Foundation [2022GXNSFAA035506]; National Natural
   Science Foundation of China [62272111, 62062013, 61962008, 62276073,
   U22B2047, U1936214]; Guangxi "Bagui Scholar" Team for Innovation and
   Research; Guangxi Talent Highland Project of Big Data Intelligence and
   Application; Guangxi Collaborative Innovation Center of Multi-source
   Information Integration and Intelligent Processing; Innovation Project
   of Guangxi Graduate Education [YCSW2022177]
FX This work is partially supported by the Guangxi Natural Science
   Foundation (2022GXNSFAA035506), the National Natural Science Foundation
   of China (62272111, 62062013, 61962008, 62276073, U22B2047, U1936214),
   Guangxi "Bagui Scholar" Team for Innovation and Research, Guangxi Talent
   Highland Project of Big Data Intelligence and Application, Guangxi
   Collaborative Innovation Center of Multi-source Information Integration
   and Intelligent Processing, and the Innovation Project of Guangxi
   Graduate Education (YCSW2022177).
CR Abdi H., 2007, Encyclopedia of measurement and statistics, P508, DOI DOI 10.4135/9781412952644.N239
   Ahn S, 2021, IEEE COMPUT SOC CONF, P344, DOI 10.1109/CVPRW53098.2021.00044
   Avcibas I, 2002, J ELECTRON IMAGING, V11, P206, DOI 10.1117/1.1455011
   Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P2392, DOI 10.1109/TIP.2016.2545863
   Benesty J, 2009, SPRINGER TOP SIGN PR, V2, P1, DOI 10.1007/978-3-642-00296-0_1
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Bosse S, 2016, IEEE IMAGE PROC, P3773, DOI 10.1109/ICIP.2016.7533065
   Chen CLZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3447393
   Chen ZY, 2022, PROC INT C TOOLS ART, P861, DOI 10.1109/ICTAI56018.2022.00132
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   Feng P, 2023, MULTIMEDIA SYST, V29, P693, DOI 10.1007/s00530-022-01003-8
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gu Jinjin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P633, DOI 10.1007/978-3-030-58621-8_37
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Guo HY, 2021, IEEE COMPUT SOC CONF, P443, DOI 10.1109/CVPRW53098.2021.00055
   Hainan Zhang, 2020, IVSP '20: Proceedings of the 2020 2nd International Conference on Image, Video and Signal Processing, P81, DOI 10.1145/3388818.3388828
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He YJ, 2022, PROC CVPR IEEE, P9109, DOI 10.1109/CVPR52688.2022.00891
   Henderson JM, 2003, TRENDS COGN SCI, V7, P498, DOI 10.1016/j.tics.2003.09.006
   Huang ZQ, 2021, IEEE T CIRC SYST VID, V31, P2808, DOI 10.1109/TCSVT.2020.3027001
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Lao SS, 2022, IEEE COMPUT SOC CONF, P1139, DOI 10.1109/CVPRW56347.2022.00123
   Laparra V., 2016, P HUM VIS EL IM HVEI, P43
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Liang J, 2022, PROC CVPR IEEE, P7993, DOI 10.1109/CVPR52688.2022.00784
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3414837
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2011, CONF REC ASILOMAR C, P723, DOI 10.1109/ACSSC.2011.6190099
   Pei SC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440172
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Seo S, 2021, IEEE T CIRC SYST VID, V31, P2602, DOI 10.1109/TCSVT.2020.3030895
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Shi R, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491229
   Tang ZJ, 2018, COMPUT J, V61, P1695, DOI 10.1093/comjnl/bxy047
   Varga D, 2020, SIGNAL IMAGE VIDEO P, V14, P1265, DOI 10.1007/s11760-020-01664-w
   Vaswani A., 2017, Adv. Neural Inf. Process. Syst, P6000
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Yu MZ, 2022, IEEE T CIRC SYST VID, V32, P7559, DOI 10.1109/TCSVT.2022.3190273
   Zar Jerrold H, 2005, ENCY BIOSTATISTICS, V7, DOI DOI 10.1002/0470011815.B2A15150
   Zhai GT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3457905
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang CF, 2022, IEEE T CIRC SYST VID, V32, P5011, DOI 10.1109/TCSVT.2022.3143321
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2012, IEEE IMAGE PROC, P1473, DOI 10.1109/ICIP.2012.6467149
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang L, 2010, IEEE IMAGE PROC, P321, DOI 10.1109/ICIP.2010.5649275
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 70
TC 4
Z9 4
U1 1
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 205
DI 10.1145/3597434
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200028
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, M
   Xu, JZ
   Zhang, L
   Li, JR
   Zhang, K
   Wang, SQ
   Ma, SW
AF Wang, Meng
   Xu, Jizheng
   Zhang, Li
   Li, Junru
   Zhang, Kai
   Wang, Shiqi
   Ma, Siwei
TI Compressed Screen Content Image Super Resolution
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Screen content; super resolution; deep learning; convolutional neural
   network; versatile video coding
ID QUALITY ASSESSMENT; CONVOLUTIONAL NETWORK; PREDICTION
AB Screen content has become one of the prominent mediums in the increasingly connected world. With the prevalence of remote collaboration and communication such as virtual conferences and online education, recent years have witnessed a dramatic increase in the data volume of the screen content. Screen content compression serves as the fundamental technology in fostering the storage, transmission, and exhibition of screen content. In this article, we target the super-resolution of the compressed screen content images, intending to tackle the real-world challenge problems. A dataset is proposed for the super-resolution of the screen contents contaminated with different compression distortion levels. Subsequently, we introduce the principle of the multi-hypothesis into the super resolution and propose a new paradigm for the restoration of the compressed screen content images. The luminance and sharpness similarity metric is adopted in the network learning to better adapt to the screen content characteristic and ensure perceptual fidelity. Experimental results verify the superiority and effectiveness of the proposed method.
C1 [Wang, Meng; Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Xu, Jizheng; Zhang, Li; Zhang, Kai] Bytedance Inc, San Diego, CA 94041 USA.
   [Li, Junru] Douyin Grp HK Ltd, Hong Kong, Peoples R China.
   [Ma, Siwei] Peking Univ, Inst Digital Media, Beijing, Peoples R China.
C3 City University of Hong Kong; Peking University
RP Zhang, L (corresponding author), Bytedance Inc, San Diego, CA 94041 USA.
EM mwang98-c@my.cityu.edu.hk; xujizheng@bytedance.com;
   lizhang.idm@bytedance.com; lijunru@bytedance.com;
   zhangkai.video@bytedance.com; shiqwang@cityu.edu.hk; swma@pku.edu.cn
RI Xu, Jizheng/JDD-5152-2023; LI, Junru/AFO-4681-2022
OI Zhang, Li/0000-0002-3463-9211; Li, Junru/0000-0001-7603-8599; WANG,
   Meng/0000-0002-5655-1464
FU National Natural Science Foundation of China [62022002]; Hong Kong
   GRF-RGC General Research Fund [11203220 (9042957)];  [PRP/059/20FX]
FX This work was supported in part by the National Natural Science
   Foundation of China under 62022002, in part by the Hong Kong GRF-RGC
   General Research Fund under Grant 11203220 (9042957), and in part by the
   PRP/059/20FX.
CR Abdoli M, 2019, IEEE IMAGE PROC, P3162, DOI [10.1109/icip.2019.8803389, 10.1109/ICIP.2019.8803389]
   [Anonymous], 2020, VVCSoftware VTM Repository
   Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bjontegaard G., 2001, Document VCEG-M33
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Chang TS, 2020, IEEE I C VI COM I PR, P261, DOI 10.1109/vcip49819.2020.9301762
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Flierl M, 2002, IEEE T CIRC SYST VID, V12, P957, DOI 10.1109/TCSVT.2002.805490
   Gao W., 2014, Advanced Video Coding Systems, P35
   Girod B, 2000, IEEE T IMAGE PROCESS, V9, P173, DOI 10.1109/83.821595
   Gu K, 2018, IEEE T VIS COMPUT GR, V24, P2689, DOI 10.1109/TVCG.2017.2771284
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim M.-J., 2013, Proceedings of the 2013 Research in Adaptive and Convergent Systems, P192, DOI DOI 10.1145/2513228.2513257
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li XG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282445
   Li Y., 2022, ACM T MULTIM COMPUT
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P1092, DOI 10.1109/TIP.2018.2872876
   Li Y, 2018, IEEE T CIRC SYST VID, V28, P2316, DOI 10.1109/TCSVT.2017.2727682
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu Y., 2022, ACM T MULTIM COMPUT, DOI [10.1145/3532864JustAccepted, DOI 10.1145/3532864JUSTACCEPTED]
   Ma H., 2022, ACM T MULTIM COMPUT, V18, P1
   Ma HC, 2020, IEEE I C VI COM I PR, P403, DOI 10.1109/vcip49819.2020.9301805
   Nguyen T, 2020, IEEE DATA COMPR CONF, P83, DOI 10.1109/DCC47342.2020.00016
   Ni ZK, 2017, IEEE T IMAGE PROCESS, V26, P4818, DOI 10.1109/TIP.2017.2718185
   Ni ZK, 2016, IEEE SIGNAL PROC LET, V23, P1394, DOI 10.1109/LSP.2016.2599294
   Paszke A., 2017, NIPS W
   Pu W, 2016, IEEE J EM SEL TOP C, V6, P420, DOI 10.1109/JETCAS.2016.2605661
   Qiu YJ, 2019, IEEE I CONF COMP VIS, P4179, DOI 10.1109/ICCV.2019.00428
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Nguyen T, 2021, IEEE T CIRC SYST VID, V31, P3801, DOI 10.1109/TCSVT.2021.3074312
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wang M, 2021, PROC SPIE, V11842, DOI 10.1117/12.2597132
   Wang M, 2021, IEEE DATA COMPR CONF, P173, DOI 10.1109/DCC50243.2021.00025
   Wang RF, 2019, IEEE ACCESS, V7, P5285, DOI 10.1109/ACCESS.2018.2889992
   Wang SQ, 2018, IEEE COMPUT GRAPH, V38, P47, DOI 10.1109/MCG.2016.46
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Xu XZ, 2016, IEEE J EM SEL TOP C, V6, P409, DOI 10.1109/JETCAS.2016.2597645
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yang JQ, 2008, ITESS: 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES, PT 1, P1, DOI 10.1109/CVPR.2008.4587647
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang Q, 2020, IEEE T BROADCAST, V66, P310, DOI 10.1109/TBC.2019.2954063
   Yang WH, 2017, IEEE T IMAGE PROCESS, V26, P5895, DOI 10.1109/TIP.2017.2750403
   Zhang DY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3398685
   Zhang J, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954503
   Zhang L, 2016, IEEE J EM SEL TOP C, V6, P446, DOI 10.1109/JETCAS.2016.2599860
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3150, DOI 10.1109/TIP.2018.2812081
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 61
TC 2
Z9 2
U1 2
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 209
DI 10.1145/3589963
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200032
DA 2024-07-18
ER

PT J
AU Wang, TY
   Cheng, H
   Chow, KP
   Nie, LQ
AF Wang, Tianyi
   Cheng, Harry
   Chow, Kam Pui
   Nie, Liqiang
TI Deep Convolutional Pooling Transformer for Deepfake Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deepfake detection; image keyframes; transformer
ID FACE MANIPULATION
AB Recently, Deepfake has drawn considerable public attention due to security and privacy concerns in social media digital forensics. As the wildly spreading Deepfake videos on the Internet become more realistic, traditional detection techniques have failed in distinguishing between real and fake. Most existing deep learning methods mainly focus on local features and relations within the face image using convolutional neural networks as a backbone. However, local features and relations are insufficient for model training to learn enough general information for Deepfake detection. Therefore, the existing Deepfake detection methods have reached a bottleneck to further improve the detection performance. To address this issue, we propose a deep convolutional Transformer to incorporate the decisive image features both locally and globally. Specifically, we apply convolutional pooling and re-attention to enrich the extracted features and enhance efficacy. Moreover, we employ the barely discussed image keyframes in model training for performance improvement and visualize the feature quantity gap between the key and normal image frames caused by video compression. We finally illustrate the transferability with extensive experiments on several Deepfake benchmark datasets. The proposed solution consistently outperforms several state-of-the-art baselines on both within- and cross-dataset experiments.
C1 [Wang, Tianyi; Chow, Kam Pui] Univ Hong Kong, Pok Fu Lam, Hong Kong 999077, Peoples R China.
   [Cheng, Harry] Shandong Univ, 9M4R 493, Qingdao 266237, Shandong, Peoples R China.
   [Nie, Liqiang] Harbin Inst Technol Shenzhen, Taoyuan St, Shenzhen 518055, Guangdong, Peoples R China.
C3 University of Hong Kong; Shandong University; Harbin Institute of
   Technology
RP Wang, TY (corresponding author), Univ Hong Kong, Pok Fu Lam, Hong Kong 999077, Peoples R China.
EM tywang@cs.hku.hk; xaCheng1996@gmail.com; chow@cs.hku.hk;
   nieliqiang@gmail.com
RI Chow, Kam Pui/C-1828-2009; Wang, Tianyi/AAY-5977-2021
OI Wang, Tianyi/0000-0003-2920-6099; Cheng, Harry/0000-0001-7436-0162
CR Afchar D., 2018, 2018 IEEE INT WORKSH, P1, DOI [DOI 10.1109/WIFS.2018.8630761, 10.1109/WIFS.2018.8630761]
   Amerini I, 2019, IEEE INT CONF COMP V, P1205, DOI 10.1109/ICCVW.2019.00152
   Apple, 2021, MPEG 2 REF INF
   Bonettini N, 2021, INT C PATT RECOG, P5012, DOI 10.1109/ICPR48806.2021.9412711
   BuzzFeedVideo, 2018, YouTube18 April
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Coccomini DA, 2022, Arxiv, DOI [arXiv:2211.10996, 10.48550/ARXIV.2211.109963, DOI 10.48550/ARXIV.2211.109963]
   Coccomini DA, 2022, LECT NOTES COMPUT SC, V13233, P219, DOI 10.1007/978-3-031-06433-3_19
   Dang H, 2020, PROC CVPR IEEE, P5780, DOI 10.1109/CVPR42600.2020.00582
   deepfakes, 2019, DEEPF
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Dosovitskiy Alexey, 2021, ICLR
   Dutta T, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3002178
   FFmpeg, 2021, FFmpeg
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Greaves Mark, 2021, DEEPFAKES RANKED MOS
   Nguyen HH, 2019, Arxiv, DOI arXiv:1910.12467
   Hasan HR, 2019, IEEE ACCESS, V7, P41596, DOI 10.1109/ACCESS.2019.2905689
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YA, 2021, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR46437.2021.00434
   Heo B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11916, DOI 10.1109/ICCV48922.2021.01172
   Jafar MT, 2020, INT CONF INFORM COMM, P053, DOI 10.1109/ICICS49469.2020.239493
   Jiang LM, 2020, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR42600.2020.00296
   Kelion Leo, 2018, DEEPFAKE PORN VIDEOS
   Kemelmacher-Shlizerman I, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925871
   Khan SA, 2022, 19TH INTERNATIONAL CONFERENCE ON CONTENT-BASED MULTIMEDIA INDEXING, CBMI 2022, P8, DOI 10.1145/3549555.3549588
   Khodabakhshi N, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422963
   Kietzmann J, 2020, BUS HORIZONS, V63, P135, DOI 10.1016/j.bushor.2019.11.006
   King Davis, 2021, DLIB 19 22 1
   Kingma D. P., 2014, AUTOENCODING VARIATI, P3581
   Kwon Patrick, 2021, P IEEE CVF INT C COM, P10744
   Li LZ, 2020, Arxiv, DOI arXiv:1912.13457
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo YC, 2021, PROC CVPR IEEE, P16312, DOI 10.1109/CVPR46437.2021.01605
   Melville K., 2019, ABC News
   Natsume R., 2018, ARXIV180403447, P1, DOI [DOI 10.1145/3230744.3230818, 10.1145/3230744.3230818]
   Norman Thomas, 2014, P INTEGRATED SECURIT, V2nd, P203
   Perov Ivan, 2021, ARXIV
   Ravi H, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2857069
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun Ke, 2022, P AAAI C ARTIFICIAL
   Tan MX, 2019, PR MACH LEARN RES, V97
   TechBytes, 2021, US
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Toews Rob, 2020, WE ARE NOT PREPARED
   Tolosana R, 2020, INFORM FUSION, V64, P131, DOI 10.1016/j.inffus.2020.06.014
   Vaswani A, 2017, ADV NEUR IN, V30
   Vijayanagar Krishna Rao, 2020, I P B FRAMES DIFFERE
   Wang Q., 2021, arXiv
   Wang Y., 2021, P 30 INT JOINT C ART, VVolume 8, P1136, DOI 10.24963/ijcai.2021/157
   Wen LY, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183518
   Westerlund M, 2019, TECHNOL INNOV MANAG, V9, P39, DOI 10.22215/timreview/1282
   Wodajo D, 2021, Arxiv, DOI arXiv:2102.11126
   Yao T, 2022, LECT NOTES COMPUT SC, V13685, P328, DOI 10.1007/978-3-031-19806-9_19
   Yao T, 2022, Arxiv, DOI arXiv:2207.04976
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509
   Zhao J, 2017, ADV NEUR IN, V30
   Zhao J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1184
   Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235
   Zhou DQ, 2021, Arxiv, DOI [arXiv:2103.11886, 10.48550/arXiv.2103.11886, DOI 10.48550/ARXIV.2103.11886]
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
   Zi BJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2382, DOI 10.1145/3394171.3413769
NR 69
TC 6
Z9 6
U1 21
U2 40
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 179
DI 10.1145/3588574
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yuan, Y
   Zhang, JW
AF Yuan, Ye
   Zhang, Jiawan
TI Shot Boundary Detection Using Color Clustering and Attention Mechanism
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Shot boundary detection; color clustering; attention mechanism
ID TEMPORAL VIDEO SEGMENTATION; SCENE DETECTION; HISTOGRAM; FEATURES;
   VECTOR
AB Shot boundary detection (SBD) is widely used in scene segmentation, semantic analysis, and video retrieval. However, existing SBD algorithms have certain applications in video processing, but they have the following three problems. First, these algorithms cannot effectively handle shot boundaries caused by sudden lighting changes. Second, when there are dimly lighting frames in the video, these algorithms cannot perform boundary detection well. Third, when there is object or camera motion in the video, these algorithms also fail to work. To resolve these issues, we propose an SBD algorithm with color clustering changes in small regions (CCSR) to detect the shot transitions, which are abrupt changes and gradual transitions (dissolve and fade). The main idea behind the CCSR algorithm is to compute the distance of color features and to preserve the spatio-temporal information as much as possible. This model has relatively less dependence on the threshold parameters and sliding windows. Unlike other SBD algorithms, the clustering results of CCSR weaken factors such as object motion and illumination changes between adjacent frames in the video, which is helpful for reducing false detections. Furthermore, we utilize an attention mechanism in the gradual transitions to improve detection efficiency and accuracy. Finally, we evaluated the SBD algorithm, which was tested on a standard TRECVID dataset. The experimental results suggest that our algorithm yields significant improvements in precision and recall compared to the current techniques, with an average improvement of 10.35% and 8.85%, respectively. Moreover, compared with state-of-the-art algorithms, the results prove that the proposed method improves the F-score by more than 2.64% and the computation time efficiency by over 10%.
C1 [Yuan, Ye; Zhang, Jiawan] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Zhang, JW (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM yuanye1989@tju.edu.cn; jwzhang@tju.edu.cn
OI Zhang, Jiawan/0000-0002-0667-6744
CR Abdulhussain SH, 2020, IEEE ACCESS, V8, P72347, DOI 10.1109/ACCESS.2020.2987870
   Abdulhussain SH, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20040214
   Adnan A., 2013, LIFE SCI J, V10, P1965
   Amiri A, 2011, COMPUT INFORM, V30, P595
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Apostolidis Evlampios, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6583, DOI 10.1109/ICASSP.2014.6854873
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bao BK, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2730889
   Baraldi L, 2015, LECT NOTES COMPUT SC, V9256, P801, DOI 10.1007/978-3-319-23192-1_67
   Barjatya A., 2004, IEEE T EVOLUTION COM, V8, P225, DOI DOI 10.1109/TEVC.2004.826069
   Bhaumik H, 2016, APPL SOFT COMPUT, V46, P1008, DOI 10.1016/j.asoc.2016.03.022
   Bi CK, 2018, IEEE ACCESS, V6, P21397, DOI 10.1109/ACCESS.2018.2825106
   Birinci M, 2014, SIGNAL PROCESS-IMAGE, V29, P410, DOI 10.1016/j.image.2013.12.003
   Chakraborty S, 2019, APPL INTELL, V49, P3207, DOI 10.1007/s10489-019-01444-1
   Chasanis VT, 2009, IEEE T MULTIMEDIA, V11, P89, DOI 10.1109/TMM.2008.2008924
   Chung MG, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P933, DOI 10.1109/ICIP.2000.899610
   Cooper Matthew, 2003, P TRECVID WORKSHOP
   Duan FF, 2020, IEEE ACCESS, V8, P214633, DOI 10.1109/ACCESS.2020.3040861
   Gonzalez-Diaz I, 2015, EXPERT SYST APPL, V42, P488, DOI 10.1016/j.eswa.2014.08.001
   Guo JT, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3131342
   Gygli M, 2018, INT WORK CONTENT MUL
   Han B, 2007, IEEE T CONSUM ELECTR, V53, P1168, DOI 10.1109/TCE.2007.4341601
   Hassanien A, 2017, Arxiv, DOI arXiv:1705.03281
   Heng WJ, 2001, J VIS COMMUN IMAGE R, V12, P217, DOI 10.1006/jvci.2001.0457
   Huang PL, 2022, IEEE-CAA J AUTOMATIC, V9, P339, DOI 10.1109/JAS.2021.1004210
   Idan ZN, 2021, IEEE ACCESS, V9, P106412, DOI 10.1109/ACCESS.2021.3100139
   Janwe NJ, 2013, 2013 IEEE SECOND INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P476, DOI 10.1109/ICIIP.2013.6707637
   Jiang XK, 2022, IEEE T MULTIMEDIA, V24, P3049, DOI 10.1109/TMM.2021.3092143
   Jung D, 2018, INT CONF ADV COMMUN, P36, DOI 10.23919/ICACT.2018.8323638
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Küçüktunç O, 2010, COMPUT VIS IMAGE UND, V114, P125, DOI 10.1016/j.cviu.2009.09.008
   Li ZJ, 2016, PROCEEDINGS 2016 FIRST INTERNATIONAL CONFERENCE ON MULTIMEDIA AND IMAGE PROCESSING (ICMIP 2016), P15, DOI 10.1109/ICMIP.2016.24
   Liang R, 2017, IEEE INT SYM MULTIM, P489, DOI 10.1109/ISM.2017.97
   Lienhart R, 1998, PROC SPIE, V3656, P290, DOI 10.1117/12.333848
   Liu HB, 2020, IEEE ACCESS, V8, P2472, DOI 10.1109/ACCESS.2019.2962328
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu ZM, 2013, IEEE T IMAGE PROCESS, V22, P5136, DOI 10.1109/TIP.2013.2282081
   NAGASAKA A, 1992, IFIP TRANS A, V7, P113
   Nam J, 1997, INT CONF ACOUST SPEE, P2665, DOI 10.1109/ICASSP.1997.595337
   Ngo CW, 2001, IEEE T CIRC SYST VID, V11, P941, DOI 10.1109/76.937435
   Nishani E, 2017, MEDD C EMBED COMPUT, P242
   Pal G, 2015, ADV INTELL SYST, V338, P119, DOI 10.1007/978-3-319-13731-5_14
   Porter S, 2003, IMAGE VISION COMPUT, V21, P1097, DOI 10.1016/j.imavis.2003.08.014
   Porter SV, 2000, INT C PATT RECOG, P409, DOI 10.1109/ICPR.2000.903571
   Priya GGL, 2014, IEEE T IMAGE PROCESS, V23, P5187, DOI 10.1109/TIP.2014.2362652
   Priya GGL, 2012, PROC TECH, V1, P247, DOI 10.1016/j.protcy.2012.10.030
   Qian SS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451215
   Qing-Ge Ji, 2010, Proceedings 2010 First International Conference on Pervasive Computing, Signal Processing and Applications (PCSPA 2010), P273, DOI 10.1109/PCSPA.2010.73
   Sheng-Hua Zhong, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3477538
   Simonyan K, 2014, Arxiv, DOI arXiv:1406.2199
   Smeaton AF, 2010, COMPUT VIS IMAGE UND, V114, P411, DOI 10.1016/j.cviu.2009.03.011
   Soucek T., 2020, arXiv
   Thounaojam D, 2019, INT ARAB J INF TECHN, V16, P686
   Tippaya S, 2017, IEEE ACCESS, V5, P12563, DOI 10.1109/ACCESS.2017.2717998
   Toet A, 2003, DISPLAYS, V24, P197, DOI 10.1016/j.displa.2004.01.006
   Tong WJ, 2015, IEEE INT SYM BROADB
   Vlachos T, 2000, IEEE SIGNAL PROC LET, V7, P173, DOI 10.1109/97.847360
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P85, DOI 10.1109/34.899949
   Wei Jyh Heng, 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P289, DOI 10.1109/ICIP.1999.817119
   Wu LF, 2019, IEEE ACCESS, V7, P77268, DOI 10.1109/ACCESS.2019.2922038
   Xu JW, 2016, 2016 30TH ANNIVERSARY OF VISUAL COMMUNICATION AND IMAGE PROCESSING (VCIP)
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Yang L, 2022, IEEE T PATTERN ANAL, V44, P9814, DOI 10.1109/TPAMI.2021.3132058
   Yang Zhe, 2018, P 3 INT C MULTIMEDIA, P71
   Zabih R, 1999, MULTIMEDIA SYST, V7, P119, DOI 10.1007/s005300050115
   Zhao WB, 2021, PROC CVPR IEEE, P16821, DOI 10.1109/CVPR46437.2021.01655
   Zhou SB, 2021, SIGNAL IMAGE VIDEO P, V15, P627, DOI 10.1007/s11760-020-01785-2
NR 67
TC 2
Z9 2
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 198
DI 10.1145/3595923
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200021
DA 2024-07-18
ER

PT J
AU Li, WX
   Cao, TT
   Liu, C
   Tian, X
   Li, Y
   Wang, XJ
   Dong, X
AF Li, Weixin
   Cao, Tiantian
   Liu, Chang
   Tian, Xue
   Li, Ya
   Wang, Xiaojie
   Dong, Xuan
TI Dual-Lens HDR using Guided 3D Exposure CNN and Guided Denoising
   Transformer
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Dual-lens HDR; disentangled; guided 3D exposure CNN; guided denoising
   transformer
ID CONVOLUTIONAL NETWORK; RECONSTRUCTION; MONOCHROME; COLORIZATION; IMAGES
AB We study the high dynamic range (HDR) imaging problem in dual-lens systems. Existing methods usually treat the HDR imaging problem as an image fusion problem and the HDR result is estimated by fusing the aligned short exposure image and long exposure image. However, the image fusion pipeline depends highly on the image alignment, which is difficult to be perfect. We propose to transfer the dual-lens HDR imaging problem into the disentangled enhancement of exposure correction and denoising for the short exposure image, guided by the long exposure image. In the guided exposure correction module, we make use of the guidance image and 3D color transformation to propose a guided 3D exposure CNN (GEC) to get the rough HDR result from the short exposure image. Then, in the guided denoising module, we make use of the cross-attention mechanism to propose a guided denoising transformer (GDT) to directly use the long exposure image as guidance to denoise the rough HDR result in a pyramid way. And in both modules, we bypass the difficult image alignment processing. Experimental results demonstrate the superiority of our method over the state-of-the-art ones.
C1 [Li, Weixin] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Software Dev Environm, 37 Xueyuanlu Rd, Beijing 100191, Peoples R China.
   [Li, Weixin] Zhongguancun Lab, 2 Cuihubei Rd, Beijing 100194, Peoples R China.
   [Cao, Tiantian; Liu, Chang; Tian, Xue; Li, Ya; Wang, Xiaojie; Dong, Xuan] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
C3 Beihang University; Zhongguancun Laboratory; Beijing University of Posts
   & Telecommunications
RP Dong, X (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
EM dongxuan8811@bupt.edu.cn
FU National Key Research and Development Plan of China [2021ZD0110503];
   National Nature Science Foundation of China [62276018, 61802026,
   61806016, 62076032]; CAAI-Huawei MindSpore Open Fund; BUPT innovation
   and entrepreneurship support program
FX This work is sponsored by the National Key Research and Development Plan
   of China (2021ZD0110503), the National Nature Science Foundation of
   China (No. 62276018, 61802026, 61806016 and 62076032), CAAI-Huawei
   MindSpore Open Fund and BUPT innovation and entrepreneurship support
   program.
CR Bätz M, 2014, SIGNAL PROCESS-IMAGE, V29, P191, DOI 10.1016/j.image.2013.08.016
   Cao G., 2022, ACM T MULTIM COMPUT
   Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696
   Chen GY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2482, DOI 10.1109/ICCV48922.2021.00250
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen XY, 2021, IEEE COMPUT SOC CONF, P354, DOI 10.1109/CVPRW53098.2021.00045
   Chen YY, 2020, IEEE T COMPUT IMAG, V6, P1044, DOI 10.1109/TCI.2020.3001398
   Chen YY, 2019, IEEE IMAGE PROC, P3502, DOI [10.1109/icip.2019.8803656, 10.1109/ICIP.2019.8803656]
   Cho H, 2014, COMPUT GRAPH FORUM, V33, P329, DOI 10.1111/cgf.12501
   Cogalan U, 2022, COMPUT GRAPH-UK, V105, P57, DOI 10.1016/j.cag.2022.04.008
   Çogalan U, 2020, IEEE T IMAGE PROCESS, V29, P7511, DOI 10.1109/TIP.2020.3004014
   Trinidad MC, 2019, IEEE I CONF COMP VIS, P4100, DOI 10.1109/ICCV.2019.00420
   Debevec PaulE., 1997, RECOVERING HIGH DYNA
   Dong X., 2019, AAAI C ARTIFICIAL IN
   Dong X, 2022, IEEE T IMAGE PROCESS, V31, P6747, DOI 10.1109/TIP.2022.3215910
   Dong X, 2021, AAAI CONF ARTIF INTE, V35, P1264
   Dong X, 2020, AAAI CONF ARTIF INTE, V34, P10721
   Dong X, 2021, IEEE T IMAGE PROCESS, V30, P6609, DOI 10.1109/TIP.2021.3096385
   Dong X, 2021, NEUROCOMPUTING, V450, P129, DOI 10.1016/j.neucom.2021.04.014
   Dong X, 2022, IEEE T VIS COMPUT GR, V28, P1469, DOI 10.1109/TVCG.2020.3022480
   Dong X, 2011, IEEE INT CON MULTI
   Dong X, 2019, NEUROCOMPUTING, V352, P22, DOI 10.1016/j.neucom.2019.04.007
   Dong X, 2015, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2015.7298671
   Du ZY, 2021, IEEE T AFFECT COMPUT, V12, P565, DOI 10.1109/TAFFC.2019.2940224
   Hao Shijie, 2022, ACMTRANSACTIONS MULT
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hu W, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3199514
   Hu XJ, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2886777
   Huang X, 2022, PROC CVPR IEEE, P18377, DOI 10.1109/CVPR52688.2022.01785
   Jeon DS, 2018, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2018.00185
   Joo J, 2014, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2014.35
   Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609
   Kim H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4439, DOI 10.1109/ICCV48922.2021.00442
   Kumar M, 2021, Arxiv, DOI [arXiv:2102.04432, 10.48550/arXiv.2102.04432]
   Li B, 2018, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2018.00682
   Li H., 2020, TIP
   Li Rongjie, 2021, CVPR
   Li WX, 2017, IEEE T MULTIMEDIA, V19, P367, DOI 10.1109/TMM.2016.2616279
   Li WB, 2022, IEEE T COMPUT SOC SY, V9, P667, DOI 10.1109/TCSS.2021.3127935
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin HY, 2009, IEEE IMAGE PROC, P4305, DOI 10.1109/ICIP.2009.5413665
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2021, IEEE COMPUT SOC CONF, P463, DOI 10.1109/CVPRW53098.2021.00057
   Ma K., 2020, TIP
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Martel JNP, 2020, IEEE T PATTERN ANAL, V42, P1642, DOI 10.1109/TPAMI.2020.2986944
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Mildenhall B, 2022, PROC CVPR IEEE, P16169, DOI 10.1109/CVPR52688.2022.01571
   Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265
   Niu YZ, 2021, IEEE T IMAGE PROCESS, V30, P3885, DOI 10.1109/TIP.2021.3064433
   Pan LY, 2017, PROC CVPR IEEE, P6987, DOI 10.1109/CVPR.2017.739
   Park WJ, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17071473
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Prabhakar K. R., 2019, 2019 IEEE INT C COMP, P1
   Prabhakar KR, 2021, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR46437.2021.00484
   Santos MS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392403
   ShuchenWeng Jimeng Sun, CT2 COLORIZATION TRA
   Sun N, 2010, IEEE IMAGE PROC, P2973, DOI 10.1109/ICIP.2010.5653371
   Wang J, 2019, IEEE INT CONF COMPUT, DOI 10.1109/iccphot.2019.8747337
   Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032
   Wang X., 2021, IEEE T AFFECT COMPUT
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XP, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355397
   Wang XP, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P283, DOI 10.1145/3206025.3206068
   Wang YH, 2012, IEEE T SYST MAN CY B, V42, P1107, DOI 10.1109/TSMCB.2012.2187051
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen LY, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183518
   Wu SZ, 2018, LECT NOTES COMPUT SC, V11206, P120, DOI 10.1007/978-3-030-01216-8_8
   Xiao Dong, 2015, 2015 IEEE International Conference on Microwaves, Communications, Antennas and Electronic Systems (COMCAS), P1, DOI 10.1109/COMCAS.2015.7360459
   Xiong Pengfei, 2021, ACM MM
   Xu K., 2022, ACM T MULTIM COMPUT
   Xu X., 2022, IEEECVF C COMPUT VIS, P17714
   Yan Chenggang, 2020, ACMTRANSACTIONS ONMU
   Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Yuan L, 2012, LECT NOTES COMPUT SC, V7575, P771, DOI 10.1007/978-3-642-33765-9_55
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P2058, DOI 10.1109/TPAMI.2020.3026740
   Zhai GT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3457905
   Zheng Y., 2019, IEEE COMPUTER VISION
   Zhou SC, 2019, PROC CVPR IEEE, P10988, DOI 10.1109/CVPR.2019.01125
NR 83
TC 2
Z9 2
U1 3
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 158
DI 10.1145/3579167
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300007
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, J
   Ling, Q
   Li, PY
AF Wang, Jian
   Ling, Qiang
   Li, Peiyan
TI Robust Video Stabilization based on Motion Decomposition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video stabilization; motion decomposition; DBSCAN; feature point
   clustering
AB Video stabilization aims to eliminate camera jitter and improve the visual experience of shaky videos. Video stabilizationmethods often ignore the active movement of the foreground objects and the camera, and may result in distortion and over-smoothing problems. To resolve these issues, this paper proposes a novel video stabilization method based on motion decomposition. Since the inter-frame movement of foreground objects is different from that of the background, we separate foreground feature points from background feature points by modifying the classic density based spatial clustering method of applications with noise (DBSCAN). The movement of background feature points is consistent with the movement of the camera, which can be decomposed into the camera jitter and the activemovement of the camera. And themovement of foreground feature points can be decomposed into the movement of the camera and the active movement of foreground objects. Based on motion decomposition, we design first-order and second-order trajectory smoothing constraints to eliminate the high-frequency and low-frequency components of the camera jitter. To reduce content distortion, shape-preserving constraints, and regularization constraints are taken to generate stabilized views of all feature points. Experimental results demonstrate the effectiveness and robustness of the proposed video stabilization method on a variety of challenging videos.
C1 [Wang, Jian; Ling, Qiang] Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
   [Li, Peiyan] Columbia Univ, New York, NY USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Columbia University
RP Ling, Q (corresponding author), Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
EM wj085@mail.ustc.edu.cn; qling@ustc.edu.cn; pl2801@columbia.edu
RI Li, Peiyan/KUD-6210-2024
OI Ling, Qiang/0000-0001-5688-4130; Jian, Wang/0000-0001-9741-2252
FU Key Science and Technology Program of Anhui [202203a05020012,
   202203f07020002]; Provincial Quality Program of High Education Schools
   of Anhui Province [2021jyxm1743]; Applied Science and Technology
   Achievement Cultivation Project of Institute of Advanced Technology,
   University of Science and Technology of China
FX This work was supported in part by the Key Science and Technology
   Program of Anhui under Grants 202203a05020012 and 202203f07020002, in
   part by Provincial Quality Program of High Education Schools of Anhui
   Province under Grant 2021jyxm1743, and in part by the Applied Science
   and Technology Achievement Cultivation Project of Institute of Advanced
   Technology, University of Science and Technology of China.
CR Buehler C, 2001, PROC CVPR IEEE, P609
   Chang HC, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P29, DOI 10.1109/ICME.2004.1394117
   Chang HC, 2006, J VIS COMMUN IMAGE R, V17, P659, DOI 10.1016/j.jvcir.2005.10.004
   Choi J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3363550
   Chu CH, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808211
   Dong J, 2017, IEEE T CIRC SYST VID, V27, P716, DOI 10.1109/TCSVT.2016.2589860
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gleicher ML, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1404880.1404882
   Goldstein A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231824
   Grundmann M, 2013, IEEE INT CONF COMPUT
   Grundmann M, 2011, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2011.5995525
   Hartley R., 2003, P CAMBRIDGE U PRESS
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Koh YJ, 2015, IEEE T IMAGE PROCESS, V24, P5260, DOI 10.1109/TIP.2015.2479918
   LEE DT, 1980, INT J COMPUT INF SCI, V9, P219, DOI 10.1007/BF00977785
   Lee KY, 2009, IEEE I CONF COMP VIS, P1397
   Lee SH, 2014, IEEE IMAGE PROC, P1120, DOI 10.1109/ICIP.2014.7025223
   Ling Qiang, 2016, P IEEE T CIRCUITS SY, P561
   Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Liu SC, 2017, IEEE T CIRC SYST VID, V27, P1922, DOI 10.1109/TCSVT.2016.2556587
   Liu SC, 2016, LECT NOTES COMPUT SC, V9910, P800, DOI 10.1007/978-3-319-46466-4_48
   Liu SC, 2014, PROC CVPR IEEE, P4209, DOI 10.1109/CVPR.2014.536
   Liu S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461995
   Liu SC, 2012, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2012.6247662
   Liu YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2279, DOI 10.1109/ICCV48922.2021.00230
   Ma TZ, 2020, IEEE T VIS COMPUT GR, V26, P3163, DOI 10.1109/TVCG.2019.2923196
   Matsushita Y, 2006, IEEE T PATTERN ANAL, V28, P1150, DOI 10.1109/TPAMI.2006.141
   Meng QL, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3350840
   Morimoto C., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P284, DOI 10.1109/ICPR.1996.546956
   Nie YW, 2018, IEEE T IMAGE PROCESS, V27, P164, DOI 10.1109/TIP.2017.2736603
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Smith BM, 2009, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2009.5459270
   Tomasi C, 1991, DETECTION TRACKING P
   Wang M, 2019, IEEE T IMAGE PROCESS, V28, P2283, DOI 10.1109/TIP.2018.2884280
   Wang YS, 2013, IEEE T VIS COMPUT GR, V19, P1354, DOI 10.1109/TVCG.2013.11
   Wang ZQ, 2013, COMPUT GRAPH FORUM, V32, P265, DOI 10.1111/cgf.12234
   Wu HC, 2021, IEEE T IMAGE PROCESS, V30, P4637, DOI 10.1109/TIP.2021.3073865
   Wu HC, 2019, IEEE T CIRC SYST VID, V29, P2873, DOI 10.1109/TCSVT.2018.2875671
   Wu Jingjing, 2022, ACM T MULTIM COMPUT, V18, P1
   Xu YF, 2022, IEEE T IMAGE PROCESS, V31, P4306, DOI 10.1109/TIP.2022.3182887
   Yang JL, 2009, IEEE T CIRC SYST VID, V19, P945, DOI 10.1109/TCSVT.2009.2020252
   Yu J., 2020, P IEEE CVF C COMP VI, P8159
   Zhang FL, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2493959
   Zhang GF, 2009, VISUAL COMPUT, V25, P997, DOI 10.1007/s00371-009-0310-z
   Zhang Lei, 2015, P IEEE T CIRCUITS SY, V27, P225
   Zhao MD, 2021, IEEE T CIRC SYST VID, V31, P3504, DOI 10.1109/TCSVT.2020.3040753
   Zhao MD, 2020, IEEE T IMAGE PROCESS, V29, P3582, DOI 10.1109/TIP.2019.2963380
   Zhou ZH, 2013, PROC CVPR IEEE, P2299, DOI 10.1109/CVPR.2013.298
NR 51
TC 0
Z9 0
U1 7
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 164
DI 10.1145/3580498
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300013
OA Bronze
DA 2024-07-18
ER

PT J
AU Lan, XH
   Yuan, YT
   Wang, X
   Wang, Z
   Zhu, WW
AF Lan, Xiaohan
   Yuan, Yitian
   Wang, Xin
   Wang, Zhi
   Zhu, Wenwu
TI A Survey on Temporal Sentence Grounding in Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video understanding; multi-modality; vision and language; cross-modal
   video retrieval
AB Temporal sentence grounding in videos (TSGV), which aims at localizing one target segment from an untrimmed video with respect to a given sentence query, has drawn increasing attentions in the research community over the past few years. Different from the task of temporal action localization, TSGV is more flexible since it can locate complicated activities via natural languages, without restrictions from predefined action categories. Meanwhile, TSGV is more challenging since it requires both textual and visual understanding for semantic alignment between two modalities (i.e., text and video). In this survey, we give a comprehensive overview for TSGV, which (i) summarizes the taxonomy of existing methods, (ii) provides a detailed description of the evaluation protocols (i.e., datasets and metrics) to be used in TSGV, and (iii) in-depth discusses potential problems of current benchmarking designs and research directions for further investigations. To the best of our knowledge, this is the first systematic survey on temporal sentence grounding. More specifically, we first discuss existing TSGV approaches by grouping them into four categories, i.e., two-stage methods, single-stage methods, reinforcement learning-based methods, and weakly supervised methods. Then we present the benchmark datasets and evaluation metrics to assess current research progress. Finally, we discuss some limitations in TSGV through pointing out potential problems improperly resolved in the current evaluation protocols, which may push forwards more cutting-edge research in TSGV. Besides, we also share our insights on several promising directions, including four typical tasks with new and practical settings based on TSGV.
C1 [Lan, Xiaohan; Wang, Zhi] Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Yuan, Yitian] Meituan, 4 Wangjing East St, Beijing 100020, Peoples R China.
   [Wang, Xin; Zhu, Wenwu] Tsinghua Univ, 30 Shuangqing Rd, Beijing 100084, Peoples R China.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University
RP Wang, X; Zhu, WW (corresponding author), Tsinghua Univ, 30 Shuangqing Rd, Beijing 100084, Peoples R China.
EM lanxh20@mails.tsinghua.edu.cn; yuanyitian@foxmail.com;
   xin_wang@tsinghua.edu.cn; wangzhi@sz.tsinghua.edu.cn;
   wwzhu@tsinghua.edu.cn
RI Wang, Zhi/GZB-2713-2022; Wang, Chen/JZE-6385-2024; Chen,
   Nuo/JZD-0344-2024; jing, wang/KCZ-2144-2024; Wang, Jiachen/KFT-0161-2024
OI Wang, Zhi/0000-0001-6952-8848; Wang, Xin/0000-0002-0351-2939; Wang,
   Zhi/0000-0002-5462-6178; Lan, Xiaohan/0000-0001-5382-6699
FU National Key Research and Development Program of China [2020AAA0106300];
   National Natural Science Foundation of China [62102222]
FX This work is supported by National Key Research and Development Program
   of China under grant No. 2020AAA0106300 and National Natural Science
   Foundation of China No. 62102222.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2002, P 10 ACM INT C MULT
   [Anonymous], 2017, P BRIT MACH VIS C
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bao PJ, 2021, AAAI CONF ARTIF INTE, V35, P920
   Stroud JC, 2019, Arxiv, DOI arXiv:1912.02256
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P898, DOI 10.1145/3394171.3413841
   Cao D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4162, DOI 10.1145/3394171.3413840
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chen DQ, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1870, DOI 10.18653/v1/P17-1171
   Chen JY, 2019, AAAI CONF ARTIF INTE, P8175
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen SX, 2021, PROC CVPR IEEE, P8421, DOI 10.1109/CVPR46437.2021.00832
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chen ZF, 2020, Arxiv, DOI arXiv:2001.09308
   Chen ZF, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1884
   Cheng Chen, 2020, Advances in Neural Networks - ISNN 2020. 17th International Symposium on Neural Networks, ISNN 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12557), P119, DOI 10.1007/978-3-030-64221-1_11
   Cheng H, 2000, IEEE T SIGNAL PROCES, V48, P2439, DOI 10.1109/78.852023
   Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981
   Cui YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P797, DOI 10.1145/3474085.3475251
   Ding XP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11553, DOI 10.1109/ICCV48922.2021.01137
   Duan X, 2018, ADV NEUR IN, V31
   Escorcia V, 2022, Arxiv, DOI arXiv:1907.12763
   Fan CY, 2019, PROC CVPR IEEE, P1999, DOI 10.1109/CVPR.2019.00210
   Gao J., 2021, P IEEECVF INT C COMP, P1523
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao MF, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1481
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Ghosh S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1984
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hahn Meera, 2020, P 31 BRIT MACHINE VI
   He DL, 2019, AAAI CONF ARTIF INTE, P8393
   Hendricks LA, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1380
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hori C, 2019, INT CONF ACOUST SPEE, P2352, DOI 10.1109/ICASSP.2019.8682583
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Huang Jiabo, 2021, P IEEE CVF INT C COM, P7199
   Jiang B, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P217, DOI 10.1145/3323873.3325019
   Jiao YF, 2018, IEEE T MULTIMEDIA, V20, P2693, DOI 10.1109/TMM.2018.2815998
   Jie Lei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P447, DOI 10.1007/978-3-030-58589-1_27
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Karaman S., 2014, P ECCV THUMOS WORKSH
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lei J, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1369
   Li XP, 2019, AAAI CONF ARTIF INTE, P8658
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Liu BB, 2018, LECT NOTES COMPUT SC, V11207, P569, DOI 10.1007/978-3-030-01219-9_34
   Liu DZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4070, DOI 10.1145/3394171.3414026
   Liu DZ, 2021, PROC CVPR IEEE, P11230, DOI 10.1109/CVPR46437.2021.01108
   Liu Daizong, 2020, P 28 INT C COMP LING, P1841
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu W., 2019, Neural Inf. Process. Syst., P536
   Liu XF, 2021, Arxiv, DOI arXiv:2104.00234
   Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144
   Lu JS, 2019, ADV NEUR IN, V32
   Luo JJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5600, DOI 10.1145/3474085.3475703
   Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214
   Mahasseni B, 2017, PROC CVPR IEEE, P2982, DOI 10.1109/CVPR.2017.318
   Minuk Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P156, DOI 10.1007/978-3-030-58604-1_10
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Otani Mayu, 2020, P 31 BRIT MACHINE VI
   Pan YW, 2020, Arxiv, DOI arXiv:2007.02375
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Qu XY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4280, DOI 10.1145/3394171.3414053
   Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Rodriguez-Opazo C, 2021, IEEE WINT CONF APPL, P1078, DOI 10.1109/WACV48630.2021.00112
   Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI [10.1109/WACV45572.2020.9093328, 10.1109/wacv45572.2020.9093328]
   Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11
   Sadhu Arka, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10414, DOI 10.1109/CVPR42600.2020.01043
   Shao D, 2018, LECT NOTES COMPUT SC, V11213, P202, DOI 10.1007/978-3-030-01240-3_13
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P601, DOI 10.1007/978-3-030-58565-5_36
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Sharghi A, 2017, PROC CVPR IEEE, P2127, DOI 10.1109/CVPR.2017.229
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Soldan Mattia, 2021, P IEEE CVF INT C COM, P3224
   Song XM, 2018, LECT NOTES COMPUT SC, V11165, P340, DOI 10.1007/978-3-030-00767-6_32
   Song YJ, 2020, Arxiv, DOI arXiv:2003.07048
   Su R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1513, DOI 10.1109/ICCV48922.2021.00156
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tan RB, 2021, IEEE WINT CONF APPL, P2082, DOI 10.1109/WACV48630.2021.00213
   Tang Zongheng, 2021, IEEE Transactions on Circuits and Systems for Video Technology
   Tellex Stefanie., 2009, P ACM INT C IM VID R, P38
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2021, PROC CVPR IEEE, P7022, DOI 10.1109/CVPR46437.2021.00695
   Wang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4116, DOI 10.1145/3394171.3413975
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Wang L, 2014, IEEE INT CONF VLSI
   Wang LW, 2021, PROC CVPR IEEE, P14085, DOI 10.1109/CVPR46437.2021.01387
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wang YC, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2021, P89
   Wu AM, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1029
   Wu J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1283, DOI 10.1145/3394171.3413862
   Wu J, 2020, AAAI CONF ARTIF INTE, V34, P12386
   Xiao SN, 2021, AAAI CONF ARTIF INTE, V35, P2986
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu MM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7200, DOI 10.1109/ICCV48922.2021.00713
   Xu YC, 2019, NEUROCOMPUTING, V357, P24, DOI 10.1016/j.neucom.2019.05.027
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu XL, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1860, DOI 10.1145/3404835.3463021
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yuan Yitian, 2021, HUMA'21: Proceedings of the 2nd International Workshop on Human-centric Multimedia Analysis, P13, DOI 10.1145/3475723.3484247
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Yuan YT, 2019, IEEE T CIRC SYST VID, V29, P226, DOI 10.1109/TCSVT.2017.2771247
   Yulan Yang, 2020, 2020 International Conference on Culture-oriented Science & Technology (ICCST), P596, DOI 10.1109/ICCST50977.2020.00123
   Zeng R., 2020, CVPR
   Zeng YW, 2021, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR46437.2021.00225
   Zhang BW, 2020, Arxiv, DOI arXiv:2011.09046
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang Hao, 2020, P 58 ANN M ASS COMPU, P6543
   Zhang Hao, 2021, P 44 INT ACM SIGIR C
   Zhang K, 2018, LECT NOTES COMPUT SC, V11212, P391, DOI 10.1007/978-3-030-01237-3_24
   Zhang LY, 2022, IEEE WINT CONF APPL, P2524, DOI 10.1109/WACV51458.2022.00258
   Zhang MX, 2021, PROC CVPR IEEE, P12664, DOI 10.1109/CVPR46437.2021.01248
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang SY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1230, DOI 10.1145/3343031.3350879
   Zhang Z., 2020, Advances in NIPS, P18123
   Zhang Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4098, DOI 10.1145/3394171.3413967
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhao B, 2018, PROC CVPR IEEE, P7405, DOI 10.1109/CVPR.2018.00773
   Zhao Y, 2021, PROC CVPR IEEE, P4195, DOI 10.1109/CVPR46437.2021.00418
   Zheng Wang, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P1459, DOI 10.1145/3474085.3475278
   Zhou H, 2021, PROC CVPR IEEE, P8441, DOI 10.1109/CVPR46437.2021.00834
   Zhu Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10665, DOI 10.1109/CVPR42600.2020.01068
NR 142
TC 5
Z9 5
U1 2
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 51
DI 10.1145/3532626
PG 33
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000001
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Rao, YB
   Yang, ZQ
   Zeng, SN
   Wang, QF
   Pu, JS
AF Rao, Yunbo
   Yang, Ziqiang
   Zeng, Shaoning
   Wang, Qifeng
   Pu, Jiansu
TI Dual Projective Zero-Shot Learning Using Text Descriptions
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Zero-shot learning; generalized zero-shot learning; autoencoder;
   inductive zero-shot learning
AB Zero-shot learning (ZSL) aims to recognize image instances of unseen classes solely based on the semantic descriptions of the unseen classes. In this field, Generalized Zero-Shot Learning (GZSL) is a challenging problem in which the images of both seen and unseen classes are mixed in the testing phase of learning. Existing methods formulate GZSL as a semantic-visual correspondence problem and apply generative models such as Generative Adversarial Networks and Variational Autoencoders to solve the problem. However, these methods suffer from the bias problem since the images of unseen classes are often misclassified into seen classes. In this work, a novel model named the Dual Projective model for Zero-Shot Learning (DPZSL) is proposed using text descriptions. In order to alleviate the bias problem, we leverage two autoencoders to project the visual and semantic features into a latent space and evaluate the embeddings by a visual-semantic correspondence loss function. An additional novel classifier is also introduced to ensure the discriminability of the embedded features. Our method focuses on a more challenging inductive ZSL setting in which only the labeled data from seen classes are used in the training phase. The experimental results, obtained from two popular datasets-Caltech-UCSD Birds-200-2011 (CUB) and North America Birds (NAB)-show that the proposed DPZSL model significantly outperforms both the inductive ZSL and GZSL settings. Particularly in the GZSL setting, our model yields an improvement up to 15.2% in comparison with state-of-the-art CANZSL on datasets CUB and NAB with two splittings.
C1 [Rao, Yunbo; Yang, Ziqiang] Univ Elect Sci & Technol China, Sch Informat & Software Engn, 4,Sect 2,North Jianshe Rd, Chengdu 610054, Sichuan, Peoples R China.
   [Zeng, Shaoning] Univ Elect Sci & Technol China, Yangtze Delta Reg Inst Huzhou, Chengdu 313000, Sichuan, Peoples R China.
   [Wang, Qifeng] Google Berkeley, Berkeley, CA 94720 USA.
   [Pu, Jiansu] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, 4,Sect 2,North Jianshe Rd, Chengdu 610054, Sichuan, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; University of Electronic
   Science & Technology of China
RP Zeng, SN (corresponding author), Univ Elect Sci & Technol China, Yangtze Delta Reg Inst Huzhou, Chengdu 313000, Sichuan, Peoples R China.
EM raoyb@uestc.edu.cn; comcxziqiang@gmail.com; zsn@outlook.com;
   qifei.wang@gmail.com; jiansu.pu@uestc.edu.cn
OI Rao, Yunbo/0000-0001-5433-7379
FU Science and Technology Project of Sichuan [2022ZHCG0033, 2021YFG0314,
   2020YFG0459]; National Natural Science Foundation of China [U19A2078]
FX This research was supported by the Science and Technology Project of
   Sichuan (grant nos. 2022ZHCG0033, 2021YFG0314, and 2020YFG0459) and the
   National Natural Science Foundation of China (grant no. U19A2078).
CR Akata Z, 2016, PROC CVPR IEEE, P59, DOI 10.1109/CVPR.2016.14
   Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Changpinyo S, 2020, INT J COMPUT VISION, V128, P166, DOI 10.1007/s11263-019-01193-1
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen Z, 2020, IEEE WINT CONF APPL, P863, DOI [10.1109/wacv45572.2020.9093610, 10.1109/WACV45572.2020.9093610]
   Chou Y.-Y., 2020, INT C LEARNING REPRE
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Elhoseiny M, 2019, IEEE I CONF COMP VIS, P5783, DOI 10.1109/ICCV.2019.00588
   Elhoseiny M, 2017, PROC CVPR IEEE, P6288, DOI 10.1109/CVPR.2017.666
   Elhoseiny M, 2017, IEEE T PATTERN ANAL, V39, P2539, DOI 10.1109/TPAMI.2016.2643667
   Frome Andrea, 2013, ADV NEURAL INF PROCE
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gu ZX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1921, DOI 10.1145/3394171.3413593
   Han XT, 2017, IEEE T MULTIMEDIA, V19, P1583, DOI 10.1109/TMM.2017.2671414
   Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089
   Kingma Diederik P., 2015, arXiv
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li JJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1587, DOI 10.1145/3343031.3350901
   Li K, 2019, IEEE I CONF COMP VIS, P3582, DOI 10.1109/ICCV.2019.00368
   Mancini M, 2021, PROC CVPR IEEE, P5218, DOI 10.1109/CVPR46437.2021.00518
   Mishra A, 2018, IEEE COMPUT SOC CONF, P2269, DOI 10.1109/CVPRW.2018.00294
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Pal A, 2019, PROC CVPR IEEE, P2184, DOI 10.1109/CVPR.2019.00229
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Qiao RZ, 2016, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2016.247
   Rahman S, 2020, IEEE T MULTIMEDIA, V22, P242, DOI 10.1109/TMM.2019.2924511
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Verma VK, 2020, AAAI CONF ARTIF INTE, V34, P6062
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang W, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3293318
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xu WJ, 2019, IEEE T MULTIMEDIA, V21, P2387, DOI 10.1109/TMM.2019.2898777
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Zhang L, 2020, IEEE T CIRC SYST VID, V30, P2843, DOI 10.1109/TCSVT.2020.2984666
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang ZM, 2016, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2016.649
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
NR 52
TC 4
Z9 4
U1 6
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 10
DI 10.1145/3514247
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400010
DA 2024-07-18
ER

PT J
AU Zhen, PN
   Wang, SQ
   Zhang, SM
   Yan, XT
   Wang, W
   Ji, ZG
   Chen, HB
AF Zhen, Peining
   Wang, Shuqi
   Zhang, Suming
   Yan, Xiaotao
   Wang, Wei
   Ji, Zhigang
   Chen, Hai-Bao
TI Towards Accurate Oriented Object Detection in Aerial Images with
   Adaptive Multi-level Feature Fusion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Remote sensing images; aerial images; oriented object detection;
   convolutional neural network
AB Detecting objects in aerial images is a long-standing and challenging problem since the objects in aerial images vary dramatically in size and orientation. Most existing neural network based methods are not robust enough to provide accurate oriented object detection results in aerial images since they do not consider the correlations between different levels and scales of features. In this paper, we propose a novel two-stage network-based detector with adaptive feature fusion towards highly accurate oriented object detection in aerial images, named AFF-Det. First, a multi-scale feature fusion module (MSFF) is built on the top layer of the extracted feature pyramids to mitigate the semantic information loss in the small-scale features. We also propose a cascaded oriented bounding box regression method to transform the horizontal proposals into oriented ones. Then the transformed proposals are assigned to all feature pyramid network (FPN) levels and aggregated by the weighted RoI feature aggregation (WRFA) module. The above modules can adaptively enhance the feature representations in different stages of the network based on the attention mechanism. Finally, a rotated decoupled-RCNN head is introduced to obtain the classification and localization results. Extensive experiments are conducted on the DOTA and HRSC2016 datasets to demonstrate the advantages of our proposed AFF-Det. The best detection results can achieve 80.73% mAP and 90.48% mAP, respectively, on these two datasets, outperforming recent state-of-the-art methods.
C1 [Zhen, Peining; Wang, Shuqi; Ji, Zhigang; Chen, Hai-Bao] Shanghai Jiao Tong Univ, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Zhang, Suming; Yan, Xiaotao; Wang, Wei] Beijing Inst Astronaut Syst Engn, 1 Donggaodi South St, Beijing 100076, Peoples R China.
C3 Shanghai Jiao Tong University
RP Chen, HB (corresponding author), Shanghai Jiao Tong Univ, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM zhenpn@sjtu.edu.cn; sqwang026@sjtu.edu.cn; zhangsm2@163.com;
   93621169@qq.com; wangwei_01984@126.com; zhigangji@sjtu.edu.cn;
   haibaochen@sjtu.edu.cn
RI Chen, Hai-Bao/Q-2368-2017
FU National Key Research and Development Program of China [2019YFB2205005]
FX This work was supported by the National Key Research and Development
   Program of China under grant 2019YFB2205005.
CR Azimi SM, 2019, LECT NOTES COMPUT SC, V11363, P150, DOI 10.1007/978-3-030-20893-6_10
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen Z., 2020, PROC EUR C COMPUT VI, P195, DOI DOI 10.1007/978-3-030-58558-7_12
   Ding J, 2019, PROC CVPR IEEE, P2844, DOI 10.1109/CVPR.2019.00296
   Fu K, 2021, IEEE T GEOSCI REMOTE, V59, P4370, DOI 10.1109/TGRS.2020.3020165
   Han JM, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3062048
   Han JM, 2021, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR46437.2021.00281
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ji RY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3432861
   Li CZ, 2019, IEEE IMAGE PROC, P3886, DOI [10.1109/icip.2019.8803521, 10.1109/ICIP.2019.8803521]
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin YT, 2021, Arxiv, DOI arXiv:1912.00969
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Luo XF, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3381086
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Ming Q, 2021, AAAI CONF ARTIF INTE, V35, P2355
   Pan X., 2020, P IEEE CVF C COMP VI, P11207
   Qian W, 2021, AAAI CONF ARTIF INTE, V35, P2458
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Sun P, 2020, IEEE T GEOSCI REMOTE, V58, P7154, DOI 10.1109/TGRS.2020.2980023
   Tong C, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3365445
   Wang JW, 2021, IEEE T GEOSCI REMOTE, V59, P4307, DOI 10.1109/TGRS.2020.3010051
   Wang JW, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11242930
   Wang PJ, 2020, IEEE T GEOSCI REMOTE, V58, P3377, DOI 10.1109/TGRS.2019.2954328
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Xu CY, 2020, IEEE T GEOSCI REMOTE, V58, P4353, DOI 10.1109/TGRS.2019.2963243
   Xu YC, 2021, IEEE T PATTERN ANAL, V43, P1452, DOI [10.1109/TGRS.2020.3026387, 10.1109/TPAMI.2020.2974745]
   Xue Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P677, DOI 10.1007/978-3-030-58598-3_40
   Yang X., 2021, arXiv
   Yang X, 2022, Arxiv, DOI [arXiv:2004.13316, 10.1109/TPAMI.2022.3166956]
   Yang X, 2021, PROC CVPR IEEE, P15814, DOI 10.1109/CVPR46437.2021.01556
   Yang X, 2021, PR MACH LEARN RES, V139
   Yang X, 2021, AAAI CONF ARTIF INTE, V35, P3163
   Yang X, 2019, IEEE I CONF COMP VIS, P8231, DOI 10.1109/ICCV.2019.00832
   Yang Xue, 2021, ALPHAROTATE ROTATION
   Yi JR, 2021, IEEE WINT CONF APPL, P2149, DOI 10.1109/WACV48630.2021.00220
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Zhang GJ, 2019, IEEE T GEOSCI REMOTE, V57, P10015, DOI 10.1109/TGRS.2019.2930982
   Zhang XR, 2021, IEEE T GEOSCI REMOTE, V59, P3518, DOI 10.1109/TGRS.2020.3018106
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
   Zhu YX, 2020, IEEE T GEOSCI REMOTE, V58, P7247, DOI 10.1109/TGRS.2020.2981203
NR 46
TC 10
Z9 10
U1 6
U2 28
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 6
DI 10.1145/3513133
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400006
DA 2024-07-18
ER

PT J
AU Man, X
   Ouyang, DQ
   Li, XP
   Song, JK
   Shao, J
AF Man, Xin
   Ouyang, Deqiang
   Li, Xiangpeng
   Song, Jingkuan
   Shao, Jie
TI Scenario-Aware Recurrent Transformer for Goal-Directed Video Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Transformer; video captioning; scenario-aware; long-time dependency
AB Fully mining visual cues to aid in content understanding is crucial for video captioning. However, most stateof-the-art video captioning methods are limited to generating captions purely based on straightforward information while ignoring the scenario and context information. To fill the gap, we propose a novel, simple but effective scenario-aware recurrent transformer (SART) model to execute video captioning. Our model contains a "scenario understanding" module to obtain a global perspective across multiple frames, providing a specific scenario to guarantee a goal-directed description. Moreover, for the sake of achieving narrative continuity in the generated paragraph, a unified recurrent transformer is adopted. To demonstrate the effectiveness of our proposed SART, we have conducted comprehensive experiments on various large-scale video description datasets, including ActivityNet, YouCookII, and VideoStory. Additionally, we extend a story-oriented evaluation framework for assessing the quality of the generated caption more precisely. The superior performance has shown that SART has a strong ability to generate correct, deliberative, and narrative coherent video descriptions.
C1 [Man, Xin; Li, Xiangpeng; Song, Jingkuan; Shao, Jie] Univ Elect Sci & Technol China, Ctr Future Media, Sch Comp Sci & Engn, 2006 Xiyuan Ave, Chengdu 611731, Peoples R China.
   [Ouyang, Deqiang] Chongqing Univ, Coll Comp Sci, 174 Shazhengjie, Chongqing 400044, Peoples R China.
   [Ouyang, Deqiang] Intelligent Terminal Key Lab Sichuan Prov, Yibin 644000, Peoples R China.
C3 University of Electronic Science & Technology of China; Chongqing
   University
RP Ouyang, DQ (corresponding author), Chongqing Univ, Coll Comp Sci, 174 Shazhengjie, Chongqing 400044, Peoples R China.
EM manxin@std.uestc.edu.cn; deqiangouyang@cqu.edu.cn;
   xiangpengli.cs@gmail.com; jingkuan.song@gmail.com; shaojie@uestc.edu.cn
OI Ouyang, Deqiang/0000-0003-2259-886X
FU National Natural Science Foundation of China [61832001]; Open Fund of
   Intelligent Terminal Key Laboratory of Sichuan Province [SCITLAB-1016];
   Zhejiang Lab's International Talent Fund for Young Professionals
FX This work was supported by the National Natural Science Foundation of
   China (grant No. 61832001), Open Fund of Intelligent Terminal Key
   Laboratory of Sichuan Province (grant No. SCITLAB-1016) and the Zhejiang
   Lab's International Talent Fund for Young Professionals.
CR Aafaq N, 2020, ACM COMPUT SURV, V52, DOI 10.1145/3355390
   Agarwal Shubham, 2018, P 2 INT WORKSH SEARC, P59
   Alfassy A, 2019, PROC CVPR IEEE, P6541, DOI 10.1109/CVPR.2019.00671
   [Anonymous], 2016, PROC 24 ACM INT C MU, DOI [DOI 10.1145/2964284.2984065, 10.1145/2964284.2984065]
   Banerjee S., 2005, P WORKSHOP INTRINSIC, P65
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chen SZ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1838, DOI 10.1145/3123266.3123420
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Devlin J., 2018, BERT PRE TRAINING DE
   Duan X, 2018, ADV NEUR IN, V31
   Fujita Soichiro, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P517, DOI 10.1007/978-3-030-58539-6_31
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gella S, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P968
   Ging S., 2020, P NEURIPS, P22605
   Guo DY, 2019, IEEE INT CONF MULTI, P687, DOI 10.1109/ICMEW.2019.00134
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kong LC, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3226036
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lei J, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2603
   Li S, 2019, IEEE T EM TOP COMP I, V3, P297, DOI 10.1109/TETCI.2019.2892755
   Long Xiang, 2018, T ASSOC COMPUT LING, V6, P173, DOI DOI 10.1162/TACL_A_00013
   Luowei Zhou, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6571, DOI 10.1109/CVPR.2019.00674
   Mei T, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487269
   Mun J, 2019, PROC CVPR IEEE, P3581, DOI 10.1109/CVPR.2019.00675
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park JS, 2019, PROC CVPR IEEE, P6591, DOI 10.1109/CVPR.2019.00676
   Pascanu R., 2013, INT C MACH LEARN, P1310
   Roy D, 2005, ARTIF INTELL, V167, P170, DOI 10.1016/j.artint.2005.04.007
   Santra A, 2017, ADV GEOSPAT TECH, P1, DOI 10.4018/978-1-5225-1814-3
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Sutskever I, 2014, ADV NEUR IN, V27
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tang PJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303083
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang AQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3226037
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Xiong YL, 2018, LECT NOTES COMPUT SC, V11215, P489, DOI 10.1007/978-3-030-01252-6_29
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang ZL, 2019, ADV NEUR IN, V32
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yi K., 2020, P 8 INT C LEARN REPR
   Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23
   Zhang JC, 2020, IEEE T IMAGE PROCESS, V29, P6209, DOI 10.1109/TIP.2020.2988435
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
NR 53
TC 26
Z9 26
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 104
DI 10.1145/3503927
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600015
DA 2024-07-18
ER

PT J
AU Liu, Z
   Han, XH
AF Liu, Zhe
   Han, Xian-Hua
TI Deep Self-Supervised Hyperspectral Image Reconstruction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; hyperspectral image reconstruction; hyperspectral and RGB
   image fusion; self-supervised learning
ID FUSION
AB Reconstructing a high-resolution hyperspectral (HR-HS) image via merging a low-resolution hyperspectral (LR-HS) image and a high-resolution RGB (HR-RGB) image has become a hot research topic, and can greatly benefit for different subsequent high-level vision tasks. Recently, deep learning-based approaches have evolved for HS image reconstruction and validated impressive performance. However, to learn a good reconstruction model in the deep learning-based methods, it is mandatory to previously collect large-scale training triplets consisting of the LR-HS, HR-RGB, and HR-HS images, which is difficult to be collected in real applications. This study proposes a deep self-supervised HS image reconstruction framework (DSSH), which does not have to depend on any handcrafted prior and previously collected training triplets at all. The proposed DSSH method leverages the designed network architecture itself for capturing the prior of the underlying structure in the latent HR-HS image and employs the observed LR-HS and HR-RGB images only for network parameter learning. Experiments on two benchmark HS image datasets validated that the proposed DSSH method manifests very impressive reconstruction performance, and is even better than some state-of-the-art supervised learning approaches.
C1 [Liu, Zhe; Han, Xian-Hua] Yamaguchi Univ, Grad Sch Sci & Technol Innovat, 1677-1 Yoshida, Yamaguchi 7530831, Japan.
C3 Yamaguchi University
RP Liu, Z (corresponding author), Yamaguchi Univ, Grad Sch Sci & Technol Innovat, 1677-1 Yoshida, Yamaguchi 7530831, Japan.
EM a501wbu@yamaguchi-u.ac.jp; hanxhua@yamaguchi-u.ac.jp
RI Han, Xian-Hua/A-5563-2017
OI Han, Xian-Hua/0000-0002-5003-3180
FU Grants-in-Aid for Scientific Research [20K11867] Funding Source: KAKEN
CR Akhtar N, 2016, LECT NOTES COMPUT SC, V9907, P103, DOI 10.1007/978-3-319-46487-9_7
   Akhtar N, 2015, PROC CVPR IEEE, P3631, DOI 10.1109/CVPR.2015.7298986
   Akhtar N, 2014, LECT NOTES COMPUT SC, V8695, P63, DOI 10.1007/978-3-319-10584-0_5
   Barnes Michael, 2018, US Patent, Patent No. [9,883,833, 9883833]
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672
   Bishop CA, 2011, INT J REMOTE SENS, V32, P2409, DOI 10.1080/01431161003698336
   Chakrabarti A, 2011, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.2011.5995660
   Dahlkamp H., 2006, P ROB SCI SYST C
   Dian RW, 2018, IEEE T NEUR NET LEAR, V29, P5345, DOI 10.1109/TNNLS.2018.2798162
   Dong WS, 2016, IEEE T IMAGE PROCESS, V25, P2337, DOI 10.1109/TIP.2016.2542360
   Fu Y, 2019, PROC CVPR IEEE, P11653, DOI 10.1109/CVPR.2019.01193
   Han XH, 2019, IEEE INT CONF COMP V, P4330, DOI 10.1109/ICCVW.2019.00533
   Han XH, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P266, DOI [10.1109/BigMM.2019.00049, 10.1109/BigMM.2019.00-13]
   Han XH, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P443, DOI [10.1109/BigMM.2019.00028, 10.1109/BigMM.2019.00076]
   Han XH, 2018, IEEE IMAGE PROC, P2506, DOI 10.1109/ICIP.2018.8451142
   Han XH, 2018, IEEE T IMAGE PROCESS, V27, P5625, DOI 10.1109/TIP.2018.2855418
   He W, 2016, IEEE T GEOSCI REMOTE, V54, P176, DOI 10.1109/TGRS.2015.2452812
   He Z, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9101042
   Imamura R, 2019, Arxiv, DOI arXiv:1907.00651
   Imamura R, 2019, IEEE INT CONF COMP V, P1416, DOI 10.1109/ICCVW.2019.00178
   Kawakami R., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2329, DOI 10.1109/CVPR.2011.5995457
   Kim H, 2007, BIOINFORMATICS, V23, P1495, DOI 10.1093/bioinformatics/btm134
   Kwan C, 2017, INT CONF ACOUST SPEE, P6180, DOI 10.1109/ICASSP.2017.7953344
   Laben C. A., 2000, US Patent, Patent No. 6,011,875
   Lanaras C, 2015, IEEE I CONF COMP VIS, P3586, DOI 10.1109/ICCV.2015.409
   Li YS, 2017, NEUROCOMPUTING, V266, P29, DOI 10.1016/j.neucom.2017.05.024
   Liu Z., 2020, Proceedings of the Asian Conference on Computer Vision, P12628, DOI 10.1007/978-3-030-69756-3_3
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Qu Y, 2018, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2018.00266
   Sidorov O, 2019, Arxiv, DOI [arXiv:1902.00301, 10.48550/arxiv.1902.00301, DOI 10.48550/ARXIV.1902.00301]
   Simsek M, 2015, 2015 XXV INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATION AND AUTOMATION TECHNOLOGIES (ICAT)
   Uezato Tatsumi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P87, DOI 10.1007/978-3-030-58539-6_6
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Vo-Dinh T, 2004, IEEE ENG MED BIOL, V23, P40
   Wang AH, 2011, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2011-76
   Wang LZ, 2019, PROC CVPR IEEE, P8024, DOI 10.1109/CVPR.2019.00822
   Wycoff E, 2013, INT CONF ACOUST SPEE, P1409, DOI 10.1109/ICASSP.2013.6637883
   Xie Q, 2019, PROC CVPR IEEE, P1585, DOI 10.1109/CVPR.2019.00168
   Xu JL, 2017, J FOOD ENG, V196, P170, DOI 10.1016/j.jfoodeng.2016.10.021
   Yasuma F, 2010, IEEE T IMAGE PROCESS, V19, P2241, DOI 10.1109/TIP.2010.2046811
   Yokoya N, 2017, IEEE T GEOSCI REMOTE, V55, P2842, DOI 10.1109/TGRS.2017.2655115
   Yokoya N, 2011, INT GEOSCI REMOTE SE, P1779, DOI 10.1109/IGARSS.2011.6049465
   Zhang HY, 2012, SIGNAL PROCESS, V92, P2082, DOI 10.1016/j.sigpro.2012.01.020
   Zhang L, 2020, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR42600.2020.00314
   Zhu ZY, 2021, IEEE T IMAGE PROCESS, V30, P1423, DOI 10.1109/TIP.2020.3044214
   Zou CZ, 2020, SIGNAL PROCESS-IMAGE, V84, DOI 10.1016/j.image.2020.115833
NR 46
TC 4
Z9 4
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 149
DI 10.1145/3510373
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800019
DA 2024-07-18
ER

PT J
AU Singh, G
   Goyal, P
AF Singh, Gurinder
   Goyal, Puneet
TI SDCN2: A Shallow Densely Connected CNN for Multi-Purpose Image
   Manipulation Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Digital image forensics; anti-forensic attacks; convolutional neural
   networks; residual learning
ID FORENSIC DETECTION; JPEG COMPRESSION; ANTI-FORENSICS; ENHANCEMENT
AB Digital image information can be easily tampered with to harm the integrity of someone. Thus, recognizing the truthfulness and processing history of an image is one of the essential concerns in multimedia forensics. Numerous forensic methods have been developed by researchers with the ability to detect targeted editing operations. However, creating a unified forensic approach capable of detecting multiple image manipulations remains a challenging problem. In this article, a new general-purpose forensic approach is designed based on a shallow densely connected convolutional neural network (SDCN2) that exploits local dense connections and global residual learning. The residual domain is considered in the proposed network rather than the spatial domain to analyze the image manipulation artifacts because the residual domain is less dependent on image content information. To attain this purpose, a residual convolutional layer is employed at the beginning of the proposed model to adaptively learn the image manipulation features by suppressing the image content information. Then, the obtained image residuals or prediction error features are further processed by the shallow densely connected convolutional neural network for high-level feature extraction. In addition, the hierarchical features produced by the densely connected blocks and prediction error features are fused globally for better information flow across the network. The extensive experiment results show that the proposed scheme outperforms the existing state-of-the-art general-purpose forensic schemes even under anti-forensic attacks, when tested on large-scale datasets. The proposed model offers overall detection accuracies of 98.34% and 99.22% for BOSSBase and Dresden datasets, respectively, for multiple image manipulation detection. Moreover, the proposed network is highly efficient in terms of computational complexity as compared to the existing approaches.
C1 [Singh, Gurinder; Goyal, Puneet] Indian Inst Technol Ropar, Comp Sci & Engn Dept, Rupnagar 140001, Punjab, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Ropar
RP Singh, G (corresponding author), Indian Inst Technol Ropar, Comp Sci & Engn Dept, Rupnagar 140001, Punjab, India.
EM gurinder.singh@iitrpr.ac.in; Puneet@iitrpr.ac.in
FU Indian Institute of Technology Ropar under ISIRD
   [9-231/2016/IIT-RPR/1395]
FX This research was supported by the Indian Institute of Technology Ropar
   under ISIRD grant 9-231/2016/IIT-RPR/1395.
CR [Anonymous], 2015, IEEE INT WORKS INFOR
   [Anonymous], 2017, Electronic Imaging
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Battiato S, 2016, COMPUTER SYSTEMS AND TECHNOLOGIES, COMPSYSTECH'16, P5, DOI 10.1145/2983468.2983470
   Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953
   Bianchi T., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P1929, DOI 10.1109/ICIP.2011.6115848
   Bianchi T, 2012, IEEE T INF FOREN SEC, V7, P1003, DOI 10.1109/TIFS.2012.2187516
   Cao G, 2010, IEEE INT CON MULTI, P89, DOI 10.1109/ICME.2010.5583869
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chen C., 2011, P 10 INT WORKSHOP DI, P361
   Chen JS, 2015, IEEE SIGNAL PROC LET, V22, P1849, DOI 10.1109/LSP.2015.2438008
   Chen YF, 2018, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY (IH&MMSEC'18), P91, DOI 10.1145/3206004.3206013
   Dalgaard N, 2010, IEEE IMAGE PROC, P1753, DOI 10.1109/ICIP.2010.5652358
   Fan W, 2015, IEEE T INF FOREN SEC, V10, P1076, DOI 10.1109/TIFS.2015.2398362
   Fan W, 2014, IEEE T INF FOREN SEC, V9, P1211, DOI 10.1109/TIFS.2014.2317949
   Fan W, 2013, INT CONF ACOUST SPEE, P3058, DOI 10.1109/ICASSP.2013.6638220
   Feng XY, 2012, IEEE T MULTIMEDIA, V14, P536, DOI 10.1109/TMM.2012.2191946
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Gloe T., 2010, Proceedings of the ACM Symposium on Applied Computing, P1584, DOI DOI 10.1145/1774088.1774427
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heng Yao, 2009, 2009 IET International Communication Conference on Wireless Mobile & Computing (CCWMC 2009), P94
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kang XG, 2013, IEEE T INF FOREN SEC, V8, P1456, DOI 10.1109/TIFS.2013.2273394
   Kingma D. P., 2014, arXiv
   Kirchberg M., 2010, System Sciences (HICSS), 2010 43rd Hawaii International Conference on, P1
   Kirchner M, 2009, IEEE INT WORKS INFOR, P21, DOI 10.1109/WIFS.2009.5386489
   Kirchner M, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P11, DOI 10.1145/1411328.1411333
   Li HD, 2018, IEEE T CIRC SYST VID, V28, P31, DOI 10.1109/TCSVT.2016.2599849
   Liao X, 2020, IEEE J-STSP, V14, P955, DOI 10.1109/JSTSP.2020.3002391
   Lisha Yang, 2020, Advances in Intelligent Information Hiding and Multimedia Signal Processing. Proceedings of the 15th International Conference on IIH-MSP in conjunction with the 12th International Conference on FITAT. Smart Innovation, Systems and Technologies (SIST 157), P361, DOI 10.1007/978-981-13-9710-3_38
   Mahdian B, 2008, IEEE T INF FOREN SEC, V3, P529, DOI 10.1109/TIFS.2004.924603
   Mazumdar A, 2018, Arxiv, DOI arXiv:1808.06323
   Neelamani R, 2006, IEEE T IMAGE PROCESS, V15, P1365, DOI 10.1109/TIP.2005.864171
   Pasquini C, 2019, IEEE SIGNAL PROC MAG, V36, P101, DOI 10.1109/MSP.2018.2887214
   Pevny T, 2010, IEEE T INF FOREN SEC, V5, P215, DOI 10.1109/TIFS.2010.2045842
   Popescu AC, 2005, IEEE T SIGNAL PROCES, V53, P758, DOI 10.1109/TSP.2004.839932
   Qiu X., 2014, P 2 ACM WORKSH INF H, P165, DOI DOI 10.1145/2600918.2600941
   Qu ZH, 2008, INT CONF ACOUST SPEE, P1661, DOI 10.1109/ICASSP.2008.4517946
   Ravi H, 2016, IEEE SIGNAL PROC LET, V23, P212, DOI 10.1109/LSP.2015.2509477
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Singh G, 2019, IEEE T INF FOREN SEC, V14, P1194, DOI 10.1109/TIFS.2018.2871751
   Singhal D, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3388634
   Stamm M, 2008, IEEE IMAGE PROC, P3112, DOI 10.1109/ICIP.2008.4712454
   Stamm MC, 2013, IEEE ACCESS, V1, P167, DOI 10.1109/ACCESS.2013.2260814
   Stamm MC, 2010, INT CONF ACOUST SPEE, P1698, DOI 10.1109/ICASSP.2010.5495488
   Stamm MC, 2010, IEEE T INF FOREN SEC, V5, P492, DOI 10.1109/TIFS.2010.2053202
   Stamm Matthew C., 2016, P 4 ACM WORKSH INF H, P5
   Wilson AC, 2017, ADV NEUR IN, V30
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Yang C., 2020, 2020 IEEE INT C MULT, P1
   Yu JJ, 2017, LECT NOTES COMPUT SC, V10082, P3, DOI 10.1007/978-3-319-53465-7_1
   Zeng H, 2016, NEUROCOMPUTING, V189, P117, DOI 10.1016/j.neucom.2015.12.089
NR 53
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 150
DI 10.1145/3510462
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800020
DA 2024-07-18
ER

PT J
AU Pan, YH
   Li, ZC
   Zhang, LY
   Tang, JH
AF Pan, Yonghua
   Li, Zechao
   Zhang, Liyan
   Tang, Jinhui
TI Causal Inference with Knowledge Distilling and Curriculum Learning for
   Unbiased VQA
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual question answering; neural networks; knowledge distillation;
   causal inference
AB Recently, many Visual Question Answering (VQA) models rely on the correlations between questions and answers yet neglect those between the visual information and the textual information. They would perform badly if the handled data distribute differently from the training data (i.e., out-of-distribution (OOD) data). Towards this end, we propose a two-stage unbiased VQA approach that addresses the unbiased issue from a causal perspective. In the causal inference stage, we mark the spurious correlation on the causal graph, explore the counterfactual causality, and devise a causal target based on the inherent correlations between the conventional and counterfactual VQA models. In the distillation stage, we introduce the causal target into the training process and leverages distilling as well as curriculum learning to capture the unbiased model. Since Causal Inference with Knowledge Distilling and Curriculum Learning (CKCL) reinforces the contribution of the visual information and eliminates the impact of the spurious correlation by distilling the knowledge in causal inference to the VQA model, it contributes to the good performance on both the standard data and out-of-distribution data. The extensive experimental results on VQA-CP v2 dataset demonstrate the superior performance of the proposed method compared to the state-of-the-art (SotA) methods.
C1 [Pan, Yonghua; Li, Zechao; Tang, Jinhui] Nanjing Univ Sci & Technol, 200 Xiaolingwei St, Nanjing 210094, Jiangsu, Peoples R China.
   [Zhang, Liyan] Nanjing Univ Aeronaut & Astronaut, 29 Yudao St, Nanjing 210016, Jiangsu, Peoples R China.
C3 Nanjing University of Science & Technology; Nanjing University of
   Aeronautics & Astronautics
RP Li, ZC (corresponding author), Nanjing Univ Sci & Technol, 200 Xiaolingwei St, Nanjing 210094, Jiangsu, Peoples R China.
EM yonghuapan@njust.edu.cn; zechao.li@njust.edu.cn; zhangliyan@nuaa.edu.cn;
   jinhuitang@njust.edu.cn
RI Tang, Jinhui/KBR-0891-2024
FU National Key Research and Development Program of China [2018AAA0102002];
   National Natural Science Foundation of China [U20B2064, 61772268];
   Natural Science Foundation of Jiangsu Province [BK20190065]
FX This work was partially supported by the National Key Research and
   Development Program of China under Grant 2018AAA0102002, the National
   Natural Science Foundation of China (Grant No. U20B2064 and 61772268),
   and the Natural Science Foundation of Jiangsu Province (Grant
   BK20190065).
CR Abbasnejad E., 2020, P IEEE CVF C COMP VI, P10044, DOI DOI 10.1109/CVPR42600.2020.01006
   Agarwal Vedika, 2020, P IEEE CVF C COMP VI, P9690
   Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2018, IEEE T NEUR NET LEAR, DOI DOI 10.1109/TNNLS.2018.2817340
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cadene R., 2019, P INT C NEUR INF PRO, P839
   Chen Long, 2020, P IEEE CVF C COMP VI
   Chung Junyoung, 2014, ARXIV14123555
   Clark C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4069
   Dou Q, 2020, IEEE T MED IMAGING, V39, P2415, DOI 10.1109/TMI.2019.2963882
   Furlanello T, 2018, PR MACH LEARN RES, V80
   Gatta Carlo, 2015, P ICLR 2015 INT C L
   Gokhale T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P878
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton Geoffrey, 2014, NIPS DEEP LEARN WORK
   Jiang H., 2020, IEEE CVF C COMP VIS, DOI 10.1109/CVPR42600.2020.01028
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Kim JH, 2018, ADV NEUR IN, V31
   Kingma Diederik, 2015, P 3 INT C LEARN REPR
   Kv Gouthaman, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P18, DOI 10.1007/978-3-030-58601-0_2
   Li K, 2020, AAAI CONF ARTIF INTE, V34, P775
   Li ZC, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-3063-0
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1175, DOI 10.1145/3343031.3350993
   Lu JS, 2019, ADV NEUR IN, V32
   Manjunatha V, 2019, PROC CVPR IEEE, P9554, DOI 10.1109/CVPR.2019.00979
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   Nguyen Duy-Kien, 2021, P INT C LEARN REPR
   Niu YL, 2021, PROC CVPR IEEE, P12695, DOI 10.1109/CVPR46437.2021.01251
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Pearl J., 2018, BOOK WHY NEW SCI CAU, V361, P2
   Pearl J., 2001, P 17 C UNCERTAINTY A, P411, DOI DOI 10.5555/2074022.2074073
   Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Ramakrishnan S., 2018, INT C NEURAL INF PRO, V31, P1541
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tang Jinhui, 2021, P 2 ACM INT C MULT A, DOI [10.1145/3444685. 3446256, DOI 10.1145/3444685.3446256]
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Teney Damien, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P580, DOI 10.1007/978-3-030-58607-2_34
   Tian Yonglong, 2020, INT C LEARN REPR ICL
   TORRALBA A, 2011, PROC CVPR IEEE, P1521, DOI DOI 10.1109/CVPR.2011.5995347
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Wang T., 2020, P IEEE C COMP VIS PA, P10760
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yu Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3743, DOI 10.1145/3394171.3413977
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
NR 56
TC 13
Z9 14
U1 5
U2 41
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 67
DI 10.1145/3487042
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600001
DA 2024-07-18
ER

PT J
AU Anand, A
   Singh, AK
AF Anand, Ashima
   Singh, Amit Kumar
TI A Comprehensive Study of Deep Learning-based Covert Communication
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; IoT; ownership; watermarking
ID COLOR IMAGE WATERMARKING; ECG STEGANOGRAPHY; NEURAL-NETWORKS; ROBUST;
   SCHEME; INFORMATION; TRANSFORM; DOMAIN; SVD; MACHINE
AB Deep learning-based methods have been popular in multimedia analysis tasks, including classification, detection, segmentation, and so on. In addition to conventional applications, this model can be widely used for cover communication, i.e., information hiding. This article presents a review of deep learning-based covert communication scheme for protecting digital contents, devices, and models. In particular, we discuss the background knowledge, current applications, and constraints of existing deep learning-based information hiding schemes, identify recent challenges, and highlight possible research directions. Further, major role of deep learning in the area of information hiding are highlighted. Then, the contribution of surveyed scheme is also summarized and compared in the context of estimation of design objectives, approaches, evaluation metric, and weaknesses. We believe that this survey can pave the way to new research in this crucial field of information hiding in deep-learning environment.
C1 [Anand, Ashima] Thapar Inst Engn & Technol, Dept CSE, Patiala 147004, Punjab, India.
   [Singh, Amit Kumar] Natl Inst Technol Patna, Dept CSE, Patna 800005, Bihar, India.
C3 Thapar Institute of Engineering & Technology; National Institute of
   Technology (NIT System); National Institute of Technology Patna
RP Singh, AK (corresponding author), Natl Inst Technol Patna, Dept CSE, Patna 800005, Bihar, India.
EM ashima.anand@thapar.edu; amit.singh@nitp.ac.in
RI Singh, Amit Kumar/D-1300-2015
OI Singh, Amit Kumar/0000-0001-7359-2068
CR Abuadbba A, 2017, IEEE T BIO-MED ENG, V64, P2186, DOI 10.1109/TBME.2016.2631885
   Ahmadi M, 2020, EXPERT SYST APPL, V146, DOI 10.1016/j.eswa.2019.113157
   Anand A, 2022, IEEE T COMPUT SOC SY, V9, P1265, DOI 10.1109/TCSS.2021.3125025
   Anand A, 2020, IEEE MULTIMEDIA, V27, P133, DOI 10.1109/MMUL.2020.2993269
   Anand A, 2021, MULTIMED TOOLS APPL, V80, P30165, DOI 10.1007/s11042-020-08801-0
   Anand A, 2020, COMPUT COMMUN, V152, P72, DOI 10.1016/j.comcom.2020.01.038
   Anju Reena., 2013, International Journal of Information and Computation Technology, V3, P691
   [Anonymous], 2011, INT J COMPUT ELECT E, DOI DOI 10.7763/IJCEE.2011.V3.285
   Araghi TK, 2018, EXPERT SYST APPL, V112, P208, DOI 10.1016/j.eswa.2018.06.024
   Arya RK, 2015, 2015 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P2042, DOI 10.1109/ICACCI.2015.7275917
   Badshah G, 2016, J DIGIT IMAGING, V29, P216, DOI 10.1007/s10278-015-9822-4
   Bagheri M., 2020, ARXIV, P1, DOI [10.48550/arXiv.2001.03251, DOI 10.48550/ARXIV.2001.03251]
   Banerjee S, 2021, BIOMED SIGNAL PROCES, V64, DOI 10.1016/j.bspc.2020.102151
   Byun SW, 2019, IEEE ACCESS, V7, P100706, DOI 10.1109/ACCESS.2019.2931039
   Cedillo-Hernandez M, 2020, BIOMED SIGNAL PROCES, V56, DOI 10.1016/j.bspc.2019.101695
   Chen BJ, 2018, MULTIMED TOOLS APPL, V77, P20809, DOI 10.1007/s11042-017-5511-2
   Chen ST, 2014, J MED SYST, V38, DOI 10.1007/s10916-014-0054-9
   Rouhani BD, 2018, Arxiv, DOI arXiv:1804.00750
   Deeba F, 2020, INF SECUR J, V29, P30, DOI 10.1080/19393555.2020.1717684
   Deeba Farah, 2020, INT J MACH LEARN COM, V10, DOI DOI 10.18178/IJMLC.2020.10.2.932
   Ding WP, 2022, IEEE TETCI, V6, P613, DOI 10.1109/TETCI.2021.3055520
   Eloff MM, 2002, INT FED INFO PROC, V86, P535
   Ernawan F, 2020, VISUAL COMPUT, V36, P19, DOI 10.1007/s00371-018-1567-x
   Etemad E, 2018, MULTIMED TOOLS APPL, V77, P2033, DOI 10.1007/s11042-016-4278-1
   Fierro-Radilla A, 2019, I W BIOMETRIC FORENS, DOI 10.1109/iwbf.2019.8739245
   Gaata M, 2011, IEEE INT SYMP SIGNAL, P218
   Ghafoor A., 2012, RADIOENGINEERING, V21, P1246
   Guan XQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2273, DOI 10.1145/3394171.3413729
   Hallur S. R., 2015, INT J TECHNOL RES EN, V2, P2440
   Hatoum MW, 2021, SIGNAL PROCESS-IMAGE, V90, DOI 10.1016/j.image.2020.116019
   He WG, 2017, J VIS COMMUN IMAGE R, V46, P58, DOI 10.1016/j.jvcir.2017.03.010
   Hongqin Shi, 2014, Journal of Software, V9, P1749, DOI 10.4304/jsw.9.7.1749-1756
   Hou JC, 2021, SIGNAL PROCESS-IMAGE, V92, DOI 10.1016/j.image.2020.116118
   Hsu LY, 2021, IEEE ACCESS, V9, P155138, DOI 10.1109/ACCESS.2021.3127917
   Huang HP, 2017, IEEE T IND INFORM, V13, P1227, DOI 10.1109/TII.2017.2687618
   Ibaida A, 2013, IEEE T BIO-MED ENG, V60, P3322, DOI 10.1109/TBME.2013.2264539
   Ingaleshwar S, 2023, MULTIMED TOOLS APPL, V82, P21957, DOI 10.1007/s11042-020-10498-0
   Islam M, 2018, J INTELL FUZZY SYST, V34, P1691, DOI 10.3233/JIFS-169462
   Jagadeesh B, 2016, SOFT COMPUT, V20, P3679, DOI 10.1007/s00500-015-1729-y
   Jane O, 2014, TURK J ELECTR ENG CO, V22, P1354, DOI 10.3906/elk-1212-75
   Jebreel NM, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11030999
   Jero SE, 2016, EXPERT SYST APPL, V49, P123, DOI 10.1016/j.eswa.2015.12.010
   Jero SE, 2016, ELECTRON LETT, V52, P283, DOI 10.1049/el.2015.3218
   Jero SE, 2014, J MED SYST, V38, DOI 10.1007/s10916-014-0132-z
   Jia YJ, 2019, SIGNAL PROCESS, V163, P238, DOI 10.1016/j.sigpro.2019.05.020
   Kandi H, 2017, COMPUT SECUR, V65, P247, DOI 10.1016/j.cose.2016.11.016
   Kang XB, 2018, MULTIMED TOOLS APPL, V77, P13197, DOI 10.1007/s11042-017-4941-1
   Kazemi MF, 2020, COMPLEX INTELL SYST, V6, P213, DOI 10.1007/s40747-020-00129-4
   Khan M., 2019, Deep Learning: Convergence to Big Data Analytics, P31, DOI [DOI 10.1007/978-981-13-3459-7_3, 10.1007/978-981-13-3459-7_3]
   Kumar C, 2018, MULTIMED TOOLS APPL, V77, P3597, DOI 10.1007/s11042-017-5222-8
   Kumar N. M., 2011, 2011 International Conference on Recent Trends in Information Technology (ICRTIT 2011), P1277, DOI 10.1109/ICRTIT.2011.5972280
   Lei BY, 2014, EXPERT SYST APPL, V41, P3178, DOI 10.1016/j.eswa.2013.11.019
   Li DM, 2019, INFORM SCIENCES, V479, P432, DOI 10.1016/j.ins.2018.02.060
   Li JZ, 2018, SOFT COMPUT, V22, P47, DOI 10.1007/s00500-016-2320-x
   Li XL, 2015, IEEE T INF FOREN SEC, V10, P2016, DOI 10.1109/TIFS.2015.2444354
   Li Y, 2021, NEUROCOMPUTING, V461, P171, DOI 10.1016/j.neucom.2021.07.051
   Liji CA, 2016, PROC TECH, V24, P1039, DOI 10.1016/j.protcy.2016.05.230
   Mastorakis S, 2021, CONSUM COMM NETWORK, DOI [10.1109/CCNC49032.2021.9369515, 10.1109/ccnc49032.2021.9369515]
   Moosazadeh M, 2019, J INF SECUR APPL, V47, P28, DOI 10.1016/j.jisa.2019.04.001
   Mun S. M., 2017, ARXIV, P1
   Mun SM, 2019, NEUROCOMPUTING, V337, P191, DOI 10.1016/j.neucom.2019.01.067
   Nagai Y, 2018, INT J MULTIMED INF R, V7, P3, DOI 10.1007/s13735-018-0147-1
   Oueslati S., 2011, International Journal of Engineering Science and Technology, V3, P748
   Pathak Yatindra, 2015, INFOCOMP Journal of Computer Science, V14, P50
   Plata M, 2020, IEEE INT CONF TRUST, P62, DOI 10.1109/TrustCom50675.2020.00022
   Qian HS, 2016, 8TH INTERNATIONAL CONFERENCE ON INTERNET MULTIMEDIA COMPUTING AND SERVICE (ICIMCS2016), P277, DOI 10.1145/3007669.3007709
   Raval MS, 2013, IEEE SYS MAN CYBERN, P328, DOI 10.1109/SMC.2013.62
   Saharan B.S., 2014, Chinese Journal of Biology, V2014, P1, DOI DOI 10.1155/2014/802984
   Salem S, 2020, GLOB MID EAST, P1, DOI [10.2214/AJR.20.23034, 10.2214/AJR.20.23034S., 10.1017/9781108868969]
   Sharma VK, 2022, J KING SAUD UNIV-COM, V34, P615, DOI 10.1016/j.jksuci.2019.03.009
   Singh AK, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3422816
   Singh AK, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3382772
   Singh AK, 2017, MULTIMED SYST APPL, P1, DOI 10.1007/978-3-319-57699-2
   Singh AK, 2018, FUTURE GENER COMP SY, V86, P926, DOI 10.1016/j.future.2016.11.023
   Singh AK, 2016, MULTIMED TOOLS APPL, V75, P8381, DOI 10.1007/s11042-015-2754-7
   Singh KN, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498342
   Singh OP, 2023, COMPLEX INTELL SYST, V9, P2759, DOI 10.1007/s40747-021-00309-w
   Sinhal R, 2021, PATTERN RECOGN LETT, V145, P171, DOI 10.1016/j.patrec.2021.02.011
   Soleymani SH, 2019, MULTIMED TOOLS APPL, V78, P19163, DOI 10.1007/s11042-019-7282-4
   Su QT, 2018, SOFT COMPUT, V22, P91, DOI 10.1007/s00500-017-2489-7
   Sun WW, 2021, IEEE T CIRC SYST VID, V31, P1208, DOI 10.1109/TCSVT.2020.2998476
   Tareef A, 2015, EXPERT SYST APPL, V42, P2224, DOI 10.1016/j.eswa.2014.09.055
   Thakur S, 2020, LECT NOTES ELECTR EN, V587, P897, DOI 10.1007/978-981-32-9775-3_80
   Thakur S, 2019, MULTIMED TOOLS APPL, V78, P3457, DOI 10.1007/s11042-018-6263-3
   Thien HT, 2019, 2019 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P552, DOI 10.1109/dicta47822.2019.8945866
   Tsai HH, 2007, INFORM SCIENCES, V177, P550, DOI 10.1016/j.ins.2006.05.002
   Tsougenis ED, 2014, EXPERT SYST APPL, V41, P6408, DOI 10.1016/j.eswa.2014.04.021
   Tu FB, 2017, IEEE T VLSI SYST, V25, P2220, DOI 10.1109/TVLSI.2017.2688340
   Uchida Y, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P274, DOI 10.1145/3078971.3078974
   Urvoy M, 2014, IEEE T INF FOREN SEC, V9, P1108, DOI 10.1109/TIFS.2014.2322497
   Verma VS, 2019, MULTIMED TOOLS APPL, V78, P23203, DOI 10.1007/s11042-019-7599-z
   Wang XC, 2021, J INF SECUR APPL, V59, DOI 10.1016/j.jisa.2021.102820
   Wang ZH, 2023, Arxiv, DOI arXiv:2107.09287
   Wu HZ, 2021, IEEE T CIRC SYST VID, V31, P2591, DOI 10.1109/TCSVT.2020.3030671
   Xing Y, 2010, RADIOENGINEERING, V19, P62
   Xiong L., 2022, ACM T MULTIM COMPUT, V2022, DOI [10.1145/3512797, DOI 10.1145/3512797]
   Xiong X., 2015, WORLD J ENG TECHNOL, V3, P177
   Xiyang Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13545, DOI 10.1109/CVPR42600.2020.01356
   Zear A, 2018, MULTIMED TOOLS APPL, V77, P4863, DOI 10.1007/s11042-016-3862-8
   Zhang C., 2021, arXiv
   Zhang JL, 2018, PROCEEDINGS OF THE 2018 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIACCS'18), P159, DOI 10.1145/3196494.3196550
   Zhang LN, 2020, SIGNAL PROCESS, V169, DOI 10.1016/j.sigpro.2019.107421
   Zhang Q., 2018, 2018 CONDITION MONIT, V2018, P1, DOI [10.1109/CMD.2018.8535878, DOI 10.1109/CMD.2018.8535878]
   Zhao MW, 2008, I C WIREL COMM NETW, P12505
   Zheng WB, 2018, C IND ELECT APPL, P1233, DOI 10.1109/ICIEA.2018.8397898
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 106
TC 11
Z9 11
U1 5
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 118
DI 10.1145/3508365
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000005
DA 2024-07-18
ER

PT J
AU Thong, W
   Snoek, CGM
AF Thong, William
   Snoek, Cees G. M.
TI Diversely-Supervised Visual Product Search
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Product retrieval; diverse supervision; diverse representation
ID IMAGE RETRIEVAL
AB This article strives for a diversely supervised visual product search, where queries specify a diverse set of labels to search for. Where previous works have focused on representing attribute, instance, or category labels individually, we consider them together to create a diverse set of labels for visually describing products. We learn an embedding from the supervisory signal provided by every label to encode their interrelationships. Once trained, every label has a corresponding visual representation in the embedding space, which is an aggregation of selected items from the training set. At search time, composite query representations retrieve images that match a specific set of diverse labels. We form composite query representations by averaging over the aggregated representations of each diverse label in the specific set. For evaluation, we extend existing product datasets of cars and clothes with a diverse set of labels. Experiments show the benefits of our embedding for diversely supervised visual product search in seen and unseen product combinations and for discovering product design styles.
C1 [Thong, William; Snoek, Cees G. M.] Univ Amsterdam, Sci Pk 904, Amsterdam, Netherlands.
C3 University of Amsterdam
RP Thong, W (corresponding author), Univ Amsterdam, Sci Pk 904, Amsterdam, Netherlands.
EM w.e.thong@uva.nl; cgmsnoek@uva.nl
CR Ak KE, 2019, IEEE I CONF COMP VIS, P10540, DOI 10.1109/ICCV.2019.01064
   Ak KE, 2018, PROC CVPR IEEE, P7708, DOI 10.1109/CVPR.2018.00804
   Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Al-Halah Z, 2017, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2017.50
   Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959
   Bergamo Alessandro, 2011, NEURIPS
   Bowman S. R., 2015, Proceedings of the 2015 conference on empirical methods in natural language processing, DOI [DOI 10.18653/V1/D15-1075, 10.18653/v1/d15-1075, 10.18653/v1/D15-1075]
   Chaudhary C, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3375786
   Chechik Gal, 2009, NEURIPS
   Chen XC, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207010
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Deselaers T, 2011, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR.2011.5995474
   Devlin J., 2018, BERT PRE TRAINING DE
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Ferrari Vittorio, 2008, NEURIPS
   Frome Andrea, 2007, NEURIPS
   Gordo A, 2017, PROC CVPR IEEE, P5272, DOI 10.1109/CVPR.2017.560
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hsiao WL, 2018, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR.2018.00748
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Huang JS, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2632165
   Jing YS, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1889, DOI 10.1145/2783258.2788621
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kim G, 2015, PROC CVPR IEEE, P1993, DOI 10.1109/CVPR.2015.7298810
   Kingma D. P., 2014, arXiv
   Kovashka A, 2012, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2012.6248026
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Kumar N, 2011, IEEE T PATTERN ANAL, V33, P1962, DOI 10.1109/TPAMI.2011.48
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lan T, 2012, LECT NOTES COMPUT SC, V7577, P129, DOI 10.1007/978-3-642-33783-3_10
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Loshchilov I., 2017, P INT C LEARN REPR
   Mall U, 2019, IEEE I CONF COMP VIS, P411, DOI 10.1109/ICCV.2019.00050
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Paszke A, 2019, ADV NEUR IN, V32
   Rastegari M, 2013, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2013.425
   Ravi H, 2018, PROC CVPR IEEE, P7613, DOI 10.1109/CVPR.2018.00794
   Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982
   Rosch E., 1978, COGNITION CATEGORIZA, P27, DOI DOI 10.1016/B978-1-4832-1446-7.50028-5
   Ruder S, 2019, AAAI CONF ARTIF INTE, P4822
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711
   Sanderson M, 2010, NAT LANG ENG, V16, P100, DOI 10.1017/S1351324909005129
   Scheirer WJ, 2012, PROC CVPR IEEE, P2933, DOI 10.1109/CVPR.2012.6248021
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Siddiquie B, 2011, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2011.5995329
   Snell J, 2017, ADV NEUR IN, V30
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   SPROLES GB, 1981, J MARKETING, V45, P116, DOI 10.2307/1251479
   Strezoski G, 2019, IEEE I CONF COMP VIS, P1375, DOI 10.1109/ICCV.2019.00146
   van den Brink D, 2006, J CONSUM MARK, V23, P15
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Veit A, 2017, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2017.193
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Williams A., 2018, P 2018 C N AM CHAPTE, P1112
   Yang F, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2101, DOI 10.1145/3097983.3098162
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Ye LW, 2018, IEEE WINT CONF APPL, P1461, DOI 10.1109/WACV.2018.00164
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
   Zakizadeh R., 2018, ARXIV180711674
   Zhai A., 2019, 30 BRIT MACH VIS C 2
   Zhai A, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2412, DOI 10.1145/3292500.3330739
   Zhai A, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P515, DOI 10.1145/3041021.3054201
   Zhang YH, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P993, DOI 10.1145/3219819.3219820
   Zhao B, 2017, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2017.652
   Zhao XY, 2018, LECT NOTES COMPUT SC, V11205, P415, DOI 10.1007/978-3-030-01246-5_25
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 82
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 13
DI 10.1145/3461646
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900013
OA Green Published
DA 2024-07-18
ER

PT J
AU Uddin, MA
   Joolee, JB
   Lee, YK
   Sohn, KA
AF Uddin, Md Azher
   Joolee, Joolekha Bibi
   Lee, Young-Koo
   Sohn, Kyung-Ah
TI A Novel Multi-Modal Network-Based Dynamic Scene Understanding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-modal network; volume symmetric gradient local graph structure;
   volume local directional transition pattern; temporal mixed pooling;
   stacked Bi-LSTM network
ID RECOGNITION
AB In recent years, dynamic scene understanding has gained attention from researchers because of its widespread applications. The main important factor in successfully understanding the dynamic scenes lies in jointly representing the appearance and motion features to obtain an informative description. Numerous methods have been introduced to solve dynamic scene recognition problem, nevertheless, a few concerns still need to be investigated. In this article, we introduce a novel multi-modal network for dynamic scene understanding from video data, which captures both spatial appearance and temporal dynamics effectively. Furthermore, two-level joint tuning layers are proposed to integrate the global and local spatial features as well as spatial and temporal stream deep features. In order to extract the temporal information, we present a novel dynamic descriptor, namely, Volume Symmetric Gradient Local Graph Structure (VSGLGS), which generates temporal feature maps similar to optical flow maps. However, this approach overcomes the issues of optical flow maps. Additionally, Volume Local Directional Transition Pattern (VLDTP) based handcrafted spatiotemporal feature descriptor is also introduced, which extracts the directional information through exploiting edge responses. Lastly, a stacked Bidirectional Long Short-Term Memory (Bi-LSTM) network along with a temporal mixed pooling scheme is designed to achieve the dynamic information without noise interference. The extensive experimental investigation proves that the proposed multi-modal network outperforms most of the state-of-the-art approaches for dynamic scene understanding.
C1 [Uddin, Md Azher; Sohn, Kyung-Ah] Ajou Univ, Dept Artificial Intelligence, 206 World Cup Ro, Suwon 16499, Gyeonggi Do, South Korea.
   [Joolee, Joolekha Bibi; Lee, Young-Koo] Kyung Hee Univ, Dept Comp Sci & Engn, 1732 Deogyeong Daero, Yongin 17104, Gyeonggi Do, South Korea.
   [Sohn, Kyung-Ah] Ajou Univ, Dept Software & Comp Engn, 206 World Cup Ro, Suwon 16499, Gyeonggi Do, South Korea.
C3 Ajou University; Kyung Hee University; Ajou University
RP Sohn, KA (corresponding author), Ajou Univ, Dept Artificial Intelligence, 206 World Cup Ro, Suwon 16499, Gyeonggi Do, South Korea.; Lee, YK (corresponding author), Kyung Hee Univ, Dept Comp Sci & Engn, 1732 Deogyeong Daero, Yongin 17104, Gyeonggi Do, South Korea.; Sohn, KA (corresponding author), Ajou Univ, Dept Software & Comp Engn, 206 World Cup Ro, Suwon 16499, Gyeonggi Do, South Korea.
EM azher006@yahoo.com; julekhajulie@gmail.com; yklee@khu.ac.kr;
   kasohn@ajou.ac.kr
RI Lee, Sungyoung/HDN-1116-2022; Uddin, Azher/KJM-3153-2024
FU Institute for Information & communications Technology Promotion (IITP) -
   Korea government (MSIT) [2016-0-00406]; National Research Foundation of
   Korea grant funded by the Korea government (MSIT)
   [NRF-2019R1A2C1006608]; BK21 FOUR program of the Ministry of Education
   [NRF5199991014091]
FX This research was supported by the Institute for Information &
   communications Technology Promotion (IITP) grant funded by the Korea
   government (MSIT) (No.2016-0-00406, SIAT CCTV Cloud Platform), by the
   National Research Foundation of Korea grant funded by the Korea
   government (MSIT) (NRF-2019R1A2C1006608) and by the BK21 FOUR program of
   the Ministry of Education (NRF5199991014091).
CR Abdullah MFA, 2014, EXPERT SYST APPL, V41, P6131, DOI 10.1016/j.eswa.2014.04.006
   Abu-El-Haija Sami, 2016, arXiv
   Al Jazaery M, 2021, IEEE T AFFECT COMPUT, V12, P262, DOI 10.1109/TAFFC.2018.2870884
   [Anonymous], 1997, NEURAL COMPUT
   Chen JK, 2018, IEEE T AFFECT COMPUT, V9, P38, DOI 10.1109/TAFFC.2016.2593719
   Derpanis KG, 2012, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2012.6247815
   Ding YK, 2020, NEUROCOMPUTING, V403, P348, DOI 10.1016/j.neucom.2020.04.110
   Du GL, 2021, IEEE T INTELL TRANSP, V22, P4570, DOI 10.1109/TITS.2020.3007357
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.56
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, IEEE T PATTERN ANAL, V38, P2389, DOI 10.1109/TPAMI.2016.2526008
   Feichtenhofer C, 2014, PROC CVPR IEEE, P2681, DOI 10.1109/CVPR.2014.343
   Gao CX, 2016, INFORM SCIENCES, V372, P84, DOI 10.1016/j.ins.2016.08.035
   Gao JY, 2016, NEUROCOMPUTING, V214, P708, DOI 10.1016/j.neucom.2016.06.055
   Hadji I., 2010, P EUR C COMP VIS ECC, P320
   Hong S, 2018, NEUROCOMPUTING, V273, P611, DOI 10.1016/j.neucom.2017.08.046
   Huang YJ, 2019, AAAI CONF ARTIF INTE, P8497
   Huang YJ, 2019, IEEE T CIRC SYST VID, V29, P1038, DOI 10.1109/TCSVT.2018.2823360
   Jabid T, 2010, IEEE ICCE
   Jiang SQ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231738
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Mirza AH, 2020, DIGIT SIGNAL PROCESS, V102, DOI 10.1016/j.dsp.2020.102742
   Nanni L, 2011, EXPERT SYST APPL, V38, P5125, DOI 10.1016/j.eswa.2010.09.137
   Peng X., 2020, P DIG IM COMP TECHN
   Qi XB, 2016, NEUROCOMPUTING, V171, P1230, DOI 10.1016/j.neucom.2015.07.071
   Shroff N, 2010, PROC CVPR IEEE, P1911, DOI 10.1109/CVPR.2010.5539864
   Simonyan K, 2015, IEEE INT C ICLR
   Simonyan K, 2014, ADV NEUR IN, V27
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tang PJ, 2017, NEUROCOMPUTING, V225, P188, DOI 10.1016/j.neucom.2016.11.023
   Tao HJ, 2020, APPL INTELL, V50, P1057, DOI 10.1007/s10489-019-01589-z
   Thériault C, 2013, PROC CVPR IEEE, P2603, DOI 10.1109/CVPR.2013.336
   Uddin MA, 2018, IEEE ACCESS, V6, P66123, DOI 10.1109/ACCESS.2018.2878865
   Ullah A, 2019, IEEE T IND ELECTRON, V66, P9692, DOI 10.1109/TIE.2018.2881943
   Ullah I, 2017, LECT NOTES COMPUT SC, V10597, P591, DOI 10.1007/978-3-319-69900-4_75
   Vasudevan AB, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P803, DOI 10.1109/ICCVW.2013.110
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
   Yu Y, 2020, NEUROCOMPUTING, V402, P134, DOI 10.1016/j.neucom.2020.03.041
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhu Y, 2018, IEEE T AFFECT COMPUT, V9, P578, DOI 10.1109/TAFFC.2017.2650899
NR 41
TC 0
Z9 0
U1 2
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 7
DI 10.1145/3462218
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900007
DA 2024-07-18
ER

PT J
AU Xu, CY
   Liu, R
   Zhang, T
   Cui, Z
   Yang, J
   Hu, CL
AF Xu, Chunyan
   Liu, Rong
   Zhang, Tong
   Cui, Zhen
   Yang, Jian
   Hu, Chunlong
TI Dual-Stream Structured Graph Convolution Network for Skeleton-Based
   Action Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Graph convolution network; dual-stream structured graph convolution;
   action recognition
AB In this work, we propose a dual-stream structured graph convolution network (DS-SGCN) to solve the skeleton-based action recognition problem. The spatio-temporal coordinates and appearance contexts of the skeletal joints are jointly integrated into the graph convolution learning process on both the video and skeleton modalities. To effectively represent the skeletal graph of discrete joints, we create a structured graph convolution module specifically designed to encode partitioned body parts along with their dynamic interactions in the spatio-temporal sequence. In more detail, we build a set of structured intra-part graphs, each of which can be adopted to represent a distinctive body part (e.g., left arm, right leg, head). The inter-part graph is then constructed to model the dynamic interactions across different body parts; here each node corresponds to an intra-part graph built above, while an edge between two nodes is used to express these internal relationships of human movement. We implement the graph convolution learning on both intra- and inter-part graphs in order to obtain the inherent characteristics and dynamic interactions, respectively, of human action. After integrating the intra- and inter-levels of spatial context/coordinate cues, a convolution filtering process is conducted on time slices to capture these temporal dynamics of human motion. Finally, we fuse two streams of graph convolution responses in order to predict the category information of human action in an end-to-end fashion. Comprehensive experiments on five single/multi-modal benchmark datasets (including NTU RGB+D 60, NTU RGB+D 120, MSR-Daily 3D, N-UCLA, and HDM05) demonstrate that the proposed DS-SGCN framework achieves encouraging performance on the skeleton-based action recognition task.
C1 [Xu, Chunyan; Liu, Rong; Zhang, Tong; Cui, Zhen; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Xiaolinwei St 200, Nanjing 210094, Peoples R China.
   [Hu, Chunlong] Jiangsu Univ Sci & Technol, Sch Comp Sci & Engn, Changhui Rd 666, Zhenjiang 212003, Jiangsu, Peoples R China.
C3 Nanjing University of Science & Technology; Jiangsu University of
   Science & Technology
RP Cui, Z (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Xiaolinwei St 200, Nanjing 210094, Peoples R China.
EM cyx@njust.edu.cn; liur@njust.edu.cn; csjyang@njust.edu.cn;
   huchunlong@just.edu.cn
FU National Natural Science Foundation of China [61972204, 62072244];
   Natural Science Foundation of Jiangsu Province [BK20191283, BK20190019];
   CCF-Tencent Open Research Fund
FX This work was supported by the National Natural Science Foundation of
   China (Grants No. 61972204 and NO. 62072244), the Natural Science
   Foundation of Jiangsu Province (Grants No. BK20191283 and No.
   BK20190019), and CCF-Tencent Open Research Fund.
CR Atwood J, 2016, ADV NEUR IN, V29
   Baradel F, 2018, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2018.00056
   Baradel F, 2017, IEEE INT CONF COMP V, P604, DOI 10.1109/ICCVW.2017.77
   Baradel Fabien., 2018, Proc. Brit. Mach. Vis. Conf, P1
   Bojchevski A, 2018, PR MACH LEARN RES, V80
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Chéron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368
   Cui Zhen, 2019, 2019 INT JOINT C NEU, P1
   Das Srijan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P72, DOI 10.1007/978-3-030-58545-7_5
   Das S, 2019, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2019.00092
   Das S, 2019, IEEE WINT CONF APPL, P71, DOI 10.1109/WACV.2019.00015
   Defferrard M, 2016, ADV NEUR IN, V29
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Hamilton WL, 2017, ADV NEUR IN, V30
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Henaff M., 2015, ARXIV150605163
   Hu JF, 2018, LECT NOTES COMPUT SC, V11211, P346, DOI 10.1007/978-3-030-01234-2_21
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Huang ZW, 2017, AAAI CONF ARTIF INTE, P2036
   Huang ZW, 2017, PROC CVPR IEEE, P1243, DOI 10.1109/CVPR.2017.137
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Jiang JT, 2019, AAAI CONF ARTIF INTE, P4007
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Ketkar N, 2017, DEEP LEARNING PYTHON, P195, DOI [10.1007/978-1-4842-2766-4_12, DOI 10.1007/978-1-4842-2766-4_12]
   Kipf T. N., 2016, ARXIV161107308, V1050, P21
   Kipf TN, 2016, ARXIV
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li CL, 2018, AAAI CONF ARTIF INTE, P3482
   Li CL, 2018, IEEE T IMAGE PROCESS, V27, P3657, DOI 10.1109/TIP.2018.2815744
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li YH, 2016, LECT NOTES COMPUT SC, V9911, P203, DOI 10.1007/978-3-319-46478-7_13
   Liang DH, 2019, IEEE COMPUT SOC CONF, P934, DOI 10.1109/CVPRW.2019.00123
   Liu C., 2017, Pku-mmd: A large scale benchmark for continuous multi-modal human action understanding
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu M, 2019, IEEE T IMAGE PROCESS, V28, P1235, DOI 10.1109/TIP.2018.2875363
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Memmesheimer R, 2020, IEEE INT C INT ROBOT, P10394, DOI 10.1109/IROS45743.2020.9341699
   Muller M., 2007, Tech. Rep. CG-2007-2
   Niepert M, 2016, PR MACH LEARN RES, V48
   Pérez-Rúa JM, 2019, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2019.00713
   Qin XL, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Ramezani M, 2016, ARTIF INTELL REV, V46, P485, DOI 10.1007/s10462-016-9473-y
   Ryoo MS, 2015, ACMIEEE INT CONF HUM, P295, DOI 10.1145/2696454.2696462
   Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shahroudy A, 2016, IEEE T PATTERN ANAL, V38, P2123, DOI 10.1109/TPAMI.2015.2505295
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shi Lei, 2020, EUR C COMP VIS 2020, P536
   Si CY, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107511
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Song SJ, 2018, IEEE INT CON MULTI
   Song SJ, 2018, IEEE T IMAGE PROCESS, V27, P3459, DOI 10.1109/TIP.2018.2818328
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Such FP, 2017, IEEE J-STSP, V11, P884, DOI 10.1109/JSTSP.2017.2726981
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Nguyen TN, 2018, INT CONF KNOWL SYS, P50, DOI 10.1109/KSE.2018.8573421
   Tsai TJ, 2015, IEEE T MULTIMEDIA, V17, P1550, DOI 10.1109/TMM.2015.2454332
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang DG, 2018, LECT NOTES COMPUT SC, V11213, P457, DOI 10.1007/978-3-030-01240-3_28
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang P, 2016, LECT NOTES COMPUT SC, V9911, P370, DOI 10.1007/978-3-319-46478-7_23
   Wang PC, 2018, KNOWL-BASED SYST, V158, P43, DOI 10.1016/j.knosys.2018.05.029
   Xu CY, 2016, IEEE T CIRC SYST VID, V26, P2273, DOI 10.1109/TCSVT.2015.2477937
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang PF, 2018, LECT NOTES COMPUT SC, V11213, P136, DOI 10.1007/978-3-030-01240-3_9
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhao R, 2019, IEEE I CONF COMP VIS, P6881, DOI 10.1109/ICCV.2019.00698
   Zhu Jiagang, 2018, ACTION MACHINE RETHI
   Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316
NR 88
TC 9
Z9 9
U1 2
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 120
DI 10.1145/3450410
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800004
DA 2024-07-18
ER

PT J
AU Dhiman, C
   Vishwakarma, DK
   Agarwal, P
AF Dhiman, Chhavi
   Vishwakarma, Dinesh Kumar
   Agarwal, Paras
TI Part-wise Spatio-temporal Attention Driven CNN-based 3D Human Action
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Attention; human action recognition; inception; residues; skeleton;
   spatio-temporal action representation
ID ENSEMBLE
AB Recently, human activity recognition using skeleton data is increasing due to its ease of acquisition and finer shape details. Still, it suffers from a wide range of intra-class variation, inter-class similarity among the actions and view variation due to which extraction of discriminative spatial and temporal features is still a challenging problem. In this regard, we present a novel Residual Inception Attention Driven CNN (RIAC-Net) Network, which visualizes the dynamics of the action in a part-wise manner. The complete skeletonis partitioned into five key parts: Head to Spine, Left Leg, Right Leg, Left Hand, Right Hand. For each part, a Compact Action Skeleton Sequence (CASS) is defined. Part-wise skeleton-based motion dynamics highlights discriminative local features of the skeleton that helps to overcome the challenges of inter-class similarity and intra-class variation with improved recognition performance. The RIAC-Net architecture is inspired by the concept of inception-residual representation that unifies the Attention Driven Residues (ADR) with inception-based Spatio-Temporal Convolution Features (STCF) to learn efficient salient action features. An ablation study is also carried out to analyze the effect of ADR over simple residue-based action representation. The robustness of the proposed framework is evaluated by performing an extensive experiment on four challenging datasets: UT Kinect Action 3D, Florence 3D action, MSR Daily Action3D, and NTU RGB-D datasets, which consistently demonstrate the superiority of the proposed method over other state-of-the-art methods.
C1 [Dhiman, Chhavi] Delhi Technol Univ, Dept Elect & Commun Engn, Delhi 110042, India.
   [Vishwakarma, Dinesh Kumar] Delhi Technol Univ, Dept Informat Technol, Delhi 110092, India.
   [Agarwal, Paras] Oyo Rooms Bangalore, Bengaluru, Karnataka, India.
   [Agarwal, Paras] Oyo Rooms Bengaluru, Bengaluru 560016, Karnataka, India.
C3 Delhi Technological University; Delhi Technological University
RP Dhiman, C (corresponding author), Delhi Technol Univ, Dept Elect & Commun Engn, Delhi 110042, India.
EM chhavi.dhiman@dtu.ac.in; dinesh@dtu.ac.in; paraspro2020@gmail.com
RI VISHWAKARMA, DINESH/ABK-7887-2022; VISHWAKARMA, DINESH KUMAR/L-3815-2018
OI VISHWAKARMA, DINESH KUMAR/0000-0002-1026-0047; Dhiman,
   Chhavi/0000-0002-3401-596X
CR [Anonymous], 2011, C NEUR INF PROC SYST
   Baradel F., 2018, BRIT MACH VIS C BMVC
   Baradel F, 2017, IEEE INT CONF COMP V, P604, DOI 10.1109/ICCVW.2017.77
   Ben Amor B, 2016, IEEE T PATTERN ANAL, V38, P1, DOI 10.1109/TPAMI.2015.2439257
   Ben Tamou A, 2017, INT J PATTERN RECOGN, V31, DOI 10.1142/S0218001417500082
   Bengio Y., 2014, TECHNICAL REPORT
   Britz D., 2017, ARXIV
   Caruana R, 2001, ADV NEUR IN, V13, P402
   Chen C., 2015, WINT C APPL COMP VIS
   Chen HZ, 2016, PATTERN RECOGN, V55, P148, DOI 10.1016/j.patcog.2016.01.020
   Cho S., 2020, WINT C APPL COMP VIS
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Dhiman C, 2020, IEEE T IMAGE PROCESS, V29, P3835, DOI 10.1109/TIP.2020.2965299
   Dhiman C, 2019, ENG APPL ARTIF INTEL, V77, P21, DOI 10.1016/j.engappai.2018.08.014
   Du Y., 2015, AS C PATT REC ACPR
   Du Y, 2016, IEEE T IMAGE PROCESS, V25, P3010, DOI 10.1109/TIP.2016.2552404
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Ghorbel E, 2020, IEEE SIGNAL PROC LET, V27, P580, DOI 10.1109/LSP.2020.2983901
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hou YH, 2018, IEEE T CIRC SYST VID, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Hussein M. E., 2013, INT JOINT C ART INT
   Pham HH, 2018, COMPUT VIS IMAGE UND, V170, P51, DOI 10.1016/j.cviu.2018.03.003
   Jin S. Y., 2012, IEEE INT C COMP VIS
   Ke Jina, 2017, The Journal of Engineering, DOI 10.1049/joe.2016.0330
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim T.-K., 2017, PERIODIC STRUCTURES
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Koniusz P., 2016, EUR C COMP VIS ECCV
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li Cheng, 2018, Hip Pelvis, V30, P138, DOI 10.5371/hp.2018.30.3.138
   Li CK, 2017, IEEE INT CONF MULTI
   Li G, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/7952974
   Li K, 2014, IEEE T PATTERN ANAL, V36, P1644, DOI 10.1109/TPAMI.2013.2297321
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Lia J., 2019, PATTERN RECOG, V98
   Lillo I, 2016, PROC CVPR IEEE, P1981, DOI 10.1109/CVPR.2016.218
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Lohit S., 2019, IEEE C COMP VIS PATT
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luvizon DC, 2017, PATTERN RECOGN LETT, V99, P13, DOI 10.1016/j.patrec.2017.02.001
   Mnih V., 2014, INT ATIONAL C NEURAL
   Papadopoulos K., 2019, ARXIV191209745
   Park E, 2016, IEEE WINT CONF APPL
   Pham H. H., 2018, ARXIV180707033V1 CSC
   Rhif M, 2018, INT C PATT RECOG, P3427, DOI 10.1109/ICPR.2018.8546027
   Salakhutdinov, 2015, ARXIV151104119
   Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shen T, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4345
   Shi L., 2019, ARXIV191206971CSCV
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Singh K, 2020, NEUROCOMPUTING, V371, P188, DOI 10.1016/j.neucom.2019.08.059
   Slama R, 2015, PATTERN RECOGN, V48, P556, DOI 10.1016/j.patcog.2014.08.011
   Snoek C. G., 2005, 13 ACM INT C MULT
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Sun J., 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Thakkar K. C., 2018, PRAC BRIZ MACH VIS C, P270
   Thien HT, 2020, INFORM SCIENCES, V513, P112, DOI 10.1016/j.ins.2019.10.047
   Tu JH, 2018, IEEE INT CON MULTI
   Vaswani A, 2017, ADV NEUR IN, V30
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Vishwakarma DK, 2019, VISUAL COMPUT, V35, P1595, DOI 10.1007/s00371-018-1560-4
   Wang C., 2016, IEEE C COMP VIS PATT
   Wang H., 2017, UNF VOIC MAIN KUCH B
   Wang HS, 2018, IEEE T IMAGE PROCESS, V27, P4382, DOI 10.1109/TIP.2018.2837386
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Weng J., 2017, C COMP VIS PATT REC
   Wu D, 2014, PROC INT CONF RECON
   Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang S., 2017, IEEE WINTER C APPL C
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 82
TC 29
Z9 29
U1 1
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 86
DI 10.1145/3441628
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400010
OA Bronze
DA 2024-07-18
ER

PT J
AU Ji, WT
   Wang, RL
AF Ji, Wanting
   Wang, Ruili
TI A Multi-instance Multi-label Dual Learning Approach for Video Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep neural networks; Dual learning; Multiple instance learning;
   Multi-media processing; Video captioning
ID LANGUAGE
AB Video captioning is a challenging task in the field of multimedia processing, which aims to generate informative natural language descriptions/captions to describe video contents. Previous video captioning approaches mainly focused on capturing visual information in videos using an encoder-decoder structure to generate video captions. Recently, a new encoder-decoder-reconstructor structure was proposed for video captioning, which captured the information in both videos and captions. Based on this, this article proposes a novel multi-instance multi-label dual learning approach (MIMLDL) to generate video captions based on the encoder-decoder-reconstructor structure. Specifically, MIMLDL contains two modules: caption generation and video reconstruction modules. The caption generation module utilizes a lexical fully convolutional neural network (Lexical FCN) with a weakly supervised multi-instance multi-label learning mechanism to learn a translatable mapping between video regions and lexical labels to generate video captions. Then the video reconstruction module synthesizes visual sequences to reproduce raw videos using the outputs of the caption generation module. A dual learning mechanism fine-tunes the two modules according to the gap between the raw and the reproduced videos. Thus, our approach can minimize the semantic gap between raw videos and the generated captions by minimizing the differences between the reproduced and the raw visual sequences. Experimental results on a benchmark dataset demonstrate that MIMLDL can improve the accuracy of video captioning.
C1 [Ji, Wanting; Wang, Ruili] Massey Univ, Sch Nat & Computat Sci, Auckland, New Zealand.
C3 Massey University
RP Wang, RL (corresponding author), Massey Univ, Sch Nat & Computat Sci, Auckland, New Zealand.
EM wantingji@outlook.com; ruili.wang@massey.ac.nz
CR Artetxe M., 2018, ICLR POSTER, P1
   Banerjee S., 2005, P WORKSHOP INTRINSIC, P65
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   He D, 2016, ADV NEUR IN, V29
   Heckerman D., 1990, Proc. of the 5th Annual Conf. on Uncertainty in Artificial Intell, P163, DOI DOI 10.1016/B978-0-444-88738-2.50020-8
   Hendricks LA, 2016, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2016.8
   Kim T, 2017, PR MACH LEARN RES, V70
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Lample Guillaume, 2018, P 6 INT C LEARN REPR
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu XW, 2014, IEEE T NEUR NET LEAR, V25, P1083, DOI 10.1109/TNNLS.2013.2287275
   Liu Z., 2019, AS C PATT REC, P74
   Luo P, 2017, IEEE I CONF COMP VIS, P2737, DOI 10.1109/ICCV.2017.296
   Ma JB, 2019, MULTIMED TOOLS APPL, V78, P29509, DOI 10.1007/s11042-018-7142-7
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Shamsolmoali P, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3355612
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Tang C, 2021, IEEE T MULTIMEDIA, V23, P624, DOI 10.1109/TMM.2020.2985541
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   Tian Y, 2019, J ARTIF INTELL RES, V64, P181, DOI 10.1613/jair.1.11338
   Toutanova K, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P252, DOI 10.3115/1073445.1073478
   Vaswani A, 2017, ADV NEUR IN, V30
   Venugopalan S., 2015, P ANN C N AM CHAPT A, P1494, DOI 10.3115/v1/N15-1173
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang AR, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115932
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Wang Y., 2019, INT C LEARN REPR, P1
   Wang YX, 2018, AAAI CONF ARTIF INTE, P5561
   Wu J, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3336495
   Wu YR, 2020, NEURAL COMPUT APPL, V32, P14533, DOI 10.1007/s00521-019-04609-8
   Wu Z., 2017, FRONTIERS MULTIMEDIA, P3, DOI DOI 10.1145/3122865.3122867
   Xia YC, 2018, PR MACH LEARN RES, V80
   Xia YC, 2017, PR MACH LEARN RES, V70
   Xia YC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3112
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang L, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3386725
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang CY, 2016, INT C PATT RECOG, P2924, DOI 10.1109/ICPR.2016.7900081
   Zhang XY, 2020, AAAI CONF ARTIF INTE, V34, P12886
   Zhao W, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P29, DOI 10.1145/3132847.3132920
   Zhao Z., 2019, P INT C LEARNING REP, P1
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 50
TC 18
Z9 19
U1 2
U2 35
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 72
DI 10.1145/3446792
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100014
OA Bronze
DA 2024-07-18
ER

PT J
AU Báez-Suárez, A
   Shah, N
   Nolazco-Flores, JA
   Huang, SHS
   Gnawali, O
   Shi, WD
AF Baez-Suarez, Abraham
   Shah, Nolan
   Nolazco-Flores, Juan Arturo
   Huang, Shou-Hsuan S.
   Gnawali, Omprakash
   Shi, Weidong
TI SAMAF: Sequence-to-sequence Autoencoder Model for Audio Fingerprinting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; sequence-to-sequence autoencoder; audio fingerprinting;
   audio identification
AB Audio fingerprinting techniques were developed to index and retrieve audio samples by comparing a content-based compact signature of the audio instead of the entire audio sample, thereby reducing memory and computational expense. Different techniques have been applied to create audio fingerprints; however, with the introduction of deep learning, new data-driven unsupervised approaches are available. This article presents Sequence-to-Sequence Autoencoder Model for Audio Fingerprinting (SAMAF), which improved hash generation through a novel loss function composed of terms: Mean Square Error, minimizing the reconstruction error; Hash Loss, minimizing the distance between similar hashes and encouraging clustering; and Bitwise Entropy Loss, minimizing the variation inside the clusters. The performance of the model was assessed with a subset of VoxCelebl dataset, a"speech in the wild" dataset. Furthermore, the model was compared against three baselines: Dejavu, a Shazam-like algorithm; Robust Audio Fingerprinting System (RAFS), a Bit Error Rate (BER) methodology robust to time-frequency distortions and coding/decoding transformations; and Panako, a constellation-based algorithm adding time-frequency distortion resilience. Extensive empirical evidence showed that our approach outperformed all the baselines in the audio identification task and other classification tasks related to the attributes of the audio signal with an economical hash size of either 128 or 256 bits for one second of audio.
C1 [Baez-Suarez, Abraham; Nolazco-Flores, Juan Arturo] ITESM, Comp Sci Dept, Av Eugenio Garza Sada 2501 Sur, Monterrey 64849, Nuevo Leon, Mexico.
   [Baez-Suarez, Abraham; Shah, Nolan; Huang, Shou-Hsuan S.; Gnawali, Omprakash; Shi, Weidong] Univ Houston, Dept Comp Sci, 4800 Calhoun Rd, Houston, TX 77204 USA.
C3 Tecnologico de Monterrey; University of Houston System; University of
   Houston
RP Báez-Suárez, A (corresponding author), ITESM, Comp Sci Dept, Av Eugenio Garza Sada 2501 Sur, Monterrey 64849, Nuevo Leon, Mexico.; Báez-Suárez, A (corresponding author), Univ Houston, Dept Comp Sci, 4800 Calhoun Rd, Houston, TX 77204 USA.
EM basuam@gmail.com; nshah10@uh.edu; jnolazco@itesm.mx; s_huang@cs.uh.edu;
   gnawali@cs.uh.edu; wshi3@central.uh.edu
RI Nolazco-Flores, Juan Arturo/GLU-4720-2022; Gnawali,
   Omprakash/AAG-4531-2020
OI Nolazco-Flores, Juan Arturo/0000-0002-4187-9352; Gnawali,
   Omprakash/0000-0003-2649-6035; Baez Suarez, Abraham/0000-0001-8729-0781;
   Huang, Shou-Hsuan Stephen/0000-0002-9428-4301
FU Department of Homeland Security (DHS) [D15PC00185]; Mexican National
   Council for Science and Technology (CONACYT) scholarship [328083]; North
   Atlantic Treaty Organization (NATO) Science for Peace and Security
   Program [G4919]; AWS Cloud Credits for Research; University of Houston
   I2C Lab
FX This work has been funded by the Department of Homeland Security (DHS)
   under Grant No. D15PC00185 and in part by the Mexican National Council
   for Science and Technology (CONACYT) scholarship 328083, and also in
   part by the North Atlantic Treaty Organization (NATO) Science for Peace
   and Security Program under Grant No. G4919. This work has also been
   supported by the AWS Cloud Credits for Research
   (https://aws.amazon.com/research-credits/) and the University of Houston
   I2C Lab (https://i2c.cs.uh.edu/).All statements of facts, opinions,
   and/or conclusions contained herein are those of the authors and should
   not be construed as representing the official views or policies of the
   sponsors.
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   Anguera X., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P455, DOI 10.1109/ICME.2012.137
   [Anonymous], 2016, P 24 ACM INT C MULT
   [Anonymous], 2017, P DCASE 2017
   [Anonymous], 2013, P 26 INT C NEUR INF
   Arzt Andreas, 2012, P 13 INT S MUS INF R
   Bagwell C., 2015, SOX SOUND EXCHANGE
   Baluja Shumeet, 2007, P INT C AC SPEECH SI
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   BROWN JC, 1992, J ACOUST SOC AM, V92, P2698, DOI 10.1121/1.404385
   Burges CJC, 2003, IEEE T SPEECH AUDI P, V11, P165, DOI 10.1109/TSA.2003.811538
   Cano P, 2005, J VLSI SIG PROC SYST, V41, P271, DOI 10.1007/s11265-005-4151-3
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Cho K., 2014, ARXIV14061078
   Chung Y.-A., 2016, Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Drevo W., 2013, AUDIO FINGERPRINTING
   Fan Y, 2016, 2016 4TH INTL CONF ON APPLIED COMPUTING AND INFORMATION TECHNOLOGY/3RD INTL CONF ON COMPUTATIONAL SCIENCE/INTELLIGENCE AND APPLIED INFORMATICS/1ST INTL CONF ON BIG DATA, CLOUD COMPUTING, DATA SCIENCE & ENGINEERING (ACIT-CSII-BCD), P363, DOI [10.1109/ACIT-CSII-BCD.2016.77, 10.1109/ACIT-CSII-BCD.2016.076]
   Gaoy Jinyang, 2014, P INT C MAN DAT SIGM
   Gupta V, 2010, INT CONF ACOUST SPEE, P261, DOI 10.1109/ICASSP.2010.5495963
   Haitsma J, 2002, ISMIR 2002 3 INT C M
   Henaff Mikael., 2011, INT SOC MUSIC INFORM, P681
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hsieh Che-Jen, 2007, P INT C INT INF HID
   Kereliuk C, 2015, IEEE T MULTIMEDIA, V17, P2059, DOI 10.1109/TMM.2015.2478068
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li Hanchao, 2016, P IEEE INT C E BUS E
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Lyons J., 2017, python_speech_features 0.6
   Nagrani A., 2017, P C INT SPEECH COMM
   Ouali Chahid, 2015, P IEEE INT S MULT IS
   Ozer Hamza, 2004, P EUR SIGN PROC C EU
   Park Y, 2015, PROC VLDB ENDOW, V9, P144
   Petetin Yohan, 2015, P EUR SIGN PROC C EU
   Roopalakshmi R, 2015, SIGNAL IMAGE VIDEO P, V9, P201, DOI 10.1007/s11760-013-0424-7
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Six Joren, 2014, P C INT SOC MUS INF
   Six Joren, 2014, P 53 AUD ENG SOC C A
   Sonnleitner R, 2016, IEEE-ACM T AUDIO SPE, V24, P409, DOI 10.1109/TASLP.2015.2509248
   Sutskever I., 2014, ADV NEURAL INFORM PR, V4, P3104, DOI DOI 10.5555/2969033.2969173
   Wang A, 2003, ISMIR
   Wang DW, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P128, DOI 10.1109/ICIT.2016.7474738
NR 47
TC 4
Z9 4
U1 1
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 43
DI 10.1145/3380828
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600004
DA 2024-07-18
ER

PT J
AU Noori, FM
   Riegler, M
   Uddin, MZ
   Torresen, J
AF Noori, Farzan Majeed
   Riegler, Michael
   Uddin, Md Zia
   Torresen, Jim
TI Human Activity Recognition from Multiple Sensors Data Using Multi-fusion
   Representations and CNNs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Data fusion; multimodal sensors; deep learning; activity recognition;
   CNN
ID RECURRENCE PLOTS; ACCELEROMETER
AB With the emerging interest in the ubiquitous sensing field, it has become possible to build assistive technologies for persons during their daily life activities to provide personalized feedback and services. For instance, it is possible to detect an individual's behavioral pattern (e.g., physical activity, location, and mood) by using sensors embedded in smart-watches and smartphones. The multi-sensor environments also come with some challenges, such as how to fuse and combine different sources of data. In this article, we explore several methods of fusion for multi-representations of data from sensors. Furthermore, multiple representations of sensor data were generated and then fused using data-level, feature-level, and decision-level fusions. The presented methods were evaluated using three publicly available human activity recognition (HAR) datasets. The presented approaches utilize Deep Convolutional Neural Networks (CNNs). A generic architecture for fusion of different sensors is proposed. The proposed method shows promising performance, with the best results reaching an overall accuracy of 98.4% for the Context-Awareness via Wrist-Worn Motion Sensors (HANDY) dataset and 98.7% for the Wireless Sensor Data Mining (WISDM version 1.1) dataset. Both results outperform previous approaches.
C1 [Noori, Farzan Majeed; Uddin, Md Zia; Torresen, Jim] Univ Oslo, Dept Informat, Postboks 1080,Blindern 0316,Gaustadalleen 23 B, Oslo, Norway.
   [Riegler, Michael] Simula Metropolitan Ctr Digitalizat, Postboks 1080,Blindern 0316,Gaustadalleen 23 B, Oslo, Norway.
   [Riegler, Michael] Kristiania Univ Coll, Postboks 1080,Blindern 0316,Gaustadalleen 23 B, Oslo, Norway.
   [Torresen, Jim] Univ Oslo, RITMO, Postboks 1080,Blindern 0316,Gaustadalleen 23 B, Oslo, Norway.
C3 University of Oslo; Kristiania University College; University of Oslo
RP Noori, FM (corresponding author), Univ Oslo, Dept Informat, Postboks 1080,Blindern 0316,Gaustadalleen 23 B, Oslo, Norway.
EM farzanmn@ifi.uio.no; michael@simula.no; uddin@sintef.no;
   jimtoer@ifi.uio.no
RI Jianbin, Lu/JOZ-0891-2023; Riegler, Michael A/E-5443-2015; Noori, Farzan
   Majeed/CAE-6023-2022; Noori, Farzan Majeed/L-1207-2019; Noori, Farzan
   Majeed/M-9609-2017
OI Noori, Farzan Majeed/0000-0003-2256-3835; Uddin, Md
   Zia/0000-0002-5215-1834
FU Research Council of Norway (RCN) as a part of the Multimodal Elderly
   Care systems (MECS) [247697]; INTROducing Mental health through Adaptive
   Technology (INTROMAT) project [259293]; Vulnerability in the Robot
   Society (VIROS) project [288285]; Centres of Excellence scheme [262762]
FX This work is partially supported by The Research Council of Norway (RCN)
   as a part of the Multimodal Elderly Care systems (MECS) project under
   Grant Agreement No. 247697, INTROducing Mental health through Adaptive
   Technology (INTROMAT) project under Grant Agreement No. 259293,
   Vulnerability in the Robot Society (VIROS) project under Grant Agreement
   No. 288285, and through its Centres of Excellence scheme, Project No.
   262762.
CR Açici K, 2018, DATA, V3, DOI 10.3390/data3030024
   Ahmad Z, 2018, IEEE INT SYM MULTIM, P223, DOI 10.1109/ISM.2018.000-2
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2012, PERVASIVE MOBILE COM
   [Anonymous], 2011, P ICML
   [Anonymous], 2012, ARXIV12066869
   [Anonymous], 2016, IEEE J BIOMEDICAL HL
   [Anonymous], 2016, ARXIV160102970
   Anwer RM, 2018, ISPRS J PHOTOGRAMM, V138, P74, DOI 10.1016/j.isprsjprs.2018.01.023
   Banos O, 2012, SENSORS-BASEL, V12, P8039, DOI 10.3390/s120608039
   Bulling A, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2499621
   Cao L, 2018, J PARALLEL DISTR COM, V118, P67, DOI 10.1016/j.jpdc.2017.05.007
   Catal C, 2015, APPL SOFT COMPUT, V37, P1018, DOI 10.1016/j.asoc.2015.01.025
   Dehzangi O, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122735
   Eckmann J.-P., 1995, WORLD SCI SERIES N A, V16, P441
   Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446
   Elmenreich W., 2002, An introduction to sensor fusion, V502
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Frogner JI, 2019, PROCEEDINGS OF THE 4TH INTERNATIONAL WORKSHOP ON MULTIMEDIA FOR PERSONAL HEALTH & HEALTH CARE (HEALTHMEDIA'19), P9, DOI 10.1145/3347444.3356238
   Garcia-Ceja E, 2018, PROCEDIA COMPUT SCI, V130, P157, DOI 10.1016/j.procs.2018.04.025
   Garcia-Ceja E, 2018, INFORM FUSION, V40, P45, DOI 10.1016/j.inffus.2017.06.004
   Gomez SR, 2014, IEEE CONF VIS ANAL, P63, DOI 10.1109/VAST.2014.7042482
   Guo HD, 2016, UBICOMP'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P1112, DOI 10.1145/2971648.2971708
   Guo HD, 2016, INT J COMMUN SYST, V29, P2403, DOI 10.1002/dac.3010
   Guo M, 2019, IEEE T HUM-MACH SYST, V49, P105, DOI 10.1109/THMS.2018.2884717
   Gupta P, 2014, IEEE T BIO-MED ENG, V61, P1780, DOI 10.1109/TBME.2014.2307069
   Hassan MM, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-0948-z
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hossain MS, 2017, IEEE SYST J, V11, P118, DOI 10.1109/JSYST.2015.2470644
   Huynh T, 2005, 2005 INTERNATIONAL SYMPOSIUM ON EMPIRICAL SOFTWARE ENGINEERING (ISESE), PROCEEDINGS, P157
   Ignatov AD, 2016, MULTIMED TOOLS APPL, V75, P7257, DOI 10.1007/s11042-015-2643-0
   Iwanski JS, 1998, CHAOS, V8, P861, DOI 10.1063/1.166372
   Jalalvand A, 2019, J SPORT REHABIL, V28, P847, DOI 10.1123/jsr.2018-0153
   Kai KZ, 2008, PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING (UBICOMP 2008), P20, DOI 10.1145/1409635.1409639
   Khaleghi B, 2013, INFORM FUSION, V14, P28, DOI 10.1016/j.inffus.2011.08.001
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwapisz JR., 2011, ACM SIGKDD EXPLORATI, V12, P74, DOI [DOI 10.1145/1964897.1964918, 10.1145/1964897.1964918]
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li M, 2010, IEEE T NEUR SYS REH, V18, P4
   Margarito J, 2016, IEEE T BIO-MED ENG, V63, P788, DOI 10.1109/TBME.2015.2471094
   Marwan N, 2007, PHYS REP, V438, P237, DOI 10.1016/j.physrep.2006.11.001
   Menhour I, 2018, INT CONF MULTIMED, P110
   Pham M, 2019, IEEE T AUTOM SCI ENG, V16, P339, DOI 10.1109/TASE.2018.2874487
   Münzner S, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (ISWC 17), P158, DOI 10.1145/3123021.3123046
   Neverova N, 2016, IEEE ACCESS, V4, P1810, DOI 10.1109/ACCESS.2016.2557846
   Noori FM, 2019, LECT NOTES COMPUT SC, V11482, P299, DOI 10.1007/978-3-030-20205-7_25
   Nweke HF, 2019, INFORM FUSION, V46, P147, DOI 10.1016/j.inffus.2018.06.002
   Plotz T., 2011, Twenty-second international joint conference on artificial intelligence
   Qiu S, 2018, INFORM FUSION, V39, P108, DOI 10.1016/j.inffus.2017.04.006
   Sani S, 2017, LECT NOTES ARTIF INT, V10412, P469, DOI 10.1007/978-3-319-63558-3_40
   Sarcevic P, 2019, J AMB INTEL HUM COMP, V10, P89, DOI 10.1007/s12652-017-0606-1
   Shoaib M, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16040426
   Shoaib M, 2015, SENSORS-BASEL, V15, P2059, DOI 10.3390/s150102059
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Soleymani M, 2018, INT J MULTIMED INF R, V7, P29, DOI 10.1007/s13735-018-0150-6
   Yang JB, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3995
   Yu HB, 2019, EURASIP J ADV SIG PR, DOI 10.1186/s13634-019-0612-x
   Zeng M, 2014, 2014 6TH INTERNATIONAL CONFERENCE ON MOBILE COMPUTING, APPLICATIONS AND SERVICES (MOBICASE), P197, DOI 10.4108/icst.mobicase.2014.257786
   Zhu C, 2012, IEEE T BIOMED ENG, V59, P2012, DOI [10.1109/TBME.2190602, DOI 10.1109/TBME.2190602]
NR 60
TC 31
Z9 31
U1 3
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 45
DI 10.1145/3377882
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600006
DA 2024-07-18
ER

PT J
AU Illahi, GK
   Van Gemert, T
   Siekkinen, M
   Masala, E
   Oulasvirta, A
   Ylä-Jääski, A
AF Illahi, Gazi Karam
   Van Gemert, Thomas
   Siekkinen, Matti
   Masala, Enrico
   Oulasvirta, Antti
   Yla-Jaaski, Antti
TI Cloud Gaming with Foveated Video Encoding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cloud gaming; foveated video encoding; adaptive bitrate encoding; game
   streaming; gaze-contingent encoding
ID QUALITY; MODEL
AB Cloud gaming enables playing high-end games, originally designed for PC or game console setups, on low-end devices such as netbooks and smartphones, by offloading graphics rendering to GPU-powered cloud servers. However, transmitting the high-resolution video requires a large amount of network bandwidth, even though it is a compressed video stream. Foveated video encoding (FVE) reduces the bandwidth requirement by taking advantage of the non-uniform acuity of human visual system and by knowing where the user is looking. Based on a consumer-grade real-time eye tracker and an open source cloud gaming platform, we provide a cloud gaming FVE prototype that is game-agnostic and requires no modifications to the underlying game engine. In this article, we describe the prototype and its evaluation through measurements with representative games from different genres to understand the effect of parametrization of the FVE scheme on bandwidth requirements and to understand its feasibility from the latency perspective. We also present results from a user study on first-person shooter games. The results suggest that it is possible to find a "sweet spot" for the encoding parameters so the users hardly notice the presence of foveated encoding but at the same time the scheme yields most of the achievable bandwidth savings.
C1 [Illahi, Gazi Karam; Van Gemert, Thomas; Siekkinen, Matti; Yla-Jaaski, Antti] Aalto Univ, Dept Comp Sci, Espoo 02150, Finland.
   [Siekkinen, Matti] Univ Helsinki, Helsinki, Finland.
   [Masala, Enrico] Politecn Torino, Control & Comp Engn Dept, Turin, Italy.
   [Oulasvirta, Antti] Aalto Univ, Dept Commun & Networking, Espoo, Finland.
C3 Aalto University; University of Helsinki; Polytechnic University of
   Turin; Aalto University
RP Illahi, GK (corresponding author), Aalto Univ, Dept Comp Sci, Espoo 02150, Finland.
EM gazi.illahi@aalto.fi; thomas.vangemert@aalto.fi;
   matti.siekkinen@aalto.fi; enrico.masala@polito.it;
   antti.oulasvirta@aalto.fi; antti.yla-jaaski@aalto.fi
RI Oulasvirta, Antti/G-8066-2011; Siekkinen, Matti/H-2447-2018; Yla-Jaaski,
   Antti/G-2484-2013
OI van Gemert, Thomas/0000-0001-7827-3760; Siekkinen,
   Matti/0000-0003-0423-1060; , Gazi Karam Illahi/0000-0001-7976-0261;
   Yla-Jaaski, Antti/0000-0002-2069-1721; Oulasvirta,
   Antti/0000-0002-2498-7837
FU Academy of Finland [297892]; Nokia Center for Advanced Research;
   European Research Council (ERC) under the European Union [637991];
   Academy of Finland (AKA) [297892] Funding Source: Academy of Finland
   (AKA)
FX This work has been supported by the Academy of Finland (grant no.
   297892), the Nokia Center for Advanced Research, and the European
   Research Council (ERC) under the European Union's Horizon 2020 research
   and innovation programme (grant agreement no. 637991).
CR Ahmadi H, 2014, MULTIMEDIA SYST, V20, P485, DOI 10.1007/s00530-014-0381-1
   Akhtar Z, 2017, IEEE ACCESS, V5, P21090, DOI 10.1109/ACCESS.2017.2750918
   Albert R, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/3127589
   [Anonymous], 2013, P 4 INT WORKSHOP PER
   [Anonymous], 2007, INT C ADV COMP ENT T
   [Anonymous], 2016, P 7 INT C MULT SYST
   Bokani Ayub., 2014, Proceedings of the Workshop on Design, Quality and Deployment of Adaptive Video Streaming, VideoNext '14, P45
   Cai W, 2013, INT CONF CLOUD COMP, P72, DOI 10.1109/CloudCom.2013.17
   Cai W, 2014, IEEE INTERNET COMPUT, V18, P12, DOI 10.1109/MIC.2014.22
   Chen K.-T., 2011, P 19 ACM INT C MULT, P1269
   Chen KT, 2014, IEEE T MULTIMEDIA, V16, P480, DOI 10.1109/TMM.2013.2291532
   Chen ZZ, 2010, IEEE T CIRC SYST VID, V20, P806, DOI 10.1109/TCSVT.2010.2045912
   Choy S, 2012, ANN WORK NETW
   Claypool M, 2006, COMMUN ACM, V49, P40, DOI 10.1145/1167838.1167860
   Deber J, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1827, DOI 10.1145/2702123.2702300
   Gibaldi A, 2017, BEHAV RES METHODS, V49, P923, DOI 10.3758/s13428-016-0762-9
   Golja T, 2014, PROC CONF EUR MANAG, P100
   Huang Chun-Ying, 2013, P 4 ACM MULT SYST C, P36, DOI DOI 10.1145/2483977.2483981
   Illahi G., 2017, 2017 IEEE 19 INT WOR, P1
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Jarschel M., 2011, Proceedings of the 2011 Fifth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), P330, DOI 10.1109/IMIS.2011.92
   JVT, 2003, JVTG050D35 ISOIEC MP
   Kämäräinen T, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P88, DOI 10.1145/3083187.3083191
   Kon K, 2016, ANTIBIOTIC RESISTANCE: MECHANISMS AND NEW ANTIMICROBIAL APPROACHES, P1
   Li ZC, 2011, IMAGE VISION COMPUT, V29, P1, DOI 10.1016/j.imavis.2010.07.001
   Lungaro P, 2017, INT WORK QUAL MULTIM
   Lungaro P, 2018, IEEE T VIS COMPUT GR, V24, P1535, DOI 10.1109/TVCG.2018.2794119
   Maintz T., 2005, DIGITAL MED IMAGE PR
   Merritt L., 2006, X264 HIGH PERFORMANC
   Mohammadi IS, 2015, 2015 IEEE INT C MULT, P1
   Mullin Jim, 2001, IHM HCI TUTORIAL 200, P1
   Patil K, 2015, EURASIP J AUDIO SPEE, P1, DOI 10.1186/s13636-015-0070-9
   Pauliks R, 2013, WORLD CONGRESS ON COMPUTER & INFORMATION TECHNOLOGY (WCCIT 2013)
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Rai Yashas, 2017, ELECT IMAG, V14, P104, DOI [10.2352/ISSN.2470-1173.2017.14.HVEI-124, DOI 10.2352/ISSN.2470-1173.2017.14.HVEI-124]
   Rucci M, 2016, VISION RES, V118, P1, DOI 10.1016/j.visres.2015.12.001
   Shanidze N, 2016, J VISION, V16, DOI 10.1167/16.15.23
   Shea R, 2013, IEEE NETWORK, V27, P16, DOI 10.1109/MNET.2013.6574660
   Speedtest by Ookla, 2019, MONTHL COMP INT SPEE
   Swenson R.S., 2006, REV CLIN FUNCTIONAL
   Tobii, 2015, DEV GUID TOB EYEX SD
   Tolia N, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.101
   Wandell B. A., 1995, Foundations of vision, V8
   Wang Z, 2003, IEEE T IMAGE PROCESS, V12, P243, DOI 10.1109/TIP.2003.809015
   Wang Z, 2006, SIGN PROC COMMUN SER, V28, P431
   Zare A., 2016, P 24 ACM INT C MULT, P601, DOI DOI 10.1145/2964284.2967292
NR 46
TC 31
Z9 33
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 7
DI 10.1145/3369110
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100006
DA 2024-07-18
ER

PT J
AU Li, MY
   Zhang, ZY
   Xie, GC
   Yu, J
AF Li, Mengyan
   Zhang, Zhaoyu
   Xie, Guochen
   Yu, Jun
TI A Deep Learning Approach for Face Hallucination Guided by Facial
   Boundary Responses
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face hallucination; face super-resolution; facial boundary response;
   feature maps fusion
ID GENERATIVE ADVERSARIAL NETWORKS; SUPERRESOLUTION
AB Face hallucination is a domain-specific super-resolution (SR) problem of learning a mapping between a low-resolution (LR) face image and its corresponding high-resolution (HR) image. Tremendous progress on deep learning has shown exciting potential for a variety of face hallucination tasks. However, most deep-learningbased methods are limited to handle facial appearance information without paying attention to facial structure priors. In this article, we propose an open sources Boundary-aware Dual-branch Network (BDN) for face hallucination, which simultaneously extracts face features and estimates facial boundary responses from LR inputs, ultimately fusing them to reconstruct HR results. Specifically, we first upsample LR face images to HR feature maps, and then feed the upsampled HR features into a memory unit and an attention unit synchronously to obtain the refined features and predict facial boundary responses. Next, they are fed into a feature map fusion unit to combine facial appearance and structure information by a spatial attention mechanism. Moreover, we employ a series of stacked units to boost performance before recovering HR face images. Finally, a discriminative network is developed to improve visual quality by introducing adversarial learning strategy. Extensive experiments show that the proposed approach achieves superior face hallucination results against the state-of-the-art ones.
C1 [Li, Mengyan; Zhang, Zhaoyu; Xie, Guochen; Yu, Jun] Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230031, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Yu, J (corresponding author), Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230031, Anhui, Peoples R China.
EM limmy@mail.ustc.edu.cn; zzy95@mail.ustc.edu.cn; xiegc@mail.ustc.edu.cn;
   harryjun@ustc.edu.cn
FU National Natural Science Foundation of China [61572450]; USTC Research
   Funds of the Double First-Class Initiative [YD2350002001]
FX This work was supported by the National Natural Science Foundation of
   China (NO.U1736123, NO.61572450), USTC Research Funds of the Double
   First-Class Initiative (YD2350002001).
CR [Anonymous], 2017, P INT C COMP VIS
   Arjovsky M, 2017, arXiv preprint arXiv:1701.07875
   Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019
   Cao Q., 2017, CVPR, P690
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Dogan Berk, 2019, P IEEE C COMP VIS PA
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Huang G.B., 2008, WORKSH FAC REAL LIF
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Ignatov Andrey, 2018, CVPRW, P691
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kolouri S, 2015, PROC CVPR IEEE, P4876, DOI 10.1109/CVPR.2015.7299121
   Kong LC, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3226036
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Li MY, 2018, IEEE IMAGE PROC, P61, DOI 10.1109/ICIP.2018.8451122
   Li XG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282445
   Li YC, 2014, PATTERN RECOGN, V47, P1261, DOI 10.1016/j.patcog.2013.09.012
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liu C, 2001, PROC CVPR IEEE, P192
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu Yongyi, 2018, ECCV
   Ma X, 2010, PATTERN RECOGN, V43, P2224, DOI 10.1016/j.patcog.2009.12.019
   Masi I, 2019, IEEE T PATTERN ANAL, V41, P379, DOI 10.1109/TPAMI.2018.2792452
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tang M, 2018, EXPERT REV ANTICANC, V18, P339, DOI 10.1080/14737140.2018.1444481
   Wang NN, 2014, INT J COMPUT VISION, V106, P9, DOI 10.1007/s11263-013-0645-9
   Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171
   Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227
   Xiao T., 2018, ELEGANT EXCHANGING L, P168
   Yang CY, 2013, PROC CVPR IEEE, P1099, DOI 10.1109/CVPR.2013.146
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yu X., 2017, CVPR
   Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14
   Yu X, 2020, INT J COMPUT VISION, V128, P500, DOI 10.1007/s11263-019-01254-5
   Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101
   Yu X, 2018, IEEE T IMAGE PROCESS, V27, P2747, DOI 10.1109/TIP.2018.2808840
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20
   Yu Xin, 2017, P 31 AAAI C ART INT
   Zhang J, 2018, MEAS SCI REV, V18, P183, DOI 10.1515/msr-2018-0026
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhou EJ, 2015, AAAI CONF ARTIF INTE, P3871
   Zhou JT, 2019, IEEE T INF FOREN SEC, V14, P2537, DOI 10.1109/TIFS.2019.2900907
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37
NR 58
TC 9
Z9 10
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 17
DI 10.1145/3377874
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA OJ1FD
UT WOS:000583712100016
DA 2024-07-18
ER

PT J
AU Li, Y
   Pan, YW
   Yao, T
   Chao, HY
   Rui, Y
   Mei, T
AF Li, Yehao
   Pan, Yingwei
   Yao, Ting
   Chao, Hongyang
   Rui, Yong
   Mei, Tao
TI Learning Click-Based Deep Structure-Preserving Embeddings with Visual
   Attention
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-view embedding; image search; click data; CNN
ID SIMILARITY
AB One fundamental problem in image search is to learn the ranking functions (i.e., the similarity between query and image). Recent progress on this topic has evolved through two paradigms: the text-based model and image ranker learning. The former relies on image surrounding texts, making the similarity sensitive to the quality of textual descriptions. The latter may suffer from the robustness problem when human-labeled query-image pairs cannot represent user search intent precisely. We demonstrate in this article that the preceding two limitations can be well mitigated by learning a cross-view embedding that leverages click data. Specifically, a novel click-based Deep Structure-Preserving Embeddings with visual Attention (DSPEA) model is presented, which consists of two components: deep convolutional neural networks followed by image embedding layers for learning visual embedding, and a deep neural networks for generating query semantic embedding. Meanwhile, visual attention is incorporated at the top of the convolutional neural network to reflect the relevant regions of the image to the query. Furthermore, considering the high dimension of the query space, a new click-based representation on a query set is proposed for alleviating this sparsity problem. The whole network is end-to-end trained by optimizing a large margin objective that combines cross-view ranking constraints with in-view neighborhood structure preservation constraints. On a large-scale click-based image dataset with 11.7 million queries and 1 million images, our model is shown to be powerful for keyword-based image search with superior performance over several state-of-the-art methods and achieves, to date, the best reported NDCG@25 of 52.21%.
C1 [Li, Yehao; Chao, Hongyang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Guangdong, Peoples R China.
   [Pan, Yingwei; Yao, Ting; Mei, Tao] JD AI Res, Beijing, Peoples R China.
   [Rui, Yong] Lenovo, Beijing, Peoples R China.
C3 Sun Yat Sen University; Legend Holdings; Lenovo
RP Yao, T (corresponding author), JD AI Res, Beijing, Peoples R China.
EM yehaoli.sysu@gmail.com; panyw.ustc@gmail.com; tingyao.ustc@gmail.com;
   isschhy@mail.sysu.edu.cn; yongrui@lenovo.com; tmei@jd.com
RI Pan, Yingwei/T-7649-2019; Mei, Tao/GQZ-0596-2022
OI Pan, Yingwei/0000-0002-4344-8898; Mei, Tao/0000-0002-5990-7307
FU NSF of China [61672548, U1611461, 61173081]; Guangzhou Science and
   Technology Program, China [201510010165]
FX This work was partially supported by the NSF of China under grants
   61672548, U1611461, and 61173081, and the Guangzhou Science and
   Technology Program, China, under grant 201510010165.
CR [Anonymous], INT C NEUR INT PROC
   [Anonymous], P 3 TEXT RETR C
   [Anonymous], 2010, P ACM INT C IMAGE VI
   [Anonymous], 2013, P ACM INT C MULT
   [Anonymous], 2018, INT C LEARN REPR
   Bahdanau Dzmitry, 2015, P 2015 INT C LEARN R
   Bai B., 2009, NIPS, P64
   Bai YL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P229, DOI 10.1145/2647868.2656402
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Fang Z, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1321, DOI 10.1145/2505515.2505532
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fukumizu K, 2007, J MACH LEARN RES, V8, P361
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Herbrich R, 2000, ADV NEUR IN, P115
   Hua X.-S., 2013, Proceedings of the 21st ACM International Conference on Multimedia, Oct. 21-25, ACM, P243
   Jain A, 2011, MICROORGANISMS IN INDUSTRY AND ENVIRONMENT: FROM SCIENTIFIC AND INDUSTRIAL RESEARCH TO CONSUMER PRODUCTS, P277
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Li Yehao., 2016, P 24 ACM INT C MULT, P928
   Liu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P955, DOI 10.1145/2733373.2806373
   Melacci S, 2011, J MACH LEARN RES, V12, P1149
   Pan YW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P233, DOI 10.1145/2647868.2656404
   Pan YW, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P717, DOI 10.1145/2600428.2609568
   ROMANO JP, 1990, J AM STAT ASSOC, V85, P686, DOI 10.2307/2290003
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Salton G, 1986, Introduction to Modern Information Retrieval
   Vaswani A., 2017, P ADV NEUR INF PROC, V2017, P5998
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Wu W., 2013, Wsdm 2013 - proceedings of the 6th acm international conference on web search and data mining, P687, DOI [DOI 10.1145/2433396.2433481, 10.1145/2433396.2433481]
   Xie HT, 2019, IEEE T MULTIMEDIA, V21, P1248, DOI 10.1109/TMM.2018.2872898
   Xu K., 2015, COMPUTER SCI, P2048
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang Y, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457456
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao T, 2015, IEEE I CONF COMP VIS, P28, DOI 10.1109/ICCV.2015.12
   Yao T, 2015, PROC CVPR IEEE, P2142, DOI 10.1109/CVPR.2015.7298826
   Yao T, 2013, IEEE T IMAGE PROCESS, V22, P1642, DOI 10.1109/TIP.2012.2236341
   Yao Ting., 2013, Proceedings of the ACM International Conference on Multimedia, P977
   Zhai CX, 2004, ACM T INFORM SYST, V22, P179, DOI 10.1145/984321.984322
   Zhang H, 2017, J CHEM-NY, V2017, DOI 10.1155/2017/4513410
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P3025, DOI 10.1109/TIP.2014.2326010
NR 46
TC 3
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2019
VL 15
IS 3
AR 78
DI 10.1145/3328994
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ6YR
UT WOS:000494301200009
DA 2024-07-18
ER

PT J
AU Ch'ng, E
AF Ch'ng, Eugene
TI Art by Computing Machinery: Is Machine Art Acceptable in the Artworld?
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Machine art; machine artist; machine artworks; art theory; artworlds
AB When does a machine-created work becomes art? What is art? Can machine artworks fit in to the historical and present discourse? Do machine artworks demonstrate creativity, or are they a type of new media from which artists extend their creativity with? Will solely machine-created artworks be acceptable by our artworlds? This article probes these questions by first identifying the frameworks for defining and explaining art and evaluating its suitability for explaining machine artworks. It then explores how artworks have a necessary relationship with their human artists and the wider context of history, institutions, styles, and approaches and with audiences and artworlds. The article then questions whether machines have such a relational context and whether machines will ever live up to our standard of what constitutes an artwork as defined by us or whether machines are good only for assisting creativity. The question of intellectual property, rights, and ownership are also discussed for human machine artworks and purely machine-produced works of art. The article critically assesses the viability of machines as artists as the central question in the historical discourse, extended through art and the artworld and evaluates machine-produced work from such a basis.
C1 [Ch'ng, Eugene] Univ Nottingham Ningbo China, NVIDIA Joint Lab Mixed Real, 199 Taikang East Rd, Ningbo 315100, Zhejiang, Peoples R China.
C3 University of Nottingham Ningbo China
RP Ch'ng, E (corresponding author), Univ Nottingham Ningbo China, NVIDIA Joint Lab Mixed Real, 199 Taikang East Rd, Ningbo 315100, Zhejiang, Peoples R China.
EM eugene.chng@nottingham.edu.cn
RI Ch´ng, Eugene/Q-8277-2019
OI Ch´ng, Eugene/0000-0003-3992-8335
CR [Anonymous], 2005, P 2 AUST C INT ENT
   [Anonymous], 2018, ARTISTES ROBOT
   Bar-Yosef O., 2006, P S
   Beardsley M., 1970, Metaphilosophy, V1, P39, DOI DOI 10.1111/J.1467-9973.1970.TB00784.X
   Beardsley M., 1976, CULTURE ART, p[194, 196]
   Beardsley MonroeC., 1982, The Aesthetic Point of View: Selected Essays, P298
   Becker Howard S., 1982, ART WORLDS
   Bell Clive., 1924, ART
   Bentkowska-Kafel Anna, 2005, COMPUTERS HIST ART S, V1
   Bird J, 2007, DIGIT CREAT, V18, P11, DOI 10.1080/14626260701252368
   Boden, 2005, CREATIVITY HUMAN EVO, P27
   Brown P., 2008, MECH MIND HIST
   Ch'ng E, 2017, PRESENCE-TELEOP VIRT, V26, P157, DOI 10.1162/PRES_a_00291
   COHEN T, 1973, PHILOS REV, V82, P69, DOI 10.2307/2184239
   Collingwood R.G., 1938, PRINCIPLES ART
   Conard NJ, 2009, NATURE, V459, P248, DOI 10.1038/nature07995
   Coremans P., 1949, VANMEEGERENS FAKED V
   Croce Benedetto., 1995, Guide to Aesthetics
   DANTO A, 1964, J PHILOS, V61, P571, DOI 10.2307/2022937
   Danto A. C., 1997, BRILLO BOX END ART C
   Danto ArthurC., 1974, The Journal of Aesthetics and Art Criticism, V33, P139, DOI DOI 10.2307/429082
   Dean JT, 2003, J AESTHET ART CRITIC, V61, P29, DOI 10.1111/1540-6245.00089
   Dewey J., 1938, ART EXPERIENCE
   DICKIE G, 1969, AM PHILOS QUART, V6, P253
   Dickie G., 1987, ART CIRCLE
   Dickie G., 2000, I THEORY ART
   Dutton D., 2007, ARGUING ART CONT PHI, P448
   Edmonds E., 2011, Interactive Art. Em E. Edmonds, L. Candy, Interacting: Art, P18
   Floss H., 2007, CHEMINS ART AURIGNAC
   Freeland C., 2003, ART THEORY VERY SHOR
   Gaut B., 2000, ART CLUSTER CONCEPT
   Lee H.D. P., 2003, The republic
   Lu L., 2010, Art Education, V63, P19
   Mellars P., 2007, Rethinking the Human Revolution
   Mithen S., 2005, CREAT HUM EVOL PREHI, P116
   Prophet J, 2001, LEONARDO, V34, P309, DOI 10.1162/00240940152549221
   Rank O., 1932, ART ARTIST
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Thornton Sarah., 2012, 7 DAYART WORLD
   Tolstoy L., 1962, What is art? And essays on art
   Turner C., 2017, TELEGR
   Weitz Morris, 1956, Journal of Aesthetics and Art Criticism, V15, P27, DOI DOI 10.2307/427491
NR 42
TC 10
Z9 10
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 59
DI 10.1145/3326338
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900012
DA 2024-07-18
ER

PT J
AU Li, Q
   Xiao, F
   An, L
   Long, XZ
   Sun, XC
AF Li, Qun
   Xiao, Fu
   An, Le
   Long, Xianzhong
   Sun, Xiaochuan
TI Semantic Concept Network and Deep Walk-based Visual Question Answering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE VQA; semantic concept network; word activation forces; deep walk;
   low-dimensional feature representation
AB Visual Question Answering (VQA) is a hot-spot in the intersection of computer vision and natural language processing research and its progress has enabled many in high-level applications. This work aims to describe a novel VQA model based on semantic concept network construction and deep walk. Extracting visual image semantic representation is a significant and effective method for spanning the semantic gap. Moreover, current research has shown that co-occurrence patterns of concepts can enhance semantic representation. This work is motivated by the challenge that semantic concepts have complex interrelations and the relationships are similar to a network. Therefore, we construct a semantic concept network adopted by leveraging Word Activation Forces (WAFs), and mine the co-occurrence patterns of semantic concepts using deep walk. Then the model performs polynomial logistic regression on the basis of the extracted deep walk vector along with the visual image feature and question feature. The proposed model effectively integrates visual and semantic features of the image and natural language question. The experimental results show that our algorithm outperforms competitive baselines on three benchmark image QA datasets. Furthermore, through experiments in image annotation refinement and semantic analysis on pre-labeled LabelMe dataset, we test and verify the effectiveness of our constructed concept network for mining concept co-occurrence patterns, sensible concept clusters, and hierarchies.
C1 [Li, Qun; Xiao, Fu; Long, Xianzhong] Nanjing Univ Posts & Telecommun, Sch Comp Sci & Technol, 9 Wenyuan Rd, Nanjing 210023, Jiangsu, Peoples R China.
   [An, Le] Huazhong Univ Sci & Technol, Sch Automat, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Sun, Xiaochuan] North China Univ Sci & Technol, Sch Informat Engn, Tangshan 063009, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Huazhong University of
   Science & Technology; North China University of Science & Technology
RP Li, Q; Xiao, F (corresponding author), Nanjing Univ Posts & Telecommun, Sch Comp Sci & Technol, 9 Wenyuan Rd, Nanjing 210023, Jiangsu, Peoples R China.
EM liqun@njupt.edu.cn; xiaof@njupt.edu.cn; lan004@ucr.edu;
   longxianzhong85@163.com; sunxiaochuan@njupt.edu.cn
RI Li, Qun/JEP-3834-2023
OI Li, Qun/0000-0002-8034-6030
FU National Natural Science Foundation of China [61401218, 61571238];
   Nature Science Foundation of Jiangsu for Distinguished Young Scientist
   [BK20170039]; Natural Science Foundation of Jiangsu Province
   [BK20150856]; Postdoctoral Research Plan of Jiangsu Province [1701167B];
   Postdoctoral Science Foundation of China [2017M621795]; Nanjing
   University of Posts and Telecommunications Program [NY218026]
FX This work was partially supported by the National Natural Science
   Foundation of China under Grants No. 61401218 and No. 61571238, the
   Nature Science Foundation of Jiangsu for Distinguished Young Scientist
   under Grant No. BK20170039, the Natural Science Foundation of Jiangsu
   Province under Grant No. BK20150856, the Postdoctoral Research Plan of
   Jiangsu Province under Grant No. 1701167B, the Postdoctoral Science
   Foundation of China under Grant No. 2017M621795, and the Nanjing
   University of Posts and Telecommunications Program under Grant No.
   NY218026.
CR Agrawal AM, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON RESEARCH IN INTELLIGENT AND COMPUTING IN ENGINEERING (RICE III)
   Anderson RA, 2018, EXPERT REV ANTICANC, V18, P1, DOI 10.1080/14737140.2018.1404453
   [Anonymous], Simple baseline for visual question answering
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Choi MJ, 2010, PROC CVPR IEEE, P129, DOI 10.1109/CVPR.2010.5540221
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Fellbaum C., 2005, Encyclopedia of Language & Linguistics, V2nd, P665
   Feng LN, 2016, IEEE T PATTERN ANAL, V38, P785, DOI 10.1109/TPAMI.2015.2469281
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Guo Jun, ACTIVATION FORCE BAS
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P5814, DOI 10.1109/TIP.2016.2614132
   Hong RC, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2043612.2043613
   Hong RC, 2014, IEEE T CYBERNETICS, V44, P669, DOI 10.1109/TCYB.2013.2265601
   Ilievski Ilija., 2016, A focused dynamic attention model for visual question answering
   Imran SA, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P1193, DOI 10.1109/ROBIO.2013.6739626
   Kafle K, 2017, COMPUT VIS IMAGE UND, V163, P3, DOI 10.1016/j.cviu.2017.06.005
   Kafle K, 2016, PROC CVPR IEEE, P4976, DOI 10.1109/CVPR.2016.538
   Kim J, 2017, J SEMICOND TECH SCI, V17, P1, DOI 10.5573/JSTS.2017.17.1.001
   Kiros R., 2015, ADV NEURAL INFORM PR, P3294, DOI DOI 10.48550/ARXIV.1506.06726
   Kolluru S. K., 2017, COGNITIVECAM VISUAL, P85, DOI [10.1007/978-981-10-6418-0_11, DOI 10.1007/978-981-10-6418-0_11]
   Li Qing., 2018, Tell-and-answer: Towards explainable visual question answering using attributes and captions
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZC, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2624140
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo X, 2014, IEEE T IND INFORM, V10, P1273, DOI 10.1109/TII.2014.2308433
   Malinowski M, 2014, ADV NEUR IN, V27
   Mao J, 2014, CELL DEATH DIS, V5, DOI 10.1038/cddis.2013.515
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Perozzi B., 2014, Deepwalk: Online learning
   Ren Mengye, 2015, NIPS, P2953
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schwartz Idan, 2017, P 31 ANN C NEUR INF, P1
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Suhr A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P217, DOI 10.18653/v1/P17-2034
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Teney Damien, 2018, P IEEE C COMP VIS PA, P1
   Tin X, 2015, PROC CVPR IEEE, P2984, DOI 10.1109/CVPR.2015.7298917
   Toor AS, 2019, PATTERN RECOGN LETT, V126, P111, DOI 10.1016/j.patrec.2018.02.013
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang Ye Yi, 1994, COMPUTER SCI, V14, P325
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500
   Wu Qi, 2015, IMAGE CAPTIONING INT
   Xiang Y, 2010, PROC CVPR IEEE, P3368, DOI 10.1109/CVPR.2010.5540015
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhang JL, 2018, GEOPHYS RES LETT, V45, P8665, DOI 10.1029/2018GL077787
   Zhang Z., 2014, IEEE Computer Vision-ECCV Workshops, P122
   Zhou N, 2011, IEEE T PATTERN ANAL, V33, P1281, DOI 10.1109/TPAMI.2010.204
NR 57
TC 8
Z9 8
U1 3
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 49
DI 10.1145/3300938
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900002
DA 2024-07-18
ER

PT J
AU Gan, T
   Li, JN
   Wong, YK
   Kankanhalli, MS
AF Gan, Tian
   Li, Junnan
   Wong, Yongkang
   Kankanhalli, Mohan S.
TI A Multi-sensor Framework for Personal Presentation Analytics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Quantified self; multi-modal analysis; presentations; learning analytics
ID PRONUNCIATION EVALUATION; SPEECH; GESTURE; CLASSIFICATION
AB Presentation has been an effective method for delivering information to an audience for many years. Over the past few decades, technological advancements have revolutionized the way humans deliver presentation. Conventionally, the quality of a presentation is usually evaluated through painstaking manual analysis with experts. Although the expert feedback is effective in assisting users to improve their presentation skills, manual evaluation suffers from high cost and is often not available to most individuals. In this work, we propose a novel multi-sensor self-quantification system for presentations, which is designed based on a new proposed assessment rubric. We present our analytics model with conventional ambient sensors (i.e., static cameras and Kinect sensor) and the emerging wearable egocentric sensors (i.e., Google Glass). In addition, we performed a cross-correlation analysis of speaker's vocal behavior and body language. The proposed framework is evaluated on a new presentation dataset, namely, NUS Multi-Sensor Presentation dataset, which consists of 51 presentations covering a diverse range of topics. To validate the efficacy of the proposed system, we have conducted a series of user studies with the speakers and an interview with an English communication expert, which reveals positive and promising feedback.
C1 [Gan, Tian] Shandong Univ, Sch Comp Sci & Technol, Jinan, Shandong, Peoples R China.
   [Li, Junnan] Natl Univ Singapore, NUS Grad Sch Integrat Sci & Engn, Singapore, Singapore.
   [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore, Singapore.
C3 Shandong University; National University of Singapore; National
   University of Singapore
RP Gan, T (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan, Shandong, Peoples R China.
EM gan-tian@sdu.edu.cn; lijunnan@u.nus.edu; yongkang.wong@nus.edu.sg;
   mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015; Wong,
   Yongkang/0000-0002-1239-4428
FU National Research Foundation, Prime Minister's Office, Singapore under
   its Strategic Capability Research Centres Funding Initiative; National
   Natural Science Foundation of China [61702302]; Fundamental Research
   Funds of Shandong University [2017HW001]
FX This research is supported by the National Research Foundation, Prime
   Minister's Office, Singapore under its Strategic Capability Research
   Centres Funding Initiative; National Natural Science Foundation of
   China, No.: 61702302; and Fundamental Research Funds of Shandong
   University (No. 2017HW001).
CR Alrahabi M, 2008, LECT NOTES ARTIF INT, V5221, P40
   [Anonymous], 1990, COMMUNICATION IS LIF
   Aryadoust V, 2016, LANG ASSESS Q, V13, P1, DOI 10.1080/15434303.2015.1133626
   Audhkhasi K, 2009, INT CONF ACOUST SPEE, P4857, DOI 10.1109/ICASSP.2009.4960719
   Banta T.W., 2007, Assessing student achievement in general education: Assessment Update collections, V5
   Barnlund D.C., 1970, FDN COMMUNICATION TH, P83
   Bernardis P, 2006, NEUROPSYCHOLOGIA, V44, P178, DOI 10.1016/j.neuropsychologia.2005.05.007
   Boersma P., 2018, Praat: doing phonetics by computer, DOI DOI 10.1097/AUD.0B013E31821473F7
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Brookhart SM, 2015, EDUC REV, V67, P343, DOI 10.1080/00131911.2014.929565
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen Lei., 2014, INT C MULTIMODAL INT, P200
   Chen T, 2014, INT WORKS HIGH MOBIL, P1, DOI 10.1109/HMWC.2014.7000203
   de Jong NH, 2009, BEHAV RES METHODS, V41, P385, DOI 10.3758/BRM.41.2.385
   Du Y, 2016, LECT NOTES COMPUT SC, V9908, P20, DOI 10.1007/978-3-319-46493-0_2
   Dunbar NE, 2006, INNOV HIGH EDUC, V31, P115, DOI 10.1007/s10755-006-9012-x
   Echeverria V., 2014, Proceedings of the 2014 ACM workshop on Multimodal Learning Analytics Workshop and Grand Challenge, P53, DOI [DOI 10.1145/2666633.2666641, DOI 10.1145/2666633.26666411,2]
   ERICSSON KA, 1993, PSYCHOL REV, V100, P363, DOI 10.1037/0033-295X.100.3.363
   Ermes M, 2008, IEEE T INF TECHNOL B, V12, P20, DOI 10.1109/TITB.2007.899496
   FAWCETT SB, 1975, J APPL BEHAV ANAL, V8, P125, DOI 10.1901/jaba.1975.8-125
   Gan T, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P601, DOI 10.1145/2733373.2806252
   Hadar U, 1998, BRAIN LANG, V62, P107, DOI 10.1006/brln.1997.1890
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hernandez J, 2014, 2014 EAI 4TH INTERNATIONAL CONFERENCE ON WIRELESS MOBILE COMMUNICATION AND HEALTHCARE (MOBIHEALTH), P55, DOI [10.1109/MOBIHEALTH.2014.7015908, 10.4108/icst.mobihealth.2014.257219]
   Hincks R., 2005, System, V33, P575
   Hoque M, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P697
   Hu WP, 2013, INTERSPEECH, P1885
   Huang YJ, 2018, IEEE ANTENNAS PROP, P1265, DOI 10.1109/APUSNCURSINRSM.2018.8608752
   Kelly SD, 2010, PSYCHOL SCI, V21, P260, DOI 10.1177/0956797609357327
   Klima E.S., 1979, SIGNS LANGUAGE
   Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239
   Krauss RM, 1995, J EXP SOC PSYCHOL, V31, P533, DOI 10.1006/jesp.1995.1024
   Kurihara K, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P358
   Lara OD, 2013, IEEE COMMUN SURV TUT, V15, P1192, DOI 10.1109/SURV.2012.110112.00192
   Li JN, 2016, IEEE INT SYM MULTIM, P222, DOI [10.1109/ISM.2016.0051, 10.1109/ISM.2016.16]
   Li W, 2013, INT CONF INFO SCI, P937, DOI 10.1109/ICIST.2013.6747693
   Luzardo G., 2014, P 2014 ACM WORKSHOP, P37, DOI [DOI 10.1145/2666633.2666639, 10.1145/2666633.26666391,2, DOI 10.1145/2666633.26666391,2]
   Mansell W, 1999, COGNITION EMOTION, V13, P673, DOI 10.1080/026999399379032
   Meignier Sylvain, 2010, P CMU SPHINX WORKSH
   Mihoub A, 2017, IUI'17: PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P331, DOI 10.1145/3025171.3025195
   Morreale S., 1996, Large scale assessment of oral communication: K-12 and higher education, V2nd
   Morreale S.P., 1993, COMPETENT SPEAKER SP
   Müller J, 2009, LECT NOTES COMPUT SC, V5538, P17, DOI 10.1007/978-3-642-01516-8_3
   Munhall KG, 2004, PSYCHOL SCI, V15, P133, DOI 10.1111/j.0963-7214.2004.01502010.x
   Nguyen Anh tuan, 2012, P IEEE S E LEARN E M
   Nikolic S, 2018, EUR J ENG EDUC, V43, P538, DOI 10.1080/03043797.2017.1298569
   Pfister T, 2011, IEEE T AFFECT COMPUT, V2, P66, DOI 10.1109/T-AFFC.2011.8
   Pfister T, 2010, LECT NOTES COMPUT SC, V6219, P151, DOI 10.1007/978-3-642-14715-9_15
   Randel D.M., 2003, HARVARD DICT MUSIC, V16
   Sargin ME, 2007, IEEE T MULTIMEDIA, V9, P1396, DOI 10.1109/TMM.2007.906583
   Schneider J, 2016, IEEE T LEARN TECHNOL, V9, P318, DOI 10.1109/TLT.2016.2627043
   Schneider J, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P538, DOI 10.1145/2818346.2830603
   Schreiber L. M., 2012, COMMUN EDUC, V61, P3
   Siegman A.W., 2014, Nonverbal behavior and communication
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Slater M, 2006, CYBERPSYCHOL BEHAV, V9, P627, DOI 10.1089/cpb.2006.9.627
   Suñol JJ, 2016, ASSESS EVAL HIGH EDU, V41, P622, DOI 10.1080/02602938.2015.1037720
   Tasko SM, 2010, J SPEECH LANG HEAR R, V53, P84, DOI 10.1044/1092-4388(2009/08-0124)
   Tepperman J, 2005, INT CONF ACOUST SPEE, P937
   Thomson S., 2002, Commun Res Rep, V19, P18, DOI [10.1080/08824090209384828, DOI 10.1080/08824090209384828]
   van Ginkel S, 2017, ASSESS EVAL HIGH EDU, V42, P953, DOI 10.1080/02602938.2016.1212984
   van Ginkel S, 2017, STUD HIGH EDUC, V42, P1671, DOI 10.1080/03075079.2015.1117064
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Wagner P, 2014, SPEECH COMMUN, V57, P209, DOI 10.1016/j.specom.2013.09.008
   Webster J, 1997, DATA BASE ADV INF SY, V28, P63
   Wei X.-Y., 2012, Proceedings of the 20th ACM International Conference on Multimedia, MM, P639, DOI [DOI 10.1145/2393347.2393436, 10.1145/2393347.2393436]
   Wei XS, 2014, IEEE DATA MINING, P1037, DOI 10.1109/ICDM.2014.16
   Weninger F, 2012, IEEE T AFFECT COMPUT, V3, P496, DOI 10.1109/T-AFFC.2012.15
   Wu Y., 2004, ACM INT C MULTIMEDIA, P572, DOI DOI 10.1145/1027527.1027665
   Yamasaki Toshihiko., 2015, Proceedings of the 1st International Workshop on Affect Sentiment in Multimedia, P33
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang X., 2015, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI 10.1109/CVPR.2015.7299081
   Zheng J, 2007, INT CONF ACOUST SPEE, P201
NR 73
TC 6
Z9 6
U1 3
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 30
DI 10.1145/3300941
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA IM3ZV
UT WOS:000477935400001
DA 2024-07-18
ER

PT J
AU Yu, T
   Jin, HM
   Tan, WT
   Nahrstedt, K
AF Yu, Tuo
   Jin, Haiming
   Tan, Wai-Tian
   Nahrstedt, Klara
TI SKEPRID: Pose and Illumination Change-Resistant Skeleton-Based Person
   Re-Identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; pose recognition; cloth-type features
AB Currently, the surveillance camera-based person re-identification is still challenging because of diverse factors such as people's changing poses and various illumination. The various poses make it hard to conduct feature matching across images, and the illumination changes make color-based features unreliable. In this article, we present SKEPRID,(1) a skeleton-based person re-identification method that handles strong pose and illumination changes jointly. To reduce the impacts of pose changes on re-identification, we estimate the joints' positions of a person based on the deep learning technique and thus make it possible to extract features on specific body parts with high accuracy. Based on the skeleton information, we design a set of local color comparison-based cloth-type features, which are resistant to various lighting conditions. Moreover, to better evaluate SKEPRID, we build the PO&LI2 dataset, which has large pose and illumination diversity. Our experimental results show that SKEPRID outperforms state-of-the-art approaches in the case of strong pose and illumination variation.
C1 [Yu, Tuo; Nahrstedt, Klara] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
   [Jin, Haiming] Shanghai Jiao Tong Univ, John Hoperoft Ctr Comp Sci, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Tan, Wai-Tian] Cisco Syst Inc, San Jose, CA 95035 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   Shanghai Jiao Tong University; Cisco Systems Inc
RP Yu, T (corresponding author), Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
EM tuoyu2@illinois.edu; jinhaiming@sjtu.edu.cn; dtan2@cisco.com;
   klara@illinois.edu
OI Yu, Tuo/0000-0002-5209-6265
FU National Science Foundation [CNS-1330491]
FX This research was funded by the National Science Foundation under Award
   No. CNS-1330491.
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 2017, P CVPR
   [Anonymous], 2010, PROC BRIT MACH VIS C
   [Anonymous], 2016, P CVPR
   [Anonymous], P IEEE 3 INT C BIOM
   [Anonymous], ARXIV170204858
   [Anonymous], CORR
   Bak S., 2015, P SPIE IS T EL IM IN
   Barbosa IB, 2012, LECT NOTES COMPUT SC, V7583, P433, DOI 10.1007/978-3-642-33863-2_43
   Bhattacharyya Anil, 1943, B CALCUTTA MATH SOC, V35, P99
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen W, 2015, IEEE I CONF COMP VIS, P3298, DOI 10.1109/ICCV.2015.377
   Cheng D. S., 2011, P BMVC, V5, P6
   Cho YJ, 2016, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2016.151
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Damen D, 2012, IEEE T PATTERN ANAL, V34, P1056, DOI 10.1109/TPAMI.2011.205
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gray Douglas, 2007, P IEEE INT WORKSH PE, V3, P1
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Haralick R., 1973, IEEE T SYST MAN CYBE, V3
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Jose C, 2016, LECT NOTES COMPUT SC, V9909, P875, DOI 10.1007/978-3-319-46454-1_53
   Kekre H. B., 2010, INT J COMPUT THEORY, V2, P695, DOI DOI 10.7763/IJCTE.2010.V2.227
   Khamis S, 2015, LECT NOTES COMPUT SC, V8927, P134, DOI 10.1007/978-3-319-16199-0_10
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Layne R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.24
   Lee L, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P155, DOI 10.1109/AFGR.2002.1004148
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li Wei, 2012, AS C COMP VIS ACCV 2, P31
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Martinel N., 2014, P ECCV WORKSH VIS SU
   Martinel N, 2016, LECT NOTES COMPUT SC, V9908, P858, DOI 10.1007/978-3-319-46493-0_52
   Munaro M, 2014, IEEE INT CONF ROBOT, P5644, DOI 10.1109/ICRA.2014.6907689
   Negi S.S., 2014, International Conference on Recent Advances and Innovations in Engineering, P1, DOI DOI 10.1109/ICRAIE.2014.6909232
   Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794
   Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426
   Raman S, 2011, POSTCOLON LIT STUD, P1
   Safdarnejad Seyed Morteza, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163105
   Satta Riccardo, 2013, Proceedings of the 8th International Conference on Computer Vision Theory and Applications. VISAPP 2013, P407
   Su C., 2017, P IEEE TPAMI
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Tao D., 2006, Computer_Vision_and_Pattern Recognition, P1670
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang HX, 2016, LECT NOTES COMPUT SC, V9908, P405, DOI 10.1007/978-3-319-46493-0_25
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Weinberger K.Q., 2006, Advances in neural information processing systems, P1473
   Weinrich C, 2013, IEEE SYS MAN CYBERN, P4384, DOI 10.1109/SMC.2013.748
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1
   Xu B., 2010, IEEE T CONSUM ELECTR, V56, P3
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yuan Shuai, 2011, Technol Disabil, V23, P75
   Zhang E, 2010, SIGNAL PROCESS, V90, P2295, DOI 10.1016/j.sigpro.2010.01.024
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng L., 2017, IEEE T IMAGE PROCESS
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
NR 65
TC 11
Z9 13
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2018
VL 14
IS 4
AR 82
DI 10.1145/3243217
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JB
UT WOS:000457139200004
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, FF
   Mao, QR
   Shen, XJ
   Zhan, YZ
   Dong, M
AF Zhang, Feifei
   Mao, Qirong
   Shen, Xiangjun
   Zhan, Yongzhao
   Dong, Ming
TI Spatially Coherent Feature Learning for Pose-Invariant Facial Expression
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Facial expression recognition; learning-based features; pose-invariant;
   spatially coherent feature learning
AB Feature learning has enjoyed much attention and achieved good performance in recent studies of image processing. Unlike the required training conditions often assumed there, far less labeled data is available for training emotion classification systems. In addition, current feature learning is typically performed on an entire face image without considering the dependency between features. These approaches ignore the fact that faces are structured and the neighboring features are dependent. Thus, the learned features lack the power to describe visually coherent facial images. Our method is therefore designed with the goal of simplifying the problem domain by removing expression- irrelevant factors from the input images, with a key region-based mechanism, which is an effort to reduce the amount of data required to effectively train the feature-learning methods. Meanwhile, we can construct geometric constraints between the key regions and its detected positions. To this end, we introduce a Spatially Coherent feature-learning method for Poseinvariant Facial Expression Recognition (SC-PFER). In our model, we first perform face frontalization through a 3D pose-normalization technique, which could normalize poses while preserving the identity information through synthesizing frontal faces for facial images with arbitrary views. Subsequently, we select a sequence of key regions around 51 key points in the synthetic frontal face images for efficient unsupervised feature learning. Finally, we introduce a linkage structure over the learning-based features and the corresponding geometry information of each key region to encode the dependencies of the regions. Our method, on the whole, does not require training multiple models for each specific pose and avoids separating training and parameter tuning for each pose. The proposed framework has been evaluated on two benchmark databases, BU-3DFE and SFEW, for pose-invariant Facial Expression Recognition (FER). The experimental results demonstrate that our algorithm outperforms current state-of-the-art FER methods. Specifically, our model achieves an improvement of 1.72% and 1.11% FER accuracy, on average, on BU-3DFE and SFEW, respectively.
C1 [Zhang, Feifei; Mao, Qirong; Shen, Xiangjun; Zhan, Yongzhao] Jiangsu Univ, 301 Xuefu Rd, Zhenjiang 212013, Jiangsu, Peoples R China.
   [Dong, Ming] Wayne State Univ, Detroit, MI 48202 USA.
C3 Jiangsu University; Wayne State University
RP Mao, QR (corresponding author), Jiangsu Univ, 301 Xuefu Rd, Zhenjiang 212013, Jiangsu, Peoples R China.; Dong, M (corresponding author), Wayne State Univ, Detroit, MI 48202 USA.
EM susanzhang@ujs.edu.cn; maoqr@mail.ujs.edu.cn; xjshen@ujs.edu.cn;
   yzzhan@ujs.edu.cn; mdong@cs.wayne.edu
RI Zhang, Feifei/A-3199-2015; jin, li/IWU-4648-2023
FU National Nature Science Foundation of China [61272211, 61672267];
   National Laboratory of Pattern Recognition [201700022]; China
   Postdoctoral Science Foundation [2015M570413]; Graduate Student
   Scientific Research Innovation Projects in Jiangsu Province
   [KYCX17_1811]; Innovation Project of Undergraduate Students in Jiangsu
   University [16A235]
FX This work is supported in part by the National Nature Science Foundation
   of China under Grants 61272211 and 61672267, the Open Project Program of
   the National Laboratory of Pattern Recognition under Grant 201700022,
   the General Financial Grant from the China Postdoctoral Science
   Foundation 2015M570413, the Graduate Student Scientific Research
   Innovation Projects in Jiangsu Province under Grant KYCX17_1811, and the
   Innovation Project of Undergraduate Students in Jiangsu University under
   Grant 16A235.
CR [Anonymous], 2016, PROC INT C MULTIMEDI
   [Anonymous], 2017, ACM T MULTIM COMPUT, DOI DOI 10.1145/3092840
   [Anonymous], Environmental Psychology & Nonverbal Behavior
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2013, 21 ACM INT C MULT, DOI DOI 10.1145/2502081.2502173
   [Anonymous], P EUR C COMP VISION
   [Anonymous], PROC CVPR IEEE
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P 3 ACM C INT C MULT
   [Anonymous], BRIT MACH VIS C
   [Anonymous], 2013, IEEE FG
   [Anonymous], 2012, Tech. Univ. Denmark
   [Anonymous], 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7299058
   Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Dahmane Mohamed, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P884, DOI 10.1109/FG.2011.5771368
   Dhall A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2106, DOI 10.1109/ICCVW.2011.6130508
   Dibeklioglu H, 2015, IEEE T MULTIMEDIA, V17, P279, DOI 10.1109/TMM.2015.2394777
   Ding CX, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2845089
   Ding CX, 2015, IEEE T IMAGE PROCESS, V24, P980, DOI 10.1109/TIP.2015.2390959
   Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Getoor, 2011, P 28 INT C MACH LEAR, P833
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gupta S. K., 2011, Proceedings of the Seventh International Conference on Signal-Image Technology & Internet-Based Systems (SITIS 2011), P422, DOI 10.1109/SITIS.2011.64
   Hesse N, 2012, INT C PATT RECOG, P3533
   Hu Y.X., 2008, IEEE INT C PATT REC, P1
   Jampour M., 2015, P 20 COMP VIS WINT W
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kan MN, 2012, LECT NOTES COMPUT SC, V7572, P808, DOI 10.1007/978-3-642-33718-5_58
   Karafotias G, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092840
   Kim Y, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808204
   Lee-Johnson CP, 2010, IEEE T SYST MAN CY B, V40, P469, DOI 10.1109/TSMCB.2009.2026826
   Levi G, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P503, DOI 10.1145/2823327.2823333
   Liu P, 2014, PROC CVPR IEEE, P1805, DOI 10.1109/CVPR.2014.233
   Moore S, 2011, COMPUT VIS IMAGE UND, V115, P541, DOI 10.1016/j.cviu.2010.12.001
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Pantic M., 2007, FACE RECOGNITION
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Ranzato M, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995710
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   Rudovic Ognjen, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4121, DOI 10.1109/ICPR.2010.1001
   Rudovic O, 2013, IEEE T PATTERN ANAL, V35, P1357, DOI 10.1109/TPAMI.2012.233
   Sangineto E, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P357, DOI 10.1145/2647868.2654916
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Shepstone SE, 2014, IEEE T MULTIMEDIA, V16, P1999, DOI 10.1109/TMM.2014.2337845
   Sikka K, 2012, LECT NOTES COMPUT SC, V7584, P250, DOI 10.1007/978-3-642-33868-7_25
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Tang H, 2010, IEEE INT CON MULTI, P1202, DOI 10.1109/ICME.2010.5582576
   Tariq U, 2014, PATTERN RECOGN LETT, V46, P89, DOI 10.1016/j.patrec.2014.05.011
   Yang SF, 2012, IEEE T SYST MAN CY B, V42, P980, DOI 10.1109/TSMCB.2012.2192269
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zeng ZH, 2008, IEEE T MULTIMEDIA, V10, P570, DOI 10.1109/TMM.2008.921737
   Zhao R, 2016, PROC CVPR IEEE, P3466, DOI 10.1109/CVPR.2016.377
   Zheng WM, 2014, IEEE T AFFECT COMPUT, V5, P71, DOI 10.1109/TAFFC.2014.2304712
   Zheng WM, 2009, IEEE I CONF COMP VIS, P1901, DOI 10.1109/ICCV.2009.5459421
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
   Zhu Z., 2006, IEEE Conf. Comput. Vision and Pattern Recogn, P681
NR 62
TC 27
Z9 27
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2018
VL 14
IS 1
SU S
AR 27
DI 10.1145/3176646
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZK
UT WOS:000433517100013
DA 2024-07-18
ER

PT J
AU Ng, PC
   She, J
   Jeon, KE
   Baldauf, M
AF Ng, Pai Chet
   She, James
   Jeon, Kang Eun
   Baldauf, Matthias
TI When Smart Devices Interact With Pervasive Screens: A Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Pervasive screen; smart device; interactive technology
ID DISPLAYS; DESIGN
AB The meeting of pervasive screens and smart devices has witnessed the birth of screen-smart device interaction (SSI), a key enabler to many novel interactive use cases. Most current surveys focus on direct human-screen interaction, and to the best of our knowledge, none have studied state-of-the-art SSI. This survey identifies three core elements of SSI and delivers a timely discussion on SSI oriented around the screen, the smart device, and the interaction modality. Two evaluation metrics (i.e., interaction latency and accuracy) have been adopted and refined to match the evaluation criterion of SSI. The bottlenecks that hinder the further advancement of the current SSI in connection with this metrics are studied. Last, future research challenges and opportunities are highlighted in the hope of inspiring continuous research efforts to realize the next generation of SSI.
C1 [Ng, Pai Chet; She, James; Jeon, Kang Eun] Hong Kong Univ Sci & Technol, HKUST NIE Social Media Lab, Hong Kong, Hong Kong, Peoples R China.
   [Baldauf, Matthias] Univ Appl Sci St Gallen, Inst Informat & Proc Management, St Gallen, Switzerland.
   [Baldauf, Matthias] Univ Appl Sci, FHS St Gallen, Rosenbergstr 59, CH-9001 St Gallen, Switzerland.
C3 Hong Kong University of Science & Technology
RP Ng, PC (corresponding author), Hong Kong Univ Sci & Technol, HKUST NIE Social Media Lab, Hong Kong, Hong Kong, Peoples R China.
EM pcng@ust.hk; eejames@ust.hk; kejeon@ust.hk; iam@matthiasbaldauf.com
RI Ng, Pc/ITV-2513-2023; Ng, Pc/AAA-3412-2020
OI Baldauf, Matthias/0000-0002-1876-5082
FU HKUST-NIE Social Media Lab, HKUST
FX This work was supported by the HKUST-NIE Social Media Lab, HKUST.
CR ACM Trans, MULTIMEDIA COMPUT CO, V13
   Alt Florian, 2013, P SIGCHI C HUM FACT, P1709, DOI [10.1145/2470654.2466226, DOI 10.1145/2470654.2466226]
   [Anonymous], POST COMMUNIST REGIM
   Ardito C, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2682623
   Baldauf Matthias, 2013, Ambient Intelligence. 4th International Joint Conference, AmI 2013. Proceedings: LNCS 8309, P32, DOI 10.1007/978-3-319-03647-2_3
   Baldauf M., 2013, P HUM FACT COMP SYST, P3015
   Baldauf M, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808202
   Baldauf Matthias., 2016, P 5 ACM INT S PERVAS, P175, DOI DOI 10.1145/2914920.2915026
   Bellucci A, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2500467
   Bellucci A, 2010, IEEE PERVAS COMPUT, V9, P72, DOI 10.1109/MPRV.2010.30
   Bellucci Paloma Diaz, 2015, ITS, P301
   Boring S, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2721
   Boring S, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2287
   Broll G, 2013, PERVASIVE MOB COMPUT, V9, P242, DOI 10.1016/j.pmcj.2012.09.007
   Ceipidor UB, 2013, PROC INT WORK NEAR, P1
   Chi PY, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3923, DOI 10.1145/2702123.2702451
   Clinch S, 2013, IEEE PERVAS COMPUT, V12, P92, DOI 10.1109/MPRV.2013.16
   Colley A, 2016, PROCEEDINGS OF THE 7TH AUGMENTED HUMAN INTERNATIONAL CONFERENCE (AUGMENTED HUMAN 2016), DOI 10.1145/2875194.2875212
   de Freitas Adrian, 2016, P 34 ANN ACM C HUM F
   Deller M, 2011, LECT NOTES COMPUT SC, V6947, P289, DOI 10.1007/978-3-642-23771-3_22
   Dingler T, 2015, LECT NOTES COMPUT SC, V9298, P402, DOI 10.1007/978-3-319-22698-9_27
   Dix A., 2010, INT J UBIQUITOUS COM, V1, P11
   Fei S., 2013, Proceedings of the 2013 ACM international conference on Interactive tabletops and surfaces, P33, DOI DOI 10.1145/2512349
   Frosini L., 2014, The 7th ACM SIGCHI Symposium on Engineering Interactive Computing Systems - EICS 2014, P55, DOI DOI 10.1145/2607023.2607032.URL
   Fu JQ, 2016, INT CONF INSTR MEAS, P327, DOI 10.1109/IMCCC.2016.43
   Gen Ava Fatah, 2014, P INT S PERV DISPL P
   Gjerlufsen T, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3383
   Greimel Felix, 2011, ADV MEDIA TECHNOLOGY
   Häkkilä J, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1011, DOI 10.1145/2556288.2557187
   Hakvoort Gido, 2013, ITS 13, DOI [10.1145/2512349.2514598, DOI 10.1145/2512349.2514598]
   Hamilton P, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2773, DOI 10.1145/2556288.2557170
   He W, 2016, IEEE INTERNET THINGS, V3, P1213, DOI 10.1109/JIOT.2016.2576479
   Holz Christian., 2012, P SIGCHI C HUMAN FAC, P503, DOI DOI 10.1145/2207676.2207745
   Houben S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1247, DOI 10.1145/2702123.2702215
   Jarusriboonchai P, 2016, PROCEEDINGS OF THE 19TH ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING COMPANION, P53, DOI 10.1145/2818052.2874323
   Jeon S, 2010, PERS UBIQUIT COMPUT, V14, P83, DOI 10.1007/s00779-009-0249-0
   Karaman S, 2016, MULTIMED TOOLS APPL, V75, P3787, DOI 10.1007/s11042-014-2192-y
   Lee JY, 2011, P IEEE VIRT REAL ANN, P221, DOI 10.1109/VR.2011.5759478
   Muller Jorg, P 32 ANN ACM C HUM F, P1415, DOI [10.1145/2556288.2557001, DOI 10.1145/2556288.2557001]
   Muta M., 2015, P 4 INT S PERVASIVE, P187, DOI [10.1145/2757710.2757732, DOI 10.1145/2757710.2757732]
   Narzt Wolfgang, 2016, 2016 13th IEEE Annual Consumer Communications & Networking Conference (CCNC), P423, DOI 10.1109/CCNC.2016.7444817
   Nebeling M, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2793, DOI 10.1145/2556288.2556980
   Ni T, 2009, UIST 2009: PROCEEDINGS OF THE 22ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P101
   Olberding S., 2013, P 4 AUGM HUM INT C, P9, DOI [10.1145/2459236.2459239, DOI 10.1145/2459236.2459239]
   Ostkamp M., 2015, P 4 INT S PERVASIVE, P39, DOI [10.1145/2757710.2757716, DOI 10.1145/2757710.2757716]
   PARKER C, 2015, UBICOMP ISWC 15 ADJU, P00807
   Parker Callum., 2016, Proc. of the 5th ACM International Symposium on Pervasive Displays (PerDis '16), P52, DOI [10.1145/2914920.2915016, DOI 10.1145/2914920.2915016]
   Pears N, 2009, IEEE PERVAS COMPUT, V8, P14, DOI 10.1109/MPRV.2009.35
   Pearson J, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1257, DOI 10.1145/2702123.2702247
   Peltonen P, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1285
   Perry M., 2010, P ACM INT C INTERACT, P109
   Pietroszek K, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P57, DOI 10.1145/2807442.2807471
   Rattanarungrot S, 2015, 2015 DIGITAL HERITAGE INTERNATIONAL CONGRESS, VOL 1: DIGITIZATION & ACQUISITION, COMPUTER GRAPHICS & INTERACTION, P329, DOI 10.1109/DigitalHeritage.2015.7413894
   Ribeiro Fernando Reinaldo, 2016, INT J MULTIMEDIA UBI, V11, P11
   Rittenbruch M, 2015, COMPUT SUPP COOP W J, V24, P121, DOI 10.1007/s10606-015-9221-x
   Scherfgen David, 2015, 2015 IEEE Symposium on 3D User Interfaces (3DUI), P137, DOI 10.1109/3DUI.2015.7131739
   Schmidt Dominik., 2012, P DESIGNING INTERACT, P318
   Schneegass Stefan., 2014, Chi'14 extended abstracts on human factors in computing systems, P2035
   Schreiner M., 2015, Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems, P2163, DOI DOI 10.1145/2702613.2732909
   Seifert J, 2014, PERS UBIQUIT COMPUT, V18, P1013, DOI 10.1007/s00779-013-0667-x
   Seifert Julian, 2012, P 2012 ACM INT C INT, P51, DOI [10.1145/2396636.2396644, DOI 10.1145/2396636.2396644]
   She J, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2557450
   She J, 2013, IEEE T EMERG TOP COM, V1, P232, DOI 10.1109/TETC.2013.2282618
   Stellmach Sophie., 2013, P SIGCHI C HUMAN FAC, P285, DOI [10.1145/2470654.2470695 10.1145/2470654.2470695, DOI 10.1145/2470654.2470695]
   Sturdee Miriam., 2015, Proceedings of the 2015 International Conference on Interactive Tabletops ; Surfaces. ITS'15. Madeira, P219
   Tomitsch Martin., 2014, P INT S PERVASIVE DI, p160:160, DOI DOI 10.1145/2611009.2611016
   Turner J, 2013, LECT NOTES COMPUT SC, V8118, P170
   Velloso E, 2016, DIS 2016: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON DESIGNING INTERACTIVE SYSTEMS, P812, DOI 10.1145/2901790.2901867
   Vepsalainen J., 2015, P 2015 INT C INTERAC, P201, DOI [10.1145/2817721.2817745, DOI 10.1145/2817721.2817745]
   Weigel M, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P179, DOI 10.1145/2556288.2557239
   Weissker Tim, 2016, P 2016 CHI C HUM FAC, P3796
   Wolf K, 2016, INT J MOB HUM COMPUT, V8, P29, DOI 10.4018/IJMHCI.2016100102
   Yamaguchi Tokuo, 2013, P 2013 ACM INT C INT, P329, DOI [10.1145/2512349.2514596, DOI 10.1145/2512349.2514596]
   Yang JS, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2783
NR 74
TC 19
Z9 20
U1 4
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 55
DI 10.1145/3115933
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300009
DA 2024-07-18
ER

PT J
AU Lisanti, G
   Karaman, S
   Masi, I
AF Lisanti, Giuseppe
   Karaman, Svebor
   Masi, Iacopo
TI Multichannel-Kernel Canonical Correlation Analysis for Cross-View Person
   Reidentification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person reidentification; KCCA; late fusion
ID SCALE
AB In this article, we introduce a method to overcome one of the main challenges of person reidentification in multicamera networks, namely cross-view appearance changes. The proposed solution addresses the extreme variability of person appearance in different camera views by exploiting multiple feature representations. For each feature, kernel canonical correlation analysis with different kernels is employed to learn several projection spaces in which the appearance correlation between samples of the same person observed from different cameras is maximized. An iterative logistic regression is finally used to select and weight the contributions of each projection and perform the matching between the two views. Experimental evaluation shows that the proposed solution obtains comparable performance on the VIPeR and PRID 450s datasets and improves on the PRID and CUHK01 datasets with respect to the state of the art.
C1 [Lisanti, Giuseppe] Univ Florence, Media Integrat & Commun Ctr, Viale Morgagni 65, I-50134 Florence, Italy.
   [Karaman, Svebor] Columbia Univ, Dept Elect Engn, Digital Video & Multimedia Lab, 500 W 120th St,Mudd 1310, New York, NY 10027 USA.
   [Masi, Iacopo] Univ Southern Calif, Inst Robot & Intelligent Syst, Dept Comp Sci, 3737 Watt Way,PHE 208, Los Angeles, CA USA.
C3 University of Florence; Columbia University; University of Southern
   California
RP Lisanti, G (corresponding author), Univ Florence, Media Integrat & Commun Ctr, Viale Morgagni 65, I-50134 Florence, Italy.
EM giuseppe.lisanti@unifi.it; svebor.karaman@columbia.edu;
   iacopo.masi@usc.edu
RI Karaman, Svebor/I-4929-2019; Lisanti, Giuseppe/AAG-8699-2020
OI Karaman, Svebor/0000-0002-2496-5822; Lisanti,
   Giuseppe/0000-0002-0785-9972
FU Social Museum and Smart Tourism, Miur project [CTN01_00034_23154_SMST]
FX This research was partially supported by the Social Museum and Smart
   Tourism, Miur project no. CTN01_00034_23154_SMST.
CR Ahmed Ejaz, 2015, P C COMP VIS PATT RE
   An L, 2016, IEEE T CIRC SYST VID, V26, P776, DOI 10.1109/TCSVT.2015.2416561
   An L, 2015, IEEE SIGNAL PROC LET, V22, P1103, DOI 10.1109/LSP.2015.2390222
   An Le, 2013, P INT C ADV VID SIGN
   [Anonymous], P INT C COMP VIS
   [Anonymous], P EUR C COMP VIS
   Bedagkar-Gala A, 2014, IMAGE VISION COMPUT, V32, P270, DOI 10.1016/j.imavis.2014.02.001
   Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Doretto G, 2011, J AMB INTEL HUM COMP, V2, P127, DOI 10.1007/s12652-010-0034-y
   Farenzena Michela, 2010, P C COMP VIS PATT RE
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hirzer M., 2012, P EUR C COMP VIS
   Hirzer Martin, 2012, P INT C ADV VID SIGN
   Karaman S, 2014, PATTERN RECOGN, V47, P3767, DOI 10.1016/j.patcog.2014.06.003
   Karaman Svebor, 2012, P EUR C COMP VIS WOR
   Kostinger Martin, 2012, P C COMP VIS PATT RE
   Li Wei, 2013, P C COMP VIS PATT RE
   Li Wei, 2013, P AS C COMP VIS
   Li Wei, 2014, P C COMP VIS PATT RE
   Liao Shengcai, 2015, P C COMP VIS PATT RE
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Lisanti Giuseppe, 2014, P ACM IEEE INT C DIS
   Liu H, 2015, IEEE SIGNAL PROC LET, V22, P910, DOI 10.1109/LSP.2014.2377204
   Liu Xiaokai, 2015, P WINT C APPL COMP V
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Paisitkriangkrai S., 2015, P C COMP VIS PATT RE
   Prates Raphael Felipe de Carvalho, 2015, P INT C BIOM
   Prosser B., 2010, P BRIT MACH VIS C BM, DOI DOI 10.5244/C.24.21
   Rahimi Ali, 2007, P C NEUR INF PROC SY
   Roth PM, 2014, ADV COMPUT VIS PATT, P247, DOI 10.1007/978-1-4471-6296-4_12
   Shen Yang, 2015, P INT C COMP VIS
   Shi Zhiyuan, 2015, P C COMP VIS PATT RE
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Vezzani R, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543596
   Wang J, 2016, IEEE SIGNAL PROC LET, V23, P84, DOI 10.1109/LSP.2015.2502271
   Wang W., 2016, PROC INT C LEARN REP
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Yi D., 2014, DEEP METRIC LEARNING
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhao Rui, 2014, P C COMP VIS PATT RE
   Zhao Rui, 2013, P INT C COMP VIS
   Zhao Rui, 2013, P C COMP VIS PATT RE
NR 45
TC 39
Z9 40
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2017
VL 13
IS 2
AR 13
DI 10.1145/3038916
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA EV1VK
UT WOS:000401537300001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, HJ
   Wang, Y
   Huang, JW
AF Wu, Haojun
   Wang, Yong
   Huang, Jiwu
TI Identification of Reconstructed Speech
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Audio forensics; reconstructed speech; identification; speaker
   verification; MFCC; GMM supervectors; LDA-ensemble classification
ID SUPPORT VECTOR MACHINES; SPEAKER; MODEL; ADAPTATION; ALGORITHMS; PHASE
AB Both voice conversion and hidden Markey model (HAIM) based speech synthesis can be used to produce artificial voices of a target speaker. They have shown great negative impacts on speaker verification (SV) systems. In order to enhance the security of SV systems, the techniques to detect converted/synthesized speech should be taken into consideration. During voice conversion and IIMM-based synthesis, speech reconstruction is applied to transform a set of acoustic parameters to reconstructed speech. Hence, the identification of reconstructed speech can be used to distinguish converted/synthesized speech from human speech. Several related works on such identification have been reported. The equal error rates (EERs) lower than 5% of detecting' reconstructed speech have been achieved. However, through the cross-database evaluations on different speech databases, we find that the EERs of several testing cases are higher than 10%. The robustness of detection algorithms to different speech databases needs to be improved. In this article, we propose an algorithm to identify the reconstructed speech. Three different speech databases and two different reconstruction methods are considered in our work, which has not been addressed in the reported works. The high-dimensional data visualization approach is used to analyze the effect of speech reconstruction on Mel-frequency cepstral coefficients (MFCC) of speech signals. The Gaussian mixture model supervectors of MFCC are used as acoustic features. Furthermore, a set of commonly used classification algorithms are applied to identify reconstructed speech. According to the comparison among different classification methods, linear discriminant analysis-ensemble classifiers are chosen in our algorithm. Extensive experimental results show that the EERs lower than 1% can be achieved by the proposed algorithm in most cases, outperforming the reported state-of-the-art identification techniques.
C1 [Wu, Haojun; Huang, Jiwu] Shenzhen Univ, Shenzhen, Peoples R China.
   [Wu, Haojun] Sun Yat Sen Univ, Guangzhou, Peoples R China.
   [Wang, Yong] Guangdong Polytechn Normal Univ, Guangzhou, Peoples R China.
   [Wu, Haojun; Huang, Jiwu] Shenzhen Univ, Coll Informat Engn, Shenzhen Key Lab Media Secur, Nanhai Ave 3688, Shenzhen, Peoples R China.
   [Wang, Yong] Guangdong Polytech Normal Univ, Sch Elect & Informat, West Zhongshan Ave 293, Guangzhou, Guangdong, Peoples R China.
C3 Shenzhen University; Sun Yat Sen University; Guangdong Polytechnic
   Normal University; Shenzhen University; Guangdong Polytechnic Normal
   University
RP Wu, HJ; Huang, JW (corresponding author), Shenzhen Univ, Shenzhen, Peoples R China.; Wu, HJ (corresponding author), Sun Yat Sen Univ, Guangzhou, Peoples R China.; Wang, Y (corresponding author), Guangdong Polytechn Normal Univ, Guangzhou, Peoples R China.; Wu, HJ; Huang, JW (corresponding author), Shenzhen Univ, Coll Informat Engn, Shenzhen Key Lab Media Secur, Nanhai Ave 3688, Shenzhen, Peoples R China.; Wang, Y (corresponding author), Guangdong Polytech Normal Univ, Sch Elect & Informat, West Zhongshan Ave 293, Guangzhou, Guangdong, Peoples R China.
EM wuhaojun@mail2.sysu.edu.cn; isswy@mail.sysu.edu.cn; jwhuang@szu.edu.cn
RI huang, jw/KVY-9917-2024
FU National Natural Science Foundation of China [61332012, U1636202,
   61672173]; Shenzhen R and D Program [GJHZ20140418191518323,
   JCYJ20160328144421330]; Characteristic innovation project of Guangdong
   Province Ordinary University [2015KTSCX083]
FX This work was supported by the National Natural Science Foundation of
   China (61332012, U1636202, 61672173), the Shenzhen R and D Program
   (GJHZ20140418191518323 and JCYJ20160328144421330), and the
   Characteristic innovation project of Guangdong Province Ordinary
   University (2015KTSCX083).
CR Alsteris LD, 2007, DIGIT SIGNAL PROCESS, V17, P578, DOI 10.1016/j.dsp.2006.06.007
   [Anonymous], INT J RES REV APPL S
   [Anonymous], 1999, P EUROSPEECH
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Campbell William M., 2003, P NEUR INF PROC SYST
   Campbell WM, 2006, COMPUT SPEECH LANG, V20, P210, DOI 10.1016/j.csl.2005.06.003
   Campbell WM, 2006, IEEE SIGNAL PROC LET, V13, P308, DOI 10.1109/LSP.2006.870086
   Chang Chih-Chung, 2014, LIBLINEAR LIB LARGE
   Chang Chih-Chung., 2014, LIBSVM: a library for support vector machines
   De Leon PL, 2012, IEEE T AUDIO SPEECH, V20, P2280, DOI 10.1109/TASL.2012.2201472
   De Leon PL, 2010, INT CONF ACOUST SPEE, P1798, DOI 10.1109/ICASSP.2010.5495413
   De Leon Phillip L., 2010, P OD SPEAK LANG REC
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Fukada T., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P137, DOI 10.1109/ICASSP.1992.225953
   Garofolo John, 2012, TIMIT ACOUSTIC PHONE
   Garofolo John, 2013, WALL STREET J SPEECH
   Hautamäki V, 2008, IEEE SIGNAL PROC LET, V15, P162, DOI 10.1109/LSP.2007.914792
   Hegde RM, 2007, EURASIP J AUDIO SPEE, DOI 10.1155/2007/79032
   Jin Q, 2008, INT CONF ACOUST SPEE, P4845
   Kain A, 2001, INT CONF ACOUST SPEE, P813, DOI 10.1109/ICASSP.2001.941039
   Kawahara H, 1999, SPEECH COMMUN, V27, P187, DOI 10.1016/S0167-6393(98)00085-5
   Kenny P, 2007, IEEE T AUDIO SPEECH, V15, P1435, DOI 10.1109/TASL.2006.881693
   Kinnunen T, 2012, INT CONF ACOUST SPEE, P4401, DOI 10.1109/ICASSP.2012.6288895
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Masuko T, 1999, P EUROSPEECH
   MATSUI T, 1995, SPEECH COMMUN, V17, P109, DOI 10.1016/0167-6393(95)00011-C
   NIST Multimodal Information Group, 2010, 2006 NIST SPEAK REC
   Reynolds D., 2015, ENCY BIOMETRICS, P827, DOI 10.1007/978-1-4899-7488-4_196
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Saratxaga I, 2009, ELECTRON LETT, V45, P381, DOI 10.1049/el.2009.3328
   Saratxaga I, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1185
   Speech Database Committee of the Priority Areas Project, 2013, ADV UT MULT PROM HIG
   Stylianou Y, 1998, IEEE T SPEECH AUDI P, V6, P131, DOI 10.1109/89.661472
   Stylianou Y, 2001, IEEE T SPEECH AUDI P, V9, P21, DOI 10.1109/89.890068
   Tamura Masatsune, 1998, P 3 ESCA COCOSDA WOR
   Toda T, 2001, INT CONF ACOUST SPEE, P841, DOI 10.1109/ICASSP.2001.941046
   Toda T, 2007, IEEE T AUDIO SPEECH, V15, P2222, DOI 10.1109/TASL.2007.907344
   Toda T, 2007, IEICE T INF SYST, VE90D, P816, DOI 10.1093/ietisy/e90-d.5.816
   Tokuda K, 2000, INT CONF ACOUST SPEE, P1315, DOI 10.1109/ICASSP.2000.861820
   van der Maaten L., 2014, T DISTRIBUTED STOCHA
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wu ZZ, 2013, INT CONF ACOUST SPEE, P7234, DOI 10.1109/ICASSP.2013.6639067
   Yamagishi J, 2009, IEEE T AUDIO SPEECH, V17, P1208, DOI 10.1109/TASL.2009.2016394
   Yamagishi J, 2009, IEEE T AUDIO SPEECH, V17, P66, DOI 10.1109/TASL.2008.2006647
   Zen H, 2009, SPEECH COMMUN, V51, P1039, DOI 10.1016/j.specom.2009.04.004
NR 46
TC 2
Z9 2
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2017
VL 13
IS 1
AR 10
DI 10.1145/3004055
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM7WJ
UT WOS:000395522700010
DA 2024-07-18
ER

PT J
AU Li, YB
   Gai, KK
   Ming, Z
   Zhao, H
   Qiu, MK
AF Li, Yibin
   Gai, Keke
   Ming, Zhong
   Zhao, Hui
   Qiu, Meikang
TI Intercrossed Access Controls for Secure Financial Services on Multimedia
   Big Data in Cloud Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Intercrossed access control; trustmanagement; secure financial services;
   multimedia big data; cloud computing
ID PRIVACY; MODEL
AB The dramatically growing demand of Cyber Physical and Social Computing (CPSC) has enabled a variety of novel channels to reach services in the financial industry. Combining cloud systems with multimedia big data is a novel approach for Financial Service Institutions (FSIs) to diversify service offerings in an efficient manner. However, the security issue is still a great issue in which the service availability often conflicts with the security constraints when the service media channels are varied. This paper focuses on this problem and proposes a novel approach using the Semantic-Based Access Control (SBAC) techniques for acquiring secure financial services on multimedia big data in cloud computing. The proposed approach is entitled IntercroSsed Secure Big Multimedia Model (2SBM), which is designed to secure accesses between various media through the multiple cloud platforms. The main algorithms supporting the proposed model include the Ontology-Based Access Recognition (OBAR) Algorithm and the Semantic Information Matching (SIM) Algorithm. We implement an experimental evaluation to prove the correctness and adoptability of our proposed scheme.
C1 [Li, Yibin] Shandong Univ, Sch Comp Sci & Technol, Shandong, Peoples R China.
   [Gai, Keke; Qiu, Meikang] Pace Univ, Dept Comp Sci, New York, NY 10038 USA.
   [Ming, Zhong] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Peoples R China.
   [Zhao, Hui] Henan Univ, Software Sch, Kaifeng 475000, Henan, Peoples R China.
C3 Shandong University; Pace University; Shenzhen University; Henan
   University
RP Ming, Z (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Peoples R China.
EM liyibing@sdu.edu.cn; kg71231w@pace.edu; mingz@suz.edu.cn;
   zhh@henu.edu.cn; mqiu@pace.edu
RI Gai, Keke/H-7138-2019; Hui, Zhao/HOF-5837-2023
OI Gai, Keke/0000-0001-6784-0221; Hui, Zhao/0000-0003-3205-3820
FU National Science Foundation [NSF CNS-1457506, NSF CNS-1359557];
   International Science and Technology Cooperation Program of China
   [2014DFR70730]
FX This work is supported by the National Science Foundation, under grant
   NSF CNS-1457506 and NSF CNS-1359557. This work is also supported by the
   International Science and Technology Cooperation Program of China under
   Grant 2014DFR70730.
CR Alamri A, 2013, IEEE T DEPEND SECURE, V10, P328, DOI 10.1109/TDSC.2013.20
   [Anonymous], 2011, J INF PRIV SECUR
   Badr Y, 2011, IEEE T SERV COMPUT, V4, P243, DOI 10.1109/TSC.2010.18
   Barker S, 2012, IEEE T DEPEND SECURE, V9, P670, DOI 10.1109/TDSC.2012.22
   Blaze M, 2009, COMPUTER, V42, P44, DOI 10.1109/MC.2009.51
   Carolina VLM, 2015, INT J DISTRIB SENS N, V11, P1
   Dou WC, 2015, IEEE T PARALL DISTR, V26, P455, DOI 10.1109/TPDS.2013.246
   Espinoza M, 2011, FORENSIC SCI INT, V204, P41, DOI 10.1016/j.forsciint.2010.05.002
   Gai K., 2015, Security and Communication Networks, P1, DOI DOI 10.1109/CSCLOUD.2015.73
   Gai KK, 2016, J NETW COMPUT APPL, V59, P46, DOI 10.1016/j.jnca.2015.05.016
   Gai KK, 2015, IEEE I C EMBED SOFTW, P1332, DOI 10.1109/HPCC-CSS-ICESS.2015.250
   Gai KK, 2012, INT C MULTIMED INFO, P142, DOI 10.1109/MINES.2012.240
   Gupta P, 2014, IEEE T DEPEND SECURE, V11, P412, DOI 10.1109/TDSC.2013.42
   Hu HX, 2013, IEEE T KNOWL DATA EN, V25, P1614, DOI 10.1109/TKDE.2012.97
   Ifinedo Princely, 2014, International Journal of Electronic Business Management, V12, P75
   Li Y, 2015, IEEE WIREL COMMUN, V22, P15, DOI 10.1109/MWC.2015.7368820
   Lili Sun, 2012, 2012 Proceedings of IEEE 16th International Conference on Computer Supported Cooperative Work in Design (CSCWD 2012), P512, DOI 10.1109/CSCWD.2012.6221866
   Liu AX, 2011, IEEE T PARALL DISTR, V22, P1969, DOI 10.1109/TPDS.2011.114
   Liu FH, 2013, J INTERNET TECHNOL, V14, P935, DOI 10.6138/JIT.2013.14.6.08
   Meng SM, 2014, IEEE T PARALL DISTR, V25, P3221, DOI 10.1109/TPDS.2013.2297117
   Pai YT, 2012, IEEE T BROADCAST, V58, P221, DOI 10.1109/TBC.2012.2189610
   Pervaiz Z, 2014, IEEE T KNOWL DATA EN, V26, P795, DOI 10.1109/TKDE.2013.71
   Qiu MK, 2018, FUTURE GENER COMP SY, V80, P421, DOI 10.1016/j.future.2016.01.006
   Qiu MK, 2015, IEEE T COMPUT, V64, P3528, DOI 10.1109/TC.2015.2409857
   Qiu MK, 2011, IEEE T SMART GRID, V2, P715, DOI 10.1109/TSG.2011.2160298
   Qiu MK, 2009, ACM T DES AUTOMAT EL, V14, DOI 10.1145/1497561.1497568
   Ramani R., 2012, INT J COMPUT APPL, V57, P15
   Ray I, 2005, IEEE T KNOWL DATA EN, V17, P844, DOI 10.1109/TKDE.2005.88
   Rico M, 2011, IEEE INTERNET COMPUT, V15, P58, DOI 10.1109/MIC.2011.83
   Runde S, 2011, IEEE T IND INFORM, V7, P723, DOI 10.1109/TII.2011.2166784
   Sánchez R, 2012, IEEE T CONSUM ELECTR, V58, P95, DOI 10.1109/TCE.2012.6170060
   Sohr K, 2008, IEEE T KNOWL DATA EN, V20, P924, DOI 10.1109/TKDE.2008.28
   Sun HM, 2012, IEEE T INF FOREN SEC, V7, P651, DOI 10.1109/TIFS.2011.2169958
   Sundareswaran S, 2012, IEEE T DEPEND SECURE, V9, P556, DOI 10.1109/TDSC.2012.26
   Tao LX, 2015, 9TH IEEE INTERNATIONAL SYMPOSIUM ON SERVICE-ORIENTED SYSTEM ENGINEERING (SOSE 2015), P127, DOI 10.1109/SOSE.2015.10
   Toninelli A, 2009, IEEE WIREL COMMUN, V16, P24, DOI 10.1109/MWC.2009.5109461
   Zhu BB, 2014, IEEE T INF FOREN SEC, V9, P891, DOI 10.1109/TIFS.2014.2312547
   Zissis D, 2012, FUTURE GENER COMP SY, V28, P583, DOI 10.1016/j.future.2010.12.006
NR 38
TC 61
Z9 63
U1 0
U2 33
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2016
VL 12
IS 4
SU S
AR 67
DI 10.1145/2978575
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8DS
UT WOS:000392736100011
DA 2024-07-18
ER

PT J
AU Wu, DP
   Yang, BR
   Wang, HG
   Wang, CG
   Wang, RY
AF Wu, Dapeng
   Yang, Boran
   Wang, Honggang
   Wang, Chonggang
   Wang, Ruyan
TI Privacy-Preserving Multimedia Big Data Aggregation in Large-Scale
   Wireless Sensor Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Wireless multimedia sensor networks; privacy-preserving method; data
   aggregation; distributed compressed sensing
AB To preserve the privacy of multimedia big data and achieve the efficient data aggregation in wireless multimedia sensor networks (WMSNs), a distributed compressed sensing-based privacy-preserving data aggregation (DCSPDA) approach is proposed in this article. First, in this approach, the original multimedia sensor data are compressed and measured by distributed compressed sensing (DCS) and the compressed data measurements are uploaded to the sink, by which the inherent characteristics between sensor data can be obtained. Second, the original multimedia data are jointly recovered and the common and innovation sparse components are obtained through solving the optimization problem and linear equations at the sink. Third, through least squares support vector machine (LSSVM) learning of the sparse components, the sparse position configuration can be determined and disseminated for each node to conduct the privacy-preserving data configuration. After receiving the configuration message, original multimedia sensor data are accordingly customized, compressed, and measured by the common measurement matrix, aggregated at the cluster heads, and transmitted to the sink. Finally, the aggregated multimedia sensor data are recovered by the sink according to the data configuration to achieve the privacy-preserving data aggregation and transmission. Our comparative simulation results validate the efficiency and scalability of DCSPDA and demonstrate that the proposed approach can effectively reduce the communication overheads and provide reliable privacy-preserving with low computational complexity for WMSNs.
C1 [Wu, Dapeng; Yang, Boran; Wang, Ruyan] Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
   [Wang, Honggang] Univ Massachusetts, Dept Elect & Comp Engn, 285 Old Westport Rd, N Dartmouth, MA 02747 USA.
   [Wang, Chonggang] Interdigital Commun, 781 3rd Ave, King Of Prussia, PA 19406 USA.
C3 Chongqing University of Posts & Telecommunications; University of
   Massachusetts System; University Massachusetts Dartmouth; InterDigital
RP Wu, DP (corresponding author), Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
EM wudp@cqupt.edu.cn; fernandoinamerica@gmail.com; hwang1@umassd.edu;
   drchongwang@gmail.com; wangry@cqupt.edu.cn
RI yang, boran/KHV-1770-2024; Wu, Dapeng/IWE-0674-2023; Wang,
   Honggang/D-6079-2013
OI Wu, Dapeng/0000-0003-2105-9418; Wang, Honggang/0000-0001-9475-2630
FU U.S. National Science Foundation [CNS 1429120]; National Foundation of
   China [61371097]; Youth Talents Training Project of Chongqing Science
   and Technology Commission [cstc2014kjrc-qnrc40001]
FX This work was supported in part by the U.S. National Science Foundation
   (CNS 1429120), the National Foundation of China (61371097), and the
   Youth Talents Training Project of Chongqing Science and Technology
   Commission (cstc2014kjrc-qnrc40001).
CR Akyildiz IF, 2008, P IEEE, V96, P1588, DOI 10.1109/JPROC.2008.928756
   Akyildiz IF, 2007, IEEE WIREL COMMUN, V14, P32, DOI 10.1109/MWC.2007.4407225
   [Anonymous], 2007, Secur. Distrib., Grid, mobile, Pervasive Comput.
   Caione C, 2012, IEEE T IND INFORM, V8, P30, DOI 10.1109/TII.2011.2173500
   Camilli M., 2011, P ACM IEEE INT C DIS, P1, DOI DOI 10.1109/ICDSC.2011.6042944
   Chen BJ, 2015, J MATH IMAGING VIS, V51, P124, DOI 10.1007/s10851-014-0511-6
   Chen C., 2010, P IEEE ICVES 10 JUL, P83, DOI DOI 10.1109/ICVES.2010.5550937
   Duarte MF, 2005, 2005 39th Asilomar Conference on Signals, Systems and Computers, Vols 1 and 2, P1537
   Eslami M, 2014, IRAN CONF ELECTR ENG, P1575, DOI 10.1109/IranianCEE.2014.6999787
   Fasolo E, 2007, IEEE WIREL COMMUN, V14, P70, DOI 10.1109/MWC.2007.358967
   Grieco Luigi Alfredo, 2009, Proceedings of the 2009 Third International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies (UBICOMM 2009), P194, DOI 10.1109/UBICOMM.2009.27
   Groat MM, 2011, IEEE INFOCOM SER, P2024, DOI 10.1109/INFCOM.2011.5935010
   He WB, 2007, IEEE INFOCOM SER, P2045, DOI 10.1109/INFCOM.2007.237
   Islam SMR, 2015, IEEE ACCESS, V3, P678, DOI 10.1109/ACCESS.2015.2437951
   Krishnamachari B, 2002, 22ND INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING SYSTEMS WORKSHOP, PROCEEDINGS, P575, DOI 10.1109/ICDCSW.2002.1030829
   Kundur D, 2008, P IEEE, V96, P112, DOI 10.1109/JPROC.2007.909914
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   Li Y, 2013, IEEE T VEH TECHNOL, V62, P3110, DOI 10.1109/TVT.2013.2256475
   Liu Xiang, 2011, 2011 8th Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks (SECON 2011), P46, DOI 10.1109/SAHCN.2011.5984932
   Luo CQ, 2014, IEEE T PARALL DISTR, V25, P3211, DOI 10.1109/TPDS.2013.2297922
   Ozdemir S, 2009, COMPUT NETW, V53, P2022, DOI 10.1016/j.comnet.2009.02.023
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Perrig A, 2004, COMMUN ACM, V47, P53, DOI 10.1145/990680.990707
   Pudlewski S, 2012, IEEE T MOBILE COMPUT, V11, P1060, DOI 10.1109/TMC.2011.175
   Qiu Fudong, 2014, IEEE T MOBILE COMPUT, V14, P112, DOI 10.1109/TMC.2014.2352253
   Rajagopalan R, 2006, IEEE COMMUN SURV TUT, V8, P48, DOI 10.1109/COMST.2006.283821
   Sun Y, 2012, IEEE T DEPEND SECURE, V9, P785, DOI 10.1109/TDSC.2012.68
   Winkler T., 2013, DSP, P1, DOI DOI 10.1109/INFCOM.2011.5935010
   Wu DP, 2015, IEEE WIREL COMMUN, V22, P66, DOI 10.1109/MWC.2015.7224729
   Wu DP, 2015, IEEE COMMUN MAG, V53, P92, DOI 10.1109/MCOM.2015.7180514
   Xia ZH, 2016, MULTIMED TOOLS APPL, V75, P1947, DOI 10.1007/s11042-014-2381-8
   Xia ZH, 2014, SECUR COMMUN NETW, V7, P1283, DOI 10.1002/sec.864
   Xie SD, 2014, WIRELESS PERS COMMUN, V78, P231, DOI 10.1007/s11277-014-1748-5
   Yu Zhenhua, 2014, P IEEE GLOB C SIGN I, P1, DOI DOI 10.1109/ICMEW.2014.6890579
NR 34
TC 25
Z9 29
U1 2
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2016
VL 12
IS 4
SU S
AR 60
DI 10.1145/2978570
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8DS
UT WOS:000392736100004
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Wang, JZ
   Li, J
AF Zhang, Yu
   Wang, James Z.
   Li, Jia
TI Parallel Massive Clustering of Discrete Distributions
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Discrete distribution; clustering; parallel computing;
   large-scale learning; image annotation; protein clustering
ID K-MEANS; ALGORITHMS
AB The trend of analyzing big data in artificial intelligence demands highly-scalable machine learning algorithms, among which clustering is a fundamental and arguably the most widely applied method. To extend the applications of regular vector-based clustering algorithms, the Discrete Distribution (D2) clustering algorithm has been developed, aiming at clustering data represented by bags of weighted vectors which are well adopted data signatures in many emerging information retrieval and multimedia learning applications. However, the high computational complexity of D2-clustering limits its impact in solving massive learning problems. Here we present the parallel D2-clustering (PD2-clustering) algorithm with substantially improved scalability. We developed a hierarchical multipass algorithm structure for parallel computing in order to achieve a balance between the individual-node computation and the integration process of the algorithm. Experiments and extensive comparisons between PD2-clustering and other clustering algorithms are conducted on synthetic datasets. The results show that the proposed parallel algorithm achieves significant speed-up with minor accuracy loss. We apply PD2-clustering to image concept learning. In addition, by extending D2-clustering to symbolic data, we apply PD2-clustering to protein sequence clustering. For both applications, we demonstrate the high competitiveness of our new algorithm in comparison with other state-of-the-art methods.
C1 [Zhang, Yu; Wang, James Z.] Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
   [Li, Jia] Penn State Univ, Dept Stat, University Pk, PA 16802 USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Pennsylvania State University; Pennsylvania State University -
   University Park
RP Zhang, Y (corresponding author), Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
EM yzz123@ist.psu.edu; jwang@ist.psu.edu; jiali@stat.psu.edu
RI Wang, James/JAD-0675-2023
OI Wang, James/0000-0003-4379-4173
FU National Science Foundation [1027854, 0347148, 0936948]; Direct For
   Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems
   [0347148] Funding Source: National Science Foundation; Division of
   Computing and Communication Foundations; Direct For Computer & Info Scie
   & Enginr [0936948] Funding Source: National Science Foundation; Office
   of Advanced Cyberinfrastructure (OAC); Direct For Computer & Info Scie &
   Enginr [1027854] Funding Source: National Science Foundation
FX This work is supported by the National Science Foundation Grant No.
   1027854, 0347148, and 0936948.
CR Arthur D, 2009, ANN IEEE SYMP FOUND, P405, DOI 10.1109/FOCS.2009.14
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Batu T, 2013, J ACM, V60, DOI 10.1145/2432622.2432626
   BEECKS C, 2011, P ACM INT C MULT, P1433
   Boeckmann B, 2003, NUCLEIC ACIDS RES, V31, P365, DOI 10.1093/nar/gkg095
   Chen WY, 2011, IEEE T PATTERN ANAL, V33, P568, DOI 10.1109/TPAMI.2010.88
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Clement P, 2008, P AM MATH SOC, V136, P333, DOI 10.1090/S0002-9939-07-09020-X
   Dahlhaus E, 2000, J ALGORITHMS, V36, P205, DOI 10.1006/jagm.2000.1090
   Daubechies I., 1992, Ten lectures on wavelets, DOI [DOI 10.1137/1.9781611970104, 10.1137/1.9781611970104]
   Dhillon IS, 2001, MACH LEARN, V42, P143, DOI 10.1023/A:1007612920971
   Enright AJ, 2000, BIOINFORMATICS, V16, P451, DOI 10.1093/bioinformatics/16.5.451
   Ferreira Cordeiro RobsonLeonardo., 2011, P 17 ACM SPECIAL INT, P690, DOI DOI 10.1145/2020408.2020516
   Garrow AG, 2005, BMC BIOINFORMATICS, V6, DOI 10.1186/1471-2105-6-56
   Gersho A., 1992, Vector quantization and signal compression
   Gonina E, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2517151
   Guha S, 2003, IEEE T KNOWL DATA EN, V15, P515, DOI 10.1109/TKDE.2003.1198387
   Huang Y, 2010, BIOINFORMATICS, V26, P680, DOI 10.1093/bioinformatics/btq003
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Kantorovich L., 1942, DOKL AKAD NAUK SSSR, V37, P227, DOI DOI 10.1287/MNSC.5.1.1
   Kelley DR, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-544
   Levina E, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P251, DOI 10.1109/ICCV.2001.937632
   Li J, 2008, IEEE T PATTERN ANAL, V30, P985, DOI 10.1109/TPAMI.2007.70847
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Monge G, 1781, MEMOIRE THEORIE DEBL
   Murty K.G., 1983, LINEAR PROGRAMMING, V57
   OLSON CF, 1995, PARALLEL COMPUT, V21, P1313, DOI 10.1016/0167-8191(95)00017-I
   Paccanaro A, 2006, NUCLEIC ACIDS RES, V34, P1571, DOI 10.1093/nar/gkj515
   PEARSON WR, 1990, METHOD ENZYMOL, V183, P63
   Pond SLK, 2010, MOL BIOL EVOL, V27, P520, DOI 10.1093/molbev/msp260
   Rosenberg A., 2007, EMNLP CONLL, P410, DOI DOI 10.7916/D80V8N84
   Sang JT, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037687
   Sigrist CJA, 2013, NUCLEIC ACIDS RES, V41, pE344, DOI 10.1093/nar/gks1067
   Thomas T. M.CoverandJ. A., 1991, ELEMENTS INFORM THEO, DOI 10.1002/0471200611
   Vinh NX, 2010, J MACH LEARN RES, V11, P2837
   Wan XJ, 2007, INFORM SCIENCES, V177, P3718, DOI 10.1016/j.ins.2007.02.045
   Xu D, 2008, IEEE T PATTERN ANAL, V30, P1985, DOI 10.1109/TPAMI.2008.129
   Zhao WZ, 2009, LECT NOTES COMPUT SC, V5931, P674, DOI 10.1007/978-3-642-10665-1_71
   Zheng QF, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1404880.1404887
NR 41
TC 5
Z9 7
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2015
VL 11
IS 4
AR 49
DI 10.1145/2700293
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CK0ZF
UT WOS:000355933700002
DA 2024-07-18
ER

PT J
AU Luque, FP
   Galloso, I
   Feijoo, C
   Martín, CA
   Cisneros, G
AF Pedro Luque, Francisco
   Galloso, Iris
   Feijoo, Claudio
   Alberto Martin, Carlos
   Cisneros, Guillermo
TI Integration of Multisensorial Stimuli and Multimodal Interaction in a
   Hybrid 3DTV System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Human Factors; Experimentation; Multimedia information systems;
   multisensorial multimodal media; immersive media; interactive media;
   hybrid-based 3DTV; binaural audio; sensory effects; interactive 3D
   objects integrated into the video scene; second screen; quality of
   experience
ID SENSORY EXPERIENCE; FRAMEWORK; QUALITY
AB This article proposes the integration of multisensorial stimuli and multimodal interaction components into a sports multimedia asset under two dimensions: immersion and interaction. The first dimension comprises a binaural audio system and a set of sensory effects synchronized with the audiovisual content, whereas the second explores interaction through the insertion of interactive 3D objects into the main screen and on-demand presentation of additional information in a second touchscreen. We present an end-to-end solution integrating these components into a hybrid (internet-broadcast) television system using current 3DTV standards. Results from an experimental study analyzing the perceived quality of these stimuli and their influence on the Quality of Experience are presented.
C1 [Alberto Martin, Carlos; Cisneros, Guillermo] GATV ETSI Telecomun, Madrid 28040, Spain.
   [Pedro Luque, Francisco; Galloso, Iris; Feijoo, Claudio; Alberto Martin, Carlos; Cisneros, Guillermo] Univ Politecn Madrid, E-28040 Madrid, Spain.
C3 Universidad Politecnica de Madrid
RP Luque, FP (corresponding author), Edificio CeDInt UPM,Campus Montegancedo, Madrid 28223, Spain.
EM franluque@cedint.upm.es; iris@cedint.upm.es; cfeijoo@cedint.upm.es;
   cam@gatv.ssr.upm.es; gcp@gatv.ssr.upm.es
RI Feijoo, Claudio/AAR-7878-2020
OI Feijoo, Claudio/0000-0002-9499-7790
FU Spanish Ministry of Industry Tourism and Commerce [TSI-020302-2010-61]
FX The work described here has been developed partially within the
   framework of the ImmersiveTV project funded by the Spanish Ministry of
   Industry Tourism and Commerce Grant # TSI-020302-2010-61. Authors'
   addresses: F. P. Luque (corresponding author), I. Galloso, and C.
   Feijoo, Edificio CeDInt-UPM, Campus de Montegancedo, 28223, Pozuelo de
   Alarcon, Madrid, Spain; emails: {franluque, iris, cfeijoo}@
   cedint.upm.es; C. A. Martin and G. Cisneros, GATV ETSI de
   Telecomunicacion, Ciudad Universitaria s/n, 28040, Madrid, Spain;
   emails: {cam, gcp}@gatv.ssr.upm.es.
CR [Anonymous], 2013, PROCEEDING EUROPEAN
   [Anonymous], 2002, ITUR200250011
   [Anonymous], 2000, SUBJ ASS STER TEL PI
   [Anonymous], 2012, METH SUBJ ASS QUAL T
   [Anonymous], 2000, 138181 ISOIEC
   [Anonymous], 2011, THESIS TAMPERE U TEC
   [Anonymous], 1998, ITUT1998P911
   [Anonymous], 1992, SENSATION PERCEPTION
   Cha J, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1596990.1596993
   Cobos M., 2013, MULTIMEDIA SYST
   ETSI, 2010, 102905 ETSI TS
   ETSI, 2006, 101812 ETSI DVB MHP
   ETSI, 2012, 102796 ETSI TS
   ETSI, 102809 ETSI TS
   ETSI, 2012, 1015472 ETSI TS 2
   Galloso I., 2012, P NEM SUMM IMPL FUT
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   HEIGHTMAN DW, 1975, J I ELECTRON RAD ENG, V45, P559, DOI 10.1049/ree.1975.0110
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   INSPIRA, 2006, INF US TEL DIG INT P
   ISO/IEC, 1998, 135226 ISOIEC 6
   ISO/IEC, 2013, 230053 ISOIEC 3
   ISO/IEC, 2007, 145433 ISOIEC 3
   Jones B. R., 2013, P SIGCHI C HUM FACT, P869
   Kowalczuk J, 2013, IEEE T CIRC SYST VID, V23, P94, DOI 10.1109/TCSVT.2012.2203200
   Mills P., 2011, WHP208BBC R D
   Murray Niall., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys'13, P162, DOI [https://doi.org/10.1145/2483977.2483999, DOI 10.1145/2483977.2483999]
   Nabi RL, 2004, COMMUN THEOR, V14, P288, DOI 10.1093/ct/14.4.288
   Olsen DR, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2168996.2168999
   Ramos G, 2013, J ACOUST SOC AM, V134, P1735, DOI 10.1121/1.4817881
   Raney AA, 2002, J COMMUN, V52, P402, DOI 10.1111/j.1460-2466.2002.tb02552.x
   Ruyter B.D., 2004, P WORKING C ADV VISU, P203, DOI DOI 10.1145/989863.989897
   Schuemie MJ, 2001, CYBERPSYCHOL BEHAV, V4, P183, DOI 10.1089/109493101300117884
   Schulzrinne H, 2003, RTP TRANSPORT PROTOC
   Slater M, 1997, PRESENCE-VIRTUAL AUG, V6, P603, DOI 10.1162/pres.1997.6.6.603
   Slater MD, 2003, J COMMUN, V53, P105, DOI 10.1111/j.1460-2466.2003.tb03008.x
   Timmerer C, 2012, SIGNAL PROCESS-IMAGE, V27, P909, DOI 10.1016/j.image.2012.01.016
   Waltl M., 2010, P 11 INT WORKSH IM A, P1
   Waltl M, 2013, SIGNAL PROCESS-IMAGE, V28, P136, DOI 10.1016/j.image.2012.10.009
   Waltl M, 2009, INT WORK QUAL MULTIM, P145, DOI 10.1109/QOMEX.2009.5246962
   Winer D., 1999, XML REMOTE PROCEDURE
   Yang ZY, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1671962.1671963
   Yoon K, 2013, SIGNAL PROCESS-IMAGE, V28, P127, DOI 10.1016/j.image.2012.10.008
   Zillmann Dolf., 2000, MEDIA ENTERTAINMENT
NR 44
TC 6
Z9 6
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2014
VL 11
IS 1
SU S
SI SI
AR 16
DI 10.1145/2617992
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AS0RG
UT WOS:000343984800008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Wang, Z
   Zhu, WW
   Chen, XW
   Sun, LF
   Liu, JC
   Chen, MH
   Cui, P
   Yang, SQ
AF Wang, Zhi
   Zhu, Wenwu
   Chen, Xiangwen
   Sun, Lifeng
   Liu, Jiangchuan
   Chen, Minghua
   Cui, Peng
   Yang, Shiqiang
TI Propagation-Based Social-Aware Multimedia Content Distribution
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurement; Design; Social network; video service
ID NETWORKS; REPLICATION
AB Online social networks have reshaped how multimedia contents are generated, distributed, and consumed on today's Internet. Given the massive number of user-generated contents shared in online social networks, users are moving to directly access these contents in their preferred social network services. It is intriguing to study the service provision of social contents for global users with satisfactory quality of experience. In this article, we conduct large-scale measurement of a real-world online social network system to study the social content propagation. We have observed important propagation patterns, including social locality, geographical locality, and temporal locality. Motivated by the measurement insights, we propose a propagation-based social-aware delivery framework using a hybrid edge-cloud and peer-assisted architecture. We also design replication strategies for the architecture based on three propagation predictors designed by jointly considering user, content, and context information. In particular, we design a propagation region predictor and a global audience predictor to guide how the edge-cloud servers backup the contents, and a local audience predictor to guide how peers cache the contents for their friends. Our trace-driven experiments further demonstrate the effectiveness and superiority of our design.
C1 [Wang, Zhi; Zhu, Wenwu; Sun, Lifeng; Cui, Peng; Yang, Shiqiang] Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Chen, Xiangwen; Chen, Minghua] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
   [Liu, Jiangchuan] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Tsinghua University; Chinese University of Hong Kong; Simon Fraser
   University
RP Wang, Z (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM wangzhi04@mails.tsinghua.edu.cn; wwzhu@tsingua.edu.cn;
   cxw1987@gmail.com; sunlf@tsingua.edu.cn; jcliu@cs.sfu.ca;
   minghua@ie.cuhk.edu.hk; cuip@tsingua.edu.cn; yangshq@tsingua.edu.cn
RI yang, shiqiang/AAH-5484-2019; Chen, Minghua/A-7476-2012
OI Chen, Minghua/0000-0003-4763-0037
FU National Basic Research Program (973) [2011CB302206]; National Natural
   Science Foundation on China [60933013, 61272231, 61003097]; Thousand
   Talents Program [553360001]; MOST International Cooperative Project
   [2013DFG12870]; Tsinghua-Tencent Joint Lab
FX This work was partially supported by the National Basic Research Program
   (973) under Grant No. 2011CB302206, the National Natural Science
   Foundation on China under Grant Nos. 60933013, 61272231, and 61003097,
   the "Thousand Talents Program" under Grant No. 553360001, the MOST
   International Cooperative Project under Grant No. 2013DFG12870, and the
   Tsinghua-Tencent Joint Lab.
CR ADHIKARI V., 2011, P IEEE INT C COMP CO
   [Anonymous], P IEEE HOT TOP MED D
   [Anonymous], 2003, P 9 ACM SIGKDD INT C
   BENEVENUTO F., 2009, P INT MEAS C ACM IMC
   Cha M., 2009, P ACM WWW
   CHENG X., 2008, P IEEE WORKSH QUAL S
   Cheng X, 2011, P ACM NOSSDAV
   Cheng Xu, 2009, P IEEE INFOCOM
   Dodds PS, 2005, J THEOR BIOL, V232, P587, DOI 10.1016/j.jtbi.2004.09.006
   DOMINGOS P., 2001, P ACM SIGKDD KNOWL D
   Fehr E, 2002, ECON J, V112, pC1, DOI 10.1111/1468-0297.00027
   HARTLINE J., 2008, P ACM WWW
   HUFFAKER B, 2002, P IEEE INT TEL S ITS
   Kangasharju J, 2002, COMPUT COMMUN, V25, P376, DOI 10.1016/S0140-3664(01)00409-1
   KRISHNAMURTHY B., 2008, P ACM WORKSH SOC NET
   Kwak H., 2010, P ACM WWW
   LI H., 2012, P ACM NOSSDAV
   Liu Y, 2008, PEER PEER NETW APPL, V1, P18, DOI 10.1007/s12083-007-0006-y
   Luo JG, 2009, IEEE T PARALL DISTR, V20, P59, DOI 10.1109/TPDS.2008.68
   MELVILLE P., 2002, P NAT C ART INT
   MISLOVE A., 2012, P NSF WORKSH SOC NET
   MISLOVE A., 2010, P ACM WSDM C
   Mislove A., 2007, P ACM INT MEAS C IMC
   Nguyen K., 2011, P IEEE ICDCS WORKSH
   PENG G., 2004, CS0411069 ARX
   Pujol JM, 2010, ACM SIGCOMM COMP COM, V40, P375, DOI 10.1145/1851275.1851227
   SCELLATO S., 2010, P C ONL SOC NETW
   Tran DA, 2012, COMPUT NETW, V56, P2001, DOI 10.1016/j.comnet.2012.02.010
   WANG Z., 2012, P IEEE INFOCOM MIN
   WIKI, 2013, CLUST COEFF
   Wu Y., 2012, P IEEE INFOCOM
   Xu DY, 2006, MULTIMEDIA SYST, V11, P383, DOI 10.1007/s00530-006-0015-3
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
   [No title captured]
NR 34
TC 12
Z9 12
U1 1
U2 32
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 52
DI 10.1145/2523001.2523005
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700022
DA 2024-07-18
ER

PT J
AU Whitman, B
AF Whitman, Brian
TI Care and Scale: Fifteen Years of Music Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Information retrieval; corporate research factors; cultural
   perception; music retrieval
AB The co-founder of The Echo Nest, a music intelligence company that now powers recommendation and discovery for most music services, discusses the notion of care and scale, cultural analysis of music, a brief history of music retrieval, and how and why The Echo Nest got started.
RP Whitman, B (corresponding author), 48 Grove St Suite 206, Somerville, MA USA.
EM brian@echonest.com
CR [Anonymous], P ACM C HUM FACT COM
   [Anonymous], 2000, ISMIR
   Berenzweig A, 2003, P INT S MUS INF RETR
   BYRD D., 2002, D LIB MAG, V8, P11
   Cohen WW, 2000, COMPUT NETW, V33, P685, DOI 10.1016/S1389-1286(00)00057-8
   Foote JT, 1997, P SOC PHOTO-OPT INS, V3229, P138, DOI 10.1117/12.290336
   JEHAN T, 2005, THESIS MIT
   Tzanetakis G., 2001, P INT S MUSIC INFORM, P205
   Whitman B., 2002, Proceedings of the 2002 International Computer Music Conference, P591
   Whitman B., 2004, P INT S MUS INF RETR
NR 10
TC 1
Z9 1
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 46
DI 10.1145/2492703
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700016
DA 2024-07-18
ER

PT J
AU Wang, B
   Wang, JQ
   Lu, HQ
AF Wang, Bo
   Wang, Jinqiao
   Lu, Hanqing
TI Exploiting Content Relevance and Social Relevance for Personalized Ad
   Recommendation on Internet TV
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Measurement; Experimentation; Personalized recommendation;
   video alignment; video advertising; Internet TV
ID DIGITAL TV; USER; SYSTEM
AB There have been not many interactions between the two dominant forms of mass communication: television and the Internet, while nowadays the appearance of Internet television makes them more closely. Different with traditional TV in a passive mode of transmission, Internet TV makes it more possible to make personalized service recommendation because of the interactivity between users and the Internet. In this article, we introduce a scheme to provide targeted ad recommendation to Internet TV users by exploiting the content relevance and social relevance. First, we annotate TV videos in terms of visual content analysis and textual analysis by aligning visual and textual information. Second, with user-user, video-video and user-video relationships, we employ Multi-Relationship based Probabilistic Matrix Factorization (MRPMF) to learn representative tags for modeling user preference. And then semantic content relevance (between product/ad and TV video) and social relevance (between product/ad and user interest) are calculated by projecting the corresponding tags into our advertising concept space. Finally, with relevancy scores we make ranking for relevant product/ads to effectively provide users personalized recommendation. The experimental results demonstrate attractiveness and effectiveness of our proposed approach.
C1 [Wang, Bo] Commun Univ China, Ctr High Performance Comp, Beijing 100024, Peoples R China.
   [Wang, Bo] Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Wang, Jinqiao] Commun Univ China, Beijing 100024, Peoples R China.
   [Wang, Jinqiao; Lu, Hanqing] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 Communication University of China; Chinese Academy of Sciences;
   Communication University of China; Chinese Academy of Sciences;
   Institute of Automation, CAS
RP Wang, JQ (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
EM bwang@cuc.edu.cn; jqwang@nlpr.ia.ac.cn; luhq@nlpr.ia.ac.cn
FU 973 Program [2010CB327905]; National Natural Science Foundation of China
   [61070104, 60905008, 61273034]
FX This work was supported by the 973 Program (2010CB327905) and the
   National Natural Science Foundation of China (61070104, 60905008 and
   61273034).
CR Amato G., 1999, RES ADV TECHNOLOGY D, V1696, P670
   [Anonymous], P ACM INT C INF KNOW
   [Anonymous], 2007, P INT WORKSH WORKSH
   [Anonymous], 2010, P 18 ACM INT C MULTI
   [Anonymous], 2008, P 16 ACM INT C MULT, DOI DOI 10.1145/1459359.1459410
   Ardissono L, 2004, HUM-COMPUT INT-SPRIN, P3
   BAUDISCH Patrick., 2002, Proceedings of the 2nd Workshop on Personalization in Future TV, P157
   Billsus D, 2000, USER MODEL USER-ADAP, V10, P147, DOI 10.1023/A:1026501525781
   Bo Wang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3252, DOI 10.1109/ICPR.2010.795
   Burke R, 2002, USER MODEL USER-ADAP, V12, P331, DOI 10.1023/A:1021240730564
   Chen Yifan, 2008, WSDM, P251
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   DIMITROVA N., 2006, ENCY MULTIMEDIA, P640
   EVANS A., 2006, P IEEE INT S CIRC SY
   Grcar M., 2005, P WORKSH END US ASP, P99
   Gunduz S., 2003, Proceedings of the International Workshop on Data Mining for Actionable Knowledge, P46
   Gurrin C., 2010, P ACM INT C MULT, P1513
   Hölbling G, 2010, MULTIMED TOOLS APPL, V46, P259, DOI 10.1007/s11042-009-0352-2
   Holbling G., 2010, P 8 EUR C INT TV VID, P273, DOI [DOI 10.1145/1809777.1809832, 10.1145/1809777.1809832.]
   Hsu SH, 2007, LECT NOTES COMPUT SC, V4471, P166
   Hua XS, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P1, DOI 10.1109/MMSP.2008.4665039
   KURAPATI K., 2001, P INT C US MOD WORKS
   LEKAKOS G., 2004, J COMPUT-MEDIAT COMM, V9, P2
   Li XH, 2008, IEEE T SIGNAL PROCES, V56, P675, DOI 10.1109/TSP.2007.907820
   Li Z., 2011, P 19 ACM INT C MULTI, P133
   Liang C, 2009, LECT NOTES COMPUT SC, V5879, P917, DOI 10.1007/978-3-642-10467-1_82
   Liang C, 2009, LECT NOTES COMPUT SC, V5879, P684, DOI 10.1007/978-3-642-10467-1_60
   Lieberman H, 1995, INT JOINT CONF ARTIF, P924
   Lu YJ, 2011, MULTIMED TOOLS APPL, V51, P247, DOI 10.1007/s11042-010-0621-0
   Ma, 2008, P ACM MULT, P1051
   Ma Hao, 2008, P CIKM08 C INFORM KN, P931
   Martín-Vicente MI, 2011, IEEE T CONSUM ELECTR, V57, P178, DOI 10.1109/TCE.2011.5735500
   Mei T, 2010, P IEEE, V98, P1416, DOI 10.1109/JPROC.2009.2039841
   Mei Tao., 2007, Proceedings of the 15th International Conference on Multimedia, P1075
   Middleton S. E., 2001, Proceedings of the First International Conference on Knowledge Capture, P100, DOI 10.1145/500737.500755
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Shin H, 2009, IEEE T CONSUM ELECTR, V55, P1417, DOI 10.1109/TCE.2009.5278008
   Velusamy S, 2008, MULTIMEDIA SYST, V14, P73, DOI 10.1007/s00530-008-0117-1
   Wang B, 2009, INT CONF DAT MIN WOR, P196, DOI 10.1109/ICDMW.2009.43
   Wang J, 2008, MULTIMED TOOLS APPL, V36, P89, DOI 10.1007/s11042-006-0075-6
   Wang X., 2009, Proc. 3rd international Workshop on Data Mining and Audience Intelligence for Advertising, P18
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P1342, DOI 10.1109/TMM.2008.2004912
   Yu ZW, 2006, USER MODEL USER-ADAP, V16, P63, DOI 10.1007/s11257-006-9005-6
   Zhongming MA, 2007, ACM T INFORM SYST, V25, DOI 10.1145/1198296.1198301
   Zhou XJ, 2006, 2006 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, (WI 2006 MAIN CONFERENCE PROCEEDINGS), P558, DOI 10.1109/WI.2006.186
   Zimmerman J, 2004, HUM-COMPUT INT-SPRIN, P27
NR 46
TC 2
Z9 2
U1 0
U2 43
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2013
VL 9
IS 4
AR 26
DI 10.1145/2501643.2501648
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 206EW
UT WOS:000323501800004
DA 2024-07-18
ER

PT J
AU Lou, XS
   Hwang, K
AF Lou, Xiaosong
   Hwang, Kai
TI Quality of Data Delivery in Peer-to-Peer Video Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Performance; Theory; Peer-to-peer; video streaming; Quality of
   service; Queuing Model
AB QoS in a P2P video streaming system is evaluated in three stages: content generation, data delivery and video playback. We use jitter-free probability as the main performance metric to study Quality of Data delivery (QoD). A new model that incorporates both bandwidth and data availability of P2P network is proposed. Our model relies on a sharing factor that models data availability among all peers. We simulate on a minimalistic network to demonstrate how to apply the analytical model to design a P2P video streaming system with a very low jitter rate. Our simulation experimental results reveal that the lower bound on jitter-free probability is indeed effective to reflect the QoD of the entire system. Our model captures the impact of many design choices, including upload bandwidth limit, peer selection strategies, and video stream chunking schemes.
C1 [Lou, Xiaosong; Hwang, Kai] Univ So Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Hwang, K (corresponding author), Univ So Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
EM xlou@usc.edu; kaihwang@usc.edu
CR AGARWAL A., 2008, P 4 INT C TESTB RES
   AHMED D., 2007, P 16 IEEE INT WORKSH
   BACCICHET P., P IEEE INT C MULT EX
   BETTAHAR H., 2005, P 3 INT C SCI EL TEC
   BIERSACK EW, 2004, P 5 INT WORKSH QUAL
   CAI Z., 2008, P 3 IEEE INT C GRID
   CHAKARESKI J., 2007, P SPIE C VIS COMM IM
   CHEN Y., 2008, PPLIVE CO 1 PAPER LA
   CHENG B., 2008, ACM T MULTIM COMPUT, V4, P4
   CHOU C., 2007, IEEE J SELECT AREAS
   CHRISTIN N, 2005, P 6 ACM C EL COMM
   GKANTSIDIS C, 2006, P ACM SIGCOMM USENIX
   GULLIVER S., 2006, ACM T MULTIM COMPUT, V2, P4
   HAYASAKA M., 2008, IPSJ DIGITAL COURIER, V4, P128
   He JY, 2009, COMPUT NETW, V53, P153, DOI 10.1016/j.comnet.2008.10.014
   HEFEEDA M., 2005, ACM T MULTIM COMPUT, V1, P4
   HUANG Y, 2008, P ACM SIGCOMM DAT CO
   Huang YN, 2007, IEEE J SEL AREA COMM, V25, P131, DOI 10.1109/JSAC.2007.070113
   HWANG K., 2009, IEEE T COMPUT
   Jain M, 2008, COMPUT NETW, V52, P2411, DOI 10.1016/j.comnet.2008.04.019
   KHAN J., 2003, 20030204 KENT STAT U
   Kim J, 2005, IEEE T IMAGE PROCESS, V14, P849, DOI 10.1109/TIP.2005.849335
   KUMAR R., 2007, P ANN JOINT C IEEE C
   KWON J. B., 2003, P 9 INT C PAR COMP
   Li Jin., 2005, P ACM SIGCOMM ASIA W
   Liang C., 2008, P 28 INT C DISTR COM
   LIANG G, 2006, P 3 INT C QUAL SERV
   Liu S, 2008, PERF E R SI, V36, P313, DOI 10.1145/1384529.1375493
   MAGHAREI N., 2007, P ANN JOINT C IEEE C
   MATHIEU B., 2008, P 8 INT C NEW TECHN
   MUSHTAQ M., 2006, P 11 IEEE S COMP COM
   Nakashima T, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P101, DOI 10.1109/IIH-MSP.2008.326
   NGUYEN T, 2002, P SPIE C MULT COMP N
   POURMOHAMMADI-FALLAH Y., 2005, INT C CONS EL ICCE
   ROBERTS JW, 1991, IEEE T COMMUN, V39, P298, DOI 10.1109/26.76467
   ROSS K., 2007, IEEE J SEL AREA COMM, V25, P9
   SAROIU S, 2002, P ANN MULT COMP NETW
   Schollmeier R., 2001, P 1 INT C PEER TO PE, P101
   Setton E, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P569, DOI 10.1109/ICME.2006.262472
   Silverston T., 2007, P 17 INT WORKSH NETW
   Singh A, 2007, IEEE ICC, P1860, DOI 10.1109/ICC.2007.310
   STEINER M., 2009, IEEE ACM T NETWORK, V17, P6
   TSANG D., 2007, IEEE J SEL AREA COMM, V25, P9
   Van Caenegem TNM, 2008, BELL LABS TECH J, V13, P53, DOI 10.1002/bltj.20282
   Wang M, 2007, IEEE T MULTIMEDIA, V9, P1554, DOI 10.1109/TMM.2007.907460
   Yiu WPK, 2007, IEEE MULTIMEDIA, V14, P50, DOI 10.1109/MMUL.2007.30
   ZHANG M., 2007, IEEE J SELECT AREAS
   ZHAO S., 2008, ACM T MULTIMEDIA COM
   ZIMMERMANN R., 2006, ENCY MULTIMEDIA
NR 49
TC 3
Z9 6
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2012
VL 8
IS 1
SI SI
AR 12
DI 10.1145/2089085.2089089
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 898YN
UT WOS:000300778400004
DA 2024-07-18
ER

PT J
AU Maddage, NC
   Li, HZ
AF Maddage, Namunu C.
   Li, Haizhou
TI Beat Space Segmentation and Octave Scale Cepstral Feature for Sung
   Language Recognition in Pop Music
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Languages; Experimentation; Measurement; Verification; Language
   characterization; feature extraction; sung language recognition
ID IDENTIFICATION; TRACKING
AB Sung language recognition relies on both effective feature extraction and acoustic modeling. In this paper, we study rhythm based music segmentation with the frame size being the duration of the smallest note in the music, as opposed to fixed length segmentation in spoken language recognition. It is found that acoustic features extracted from the rhythm based segmentation scheme outperform those from fixed length segmentation. We also study the effectiveness of a musically motivated acoustic feature. Octave scale cepstral coefficients (OSCCs) by comparing with the other acoustic features: Log frequency cepstral coefficients, Linear prediction coefficients (LPC) and LPC-derived cepstral coefficients. Finally, we examine the modeling capabilities of Gaussian mixture models and support vector machines in sung language recognition experiments. Experiments conducted on a corpus of 400 popular songs sung in English, Chinese, German, and Indonesian, showed that the OSCC feature outperforms other features. A sung language recognition accuracy of 64.9% was achieved when Gaussian mixture models were trained on shifted-delta-OSCC acoustic features, extracted via rhythm based music segmentation.
C1 [Maddage, Namunu C.] RMIT Univ, Sch Elect & Comp Engn, Melbourne, Vic 3000, Australia.
   [Li, Haizhou] Inst Infocomm Res I2R, Singapore 138632, Singapore.
C3 Royal Melbourne Institute of Technology (RMIT); Agency for Science
   Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research
   (I2R)
RP Maddage, NC (corresponding author), RMIT Univ, Sch Elect & Comp Engn, Swanston St, Melbourne, Vic 3000, Australia.
EM namunu.maddage@rmit.edu.au; hli@i2r.a-star.edu.sg
RI Li, Haizhou/Q-6438-2019
OI Li, Haizhou/0000-0001-9158-9401
CR Adami A. G., 2003, P 8 EUR C SPEECH COM
   ADDADECKER M, 2003, P INT C PHON SCI ICP
   BROWN JC, 1991, J ACOUST SOC AM, V92, P1933
   Campbell WM, 2006, COMPUT SPEECH LANG, V20, P210, DOI 10.1016/j.csl.2005.06.003
   DAI P, 2003, P MULT INF RETR WORK
   DEUTSCH D, 1986, J ACOUST SOC AM, V80, P1346, DOI 10.1121/1.394387
   DUXBURG C, 2002, P INT C DIG AUD EFF
   ELLIS DPW, 2006, P INT C AC SPEECH SI
   Fletcher H, 1931, J ACOUST SOC AM, V3, pCOVER2
   FRIBERG A, 1995, J ACOUST SOC AM, V98, P2524, DOI 10.1121/1.413218
   FRY DB, 1957, J ACOUST SOC AM, V29, P690, DOI 10.1121/1.1909011
   Gauvain JL, 2000, P IEEE, V88, P1181, DOI 10.1109/5.880079
   Goto M, 2001, J NEW MUSIC RES, V30, P159, DOI 10.1076/jnmr.30.2.159.7114
   GUAN MB, 2002, P INT C SPOK LANG PR
   HAZEN T, 1993, THESIS MIT
   HOUSE AS, 1977, J ACOUST SOC AM, V62, P708, DOI 10.1121/1.381582
   Iskandar D., 2006, P 14 ACM INT C MULTI, P659
   Jiang DN, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P113, DOI 10.1109/ICME.2002.1035731
   JOHN RD, 1999, DISCRETE TIME PROCES
   Jourdain R., 1997, Music, the brain, and ecstasy: How music captures our imagination
   Kim Y., 2003, Strategic advantages of information technology in construction
   KIRCHHOFF K, 2002, P IEEE INT C AC SPEE
   LI H, 2005, P 43 ANN M ASS COMP
   Li HZ, 2007, IEEE T AUDIO SPEECH, V15, P271, DOI 10.1109/TASL.2006.876860
   Lu L, 2006, IEEE T AUDIO SPEECH, V14, P5, DOI 10.1109/TSA.2005.860344
   Maddage N. C., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P67, DOI 10.1145/1148170.1148185
   MADDAGE NC, 2004, P 12 ANN ACM INT C M, P112, DOI DOI 10.1145/1027527.1027549
   MATROUF D, 1998, P INT C SPOK LANG PR
   NWE TL, 2004, P 5 INT S C MUS INF
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   REYNOLDS DA, 2005, IEEE T SPEECH AUDIO, V3, P73
   Rossing T.D., 2001, The Science of Sound, V3rd
   *ROYAL SCH MUS, 1949, RUD THEOR MUS
   SCARINGELLA N, 2006, IEEE SIGN PROCESS MA, V23, P2
   Scheirer ED, 1998, J ACOUST SOC AM, V103, P588, DOI 10.1121/1.421129
   SCHWENNINGER J, 2006, P INT C MUS INF RETR
   Singer E., 2003, P 8 EUR C SPEECH COM
   Stevens SS, 1937, J ACOUST SOC AM, V8, P185, DOI 10.1121/1.1915893
   SUGIYAMA M, 1991, P IEEE INT C AC SPEE
   SUNDBERG JE, 1973, J ACOUST SOC AM, V54, P922, DOI 10.1121/1.1914347
   THYMEGOBBEL AE, 1996, P 4 INT C SPOK LANG
   Torres-Carrasquillo P., 2002, 7 INT C SPOK LANG PR
   TSAI WH, 2004, P INT C MUS INF RETR
   Typke R., 2005, P INT C MUS INF RETR
   Waibel A, 2000, P IEEE, V88, P1297, DOI 10.1109/5.880085
   WARD WD, 1954, J ACOUST SOC AM, V26, P369, DOI 10.1121/1.1907344
   Wijitha Senadeera Wijitha Senadeera, 2006, International Journal of Food Engineering, V2, P7
   Xiao ZZ, 2008, INT WORK CONTENT MUL, P1
   ZHU Y, 2005, P 11 INT MULT MOD C
   Zissman MA, 1996, IEEE T SPEECH AUDI P, V4, P31, DOI 10.1109/TSA.1996.481450
   2011, ACM T MULTIMEDIA COM, V7
   2011, ACM T MULTIMEDIA COM, V7
NR 53
TC 0
Z9 0
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2011
VL 7
IS 4
AR 37
DI 10.1145/2043612.2043615
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 856ZS
UT WOS:000297684000003
DA 2024-07-18
ER

PT J
AU Hossain, MA
   Atrey, PK
   El Saddik, A
AF Hossain, M. Anwar
   Atrey, Pradeep K.
   El Saddik, Abdulmotaleb
TI Modeling and Assessing Quality of Information in Multisensor Multimedia
   Monitoring Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Measurement; Experimentation; Quality of information;
   multimedia monitoring systems; event detection; multimodal fusion
ID DETECTION ALGORITHMS
AB Current sensor-based monitoring systems use multiple sensors in order to identify high-level information based on the events that take place in the monitored environment. This information is obtained through low-level processing of sensory media streams, which are usually noisy and imprecise, leading to many undesired consequences such as false alarms, service interruptions, and often violation of privacy. Therefore, we need a mechanism to compute the quality of sensor-driven information that would help a user or a system in making an informed decision and improve the automated monitoring process. In this article, we propose a model to characterize such quality of information in a multisensor multimedia monitoring system in terms of certainty, accuracy/confidence and timeliness. Our model adopts a multimodal fusion approach to obtain the target information and dynamically compute these attributes based on the observations of the participating sensors. We consider the environment context, the agreement/disagreement among the sensors, and their prior confidence in the fusion process in determining the information of interest. The proposed method is demonstrated by developing and deploying a real-time monitoring system in a simulated smart environment. The effectiveness and suitability of the method has been demonstrated by dynamically assessing the value of the three quality attributes with respect to the detection and identification of human presence in the environment.
C1 [Hossain, M. Anwar] King Saud Univ, CCIS, Software Engn Dept, Riyadh, Saudi Arabia.
   [Atrey, Pradeep K.] Univ Winnipeg, Dept Appl Comp Sci, Winnipeg, MB R3B 2E9, Canada.
   [El Saddik, Abdulmotaleb] Univ Ottawa, Commun Res Lab, Ottawa, ON K1N 6N5, Canada.
C3 King Saud University; University of Winnipeg; University of Ottawa
RP Hossain, MA (corresponding author), King Saud Univ, CCIS, Software Engn Dept, Riyadh, Saudi Arabia.
EM mahossain@ksu.edu.sa
RI Hossain, M. Anwar/J-9601-2013; /D-4159-2009
OI Hossain, M. Anwar/0000-0002-7673-8410; /0000-0002-7690-8547
CR Atrey PK, 2006, MULTIMEDIA SYST, V12, P239, DOI 10.1007/s00530-006-0063-8
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Ballou DP, 1999, COMMUN ACM, V42, P73, DOI 10.1145/291469.291471
   Beccari G, 2005, REAL-TIME SYST, V30, P187, DOI 10.1007/s11241-005-2461-y
   Bisdikian C, 2007, FIFTH ANNUAL IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS WORKSHOPS, PROCEEDINGS, P279, DOI 10.1109/PERCOMW.2007.88
   BLASCH E, 2005, P 8 INT C INF FUS, V1, pR35
   Carvalho HS, 2003, FUSION 2003: PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE OF INFORMATION FUSION, VOLS 1 AND 2, P1465
   Chen DT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198308
   Hall DL, 1997, P IEEE, V85, P6, DOI [10.1109/5.554205, 10.1109/ISCAS.1998.705329]
   Han Q, 2007, IEEE T PARALL DISTR, V18, P158, DOI 10.1109/TPDS.2007.31
   Hossain M. A., 2007, P 3 IET INT C INT EN, P589
   Hossain MA, 2007, I C DATA ENGIN WORKS, P11, DOI 10.1109/ICDEW.2007.4400968
   HOSSAIN MA, 2008, P 1 IEEE WORKSH QUAL
   HUGHES K, 1993, P IEEE INT C ROB AUT, V2, P136
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Kahn B.K., 2002, COMMUN ACM, V46, P184, DOI DOI 10.1145/505248.506007
   Klein A, 2007, I C DATA ENGIN WORKS, P3, DOI 10.1109/ICDEW.2007.4400967
   KNUTH D. E., 1981, Addison-Wesley Series in Computer Science and Information Processing, V2
   LAZAREVICMCMANU.N, 2006, P 4 ACM INT WORKSH V, P45
   Mariano VY, 2002, INT C PATT RECOG, P965, DOI 10.1109/ICPR.2002.1048198
   Miller H, 1996, INFORM SYST MANAGE, V13, P79, DOI 10.1080/10580539608906992
   Muller-Schneiders S., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P137
   Nakamura EF, 2007, ACM COMPUT SURV, V39, DOI 10.1145/1267070.1267073
   Nascimento JC, 2006, IEEE T MULTIMEDIA, V8, P761, DOI 10.1109/TMM.2006.876287
   NEUS A, 2001, P 6 INT C INF QUAL E
   PENG L, 2005, P 2 INT WORKSH DAT M, P45
   Peng L, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1191
   Radke RJ, 2005, IEEE T IMAGE PROCESS, V14, P294, DOI 10.1109/TIP.2004.838698
   Sastry S, 2005, INT J DISTRIB SENS N, V1, P17, DOI 10.1080/15501320490886314
   Schlögl T, 2004, INT C PATT RECOG, P519, DOI 10.1109/ICPR.2004.1333825
   SIEGEL M, 2004, P IEEE INT WORKSH RO, P96
   Snidaro L, 2007, IEEE T SYST MAN CY B, V37, P1044, DOI 10.1109/TSMCB.2007.895331
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Sylvanus E, 1999, 18TH INTERNATIONAL CONFERENCE OF THE NORTH AMERICAN FUZZY INFORMATION PROCESSING SOCIETY - NAFIPS, P635, DOI 10.1109/NAFIPS.1999.781771
   Wald L, 1999, IEEE T GEOSCI REMOTE, V37, P1190, DOI 10.1109/36.763269
   WANG J, 2003, P ACM WORKSH VID SUR
   Wang R. Y., 1996, Journal of Management Information Systems, V12, P5
   WELFORD BP, 1962, TECHNOMETRICS, V4, P419, DOI 10.2307/1266577
   Yates DJ, 2008, PERVASIVE MOB COMPUT, V4, P851, DOI 10.1016/j.pmcj.2008.09.003
   Ziliani F, 2005, AVSS 2005: Advanced Video and Signal Based Surveillance, Proceedings, P201
NR 40
TC 30
Z9 33
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2011
VL 7
IS 1
AR 3
DI 10.1145/1870121.1870124
PG 30
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 712GI
UT WOS:000286653800003
DA 2024-07-18
ER

PT J
AU Shacham, R
   Schulzrinne, H
   Thakolsri, S
   Kellerer, W
AF Shacham, Ron
   Schulzrinne, Henning
   Thakolsri, Srisakul
   Kellerer, Wolfgang
TI Ubiquitous device personalization and use: The next generation of IP
   multimedia communications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE design; standardization; measurement; location-based services;
   ubiquitous computing; Internet multimedia; mobile communications
AB Service usage in emerging ubiquitous environments includes seamless and personalized usage of public and private devices discovered in the vicinity of a user. In our work, we describe an architecture for device discovery, device configuration, and the transfer of active sessions between devices. The presented architecture uses the Session Initiation Protocol (SIP) as a standardized, widely used signaling protocol for IP-based multimedia services. Our solution includes support of simple existing devices, split of sessions between devices, user-control of location-based behavior, and handling of security and privacy concerns. We present the implementation and show the feasibility of our work with analytical evaluation and measurements.
C1 Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
C3 Columbia University
RP Shacham, R (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
EM Rs2194@cs.columbia.edu
RI Kellerer, Wolfgang/E-7271-2017
OI Kellerer, Wolfgang/0000-0003-4358-8038
CR ANDREASEN F, 2006, INTERNET ENG TAST FO, V4568
   [Anonymous], 2004, 3711 RFC
   Berger Stefan., 2003, Proceedings ofthe 13th intemational workshop on Network and operating systems supportfor digital audio and video, ACM, Monterey, CA, USA, P82
   Berners-Lee T., 1998, 2396 RFC
   Camarillo G, 2005, 4117 IETF RFC
   Capkun Srdjan., 2003, MOBIHOC 03 P 4 ACM I, P46
   DIERKS T, 1999, TLS PROTOCOL VERSION
   HANDLEY M, 1998, 2327 RFC
   HASEGAWA M, 2003, P 6 INT S WIR PERS M, P357
   JOHNSTON A, 2007, IN PRESS SESSION INI
   Kaneko Kunitake., 2003, Proceedings o f the Sixth International Symposium on Wireless Personal Multimedia Communications (WPMC 2003), Yokosuka, Japan, P347
   Kempf J., 2001, 3082 RFC
   KIKUTA Y, 2003, P 8 INT C MOB MULT C
   KUTSCHER D, 2003, P 8 INT C PERS WIR C
   MAHY R, 2004, 3891 RFC
   NIEMI A, 2004, 3903 RFC
   *OPENGIS, 2003, 02023RF OGC
   OTT J, 2002, IN PRESS MBUS PROFIL
   OTT J, 2004, IN PRESS RTP PAYLOAD
   OTT J, 2002, 3259 RFC
   Peterson J., 2005, 4119 RFC
   PETRIE D, 2007, IN PRESS FRAMEWORK S
   Roach A. B., 2002, 3265 RFC
   ROSENBERG J, 2007, IN PRESS EXTENSIBLE
   Rosenberg J, 2004, 3856 RFC
   ROSENBERG J, 2007, IN PRESS PRESENCE AU
   ROSENBERG J, 2004, 3725 RFC
   ROSENBERG J, 2006, IN PRESS EXTENSIBLE
   Rosenberg J., 2002, 3264 RFC
   Rosenberg J., 2004, 3841 RFC
   ROSENBERG J, 2004, 3840 RFC
   Schooler E., 2002, 3261 RFC
   SCHULZRINNE H, 2006, ACM MOBILE COMPUT CO, V4
   SCHULZRINNE H, 2006, IN PRESS DOCUMENT FO
   Shacham R, 2005, WiMob'2005: IEEE International Conference on Wireless and Mobile Computing, Networking and Communications, Vol, 4 Proceedings, P73
   SHACHAM R, 2006, IN PRESS SESSION INI
   SHACHAM R, 2004, ACM MOB 04 WORKSH CO
   SHACHAM R, 2006, IN PRESS USE SIP PRE
   SONG H, 2002, 11 INT WORLD WID WEB
   SPARKS R, 2003, 3515 RFC
   SPARKS R, 2004, 3892 RFC
   Sugano H., 2004, 3863 RFC
   VEIZADES J, 1997, 2165 RFC
   WU X, 2004, P 7 IFIP IEEE INT C, P269
   WU X, 2005, P IEEE CONS COMM NET
   Wu XT, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, P789
NR 46
TC 20
Z9 26
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 2
AR 12
DI 10.1145/1230812.1230818
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230JA
UT WOS:000250871600006
DA 2024-07-18
ER

PT J
AU Urruty, T
   Lew, S
   Ihadaddene, N
   Simovici, DA
AF Urruty, Thierry
   Lew, Stanislas
   Ihadaddene, Nacim
   Simovici, Dan A.
TI Detecting eye fixations by projection clustering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; experimentation; performance; eye fixations; interaction
   modeling; projected clustering; static pictures; videos
AB Eye movements are certainly the most natural and repetitive movement of a human being. The most mundane activity, such as watching television or reading a newspaper, involves this automatic activity which consists of shifting our gaze from one point to another.
   Identification of the components of eye movements (fixations and saccades) is an essential part in the analysis of visual behavior because these types of movements provide the basic elements used by further investigations of human vision.
   However, many of the algorithms that detect fixations present a number of problems. In this article, we present a new fixation identification technique that is based on clustering of eye positions, using projections and projection aggregation applied to static pictures. We also present a new method that computes dispersion of eye fixations in videos considering a multiuser environment.
   To demonstrate the performance and usefulness of our approach we discuss our experimental work with two different applications: on fixed image and video.
C1 [Urruty, Thierry; Lew, Stanislas; Ihadaddene, Nacim] Univ Lille 1, F-59655 Villeneuve Dascq, France.
   [Simovici, Dan A.] Univ Massachusetts, Boston, MA 02215 USA.
C3 Universite de Lille; University of Massachusetts System; University of
   Massachusetts Boston
RP Urruty, T (corresponding author), Univ Lille 1, Bat A3 Cite Sci, F-59655 Villeneuve Dascq, France.
EM Thierry.urruty@lifl.fr
CR Agarwal P.K., 2004, P 23 ACM SIGMOD SIGA, P155
   Aggarwal CC, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P61, DOI 10.1145/304181.304188
   Baccino T., 2001, INTERACTIONS HOMME S, P127
   BANKS AS, 1991, J OPT SOC AM, P1775
   BURR DC, 1994, NATURE, V371, P511, DOI 10.1038/371511a0
   CHAUDRI AB, 2002, LECT NOTES COMPUTER, V2490
   Cowen L, 2002, BCS CONF SERIES, P317
   Crossland MD, 2002, OPTOMETRY VISION SCI, V79, P735, DOI 10.1097/00006324-200211000-00011
   Dasgupta S., 1999, Rep. TR-99-006
   Djeraba C., 2003, Multimedia Mining - A Highway to Intelligent Multimedia Documents
   DJERABA C, 2006, P 14 ACM INT C MULT, P23
   ERKELENS CJ, 1975, EYE MOVEMENT RES MEC, P133
   FRANKL P, 1988, J COMB THEORY B, V44, P355, DOI 10.1016/0095-8956(88)90043-3
   Goldberg JH, 2003, MIND'S EYE: COGNITIVE AND APPLIED ASPECTS OF EYE MOVEMENT RESEARCH, P493, DOI 10.1016/B978-044451020-4/50027-X
   GOLDBERG JH, 1995, BEHAV RES METH INSTR, V27, P338, DOI 10.3758/BF03200428
   GOLDSTEIN RB, 2006, COMPUT BIOL MED
   GUBA E, 1964, AUDIO VISUAL COMMUN, P386
   Gulliver SR, 2004, IEEE T SYST MAN CY A, V34, P472, DOI 10.1109/TSMCA.2004.826309
   Jacob R. J. K., 2004, MINDS EYES COGNITIVE
   Jain A. K., 1996, Advances in Image Understanding: A Festschrift for Azriel Rosenfeld, P65
   Jain A. K., 1988, Algorithms for Clustering Data, P446
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Johnson W.B., 1984, CONTEMP MATH-SINGAP, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Karsh R., 1983, Eye movements and psychological functions: International views, P53
   Kumar V., 2006, Introduction to Data Mining
   LANKFORD C, 2000, EYE TRACKING RES APP, P51
   POOLE A, 2004, P C HUM COMP INT HCI, P19
   Prabhakar S, 1998, PROC INT CONF DATA, P94, DOI 10.1109/ICDE.1998.655763
   RAYNER K, 1998, EYE MOVEMENTS INFORM
   Salvucci D.D., 2000, P 2000 S EYE TRACK R, P71, DOI [DOI 10.1145/355017.355028, 10.1145/355017.355028]
   SANTELLA A, 2004, P EY TRACK RES APPL
   TOSI V, 1992, INT J NEUROSCI, P47
   Vempala SantoshS., 2004, The Random Projection Method
   WIDDEL H, 1984, THEORETICAL APPL ASP, P21
   ZAIANE OR, 2002, LECT NOTES ARTIFICIA, V2797
NR 35
TC 9
Z9 10
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 4
AR 23
DI 10.1145/1314303.1314308
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 250QU
UT WOS:000252315900005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, SP
   Cai, WT
   Turner, SJ
   Lee, BS
   Wei, JH
AF Zhou, Suiping
   Cai, Wentong
   Turner, Stephen J.
   Lee, Bu-Sung
   Wei, Junhu
TI Critical causal order of events in distributed virtual environments
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE performance; human factors; algorithms; causal order; distributed
   simulation; virtual environments
AB We investigate the causal order of events in distributed virtual environments (DVEs). We first define the critical causal order relation among the events. Then, we propose some mechanisms to enhance the prevalent RO (receive order delivery) mechanism in DVEs so that the real-time property of DVEs is preserved while the critical causal order violations are reduced. These mechanisms are implemented as a middleware. Experimental results show that the middleware performs well in reducing the critical causality violations in simulation and incurs little processing overhead.
C1 Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   Xi An Jiao Tong Univ, Syst Engn Inst, Xian 719949, Shaanxi, Peoples R China.
C3 Nanyang Technological University; Xi'an Jiaotong University
RP Zhou, SP (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Nanayang Ave, Singapore 639798, Singapore.
EM ASSPZhou@ntu.edu.sg
RI Turner, Stephen/GXM-4654-2022; Lee, Francis BS/G-9323-2014; Cai,
   Wentong/A-3720-2011
OI Lee, Francis BS/0000-0001-7828-7900; Cai, Wentong/0000-0002-0183-3835;
   Turner, Stephen John/0000-0002-7421-9801
CR ANDERSON DB, 1995, IEEE MULTIMEDIA, V2, P77, DOI 10.1109/93.482298
   [Anonymous], 1992, Data networks
   [Anonymous], 1995, ACM Transactions on Computer-Human Interaction (TOCHI), DOI DOI 10.1145/210079.210088
   BALIKHINA T, 2002, P 20 EUR UK C EGUK L
   BIRMAN K, 1991, ACM T COMPUT SYST, V9, P272, DOI 10.1145/128738.128742
   Buehner MJ, 2003, Q J EXP PSYCHOL-A, V56, P865, DOI 10.1080/02724980244000675
   Cai W., 2005, ACM Transactions on Modeling and Computer Simulation, V15, P109, DOI 10.1145/1060576.1060577
   Cai WT, 2002, J PARALLEL DISTR COM, V62, P111, DOI 10.1006/jpdc.2001.1774
   CAVAZZA M, 2005, P 5 C CREAT COGN LON
   Chengzheng Sun, 1998, ACM Transactions on Computer-Human Interaction, V5, P63, DOI 10.1145/274444.274447
   Dahmann JS, 1998, SIMULATION, V71, P378, DOI 10.1177/003754979807100603
   *DMSO, 2002, RTI 1 3NG PROGR GUID
   ELLIS CA, 1989, SIGMOD REC, V18, P399, DOI 10.1145/66926.66963
   FRECON E, 1998, DISTRIB SYST ENG J, V50, P3
   Hagsand O, 1996, IEEE MULTIMEDIA, V3, P30, DOI 10.1109/93.486702
   KATO J, 1999, P INT WORKSH PAR PRO, P21
   LAMPORT L, 1978, COMMUN ACM, V21, P558, DOI 10.1145/359545.359563
   Macedonia M., 1994, PRESENCE, V3, P4
   Maudlin T., 2004, Causation, Counterfactuals and the Third Factor
   Michotte A., 1946, PERCEPTION CAUSALITY
   Pearl Judea, 2000, MODELS REASONING INF, V19, DOI DOI 10.1017/CBO9780511803161
   *PITCH, 2004, PRTI VERS 2 3 PROD S, V1516
   ROBERTS DJ, 1999, P SCS W MULT C JAN
   SCHLOTTMANN A, 1992, Q J EXP PSYCHOL-A, V44, P321, DOI 10.1080/02724989243000055
   Scholl BJ, 2004, PERCEPTION, V33, P455, DOI 10.1068/p5172
   Scholl BJ, 2002, PSYCHOL SCI, V13, P493, DOI 10.1111/1467-9280.00487
   SCHWARZ R, 1994, DISTRIB COMPUT, V7, P149, DOI 10.1007/BF02277859
   Singhal S., 1999, Networked Virtual Environments
   ZHOU S, 2002, P 16 WORKSH PAR DIST
   ZUBERI KM, 1996, P S REL DISTR SYST O
   1995, MERL DIAMOND PARK PR
NR 31
TC 16
Z9 19
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 3
AR 15
DI 10.1145/1236471.1236474
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230JB
UT WOS:000250871700003
DA 2024-07-18
ER

PT J
AU Gopalan, K
   Huang, L
   Peng, G
   Chiueh, TC
   Lin, YJ
AF Gopalan, Kartik
   Huang, Lan
   Peng, Gang
   Chiueh, Tzi-Cker
   Lin, Yow-Jian
TI Statistical admission control using delay distribution measurements
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; measurement; performance
ID CONTROL ALGORITHM
AB Growth of performance sensitive applications, such as voice and multimedia, has led to widespread adoption of resource virtualization by a variety of service providers (xSPS). For instance, Internet Service Providers (ISPs) increasingly differentiate their offerings by means of customized services, such as virtual private networks (VPN) with Quality of service (QoS) guarantees or QVPNs. Similarly Storage Service Providers (SSPs) use storage area networks (SAN)/network attached storage (NAS) technology to provision virtual disks with QoS guarantees or QVDs. The key challenge faced by these xSPs is to maximize the number of virtual resource units they can support by exploiting the statistical multiplexing nature of the customers' input request load.
   While a number of measurement-based admission control algorithms utilize statistical multiplexing along the bandwidth dimension, they do not satisfactorily exploit statistical multiplexing along the delay dimension to guarantee distinct per-virtual-unit delay bounds. This article presents Delay Distribution Measurement (DDM) based admission control algorithm, the first measurement-based approach that effectively exploits statistical multiplexing along the delay dimension. In other words, DDM exploits the well-known fact that the actual delay experienced by most service requests (packets or disk I/O requests) for a virtual unit is usually far smaller than its worst-case delay bound requirement because multiple virtual units rarely send request bursts at the same time. Additionally, DDM supports virtual units with distinct probabilistic delay guarantees-virtual units that can tolerate more delay violations can reserve fewer resources than those that tolerate less, even though they require the same delay bound. Comprehensive trace-driven performance evaluation of QVPNs (using Voice over IP traces) and QVDs (using video stream, TPC-C, and Web search I/O traces) shows that, when compared to deterministic admission control, DDM can potentially increase the number of admitted virtual units (and resource utilization) by up to a factor of 3.
C1 SUNY Binghamton, Binghamton, NY 13902 USA.
   IBM Almaden Res Ctr, San Jose, CA 95120 USA.
   SUNY Stony Brook, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Binghamton; International Business Machines (IBM); State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Gopalan, K (corresponding author), SUNY Binghamton, Binghamton, NY 13902 USA.
EM kartik@cs.binghamton.edu; lanhuang@us.ibm.com; gpeng@cs.sunysb.edu;
   chiueh@cs.sunysb.edu; yjlin@research.telcordia.com
OI Gopalan, Kartik/0000-0003-1078-4446
CR ANDREWS M, 2000, P IEEE INFOCOM MARCH
   [Anonymous], COMMENTS MEASUREMENT
   Boorstyn RR, 2000, IEEE J SEL AREA COMM, V18, P2651, DOI 10.1109/49.898747
   BOUDEC JYL, 2002, IEEE INFOCOM
   BRESLAU L, 2000, P IEEE INFOCOM MARCH
   CHANG CS, 2001, ACM SIGMETRICS 2001, P184
   CROSBY S, 1997, P IEEE ATM
   CRUZ RL, 1991, IEEE T INFORM THEORY, V37, P114, DOI 10.1109/18.61109
   Elwalid A, 1999, IEEE INFOCOM SER, P1220, DOI 10.1109/INFCOM.1999.751679
   GIBBENS RJ, 1997, P 15 INT TEL C JUN
   GOPALAN K, 2001, P ACM MULT NEW YORK
   GUILLEMIN FM, 2002, P IEEE INFOCOM NEW Y
   HUANG L, 2004, P ACM SIGM PERF NEW
   Jamin S, 1997, IEEE ACM T NETWORK, V5, P56, DOI 10.1109/90.554722
   JIANG W, 1996, P ICCCN MARCH
   Kesidis G, 2000, IEEE COMMUN LETT, V4, P26, DOI 10.1109/4234.823539
   Knightly EW, 1999, IEEE NETWORK, V13, P20, DOI 10.1109/65.768485
   LUMB CR, 2003, P 2 USENIX C FIL STO
   Parekh AK, 1993, IEEE ACM T NETWORK, V1, P344, DOI 10.1109/90.234856
   Qiu JY, 2001, IEEE ACM T NETWORK, V9, P199, DOI 10.1109/90.917076
   Reisslein M, 2002, IEEE ACM T NETWORK, V10, P27, DOI 10.1109/90.986511
   SIVARAMAN V, 2000, P IEEE INFOCOM MARCH
   Urgaonkar B., 2002, P S OP SYST DES IMPL
   VERNICK M, 1996, P ACM MULT
   VIN HM, 1994, P ACM MULT
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   ZHANG LX, 1991, ACM T COMPUT SYST, V9, P101, DOI 10.1145/103720.103721
   ZHANG Y, 1991, RANDOM STRUCT ALGOR, V2, P101
   Ziedins I, 1996, STOCHASTIC NETWORKS, V4, P141
   [No title captured]
NR 30
TC 7
Z9 9
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2006
VL 2
IS 4
BP 258
EP 281
DI 10.1145/1201730.1201732
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IY
UT WOS:000250871400002
DA 2024-07-18
ER

PT J
AU Chen, XM
   Zheng, XT
   Lu, XQ
AF Chen, Xiumei
   Zheng, Xiangtao
   Lu, Xiaoqiang
TI Identity Feature Disentanglement for Visible-Infrared Person
   Re-Identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visible-infrared person re-identification; cross-modal; deep learning;
   feature disentanglement
AB Visible-infrared person re-identification (VI-ReID) task aims to retrieve persons from different spectrum cameras (i.e., visible and infrared images). The biggest challenge of VI-ReID is the huge cross-modal discrepancy caused by different imaging mechanisms. Many VI-ReID methods have been proposed by embedding different modal person images into a shared feature space to narrow the cross-modal discrepancy. However, these methods ignore the purification of identity features, which results in identity features containing different modal information and failing to align well. In this article, an identity feature disentanglement method is proposed to disentangle the identity features from identity-irrelevant information, such as pose and modality. Specifically, images of different modalities are first processed to extract shared features that reduce the cross-modal discrepancy preliminarily. Then the extracted feature of each image is disentangled into a latent identity variable and an identity-irrelevant variable. In order to enforce the latent identity variable to contain as much identity information as possible and as little identity-irrelevant information, an ID-discriminative loss and an ID-swapping reconstruction process are additionally designed. Extensive quantitative and qualitative experiments on two popular public VI-ReID datasets, RegDB and SYSU-MM01, demonstrate the efficacy and superiority of the proposed method.
C1 [Chen, Xiumei] Xidian Univ, Hangzhou Inst Technol, Hangzhou 311200, Zhejiang, Peoples R China.
   [Chen, Xiumei] Xidian Univ, Sch Comp Sci Technol, Xian 710071, Shaanxi, Peoples R China.
   [Chen, Xiumei] Chinese Acad Sci, Xian Inst Opt Precis Mech, Key Lab Spectral Imaging Technol CAS, Xian 710119, Shaanxi, Peoples R China.
   [Zheng, Xiangtao; Lu, Xiaoqiang] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Fujian, Peoples R China.
   [Zheng, Xiangtao; Lu, Xiaoqiang] Chinese Acad Sci, Key Lab Spectral Imaging Technol CAS, Xian Inst Opt & Precis Mech, Xian 710119, Shaanxi, Peoples R China.
C3 Xidian University; Xidian University; Chinese Academy of Sciences; Xi'an
   Institute of Optics & Precision Mechanics, CAS; Fuzhou University;
   Chinese Academy of Sciences; Xi'an Institute of Optics & Precision
   Mechanics, CAS
RP Zheng, XT (corresponding author), Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Fujian, Peoples R China.; Zheng, XT (corresponding author), Chinese Acad Sci, Key Lab Spectral Imaging Technol CAS, Xian Inst Opt & Precis Mech, Xian 710119, Shaanxi, Peoples R China.
EM xiangtaoz@gmail.com
OI Lu, Xiaoqiang/0000-0002-7037-5188; Zheng, Xiangtao/0000-0002-8398-6324
FU National Natural Science Foundation of China [62271484]; National
   Science Fund for Distinguished Young Scholars [61925112]; Key Research
   and Development Program of Shaanxi [2023-YBGY-225]; Fundamental Research
   Funds for the Central Universities [XJSJ23007]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62271484, in part by the National
   Science Fund for Distinguished Young Scholars under Grant 61925112, in
   part by the Key Research and Development Program of Shaanxi under Grant
   2023-YBGY-225, and in part by the Fundamental Research Funds for the
   Central Universities under Grant XJSJ23007.
CR Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen XS, 2020, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR42600.2020.00336
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Fu CY, 2019, ADV NEUR IN, V32
   Ge YX, 2018, ADV NEUR IN, V31
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Higgins I., 2016, BETA VAE LEARNING BA
   Hu WP, 2022, IEEE T CIRC SYST VID, V32, P5095, DOI 10.1109/TCSVT.2022.3147813
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang Y, 2022, IEEE T MULTIMEDIA, V24, P1570, DOI 10.1109/TMM.2021.3067760
   Huang YW, 2021, IEEE T CIRC SYST VID, V31, P1790, DOI 10.1109/TCSVT.2020.3014167
   Kai Jungling, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P448, DOI 10.1109/AVSS.2010.75
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kansal K, 2020, IEEE T CIRC SYST VID, V30, P3422, DOI 10.1109/TCSVT.2019.2963721
   King DB, 2015, ACS SYM SER, V1214, P1
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li YY, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3412384
   Li ZJ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3362988
   Liu C, 2020, PROC CVPR IEEE, P6886, DOI 10.1109/CVPR42600.2020.00692
   Liu Y, 2018, PROC CVPR IEEE, P2080, DOI 10.1109/CVPR.2018.00222
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Pang B, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485061
   Pu N, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2149, DOI 10.1145/3394171.3413673
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Tang Zengming, 2022, ACM T MULTIM COMPUT, V18, P4
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wei ZY, 2022, IEEE T NEUR NET LEAR, V33, P4676, DOI 10.1109/TNNLS.2021.3059713
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu X, 2019, AAAI CONF ARTIF INTE, P9005
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2020, IEEE T IMAGE PROCESS, V29, P9387, DOI 10.1109/TIP.2020.2998275
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12
   Zhan HJ, 2021, IEEE T MULTIMEDIA, V23, P133, DOI 10.1109/TMM.2020.2978669
   Zhang La, 2022, ACM T MULTIM COMPUT, V18
   Zhang P, 2020, IEEE T CIRC SYST VID, V30, P4554, DOI 10.1109/TCSVT.2019.2939564
   Zhao ZW, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491225
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng XT, 2020, IEEE T IMAGE PROCESS, V29, P4747, DOI 10.1109/TIP.2020.2972104
   Zheng Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3501404
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
NR 51
TC 0
Z9 0
U1 13
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 201
DI 10.1145/3595183
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200024
DA 2024-07-18
ER

PT J
AU Liu, H
   Yang, XS
   Xu, CS
AF Liu, Hao
   Yang, Xiaoshan
   Xu, Changsheng
TI Counterfactual Scenario-relevant Knowledge-enriched Multi-modal Emotion
   Reasoning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Neural networks; emotion reasoning; knowledge enhancement;
   counterfactual
AB Multi-modal video emotion reasoning (MERV) has recently attracted increasing attention due to its potential application in human-computer interaction. This task needs to not only recognize utterance-level emotions for conspicuous speakers, but also perceive the emotions of non-speakers in videos. Existing methods focus on modeling multi-modal multi-level contexts to capture emotion-relevant clues from the complex scenarios in videos. However, the context information is far from enough to infer the emotion labels of non-speakers due to the large gap between the scenario situation and emotions labels. Inspired by the observation that humans can find solutions to complex problems with the leverage of experience and knowledge, we propose SK-MER, a Scenario-relevant Knowledge-enhanced Multi-modal Emotion Reasoning framework for MERV task, which can leverage external knowledge to enhance the video scenario understanding and emotion reasoning. Specifically, we use scenario concepts extracted from videos to build knowledge subgraphs from external knowledge bases. The knowledge subgraphs are then utilized to obtain scenario-relevant knowledge representations through dynamic knowledge graph attention. Next, we incorporate the knowledge representations into context modeling to enhance emotion reasoning with external scenario-relevant knowledge. In addition, we propose a counterfactual knowledge representation learning approach to obtain more effective scenario-relevant knowledge representations. Extensive experimental results on MEmoR dataset show that the proposed SK-MER framework achieves new state-of-the-art results.
C1 [Liu, Hao; Yang, Xiaoshan; Xu, Changsheng] Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence, Beijing, Peoples R China.
   [Liu, Hao; Yang, Xiaoshan; Xu, Changsheng] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Yang, Xiaoshan; Xu, Changsheng] Peng Cheng Lab PCL, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Liu, H (corresponding author), Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence, Beijing, Peoples R China.; Liu, H (corresponding author), Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing, Peoples R China.
EM liuhao2019@ia.ac.cn; xiaoshan.yang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; yang, xiaoshan/HSE-6093-2023
OI xu, chang sheng/0000-0001-8343-9665; Liu, Hao/0000-0002-5336-273X
FU National Natural Science Foundation of China [62036012, 62072455,
   61721004, 62106262]; Beijing Natural Science Foundation [L201001]
FX This work was supported by National Natural Science Foundation of China
   (No. 62036012, 62072455, 61721004, 62106262), Beijing Natural Science
   Foundation (L201001).
CR [Anonymous], 2016, Proceedings of the 24th ACM international conference on Multimedia, DOI DOI 10.1145/2964284.2967196
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Bhattacharya U, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8137, DOI 10.1109/ICCV48922.2021.00805
   Cambria E, 2018, AAAI CONF ARTIF INTE, P1795
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen FY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1064, DOI 10.1145/3474085.3475661
   Chen L, 2019, IEEE I CONF COMP VIS, P4612, DOI 10.1109/ICCV.2019.00471
   Chen SZ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P571, DOI 10.1145/2964284.2967286
   Chen T, 2020, PR MACH LEARN RES, V119
   Dai B, 2017, ADV NEUR IN, V30
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141
   Eyben F., 2013, P 21 ACM INT C MULT, P835, DOI DOI 10.1145/2502081.2502224
   Garcia Noa, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P581, DOI 10.1007/978-3-030-58523-5_34
   Ghosal D, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P154
   Gu Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P157, DOI 10.1145/3343031.3351039
   Guo LT, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P767
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han Sangdo, 2015, P 16 ANN M SPECIAL I, P129
   Hao YC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P221, DOI 10.18653/v1/P17-1021
   Hara K, 2018, Arxiv, DOI arXiv:1711.09577
   Hazarika D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2594
   Hazarika Devamanyu, 2018, Proc Conf, V2018, P2122, DOI 10.18653/v1/n18-1193
   Hu Dou, 2021, ACLIJCNLP, V1, P7042
   Hu Jingwen, 2021, ACLIJCNLP, V1, P5666
   Huang J, 2020, INTERSPEECH, P4079, DOI 10.21437/Interspeech.2020-1391
   Huang Y, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409332
   Goodfellow IJ, 2013, Arxiv, DOI arXiv:1307.0414
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Khosla Prannay, 2020, ADV NEURAL INFORM PR, V33, P18661
   Kingma D. P., 2014, arXiv
   Li S, 2021, Arxiv, DOI arXiv:2110.03562
   Lian Z, 2021, IEEE-ACM T AUDIO SPE, V29, P985, DOI 10.1109/TASLP.2021.3049898
   Liu SM, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1489
   Long Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10797, DOI 10.1109/CVPR42600.2020.01081
   Lu D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4013
   Ma JX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P176, DOI 10.1145/3343031.3350871
   Ma Xuan, 2022, ACM T MULTIM COMPUT, DOI [10.1145/3573201JustAccepted, DOI 10.1145/3573201JUSTACCEPTED]
   Majumder N, 2019, AAAI CONF ARTIF INTE, P6818
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   Mittal Trisha, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14222, DOI 10.1109/CVPR42600.2020.01424
   Montani Ines, 2023, Zenodo
   Nie WZ, 2022, IEEE T MULTIMEDIA, V24, P4471, DOI 10.1109/TMM.2021.3118881
   Niu YL, 2021, PROC CVPR IEEE, P12695, DOI 10.1109/CVPR46437.2021.01251
   Plutchik R, 2001, AM SCI, V89, P344, DOI 10.1511/2001.4.344
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Qi F, 2021, IEEE T MULTIMEDIA, V23, P3999, DOI 10.1109/TMM.2020.3035285
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren MJ, 2022, IEEE T MULTIMEDIA, V24, P4422, DOI 10.1109/TMM.2021.3117062
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Schuller B, 2013, INTERSPEECH, P148
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shen GY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P493, DOI 10.1145/3394171.3413909
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Su Z, 2018, PROC CVPR IEEE, P7736, DOI 10.1109/CVPR.2018.00807
   SUCHANEK Fabian M., 2007, 16 INT WORLD WID WEB, V16, P697, DOI DOI 10.1145/1242572.1242667
   Sun HT, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4231
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Wanyan YY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3572914
   Wen HL, 2021, J VIS COMMUN IMAGE R, V78, DOI 10.1016/j.jvcir.2021.103178
   Wenbo Zheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12442, DOI 10.1109/CVPR42600.2020.01246
   Wu Y., 2019, DETECTRON2
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   You Y, 2020, ADV NEURAL INFORM PR, V33, P5812, DOI [10.48550/arXiv.2010.13902, DOI 10.48550/ARXIV.2010.13902]
   Young T, 2018, AAAI CONF ARTIF INTE, P4970
   Zadeh Amir, 2017, P 2017 C EMP METH NA, P1103
   Zhang D., 2020, P 28 INT C COMPUTATI, P4429
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1793, DOI 10.1145/3474085.3475328
   Zhang Zhanqiu, 2020, ADV NEURAL INFORM PR
   Zhao Tong, 2021, ARXIV
   Zheng K, 2021, Arxiv, DOI arXiv:2112.04174
   Zhong PX, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P165
   Zhong PX, 2019, AAAI CONF ARTIF INTE, P7492
   Zhou H, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4623
   Zhuang CX, 2019, IEEE I CONF COMP VIS, P6001, DOI 10.1109/ICCV.2019.00610
   Zolfaghari Mohammadreza, 2021, ARXIV
NR 79
TC 1
Z9 1
U1 5
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2023
VL 19
IS 5
SU S
AR 176
DI 10.1145/3583690
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L0SK0
UT WOS:001020437100012
DA 2024-07-18
ER

PT J
AU Carlsson, N
   Eager, D
AF Carlsson, Niklas
   Eager, Derek
TI Cross-User Similarities in Viewing Behavior for 360° Video and Caching
   Implications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 360 degrees streaming; caching; tiled video caching; viewport overlap
ID OPTIMIZATION; EDGE
AB The demand and usage of 360 degrees video services are expected to increase. However, despite these services being highly bandwidth intensive, not much is known about the potential value that basic bandwidth saving techniques such as server or edge-network on-demand caching (e.g., in a CDN) could have when used for delivery of such services. This problem is both important and complicated as client-side solutions have been developed that split the full 360 degrees view into multiple tiles, and adapt the quality of the downloaded tiles based on the user's expected viewing direction and bandwidth conditions. This article presents new trace-based analysis methods that incorporate users' viewports (the area of the full 360 degrees view the user actually sees), a first characterization of the cross-user similarities of the users' viewports, and a trace-based analysis of the potential bandwidth savings that caching-based techniques may offer under different conditions. Our analysis takes into account differences in the time granularity over which viewport overlaps can be beneficial for resource saving techniques, compares and contrasts differences between video categories, and accounts for uncertainties in the network conditions and the prediction of the future viewing direction when prefetching. The results provide substantial insight into the conditions under which overlap can be considerable and caching effective, and inform the design of new caching system policies tailored for 360 degrees video.
C1 [Carlsson, Niklas] Linkoping Univ, Dept Comp & Informat Sci, SE-58183 Linkoping, Sweden.
   [Eager, Derek] Univ Saskatchewan, Dept Comp Sci, Saskatoon, SK S7N 5C9, Canada.
C3 Linkoping University; University of Saskatchewan
RP Carlsson, N (corresponding author), Linkoping Univ, Dept Comp & Informat Sci, SE-58183 Linkoping, Sweden.
EM niklas.carlsson@liu.se; eager@cs.usask.ca
OI Carlsson, Niklas/0000-0003-1367-1594
FU Swedish Research Council (VR)
FX This work was funded in part by the Swedish Research Council (VR).
CR Almquist M, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P258, DOI 10.1145/3204949.3204970
   Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720
   Benno S, 2011, BELL LABS TECH J, V16, P101, DOI 10.1002/bltj.20505
   Carlsson N., 2021, IEEE T CLOUD COMPUTI
   Carlsson N, 2017, IEEE T MULTIMEDIA, V19, P1637, DOI 10.1109/TMM.2017.2673412
   Carlsson N, 2017, IEEE T PARALL DISTR, V28, P1621, DOI 10.1109/TPDS.2016.2614805
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   DAcunto L., 2015, PROC WORKSHOP INTERA
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   De Vleeschauwer D, 2013, IEEE INFOCOM SER, P989
   Fremerey S, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P403, DOI 10.1145/3204949.3208134
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Gao GY, 2018, IEEE T MULTIMEDIA, V20, P3399, DOI 10.1109/TMM.2018.2838330
   Gast N, 2016, 2016 28TH INTERNATIONAL TELETRAFFIC CONGRESS (ITC 28), VOL 1, P157, DOI 10.1109/ITC-28.2016.128
   Gouta A, 2013, I S MOD ANAL SIM COM, P90, DOI 10.1109/MASCOTS.2013.17
   Graf M, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P261, DOI 10.1145/3083187.3084016
   Hosseini M, 2016, IEEE INT SYM MULTIM, P107, DOI [10.1109/ISM.2016.45, 10.1109/ISM.2016.0028]
   Krishnamoorthi Vengatanathan, 2018, 2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS), DOI 10.1109/IWQoS.2018.8624170
   Krishnamoorthi V, 2013, I S MOD ANAL SIM COM, P182, DOI 10.1109/MASCOTS.2013.26
   Lee D. H., 2014, P ACM NOSSDAV
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P965, DOI 10.1109/TMM.2017.2757761
   Liang K., 2015, PROC ACM MMSYS
   Linder T, 2016, 2016 12TH ANNUAL CONFERENCE ON WIRELESS ON-DEMAND NETWORK SYSTEMS AND SERVICES (WONS), P33
   Liu KD, 2019, LECT NOTES COMPUT SC, V11295, P92, DOI 10.1007/978-3-030-05710-7_8
   Lo WC, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P211, DOI 10.1145/3083187.3083219
   Ma R, 2018, IEEE T MULTIMEDIA, V20, P1595, DOI 10.1109/TMM.2017.2779039
   Maggs BM, 2015, ACM SIGCOMM COMP COM, V45, P52, DOI 10.1145/2805789.2805800
   Mahzari A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P173, DOI 10.1145/3240508.3240680
   Mehr SK, 2018, IEEE IFIP NETW OPER
   Ochi D, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P763, DOI 10.1145/2647868.2654870
   Papaioannou G, 2019, PROCEEDINGS OF THE 2019 THE TWENTIETH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING (MOBIHOC '19), P171, DOI 10.1145/3323679.3326515
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Redi J., 2015, PROC ACM TVX
   Ren DN, 2014, IEEE T MULTIMEDIA, V16, P1874, DOI 10.1109/TMM.2014.2332139
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Son J, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P521, DOI 10.1145/3204949.3208119
   Son J, 2018, PROCEEDINGS OF THE 28TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'18), P61, DOI 10.1145/3210445.3210455
   Sun LY, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P162, DOI 10.1145/3204949.3204978
   Thomas Emmanuel, 2017, Motion Imaging Journal, V126, P22, DOI 10.5594/JMI.2016.2632338
   Thomas E., 2016, Applications and deployments of server and network assisted DASH (SAND)
   Toni L, 2017, IEEE T MULTIMEDIA, V19, P2775, DOI 10.1109/TMM.2017.2713644
   Toni L, 2016, IEEE T MULTIMEDIA, V18, P852, DOI 10.1109/TMM.2016.2537207
   van Brandenburg R., 2011, 2011 15th International Conference on Intelligence in Next Generation Networks (ICIN): "From Bits to Data, from Pipes to Clouds", P151, DOI 10.1109/ICIN.2011.6081064
   Xie L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P564, DOI 10.1145/3240508.3240556
   Yuan H, 2020, IEEE T BROADCAST, V66, P251, DOI 10.1109/TBC.2019.2954074
   Yuan H, 2020, IEEE J-STSP, V14, P177, DOI 10.1109/JSTSP.2019.2957981
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhou C, 2018, IEEE INFOCOM SER, P962, DOI 10.1109/INFOCOM.2018.8486282
   Zhou C, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P27, DOI 10.1145/3083187.3083190
NR 51
TC 1
Z9 1
U1 3
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 152
DI 10.1145/3507917
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300001
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Hao, JC
   Sun, HF
   Ren, PF
   Zhong, YM
   Wang, JY
   Qi, Q
   Liao, JX
AF Hao, Jiachang
   Sun, Haifeng
   Ren, Pengfei
   Zhong, Yiming
   Wang, Jingyu
   Qi, Qi
   Liao, Jianxin
TI Fine-Grained Text-to-Video Temporal Grounding from Coarse Boundary
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Text-to-video temporal grounding; cross-modal retrieval
ID NETWORK
AB Text-to-video temporal grounding aims to locate a target video moment that semantically corresponds to the given sentence query in an untrimmed video. In this task, fully supervised works require text descriptions for each event along with its temporal segment coordinate for training, which is labor-consuming. Existing weakly supervised works require only video-sentence pairs but cannot achieve satisfactory performance. However, many available annotations in the form of coarse temporal boundaries for sentences are ignored and unexploited. These coarse boundaries are common in streaming media platform and can be collected in a mechanical manner. We propose a novel approach to perform fine-grained text-to-video temporal grounding from these coarse boundaries. We take dense video captioning as base task and leverage the trained captioning model to identify the relevance of each video frame to the sentence query according to the frame participation in event captioning. To quantify the frame participation in event captioning, we propose event activation sequence, a simple method that highlights the temporal regions which have high correlations to the text modality in videos. Experiments on modified ActivityNet Captions and a use case demonstrate the promising fine-grained performance of our approach.
C1 [Hao, Jiachang; Sun, Haifeng; Ren, Pengfei; Zhong, Yiming; Wang, Jingyu; Qi, Qi; Liao, Jianxin] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Sun, HF; Wang, JY (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
EM haojc@bupt.edu.cn; hfsun@bupt.edu.cn; rpf@bupt.edu.cn;
   yamingchung@bupt.edu.cn; wangjingyu@bupt.edu.cn; qiqi8266@bupt.edu.cn;
   jxlbupt@gmail.com
RI guo, yi/KHC-4669-2024; liu, qi/KHC-7509-2024; su, lin/KHC-5034-2024;
   ren, pengfei/GOV-5453-2022; Li, Hongbo/KHV-4191-2024; Sun,
   Haifeng/Y-7829-2019; Wang, Jingyu/JFK-6346-2023; Liao, Yi/JVM-9413-2024;
   xiang, wei/JXL-3308-2024; liu, jiajia/ISS-0316-2023; Zhang,
   Wenli/JXL-4317-2024; qi, qi/JWP-1757-2024; li, jing/KHC-8303-2024; Ma,
   Mingyang/JXM-3330-2024; wang, wang/KGW-2828-2024; qi, li/JFE-7167-2023;
   zhang, yan/KHC-3163-2024; ren, pengfei/KEI-4782-2024
OI ren, pengfei/0000-0002-1691-6457; Li, Hongbo/0000-0003-4495-0756; Wang,
   Jingyu/0000-0002-2182-2228; Qi, Qi/0000-0003-0829-4624; Sun,
   Haifeng/0000-0003-3072-7422
FU National Natural Science Foundation of China [62201072, 62101064,
   62071067, 62171057, 62001054]; Beijing University of Posts and
   Telecommunications-China Mobile Research Institute Joint Innovation
   Center; Ministry of Education [MCM20200202]; China Mobile [MCM20200202]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants (62201072, 62101064, 62071067,
   62171057, 62001054), in part by the Ministry of Education and China
   Mobile Joint Fund (MCM20200202), Beijing University of Posts and
   Telecommunications-China Mobile Research Institute Joint Innovation
   Center.
CR Ba J.L., 2016, arXiv
   Banerjee S., 2005, P WORKSHOP INTRINSIC, P65
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao M, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9810
   Chen Jingwen, 2022, ACM T MULTIM COMPUT, V2022
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Chen YW, 2021, ADV NEUR IN, V34
   Duan X, 2018, ADV NEUR IN, V31
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Gao J., 2021, P IEEECVF INT C COMP, P1523
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gao MF, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1481
   Ghosh S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1984
   Hao JC, 2022, NEUROCOMPUTING, V483, P72, DOI 10.1016/j.neucom.2022.01.085
   Hao Jiachang, 2022, EUROPEAN C COMPUTER
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hendricks Lisa Anne, 2018, EMNLP
   Hou Z., 2021, ACM MM, P20
   Huang Jiabo, 2021, P IEEE CVF INT C COM, P7199
   Iashin Vladimir, 2020, P 31 BRIT MACHINE VI
   Ji WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446792
   Jin T, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P630
   Jin WK, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321505
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Jonghwan Mun, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6581, DOI 10.1109/CVPR.2019.00675
   Lan Xiaohan, 2022, ACM T MULTIM COMPUT, V2022
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Li Mengze, 2022, ARXIV
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Lin Chin-Yew, 2004, P 42 ANN M AOC COMP, P605, DOI [DOI 10.3115/.1218955.1219032, DOI 10.3115/1218955.1219032]
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Lin ZJ, 2020, IEEE T IMAGE PROCESS, V29, P3750, DOI 10.1109/TIP.2020.2965987
   Liu BB, 2018, LECT NOTES COMPUT SC, V11207, P569, DOI 10.1007/978-3-030-01219-9_34
   Liu D., 2022, ARXIV
   Liu DZ, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9302
   Liu DZ, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9292
   Liu DZ, 2022, Arxiv, DOI arXiv:2201.00454
   Liu DZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4070, DOI 10.1145/3394171.3414026
   Liu DZ, 2021, PROC CVPR IEEE, P11230, DOI 10.1109/CVPR46437.2021.01108
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu W., 2019, Neural Inf. Process. Syst., P536
   Liu XF, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441577
   Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Minuk Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P156, DOI 10.1007/978-3-030-58604-1_10
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Moltisanti D, 2019, PROC CVPR IEEE, P9907, DOI 10.1109/CVPR.2019.01015
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI [10.1109/WACV45572.2020.9093328, 10.1109/wacv45572.2020.9093328]
   Sadhu Arka, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10414, DOI 10.1109/CVPR42600.2020.01043
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P601, DOI 10.1007/978-3-030-58565-5_36
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Su R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1513, DOI 10.1109/ICCV48922.2021.00156
   Tang PJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303083
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Xiao SN, 2021, AAAI CONF ARTIF INTE, V35, P2986
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang WF, 2021, IEEE T IMAGE PROCESS, V30, P3252, DOI 10.1109/TIP.2021.3058614
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng Yawen, 2022, ACM T MULTIM COMPUT, V18, P2
   Zha ZJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3320061
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang H, 2022, IEEE T PATTERN ANAL, V44, P4252, DOI 10.1109/TPAMI.2021.3060449
   Zhang HY, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2031
   Zhang S., 2021, IEEE T PATTERN ANAL
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhang Zihan, 2020, ADV NEUR IN, V33
   Zhang ZJ, 2021, IEEE T MULTIMEDIA, V23, P3306, DOI 10.1109/TMM.2020.3023339
   Zheng Qi, 2022, ACM T MULTIM COMPUT, V2022
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
   Zhou H, 2021, PROC CVPR IEEE, P8441, DOI 10.1109/CVPR46437.2021.00834
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
   Zhuang YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3366710
NR 93
TC 0
Z9 0
U1 2
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 157
DI 10.1145/3579825
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300006
OA Bronze
DA 2024-07-18
ER

PT J
AU Wu, XT
   Wang, HY
   Wu, YM
   Li, X
AF Wu, Xintian
   Wang, Huanyu
   Wu, Yiming
   Li, Xi
TI D<SUP>3</SUP>T- GAN: Data-Dependent Domain Transfer GANs for Image
   Generation with Limited Data
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Limited data; data-dependent knowledge transfer; projection and
   reconstruction
AB As an important and challenging problem, image generation with limited data aims at generating realistic images through training a GAN model given few samples. A typical solution is to transfer a well-trained GAN model from a data-rich source domain to the data-deficient target domain. In this paper, we propose a novel self-supervised transfer scheme termed (DT)-T-3-GAN, addressing the cross-domain GANs transfer in limited image generation. Specifically, we design two individual strategies to transfer knowledge between generators and discriminators, respectively. To transfer knowledge between generators, we conduct a data-dependent transformation, which projects target samples into the latent space of source generator and reconstructs them back. Then, we perform knowledge transfer from transformed samples to generated samples. To transfer knowledge between discriminators, we design a multi-level discriminant knowledge distillation from the source discriminator to the target discriminator on both the real and fake samples. Extensive experiments show that our method improves the quality of generated images and achieves the state-of-the-art FID scores on commonly used datasets.
C1 [Wu, Xintian; Wang, Huanyu; Wu, Yiming] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Li, Xi] Zhejiang Univ, Shanghai Inst Adv Study Zhejiang Univ, Coll Comp Sci & Technol, Zhejiang Singapore Innovat & AI Joint Res Lab, Hangzhou, Peoples R China.
   [Li, Xi] Shanghai AI Lab Coll Comp Sci, Shanghai, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Li, X (corresponding author), Zhejiang Univ, Shanghai Inst Adv Study Zhejiang Univ, Coll Comp Sci & Technol, Zhejiang Singapore Innovat & AI Joint Res Lab, Hangzhou, Peoples R China.; Li, X (corresponding author), Shanghai AI Lab Coll Comp Sci, Shanghai, Peoples R China.
EM hsintien@zju.edu.cn; huanyuhello@zju.edu.cn; ymw@zju.edu.cn;
   xilizju@zju.edu.cn
RI xu, chen/JNE-5010-2023; Wang, Fei/KEH-6292-2024; chen,
   bin/KBQ-8114-2024; WANG, SHIHAO/KHC-8263-2024; Zhang,
   Xiaoxi/KBP-8753-2024; Li, Kexin/KAO-2519-2024; chen,
   yanhong/JVE-0289-2024; xiang, wei/JXL-3308-2024; wang,
   shuo/KCL-3379-2024; Wang, Zhuo/JVO-1874-2024; Li, Chun/KBC-9591-2024;
   Wang, Yuchen/JPW-9345-2023
OI chen, bin/0000-0002-3398-1314; Wu, Yiming/0000-0002-9866-669X
FU National Key Research and Development Program of China [2020AAA0107400];
   Zhejiang Provincial Natural Science Foundation of China [LR19F020004];
   National Natural Science Foundation of China [U20A20222]; National
   Science Foundation for Distinguished Young Scholars [62225605]; Ant
   Group; CAAI-HUAWEI MindSpore Open Fund
FX This work is supported in part by National Key Research and Development
   Program of China under Grant 2020AAA0107400, Zhejiang Provincial Natural
   Science Foundation of China under Grant LR19F020004, National Natural
   Science Foundation of China under Grant U20A20222, National Science
   Foundation for Distinguished Young Scholars under Grant 62225605, Ant
   Group, and sponsored by CAAI-HUAWEI MindSpore Open Fund.
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   [Anonymous], 2017, P ICLR
   Antoniou Antreas., 2017, P ICLR
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bartunov S, 2018, PR MACH LEARN RES, V84
   Brock A., 2019, INT C LEARN REPR
   Clouƒtre L, 2019, Arxiv, DOI [arXiv:1901.02199, DOI 10.48550/ARXIV.1901.02199]
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Fan CY, 2021, Arxiv, DOI arXiv:2104.00365
   Gao Z, 2021, IEEE T IMAGE PROCESS, V30, P767, DOI 10.1109/TIP.2020.3038372
   Goodfellow Ian J., 2014, PROC NEURIPS
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Guo ZH, 2020, OCEANS-IEEE, DOI 10.1109/IEEECONF38699.2020.9389005
   Ho TT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3396237
   Hong Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2535, DOI 10.1145/3394171.3413561
   Hong Y, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102917
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiang SQ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3391624
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma Diederik P., 2015, INT C LEARN REPR ICL
   Lehtinen J., 2020, Advances in Neural Information Processing Systems (NeurIPS), V33, P12104
   Li Yijun, 2020, PROC NEURIPS
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P5881, DOI 10.1109/TIP.2019.2922854
   Liang WX, 2020, Arxiv, DOI arXiv:2001.00576
   Mangla Puneet, 2020, arXiv
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Miyato T., 2018, 6 INT C LEARNING REP
   Mo Sangwoo, 2020, PROC CVPR WORKSHOP
   Nichol A, 2018, Arxiv, DOI [arXiv:1803.02999, DOI 10.48550/ARXIV.1803.02999]
   Noguchi A, 2019, IEEE I CONF COMP VIS, P2750, DOI 10.1109/ICCV.2019.00284
   Nowozin S, 2016, ADV NEUR IN, V29
   Phaphuangwittayakul Aniwat, 2021, IEEE Transactions on Multimedia
   Radford A., 2015, ARXIV
   Robb E, 2020, Arxiv, DOI arXiv:2010.11943
   Si ZZ, 2012, IEEE T PATTERN ANAL, V34, P1354, DOI 10.1109/TPAMI.2011.227
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang H, 2020, IEEE T IMAGE PROCESS, V29, P8916, DOI 10.1109/TIP.2020.3021789
   Tran NT, 2021, IEEE T IMAGE PROCESS, V30, P1882, DOI 10.1109/TIP.2021.3049346
   Wang Ting-Chun, 2019, PROC NEURIPS
   Wang XP, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355397
   Wang YQ, 2020, Arxiv, DOI arXiv:1904.05046
   Wang YX, 2018, LECT NOTES COMPUT SC, V11210, P220, DOI 10.1007/978-3-030-01231-1_14
   Wang Yaxing, 2020, P IEEE CVF C COMP VI
   Yang CY, 2021, Arxiv, DOI arXiv:2106.04566
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Zhang H, 2019, P ICLR
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhao M, 2020, P INT C MACH LEARN, P11340
   Zhao Shengyu, 2020, PROC NEURIPS
   Zhao ZL, 2020, Arxiv, DOI arXiv:2006.02595
   Zhao ZL, 2021, AAAI CONF ARTIF INTE, V35, P11033
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
   Zhu LC, 2021, IEEE T IMAGE PROCESS, V30, P4253, DOI 10.1109/TIP.2021.3070733
NR 57
TC 1
Z9 1
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2023
VL 19
IS 4
AR 146
DI 10.1145/3576858
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FX8
UT WOS:001011937600008
DA 2024-07-18
ER

PT J
AU Yan, XH
   Li, LL
   Sun, L
   Chen, J
   Wang, SD
AF Yan, Xuehu
   Li, Longlong
   Sun, Lei
   Chen, Jia
   Wang, Shudong
TI Fake and Dishonest Participant Immune Secret Image Sharing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Secret image sharing; hash function; participant authentication; no
   pixel expansion
ID VISUAL CRYPTOGRAPHY; SCHEME; ROBUST
AB Secret image sharing (SIS) has received increased attention from the research community because of its usefulness in multiparty secure computing, access control, blockchain distributive storage and other security-oriented applications. Prevention of fake and dishonest participants is a key issue that has spurred interest in practical applications of SIS. Unfortunately, most previous SIS schemes failed to detect and locate fake or dishonest participants. In this article, an SIS for a (k, n)-threshold without pixel expansion is presented, which can detect and locate both fake and dishonest participants. Using a screening operation, the proposed approach fuses the benefits of polynomial-based SIS, visual cryptographic scheme (VCS), and hash functions to authenticate separate participants both with and without a dealer. In addition, the proposed approach achieves lossless rebuilding of the secret image. Analyses and experiments are conducted in this study to establish the effectiveness of the presented approach.
C1 [Yan, Xuehu; Li, Longlong; Sun, Lei; Chen, Jia; Wang, Shudong] Natl Univ Def Technol, 460 HUANGSHAN Rd, Hefei 230037, Anhui, Peoples R China.
C3 National University of Defense Technology - China
RP Yan, XH (corresponding author), Natl Univ Def Technol, 460 HUANGSHAN Rd, Hefei 230037, Anhui, Peoples R China.
EM publictiger@126.com; lilongs8636@163.com; sun19960119@163.com;
   1003070757@qq.com; shudong0905@163.com
OI Li, Longlong/0000-0001-7390-3647; Yan, Xuehu/0000-0001-6388-1720
FU National Natural Science Foundation of China [62271496]
FX This work is funded by the National Natural Science Foundation of China
   under Grant No. 62271496.
CR Abd El-Latif AA, 2020, IEEE T NETW SERV MAN, V17, P118, DOI 10.1109/TNSM.2020.2969863
   Abd El-Latif Ahmed A., 2017, CLUSTER COMPUT
   Chang CC, 2008, PATTERN RECOGN, V41, P3130, DOI 10.1016/j.patcog.2008.04.006
   Cheng YQ, 2018, IEEE T INF FOREN SEC, V13, P2393, DOI 10.1109/TIFS.2018.2819125
   Chih-Ching Thien, 2002, Computers & Graphics, V26, P765, DOI 10.1016/S0097-8493(02)00131-0
   De Prisco R, 2014, IEEE T INF FOREN SEC, V9, P653, DOI 10.1109/TIFS.2014.2305574
   Fu ZX, 2014, LECT NOTES COMPUT SC, V8389, P109, DOI 10.1007/978-3-662-43886-2_8
   Fukumitsu M, 2017, INT CON ADV INFO NET, P803, DOI 10.1109/AINA.2017.11
   Guo T, 2013, J SYST SOFTWARE, V86, P2094, DOI 10.1016/j.jss.2013.03.062
   Jia XX, 2019, INFORM SCIENCES, V473, P13, DOI 10.1016/j.ins.2018.09.024
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Jiang Y, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8020234
   KAFRI O, 1987, OPT LETT, V12, P377, DOI 10.1364/OL.12.000377
   Komargodski I, 2017, J CRYPTOL, V30, P444, DOI 10.1007/s00145-015-9226-0
   Li P, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102911
   Li YN, 2018, IEEE SIGNAL PROC LET, V25, P140, DOI 10.1109/LSP.2017.2777881
   Lin CC, 2004, J SYST SOFTWARE, V73, P405, DOI 10.1016/S0164-1212(03)00239-5
   Liu JJ, 2021, MATH BIOSCI ENG, V18, P2473, DOI 10.3934/mbe.2021126
   Liu YX, 2018, EURASIP J WIREL COMM, DOI 10.1186/s13638-018-1084-7
   Liu YJ, 2018, MULTIMED TOOLS APPL, V77, P25295, DOI 10.1007/s11042-018-5785-z
   Liu YX, 2018, INFORM SCIENCES, V453, P21, DOI 10.1016/j.ins.2018.04.043
   Liu ZQ, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3418212
   Mohanty M, 2013, INT CONF CLOUD COMP, P531, DOI 10.1109/CloudCom.2013.77
   Naor M., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P1, DOI 10.1007/BFb0053419
   Peng Li, 2010, Proceedings 2010 First International Conference on Pervasive Computing, Signal Processing and Applications (PCSPA 2010), P367, DOI 10.1109/PCSPA.2010.95
   Peri N., 2020, EUROPEAN C COMPUTER
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Shivani S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2935618
   Ulutas G, 2013, PATTERN RECOGN LETT, V34, P283, DOI 10.1016/j.patrec.2012.10.017
   Wang P, 2019, COMPUT SECUR, V85, P107, DOI 10.1016/j.cose.2019.04.010
   Wang ZM, 2009, IEEE T INF FOREN SEC, V4, P383, DOI 10.1109/TIFS.2009.2024721
   Weir J, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2344436.2344438
   Weir J, 2010, LECT NOTES COMPUT SC, V6010, P70, DOI 10.1007/978-3-642-14298-7_5
   Xiong LZ, 2021, IEEE T INF FOREN SEC, V16, P2912, DOI 10.1109/TIFS.2021.3065794
   Yan XH, 2021, IEEE T CIRC SYST VID, V31, P2896, DOI 10.1109/TCSVT.2020.3025527
   Yan XH, 2020, SIGNAL PROCESS-IMAGE, V82, DOI 10.1016/j.image.2019.115721
   Yan XH, 2018, J REAL-TIME IMAGE PR, V14, P61, DOI 10.1007/s11554-015-0540-4
   Yang CN, 2014, INFORM SCIENCES, V278, P141, DOI 10.1016/j.ins.2014.03.033
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
   Zhang Y, 2018, SIGNAL PROCESS, V146, P99, DOI 10.1016/j.sigpro.2018.01.011
   Zhou H, 2019, IEEE T MULTIMEDIA, V21, P1384, DOI 10.1109/TMM.2018.2882088
   Zhou ZL, 2018, IEEE ACCESS, V6, P15021, DOI 10.1109/ACCESS.2018.2811722
NR 43
TC 3
Z9 3
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2023
VL 19
IS 4
AR 139
DI 10.1145/3572842
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FX8
UT WOS:001011937600001
DA 2024-07-18
ER

PT J
AU Yang, BC
   Wu, GS
AF Yang, Bin-Cheng
   Wu, Gangshan
TI Efficient Single-image Super-resolution Using Dual path Connections with
   Multiple scale Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Single-image super-resolution; dual path connection; multiple scale
   training and inference
AB Deep convolutional neural networks have been demonstrated to be effective for single-image super-resolution in recent years. On the one hand, residual connections and dense connections have been used widely to ease forward information and backward gradient flows to boost performance. However, current methods use residual connections and dense connections separately in most network layers in a sub-optimal way. On the other hand, although various networks and methods have been designed to improve computation efficiency, save parameters, or utilize training data of multiple scale factors for each other to boost performance, they either do super-resolution in high-resolution space to have a high computation cost or cannot share parameters between models of different scale factors to save parameters and inference time. To tackle these challenges, we propose an efficient single-image super-resolution network using dual path connections with multiple scale learning (EMSRDPN). By introducing dual path connections inspired by Dual path Networks into EMSRDPN, it uses residual connections and dense connections in an integrated way in most network layers. Dual path connections have the benefits of both reusing common features of residual connections and exploring new features of dense connections to learn a good representation for single-image super-resolution. To utilize the feature correlation of multiple scale factors, EMSRDPN shares all network units in low-resolution space between different scale factors to learn shared features and only uses a separate reconstruction unit for each scale factor, which can utilize training data of multiple scale factors to help each other to boost performance, meanwhile, which can save parameters and support shared inference for multiple scale factors to improve efficiency. Experiments show EMSRDPN achieves better performance and comparable or even better parameter and inference efficiency over state-of-the-art methods. Code will be available at https://github.com/yangbincheng/EMSRDPN.
C1 [Yang, Bin-Cheng; Wu, Gangshan] Nanjing Univ, State Key Lab Novel Software Technol, 163 Xianlin Ave, Nanjing 210023, Jiangsu, Peoples R China.
C3 Nanjing University
RP Yang, BC (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, 163 Xianlin Ave, Nanjing 210023, Jiangsu, Peoples R China.
EM yangbincheng@hotmail.com; gswu@nju.edu.cn
OI Yang, Bin-Cheng/0000-0002-3903-5425
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Allebach J, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P707, DOI 10.1109/ICIP.1996.560768
   Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chen YP, 2017, ADV NEUR IN, V30
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Freeman W. T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1182, DOI 10.1109/ICCV.1999.790414
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Fujimoto A, 2016, PROCEEDINGS OF THE 1ST INTERNATIONAL WORKSHOP ON COMICS ANALYSIS, PROCESSING AND UNDERSTANDING (MANPU 2016), DOI 10.1145/3011549.3011551
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Li YC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414838
   Liebel L, 2016, INT ARCH PHOTOGRAMM, V41, P883, DOI 10.5194/isprsarchives-XLI-B3-883-2016
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   LIU D., 2018, ADV NEURAL INF PROCE, V31, P1673, DOI [DOI 10.48550/ARXIV.1806.02919, 10.48550/arXiv.1806.02919]
   Mao XJ, 2016, ADV NEUR IN, V29
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Qiu YJ, 2019, IEEE I CONF COMP VIS, P4179, DOI 10.1109/ICCV.2019.00428
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi WZ, 2013, LECT NOTES COMPUT SC, V8151, P9, DOI 10.1007/978-3-642-40760-4_2
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tai YW, 2010, PROC CVPR IEEE, P2400, DOI 10.1109/CVPR.2010.5539933
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Wilman W.W.Z., 2010, P 4 IEEE INT C BIOME, P1, DOI 10.1109/BTAS.2010.5634490
   Xu L., 2016, 2016 IEEE International Workshop on Acoustic Signal Enhancement (IWAENC), P1, DOI [DOI 10.1109/IWAENC.2016.7602891, 10.1109/IWAENC.2016.7602891]
   Yang BC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1552, DOI 10.1145/3343031.3350878
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang DY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3398685
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zhang YL, 2019, Arxiv, DOI arXiv:1903.10082
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 62
TC 0
Z9 0
U1 10
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 115
DI 10.1145/3570164
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sun, SCC
   Zhao, YK
   Fu, FW
   Ren, YW
AF Sun, Sophie C. C.
   Zhao, Yongkang
   Fu, Fang-Wei
   Ren, Yawei
TI Improved Random Grid-based Cheating Prevention Visual Cryptography Using
   Latin Square
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual cryptography; cheating prevention; random grid; threshold; Latin
   square
AB Visual cryptography scheme is a method of encrypting secret image into n noiselike shares. The secret image can be reconstructed by stacking adequate shares. In the past two decades, many schemes have been proposed to realize the cheating prevention visual cryptography scheme (CPVCS). Significantly, Ren et al. [9] first introduced the idea of CPVCS with the help of Latin square. Inspired by their work, in this article, a new reliable scheme is proposed. More precisely, to facilitate the certification process, we embed meaningful characters into the randomly chosen authentication patterns in each divided blocks. Furthermore, we fix the security vulnerability in the stacked results of share S. and verification Ver., where 1 =. = n. Since the improved scheme encrypts the secret image by utilizing random grids, the generated shares have no pixel expansion. Finally, theoretical analysis and experimental results are conducted to evaluate the efficiency and security of the proposed scheme.
C1 [Sun, Sophie C. C.] Tianjin Univ Finance & Econ, Dept Math, Tianjin 300222, Peoples R China.
   [Zhao, Yongkang; Fu, Fang-Wei] Nankai Univ, Chern Inst Math, Tianjin 300071, Peoples R China.
   [Zhao, Yongkang; Fu, Fang-Wei] Nankai Univ, LPMC, Tianjin 300071, Peoples R China.
   [Ren, Yawei] Beijing Informat Sci & Technol Univ, Sch Informat Management, Beijing 100192, Peoples R China.
   [Ren, Yawei] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China.
C3 Tianjin University of Finance & Economics; Nankai University; Nankai
   University; Beijing Information Science & Technology University; Chinese
   Academy of Sciences; Institute of Information Engineering, CAS
RP Sun, SCC (corresponding author), Tianjin Univ Finance & Econ, Dept Math, Tianjin 300222, Peoples R China.
EM sophiesun@tjufe.edu.cn; zhaoyk@mail.nankai.edu.cn; fwfu@nankai.edu.cn;
   ryw@bistu.edu.cn
RI Fu, Fang-Wei/L-8164-2015; Sun, Sophie/IVV-3913-2023
OI Sun, Sophie/0000-0001-6483-1098
FU National Key Research and Development Program of China [2018YFA0704703];
   National Natural Science Foundation of China [12141108, 61971243];
   Natural Science Foundation of Tianjin [20JCZDJC00610]; Fundamental
   Research Funds for the Central Universities of China (Nankai
   University); National Science Foundation of China [12001398]; State Key
   Laboratory of information Security, Institute of Information Engineering
   Chinese Academy of Sciences [2021-MS-08]
FX This research is supported by the National Key Research and Development
   Program of China (Grant No. 2018YFA0704703), the National Natural
   Science Foundation of China (Grants No. 12141108 and No. 61971243), the
   Natural Science Foundation of Tianjin (20JCZDJC00610), and the
   Fundamental Research Funds for the Central Universities of China (Nankai
   University); the National Science Foundation of China (No. 12001398);
   the project of State Key Laboratory of information Security, Institute
   of Information Engineering Chinese Academy of Sciences (Grant No.
   2021-MS-08).
CR [Anonymous], 2009, PATTERN RECOGN
   Chang CC, 2009, FUND INFORM, V92, P27, DOI 10.3233/FI-2009-0064
   Chen TH, 2011, J SYST SOFTWARE, V84, P1197, DOI 10.1016/j.jss.2011.02.023
   Horng G, 2006, DESIGN CODE CRYPTOGR, V38, P219, DOI 10.1007/s10623-005-6342-0
   Hu CM, 2007, IEEE T IMAGE PROCESS, V16, P36, DOI 10.1109/TIP.2006.884916
   Hu H, 2018, KSII T INTERNET INF, V12, P3401, DOI 10.3837/tiis.2018.07.022
   KAFRI O, 1987, OPT LETT, V12, P377, DOI 10.1364/OL.12.000377
   Lin PY, 2015, INFORM SCIENCES, V301, P61, DOI 10.1016/j.ins.2014.12.046
   Liu F, 2011, IET INFORM SECUR, V5, P51, DOI 10.1049/iet-ifs.2008.0064
   Naor M., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P1, DOI 10.1007/BFb0053419
   Ren YW, 2017, IET INFORM SECUR, V11, P211, DOI 10.1049/iet-ifs.2016.0126
   Tsai DS, 2007, PATTERN RECOGN, V40, P2356, DOI 10.1016/j.patcog.2007.01.013
   Yadav Mainejar, 2020, Ingenierie des systemes d'information, P453
   Yan XH, 2018, J REAL-TIME IMAGE PR, V14, P61, DOI 10.1007/s11554-015-0540-4
   Yan XH, 2017, DIGIT SIGNAL PROCESS, V71, P36, DOI 10.1016/j.dsp.2017.08.006
   Yang CN, 2020, J INF SECUR APPL, V55, DOI 10.1016/j.jisa.2020.102660
   Yang CN, 2014, INFORM SCIENCES, V278, P141, DOI 10.1016/j.ins.2014.03.033
   Zhao YK, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103408
   Zhao YK, 2022, MULTIMED TOOLS APPL, V81, P6235, DOI 10.1007/s11042-021-11692-4
NR 19
TC 1
Z9 1
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 77
DI 10.1145/3550275
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300002
DA 2024-07-18
ER

PT J
AU Liang, S
   Zhu, AJ
   Zhang, JS
   Shao, J
AF Liang, Shuang
   Zhu, Anjie
   Zhang, Jiasheng
   Shao, Jie
TI Hyper-node Relational Graph Attention Network for Multi-modal Knowledge
   Graph Completion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-modal knowledge graph; knowledge graph completion; relational
   graph attention network; low-rank multi-modal fusion
AB Knowledge graphs often suffer from incompleteness, and knowledge graph completion (KGC) aims at inferring the missing triplets through knowledge graph embedding from known factual triplets. However, most existing knowledge graph embedding methods only use the relational information of knowledge graph and treat the entities and relations as IDs with simple embedding layer, ignoring the multi-modal information among triplets, such as text descriptions, images, etc. In this work, we propose a novel network to incorporate different modal information with graph structure information for more precise representation of multi-modal knowledge graph, termed as hyper-node relational graph attention (HRGAT) network. In HRGAT, we use low-rank multi-modal fusion to model the intra-modality and inter-modality dynamics, which transforms the original knowledge graph to a hyper-node graph. Then, relational graph attention (RGAT) network is used, which contains relation-specific attention and entity-relation fusion operation to capture the graph structure information. Finally, we aggregate the updated multi-modal information and graph structure information to generate the final embeddings of knowledge graph to achieve KGC. By exploring multi-modal information and graph structure information, HRGAT embraces faster convergence speed and achieves the state-of-the-art for KGC on the standard datasets. Implementation code is available at https://github.com/broliang/HRGAT.
C1 [Liang, Shuang; Zhu, Anjie; Zhang, Jiasheng; Shao, Jie] Univ Elect Sci & Technol China, 2006 Xiyuan Ave, Chengdu 611731, Peoples R China.
   [Zhang, Jiasheng] Sichuan Artificial Intelligence Res Inst, Yibin 644000, Peoples R China.
   [Shao, Jie] UESTC, Shenzhen Inst Adv Study, Shenzhen 518110, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China
RP Shao, J (corresponding author), Univ Elect Sci & Technol China, 2006 Xiyuan Ave, Chengdu 611731, Peoples R China.; Shao, J (corresponding author), UESTC, Shenzhen Inst Adv Study, Shenzhen 518110, Peoples R China.
EM shuangliang@std.uestc.edu.cn; anjiezhu@std.uestc.edu.cn;
   zjss12358@std.uestc.edu.cn; shaojie@uestc.edu.cn
RI 梁, 爽/IZQ-4642-2023
OI 梁, 爽/0000-0001-7387-2801; Zhu, Anjie/0000-0002-4634-7961
FU National Natural Science Foundation of China [61832001]; Shenzhen
   Science and Technology Program [JCYJ20210324121213037]; Zhejiang Lab's
   International Talent Fund for Young Professionals
FX This work was supported by the National Natural Science Foundation of
   China (No. 61832001), Shenzhen Science and Technology Program (No.
   JCYJ20210324121213037) and the Zhejiang Lab's International Talent Fund
   for Young Professionals. Authors' addresses: S. Liang and A. Zhu,
   University of Electronic Science and Technology of China, No. 2006
   Xiyuan Ave., West High-tech Zone, Chengdu, China, 611731; emails:
   {shuangliang, anjiezhu}@std.uestc.edu.cn;J.Zhang, University of
   Electronic Science and Technology of China, No. 2006 Xiyuan Ave., West
   High-tech Zone, Chengdu, China, 611731 and Sichuan Artificial
   Intelligence Research Institute, Yibin, China, 644000; email:
   zjss12358@std.uestc.edu.cn;J.Shao (corresponding author), University of
   Electronic Science and Technology of China, No. 2006 Xiyuan Ave., West
   Hightech Zone, Chengdu, China, 611731 and Shenzhen Institute for
   Advanced Study, UESTC, Shenzhen, China, 518110; email:
   shaojie@uestc.edu.cn.
CR An Bo, 2018, INPROCEEDINGS 2018 C, P745
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Balazevic I, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5185
   Balazevic I, 2019, LECT NOTES COMPUT SC, V11731, P553, DOI 10.1007/978-3-030-30493-5_52
   Bollacker K., 2008, P 2008 ACM SIGMOD IN, P1247, DOI 10.1145/1376616.1376746
   Bordes A., 2013, P 26 INT C NEURAL IN, P2787
   Bordes Antoine, 2011, P TWENTYFIFTH AAAI C
   Chaudhary C, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3375786
   Chen LY, 2020, LECT NOTES COMPUT SC, V12274, P134, DOI 10.1007/978-3-030-55130-8_12
   Chen Y, 2021, KNOWL-BASED SYST, V227, DOI 10.1016/j.knosys.2021.107181
   Chen Zhenfang, 2021, 9 INT C LEARNING REP
   Chen Zhenfang, 2022, 10 INT C LEARN REPR
   Dettmers T, 2018, AAAI CONF ARTIF INTE, P1811
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong XL, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P601, DOI 10.1145/2623330.2623623
   Ebisu T, 2020, IEEE T KNOWL DATA EN, V32, P941, DOI 10.1109/TKDE.2019.2893920
   García-Durán A, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P372
   Huang XQ, 2021, KNOWL-BASED SYST, V229, DOI 10.1016/j.knosys.2021.107310
   Jiang D, 2021, KNOWL-BASED SYST, V227, DOI 10.1016/j.knosys.2021.107188
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Kipf T, 2018, PR MACH LEARN RES, V80
   Lin YK, 2015, AAAI CONF ARTIF INTE, P2181
   Liu HX, 2017, PR MACH LEARN RES, V70
   Liu Sijia, 2021, P NEURAL INFORM PROC, V1
   Liu Y, 2019, LECT NOTES COMPUT SC, V11503, P459, DOI 10.1007/978-3-030-21348-0_30
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Nathani D, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4710
   Nguyen D.Q., 2018, P NAACL HLT, P327, DOI [DOI 10.18653/V1/N18-2053, 10.18653/v1/n18-2053]
   Nickel M., 2011, P 28 INT C MACH LEAR, V11, P3104482, DOI 10.5555/3104482.3104584
   Pezeshkpour P, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3208
   Qian SS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451215
   Reimers N, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P567
   Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Shang C, 2019, AAAI CONF ARTIF INTE, P3060
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   SUCHANEK Fabian M., 2007, 16 INT WORLD WID WEB, V16, P697, DOI DOI 10.1145/1242572.1242667
   Sun R, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1405, DOI 10.1145/3340531.3411947
   Sun ZY, 2022, INT J COAL PREP UTIL, V42, P221, DOI [10.1080/19392699.2019.1590346, 10.1109/eurosime.2019.8724592]
   Toutanova K., 2015, P 3 WORKSH CONT VECT, P57, DOI [10.18653/v1/W15-4007, DOI 10.18653/V1/W15-4007]
   Trouillon T, 2016, PR MACH LEARN RES, V48
   Vashishth S., 2020, P 8 INT C LEARN REPR
   Vrandecic D, 2014, COMMUN ACM, V57, P78, DOI 10.1145/2629489
   Xiao H, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2316
   Xie RB, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3140
   Yang B, 2014, P INT C LEARNING REP
   Yao L, 2019, Arxiv, DOI arXiv:1909.03193
   Yi Kexin, 2020, 8 INT C LEARNING REP
   Zadeh Amir, 2017, P 2017 C EMP METH NA, P1103
   Zeb AN, 2021, KNOWL-BASED SYST, V229, DOI 10.1016/j.knosys.2021.107369
   Zhang YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3956, DOI 10.1145/3394171.3413736
   Zhang YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1089, DOI 10.1145/3343031.3351033
   Zhang ZL, 2022, IEEE T KNOWL DATA EN, V34, P2335, DOI 10.1109/TKDE.2020.3005952
   Zheng D, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P739, DOI 10.1145/3397271.3401172
   Zhou ZH, 2022, KNOWL-BASED SYST, V240, DOI 10.1016/j.knosys.2021.108100
NR 54
TC 6
Z9 7
U1 25
U2 57
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 62
DI 10.1145/3545573
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000012
DA 2024-07-18
ER

PT J
AU Chen, JW
   Luo, JJ
   Pan, YW
   Li, YA
   Yao, T
   Chao, HY
   Mei, T
AF Chen, Jingwen
   Luo, Jianjie
   Pan, Yingwei
   Li, Yehao
   Yao, Ting
   Chao, Hongyang
   Mei, Tao
TI Boosting Vision-and-Language Navigation with Direction Guiding and
   Backtracing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Vision-and-language navigation; cross-modal matching
ID INSTRUCTIONS
AB Vision-and-Language Navigation (VLN) has been an emerging and fast-developing research topic, where an embodied agent is required to navigate in a real-world environment based on natural language instructions. In this article, we present a Direction-guided Navigator Agent (DNA) that novelly integrates direction clues derived from instructions into the essential encoder-decoder navigation framework. Particularly, DNA couples the standard instruction encoder with an additional direction branch which sequentially encodes the direction clues in the instructions to boost navigation. Furthermore, an Instruction Flipping mechanism is uniquely devised to enable fast data augmentation as well as a follow-up backtracing for navigating the agent in a backward direction. Such a way naturally amplifies the grounding of instruction in the local visual scenes along both forward and backward directions, and thus strengthens the alignment between instruction and action sequence. Extensive experiments conducted on Room to Room (R2R) dataset validate our proposal and demonstrate quantitatively compelling results.
C1 [Chen, Jingwen; Luo, Jianjie; Chao, Hongyang] Sun Yat Sen Univ, Guangzhou Higher Educ Mega Ctr, 132 Waihuan East Rd, Guangzhou 510006, Peoples R China.
   [Pan, Yingwei; Li, Yehao; Yao, Ting; Mei, Tao] JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
C3 Sun Yat Sen University
RP Yao, T (corresponding author), JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
EM chenjingwen.sysu@gmail.com; jianjieluo.sysu@gmail.com;
   panyw.ustc@gmail.com; yehaoli.sysu@gmail.com; tingyao.ustc@gmail.com;
   isschhy@mail.sysu.edu.cn; tmei@jd.com
RI chen, jw/IQW-1558-2023
FU National Key R&D Program of China [2020AAA0108600]; NSF of China
   [61672548, U1611461, 61173081]
FX This work was supported by the National Key R&D Program of China under
   Grant No. 2020AAA0108600, and was partially supported by NSF of China
   under Grant 61672548, U1611461, 61173081.
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   [Anonymous], 2011, P 25 AAAI C ARTIFICI
   [Anonymous], 2006, P 21 NATL C ARTIFICI
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Artzi Y, 2013, T ASSOC COMPUT LING, V1, P49, DOI DOI 10.1162/TACL_A_00209
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Chaplot DS, 2018, AAAI CONF ARTIF INTE, P2819
   Das A, 2019, IEEE T PATTERN ANAL, V41, P1242, DOI 10.1109/TPAMI.2018.2828437
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Fengda Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10009, DOI 10.1109/CVPR42600.2020.01003
   Fried Daniel, 2018, ADV NEURAL INFORM PR, V31, P3318
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Gordon D, 2018, PROC CVPR IEEE, P4089, DOI 10.1109/CVPR.2018.00430
   Hao Weituo, 2020, P IEEE CVF C COMP VI, P13134, DOI DOI 10.1109/CVPR42600.2020.01315
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Huang HS, 2019, IEEE I CONF COMP VIS, P7403, DOI 10.1109/ICCV.2019.00750
   Jain Vihan, 2019, P 57 ANN M ASS COMPU
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Kingma Diederik P., 2015, P 3 INT C LEARN
   Li YH, 2022, Arxiv, DOI [arXiv:2201.04026, DOI 10.48550/ARXIV.2201.04026]
   Li YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3799, DOI 10.1145/3474085.3478331
   Li YA, 2021, AAAI CONF ARTIF INTE, V35, P8518
   Luo JJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5600, DOI 10.1145/3474085.3475703
   Ma C., 2019, ARXIV190103035
   Ma CY, 2019, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR.2019.00689
   Majumdar Arjun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P259, DOI 10.1007/978-3-030-58539-6_16
   Mei HY, 2016, AAAI CONF ARTIF INTE, P2772
   Mirowski P., 2017, P 5 INT C LEARNING R
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Pan Yingwei, 2022, ACM T MULTIM COMPUT, V18
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Plummer BA, 2017, INT J COMPUT VISION, V123, P74, DOI 10.1007/s11263-016-0965-7
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tellex S., 2011, P 25 AAAI C ARTIFICI
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vogel A, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P806
   Wang X, 2018, LECT NOTES COMPUT SC, V11220, P38, DOI 10.1007/978-3-030-01270-0_3
   Wang X, 2019, PROC CVPR IEEE, P3622, DOI 10.1109/CVPR.2019.00679
   Wijmans E, 2019, PROC CVPR IEEE, P6652, DOI 10.1109/CVPR.2019.00682
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3743, DOI 10.1145/3394171.3413977
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
NR 55
TC 1
Z9 1
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 9
DI 10.1145/3526024
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400009
DA 2024-07-18
ER

PT J
AU Cheung, M
   Sun, WW
   She, J
   Zhou, JT
AF Cheung, Ming
   Sun, Weiwei
   She, James
   Zhou, Jiantao
TI Social Network Analytic-Based Online Counterfeit Seller Detection using
   User Shared Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Counterfeit seller detection; social network analytic; deep learning
ID INFLUENTIAL USERS; ANOMALY DETECTION; CENTRALITY; FRAMEWORK
AB Selling counterfeit online has become a serious problem, especially with the advancement of social media and mobile technology. Instead of investigating the products directly, one can only check the images, tags annotated by the sellers on the images, or the price to decide if a seller sells counterfeits. One of the ways to detect counterfeit sellers is to investigate their social graphs, in which counterfeit sellers show different behaviour in network measurements, such as those in centrality and EgoNet. However, social graphs are not easily accessible. They may be kept private by the operators, or there are no connections at all. This article proposes a framework to detect counterfeit sellers using their connection graphs discovered from their shared images. Based on 153 K shared images from Taobao, it is proven that counterfeit sellers have different network behaviours. It is observed that the network measurements follow Beta function well. Those distributions are formulated to detect counterfeit sellers by the proposed framework, which is 60% better than approaches using classification.
C1 [Cheung, Ming] Social Face Ltd, Hong Kong, Peoples R China.
   [Sun, Weiwei; Zhou, Jiantao] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Macau, Peoples R China.
   [She, James] HKUST NIE Social Media Lab, Hong Kong, Peoples R China.
C3 University of Macau
RP Zhou, JT (corresponding author), Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Macau, Peoples R China.
EM ming@socialface.ai; YB57457@umac.mo; eejames@ust.hk; jtzhou@umac.mo
FU HKUST-NIE Social Media Lab., HKUST; Macau Science and Technology
   Development Fund [SKL-IOTSC(UM)-2021-2023, 0072/2020/AMJ, 0060/2019/A1];
   Research Committee at University of Macau [MYRG2020-00101-FST]; Natural
   Science Foundation of China [61971476]
FX This work was supported in part by HKUST-NIE Social Media Lab., HKUST,
   and Macau Science and Technology Development Fund under
   SKL-IOTSC(UM)-2021-2023, 0072/2020/AMJ, and 0060/2019/A1, by Research
   Committee at University of Macau under MYRG2020-00101-FST, and by
   Natural Science Foundation of China under 61971476.
CR Akoglu L, 2015, DATA MIN KNOWL DISC, V29, P626, DOI 10.1007/s10618-014-0365-y
   Akoglu L, 2010, LECT NOTES ARTIF INT, V6119, P410
   [Anonymous], 2010, Identifying key users in online social networks: A pagerank based approach
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Becker Hila, 2011, P 5 INT AAAI C WEB S
   Bonacich P, 2001, SOC NETWORKS, V23, P191, DOI 10.1016/S0378-8733(01)00038-7
   Chatfield K, 2014, Arxiv, DOI arXiv:1405.3531
   Chen C, 2013, 2013 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM), P122
   Cheng H., 2016, P 1 WORKSH DEEP LEAR, P7, DOI [DOI 10.1145/2988450.2988454, 10.1145/2988450.2988454]
   Cheung M., 2017, ACM T MULTIM COMPUT, V13, P1
   Cheung M, 2020, IEEE T MULTIMEDIA, V22, P407, DOI 10.1109/TMM.2019.2930043
   Cheung M, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311785
   Cheung M, 2018, IEEE T BIG DATA, V4, P447, DOI 10.1109/TBDATA.2017.2762719
   Cheung M, 2018, IEEE CONF COMPUT, P51, DOI 10.1109/INFCOMW.2018.8406896
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Epasto A, 2015, PROC VLDB ENDOW, V9, P324
   Fan WQ, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P417, DOI 10.1145/3308558.3313488
   FREEMAN LC, 1979, SOC NETWORKS, V1, P215, DOI 10.1016/0378-8733(78)90021-7
   Garg S, 2019, IEEE T MULTIMEDIA, V21, P566, DOI 10.1109/TMM.2019.2893549
   Guimerá R, 2005, P NATL ACAD SCI USA, V102, P7794, DOI 10.1073/pnas.0407994102
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Koschützki D, 2008, GENE REGUL SYST BIO, V2, P193, DOI 10.4137/GRSB.S702
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Mtibaa A., 2010, P 29 C INFORM COMMUN, P111
   Mukherjee A, 2012, P 21 INT C WORLD WID, P191
   Mukherjee Arjun, 2013, P INT AAAI C WEB SOC, DOI DOI 10.1609/ICWSM.V7I1.14389
   Page L., 1999, PAGERANK CITATION RA
   Probst F, 2013, BUS INFORM SYST ENG+, V5, P179, DOI 10.1007/s12599-013-0263-7
   Rathore S, 2018, APPL SOFT COMPUT, V67, P920, DOI 10.1016/j.asoc.2017.09.032
   Rathore S, 2017, INFORM SCIENCES, V421, P43, DOI 10.1016/j.ins.2017.08.063
   Rathore S, 2017, J INF PROCESS SYST, V13, P1014, DOI 10.3745/JIPS.03.0079
   Savage D, 2014, SOC NETWORKS, V39, P62, DOI 10.1016/j.socnet.2014.05.002
   She J, 2017, 2017 19TH IEEE INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS (HPCC) / 2017 15TH IEEE INTERNATIONAL CONFERENCE ON SMART CITY (SMARTCITY) / 2017 3RD IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND SYSTEMS (DSS), P278, DOI 10.1109/HPCC-SmartCity-DSS.2017.36
   Sheikholeslami F, 2016, IEEE GLOB CONF SIG, P341, DOI 10.1109/GlobalSIP.2016.7905860
   Tan EH, 2012, INT CON DISTR COMP S, P305, DOI 10.1109/ICDCS.2012.40
   Trusov M, 2010, J MARKETING RES, V47, P643, DOI 10.1509/jmkr.47.4.643
   Van Rijsbergen C. J., 1979, Information Retrieval, V2nd
   Wang AH, 2010, SECRYPT 2010: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SECURITY AND CRYPTOGRAPHY, P142
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wei Y., 2021, IEEE Transactions on Multimedia
   Wei YW, 2020, IEEE T IMAGE PROCESS, V29, P1, DOI 10.1109/TIP.2019.2923608
   Xu K, 2020, IEEE T MULTIMEDIA, V22, P394, DOI 10.1109/TMM.2019.2929931
   Yan EJ, 2009, J AM SOC INF SCI TEC, V60, P2107, DOI 10.1002/asi.21128
   Yang C, 2011, LECT NOTES COMPUT SC, V6961, P318, DOI 10.1007/978-3-642-23644-0_17
   Yuan ZQ, 2014, IEEE T MULTIMEDIA, V16, P1624, DOI 10.1109/TMM.2014.2322338
   Zhang QY, 2013, IEEE INT CONGR BIG, P141, DOI 10.1109/BigData.Congress.2013.27
   Zheng XH, 2015, NEUROCOMPUTING, V159, P27, DOI 10.1016/j.neucom.2015.02.047
NR 48
TC 2
Z9 2
U1 7
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 23
DI 10.1145/3524135
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400023
DA 2024-07-18
ER

PT J
AU Jin, X
   Li, XN
   Lou, H
   Fan, CY
   Deng, Q
   Xiao, CE
   Cui, S
   Singh, AK
AF Jin, Xin
   Li, Xinning
   Lou, Hao
   Fan, Chenyu
   Deng, Qiang
   Xiao, Chaoen
   Cui, Shuai
   Singh, Amit Kumar
TI Aesthetic Attribute Assessment of Images Numerically on Mixed
   Multi-attribute Datasets
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Aesthetic mixed dataset with attributes; multitasking; external
   attribute features; ECA channel attention
AB With the continuous development of social software and multimedia technology, images have become a kind of important carrier for spreading information and socializing. How to evaluate an image comprehensively has become the focus of recent researches. The traditional image aesthetic assessment methods often adopt single numerical overall assessment scores, which has certain subjectivity and can no longer meet the higher aesthetic requirements. In this article, we construct an new image attribute dataset called aesthetic mixed dataset with attributes (AMD-A) and design external attribute features for fusion. Besides, we propose an efficient method for image aesthetic attribute assessment on mixed multi-attribute dataset and construct a multitasking network architecture by using the EfficientNet-B0 as the backbone network. Our model can achieve aesthetic classification, overall scoring, and attribute scoring. In each sub-network, we improve the feature extraction through ECA channel attention module. As for the final overall scoring, we adopt the idea of the teacher-student network and use the classification sub-network to guide the aesthetic overall fine-grain regression. Experimental results, using the MindSpore, show that our proposed method can effectively improve the performance of the aesthetic overall and attribute assessment.
C1 [Jin, Xin; Li, Xinning; Lou, Hao; Fan, Chenyu; Deng, Qiang; Xiao, Chaoen] Beijing Elect Sci & Technol Inst, Beijing, Peoples R China.
   [Cui, Shuai] Univ Calif Davis, Davis, CA 95616 USA.
   [Singh, Amit Kumar] Natl Inst Technol Palma, Hanamkonda, India.
C3 Beijing Electronic Science & Technology Institute; University of
   California System; University of California Davis
RP Lou, H; Xiao, CE (corresponding author), Beijing Elect Sci & Technol Inst, Beijing, Peoples R China.
EM jinxinbesti@foxmail.com; l_xinning@163.com; 452392771@qq.com;
   3497961491@qq.com; 1352110584@qq.com; xcecd@qq.com; shucui@ucdavis.edu;
   amit.singh@nitp.ac.in
RI lou, hao/JKI-8455-2023; Jin, Xin/S-9172-2017; Singh, Amit
   Kumar/D-1300-2015
OI Jin, Xin/0000-0003-2211-2006; Singh, Amit Kumar/0000-0001-7359-2068
FU National Natural Science Foundation of China [62072014, 62106118];
   CAAIHuawei MindSpore Open Fund [CAAIXSJLJJ-2021-022A]; Open Fund Project
   of the State Key Laboratory of Complex System Management and Control
   [2022111]; Project of Philosophy and Social Science Research, Ministry
   of Education of China [20YJC760115]; Advanced Discipline Construction
   Project of Beijing Universities [20210051Z0401]; MindSpore; CANN
   (Compute Architecture for Neural Networks); Ascend AI Processor;
   Zhongyuan AI Computing Center
FX This work is partially supported by the National Natural Science
   Foundation of China (62072014 & 62106118), the CAAIHuawei MindSpore Open
   Fund (CAAIXSJLJJ-2021-022A), the Open Fund Project of the State Key
   Laboratory of Complex System Management and Control (2022111), the
   Project of Philosophy and Social Science Research, Ministry of Education
   of China (No. 20YJC760115), and the Advanced Discipline Construction
   Project of Beijing Universities (20210051Z0401). We gratefully
   acknowledge the support of MindSpore, CANN (Compute Architecture for
   Neural Networks), Ascend AI Processor, and Zhongyuan AI Computing Center
   used for this research.
CR [Anonymous], 2012, P 21 INT C WORLD WID, DOI DOI 10.1145/2187980.2188075
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Brachmann A, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00102
   Chang KY, 2017, IEEE I CONF COMP VIS, P3534, DOI 10.1109/ICCV.2017.380
   Chen Kang, 2020, ATQAM/MAST'20: Joint Workshop on Aesthetic and Technical Quality Assessment of Multimedia and Media Analytics for Societal Trends, P5, DOI 10.1145/3423268.3423590
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng YB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P870, DOI 10.1145/3240508.3240531
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jin X, 2019, IEEE ACCESS, V7, P183647, DOI 10.1109/ACCESS.2019.2958119
   Jin X, 2018, LECT NOTES COMPUT SC, V11257, P41, DOI 10.1007/978-3-030-03335-4_4
   Kairanbay M, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3328993
   Kao YY, 2015, IEEE IMAGE PROC, P1583, DOI 10.1109/ICIP.2015.7351067
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Liang LY, 2018, INT C PATT RECOG, P1598, DOI 10.1109/ICPR.2018.8546038
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JJ, 2020, IEEE T IMAGE PROCESS, V29, P8652, DOI 10.1109/TIP.2020.3017352
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   MindSpore, 2022, US
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   She DY, 2021, PROC CVPR IEEE, P8471, DOI 10.1109/CVPR46437.2021.00837
   Sperry R. W., 1969, HDB CLIN NEUROL, V4, P145
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tong HG, 2004, LECT NOTES COMPUT SC, V3331, P198
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang RR, 2022, IEEE T IND INFORM, V18, P8129, DOI 10.1109/TII.2022.3163558
   Wang RR, 2022, IEEE T INTELL TRANSP, V23, P25408, DOI 10.1109/TITS.2022.3140903
   Xu Liming, 2022, ACM T MULTIM COMPUT
   Zhang Y, 2020, IEEE T NETW SCI ENG, V7, P2145, DOI 10.1109/TNSE.2020.2990963
   Zhao K, 2022, IEEE T PATTERN ANAL, V44, P4793, DOI 10.1109/TPAMI.2021.3077129
   Zhen PN, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3513133
   Zhou Q, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108290
   Zhou Q, 2019, WORLD WIDE WEB, V22, P555, DOI 10.1007/s11280-018-0556-3
   Zhou Wei, 2022, ACM T MULTIM COMPUT
NR 41
TC 3
Z9 3
U1 5
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 138
DI 10.1145/3547144
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800008
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Liu, YF
   Li, Y
   You, SD
   Lu, F
AF Liu, Yunfei
   Li, Yu
   You, Shaodi
   Lu, Feng
TI Semantic Guided Single Image Reflection Removal
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Reflection removal; semantic segmentation; multi-task learning;
   highlevel guidance; deep learning
ID SEPARATION
AB Reflection is common when we see through a glass window, which not only is a visual disturbance but also influences the performance of computer vision algorithms. Removing the reflection from a single image, however, is highly ill-posed since the color at each pixel needs to be separated into two values belonging to the clear background and the reflection, respectively. To solve this, existing methods use additional priors such as reflection layer smoothness, double reflection effect, and color consistency to distinguish the two layers. However, these low-level priors may not be consistently valid in real cases. In this paper, inspired by the fact that human beings can separate the two layers easily by recognizing the objects and understanding the scene, we propose to use the object semantic cue, which is high-level information, as the guidance to help reflection removal. Based on the data analysis, we develop a multi-task end-to-end deep learning method with a semantic guidance component, to solve reflection removal and semantic segmentation jointly. Extensive experiments on different datasets show significant performance gain when using high-level object-oriented information. We also demonstrate the application of our method to other computer vision tasks.
C1 [Liu, Yunfei; Lu, Feng] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, XueYuan Rd 37, Beijing 100191, Peoples R China.
   [Li, Yu] Int Digital Econ Acad, Hua Rd 5, Shenzhen 518045, Peoples R China.
   [You, Shaodi] Univ Amsterdam, Louwesweg 1, NL-1066 EA Amsterdam, Netherlands.
   [Lu, Feng] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Beihang University; International Digital Economy Academy; University of
   Amsterdam; Peng Cheng Laboratory
RP Lu, F (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, XueYuan Rd 37, Beijing 100191, Peoples R China.; Lu, F (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
EM lyun-fei@buaa.edu.cn; yul@illinois.edu; s.you@uva.nl; lufeng@buaa.edu.cn
RI Shaodi, YOU/AAA-4524-2022
OI Shaodi, YOU/0000-0001-8973-645X; LI, Yu/0000-0003-1865-8276; Lu,
   Feng/0000-0001-9064-7964
FU National Natural Science Foundation of China (NSFC) [61732016, 62006101]
FX This work was supported by the National Natural Science Foundation of
   China (NSFC) under Grant 61732016 and 62006101.
CR Agrawal A, 2005, ACM T GRAPHIC, V24, P828, DOI 10.1145/1073204.1073269
   [Anonymous], 2011, Torch7: A matlab-like environment for machine learning
   Arvanitopoulos N, 2017, PROC CVPR IEEE, P1752, DOI 10.1109/CVPR.2017.190
   Baslamisli AS, 2018, LECT NOTES COMPUT SC, V11210, P289, DOI 10.1007/978-3-030-01231-1_18
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351
   Guo XJ, 2014, PROC CVPR IEEE, P2195, DOI 10.1109/CVPR.2014.281
   Hao ZX, 2019, IEEE INT CONF COMP V, P4340, DOI 10.1109/ICCVW.2019.00534
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong Yicong, 2021, CVPR
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Kim S, 2020, PROC CVPR IEEE, P5163, DOI 10.1109/CVPR42600.2020.00521
   Kim S, 2020, Arxiv, DOI arXiv:2009.00702
   Kong NJ, 2014, IEEE T PATTERN ANAL, V36, P209, DOI 10.1109/TPAMI.2013.45
   Lei CY, 2021, PROC CVPR IEEE, P14806, DOI 10.1109/CVPR46437.2021.01457
   Lei CY, 2020, PROC CVPR IEEE, P1747, DOI 10.1109/CVPR42600.2020.00182
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li C, 2020, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR42600.2020.00362
   Li Rui, 2020, ECCV
   Li Y., 2020, arXiv
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Li Y, 2013, IEEE I CONF COMP VIS, P2432, DOI 10.1109/ICCV.2013.302
   Liu D, 2020, IEEE T IMAGE PROCESS, V29, P3695, DOI 10.1109/TIP.2020.2964518
   Liu YF, 2020, AAAI CONF ARTIF INTE, V34, P11661
   Liu Yunfei, 2020, EUR C COMP VIS ECCV
   Liu Yunfei, 2019, PBRR PHYS BASED RAIN
   Ma DQ, 2019, IEEE I CONF COMP VIS, P2444, DOI 10.1109/ICCV.2019.00253
   MATLAB, 2010, VERS R2017B
   Nandoriya A, 2017, IEEE I CONF COMP VIS, P2430, DOI 10.1109/ICCV.2017.264
   Niklaus S, 2021, IEEE WINT CONF APPL, P3712, DOI 10.1109/WACV48630.2021.00376
   Paszke A., 2017, NIPS W
   Punnappurath A, 2019, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2019.00165
   Quan YH, 2019, IEEE I CONF COMP VIS, P2463, DOI 10.1109/ICCV.2019.00255
   Rad MS, 2019, IEEE I CONF COMP VIS, P2710, DOI 10.1109/ICCV.2019.00280
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178
   Sandhan T, 2017, PROC CVPR IEEE, P1675, DOI 10.1109/CVPR.2017.182
   Schechner YY, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1061, DOI 10.1109/ICCV.1998.710848
   Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939
   Simon C, 2015, PROC CVPR IEEE, P4231, DOI 10.1109/CVPR.2015.7299051
   Sinha SN, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185596
   Sutskever I., 2013, P 30 INT C MACH LEAR
   Wan RJ, 2020, IEEE T PATTERN ANAL, V42, P2969, DOI 10.1109/TPAMI.2019.2921574
   Wan RJ, 2018, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2018.00502
   Wan RJ, 2017, IEEE I CONF COMP VIS, P3942, DOI 10.1109/ICCV.2017.423
   Wang GQ, 2020, IEEE T IMAGE PROCESS, V29, P9190, DOI 10.1109/TIP.2020.3023773
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837
   Wen Q, 2019, PROC CVPR IEEE, P3766, DOI 10.1109/CVPR.2019.00389
   Wen SJ, 2021, IEEE T IMAGE PROCESS, V30, P7280, DOI 10.1109/TIP.2021.3104188
   Wieschollek P, 2018, LECT NOTES COMPUT SC, V11217, P90, DOI 10.1007/978-3-030-01261-8_6
   Williams C.K.I., PASCAL VISUAL OBJECT
   Xue TF, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766940
   Yang JL, 2016, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2016.157
   Yang J, 2018, LECT NOTES COMPUT SC, V11207, P675, DOI 10.1007/978-3-030-01219-9_40
   Yang Y, 2019, PROC CVPR IEEE, P8133, DOI 10.1109/CVPR.2019.00833
   Yu-Lun Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14203, DOI 10.1109/CVPR42600.2020.01422
   Yun JS, 2018, PROC CVPR IEEE, P4597, DOI 10.1109/CVPR.2018.00483
   Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503
   Zhao YQ, 2015, IEEE IMAGE PROC, P3397, DOI 10.1109/ICIP.2015.7351434
   Zheng Q, 2021, PROC CVPR IEEE, P13390, DOI 10.1109/CVPR46437.2021.01319
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 67
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 151
DI 10.1145/3510821
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jia, W
   Li, L
   Li, Z
   Zhang, X
   Liu, S
AF Jia, Wei
   Li, Li
   Li, Zhu
   Zhang, Xiang
   Liu, Shan
TI Residual-guided In-loop Filter Using Convolution Neural Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Convolutional neural network; high efficiency video coding; in-loop
   filter; reconstruction; residual
ID DEBLOCKING FILTER; VIDEO; SUPERRESOLUTION; FRAMEWORK
AB The block-based coding structure in the hybrid video coding framework inevitably introduces compression artifacts such as blocking, ringing, and so on. To compensate for those artifacts, extensive filtering techniques were proposed in the loop of video codecs, which are capable of boosting the subjective and objective qualities of reconstructed videos. Recently, neural network-based filters were presented with the power of deep learning from a large magnitude of data. Though the coding efficiency has been improved from traditional methods in High-Efficiency Video Coding (HEVC), the rich features and information generated by the compression pipeline have not been fully utilized in the design of neural networks. Therefore, in this article, we propose the Residual-Reconstruction-based Convolutional Neural Network (RRNet) to further improve the coding efficiency to its full extent, where the compression features induced from bitstream in form of prediction residual are fed into the network as an additional input to the reconstructed frame. In essence, the residual signal can provide valuable information about block partitions and can aid reconstruction of edge and texture regions in a picture. Thus, more adaptive parameters can be trained to handle different texture characteristics. The experimental results show that our proposed RRNet approach presents significant BD-rate savings compared to HEVC and the state-of-the-art CNN-based schemes, indicating that residual signal plays a significant role in enhancing video frame reconstruction.
C1 [Jia, Wei; Li, Zhu] Univ Missouri, 5000 Holmes St, Kansas City, MO 64110 USA.
   [Li, Li] Univ Sci & Technol China, 96 Jin Zhai Rd, Hefei 230026, Anhui, Peoples R China.
   [Zhang, Xiang; Liu, Shan] Tencent Amer, 2747 Pk Blvd, Palo Alto, CA 94306 USA.
C3 University of Missouri System; University of Missouri Kansas City;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Li, Z (corresponding author), Univ Missouri, 5000 Holmes St, Kansas City, MO 64110 USA.
EM wj3wr@umsystem.edu; lil1@ustc.edu.cn; zhu.li@ieee.org;
   xxiangzhang@tencent.com; shanl@tencent.com
RI Li, Zhu/AAD-8182-2021
OI Li, Zhu/0000-0002-8246-177X; Jia, Wei/0000-0003-0053-6959
FU National Science Foundation (NSF) Center for Big Learning [1747751]
FX The work is supported by a grant from National Science Foundation (NSF)
   Center for Big Learning under award 1747751.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Bellard Fabrice, 2019, Ffmpeg Software, A Complete, Cross-Platform Solution To Record, Convert And Stream Audio And Video
   Bjontegaard G., 2001, CALCULATION AVERAGE
   Bross B., 2019, Document JVET-M1001
   Chen C.-Y., 2010, JCTVCC147 ITUTISOIEC
   Chen Xi, 2016, ARXIV161102731
   Cheng J, 2018, FRONT INFORM TECH EL, V19, P64, DOI 10.1631/FITEE.1700789
   Cheng Y, 2018, IEEE SIGNAL PROC MAG, V35, P126, DOI 10.1109/MSP.2017.2765695
   Cheng Yu, 2017, ARXIV
   Chien W.-J, 2009, VCEGAL27 ITUTISOIEC
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Fu C., 2011, 4 JCTVC M JAN
   Fu C.-M., 2011, JCTVCE049 ITUTISOIEC
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Han QL, 2014, J VIS COMMUN IMAGE R, V25, P1044, DOI 10.1016/j.jvcir.2014.03.001
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XY, 2018, IEEE IMAGE PROC, P216, DOI 10.1109/ICIP.2018.8451086
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang Y.-W., 2010, JCTVCB077 ITUTISOIEC
   Ignatov A, 2019, LECT NOTES COMPUT SC, V11133, P315, DOI 10.1007/978-3-030-11021-5_20
   Jia CM, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Jia W, 2020, IEEE IMAGE PROC, P3109, DOI [10.1109/ICIP40778.2020.9191106, 10.1109/icip40778.2020.9191106]
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   King DB, 2015, ACS SYM SER, V1214, P1
   Li T, 2018, IEEE T MULTIMEDIA, V20, P1305, DOI 10.1109/TMM.2017.2766889
   Li Yiming, 2019, WP3 ITUTSG16
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lim H, 2011, IEEE T CIRC SYST VID, V21, P879, DOI 10.1109/TCSVT.2011.2133250
   Lim S, 2016, IEEE ICC, P1, DOI 10.1109/ICC.2016.7510599
   Lin Weiyao, 2019, IEEE T MULTIMEDIA, V22, P11
   List P, 2003, IEEE T CIRC SYST VID, V13, P614, DOI 10.1109/TCSVT.2003.815175
   Liu TM, 2007, IEEE T CIRC SYST VID, V17, P937, DOI 10.1109/TCSVT.2007.897467
   Lu G, 2018, LECT NOTES COMPUT SC, V11218, P591, DOI 10.1007/978-3-030-01264-9_35
   Ma SW, 2016, IEEE MULTIMEDIA, V23, P16, DOI 10.1109/MMUL.2016.16
   McCann Ken, 2010, SG16WP3 ITUT
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sharman Karl., 2018, SG16WP3 ITUT
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tsai CY, 2013, IEEE J-STSP, V7, P934, DOI 10.1109/JSTSP.2013.2271974
   Veit A, 2016, ADV NEUR IN, V29
   Wackerly D., 2014, CENGAGE LEARNING
   Wang TT, 2017, IEEE DATA COMPR CONF, P410, DOI 10.1109/DCC.2017.42
   Wang Y., 2018, 2018 IEEE VIS COMM I, P1, DOI [10.1109/VCIP.2018.8698740, DOI 10.1109/VCIP.2018.8698740]
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xue, 2017, ARXIV171109078, P1
   Yang R, 2019, IEEE T CIRC SYST VID, V29, P2039, DOI 10.1109/TCSVT.2018.2867568
   Yang R, 2017, IEEE INT CON MULTI, P817, DOI 10.1109/ICME.2017.8019299
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang XF, 2017, IEEE T CIRC SYST VID, V27, P2177, DOI 10.1109/TCSVT.2016.2581618
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3827, DOI 10.1109/TIP.2018.2815841
   Zhang YD, 2012, IEEE T MULTIMEDIA, V14, P510, DOI 10.1109/TMM.2012.2190391
NR 58
TC 3
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 139
DI 10.1145/3460820
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800023
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, ZY
   Wang, ZW
   Yuan, Y
   Zhang, JM
   Wang, ZY
   Jin, HL
AF Wu, Zhenyu
   Wang, Zhaowen
   Yuan, Ye
   Zhang, Jianming
   Wang, Zhangyang
   Jin, Hailin
TI Black-Box Diagnosis and Calibration on GAN Intra-Mode Collapse: A Pilot
   Study
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Mode collapse; black-box; diagnosis; calibration; hypothesis testing
AB Generative adversarial networks (GANs) nowadays are capable of producing images of incredible realism. Two concerns raised are whether the state-of-the-art GAN's learned distribution still suffers from mode collapse and what to do if so. Existing diversity tests of samples from GANs are usually conducted qualitatively on a small scale and/or depend on the access to original training data as well as the trained model parameters. This article explores GAN intra-mode collapse and calibrates that in a novel black-box setting: access to neither training data nor the trained model parameters is assumed. The new setting is practically demanded yet rarely explored and significantly more challenging. As a first stab, we devise a set of statistical tools based on sampling that can visualize, quantify, and rectify intra-mode collapse. We demonstrate the effectiveness of our proposed diagnosis and calibration techniques, via extensive simulations and experiments, on unconditional GAN image generation (e.g., face and vehicle). Our study reveals that the intra-mode collapse is still a prevailing problem in state-of-the-art GANs and the mode collapse is diagnosable and calibratable in black-box settings. Our codes are available at https://github.com/VITA- Group/BlackBoxGANCollapse.
C1 [Wu, Zhenyu; Yuan, Ye] Texas A&M Univ, 400 Bizzell St, College Stn, TX 77843 USA.
   [Wang, Zhaowen; Zhang, Jianming; Jin, Hailin] Adobe Res, 345 Pk Ave, San Jose, CA 95110 USA.
   [Wang, Zhangyang] Univ Texas Austin, 110 Inner Campus Dr, Austin, TX 78705 USA.
C3 Texas A&M University System; Texas A&M University College Station; Adobe
   Systems Inc.; University of Texas System; University of Texas Austin
RP Wu, ZY (corresponding author), Texas A&M Univ, 400 Bizzell St, College Stn, TX 77843 USA.
EM wuzhenyu_sjtu@tamu.edu; atlaswang@utexas.edu; ye.yuan@tamu.edu;
   jianmzha@adobe.com; atlaswang@utexas.edu; hljin@adobe.com
RI Wu, Zhenyu/ISU-2246-2023
CR Amini A, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P289, DOI 10.1145/3306618.3314243
   AngelinaWang Arvind Narayanan, 2020, P EUR C COMP VIS ECC
   [Anonymous], 2017, ARXIV170608224
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Arora S., 2018, INT C LEARN REPR, DOI 10.48550/arXiv.1706.08224
   Azadi S., 2018, INT C LEARN REPR ICL
   Barratt Shane, 2018, ARXIV180101973
   Bau D, 2019, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2019.00460
   Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009
   Bowles Christopher, 2018, ARXIV181110669
   Brock A., 2018, PREPRINT
   Che Tong, 2016, CoRR
   Deng J., 2019, ARXIV, DOI 10.48550/arXiv.1905.00641
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2019, INT J COMPUT VISION, V127, P599, DOI 10.1007/s11263-018-1134-y
   Durugkar Ishan, 2016, ARXIV161101673
   Fanti Giulia, 2018, P C NEUR INF PROC SY
   Filipovych R, 2011, NEUROIMAGE, V55, P1109, DOI 10.1016/j.neuroimage.2010.12.066
   Ghosh A, 2018, PROC CVPR IEEE, P8513, DOI 10.1109/CVPR.2018.00888
   Goeau H., 2017, CLEF 2017 C LABS EVA
   Goeau H., 2016, CLEF C LABS EVALUATI
   Goeau Herve, 2020, CLEF WORKING NOTES 2
   Gong XY, 2019, IEEE I CONF COMP VIS, P3223, DOI 10.1109/ICCV.2019.00332
   Goodfellow I., 2016, NIPS 2016 TUTORIAL G
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo J., 2018, ARXIV181201936
   Hensel M, 2017, ADV NEUR IN, V30
   Holstein Kenneth, 2018, ARXIV PREPRINT ARXIV
   Huang H., 2018, ARXIV180304469
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jiang Yifan, 2021, ARXIV PREPRINT ARXIV, V1, P3
   Karnewar Animesh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7796, DOI 10.1109/CVPR42600.2020.00782
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Liu ST, 2019, INT CONF ACOUST SPEE, P1902, DOI [10.1109/icassp.2019.8683590, 10.1109/ICASSP.2019.8683590]
   Liu X., 2018, IEEE INT CON MULTI
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mei Jian, 2020, DONGNIAO INT B UNPUB
   Metz L., 2016, INT C LEARN REPR
   Miyato T, 2018, INT C LEARN REPR
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Radford A., 2015, ARXIV151106434
   Salimans T, 2016, ADV NEUR IN, V29
   Santurkar Shibani, 2017, P INT C MACH LEARN P, P4480
   Sattigeri Prasanna, 2018, ARXIV180509910
   Savchenko AV, 2019, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.197
   Seeland Marco, 2017, JENA FLOWERS 30 DATA, DOI DOI 10.7910/DVN/QDHYST
   Shuai Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P601, DOI 10.1007/978-3-030-58555-6_36
   Srivastava A., 2017, Advances in Neural Information Processing Systems, P3308
   Sutskever I., 2015, ARXIV PREPRINT ARXIV
   Turner R., 2018, P INT C MACH LEARN P
   Uplavikar P. M., 2019, P IEEE C COMP VIS PA, P1
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Wang Huiyu, 2020, EUR C COMP VIS
   Wang Zhangyang, 2020, PCAL PRIVACY P UNPUB
   Webster Ryan, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11265, DOI 10.1109/CVPR.2019.01153
   Welinder P., 2010, Technical Report CNS-TR-2010-001
   Wu ZY, 2019, IEEE I CONF COMP VIS, P1201, DOI 10.1109/ICCV.2019.00129
   Xie L., 2018, CoRR
   Xu DP, 2018, IEEE INT CONF BIG DA, P570, DOI 10.1109/BigData.2018.8622525
   Yang S, 2019, IEEE I CONF COMP VIS, P4441, DOI 10.1109/ICCV.2019.00454
   Yang Shuai, 2021, IEEE T PATTERN ANAL
   Yang XP, 2020, INT J MACH LEARN CYB, V11, P667, DOI 10.1007/s13042-019-01025-1
   Yu Fisher, 2015, ARXIV150603365
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang XF, 2019, INT CONF ACOUST SPEE, P2807, DOI [10.1109/icassp.2019.8683197, 10.1109/ICASSP.2019.8683197]
   Zheng A., 2019, ARXIV190508997
   Zhenyu Wu, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P2508, DOI 10.1145/3394171.3413555
   Zhenyu Wu, 2020, IEEE T PATTERN ANAL
   Zhenyu Wu, 2018, P EUR C COMP VIS ECC
   Zhou Z., 2017, IJCAI, P3553
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 75
TC 1
Z9 1
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 114
DI 10.1145/3472768
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tong, C
   Zhang, MZ
   Lang, C
   Zheng, ZG
AF Tong, Chao
   Zhang, Mengze
   Lang, Chao
   Zheng, Zhigao
TI An Image Privacy Protection Algorithm Based on Adversarial Perturbation
   Generative Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Social network; neural networks; privacy; adversarial perturbation
   generative network
AB Today, users of social platforms upload a large number of photos. These photos contain personal private information, including user identity information, which is easily gleaned by intelligent detection algorithms. To thwart this, in this work, we propose an intelligent algorithm to prevent deep neural network (DNN) detectors from detecting private information, especially human faces, while minimizing the impact on the visual quality of the image. More specifically, we design an image privacy protection algorithm by training and generating a corresponding adversarial sample for each image to defend DNN detectors. In addition, we propose an improved model based on the previous model by training an adversarial perturbation generative network to generate perturbation instead of training for each image. We evaluate and compare our proposed algorithm with other methods on wider face dataset and others by three indicators: Mean average precision, Averaged distortion, and Time spent. The results show that our method significantly interferes with DNN detectors while causing weak impact to the visual quality of images, and our improved model does speed up the generation of adversarial perturbations.
C1 [Tong, Chao; Zhang, Mengze; Lang, Chao] Beihang Univ, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
   [Zheng, Zhigao] Huazhong Univ Sci & Technol, Luoyu Rd 1037, Wuhan, Peoples R China.
C3 Beihang University; Huazhong University of Science & Technology
RP Zheng, ZG (corresponding author), Huazhong Univ Sci & Technol, Luoyu Rd 1037, Wuhan, Peoples R China.
EM tongchao@buaa.edu.cn; zhangmengze@buaa.edu.cn; langchao007@buaa.edu.cn;
   zhengzhigao@hust.edu.cn
RI Zheng, Zhigao/ITT-8238-2023
OI Zheng, Zhigao/0000-0002-2504-9607; Zhang, Mengze/0000-0002-8774-4568
FU National Natural Science Foundation of China [61472024, U1433203];
   National Engineering Laboratory for Internet Medical System and
   Application [NELIMSA2018P01]
FX This work was supported by the National Natural Science Foundation of
   China (Grants No. 61472024 and No. U1433203) and Project of National
   Engineering Laboratory for Internet Medical System and Application
   (NELIMSA2018P01).
CR Acquisti A, 2020, MANAGE SCI, V66, P1005, DOI 10.1287/mnsc.2018.3269
   [Anonymous], 2015, STAT-US
   [Anonymous], ACM T MULTIMEDIA COM, V17
   [Anonymous], 2016, 2016 IEEE WINT C APP
   Baluja S., 2017, ARXIV PREPRINT ARXIV
   Bonneau J., 2009, ACM International Conf. Proc. of the 5th Symposium on Usable Privacy and Security, P1
   Bose AJ, 2018, IEEE INT WORKSH MULT
   Dong Xueying, 2018, INT C CLOUD COMPUTIN, P1, DOI DOI 10.1109/ICCBB.2018.8756447
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fang Lujun, 2010, P 19 INT C WORLD WID, P351, DOI DOI 10.1145/1772690.1772727
   He YW, 2019, PROC SPIE, V11069, DOI 10.1117/12.2524274
   Huang S., 2017, ARXIV PREPRINT ARXIV
   Klemperer P., 2012, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P377
   Kos J, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P36, DOI 10.1109/SPW.2018.00014
   McPherson Richard, 2016, ARXIV PREPRINT ARXIV
   Oh SJ, 2017, IEEE I CONF COMP VIS, P1491, DOI 10.1109/ICCV.2017.165
   Ravichandran R, 2009, LECT NOTES COMPUT SC, V5672, P1, DOI 10.1007/978-3-642-03168-7_1
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Spyromitros-Xioufis E, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P71, DOI 10.1145/2911996.2912018
   Squicciarini A, 2012, COLLABORATECOM, P89, DOI 10.4108/icst.collaboratecom.2012.250490
   Szegedy C., 2014, P INT C LEARN REPR
   Tabacof Pedro, 2016, ARXIV PREPRINT ARXIV
   Tonge A, 2016, AAAI CONF ARTIF INTE, P4266
   Tonge Ashwini, 2015, ARXIV PREPRINT ARXIV
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yu J, 2017, IEEE T INF FOREN SEC, V12, P1005, DOI 10.1109/TIFS.2016.2636090
   Zerr S, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P35, DOI 10.1145/2348283.2348292
   Zhong HT, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3952
   Zhu ZT, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3609
NR 29
TC 5
Z9 5
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 43
DI 10.1145/3381088
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000002
DA 2024-07-18
ER

PT J
AU Fu, SC
   Liu, WF
   Guan, WL
   Zhou, YC
   Tao, DP
   Xu, CS
AF Fu, Sichao
   Liu, Weifeng
   Guan, Weili
   Zhou, Yicong
   Tao, Dapeng
   Xu, Changsheng
TI Dynamic Graph Learning Convolutional Networks for Semi-supervised
   Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Graph representation learning; graph convolutional networks;
   semisupervised classification
AB Over the past few years, graph representation learning (GRL) has received widespread attention on the feature representations of the non-Euclidean data. As a typical model of GRL, graph convolutional networks (GCN) fuse the graph Laplacian-based static sample structural information. GCN thus generalizes convolutional neural networks to acquire the sample representations with the variously high-order structures. However, most of existing GCN-based variants depend on the static data structural relationships. It will result in the extracted data features lacking of representativeness during the convolution process. To solve this problem, dynamic graph learning convolutional networks (DGLCN) on the application of semi-supervised classification are proposed. First, we introduce a definition of dynamic spectral graph convolution operation. It constantly optimizes the high-order structural relationships between data points according to the loss values of the loss function, and then fits the local geometry information of data exactly. After optimizing our proposed definition with the one-order Chebyshev polynomial, we can obtain a single-layer convolution rule of DGLCN. Due to the fusion of the optimized structural information in the learning process, multi-layer DGLCN can extract richer sample features to improve classification performance. Substantial experiments are conducted on citation network datasets to prove the effectiveness of DGLCN. Experiment results demonstrate that the proposed DGLCN obtains a superior classification performance compared to several existing semi-supervised classification models.
C1 [Fu, Sichao; Liu, Weifeng] China Univ Petr East China, Coll Control Sci & Engn, Qingdao 266580, Peoples R China.
   [Guan, Weili] Monash Univ, Fac Informat Technol, Clayton Campus, Melbourne, Vic, Australia.
   [Zhou, Yicong] Univ Macau, Fac Sci & Technol, Macau 999078, Peoples R China.
   [Tao, Dapeng] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Yunnan, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 China University of Petroleum; Monash University; University of Macau;
   Yunnan University; Chinese Academy of Sciences; Institute of Automation,
   CAS
RP Liu, WF (corresponding author), China Univ Petr East China, Coll Control Sci & Engn, Qingdao 266580, Peoples R China.
EM fusichao_upc@163.com; liuwf@upc.edu.cn; honeyguan@gmail.com;
   yicongzhou@mn.edu.mo; dapeng.tao@gmail.com; csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023; liu, weifeng/B-7909-2008;
   Tao, Dapeng/E-8649-2013; Zhou, Yicong/A-8017-2009
OI Zhou, Yicong/0000-0002-4487-6384; Sichao, Fu/0000-0002-4363-1000
FU Major Scientific and Technological Projects of CNPC [ZD2019-183008];
   Open Project Program of the National Laboratory of Pattern Recognition
   (NLPR) [202000009]; Science and Technology Development Fund, Macau SAR
   [189/2017/A3]
FX This work was supported in part by the Major Scientific and
   Technological Projects of CNPC under Grant no. ZD2019-183008; in part by
   the Open Project Program of the National Laboratory of Pattern
   Recognition (NLPR) (Grant no. 202000009); in part by the Science and
   Technology Development Fund, Macau SAR (File no. 189/2017/A3).
CR ACM Trans, 2021, COMMUN APPL NUMER M, V17
   Alexandrescu A, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING, VOLS 1 AND 2, P359, DOI 10.1109/ASRU.2007.4430138
   Allegretto W, 1998, NONLINEAR ANAL-THEOR, V32, P819
   [Anonymous], 2021, ACM T MULTIM COMPUT, V17
   Atwood J, 2016, ADV NEUR IN, V29
   Bai J, 2013, IEEE T GEOSCI REMOTE, V51, P803, DOI 10.1109/TGRS.2012.2205002
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Cabanes C, 2013, OCEAN SCI, V9, P1, DOI 10.5194/os-9-1-2013
   Cao Y, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P357
   Cook DJ, 1993, J ARTIF INTELL RES, V1, P231, DOI 10.1613/jair.43
   Defferrard M, 2016, ADV NEUR IN, V29
   Fu SC, 2019, NEUROCOMPUTING, V362, P166, DOI 10.1016/j.neucom.2019.06.068
   Gao Y, 2014, TSINGHUA SCI TECHNOL, V19, P250, DOI 10.1109/TST.2014.6838195
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Geng B, 2012, IEEE T PATTERN ANAL, V34, P1227, DOI 10.1109/TPAMI.2012.57
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Hong Y, 2020, IEEE ACCESS, V8, P51315, DOI 10.1109/ACCESS.2020.2980859
   Kearnes S, 2016, J COMPUT AID MOL DES, V30, P595, DOI 10.1007/s10822-016-9938-8
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Lambert F, 2010, INFORM PROCESS MANAG, V46, P343, DOI 10.1016/j.ipm.2009.10.008
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Liu WF, 2016, IEEE T IND ELECTRON, V63, P5120, DOI 10.1109/TIE.2016.2552147
   Liu WF, 2015, SIGNAL PROCESS, V110, P101, DOI 10.1016/j.sigpro.2014.08.002
   Liu X, 2015, IEEE ENG MED BIO, P4130, DOI 10.1109/EMBC.2015.7319303
   Luo Y, 2013, IEEE T IMAGE PROCESS, V22, P523, DOI 10.1109/TIP.2012.2218825
   Ma XQ, 2019, IEEE T GEOSCI REMOTE, V57, P1585, DOI 10.1109/TGRS.2018.2867570
   Puente RR, 2012, IEEE LAT AM T, V10, P2201, DOI 10.1109/TLA.2012.6418123
   Qiu JZ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2110, DOI 10.1145/3219819.3220077
   Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157
   Talukdar Partha, 2018, P INT C MULT EXP
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang Y., 2016, P 25 INT JOINT C ART, P2153
   Wang Y, 2016, LECT NOTES COMPUT SC, V10041, P35, DOI 10.1007/978-3-319-48740-3_3
   Wang Y, 2017, IEEE T IMAGE PROCESS, V26, P1393, DOI 10.1109/TIP.2017.2655449
   Wang Y, 2017, IEEE T NEUR NET LEAR, V28, P57, DOI 10.1109/TNNLS.2015.2498149
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Wu J, 2018, IEEE T NEUR NET LEAR, V29, P3236, DOI 10.1109/TNNLS.2017.2703832
   Wu L, 2021, IEEE T NEUR NET LEAR, V32, P722, DOI 10.1109/TNNLS.2020.2979190
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu L, 2019, IEEE T IMAGE PROCESS, V28, P1602, DOI 10.1109/TIP.2018.2878970
   Xu Keyulu., 2018, P I NT C MACH LEARN
   Yang W, 2018, ACM T MULTIM COMPUT, V37
   Yang YY, 2016, INFORM PROCESS MANAG, V52, P911, DOI 10.1016/j.ipm.2016.04.001
   Zadrozny S, 2012, INFORM PROCESS MANAG, V48, P390, DOI 10.1016/j.ipm.2011.05.001
   Zhang LF, 2015, PATTERN RECOGN, V48, P3102, DOI 10.1016/j.patcog.2014.12.016
   Zhang YS, 2016, NEUROCOMPUTING, V207, P684, DOI 10.1016/j.neucom.2016.05.053
   Zhang Z, 2021, IEEE T NEUR NET LEAR, V32, P947, DOI 10.1109/TNNLS.2020.2979748
   Zhao NW, 2017, IEEE T KNOWL DATA EN, V29, P2498, DOI 10.1109/TKDE.2017.2732986
   Zhou J., 2018, ARTIF CELL NANOMED B, DOI [10.1016/j.aiopen.2021.01.001, DOI 10.1080/21691401.2018.1442841]
   Zhou X, 2020, IEEE T CYBERNETICS, V50, P1460, DOI 10.1109/TCYB.2018.2883970
NR 52
TC 12
Z9 12
U1 1
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 4
DI 10.1145/3412846
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA RW2ZT
UT WOS:000646396900004
DA 2024-07-18
ER

PT J
AU Zhang, J
   Guo, JQ
   Ren, YG
AF Zhang, Jing
   Guo, Jiaqi
   Ren, Yonggong
TI Robust Ordinal Regression: User Credit Grading with Triplet Loss-Based
   Sampling
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE User credit evaluation; ordinal regression; metric learning; hard
   negative sampling
ID VECTOR MACHINES
AB With the development of social media sites, user credit grading, which served as an important and fashionable problem, has attracted substantial attention from a slew of developers and operators of mobile applications. In particular, multi-grades of user credit aimed to achieve (1) anomaly detection and risk early warning and (2) personalized information and service recommendation for privileged users. The above two goals still remained as up-to-date challenges. To these ends, in this article, we propose a novel regression-based method. Technically speaking, we define three natural ordered categories including BlockList, GeneralList, and AllowList according to users' registration and behavior information, which preserve both the global hierarchical relationship of user credit and the local coincident features of users, and hence formulate user credit grading as the ordinal regression problem. Our method is inspired by KDLOR (kernel discriminant learning for ordinal regression), which is an effective and efficient model to solve ordinal regression by mapping high-dimension samples to the discriminant region with supervised conditions. However, the performance of KDLOR is fragile to the extreme imbalanced distribution of users. To address this problem, we propose a robust sampling model to balance distribution and avoid overfit or underfit learning, which induces the triplet metric constraint to obtain hard negative samples that well represent the latent ordered class information. A step further, another salient problem lies in ambiguous samples that are noises or located in the classification boundary to impede optimized mapping and embedding. To this problem, we improve sampling by identifying and evading noises in triplets to obtain hard negative samples to enhance robustness and effectiveness for ordinal regression. We organized training and testing datasets for user credit grading by selecting limited items from real-life huge tables of users in the mobile application, which are used in similar problems; moreover, we theoretically and empirically demonstrate the advantages of the proposed model over established datasets.
C1 [Zhang, Jing; Guo, Jiaqi; Ren, Yonggong] Liaoning Normal Univ, 1 Liushu South St, Dalian, Peoples R China.
C3 Liaoning Normal University
RP Zhang, J (corresponding author), Liaoning Normal Univ, 1 Liushu South St, Dalian, Peoples R China.
EM zhangjing_0412@163.com; guojiaqi96@163.com; ryg@lnnu.edu.cn
RI Guo, Jiaqi/GSO-0702-2022
FU National Natural Science Foundation of China Youth Fund [6190070373];
   National Science Foundation of China [61976109, 61772252]; Liaoning
   Ministry of Education Youth Fund [LQ2019029]; Liaoning Natural Science
   Foundation [20180550542]; Dalian Science and Technology Innovation Fund
   [2018J12GX047]; Dalian Key Laboratory Special Fund
FX This work is in part supported by the National Natural Science
   Foundation of China Youth Fund (No. 6190070373), National Science
   Foundation of China (No. 61976109, 61772252), Liaoning Ministry of
   Education Youth Fund (LQ2019029), Liaoning Natural Science Foundation
   (No. 20180550542), Dalian Science and Technology Innovation Fund (No.
   2018J12GX047), and Dalian Key Laboratory Special Fund.
CR Agarwal S, 2008, LECT NOTES ARTIF INT, V5254, P7, DOI 10.1007/978-3-540-87987-9_6
   Agresti A, 2002, CATEGORICAL DATA ANA
   Ala'raj M, 2016, EXPERT SYST APPL, V64, P36, DOI 10.1016/j.eswa.2016.07.017
   Ala'raj M, 2016, KNOWL-BASED SYST, V104, P89, DOI 10.1016/j.knosys.2016.04.013
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   Gutiérrez PA, 2016, IEEE T KNOWL DATA EN, V28, P127, DOI 10.1109/TKDE.2015.2457911
   Barnett WA, 2019, J FINANC STABIL, V42, P18, DOI 10.1016/j.jfs.2019.05.005
   Bequé A, 2017, EXPERT SYST APPL, V86, P42, DOI 10.1016/j.eswa.2017.05.050
   Chen YF, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P665, DOI 10.1145/3331184.3331196
   Cheng JL, 2008, IEEE IJCNN, P1279, DOI 10.1109/IJCNN.2008.4633963
   Deng WY, 2010, NEUROCOMPUTING, V74, P447, DOI 10.1016/j.neucom.2010.08.022
   DeYoreo M, 2018, J COMPUT GRAPH STAT, V27, P71, DOI 10.1080/10618600.2017.1316280
   Fathony R., 2017, NIPS, P563
   Frank Eibe, 2001, EUR C MACH LEARN, P145, DOI 10.1007/3-540-44795-413
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Gu B, 2015, IEEE T NEUR NET LEAR, V26, P1403, DOI 10.1109/TNNLS.2014.2342533
   Holte R. C., WORKSH LEARN IMB DAT, V11, P1
   Jeong B, 2009, EXPERT SYST APPL, V36, P7309, DOI 10.1016/j.eswa.2008.09.034
   Kasower S, 2017, U.S. Patent Application, Patent No. [15/482,318[P], 15482318]
   Kim KJ, 2012, COMPUT OPER RES, V39, P1800, DOI 10.1016/j.cor.2011.06.023
   Kwon Y. S., 1997, International Journal of Intelligent Systems in Accounting, Finance and Management, V6, P23, DOI 10.1002/(SICI)1099-1174(199703)6:1<23::AID-ISAF113>3.0.CO;2-4
   Li CS, 2015, IEEE T NEUR NET LEAR, V26, P1551, DOI 10.1109/TNNLS.2014.2339100
   Liu Y., 2012, P 4 INT C INT MULT C, P119
   Liu Yang, 2011, 25 AAAI C ART INT
   Louzada Francisco, 2016, Surveys in Operations Research and Management Science, V21, P117, DOI 10.1016/j.sorms.2016.10.001
   Luo CC, 2017, ENG APPL ARTIF INTEL, V65, P465, DOI 10.1016/j.engappai.2016.12.002
   Mathieson M, 1997, ADV NEUR IN, V9, P550
   MCCULLAGH P, 1980, J ROY STAT SOC B MET, V42, P109
   Mori R, 1992, U.S. Patent, Patent No. [5,103,392[P], 5103392]
   Niu ZX, 2016, PROC CVPR IEEE, P4920, DOI 10.1109/CVPR.2016.532
   Pérez-Ortiz M, 2015, IEEE T KNOWL DATA EN, V27, P1233, DOI 10.1109/TKDE.2014.2365780
   Polanía LF, 2019, IEEE WINT CONF APPL, P782, DOI 10.1109/WACV.2019.00088
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sun BY, 2010, IEEE T KNOWL DATA EN, V22, P906, DOI 10.1109/TKDE.2009.170
   Taha AA, 2020, IEEE ACCESS, V8, P25579, DOI 10.1109/ACCESS.2020.2971354
   Torra V, 2006, INFORM SCIENCES, V176, P465, DOI 10.1016/j.ins.2005.07.007
   Tsang IW, 2005, J MACH LEARN RES, V6, P363
   Waegeman W., 2009, INT J COMPUT SYST SC, V3, P47
   Wang Y., 2020, ACM T MULTIMEDIA COM
   Wang Y, 2017, IEEE T IMAGE PROCESS, V26, P1393, DOI 10.1109/TIP.2017.2655449
   Wang Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P79, DOI 10.1145/2733373.2806233
   Wu L, 2019, IEEE T IMAGE PROCESS, V28, P1602, DOI 10.1109/TIP.2018.2878970
   [杨贵军 Yang Guijun], 2019, [数据分析与知识发现, Data Analysis and Knowledge Discovery], V3, P118
   Yuan T, 2018, 2018 INTERNATIONAL CONFERENCE ON INFORMATION MANAGEMENT AND PROCESSING (ICIMP 2018), P16, DOI 10.1109/ICIMP1.2018.8325834
   Zhao B, 2009, IEEE T NEURAL NETWOR, V20, P882, DOI 10.1109/TNN.2009.2017533
   Zhu TF, 2019, KNOWL-BASED SYST, V166, P140, DOI 10.1016/j.knosys.2018.12.021
NR 46
TC 1
Z9 1
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 7
DI 10.1145/3408303
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900007
DA 2024-07-18
ER

PT J
AU Xu, XL
   Huang, QH
   Zhang, YW
   Li, SC
   Qi, LY
   Dou, WC
AF Xu, Xiaolong
   Huang, Qihe
   Zhang, Yiwen
   Li, Shancang
   Qi, Lianyong
   Dou, Wanchun
TI An LSH-based Offloading Method for IoMT Services in Integrated
   Cloud-Edge Environment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE IoMT; cloud-edge computing; service offloading; privacy preservation;
   LSH
ID IOT; TRANSMISSION
AB Benefiting from the massive available data provided by Internet of multimedia things (IoMT), enormous intelligent services requiring information of various types to make decisions are emerging. Generally, the IoMT devices are equipped with limited computing power, interfering with the process of computation-intensive services. Currently, to satisfy a wide range of service requirements, the novel computing paradigms, i.e., cloud computing and edge computing, can potentially be integrated for service accommodation. Nevertheless, the private information (i.e., location, service type, etc.) in the services is prone to spilling out during service offloading in the cloud-edge computing. To avoid privacy leakage while improving service utility, including the service response time and energy consumption for service executions, a Locality-sensitive-hash (LSH)-based offloading method, named LOM, is devised. Specifically, LSH is leveraged to encrypt the feature information for the services offloaded to the edge servers with the intention of privacy preservation. Eventually, comparative experiments are conducted to verify the effectiveness of LOM with respect to promoting service utility.
C1 [Xu, Xiaolong; Huang, Qihe] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing, Peoples R China.
   [Xu, Xiaolong] Nanjing Univ Informat Sci & Technol, Jiangsu Engn Ctr Network Monitoring, Nanjing, Peoples R China.
   [Xu, Xiaolong] Minist Educ, Engn Res Ctr Digital Fors, Nanjing, Peoples R China.
   [Xu, Xiaolong] Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing, Peoples R China.
   [Zhang, Yiwen] Anhui Univ, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.
   [Li, Shancang] Univ West England, Comp Sci & Creat Technol Dept, Bristol, Avon, England.
   [Qi, Lianyong] Qufu Normal Univ, Sch Informat Sci & Engn, Jining, Peoples R China.
   [Dou, Wanchun] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; Nanjing University of
   Information Science & Technology; Anhui University; University of West
   England; Qufu Normal University; Nanjing University
RP Qi, LY (corresponding author), Qufu Normal Univ, Sch Informat Sci & Engn, Jining, Peoples R China.
EM njuxlxu@gmail.com; qhuang@nuist.edu.cn; zhangyiwen@ahu.edu.cn;
   Shancang.Li@uwe.ac.uk; lianyongqi@gmail.com; douwc@nju.edu.cn
RI Xu, Xiaolong/U-2547-2019; Qi, Lianyong/AAO-2681-2020; li,
   zhang/JHV-1750-2023
OI Xu, Xiaolong/0000-0003-4879-9803; Zhang, Yiwen/0000-0001-8709-1088
FU Financial and Science Technology Plan Project of Xinjiang Production and
   Construction Corps [2020DB005]; National Natural Science Foundation of
   China [61702277, 61872219, 61672276]; Priority Academic Program
   Development of Jiangsu Higher Education Institutions fund
FX This research is supported by the Financial and Science Technology Plan
   Project of Xinjiang Production and Construction Corps under Grant No.
   2020DB005, and the National Natural Science Foundation of China under
   Grants No. 61702277, No. 61872219, and No. 61672276. This research is
   also supported by the Priority Academic Program Development of Jiangsu
   Higher Education Institutions fund.
CR Bai T, 2019, IEEE T VEH TECHNOL, V68, P6074, DOI 10.1109/TVT.2019.2912227
   Ding K, 2018, IEEE T NEUR NET LEAR, V29, P87, DOI 10.1109/TNNLS.2016.2615085
   Guo C, 2020, IEEE INTERNET THINGS, V7, P3104, DOI 10.1109/JIOT.2020.2964412
   Han SJ, 2019, IEEE INTERNET THINGS, V6, P5674, DOI 10.1109/JIOT.2019.2904741
   Hao F, 2019, IEEE INTERNET THINGS, V6, P4764, DOI 10.1109/JIOT.2018.2867351
   He S, 2020, IEEE T NETW SCI ENG, V7, P398, DOI 10.1109/TNSE.2019.2897483
   He XF, 2020, IEEE T WIREL COMMUN, V19, P1814, DOI 10.1109/TWC.2019.2958091
   He XF, 2019, IEEE INTERNET THINGS, V6, P4547, DOI 10.1109/JIOT.2018.2878718
   Hong ZC, 2019, IEEE T PARALL DISTR, V30, P2759, DOI 10.1109/TPDS.2019.2926979
   Hu WM, 2018, IEEE T IMAGE PROCESS, V27, P4452, DOI 10.1109/TIP.2018.2839886
   Hu XY, 2020, IEEE T WIREL COMMUN, V19, P1070, DOI 10.1109/TWC.2019.2950632
   Jan MA, 2019, IEEE INTERNET THINGS, V6, P1576, DOI 10.1109/JIOT.2018.2848284
   Jia M, 2019, IEEE INTERNET THINGS, V6, P4252, DOI 10.1109/JIOT.2018.2875743
   Li E, 2020, IEEE T WIREL COMMUN, V19, P447, DOI 10.1109/TWC.2019.2946140
   Li HY, 2019, IEEE T KNOWL DATA EN, V31, P423, DOI 10.1109/TKDE.2018.2836464
   Lin B, 2019, IEEE T IND INFORM, V15, P4254, DOI 10.1109/TII.2019.2905659
   Min MH, 2019, IEEE INTERNET THINGS, V6, P4307, DOI 10.1109/JIOT.2018.2875926
   Ren JK, 2019, IEEE T VEH TECHNOL, V68, P5031, DOI 10.1109/TVT.2019.2904244
   Ruan LN, 2020, IEEE T IND INFORM, V16, P1848, DOI 10.1109/TII.2019.2933631
   Shao WH, 2019, IEEE INTERNET THINGS, V6, P7122, DOI 10.1109/JIOT.2019.2914344
   Shen J, 2019, IEEE T DEPEND SECURE, V16, P996, DOI 10.1109/TDSC.2017.2725953
   Singhal C, 2019, IEEE T VEH TECHNOL, V68, P11186, DOI 10.1109/TVT.2019.2938406
   Skourletopoulos G, 2019, IEEE T GREEN COMMUN, V3, P122, DOI 10.1109/TGCN.2018.2890034
   Son J, 2019, NPJ QUANTUM MATER, V4, DOI 10.1038/s41535-019-0157-0
   Thai MT, 2020, IEEE T NETW SERV MAN, V17, P227, DOI 10.1109/TNSM.2019.2937342
   Thiyagarajan K, 2019, IEEE T CIRC SYST VID, V29, P610, DOI 10.1109/TCSVT.2018.2808174
   Tian ZH, 2019, IEEE T IND INFORM, V15, P4285, DOI 10.1109/TII.2019.2907754
   Wang L, 2019, IEEE T MOBILE COMPUT, V18, P1843, DOI 10.1109/TMC.2018.2867520
   Wang X, 2020, ZOOKEYS, P1, DOI 10.3897/zookeys.965.56199
   Wu W, 2019, IEEE T KNOWL DATA EN, V31, P2332, DOI 10.1109/TKDE.2018.2876250
   Xu XL, 2020, IEEE INTERNET THINGS, V7, P2622, DOI 10.1109/JIOT.2019.2944007
   Zhang ZY, 2019, IEEE T BIG DATA, V5, P520, DOI 10.1109/TBDATA.2017.2657623
   Zhou X., 2020, 2020 IEEE INT S CIRC, P1
   Zhou XK, 2019, IEEE T COMPUT SOC SY, V6, P888, DOI 10.1109/TCSS.2019.2918285
   Zhou XT, 2018, TRANSBOUND EMERG DIS, V65, P1482, DOI 10.1111/tbed.12989
   Zhu YF, 2020, IEEE T NETW SCI ENG, V7, P42, DOI 10.1109/TNSE.2018.2873311
NR 36
TC 21
Z9 21
U1 1
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 3
SU S
AR 94
DI 10.1145/3408319
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2RB
UT WOS:000612586300002
DA 2024-07-18
ER

PT J
AU Lv, CL
   Wu, ZK
   Wang, XC
   Zhou, MQ
AF Lv, Chenlei
   Wu, Zhongke
   Wang, Xingce
   Zhou, Mingquan
TI 3D Facial Similarity Measurement and Its Application in Facial
   Organization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Kendall shape space; facial feature landmarks model; face recognition;
   facial data organization
ID FACE RECOGNITION; DATABASE; FUSION; DEEP
AB We propose a novel framework for 3D facial similarity measurement and its application in facial organization. The construction of the framework is based on Kendall shape space theory. Kendall shape space is a quotient space that is constructed by shape features. In Kendall shape space, the shape features can be measured and is robust to similarity transformations. In our framework, a 3D face is represented by the facial feature landmarks model (FFLM), which can be regarded as the facial shape features. We utilize the geodesic in Kendall shape space to represent the FFLM similarity measurement, which can be regarded as the 3D facial similarity measurement. The FFLM similarity measurement is robust to facial expressions, head poses, and partial facial data. In our experiments, we compute the distance between different FFLMs in two public facial databases: FRGC2.0 and BosphorusDB. On average, we achieve a rank-one facial recognition rate of 98%. Based on the similarity results, we propose a method to construct the facial organization. The facial organization is a hierarchical structure that is achieved from the facial clustering by FFLM similarity measurement. Based on the facial organization, the performance of face searching in a large facial database can be improved obviously (about 400% improvement in experiments).
C1 [Lv, Chenlei; Wu, Zhongke; Wang, Xingce; Zhou, Mingquan] Beijing Normal Univ, Sch Artificial Intelligence, Beijing, Peoples R China.
C3 Beijing Normal University
RP Lv, CL (corresponding author), Beijing Normal Univ, Sch Artificial Intelligence, Beijing, Peoples R China.
EM chenleilv@mail.bnu.edu.cn; zwu@bnu.edu.cn; wangxingce@bnu.edu.cn;
   mqzhou@bnu.edu.cn
RI ZHOU, MING/JVP-2920-2024
OI Chenlei, Lv/0000-0002-8203-3118
CR Abbas HH, 2019, CYBERN INF TECHNOL, V19, P28, DOI 10.2478/cait-2019-0013
   Ahdid R., 2017, International Journal of Informatics and Communication Technology, V6, P10
   Ahdid R., 2015, J COMPUT SCI APPL, V3, P67
   Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   [Anonymous], 2017, BIOMED RES INT
   [Anonymous], IEEE 2 INT C BIOM TH, DOI DOI 10.1109/BTAS.2008.4699378
   [Anonymous], 1995, COMPUTER VISION IMAG
   Ansari AN, 2005, PATTERN RECOGN, V38, P2549, DOI 10.1016/j.patcog.2005.04.016
   Berretti S, 2014, VISUAL COMPUT, V30, P1275, DOI 10.1007/s00371-014-0932-7
   Berretti S, 2013, COMPUT GRAPH-UK, V37, P509, DOI 10.1016/j.cag.2013.04.001
   Berretti S, 2010, IEEE T PATTERN ANAL, V32, P2162, DOI 10.1109/TPAMI.2010.43
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7
   Bronstein AM, 2005, INT J COMPUT VISION, V64, P5, DOI 10.1007/s11263-005-1085-y
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chen H, 2007, IEEE T PATTERN ANAL, V29, P718, DOI 10.1109/TPAMI.2007.1005
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dorai C, 1997, IEEE T PATTERN ANAL, V19, P1115, DOI 10.1109/34.625113
   Drira H, 2013, IEEE T PATTERN ANAL, V35, P2270, DOI 10.1109/TPAMI.2013.48
   Drira H, 2009, IEEE I CONF COMP VIS, P2050, DOI 10.1109/ICCV.2009.5459451
   Dryden IL, 2008, BIOMETRIKA, V95, P779, DOI 10.1093/biomet/asn050
   Efraty B, 2012, PATTERN RECOGN, V45, P43, DOI 10.1016/j.patcog.2011.07.010
   Emambakhsh M, 2017, IEEE T PATTERN ANAL, V39, P995, DOI 10.1109/TPAMI.2016.2565473
   Ferrari C, 2017, IEEE T MULTIMEDIA, V19, P2666, DOI 10.1109/TMM.2017.2707341
   Frey B., 2007, MULTI DATABASE RETRI, Vvol. 315, ppp, DOI [DOI 10.1126/SCIENCE.1136800, 10.1126/science.1136800]
   Gaikwad AV, 2015, LECT NOTES COMPUT SC, V9351, P628, DOI 10.1007/978-3-319-24574-4_75
   Ganguly S, 2017, INTELL AUTOM SOFT CO, V23, P51, DOI 10.1080/10798587.2015.1121616
   Gilani S. Z., 2016, 2016 INT C DIGITAL I, P1
   Gilani SZ, 2017, PATTERN RECOGN, V69, P238, DOI 10.1016/j.patcog.2017.04.013
   Gilani SZ, 2015, PROC CVPR IEEE, P4639, DOI 10.1109/CVPR.2015.7299095
   Gökberk B, 2005, LECT NOTES COMPUT SC, V3546, P1019
   Huang D, 2012, IEEE T INF FOREN SEC, V7, P1551, DOI 10.1109/TIFS.2012.2206807
   Jahanbin S, 2013, INT J COMPUT VISION, V105, P87, DOI 10.1007/s11263-013-0631-2
   Johnson A., 1997, Thesis
   Kafai M, 2014, IEEE T MULTIMEDIA, V16, P1090, DOI 10.1109/TMM.2014.2305633
   KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81
   Koppen P, 2018, PATTERN RECOGN, V74, P617, DOI 10.1016/j.patcog.2017.09.006
   Kurtek S, 2015, COMPUT GRAPH-UK, V51, P52, DOI 10.1016/j.cag.2015.05.027
   Lei YJ, 2014, PATTERN RECOGN, V47, P509, DOI 10.1016/j.patcog.2013.07.018
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Li HB, 2015, INT J COMPUT VISION, V113, P128, DOI 10.1007/s11263-014-0785-6
   Lv C., 2018, MATH PROBL ENG, V2018, P1
   Lv CL, 2019, NEUROCOMPUTING, V355, P155, DOI 10.1016/j.neucom.2019.04.050
   Lv CL, 2019, PATTERN RECOGN, V88, P458, DOI 10.1016/j.patcog.2018.12.006
   Mahmood Sawsen Abdulhadi, 2014, 2014 6th Computer Science and Electronic Engineering Conference (CEEC). Proceedings, P103, DOI 10.1109/CEEC.2014.6958563
   Mpiperis I, 2008, IEEE T INF FOREN SEC, V3, P498, DOI 10.1109/TIFS.2008.924598
   Ouamane A, 2017, IEEE T INF FOREN SEC, V12, P2751, DOI 10.1109/TIFS.2017.2718490
   Patel A., 2015, PNAS, V52, P206
   Perakis P, 2013, IEEE T PATTERN ANAL, V35, P1552, DOI 10.1109/TPAMI.2012.247
   Phillips PJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P15
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Schwab S, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-10
   Smeets D, 2013, COMPUT VIS IMAGE UND, V117, P158, DOI 10.1016/j.cviu.2012.10.002
   Soltanpour S, 2019, IEEE T IMAGE PROCESS, V28, P3020, DOI 10.1109/TIP.2019.2893524
   Sukno FM, 2015, IEEE T CYBERNETICS, V45, P1717, DOI 10.1109/TCYB.2014.2359056
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Wang YM, 2006, LECT NOTES COMPUT SC, V3851, P581
   Xu M, 2018, IEEE T MULTIMEDIA, V20, P1335, DOI 10.1109/TMM.2017.2767784
   Yan P, 2007, IEEE T PATTERN ANAL, V29, P1297, DOI 10.1109/TPAMI.2007.1067
   Yuan L, 2016, NEUROCOMPUTING, V171, P540, DOI 10.1016/j.neucom.2015.06.074
   Zeng W, 2010, IEEE T PATTERN ANAL, V32, P662, DOI 10.1109/TPAMI.2009.201
   Zhang L, 2016, IEEE T MULTIMEDIA, V18, P1531, DOI 10.1109/TMM.2016.2566578
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
NR 65
TC 11
Z9 11
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 82
DI 10.1145/3397765
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200008
DA 2024-07-18
ER

PT J
AU Li, ZJ
   Zhou, ZW
   Jiang, N
   Han, ZJ
   Xing, JL
   Jiao, JB
AF Li, Zhaoju
   Zhou, Zongwei
   Jiang, Nan
   Han, Zhenjun
   Xing, Junliang
   Jiao, Jianbin
TI Spatial Preserved Graph Convolution Networks for Person
   Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; graph convolution; feature embedding
AB Person Re-identification is a very challenging task due to inter-class ambiguity caused by similar appearances, and large intra-class diversity caused by viewpoints, illuminations, and poses. To address these challenges, in this article, a graph convolution network based model for person re-identification is proposed to learn more discriminative feature embeddings, where a graph-structured relationship between person images and person parts are together integrated. Graph convolution networks extract common characteristics of the same person, while pyramid feature embedding exploits parts relations and learns stable representation with each person image. We achieve a very competitive performance respectively on three widely used datasets, indicating that the proposed approach significantly outperforms the baseline methods and achieves the state-of-the-art performance.
C1 [Li, Zhaoju; Jiang, Nan; Han, Zhenjun; Jiao, Jianbin] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Zhou, Zongwei; Xing, Junliang] Univ Chinese Acad Sci, Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Institute of Automation, CAS
RP Han, ZJ (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM lizhaoju13@mails.ucas.ac.cn; zhouzongwei2016@ia.ac.cn;
   jiangnan18@mails.ucas.ac.cn; hanzhj@ucas.ac.cn; jlxing@nlpr.ia.ac.cn;
   jiaojb@ucas.ac.cn
RI Xing, Junliang/HGE-9630-2022; JIANG, NAN/AHB-1945-2022
OI Xing, Junliang/0000-0001-6801-0510; Jiang, Nan/0000-0002-8838-7427
CR [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247939
   Bai S, 2017, AAAI CONF ARTIF INTE, P1281
   Cai Y, 2017, PROCEEDINGS OF THE ASME POWER CONFERENCE JOINT WITH ICOPE-17, 2017, VOL 2
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Defferrard M, 2016, ADV NEUR IN, V29
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Gao MF, 2017, 2017 13TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (ICNC-FSKD), P1077, DOI 10.1109/FSKD.2017.8392913
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   He ZY, 2017, IEEE T CYBERNETICS, V47, P354, DOI 10.1109/TCYB.2016.2514714
   He ZY, 2016, IEEE T IMAGE PROCESS, V25, P3698, DOI 10.1109/TIP.2016.2570553
   Hermans Alexander, 2017, ARXIV170307737
   Karaman S, 2014, PATTERN RECOGN, V47, P3767, DOI 10.1016/j.patcog.2014.06.003
   Kipf TN, 2016, ARXIV
   Kviatkovsky Igor, 2013, IEEE Trans Pattern Anal Mach Intell, V35, P1622, DOI 10.1109/TPAMI.2012.246
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2017, PATTERN RECOGN, V61, P327, DOI 10.1016/j.patcog.2016.08.001
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu CX, 2014, PATTERN RECOGN, V47, P1602, DOI 10.1016/j.patcog.2013.11.001
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Roth PM, 2014, ADV COMPUT VIS PATT, P247, DOI 10.1007/978-1-4471-6296-4_12
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Szegedy C, 2014, INT C LEARN REPR
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xu HC, 2017, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/aa7a3e
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang SP, 2018, IEEE T INTELL TRANSP, V19, P187, DOI 10.1109/TITS.2017.2766093
   Zhang SP, 2017, IEEE T NEUR NET LEAR, V28, P2357, DOI 10.1109/TNNLS.2016.2586194
   Zhang SP, 2017, IEEE T CIRC SYST VID, V27, P421, DOI 10.1109/TCSVT.2016.2539860
   Zhang SP, 2015, IEEE T CIRC SYST VID, V25, P1749, DOI 10.1109/TCSVT.2015.2406194
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
NR 57
TC 9
Z9 9
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 26
DI 10.1145/3362988
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300008
DA 2024-07-18
ER

PT J
AU Wang, ZC
   Li, Y
   Hong, RC
   Tian, XM
AF Wang, Zhangcheng
   Li, Ya
   Hong, Richang
   Tian, Xinmei
TI Eigenvector-Based Distance Metric Learning for Image Classification and
   Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Distance metric learning; reproducing kernel Hilbert space; nonsmooth
   function optimization
AB Distance metric learning has been widely studied in multifarious research fields. The mainstream approaches learn a Mahalanobis metric or learn a linear transformation. Recent related works propose learning a linear combination of base vectors to approximate the metric. In this way, fewer variables need to be determined, which is efficient when facing high-dimensional data. Nevertheless, such works obtain base vectors using additional data from related domains or randomly generate base vectors. However, obtaining base vectors from related domains requires extra time and additional data, and random vectors introduce randomness into the learning process, which requires sufficient random vectors to ensure the stability of the algorithm. Moreover, the random vectors cannot capture the rich information of the training data, leading to a degradation in performance. Considering these drawbacks, we propose a novel distance metric learning approach by introducing base vectors explicitly learned from training data. Given a specific task, we can make a sparse approximation of its objective function using the top eigenvalues and corresponding eigenvectors of a predefined integral operator on the reproducing kernel Hilbert space. Because the process of generating eigenvectors simply refers to the training data of the considered task, our proposed method does not require additional data and can reflect the intrinsic information of the input features. Furthermore, the explicitly learned eigenvectors do not result in randomness, and we can extend our method to any kernel space without changing the objective function. We only need to learn the coefficients of these eigenvectors, and the only hyperparameter that we need to determine is the number of eigenvectors that we utilize. Additionally, an optimization algorithm is proposed to efficiently solve this problem. Extensive experiments conducted on several datasets demonstrate the effectiveness of our proposed method.
C1 [Wang, Zhangcheng; Tian, Xinmei] Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230027, Anhui, Peoples R China.
   [Li, Ya] IFLYTEK Res, 666 Wangjiang West Rd, Hefei 230088, Anhui, Peoples R China.
   [Hong, Richang] Hefei Univ Technol, 193 Tunxi Rd, Hefei 230009, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Hefei University of Technology
RP Wang, ZC (corresponding author), Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230027, Anhui, Peoples R China.
EM wzc1@mail.ustc.edu.cn; yali8@iflytek.com; hongrc.hfut@gmail.com;
   xinmei@ustc.edu.cn
FU NSFC [61872329, 61722204, 61732007, 61572451]
FX This work was supported by NSFC No. 61872329, No. 61722204, No.
   61732007, and No. 61572451.
CR [Anonymous], 2014, P EUR C COMP VIS ECC
   [Anonymous], 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.527
   Baghshah MS, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1217
   Bar-Hillel AB, 2005, J MACH LEARN RES, V6, P937
   Bhattarai B, 2016, PROC CVPR IEEE, P4226, DOI 10.1109/CVPR.2016.458
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Davis J. V., 2007, ICML, P209
   Dong Husheng, 2018, ACM T MULTIM COMPUT, V14, P3
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Golub G.H., 1996, MATRIX COMPUTATIONS, P374
   Guo X, 2012, APPL COMPUT HARMON A, V32, P389, DOI 10.1016/j.acha.2011.07.005
   Hoi SCH, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823752
   Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129
   Jin R., 2009, Advances in neural information processing systems, P862
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   Li Y, 2016, IEEE DATA MINING, P1015, DOI [10.1109/ICDM.2016.0129, 10.1109/ICDM.2016.133]
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu Wei, 2010, ASS ADV ARTIFICIAL I
   Luo Y, 2019, IEEE T PATTERN ANAL, V41, P1013, DOI 10.1109/TPAMI.2018.2824309
   Luo Y, 2014, IEEE T IMAGE PROCESS, V23, P3789, DOI 10.1109/TIP.2014.2332398
   Luo Yong, 2018, ARXIV181003944
   Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523
   Mignon A, 2012, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2012.6247987
   Negrel Romain, 2015, 26 BRIT MACH VIS C
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426
   Saul LK, 2004, J MACH LEARN RES, V4, P119, DOI 10.1162/153244304322972667
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shalev-Shwartz S., 2004, ICML, P94, DOI DOI 10.1145/1015330.1015376
   Tan M., 2018, Int. J. Antennas Propag., V2018, P1
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tian XM, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240139
   Tianyi Zhou, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P679, DOI 10.1109/ICDM.2010.135
   Wang ZC, 2017, LECT NOTES COMPUT SC, V10634, P586, DOI 10.1007/978-3-319-70087-8_61
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yang X, 2017, T GIS, V21, P1204, DOI 10.1111/tgis.12273
   Yu J, 2017, IEEE T CYBERNETICS, V47, P4014, DOI 10.1109/TCYB.2016.2591583
   Yu J, 2014, INFORM SCIENCES, V281, P674, DOI 10.1016/j.ins.2014.01.025
   Yu J, 2014, IEEE T CYBERNETICS, V44, P2431, DOI 10.1109/TCYB.2014.2307862
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P4636, DOI 10.1109/TIP.2012.2207395
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zwald L., 2005, Neural Inf.Process. Syst., P1649
NR 42
TC 3
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2019
VL 15
IS 3
AR 84
DI 10.1145/3340262
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ6YR
UT WOS:000494301200013
DA 2024-07-18
ER

PT J
AU Wang, S
   Guo, D
   Xu, X
   Zhuo, L
   Wang, M
AF Wang, Shuo
   Guo, Dan
   Xu, Xin
   Zhuo, Li
   Wang, Meng
TI Cross-Modality Retrieval by Joint Correlation Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modality retrieval; joint loss; auto-encoder; MLP
ID IMAGE RETRIEVAL
AB As an indispensable process of cross-media analyzing, comprehending heterogeneous data faces challenges in the fields of visual question answering (VQA), visual captioning, and cross-modality retrieval. Bridging the semantic gap between the two modalities is still difficult. In this article, to address the problem in cross-modality retrieval, we propose a cross-modal learning model with joint correlative calculation learning. First, an auto-encoder is used to embed the visual features by minimizing the error of feature reconstruction and a multi-layer perceptron (MLP) is utilized to model the textual features embedding. Then we design a joint loss function to optimize both the intra- and the inter-correlations among the image-sentence pairs, i.e., the reconstruction loss of visual features, the relevant similarity loss of paired samples, and the triplet relation loss between positive and negative examples. In the proposed method, we optimize the joint loss based on a batch score matrix and utilize all mutual mismatched paired samples to enhance its performance. Our experiments in the retrieval tasks demonstrate the effectiveness of the proposed method. It achieves comparable performance to the state-of-the-art on three benchmarks, i.e., Flickr8k, Flickr30k, and MS-COCO.
C1 [Wang, Shuo; Guo, Dan; Wang, Meng] Hefei Univ Technol, Sch Artificial Intelligence, Sch Comp Sci & Informat, Hefei 230601, Anhui, Peoples R China.
   [Xu, Xin] Wuhan Univ Sci & Technol, Coll Comp Sci & Technol, Wuhan, Hubei, Peoples R China.
   [Zhuo, Li] Beijing Univ Technol, Signal & Informat Proc Lab, Beijing, Peoples R China.
C3 Hefei University of Technology; Wuhan University of Science &
   Technology; Beijing University of Technology
RP Wang, S; Guo, D (corresponding author), Hefei Univ Technol, Sch Artificial Intelligence, Sch Comp Sci & Informat, Hefei 230601, Anhui, Peoples R China.
EM shuowang.hfut@gmail.com; guodan@hfut.edu.cn; xuxin0336@163.com;
   zhuoli@bjut.edu.cn; eric.mengwang@gmail.com
RI Wang, Meng/ITR-8699-2023; Xu, Xin/JRW-5800-2023
OI Xu, Xin/0000-0003-0748-3669
FU National Natural Science Foundation of China (NSFC) [61725203, 61732008,
   61876058]; National Key Research and Development Program of China
   [2018YFB0804200]
FX This work is supported by the National Natural Science Foundation of
   China (NSFC), under grants 61725203, 61732008, and 61876058, and the
   National Key Research and Development Program of China under grant
   2018YFB0804200.
CR [Anonymous], GLOBAL J RES ENG
   [Anonymous], 1998, ATMOS ENV, DOI DOI 10.1016/S1352-2310(97)00447-0
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, P INT C LEARN REPR I
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], IEEE T BIG DATA
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, 3 INT C LEARN REPR I
   [Anonymous], 2014, ARXIV14113409
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Bui T, 2017, COMPUT VIS IMAGE UND, V164, P27, DOI 10.1016/j.cviu.2017.06.007
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Frome A, 2007, IEEE I CONF COMP VIS, P94
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P5814, DOI 10.1109/TIP.2016.2614132
   Hong RC, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2043612.2043613
   Huang YF, 2017, IEEE INT CONF COMP V, P2313, DOI 10.1109/ICCVW.2017.273
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Klein Benjamin, 2014, ARXIV14117399
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Ledwich L., 2004, AUSTR C ROB AUT, V322, P3
   Lev G, 2016, LECT NOTES COMPUT SC, V9910, P833, DOI 10.1007/978-3-319-46466-4_50
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu F, 2018, PROC CVPR IEEE, P8611, DOI 10.1109/CVPR.2018.00898
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Ma Z, 2015, PR MACH LEARN RES, V37, P169
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Mishra A, 2013, IEEE I CONF COMP VIS, P3040, DOI 10.1109/ICCV.2013.378
   Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975
   Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Ren X, 2015, INT WORKS HIGH MOBIL, P21, DOI 10.1109/HMWC.2015.7353348
   Reynolds D, 2015, ENCY BIOMETRICS, P827, DOI [10.1007/978-0-387-73003-5%20196, DOI 10.1007/978-0-387-73003-5196, DOI 10.1007/978-1-4899-7488-4_196]
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Thompson B., 2000, READING UNDERSTANDIN, P285
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Xiao L, 2016, ADV SOC SCI EDUC HUM, V50, P261
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang X, 2018, IEEE T IMAGE PROCESS, V27, P791, DOI 10.1109/TIP.2017.2765836
NR 53
TC 13
Z9 13
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 56
DI 10.1145/3314577
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900009
DA 2024-07-18
ER

PT J
AU Jiang, SQ
   Chen, GW
   Song, XH
   Liu, LH
AF Jiang, Shuqiang
   Chen, Gongwei
   Song, Xinhang
   Liu, Linhu
TI Deep Patch Representations with Shared Codebook for Scene Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Scene classification; convolutional neural network; feature encoding;
   shared codebook
AB Scene classification is a challenging problem. Compared with object images, scene images are more abstract, as they are composed of objects. Object and scene images have different characteristics with different scales and composition structures. How to effectively integrate the local mid-level semantic representations including both object and scene concepts needs to be investigated, which is an important aspect for scene classification. In this article, the idea of a sharing codebook is introduced by organically integrating deep learning, concept feature, and local feature encoding techniques. More specifically, the shared local feature codebook is generated from the combined ImageNet1K and Places365 concepts (Mixed1365) using convolutional neural networks. As the Mixed1365 features cover all the semantic information including both object and scene concepts, we can extract a shared codebook from the Mixed1365 features, which only contain a subset of the whole 1,365 concepts with the same codebook size. The shared codebook can not only provide complementary representations without additional codebook training but also be adaptively extracted toward different scene classification tasks. A method of fusing the encoded features with both the original codebook and the shared codebook is proposed for scene classification. In this way, more comprehensive and representative image features can be generated for classification. Extensive experimentations conducted on two public datasets validate the effectiveness of the proposed method. Besides, some useful observations are also revealed to show the advantage of shared codebook.
C1 [Jiang, Shuqiang; Chen, Gongwei; Song, Xinhang; Liu, Linhu] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.
EM sqjiang@ict.ac.cn; gongwei.chen@vipl.ict.ac.cn;
   xinhang.song@vipl.ict.ac.cn; linhu.liu@vipl.ict.ac.cn
OI Chen, Gongwei/0000-0002-0634-6075
FU National Natural Science Foundation of China [61532018]; Lenovo
   Outstanding Young Scientists Program; National Program for Special
   Support of Eminent Professionals; National Program for Support of
   Top-notch Young Professionals; National Postdoctoral Program for
   Innovative Talents [BX201700255]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61532018, in part by the Lenovo
   Outstanding Young Scientists Program, in part by National Program for
   Special Support of Eminent Professionals and National Program for
   Support of Top-notch Young Professionals, in part by the National
   Postdoctoral Program for Innovative Talents under Grant BX201700255.
CR [Anonymous], 2013, International Conference on Machine Learning
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], 2014, P INT C MACH LEARN I
   [Anonymous], P ANN EUR C COMP VIS
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P ANN EUR C COMP VIS
   [Anonymous], P ANN C NEUR INF PRO
   [Anonymous], P INT C COMP VIS ICC
   [Anonymous], IEEE T CIRC SYST VID
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P ANN C NEUR INF PRO
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], P IEEE COMP VIS PATT
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], SEMANTIC TYPICALITY
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], 2015, 3 INT C LEARN REPR I
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P 2007 IEEE COMP SOC
   [Anonymous], 2013, NIPS
   [Anonymous], P ANN C NEUR INF PRO
   [Anonymous], P ANN C NEUR INF PRO
   [Anonymous], 2006, P IEEE COMP SOC C CO
   Bai X, 2016, IEEE T IMAGE PROCESS, V25, P2789, DOI 10.1109/TIP.2016.2555080
   Bergamo A, 2014, IEEE T PATTERN ANAL, V36, P1988, DOI 10.1109/TPAMI.2014.2313111
   Cao XC, 2015, IEEE T CYBERNETICS, V45, P1327, DOI 10.1109/TCYB.2014.2350517
   Cinbis RG, 2012, PROC CVPR IEEE, P2184, DOI 10.1109/CVPR.2012.6247926
   Dixit M. D., 2016, ADV NEURAL INFORM PR, P2811
   Herranz L, 2016, PROC CVPR IEEE, P571, DOI 10.1109/CVPR.2016.68
   Li LJ, 2014, INT J COMPUT VISION, V107, P20, DOI 10.1007/s11263-013-0660-x
   Li L, 2012, IEEE T MULTIMEDIA, V14, P1401, DOI 10.1109/TMM.2012.2194993
   Li ZT, 2018, IEEE T IMAGE PROCESS, V27, P4478, DOI 10.1109/TIP.2018.2839916
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Rasiwasia N, 2013, IEEE T PATTERN ANAL, V35, P2665, DOI 10.1109/TPAMI.2013.69
   Rasiwasia N, 2009, PROC CVPR IEEE, P1889, DOI 10.1109/CVPRW.2009.5206826
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song XH, 2017, IEEE T IMAGE PROCESS, V26, P2721, DOI 10.1109/TIP.2017.2686017
   Torralba A. B., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1253, DOI 10.1109/ICCV.1999.790424
   van Gemert Jan C., 2008, Computer Vision. Proceedings 10th European Conference on Computer Vision, ECCV 2008, P696, DOI 10.1007/978-3-540-88690-7_52
   Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1
   Wang S, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700292
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P3241, DOI 10.1109/TIP.2014.2328894
   Zhang W, 2016, IEEE T IMAGE PROCESS, V25, P4186, DOI 10.1109/TIP.2016.2590321
   Zhou BL, 2014, ADV NEUR IN, V27
NR 53
TC 19
Z9 19
U1 2
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 5
DI 10.1145/3231738
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100005
DA 2024-07-18
ER

PT J
AU Okada, S
   Nguyen, LS
   Aran, O
   Gatica-Perez, D
AF Okada, Shogo
   Nguyen, Laurent Son
   Aran, Oya
   Gatica-Perez, Daniel
TI Modeling Dyadic and Group Impressions with Intermodal and Interperson
   Features
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Impression; personality trait; multimodal interaction; inference
AB This article proposes a novel feature-extraction framework for inferring impression personality traits, emergent leadership skills, communicative competence, and hiring decisions. The proposed framework extracts multimodal features, describing each participant's nonverbal activities. It captures intermodal and interperson relationships in interactions and captures how the target interactor generates nonverbal behavior when other interactors also generate nonverbal behavior. The intermodal and interperson patterns are identified as frequent co-occurring events based on clustering from multimodal sequences. The proposed framework is applied to the SONVB corpus, which is an audiovisual dataset collected from dyadic job interviews, and the ELEA audiovisual data corpus, which is a dataset collected from group meetings. We evaluate the framework on a binary classification task involving 15 impression variables from the two data corpora. The experimental results show that the model trained with co-occurrence features is more accurate than previous models for 14 out of 15 traits.
C1 [Okada, Shogo] JAIST, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.
   [Okada, Shogo] RIKEN, Ctr Adv Intelligence Project AIP, Wako, Saitama, Japan.
   [Nguyen, Laurent Son; Gatica-Perez, Daniel] Idiap Res Inst, Martigny, Switzerland.
   [Aran, Oya] De La Salle Univ, Manila, Philippines.
   [Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Aran, Oya] Coll Comp Studies, 10th Floor,Br Andrew Gonzalez Hall,2401 Taft Ave, Manila 1004, Philippines.
C3 Japan Advanced Institute of Science & Technology (JAIST); RIKEN; De La
   Salle University; Swiss Federal Institutes of Technology Domain; Ecole
   Polytechnique Federale de Lausanne
RP Okada, S (corresponding author), JAIST, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.; Okada, S (corresponding author), RIKEN, Ctr Adv Intelligence Project AIP, Wako, Saitama, Japan.
EM okada-s@jaist.ac.jp; nguyen@idiap.ch; aran.oya@dlsu.edu.ph;
   gatica@idiap.ch
RI Aran, Oya/ABR-6400-2022
OI Aran, Oya/0000-0002-4679-9335; Okada, Shogo/0000-0002-9260-0403
FU Swiss National Science Foundation (SNSF) [CRSII2 147611]; Japan Society
   for the Promotion of Science (JSPS) KAK-ENHI [25730132, 15K00300,
   25280076]; SOBE Ambizione Fellowship [PZ00P2 136811]; Grants-in-Aid for
   Scientific Research [25730132, 25280076] Funding Source: KAKEN; Swiss
   National Science Foundation (SNF) [CRSII2_147611, PZ00P2_136811] Funding
   Source: Swiss National Science Foundation (SNF)
FX We appreciate the support of the Swiss National Science Foundation
   (SNSF), through the UBImpressed Sinergia project (CRSII2 147611) and the
   SOBE Ambizione Fellowship (PZ00P2 136811), and the Japan Society for the
   Promotion of Science (JSPS) KAK-ENHI (25730132, 15K00300, 25280076).
CR Alameda-Pineda X, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P5, DOI 10.1145/2733373.2806238
   Alameda-Pineda X, 2016, IEEE T PATTERN ANAL, V38, P1707, DOI 10.1109/TPAMI.2015.2496269
   ALLEN JF, 1983, COMMUN ACM, V26, P832, DOI 10.1145/182.358434
   [Anonymous], 2008, P ICMI 2008, DOI DOI 10.1145/1452392.1452404
   [Anonymous], 2014, P 16 INT C MULTIMODA
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2014, P ACM ICMI
   [Anonymous], 1998, HDB BRAIN THEORY NEU
   [Anonymous], P INT C AUGM COGN LA
   Aran O, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P11, DOI 10.1145/2522848.2522859
   Aran O, 2014, IEEE T MULTIMEDIA, V16, P201, DOI 10.1109/TMM.2013.2284893
   Aslam J, 1999, PROCEEDINGS OF THE TENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P51
   Avci U, 2016, IEEE T MULTIMEDIA, V18, P643, DOI 10.1109/TMM.2016.2521348
   Batrinca L, 2016, IEEE T MULTIMEDIA, V18, P659, DOI 10.1109/TMM.2016.2522763
   Beauchemin SS, 1995, ACM COMPUT SURV, V27, P433, DOI 10.1145/212094.212141
   Biel JI, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P53
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Breiman L., 2001, Mach. Learn., V45, P5
   Chatterjee M, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P7, DOI 10.1145/2818346.2820747
   Curhan JR, 2007, J APPL PSYCHOL, V92, P802, DOI 10.1037/0021-9010.92.3.802
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   Gosling SD, 2003, J RES PERS, V37, P504, DOI 10.1016/S0092-6566(03)00046-1
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hung Hayley, 2011, P 13 INT C MULT INT, P231
   Jayagopi D, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P433
   John O. P., 1999, BIG 5 TRAIT TAXONOMY
   Kumar V, 2015, IEEE I CONF COMP VIS, P1994, DOI 10.1109/ICCV.2015.231
   Nguyen L, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P289
   Nguyen LS, 2014, IEEE T MULTIMEDIA, V16, P1018, DOI 10.1109/TMM.2014.2307169
   Martinez H. P., 2011, P 13 INT C MULTIMODA, P3
   Miller C, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P101
   Naim Iftekhar, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163127
   Nakano YI, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P91, DOI 10.1145/2818346.2820764
   ODOBEZ JM, 1995, J VIS COMMUN IMAGE R, V6, P348, DOI 10.1006/jvci.1995.1029
   Okada S, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P15, DOI 10.1145/2818346.2820757
   Okada S, 2016, IEICE T INF SYST, VE99D, P1462, DOI 10.1587/transinf.2015CBP0003
   Park S, 2015, IEEE T AFFECT COMPUT, V6, P86, DOI 10.1109/TAFFC.2015.2396079
   Qian XM, 2017, IEEE T MULTIMEDIA, V19, P813, DOI 10.1109/TMM.2016.2638207
   Rienks R., 2006, Machine Learning for Multimodal Interaction. Second International Workshop, MLMI 2005. Revised Selected Papers (Lecture Notes in Computer Science Vol. 3869), P76
   Sanchez-Cortes D, 2013, J MULTIMODAL USER IN, V7, P39, DOI 10.1007/s12193-012-0101-0
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Scherer S, 2013, IEEE INT CONF AUTOMA
   Song Y, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P27
   Teijeiro-Mosquera L, 2015, IEEE T AFFECT COMPUT, V6, P193, DOI 10.1109/TAFFC.2014.2370044
   Vahdatpour A, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1261
   Vinciarelli A., 2009, 2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops, P1
   Vinciarelli A, 2007, IEEE T MULTIMEDIA, V9, P1215, DOI 10.1109/TMM.2007.902882
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Yang JB, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3995
   Yang XY, 2015, IEEE T IMAGE PROCESS, V24, P1709, DOI 10.1109/TIP.2015.2411433
   Zancanaro M., 2006, Proceedings of the 8th international conference on Multimodal interfaces, P28
   Zhang SL, 2015, IEEE T PATTERN ANAL, V37, P2573, DOI 10.1109/TPAMI.2015.2417573
   Zhang SL, 2011, IEEE T IMAGE PROCESS, V20, P2664, DOI 10.1109/TIP.2011.2128333
   Zhao GS, 2016, IEEE T KNOWL DATA EN, V28, P3382, DOI 10.1109/TKDE.2016.2607172
NR 55
TC 14
Z9 15
U1 1
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 13
DI 10.1145/3265754
PG 30
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100013
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Duan, MX
   Li, KL
   Liao, XK
   Li, KQ
   Tian, Q
AF Duan, Mingxing
   Li, Kenli
   Liao, Xiangke
   Li, Keqin
   Tian, Qi
TI Features-Enhanced Multi-Attribute Estimation with Convolutional Tensor
   Correlation Fusion Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Attribute estimation; correlation; subnetwork; score fusion
ID CANONICAL CORRELATION-ANALYSIS; GENDER ESTIMATION; AGE ESTIMATION; FACE;
   RECOGNITION
AB To achieve robust facial attribute estimation, a hierarchical prediction system referred to as tensor correlation fusion network (TCFN) is proposed for attribute estimation. The system includes feature extraction, correlation excavation among facial attribute features, score fusion, and multi-attribute prediction. Subnetworks (Age-Net, Gender-Net, Race-Net, and Smile-Net) are used to extract corresponding features while Main-Net extracts features not only from an input image but also from corresponding pooling layers of subnetworks. Dynamic tensor canonical correlation analysis (DTCCA) is proposed to explore the correlation of different targets' features in the F7 layers. Then, for binary classifications of gender, race, and smile, corresponding robust decisions are achieved by fusing the results of subnetworks with those of TCFN while for age prediction, facial image into one of age groups, and then ELM regressor performs the final age estimation. Experimental results on benchmarks with multiple face attributes (MORPH-II, Adience Benchmark datasets, LAP-2016, and CelebA) show that the proposed approach has superior performance compared to state of the art.
C1 [Duan, Mingxing; Li, Kenli] Hunan Univ, Sch Informat Sci & Engn, Lushang Rd, Changsha 410000, Hunan, Peoples R China.
   [Liao, Xiangke] Natl Univ Def Technol, Sch Comp, 109 Deya Rd, Changsha 410000, Hunan, Peoples R China.
   [Li, Keqin] SUNY Coll New Paltz, Comp Sci, New Paltz, NY 12561 USA.
   [Tian, Qi] Univ Texas San Antonio, Comp Sci, San Antonio, TX 78249 USA.
C3 Hunan University; National University of Defense Technology - China;
   State University of New York (SUNY) System; SUNY New Paltz; University
   of Texas System; University of Texas at San Antonio (UTSA)
RP Duan, MX (corresponding author), Hunan Univ, Sch Informat Sci & Engn, Lushang Rd, Changsha 410000, Hunan, Peoples R China.
EM duanmingxing16@nudt.edu.cn; lkl@hnu.edu.cn; xkliao@nudt.edu.cn;
   lik@newpaltz.edu; qitian@cs.utsa.edu
RI Duan, Mingxing/AAF-8792-2019
OI Mingxing, Duan/0000-0003-2281-0672; Xiao, Guoqing/0000-0001-5008-4829
FU National Outstanding Youth Science Program of National Natural Science
   Foundation of China [61625202]; International (Regional) Cooperation and
   Exchange Program of National Natural Science Foundation of China
   [61661146006]; National Key R&D Program of China [2016YT80201900];
   National Youth Science Program of National Natural Science Foundation of
   China [6190050362]; China Postdoctoral Science Foundation
FX This work was supported in part by the National Outstanding Youth
   Science Program of National Natural Science Foundation of China under
   Grant no. 61625202, in part by the International (Regional) Cooperation
   and Exchange Program of National Natural Science Foundation of China
   under Grant no. 61661146006, in part by the National Key R&D Program of
   China under Grant no. 2016YT80201900, in part by the National Youth
   Science Program of National Natural Science Foundation of China under
   Grant no. 6190050362, and in part by the Project funded by China
   Postdoctoral Science Foundation. Authors' addresses: M. Duan and K. Li,
   Hunan University, School of Information Science and Engineering, Lushang
   Rd., Changsha, Hunan 410000, China; emails:
   duanmingxing16@nudt.edu.cn,lkl@hnu.edu.cn;X.Liao, National University of
   Defense Technology, School of Computing, 109 Deya Rd., Changsha, Hunan
   410000, China; email: xkliao@nudt.edu.cn;K.Li, State University of New
   York, Computer Science, New Paltz, New York 12561; email:
   lik@newpaltz.edu;Q.Tian, University of Texas at San Antonio, Computer
   Science, San Antonio, Texas 78249; email: qitian@cs.utsa.edu.
CR Abadi M, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2018, IJCAI 2018
   [Anonymous], 2016, IEEE COMPUT SOC CONF, DOI DOI 10.1109/CVPRW.2016.96
   [Anonymous], INT J COMPUTER VISIO
   [Anonymous], P 4 EUR C 3D OBJ RET
   [Anonymous], 2018, SECUR COMMUN NETW
   [Anonymous], 2014, P AS C COMP VIS
   [Anonymous], ARXIV180402810
   [Anonymous], 1990, P ADV NEUR INF PROC
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2016, P IEEE C COMP VIS PA
   [Anonymous], 2001, J Appl Sci Eng, DOI DOI 10.6180/JASE.2001.4.3.05
   [Anonymous], P IEEE C COMP VIS PA
   Berretti S, 2006, INT C PATT RECOG, P19
   Dantcheva A, 2017, IEEE T INF FOREN SEC, V12, P719, DOI 10.1109/TIFS.2016.2632070
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995
   Dibeklioglu H, 2015, IEEE T IMAGE PROCESS, V24, P1928, DOI 10.1109/TIP.2015.2412377
   Duan MX, 2018, NEUROCOMPUTING, V275, P448, DOI 10.1016/j.neucom.2017.08.062
   Duan MX, 2018, IEEE T INF FOREN SEC, V13, P758, DOI 10.1109/TIFS.2017.2766583
   Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646
   Escalera S, 2016, IEEE COMPUT SOC CONF, P706, DOI 10.1109/CVPRW.2016.93
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Gunay Asuman, 2008, P INT S COMPUTER INF, P1
   Guo GD, 2014, IMAGE VISION COMPUT, V32, P761, DOI 10.1016/j.imavis.2014.04.011
   Guo J, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL V, P71
   Gürpinar F, 2016, IEEE COMPUT SOC CONF, P785, DOI 10.1109/CVPRW.2016.103
   Hajizadeh M.A., 2011, 2011 7th IEEE Iranian Machine Vision and Image Processing (MVIP), P1
   Han H, 2018, IEEE T PATTERN ANAL, V40, P2597, DOI 10.1109/TPAMI.2017.2738004
   Hand EM, 2017, AAAI CONF ARTIF INTE, P4068
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hu ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3087, DOI 10.1109/TIP.2016.2633868
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Jing XY, 2017, IEEE T IMAGE PROCESS, V26, P1363, DOI 10.1109/TIP.2017.2651364
   Jing XY, 2004, IEEE T SYST MAN CY B, V34, P2405, DOI 10.1109/TSMCB.2004.837586
   Kumar N, 2008, LECT NOTES COMPUT SC, V5305, P340, DOI 10.1007/978-3-540-88693-8_25
   Kwon YH, 1999, COMPUT VIS IMAGE UND, V74, P1, DOI 10.1006/cviu.1997.0549
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Liu KH, 2015, IEEE T INF FOREN SEC, V10, P2408, DOI 10.1109/TIFS.2015.2462732
   Liu X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P258, DOI 10.1109/ICCVW.2015.42
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Luo P, 2013, IEEE I CONF COMP VIS, P2864, DOI 10.1109/ICCV.2013.356
   Luo Y, 2015, IEEE T KNOWL DATA EN, V27, P3111, DOI 10.1109/TKDE.2015.2445757
   Malli RC, 2016, IEEE COMPUT SOC CONF, P714, DOI 10.1109/CVPRW.2016.94
   Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47
   Meng H., 2013, P 3 ACM INT WORKSH A, P21, DOI [10.1145/2512530.2512532., DOI 10.1145/2512530.2512532]
   Polikar R., 2006, IEEE Circuits and Systems Magazine, V6, P21, DOI 10.1109/MCAS.2006.1688199
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Ranjan R., 2016, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Ranjan R, 2017, IEEE INT CONF AUTOMA, P17, DOI 10.1109/FG.2017.137
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Simonyan K., 2014, 14091556 ARXIV
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Thukral P, 2012, INT CONF ACOUST SPEE, P1529, DOI 10.1109/ICASSP.2012.6288182
   Wu Z, 2011, IEEE T PATTERN ANAL, V33, P1991, DOI 10.1109/TPAMI.2011.111
   Zhang K, 2018, INT C PATT RECOG, P788, DOI 10.1109/ICPR.2018.8545333
   Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212
   Zhang ZP, 2018, INT J COMPUT VISION, V126, P550, DOI 10.1007/s11263-017-1055-1
   Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286
   Zhong Y, 2016, BLOOMS SHINTO STUD, P1
   Zhu XK, 2018, IEEE T IMAGE PROCESS, V27, P5683, DOI 10.1109/TIP.2018.2861366
NR 63
TC 20
Z9 20
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2019
VL 15
IS 3
SU S
SI SI
AR 91
DI 10.1145/3355542
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LR5EU
UT WOS:000535718800007
DA 2024-07-18
ER

PT J
AU Liu, ZW
   Zhu, XY
   Tang, M
   Lei, Z
   Wang, JQ
AF Liu, Zhiwei
   Zhu, Xiangyu
   Tang, Ming
   Lei, Zhen
   Wang, Jinqiao
TI Efficient Face Alignment with Fast Normalization and Contour Fitting
   Loss
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face alignment; convolutional neural networks; real-time; semantic
   meaning
ID NETWORK; SHAPE
AB Face alignment is a key component of numerous face analysis tasks. In recent years, most existing methods have focused on designing high-performance face alignment systems and paid less attention to efficiency. However more face alignment systems are now applied on low-cost devices, such as mobile phones. In this article, we design a common efficient framework that can team with any face alignment regression network and improve the overall performance with nearly no extra computational cost. First, we discover that the maximum regression error exists in the face contour, where landmarks do not have distinct semantic positions, and thus are randomly labeled along the face contours in training data. To address this problem, we propose a novel contour fitting loss that dynamically adjusts the regression target during training so the network can learn more accurate semantic meanings of the contour landmarks and achieve better localization performance. Second, we decouple the complex sample variations in face alignment task and propose a Fast Normalization Module (FNM) to efficiently normalize considerable variations that can be described by geometric transformation. Finally, a new lightweight network architecture named Lightweight Alignment Module (LAM) is also proposed to achieve fast and precise face alignment on mobile devices. Our method achieves competitive performance with state-of-the-arts on 300W and AFLW2000-3D benchmarks. Meanwhile, the speed of our framework is significantly faster than other CNN-based approaches.
C1 [Liu, Zhiwei; Zhu, Xiangyu; Tang, Ming; Lei, Zhen; Wang, Jinqiao] Univ Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, CAS, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Wang, JQ (corresponding author), Univ Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, CAS, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
EM zhiwei.liu@nlpr.ia.ac.cn; xiangyu.zhu@nlpr.ia.ac.cn;
   tangm@nlpr.ia.ac.cn; zlei@nlpr.ia.ac.cn; jqwang@nlpr.ia.ac.cn
RI Liu, Zhiwei/AAN-8965-2021
FU Natural Science Foundation of China [61772527, 61806200, 61876178,
   61806196]
FX This work is supported by the Natural Science Foundation of China (Grant
   nos. 61772527, 61806200, 61876178, 61806196).
CR Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   [Anonymous], 2016, ARXIV160202830
   [Anonymous], 2005, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2005.145
   [Anonymous], P 2 INT C AUD VID BI
   [Anonymous], 1995, COMPUTER VISION IMAG
   Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bhagavatula C, 2017, IEEE I CONF COMP VIS, P4000, DOI 10.1109/ICCV.2017.429
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Deng JK, 2019, IEEE T IMAGE PROCESS, V28, P3636, DOI 10.1109/TIP.2019.2899267
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Dong XY, 2018, PROC CVPR IEEE, P360, DOI 10.1109/CVPR.2018.00045
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291
   Jourabloo A, 2017, IEEE I CONF COMP VIS, P3219, DOI 10.1109/ICCV.2017.347
   KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI [DOI 10.1109/CVPR.2014.241, 10.1109/CVPR.2014.241]
   Koestinger M., 2011, IEEE INT C COMP VIS, P2144, DOI DOI 10.1109/ICCVW.2011.6130513
   Kumar A, 2018, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2018.00052
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Liu YJ, 2017, IEEE INT CONF COMP V, P1619, DOI 10.1109/ICCVW.2017.190
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Ren SQ, 2016, IEEE T IMAGE PROCESS, V25, P1233, DOI 10.1109/TIP.2016.2518867
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Valle R, 2018, LECT NOTES COMPUT SC, V11218, P609, DOI 10.1007/978-3-030-01264-9_36
   Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227
   Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P52, DOI 10.1007/978-3-319-46454-1_4
   Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zhu XY, 2019, IEEE T PATTERN ANAL, V41, P78, DOI 10.1109/TPAMI.2017.2778152
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 49
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2019
VL 15
IS 3
SU S
SI SI
AR 89
DI 10.1145/3338842
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LR5EU
UT WOS:000535718800005
DA 2024-07-18
ER

PT J
AU Sun, WW
   Zhou, JT
   Zhu, SY
   Tang, YY
AF Sun, Weiwei
   Zhou, Jiantao
   Zhu, Shuyuan
   Tang, Yuan Yan
TI Robust Privacy-Preserving Image Sharing over Online Social Networks
   (OSNs)
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Online social networks; image sharing; privacy-preserving; JPEG
ID COMPRESSION
AB Sharing images online has become extremely easy and popular due to the ever-increasing adoption of mobile devices and online social networks (OSNs). The privacy issues arising from image sharing over OSNs have received significant attention in recent years. In this article, we consider the problem of designing a secure, robust, high-fidelity, storage-efficient image-sharing scheme over Facebook, a representative OSN that is widely accessed. To accomplish this goal, we first conduct an in-depth investigation on the manipulations that Facebook performs to the uploaded images. Assisted by such knowledge, we propose a DCT-domain image encryption/decryption framework that is robust against these lossy operations. As verified theoretically and experimentally, superior performance in terms of data privacy, quality of the reconstructed images, and storage cost can be achieved.
C1 [Sun, Weiwei; Zhou, Jiantao; Tang, Yuan Yan] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
   [Zhu, Shuyuan] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu, Sichuan, Peoples R China.
C3 University of Macau; University of Electronic Science & Technology of
   China
RP Zhou, JT (corresponding author), Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
EM Yb57457@umac.mo; jtzhou@umac.mo; eezsy@uestc.edu.cn; yytang@umac.mo
FU Macau Science and Technology Development Fund [FDCT/046/2014/A1,
   FDCT/022/2017/A1]; Research Committee at the University of Macau
   [MYRG2015-00056-FST, MYRG2016-00137-FST]; National Science Foundation of
   China [61402547, 61672134]
FX This work was supported in part by the Macau Science and Technology
   Development Fund under Grant No. FDCT/046/2014/A1 and Grant No.
   FDCT/022/2017/A1, in part by the Research Committee at the University of
   Macau under Grant No. MYRG2015-00056-FST and Grant No.
   MYRG2016-00137-FST, and in part by the National Science Foundation of
   China under Grant No. 61402547 and Grant No. 61672134.
CR Al Boum B, 2011, SECRYPT 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SECURITY AND CRYPTOGRAPHY, P345
   Al Bouna B, 2013, LECT NOTES SOC NETW, P337, DOI 10.1007/978-3-7091-0894-9_11
   [Anonymous], 2012, P 8 ACM S US PRIV SE
   [Anonymous], P 4 S US PRIV SEC SO
   [Anonymous], ARXIV14106589
   [Anonymous], ARXIV11122649
   [Anonymous], 2017, TOP 20 VAL FAC STAT
   [Anonymous], 2016, 2016 IEEE Symposium on VLSI Circuits (VLSI-Circuits), DOI 10.1109/VLSIC.2016.7573558
   [Anonymous], P APPL DIG IM PROC 3
   [Anonymous], 2012, P 5 WORKSHOP SOCIAL
   [Anonymous], 2011, P HT 2011
   Beato F, 2011, LECT NOTES COMPUT SC, V6794, P211, DOI 10.1007/978-3-642-22263-4_12
   Choi K., 2008, Proc. IEEE Conference on Automatic Face and Gesture Recognition, P1
   Dirik AE, 2013, COMM COM INF SC, V368, P71
   Gunther F., 2011, Proc. IEEE Int. Symp.World of Wireless, P1
   Gurses Seda, 2014, P W3C WORKSH PRIV US
   He ZB, 2016, PERS UBIQUIT COMPUT, V20, P833, DOI 10.1007/s00779-016-0952-6
   Honda T, 2013, MIDWEST SYMP CIRCUIT, P1371, DOI 10.1109/MWSCAS.2013.6674911
   Ilia P, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P781, DOI 10.1145/2810103.2813603
   Jianping He, 2016, 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). Proceedings, P359, DOI 10.1109/DSN.2016.40
   Klemperer P., 2012, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P377
   Tran L, 2016, AAAI CONF ARTIF INTE, P1317
   Li Zhang, 2011, 2011 International Conference on Multimedia and Signal Processing (CMSP), P312, DOI 10.1109/CMSP.2011.69
   Liang KT, 2015, IEEE INTERNET COMPUT, V19, P58, DOI 10.1109/MIC.2014.107
   Miller AD, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P347
   Ning JX, 2014, IEEE CONF COMM NETW, P319, DOI 10.1109/CNS.2014.6997500
   Pallas F., 2014, CHI'14 Extended Abstracts on Human Factors in Computing Systems, CHI EA'14, P2179, DOI [DOI 10.1145/2559206.2581195, 10.1145/2559206]
   Polakis I, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P501, DOI 10.1145/2660267.2660317
   Qiudong Sun, 2012, 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery, P1669, DOI 10.1109/FSKD.2012.6233963
   Ra M.-R., 2013, P USENIX S NETW SYST, P515
   Rosenblum D, 2007, IEEE SECUR PRIV, V5, P40, DOI 10.1109/MSP.2007.75
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Stone Z, 2010, P IEEE, V98, P1408, DOI 10.1109/JPROC.2010.2044551
   Stone Zak, 2008, P IEEE INT C COMPUTE, P1
   Sun W., 2016, P 24 ACM INT C MULT, P581
   Tierney M., 2013, Proceedings of the first ACM conference on Online social networks, P75
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wu CP, 2005, IEEE T MULTIMEDIA, V7, P828, DOI 10.1109/TMM.2005.854469
   Xu KH, 2014, IEEE GLOB COMM CONF, P704, DOI 10.1109/GLOCOM.2014.7036890
   Ya-Nan Liu, 2016, International Journal of Security and Networks, V11, P12
   Yuan L, 2015, IEEE INT CONF AUTOMA
   Yuan L, 2015, IEEE CONF COMPUT, P185, DOI 10.1109/INFCOMW.2015.7179382
   Zhang C, 2010, IEEE NETWORK, V24, P13, DOI 10.1109/MNET.2010.5510913
   Zhou JT, 2014, IEEE T INF FOREN SEC, V9, P39, DOI 10.1109/TIFS.2013.2291625
NR 44
TC 18
Z9 22
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2018
VL 14
IS 1
AR 14
DI 10.1145/3165265
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW9HO
UT WOS:000425646500014
DA 2024-07-18
ER

PT J
AU Cheung, M
   She, J
AF Cheung, Ming
   She, James
TI An Analytic System for User Gender Identification through User Shared
   Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Big data; mobile; user shared images; gender; recommendation; social
   network analysis
AB Many social media applications, such as recommendation, virality prediction, and marketing, make use of user gender, which may not be explicitly specified or kept privately. Meanwhile, advanced mobile devices have become part of our lives and a huge amount of content is being generated by users every day, especially user shared images shared by individuals in social networks. This particular form of user generated content is widely accessible to others due to the sharing nature. When user gender is only accessible to exclusive parties, these user shared images are proved to be an easier way to identify user gender. This work investigated 3,152,344 images by 7,450 users from Fotolog and Flickr, two image-oriented social networks. It is observed that users who share visually similar images are more likely to have the same gender. A multimedia big data system that utilizes this phenomenon is proposed for user gender identification with 79% accuracy. These findings are useful for information or services in any social network with intensive image sharing.
C1 [Cheung, Ming; She, James] HKUST NIE Social Media Lab, Hong Kong, Hong Kong, Peoples R China.
   [Cheung, Ming; She, James] Hong Kong Univ Sci & Technol, Elect & Comp Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Cheung, M (corresponding author), HKUST NIE Social Media Lab, Hong Kong, Hong Kong, Peoples R China.; Cheung, M (corresponding author), Hong Kong Univ Sci & Technol, Elect & Comp Engn, Hong Kong, Hong Kong, Peoples R China.
EM cpming@connect.ust.hk; eejames@ust.hk
FU HKUST-NIE Social Media Lab
FX This work is supported by the HKUST-NIE Social Media Lab.
CR Alowibdi Jalal S., 2013, 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), P739
   Alowibdi JS, 2013, 2013 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2013), VOL 1, P365, DOI 10.1109/ICMLA.2013.74
   [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], 2007, SOC IND APPL MATH
   [Anonymous], 2011, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing
   [Anonymous], 2014, ABS14053531 CORR
   Argamon S, 2009, COMMUN ACM, V52, P119, DOI 10.1145/1461928.1461959
   Cheung M., 2014, P 4 INT C ADV INF MI, P83
   Cheung M, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978568
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Cheung Ming, 2015, P 2015 IEEE ACM INT
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Estabrooks A, 2004, COMPUT INTELL-US, V20, P18, DOI 10.1111/j.0824-7935.2004.t01-1-00228.x
   Goswami S., 2009, P INT C WEB SOCIAL M
   Hum NJ, 2011, COMPUT HUM BEHAV, V27, P1828, DOI 10.1016/j.chb.2011.04.003
   Jie Zhanming, 2015, P 4 IEEE S NETW CLOU
   Jurie F, 2005, IEEE I CONF COMP VIS, P604
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Kapoor A, 2007, IEEE I CONF COMP VIS, P134
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   Li XH, 2008, IEEE T SIGNAL PROCES, V56, P675, DOI 10.1109/TSP.2007.907820
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   Liu Wendy, 2013, P AAAI SPRING S AN M
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   McCallum A., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P169, DOI 10.1145/347090.347123
   Meyerson A, 2001, ANN IEEE SYMP FOUND, P426, DOI 10.1109/SFCS.2001.959917
   Moxley E, 2009, IEEE INT CON MULTI, P1452, DOI 10.1109/ICME.2009.5202776
   Mukherjee A., 2010, Proceedings of the 2010 conference on Empirical Methods in natural Language Processing, P207, DOI DOI 10.5555/1870658.1870679
   Muscanell NL, 2012, COMPUT HUM BEHAV, V28, P107, DOI 10.1016/j.chb.2011.08.016
   Peersman Claudia, 2011, P 3 INT WORKSH SEARC, P37, DOI DOI 10.1145/2065023.2065035
   Rao D., 2010, P 2 INT WORKSHOP SEA, P37, DOI DOI 10.1145/1871985.1871993
   Rose J, 2012, COMMUN Q, V60, P588, DOI 10.1080/01463373.2012.725005
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Schwartz HA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073791
   Shepitsen A, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P259
   Strano M.M., 2008, Cyberpsychology: Journal of Psychosocial Research on Cyberspace, V2, P5
   Thompson S., 2012, COLL STUD J, V46, P88
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   You QZ, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOP (ICDMW), P1026, DOI 10.1109/ICDMW.2014.93
   Zhang X, 2012, INT J SPECTROSC, V8, P1, DOI DOI 10.1371/J0URNAL.P0NE.0052514
   Zhang XM, 2013, SIGNAL PROCESS, V93, P2178, DOI 10.1016/j.sigpro.2012.05.021
   Zhou TomChao., 2010, AAAI
NR 42
TC 4
Z9 4
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
AR 30
DI 10.1145/3095077
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD5TE
UT WOS:000407591900008
DA 2024-07-18
ER

PT J
AU Singh, P
   Raman, B
   Agarwal, N
   Atrey, PK
AF Singh, Priyanka
   Raman, Balasubramanian
   Agarwal, Nishant
   Atrey, Pradeep K.
TI Secure Cloud-Based Image Tampering Detection and Localization Using POB
   Number System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Permutation ordered binary (POB) number system; secret sharing;
   encrypted domain
AB The benefits of high-end computation infrastructure facilities provided by cloud-based multimedia systems are attracting people all around the globe. However, such cloud-based systems possess security issues as third party servers become involved in them. Rendering data in an unreadable form so that no information is revealed to the cloud data centers will serve as the best solution to these security issues. One such image encryption scheme based on a Permutation Ordered Binary Number System has been proposed in this work. It distributes the image information in totally random shares, which can be stored at the cloud data centers. Further, the proposed scheme authenticates the shares at the pixel level. If any tampering is done at the cloud servers, the scheme can accurately identify the altered pixels via authentication bits and localizes the tampered area. The tampered portion is also reflected back in the reconstructed image that is obtained at the authentic user end. The experimental results validate the efficacy of the proposed scheme against various kinds of possible attacks, tested with a variety of images. The tamper detection accuracy has been computed on a pixel basis and found to be satisfactorily high for most of the tampering scenarios.
C1 [Singh, Priyanka; Atrey, Pradeep K.] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA.
   [Raman, Balasubramanian] Indian Inst Technol Roorkee, Dept Comp Sci & Engn, Roorkee, Uttarakhand, India.
   [Agarwal, Nishant] Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra, Haryana, India.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Albany; Indian Institute of Technology System (IIT System);
   Indian Institute of Technology (IIT) - Roorkee; National Institute of
   Technology (NIT System); National Institute of Technology Kurukshetra
RP Singh, P (corresponding author), SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA.
EM psingh9@albany.edu; balarfma@iitr.ac.in; nishant_1130495@nitkkr.ac.in;
   patrey@albany.edu
RI Singh, Priyanka/N-1372-2018; Singh, Priyanka/GRF-6098-2022; singh,
   priyanka/JWP-2636-2024
OI Singh, Priyanka/0000-0003-0841-1544; Singh,
   Priyanka/0000-0001-7874-7778; SINGH, PRIYANKA/0000-0001-5002-8800
CR Ali M, 2015, INFORM SCIENCES, V305, P357, DOI 10.1016/j.ins.2015.01.025
   [Anonymous], S CONT SEC DAT HID D
   ASMUTH C, 1983, IEEE T INFORM THEORY, V29, P208, DOI 10.1109/TIT.1983.1056651
   Bhatnagar G, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2542205.2542207
   Binu V. P., 2014, ABS14073609 CORR
   Blakley G. R., 1979, 1979 International Workshop on Managing Requirements Knowledge (MARK), P313, DOI 10.1109/MARK.1979.8817296
   Botta M, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2568224
   Chiba Zouhair., 2016, ENG MIS ICEMIS INT C, P1
   Dan Bogdanov, 2007, FDN PROPERTIES SHAMI
   Deepika MP., 2016, INT J APPL ENG RES, V11, P2049
   Deng RH, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1596990.1596992
   Fridrich J, 2002, J ELECTRON IMAGING, V11, P262, DOI 10.1117/1.1459449
   Hefeeda M, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1671954.1671960
   Lathey A, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2656205
   Lin PY, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000489
   MIGNOTTE M, 1983, LECT NOTES COMPUT SC, V149, P371
   Mohammadi M., 2013, Iranian Conference on Electrical Engineering (ICEE), P1, DOI DOI 10.1109/PESMG.2013.6672514
   Mohanty M, 2016, MULTIMED TOOLS APPL, V75, P6207, DOI 10.1007/s11042-015-2567-8
   Naskar R, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487272
   Popovic Kresimir, 2010, 2010 33rd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), P344
   Rawat S, 2011, AEU-INT J ELECTRON C, V65, P840, DOI 10.1016/j.aeue.2011.01.016
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Shivani S., 2011, P 2011 INT C COMM CO, P221
   Singh P., 2015, MULTIMED TOOLS APPL, P1
   So Kuyoro, 2011, INT J COMPUT NETWORK, V3
   Sreekumar A., 2009, Hack, V2009, P33
   Zissis D, 2012, FUTURE GENER COMP SY, V28, P583, DOI 10.1016/j.future.2010.12.006
NR 29
TC 19
Z9 20
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
AR 23
DI 10.1145/3077140
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD5TE
UT WOS:000407591900001
DA 2024-07-18
ER

PT J
AU Silva, BMC
   Rodrigues, JJPC
   Kumar, N
   Proença, ML
   Han, GJ
AF Silva, Bruno M. C.
   Rodrigues, Joel J. P. C.
   Kumar, Neeraj
   Proenca, Mario L., Jr.
   Han, Guangjie
TI MobiCoop: An Incentive-Based Cooperation Solution for Mobile
   Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Mobile computing; mobile applications; cooperation; incentive-based
   cooperation; reputation-based cooperation
ID WIRELESS; GAME; LOCALIZATION; STRATEGIES; ENERGY; NETWORK; DEVICES;
   SYSTEM
AB Network architectures based on mobile devices and wireless communications present several constraints (e.g., processor, energy storage, bandwidth, etc.) that affect the overall network performance. Cooperation strategies have been considered as a solution to address these network limitations. In the presence of unstable network infrastructures, mobile nodes cooperate with each other, forwarding data and performing other specific network functionalities. This article proposes a generalized incentive-based cooperation solution for mobile services and applications called MobiCoop. This reputation-based scheme includes an application framework for mobile applications that uses a Web service to handle all the nodes reputation and network permissions. The main goal of MobiCoop is to provide Internet services to mobile devices without network connectivity through cooperation with neighbor devices. The article includes a performance evaluation study of MobiCoop considering both a real scenario (using a prototype) and a simulation-based study. Results show that the proposed approach provides network connectivity independency to users with mobile apps when Internet connectivity is unavailable. Then, it is concluded that MobiCoop improved significantly the overall system performance and the service provided for a given mobile application.
C1 [Silva, Bruno M. C.; Rodrigues, Joel J. P. C.] Univ Beira Interior, Inst Telecomunicacoes, Rua Marques dAvila e Bolama, P-6201001 Covilha, Portugal.
   [Rodrigues, Joel J. P. C.] Univ Fortaleza UNIFOR, Fortaleza, Ceara, Brazil.
   [Rodrigues, Joel J. P. C.] Univ ITMO, St Petersburg, Russia.
   [Kumar, Neeraj] Thapar Univ, Dept Comp Sci & Engn, Patiala, Punjab, India.
   [Proenca, Mario L., Jr.] State Univ Londrina UEL, Dept Comp Sci, Londrina, Brazil.
   [Han, Guangjie] Hohai Univ, Dept Informat & Commun Syst, Changzhou, Peoples R China.
C3 Universidade da Beira Interior; Universidade Fortaleza; ITMO University;
   Thapar Institute of Engineering & Technology; Universidade Estadual de
   Londrina; Hohai University
RP Silva, BMC (corresponding author), Univ Beira Interior, Inst Telecomunicacoes, Rua Marques dAvila e Bolama, P-6201001 Covilha, Portugal.
EM bruno.silva@it.ubi.pt; joeljr@ieee.org; neeraj.kumar@thapar.edu;
   proenca@uel.br; hanguangjie@gmail.com
RI Proença, Mario Lemes/B-8340-2016; Silva, Bruno/U-9003-2019; Kumar,
   Neeraj/L-3500-2016; Rodrigues, Joel J. P. C./A-8103-2013; Han,
   Guangjie/AAE-1496-2019; Kumar, Neeraj/J-4123-2017; Silva, Bruno
   M.C./E-9113-2015
OI Proença, Mario Lemes/0000-0002-0492-322X; Silva,
   Bruno/0000-0002-5939-8370; Kumar, Neeraj/0000-0002-3020-3947; Rodrigues,
   Joel J. P. C./0000-0001-8657-3800; Han, Guangjie/0000-0002-6921-7369;
   Silva, Bruno M.C./0000-0002-5939-8370
FU Instituto de Telecomunicacoes; Next Generation Networks and Applications
   Group (NetGNA); Covilha Delegation; Government of Russian Federation
   [074-U01]; National Funding from the FCT-Fundacao para a Ciencia e a
   Tecnologia [UID/EEA/500008/2013]
FX This work has been partially supported by Instituto de Telecomunicacoes,
   Next Generation Networks and Applications Group (NetGNA), Covilha
   Delegation, by Government of Russian Federation, Grant No. 074-U01, and
   by National Funding from the FCT-Fundacao para a Ciencia e a Tecnologia
   through the UID/EEA/500008/2013 Project.
CR Abdellatif M, 2013, IEEE ICC, P4425, DOI 10.1109/ICC.2013.6655263
   Al-Kanj Lina, 2010, 2010 17th International Conference on Telecommunications (ICT 2010), P471, DOI 10.1109/ICTEL.2010.5478815
   Aldini A., 2014, User-Centric Networking
   [Anonymous], 2013, 2013 FUTURE NETWORK
   [Anonymous], 2001, ACM SIGMOBILE MOBILE
   [Anonymous], P IEEE 19 SCVT BEN N
   Antonopoulos A, 2014, IEEE T WIREL COMMUN, V13, P592, DOI 10.1109/TWC.2013.120713.120790
   Antonopoulos A, 2012, IEEE GLOB COMM CONF, P3043, DOI 10.1109/GLOCOM.2012.6503581
   Bansal S., 2003, OBSERVATION BASED CO
   Buchegger S., 2002, P 3 ACM INT S MOB AD, P226, DOI DOI 10.1145/513800.513828
   Buttyán L, 2003, MOBILE NETW APPL, V8, P579, DOI 10.1023/A:1025146013151
   Buttyan L., 2001, TECHNICAL REPORT
   Cerf V., 2007, Delay-Tolerant Networking Architecture
   Charilas DE, 2010, COMPUT NETW, V54, P3421, DOI 10.1016/j.comnet.2010.06.020
   Cong Liu, 2011, Proceedings of the 2011 31st International Conference on Distributed Computing Systems Workshops (ICDCS Workshops), P339, DOI 10.1109/ICDCSW.2011.68
   Corson S., 1999, MOBILE AD HOC NETWOR
   Dias JAFF, 2015, IEEE COMMUN MAG, V53, P88, DOI 10.1109/MCOM.2015.7355571
   Dias JAFF, 2015, IEEE T IND ELECTRON, V62, P7929, DOI 10.1109/TIE.2015.2425357
   Froushani M. H. L., 2011, 18th International Conference on Telecommunications (ICT 2011), P78, DOI 10.1109/CTS.2011.5898989
   Gyarmati L, 2011, PERVASIVE MOB COMPUT, V7, P545, DOI 10.1016/j.pmcj.2010.11.006
   HABAK K., 2013, CORR
   Habak K, 2015, IEEE INT CONF MOB, P37, DOI 10.1109/MASS.2015.42
   Habak K, 2013, COMPUT NETW, V57, P3067, DOI 10.1016/j.comnet.2013.07.012
   He Q, 2004, IEEE WCNC, P825, DOI 10.1109/WCNC.2004.1311293
   Helgason Olafur Ragnar, 2010, 2010 European Wireless Conference (EW), P903, DOI 10.1109/EW.2010.5483523
   Hoque MA, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2556942
   HU J., 2006, Proceedings of the 44th annual Southeast regional conference, P119
   Hu JY, 2009, COMPUT COMMUN NETW S, P43, DOI 10.1007/978-1-84800-328-6_3
   Hubaux J.-P., 2007, TECHNICAL REPORT
   Jaramillo JJ, 2007, MOBICOM'07: PROCEEDINGS OF THE THIRTEENTH ACM INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P87
   Jonsson J., 2003, Public-key cryptography standards (pkcs) #1: Rsa cryptography speci cations version 2.1
   Kang Chen, 2013, 2013 IEEE International Conference on Sensing, Communications and Networking (SECON), P532, DOI 10.1109/SAHCN.2013.6645025
   Keranen Ari, 2016, ONE OPPORTUNISTIC NE
   Khan UA, 2009, IEEE T SIGNAL PROCES, V57, P2000, DOI 10.1109/TSP.2009.2014812
   Korakis T, 2009, EURASIP J WIREL COMM, DOI 10.1155/2009/598140
   Kramer G, 2006, FOUND TRENDS NETW, V1, P271, DOI 10.1561/1300000004
   Lai LF, 2006, IEEE T INFORM THEORY, V52, P805, DOI 10.1109/TIT.2005.864421
   Lei L, 2013, IEEE WIREL COMMUN, V20, P34, DOI 10.1109/MWC.2013.6549281
   Li Yun, 2011, 8 IEEE INT C WIR OPT, P1
   Li Z, 2012, IEEE T MOBILE COMPUT, V11, P1287, DOI 10.1109/TMC.2011.151
   Liu P, 2007, IEEE J SEL AREA COMM, V25, P340, DOI 10.1109/JSAC.2007.070210
   Ma K, 2010, CHIN CONTR CONF, P4175
   Michiardi P, 2002, INT FED INFO PROC, V100, P107
   Mshvidobadze T., 2012, 2012 6 INT C APPL IN, P1, DOI [DOI 10.1109/ICAICT.2012.6398495, 10.1109/ICAICT.2012.6398495]
   Murphy P, 2009, EURASIP J WIREL COMM, DOI 10.1155/2009/972739
   Patwari N, 2005, IEEE SIGNAL PROC MAG, V22, P54, DOI 10.1109/MSP.2005.1458287
   Pozo Julian Morillo, 2008, 2008 28th International Conference on Distributed Computing Systems Workshops (ICDCS Workshops), P192, DOI 10.1109/ICDCS.Workshops.2008.58
   Raeburn K., 2005, 3962 RFC
   Raychaudhuri D, 2012, P IEEE, V100, P824, DOI 10.1109/JPROC.2011.2182095
   Rivest R., 1992, MD5 MESSAGE DIGEST A
   Rowe LA, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490825
   Sammarco C., 2008, ICC Workshops 2008 - IEEE International Conference on Communications Workshops, P149, DOI 10.1109/ICCW.2008.33
   Shen Y, 2010, IEEE T INFORM THEORY, V56, P4981, DOI 10.1109/TIT.2010.2059720
   Shevade U, 2008, I C NETWORK PROTOCOL, P238, DOI 10.1109/ICNP.2008.4697042
   Silva BM, 2013, J MED INTERNET RES, V15, DOI 10.2196/jmir.2498
   Silva BMC, 2013, IEEE J SEL AREA COMM, V31, P28, DOI 10.1109/JSAC.2013.SUP.0513003
   Soares VNGJ, 2011, COOPERATIVE NETWORKING, P101
   Wen Shuhuan, 2012, MATH PROBL ENG, V2012
   Win MZ, 2011, IEEE COMMUN MAG, V49, P56, DOI 10.1109/MCOM.2011.5762798
   Wu JP, 2013, CHINA COMMUN, V10, P14, DOI 10.1109/CC.2013.6549255
   Wymeersch H, 2009, P IEEE, V97, P427, DOI 10.1109/JPROC.2008.2008853
   Yin Lei, 2010, 2 INT C SIGN PROC SY, V1
   Yoo JW, 2011, IEEE T MOBILE COMPUT, V10, P491, DOI 10.1109/TMC.2010.161
   Yuting Luo, 2009, 2009 IEEE Asia-Pacific Services Computing Conference (APSCC 2009), P213, DOI 10.1109/APSCC.2009.5394122
   Zetterberg P, 2009, EURASIP J WIREL COMM, DOI 10.1155/2009/368752
   Zhong S, 2003, IEEE INFOCOM SER, P1987
NR 66
TC 5
Z9 5
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2016
VL 12
IS 4
AR 49
DI 10.1145/2957752
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EH
UT WOS:000382877500003
DA 2024-07-18
ER

PT J
AU Zhang, LY
   Dong, HW
   El Saddik, A
AF Zhang, Longyu
   Dong, Haiwei
   El Saddik, Abdulmotaleb
TI From 3D Sensing to Printing: A Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; 3D sensing technologies; 3D model reconstruction; 3D
   printers
ID CULTURAL-HERITAGE; REGISTRATION; RECONSTRUCTION; ACCURACY; SENSOR
AB Three-dimensional (3D) sensing and printing technologies have reshaped our world in recent years. In this article, a comprehensive overview of techniques related to the pipeline from 3D sensing to printing is provided. We compare the latest 3D sensors and 3D printers and introduce several sensing, postprocessing, and printing techniques available from both commercial deployments and published research. In addition, we demonstrate several devices, software, and experimental results of our related projects to further elaborate details of this process. A case study is conducted to further illustrate the possible tradeoffs during the process of this pipeline. Current progress, future research trends, and potential risks of 3D technologies are also discussed.
C1 [Zhang, Longyu; Dong, Haiwei; El Saddik, Abdulmotaleb] Univ Ottawa, Multimedia Commun Res Lab, 800 King Edward, Ottawa, ON K1N5N6, Canada.
C3 University of Ottawa
RP Zhang, LY; Dong, HW; El Saddik, A (corresponding author), Univ Ottawa, Multimedia Commun Res Lab, 800 King Edward, Ottawa, ON K1N5N6, Canada.
EM lzhan121@uottawa.ca; hdong@uottawa.ca; elsaddik@uottawa.ca
RI Dong, Haiwei/I-1273-2014; El Saddik, Abdulmotaleb/D-4159-2009
OI Dong, Haiwei/0000-0003-1437-7805; 
CR Adm M. B., 2012, Proceedings of the 2012 International Conference on Computer & Information Science (ICCIS), P1000, DOI 10.1109/ICCISci.2012.6297172
   Ahn SH, 2002, RAPID PROTOTYPING J, V8, P248, DOI 10.1108/13552540210441166
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Anderson D., 2005, P INT C SENS TECHN, V2, P17
   [Anonymous], 2005, RANGE IMAGING DAY ZU
   [Anonymous], 2015, CYB WHOL BOD COL 3D
   [Anonymous], 2015, CREATF METRASCAN 210
   [Anonymous], 2014, RGBD MAPPING USING D
   [Anonymous], P COMP APPL QUANT ME
   [Anonymous], GETTING STARTED MAKE
   [Anonymous], 2007, ROBOTICS SCI SYSTEMS
   [Anonymous], 2008, J MANUF PROCESS, DOI DOI 10.1016/J.JMAPRO.2009.03.002
   Barazzetti L, 2010, PHOTOGRAMM REC, V25, P356, DOI 10.1111/j.1477-9730.2010.00599.x
   Beard MA, 2011, J RAMAN SPECTROSC, V42, P744, DOI 10.1002/jrs.2771
   Beeler T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778777
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Blais F, 2004, J ELECTRON IMAGING, V13, P231, DOI 10.1117/1.1631921
   Brutzman D., 2010, X3D: extensible 3D graphics for Web authors, V1st
   Campbell T, 2011, APPL LEGAL PHILOS, P1
   Chatterjee Avishek, 2012, P 8 IND C COMP VIS G
   Chen D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461994
   CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C
   Ciaccio EJ, 2013, COMPUT METH PROG BIO, V111, P676, DOI 10.1016/j.cmpb.2013.06.002
   Cui Y, 2013, IEEE T PATTERN ANAL, V35, P1039, DOI 10.1109/TPAMI.2012.190
   Cui Y, 2010, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2010.5540082
   De Chiffre L, 2014, CIRP ANN-MANUF TECHN, V63, P655, DOI 10.1016/j.cirp.2014.05.011
   Dehoff R, 2013, ADV MATER PROCESS, V171, P19
   Echevarria JI, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601133
   Ellerin Stephen, 2004, EMEDIA, V17, P14
   Feldmar J, 1996, INT J COMPUT VISION, V18, P99, DOI 10.1007/BF00054998
   Figueroa N, 2013, IEEE SYS MAN CYBERN, P4897, DOI 10.1109/SMC.2013.833
   Fish Timothy, 2011, EXTENDING ART ILLUSI
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Gartner, 2015, COOL VEND 3D PRINT
   Gokturk S. B., 2004, 2004 C COMPUTER VISI, V2004, P35
   Gomes L, 2014, PATTERN RECOGN LETT, V50, P3, DOI 10.1016/j.patrec.2014.03.023
   GREENBAUM E, 2013, J INTELLECTUAL PROPE, V2, P257
   Hacioglu Alper, 2014, GNU GEN PUBLIC LICEN
   Hansard M., 2012, Time-of-Flight Cameras: Principles, Methods and Applications
   Henry Segerman, 2012, MATH INTELL, V34, P1
   Horvath J., 2014, Mastering 3D printing, DOI DOI 10.1007/978-1-4842-0025-41
   Iancu C., 2010, FIABILITY DURABILITY, V1, P73
   Ikeuchi K, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P117, DOI 10.1109/IM.2001.924417
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Javidi Bahram, 2013, P WORKSH INF OPT, P1
   Kang YS, 2010, IEEE INT CON MULTI, P1405, DOI 10.1109/ICME.2010.5583208
   Kaveh H, 2013, IRAN CONF MACH, P304, DOI 10.1109/IranianMVIP.2013.6780000
   Kazhdan M., 2006, P 4 EUROGRAPHICS S G
   Ketcham RA, 2001, COMPUT GEOSCI-UK, V27, P381, DOI 10.1016/S0098-3004(00)00116-3
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Kirmani A, 2013, IEEE INT CON MULTI
   Kolb A, 2010, COMPUT GRAPH FORUM, V29, P141, DOI 10.1111/j.1467-8659.2009.01583.x
   Kruth JP, 2011, CIRP ANN-MANUF TECHN, V60, P821, DOI 10.1016/j.cirp.2011.05.006
   Lange R, 2001, IEEE J QUANTUM ELECT, V37, P390, DOI 10.1109/3.910448
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   Lipson H, 2005, J MECH DESIGN, V127, P1029, DOI 10.1115/1.1902999
   Lipson H., 2013, Fabricated: The new world of 3D printing, P65
   Liu K, 2010, J OPT SOC AM A, V27, P553, DOI 10.1364/JOSAA.27.000553
   Liu K, 2010, OPT EXPRESS, V18, P5229, DOI 10.1364/OE.18.005229
   LMI Technologies, 2015, HDI ADV R3X
   Manfredi D, 2013, MATERIALS, V6, P856, DOI 10.3390/ma6030856
   Marcoux J., 2012, ICDS 2012, The Sixth International Conference on Digital Society, P54
   Markelj P, 2012, MED IMAGE ANAL, V16, P642, DOI 10.1016/j.media.2010.03.005
   MESA-Imaging, 2015, SWISSRANGER 4500
   Microsoft, 2015, MICR KIN
   Milani S, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037680
   Mumtaz KA, 2010, J MATER PROCESS TECH, V210, P279, DOI 10.1016/j.jmatprotec.2009.09.011
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   North-Star-Imaging, 2015, X VIEW X5000
   Pan T, 2013, ADV MATER RES-SWITZ, V753-755, P1283, DOI 10.4028/www.scientific.net/AMR.753-755.1283
   Patel N. M., 2012, 2012 International Conference on Audio, Language and Image Processing (ICALIP 2012). Proceedings, P674, DOI 10.1109/ICALIP.2012.6376701
   Pearce JM., 2010, J. Sustain. Dev, V3, P17, DOI DOI 10.5539/JSD.V3N4P17
   Pellerin J, 2014, COMPUT GEOSCI-UK, V62, P103, DOI 10.1016/j.cageo.2013.09.008
   Rau JY, 2012, SENSORS-BASEL, V12, P11271, DOI 10.3390/s120811271
   Remondino F, 2006, PHOTOGRAMM REC, V21, P269, DOI 10.1111/j.1477-9730.2006.00383.x
   Rooker M, 2013, COMM COM INF SC, V371, P158
   Salvi J, 2010, PATTERN RECOGN, V43, P2666, DOI 10.1016/j.patcog.2010.03.004
   Sansoni G, 2009, SENSORS-BASEL, V9, P568, DOI 10.3390/s90100568
   Sells E, 2010, Handb. Res. Mass Cust. Pers, P568, DOI [DOI 10.1142/97898142802800028, 10.1142/9789814280280_0028]
   Shiping Zhu, 2012, Proceedings of the 2012 International Conference on Computer Science and Information Processing (CSIP), P1336, DOI 10.1109/CSIP.2012.6309109
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Shoubin Liu, 2012, Proceedings of the 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE 2012), P407, DOI 10.1109/CSAE.2012.6272982
   Slavkovsky E., 2012, THESIS HARVARD U
   Smisek J., 2013, 3D with Kinect Consumer Depth Cameras for Computer Vision, P3, DOI [DOI 10.1007/978-1-4471-4640-7_1, 10.1007/978-1-4471-4640-7-1, DOI 10.1007/978-1-4471-4640-7-1]
   Stoykova E, 2007, IEEE T CIRC SYST VID, V17, P1568, DOI 10.1109/TCSVT.2007.909975
   Sturm J, 2013, LECT NOTES COMPUT SC, V8142, P405, DOI 10.1007/978-3-642-40602-7_43
   Tang Liangwen, 2014, P IEEE CHIN GUID NAV, P2007
   Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56
   Toshiba, 2015, TOSCANER 20000AV
   Vaupotic Bostjan, 2006, J ACHIEVEMENTS MAT M, V18, P1
   Vidimce K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461993
   Wang JF, 2012, INT CONF ACOUST SPEE, P5429, DOI 10.1109/ICASSP.2012.6289149
   Wang Jinfeng, 2011, COMPUTER KNOWLEDGE T, V10
   WIJENAYAKE U, 2012, P INT C 3D IM MOD PR, P517, DOI DOI 10.1109/3DIMPVT.2012.68
   Willis KDD, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P589
   Xia GY, 2014, 2014 7TH INTERNATIONAL CONFERENCE ON BIOMEDICAL ENGINEERING AND INFORMATICS (BMEI 2014), P410, DOI 10.1109/BMEI.2014.7002809
   Yang L, 2015, IEEE SENS J, V15, P4275, DOI 10.1109/JSEN.2015.2416651
   Yang Zhou, 2013, IEEE SoutheastCon 2013. Moving America into the Future. Proceedings, P1
   Zbontar K, 2013, APPL OPTICS, V52, P2750, DOI 10.1364/AO.52.002750
   Zhang LY, 2014, J DISP TECHNOL, V10, P647, DOI 10.1109/JDT.2014.2312488
   Zhang Z, 2012, INT C PAR DISTRIB SY, P284, DOI 10.1109/ICPADS.2012.47
   ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149
   Zhu Jiejie., 2008, IEEE C COMPUTER VISI, P1, DOI [DOI 10.1109/CVPR.2008.4587761, DOI 10.1145/1477862.1477875]
NR 103
TC 23
Z9 23
U1 2
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2016
VL 12
IS 2
AR 27
DI 10.1145/2818710
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PK
UT WOS:000373906200001
DA 2024-07-18
ER

PT J
AU Chu, CH
AF Chu, Chung-Hua
TI Visual Comfort for Stereoscopic 3D by Using Motion Sensors on 3D Mobile
   Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; Performance; Stereoscopic 3D; mobile devices
ID IMAGES; MODEL
AB Advanced 3D mobile devices attract a lot of attentions for 3D visualization nowadays. Stereoscopic images and video taken from the 3D mobile devices are uncomfortable for 3D viewing experiences due to the limited hardware for stereoscopic 3D stabilization. The existing stereoscopic 3D stabilization methods are computationally inefficient for the 3D mobile devices. In this article, we point out that this critical issue deteriorates the 3D viewing experiences on the 3D mobile devices. To improve visual comfort, we propose an efficient and effective algorithm to stabilize the stereoscopic images and video for the 3D mobile devices. To rectify the video jitter, we use the gyroscope and accelerometer embedded on the mobile devices to obtain the geometry information of the cameras. Using a different method than video-content-based motion estimation, our algorithm based on the gyroscope and acceleration data can achieve higher accuracy to effectively stabilize the video. Therefore, our approach is robust in video stabilization even under poor lighting and substantial foreground motion. Our algorithm outperforms previous approaches in not only smaller running time but also the better comfort of the stereoscopic 3D visualization for the 3D mobile devices.
C1 Natl Taichung Univ Sci & Technol, Taichung, Taiwan.
C3 National Taichung University of Science & Technology
RP Chu, CH (corresponding author), Natl Taichung Univ Sci & Technol, Taichung, Taiwan.
EM jony@arbor.ee.ntu.edu.tw
FU National Science Council of Taiwan, R.O.C. [NSC103-2221-E-025-009]
FX The work was supported in part by the National Science Council of
   Taiwan, R.O.C., under Contracts NSC103-2221-E-025-009.
CR Abramowitz M., 1972, Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, pp 72
   Allison RS, 2007, J IMAGING SCI TECHN, V51, P317, DOI 10.2352/J.ImagingSci.Technol.(2007)51:4(317)
   Bhat P., 2007, P ESRT GREN FRANC, P327
   Didyk P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964991
   Falkenhagen L., 1994, IMAGE PROCESSING BRO, P115
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Geng Sun, 2009, Proceedings of the SPIE - The International Society for Optical Engineering, V7237, DOI 10.1117/12.807136
   Heinzle S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964989
   HUANG TS, 1994, P IEEE, V82, P252, DOI 10.1109/5.265351
   Jain S, 2006, INT C PATT RECOG, P551
   Jeehong Lee, 2012, 2012 IEEE International Conference on Emerging Signal Processing Applications, P139, DOI 10.1109/ESPA.2012.6152465
   Kang SB, 1998, PROC SPIE, V3641, P2, DOI 10.1117/12.333774
   Karasulu Bahadir, 2013, SPRINGERBRIEFS COMPU, P63
   Kooi FL, 2004, DISPLAYS, V25, P99, DOI 10.1016/j.displa.2004.07.004
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Lee KY, 2009, IEEE I CONF COMP VIS, P1397
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   Liu C., 2011, Proceedings of the 19th ACM international conference on Multimedia (MM '11), P253, DOI DOI 10.1145/2072298.2072332
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Lo WY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866173
   Mangiat S., 2012, 2012 IEEE International Conference on Emerging Signal Processing Applications, P147, DOI 10.1109/ESPA.2012.6152467
   Matusik W, 2004, ACM T GRAPHIC, V23, P814, DOI 10.1145/1015706.1015805
   McMillan L, 1997, IMAGE BASED APPROACH
   Mendapara P, 2010, IEEE INT CON MULTI, P1409, DOI 10.1109/ICME.2010.5583171
   Morimoto C, 1998, INT CONF ACOUST SPEE, P2789, DOI 10.1109/ICASSP.1998.678102
   Phuong NHQ, 2009, J UNIVERS COMPUT SCI, V15, P859
   Rabaud V., 2006, P IEEE COMP SOC C CO
   Raguram R, 2008, LECT NOTES COMPUT SC, V5303, P500, DOI 10.1007/978-3-540-88688-4_37
   Ritter N, 1999, IEEE T MED IMAGING, V18, P404, DOI 10.1109/42.774168
   Sabatini A., 2006, IEEE T BIOMED ENG, V53, P7
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   SHI J, 1994, P IEEE COMP SOC C CO
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Smith BM, 2009, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2009.5459270
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Speranza F., 2006, P SOC PHOTO-OPT INS, V6055, P94
   Suh YS, 2010, IEEE T INSTRUM MEAS, V59, P3296, DOI 10.1109/TIM.2010.2047157
   Tam WJ, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1869, DOI 10.1109/ICME.2006.262919
   Tam WJ, 2011, IEEE T BROADCAST, V57, P335, DOI 10.1109/TBC.2011.2125070
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Uchida N., 2005, P IEEE INT C IM PROC, V2, P950
   Wang C, 2008, PROC SPIE, V6803, DOI 10.1117/12.767702
   Wang JM, 2009, IEEE IMAGE PROC, P3477, DOI 10.1109/ICIP.2009.5413831
   Wang OL, 2011, PITUITARY, 3RD EDITION, P47, DOI 10.1016/B978-0-12-380926-1.10003-3
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
NR 47
TC 5
Z9 5
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 14
DI 10.1145/2808211
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100004
DA 2024-07-18
ER

PT J
AU Zhang, B
   Conci, N
   De Natale, FGB
AF Zhang, Bo
   Conci, Nicola
   De Natale, Francesco G. B.
TI Segmentation of Discriminative Patches in Human Activity Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Performance; Discriminative patches; human activity;
   nonnegative sparse coding; error-correcting code SVM
ID SPARSE; SELECTION
AB In this article, we present a novel approach to segment discriminative patches in human activity videos. First, we adopt the spatio-temporal interest points (STIPs) to represent significant motion patterns in the video sequence. Then, nonnegative sparse coding is exploited to generate a sparse representation of each STIP descriptor. We construct the feature vector for each video by applying a two-stage sum-pooling and l(2)-normalization operation. After training a multi-class classifier through the error-correcting code SVM, the discriminative portion of each video is determined as the patch that has the highest confidence while also being correctly classified according to the video category. Experimental results show that the video patches extracted by our method are more separable, while preserving the perceptually relevant portion of each activity.
C1 [Zhang, Bo; Conci, Nicola; De Natale, Francesco G. B.] Univ Trento, Dept Informat Engn & Comp Sci, Trento, Italy.
C3 University of Trento
RP Zhang, B (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci, Trento, Italy.
EM bzhang@disi.unitn.it
RI Conci, Nicola/AAH-4671-2020
OI Conci, Nicola/0000-0002-7858-0928; zhang, bo/0000-0002-9006-1303
CR Akman O., 2008, P ECCV WORKSH MULT M
   [Anonymous], UT-Interaction Dataset, ICPR contest on Semantic Description of Human Activities (SDHA)
   [Anonymous], 2008, PROC BRIT MACH VIS C
   [Anonymous], 2009, P BRIT MACH VIS C
   Borges PVK, 2013, IEEE T CIRC SYST VID, V23, P1993, DOI 10.1109/TCSVT.2013.2270402
   Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303
   Castrodad A, 2012, INT J COMPUT VISION, V100, P1, DOI 10.1007/s11263-012-0534-7
   Cavallaro A, 2005, IEEE T CIRC SYST VID, V15, P575, DOI 10.1109/TCSVT.2005.844447
   Chen M.-Y., 2009, Tech. Rep. CMU-CS- 09-161
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Elad M., 2006, IEEE COMPUTER SOC C, V1, P895, DOI 10.1109/CVPR.2006.142
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   García-Pedrajas N, 2011, INFORM FUSION, V12, P111, DOI 10.1016/j.inffus.2010.06.010
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253
   HALL ET, 1963, AM ANTHROPOL, V65, P1003, DOI 10.1525/aa.1963.65.5.02a00020
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hoyer PO, 2002, NEURAL NETWORKS FOR SIGNAL PROCESSING XII, PROCEEDINGS, P557, DOI 10.1109/NNSP.2002.1030067
   Huang TK, 2006, J MACH LEARN RES, V7, P85
   Kreutz-Delgado K, 2003, NEURAL COMPUT, V15, P349, DOI 10.1162/089976603762552951
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Mairal J., 2012, SPARSE MODELING SOFT
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   Patron-Perez A, 2012, IEEE T PATTERN ANAL, V34, P2441, DOI 10.1109/TPAMI.2012.24
   Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342
   Raptis M, 2010, LECT NOTES COMPUT SC, V6311, P577, DOI 10.1007/978-3-642-15549-9_42
   Sapiro G., 2008, P INT C NEUR INF PRO
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Sprechmann P, 2010, INT CONF ACOUST SPEE, P2042, DOI 10.1109/ICASSP.2010.5494985
   Tamrakar A, 2012, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2012.6248114
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2013, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2013.345
   Wang X., 2012, ACCV, P572
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Yan Y, 2014, IEEE T IMAGE PROCESS, V23, P5599, DOI 10.1109/TIP.2014.2365699
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhang B., 2013, P SPIE VID SURV TRAN
   Zhang B, 2013, IEEE IMAGE PROC, P3557, DOI 10.1109/ICIP.2013.6738734
NR 43
TC 10
Z9 10
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2015
VL 12
IS 1
AR 4
DI 10.1145/2750780
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CP9LG
UT WOS:000360215200004
DA 2024-07-18
ER

PT J
AU Zhao, X
   Li, X
   Pang, CY
   Sheng, QZ
   Wang, S
   Ye, M
AF Zhao, Xin
   Li, Xue
   Pang, Chaoyi
   Sheng, Quan Z.
   Wang, Sen
   Ye, Mao
TI Structured Streaming Skeleton - A New Feature for Online Human Gesture
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Gesture recognition; feature extraction;
   depth camera
ID SELECTION
AB Online human gesture recognition has a wide range of applications in computer vision, especially in human-computer interaction applications. The recent introduction of cost-effective depth cameras brings a new trend of research on body-movement gesture recognition. However, there are two major challenges: (i) how to continuously detect gestures from unsegmented streams, and (ii) how to differentiate different styles of the same gesture from other types of gestures. In this article, we solve these two problems with a new effective and efficient feature extractionmethod-Structured Streaming Skeleton (SSS)-which uses a dynamic matching approach to construct a feature vector for each frame. Our comprehensive experiments on MSRC-12 Kinect Gesture, Huawei/3DLife-2013, and MSR-Action3D datasets have demonstrated superior performances than the state-of-the-art approaches. We also demonstrate model selection based on the proposed SSS feature, where the classifier of squared loss regression with l(2,1) norm regularization is a recommended classifier for best performance.
C1 [Zhao, Xin; Li, Xue; Wang, Sen] Univ Queensland, St Lucia, Qld 4072, Australia.
   [Pang, Chaoyi] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Pang, Chaoyi] Hebei Acad Sci, Shijiazhuang, Peoples R China.
   [Sheng, Quan Z.] Univ Adelaide, Adelaide, SA 5005, Australia.
   [Ye, Mao] Univ Elect Sci & Technol China, Chengdu, Peoples R China.
C3 University of Queensland; Zhejiang University; Hebei Academy of
   Sciences; University of Adelaide; University of Electronic Science &
   Technology of China
RP Zhao, X (corresponding author), Univ Queensland, Room 626,Bldg 78, St Lucia, Qld 4072, Australia.
EM x.zhao@uq.edu.au; xueli@uq.edu.au
RI wu, sen/HKE-6181-2023; Sheng, Quan Z./ITV-5105-2023; Ye,
   Mao/K-3012-2019; Pang, Chaoyi/JQX-1513-2023; Unankard,
   Sayan/M-4084-2016; Sheng, Quan Z./B-8169-2008
OI Sheng, Quan Z./0000-0002-3326-4147; Ye, Mao/0000-0001-9253-1332; Sheng,
   Quan Z./0000-0002-3326-4147; Ye, Mao/0000-0003-4760-8702; Wang,
   Sen/0000-0002-5414-8276; LI, Xue/0000-0002-4515-6792
FU Australian Research Council [DP130104614]; Natural Science Foundation of
   China [61232006]
FX This research is partially supported by the Australian Research Council
   (Grant No. DP130104614) and Natural Science Foundation of China (Grant
   No. 61232006).
CR Ankerst M, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P49
   [Anonymous], 2010, BRIEF INTRO COUPLING
   [Anonymous], 2009, P 2009 ACM SIGGRAPH
   Bartlett PL, 2002, MACH LEARN, V48, P85, DOI 10.1023/A:1013999503812
   Berndt D.J., 1994, AAAI 94 WORKSH KNOWL, V10, P359, DOI [10.5555/3000850.3000887, DOI 10.5555/3000850.3000887]
   Bloom V., 2012, 2012 IEEE COMP SOC C, P7, DOI [DOI 10.1109/CVPRW.2012.6239175, 10.1109/CVPRW.2012.6239175]
   Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P82
   Chaquet JM, 2013, COMPUT VIS IMAGE UND, V117, P633, DOI 10.1016/j.cviu.2013.01.013
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Fothergill S., 2012, P SIGCHI C HUM FACT, P1737, DOI DOI 10.1145/2207676.2208303
   Gong D, 2012, LECT NOTES COMPUT SC, V7574, P229, DOI 10.1007/978-3-642-33712-3_17
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253
   Gupta Raj., 2013, P 21 ACM INT C MULTI, P283, DOI DOI 10.1145/2502081.2502099
   Huawei, 2013, HUAW 3DLIF ACM MULT
   JOHANSSON G, 1975, SCI AM, V232, P76, DOI 10.1038/scientificamerican0675-76
   Kaufman L., 1987, Statistical Data Analysis Based on the L1-Norm and Related Methods. First International Conference, P405
   Li H, 2011, PATTERN RECOGN, V44, P1614, DOI 10.1016/j.patcog.2010.12.014
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Lin CD, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, VOLS 1 AND 2, P1
   Lin Shih-Yao, 2012, P 20 ACM INT C MULTI, P39
   Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359
   MARTENS J, 2011, P 28 INT C MACH LEAR, P1033
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Papapetrou P, 2011, ACM T DATABASE SYST, V36, DOI 10.1145/2000824.2000827
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Rifkin R., 2003, Computer and Systems Sciences, V190, P131
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   Sakurai Y., 2007, P INT C DAT, V23, P1046, DOI [10.1109/ICDE.2007.368963,2007, DOI 10.1109/ICDE.2007.368963]
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Song Y, 2012, ACM T INTERACT INTEL, V2, DOI 10.1145/2133366.2133371
   Tran KN, 2012, PATTERN RECOGN, V45, P2562, DOI 10.1016/j.patcog.2011.12.028
   Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594
   Veeraraghavan A., 2006, CVPR 06, P959, DOI [DOI 10.1109/CVPR.2006.304, 10.1109/CVPR.2006.304]
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang S, 2012, PROC CVPR IEEE, P1370, DOI 10.1109/CVPR.2012.6247823
   Yang Y, 2013, IEEE T PATTERN ANAL, V35, P1635, DOI 10.1109/TPAMI.2012.253
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Zhang Z, 2012, IEEE T PATTERN ANAL, V34, P436, DOI 10.1109/TPAMI.2011.157
NR 38
TC 14
Z9 14
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2014
VL 11
IS 1
SU S
SI SI
AR 22
DI 10.1145/2648583
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS0RG
UT WOS:000343984800014
DA 2024-07-18
ER

PT J
AU Bulterman, DCA
   Cesar, P
   Guimaraes, RL
AF Bulterman, Dick C. A.
   Cesar, Pablo
   Guimaraes, Rodrigo Laiola
TI Socially-Aware Multimedia Authoring: Past, Present, and Future
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Authoring systems; socially-aware multimedia;
   personalization
AB Creating compelling multimedia productions is a nontrivial task. This is as true for creating professional content as it is for nonprofessional editors. During the past 20 years, authoring networked content has been a part of the research agenda of the multimedia community. Unfortunately, authoring has been seen as an initial enterprise that occurs before 'real' content processing takes place. This limits the options open to authors and to viewers of rich multimedia content for creating and receiving focused, highly personal media presentations. This article reflects on the history of multimedia authoring. We focus on the particular task of supporting socially-aware multimedia, in which the relationships within particular social groups among authors and viewers can be exploited to create highly personal media experiences. We provide an overview of the requirements and characteristics of socially-aware multimedia authoring within the context of exploiting community content. We continue with a short historical perspective on authoring support for these types of situations. We then present an overview of a current system for supporting socially-aware multimedia authoring within the community content. We conclude with a discussion of the issues that we feel can provide a fruitful basis for future multimedia authoring support. We argue that providing support for socially-aware multimedia authoring can have a profound impact on the nature and architecture of the entire multimedia information processing pipeline.
C1 [Bulterman, Dick C. A.; Guimaraes, Rodrigo Laiola] Vrije Univ Amsterdam, Amsterdam, Netherlands.
C3 Vrije Universiteit Amsterdam
RP Bulterman, DCA (corresponding author), FX Palo Alto Lab, 3174 Porter Dr, Palo Alto, CA 94304 USA.
EM dick.bulterman@fxpal.com
OI Cesar, Pablo/0000-0003-1752-6837
FU EU [FP7-ICT-214793, FP7-ICT-287760]
FX This work was supported in part by the EU projects TA2: Together
   Anywhere, Together Anytime (FP7-ICT-214793) and Vconect:
   (FP7-ICT-287760).
CR Abowd G.D., 2003, Proceedings of the 5th ACM SIGMM international workshop on Multimedia information retrieval, P1, DOI DOI 10.1145/973264.973266
   Adams B, 2005, ACM T MULTIM COMPUT, V1
   [Anonymous], 2013, ACM T MULTIMEDIA COM, V9
   Bocconi S, 2008, J WEB SEMANT, V6, P139, DOI 10.1016/j.websem.2008.01.004
   Bulterman DCA, 2005, ACM T MULTIM COMPUT, V1, P89, DOI 10.1145/1047936.1047943
   Cattelan RG, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1412196.1412201
   CESAR P., 2009, J ACM T MULTIMEDIA C, V5, P3
   DING J. R., 2006, P JOINT C INF SCI CI
   Eisenstein Sergei., 1949, Film Form : Essays in Film Theory
   GAO B., 2011, P NOSSDAV, P105, DOI DOI 10.1145/1989240.1989266
   Soares LFG, 2010, IEEE COMMUN MAG, V48, P74
   Guimaraes R., 2012, P 18 BRAZ S MULT WEB, P253, DOI DOI 10.1145/2382636.2382690
   Hardman L., 1993, Proceedings ACM Multimedia 93, P283, DOI 10.1145/166266.168402
   Ibáñez J, 2008, KNOWL ENG REV, V23, P339, DOI 10.1017/S0269888908000039
   Kennedy Lyndon., 2009, WWW 09, P311, DOI [DOI 10.1145/1526709.1526752, 10.1145/1526709.1526752]
   KIRK D, 2007, C HUM FACT COMP, P61
   Kruitbosch G., 2008, Proceedings of the 3rd ACM Workshop on Human-Centered Computing, P7, DOI [10.1145/1462027.1462029, DOI 10.1145/1462027.1462029]
   Laiola Guimaraes R., 2011, Proceedings of the 19th ACM international conference on Multimedia, MM'11, P303, DOI DOI 10.1145/2072298.2072339
   Lienhart R., 1999, ACM MULTIMEDIA, P37, DOI DOI 10.1145/319878.319888
   Naci S.U., 2007, Proceedings of the 15th international conference on Multimedia (MULTIMEDIA '07), P150, DOI [10.1145/1291233.1291264, DOI 10.1145/1291233.1291264]
   Pea R, 2004, IEEE MULTIMEDIA, V11, P54, DOI 10.1109/MMUL.2004.1261108
   Piacenza A., 2011, Proceedings of the 19th ACM international conference on Multimedia (MM '11), P223, DOI DOI 10.1145/2072298.2072329
   Purcell K., 2010, STATE ONLINE VIDEO
   Rowe LA, 2005, ACM T MULTIM COMPUT, V1, P3, DOI 10.1145/1047936.1047938
   Schmandt C., 1993, Proceedings ACM Multimedia 93, P373, DOI 10.1145/166266.168430
   Shamma DA., 2007, P INT WORKSHOP WORKS, P275, DOI [10.1145/1290082.1290120, DOI 10.1145/1290082.1290120]
   Shipman F, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1413862.1413868
   Shrestha P., 2010, Proceedings of the international conference on Multimedia (MM '10), P541, DOI DOI 10.1145/1873951.1874023
   Singh V.K., 2011, Proceedings of the 19th ACM international conference on Multimedia (MM '11), P333, DOI DOI 10.1145/2072298.2072342
   Snoek CeesG. M., 2010, Proceedings of the International Conference on Multimedia, P1535, DOI DOI 10.1145/1873951.1874278
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   SU K., 2012, P ACM INT C MULT RET
   Sundaram H., 2000, Proceedings ACM Multimedia 2000, P95, DOI 10.1145/354384.354440
   Ursu MF, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1412196.1412198
   Xie L, 2011, P 19 ACM INT C MULT, P53, DOI DOI 10.1145/2072298.2072307
   Zsombori Vilmos., 2011, P 22 ACM C HYPERTEXT, P325, DOI DOI 10.1145/1995966.1996009
NR 36
TC 5
Z9 5
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 35
DI 10.1145/2491893
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700005
DA 2024-07-18
ER

PT J
AU Huang, ZX
   Nahrstedt, K
   Steinmetz, R
AF Huang, Zixia
   Nahrstedt, Klara
   Steinmetz, Ralf
TI Evolution of Temporal Multimedia Synchronization Principles: A
   Historical Viewpoint
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Theory; Design; Performance; Multimedia synchronization; survey
ID MEDIA SYNCHRONIZATION; PROTOCOLS; DELAY
AB The evolution of multimedia applications has drastically changed human life and behaviors. New communication technologies lead to new requirements for multimedia synchronization. This article presents a historical view of temporal synchronization studies focusing on continuous multimedia. We demonstrate how the development of multimedia systems has created new challenges for synchronization technologies. We conclude with a new application-dependent, multilocation, multirequirement synchronization framework to address these new challenges.
C1 [Huang, Zixia; Nahrstedt, Klara] Univ Illinois, Urbana, IL 61801 USA.
   [Steinmetz, Ralf] Tech Univ Darmstadt, Darmstadt, Germany.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   Technical University of Darmstadt
RP Huang, ZX (corresponding author), Univ Illinois, Urbana, IL 61801 USA.
EM zhuang21@illinois.edu; klara@illinois.edu;
   ralf.steinmetz@kom.tu-darmstadt.de
RI Steinmetz, Patrick R. H./AAD-4093-2022
OI Steinmetz, Ralf/0000-0002-6839-9359
FU Direct For Computer & Info Scie & Enginr; Division Of Computer and
   Network Systems [1012194] Funding Source: National Science Foundation
CR Akyildiz IF, 1996, IEEE J SEL AREA COMM, V14, P162, DOI 10.1109/49.481702
   ANDERSON DP, 1991, COMPUTER, V24, P51, DOI 10.1109/2.97251
   [Anonymous], 2008, 15882008 IEEE
   [Anonymous], 2006, RFC4340
   [Anonymous], 1969, BELL LAB REC, V47, P134
   [Anonymous], CMUCS02166
   [Anonymous], 2007, P 2007 2 IEEE IFIP I
   Bailey B., 1998, Proceedings ACM Multimedia 98, P257, DOI 10.1145/290747.290779
   Barrios Richard., 1995, SONG DARK
   Blakowski G, 1996, IEEE J SEL AREA COMM, V14, P5, DOI 10.1109/49.481691
   BLESSER BA, 1978, J AUDIO ENG SOC, V26, P739
   BORONAT F., 2008, J MULTIMEDIA TOOLS A, V40, P285
   Boronat F, 2009, INFORM SYST, V34, P108, DOI 10.1016/j.is.2008.05.001
   BRANDENBURG R., 2012, IETF DRAFT RTCP INTE
   Buchanan MC, 2005, ACM T MULTIM COMPUT, V1, P60, DOI 10.1145/1047936.1047942
   Bulterman D. C. A., 1993, MULTIMEDIA SYST, V1, P68
   BUNN J., 1998, P C COMP HIGH EN PHY
   CAMPBELL A., 1992, P IFIP INT C HIGH PE
   Cluver K, 1996, PROCEEDINGS OF THE IEEE-SP INTERNATIONAL SYMPOSIUM ON TIME-FREQUENCY AND TIME-SCALE ANALYSIS, P277, DOI 10.1109/TFSA.1996.547467
   CRONIN E., 2004, P 1 WORKSH NETW SUPP
   CURCIO I., 2007, P IEEE INT S WORLD W
   Damer Bruce., 1998, AVATARS EXPLORING BU
   DANNENBERG R., 1993, COMMUNICATION
   EHLEY L, 1994, P INT C MULTIMEDIA C, P110
   Fujimoto T, 2008, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2008, PROCEEDINGS, P113
   GARDNER B., 1992, P 124 M AC SOC AM
   Ghinea G, 2010, IEEE T SYST MAN CY A, V40, P657, DOI 10.1109/TSMCA.2010.2041224
   Goldmann L, 2010, IEEE IMAGE PROC, P3241, DOI 10.1109/ICIP.2010.5651142
   HODGES M, 1989, IEEE SOFTWARE
   Hoshino S., 2011, P IEEE INT WORKSH TE, P1, DOI [10.1109/CQR.2011.5996082, DOI 10.1109/CQR.2011.5996082]
   HUANG Z., 2011, P ACM MULT SYST C
   HUANG Z., 2012, THESIS U ILLINOIS UR
   HUANG Z., 2010, P ACM WORKSH NETW OP
   Huang Zixia, 2011, P IEEE INT C COMP CO
   *IETF, 2003, RFC3550 IETF
   Ishibashi Y, 2000, C LOCAL COMPUT NETW, P337, DOI 10.1109/LCN.2000.891066
   Ishibashi Y, 1997, IEEE INFOCOM SER, P692, DOI 10.1109/INFCOM.1997.644522
   Little T., 1993, Multimedia System, V1, P87
   LITTLE TDC, 1991, COMPUTER, V24, P42, DOI 10.1109/2.97250
   MEYER T., 1994, P IEEE WORKSH FUT TR, P97
   MICHEL U., 2006, P BERL BEAMC C
   PICTURETEL, 1991, NY TIMES
   QIAO L, 1997, P SPIE INT C MULT CO, P170
   RAMANATHAN S, 1993, COMPUT J, V36, P19, DOI 10.1093/comjnl/36.1.19
   RAVINDRAN K, 1993, IEEE T KNOWL DATA EN, V5, P574, DOI 10.1109/69.234770
   Rothermel K., 1995, Proceedings of the 5th International Workshop on Network and Operating System Support for Digital Audio and Video, P189
   Rouskas GN, 1997, IEEE J SEL AREA COMM, V15, P346, DOI 10.1109/49.564133
   Shi Sherlia Y., 2001, NOSSDAV: Proceedings of the 11th international workshop on Network and operating systems support for digital audio and video, P83
   Steinmetz R., 1995, MULTIMEDIA COMPUTING
   STEINMETZ R., 1990, P GI ITG WORKSH SPRA, P39
   STEINMETZ R, 1993, 439310 IBM EUR NETW
   Stockham T., 1972, Digital Signal Processing, P55
   Tov S.- Y., 2005, HAPPY 10 BIRTHDAY
   Wahl T., 1994, Proceedings of the International Conference on Multimedia Computing and Systems (Cat. No.94TH0631-2), P538, DOI 10.1109/MMCS.1994.292502
   WALLIS A., 1995, TWANZ, V5, P10
   WILLIAMS A., 2013, RTP CLOCK SOURCE SIG
   WOO M, 1994, IEEE NETWORK, V8, P52, DOI 10.1109/65.260079
   YAVATKAR R, 1992, INT CON DISTR COMP S, P606, DOI 10.1109/ICDCS.1992.235100
   Zimmermann R., 2008, PROCEEDING 16 ACM IN, P299
   [No title captured]
NR 60
TC 12
Z9 13
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 34
DI 10.1145/2490821
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700004
DA 2024-07-18
ER

PT J
AU Li, BC
   Wang, Z
   Liu, JC
   Zhu, WW
AF Li, Baochun
   Wang, Zhi
   Liu, Jiangchuan
   Zhu, Wenwu
TI Two Decades of Internet Video Streaming: A Retrospective View
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Experimentation; Video streaming; multicast; P2P streaming;
   HTTP streaming; cloud computing; social media; multimedia streaming
ID MULTICAST; SERVICE
AB For over two decades, video streaming over the Internet has received a substantial amount of attention from both academia and industry. Starting from the design of transport protocols for streaming video, research interests have later shifted to the peer-to-peer paradigm of designing streaming protocols at the application layer. More recent research has focused on building more practical and scalable systems, using Dynamic Adaptive Streaming over HTTP. In this article, we provide a retrospective view of the research results over the past two decades, with a focus on peer-to-peer streaming protocols and the effects of cloud computing and social media.
C1 [Li, Baochun] Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
   [Wang, Zhi; Zhu, Wenwu] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Liu, Jiangchuan] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 University of Toronto; Tsinghua University; Simon Fraser University
RP Li, BC (corresponding author), Univ Toronto, Dept Elect & Comp Engn, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
EM bli@eecg.toronto.edu; wangzhi04@mails.tsinghua.edu.cn; jcliu@cs.sfu.ca;
   wwzhu@tsinghua.edu.cn
RI baochun, Li/AAD-3188-2022
CR *ACC, 2006, INT IPTV CONS READ S
   Adhikari V. K., 2012, P IEEE INFOCOM
   Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   Akhshabi S., 2011, P ACM MMSYS
   Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   AMIR E, 1995, P ACM MULT
   [Anonymous], P IEEE INFOCOM
   [Anonymous], 2011, P ACM MMSYS
   [Anonymous], 2011, P ACM SIGCOMM
   AREFIN A., 2012, P IEEE INT C DISTR C
   Aurrecoechea C, 1998, MULTIMEDIA SYST, V6, P138, DOI 10.1007/s005300050083
   BALDINO B., 2011, FRAMEWORK TELEPRESEN
   Bergkvist Adam, 2012, WEBRTC 1 0 IN PRESS
   BOLOT JC, 1996, P IEEE INT C IM PROC
   BROXTON T., 2010, P IEEE INT C DAT MIN
   BUDAK C., 2011, P ACM WWW
   Cahill A. J., 2004, P ACM MULT
   Carmel Sharon, 2002, Network Media Streaming. Patent No. US, Patent No. 6389473
   CASTRO M, 2003, P ACM S OP SYST PRIN
   CHENG X., 2012, IEEE T MULTIMEDIA, V15, P5
   Cheng X, 2011, P ACM NOSSDAV
   Cheng Xu, 2009, P IEEE INFOCOM
   Chu Y., 2000, P ACM SIGMETRICS
   CLEARY K., 1995, P BROADC CONV
   Cockcroft A., 2011, Netflix in the Cloud
   Cranley N, 2006, INT J HUM-COMPUT ST, V64, P637, DOI 10.1016/j.ijhcs.2005.12.002
   Cui Y, 2004, IEEE J SEL AREA COMM, V22, P91, DOI 10.1109/JSAC.2003.818799
   DEERING SE, 1990, ACM T COMPUT SYST, V8, P85, DOI 10.1145/78952.78953
   Diot C, 2000, IEEE NETWORK, V14, P78, DOI 10.1109/65.819174
   Feng C., 2009, P IEEE INFOCOM
   Fielding R., 1999, Tech. Rep
   FLOYD S, 2000, P SIGCOMM
   FORGIE J., 1979, ST A PROPOSED INTERN
   Ghodsi A., 2011, P ACM WORKSH HOT TOP
   Gouache S., 2011, P IEEE INT C MULT EX
   Havey D., 2012, P ACM MMSYS
   HO T, 2003, P INT S INF THEOR IS
   Hosseini M, 2007, IEEE COMMUN SURV TUT, V9, P58, DOI 10.1109/COMST.2007.4317616
   HUA K. A., 1998, P ACM INT C MULT
   HUA KA, 1997, COMPUT COMMUN REV, V27, P89
   Huang Y., 2008, P ACM SIGCOMM
   Huang Z., 2011, P IEEE INFOCOM
   INFONETICS, 2011, IPTV SERV GETT HIGHL
   Jacobs S, 1998, J VIS COMMUN IMAGE R, V9, P211, DOI 10.1006/jvci.1998.0389
   JANNOTTI J., 2000, P 4 S OP SYST DES IM, V4
   Jiang J., 2012, P ACM CONEXT
   KIM T., 2001, P ACM NOSSDAV
   KOSTIC D, 2003, P ACM S OP SYST PRIN
   Li BC, 1999, IEEE J SEL AREA COMM, V17, P1632, DOI 10.1109/49.790486
   Liu JC, 2004, IEEE COMMUN MAG, V42, P88, DOI 10.1109/MCOM.2004.1321397
   Liu Z., 2010, Proceedings of IEEE INFOCOM
   Magharei N., 2007, P IEEE INFOCOM
   McCanne S., 1996, Computer Communication Review, V26, P117, DOI 10.1145/248157.248168
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   MPEG, 2010, DYN AD STREAM HTTP
   NAHRSTEDT K., 2011, HOT TOPOCS MULTIMEDI, V51, P23
   Niu D., 2012, P IEEE INFOCOM
   OOI W. T., 2000, P ACM NOSSDAV
   Pai V, 2005, LECT NOTES COMPUT SC, V3640, P127
   Pallis G, 2006, COMMUN ACM, V49, P101, DOI 10.1145/1107458.1107462
   Park JS, 2006, IEEE WIREL COMMUN, V13, P76, DOI 10.1109/WC-M.2006.250362
   Peng G., 2004, CS0411069 ARXIV
   PODOLSKY M, 1999, RTCP BASED RETRANSMI
   Pujol J. M., 2010, P ACM SIGCOMM
   RODRIGUES T., 2011, P ACM IMC
   SAXENA M., 2008, P ACM NOSSDAV
   Schooler E., 2002, 3261 RFC
   SCHULZRINNE H, 1996, 1889 RFC
   Schulzrinne H., 1998, 2326 RFC
   SHEU S, 1997, P IEEE INT C MULT CO
   SILVERSTON T, 2007, P ACM NOSSDAV
   SNL, 2011, GLOB MULT MARK SPEC
   Song Han Hee, 2011, P 2011 ACM SIGCOMM C
   Sripanidkulchai K., 2004, P ACM SIGCOMM
   Stockhammer T., 2011, P ACM C MULT SYST
   STOICA I., 2010, P 9 INT WORKSH PEER
   THOMAS V., 1998, WHITE PAPER IP MULTI, P1
   Tran D.A., 2003, P IEEE INFOCOM
   Vakali A, 2003, IEEE INTERNET COMPUT, V7, P68, DOI 10.1109/MIC.2003.1250586
   Venkataraman V, 2006, PROCEEDINGS OF THE 2006 IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, P2, DOI 10.1109/ICNP.2006.320193
   Wang F., 2007, P IEEE ICDCS
   Wang F, 2012, P IEEE INFOCOM
   WANG M., 2007, IEEE J SELECT AREAS
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   WATSON M., 2011, ACM T MULTIMEDIA COM, V9
   Wu DP, 2001, IEEE T CIRC SYST VID, V11, P282, DOI 10.1109/76.911156
   Wu Y., 2011, P IEEE ICDCS
   Yu F, 2003, IEEE T CIRC SYST VID, V13, P257, DOI 10.1109/TCSVT.2003.809829
   Zhang L., 1993, IEEE Network, V7, P8, DOI 10.1109/65.238150
   ZHANG M, 2005, P ACM MULT
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
   Zhang Q, 2001, IEEE T MULTIMEDIA, V3, P339, DOI 10.1109/6046.944477
   ZHANG X, 2005, P IEEE INFOCOM
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
   [No title captured]
NR 96
TC 73
Z9 80
U1 0
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 33
DI 10.1145/2505805
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700003
DA 2024-07-18
ER

PT J
AU Sakai, K
   Ku, WS
   Sun, MT
   Zimmermann, R
AF Sakai, Kazuya
   Ku, Wei-Shinn
   Sun, Min-Te
   Zimmermann, Roger
TI Privacy Preserving Continuous Multimedia Streaming in MANETs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Performance; Security; Link availability; streaming; mobile
   ad-hoc networks; mobility models; privacy
ID AD; COMMUNICATION; NETWORKS; THEOREM
AB At present, mobile devices are prevalent with end users and continuous media streaming services in mobile ad-hoc networks (MANETs) support popular applications. It is required for applications that stream isochronous media that the network link be continuously available. In this study, we introduce two group-server scheduling schemes to improve link continuity: static group-server scheduling and dynamic group-server scheduling. With our solution, if one of the current links between a client and a server instance breaks, the client can still download the multimedia content from another scheduled server peer. In addition, we incorporate the data link layer constraints as well as privacy concerns into our protocol design. The simulation results show that the proposed schemes significantly improve the effective link duration, overall system performance, and degree of privacy in MANETs.
C1 [Sakai, Kazuya] Ohio State Univ, Columbus, OH 43210 USA.
   [Ku, Wei-Shinn] Auburn Univ, Auburn, AL 36849 USA.
   [Sun, Min-Te] Natl Cent Univ, Tao Yuan 320, Taiwan.
   [Zimmermann, Roger] Natl Univ Singapore, Dept Comp Sci, Singapore 117417, Singapore.
C3 University System of Ohio; Ohio State University; Auburn University
   System; Auburn University; National Central University; National
   University of Singapore
RP Sakai, K (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.
EM sakai.16@osu.edu; weishinn@auburn.edu; msun@csie.ncu.edu.tw;
   rogerz@comp.nus.edu.sg
RI Zimmermann, Roger/D-7944-2015
OI Zimmermann, Roger/0000-0002-7410-2590; Ku, Wei Shinn/0000-0001-8636-4689
CR BACCELLI F., 1987, IEEE J SEL AREA COMM
   BOUDEC J.-Y. L., 2005, P ANN JOINT C IEEE C
   Camp T, 2002, WIREL COMMUN MOB COM, V2, P483, DOI 10.1002/wcm.72
   Cao Min., 2005, MOBIHOC 05, P78, DOI DOI 10.1145/1062689.1062701
   Chhaya HS, 1997, WIREL NETW, V3, P217, DOI 10.1023/A:1019109301754
   Diaz Claudia, 2002, P PRIV ENH TECHN WOR
   Han Q, 2011, IET COMMUN, V5, P2291, DOI 10.1049/iet-com.2010.0946
   Han YJ, 2006, COMPUT NETW, V50, P1887, DOI 10.1016/j.comnet.2005.10.005
   Jain R, 2002, ANN IEEE SYMP FOUND, P429, DOI 10.1109/SFCS.2002.1181967
   Jiang SM, 2001, IEEE INFOCOM SER, P1745, DOI 10.1109/INFCOM.2001.916672
   Kong Jiejun., 2003, P ACM MOBIHOC, P291
   Li BC, 2003, IEEE J SEL AREA COMM, V21, P1627, DOI 10.1109/JSAC.2003.815964
   McDonald AB, 1999, IEEE J SEL AREA COMM, V17, P1466, DOI 10.1109/49.780353
   Min Qin, 2005, 13th Annual ACM International Conference on Multimedia, P956, DOI 10.1145/1101149.1101350
   Qin M, 2006, P 14 ANN ACM INT C M, P153
   Ramachandran I, 2007, ACM T SENSOR NETWORK, V3, DOI 10.1145/1210669.1210673
   Sadagopan Narayanan., 2003, P 4 ACM INT S MOBILE, P245
   SERJANTOV A, 2002, P PRIV ENH TECHN WOR
   Wu XX, 2007, IEEE T DEPEND SECURE, V4, P252, DOI 10.1109/TDSC.2007.70213
   Zheng JL, 2004, IEEE COMMUN MAG, V42, P140
NR 20
TC 3
Z9 3
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2013
VL 9
IS 4
AR 23
DI 10.1145/2501643.2501645
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 206EW
UT WOS:000323501800001
DA 2024-07-18
ER

PT J
AU Qi, H
   Li, KQ
   Shen, YM
   Qu, WY
AF Qi, Heng
   Li, Keqiu
   Shen, Yanming
   Qu, Wenyu
TI Object-Based Image Retrieval with Kernel on Adjacency Matrix and Local
   Combined Features
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Object-based image retrieval;
   feedback processing; kernel; local combined features
AB In object-based image retrieval, there are two important issues: an effective image representation method for representing image content and an effective image classification method for processing user feedback to find more images containing the user-desired object categories. In the image representation method, the local-based representation is the best selection for object-based image retrieval. As a kernel-based classification method, Support Vector Machine (SVM) has shown impressive performance on image classification. But SVM cannot work on the local-based representation unless there is an appropriate kernel. To address this problem, some representative kernels are proposed in literatures. However, these kernels cannot work effectively in object-based image retrieval due to ignoring the spatial context and the combination of local features.
   In this article, we present Adjacent Matrix (AM) and the Local Combined Features (LCF) to incorporate the spatial context and the combination of local features into the kernel. We propose the AM-LCF feature vector to represent image content and the AM-LCF kernel to measure the similarities between AM-LCF feature vectors. According to the detailed analysis, we show that the proposed kernel can overcome the deficiencies of existing kernels. Moreover, we evaluate the proposed kernel through experiments of object-based image retrieval on two public image sets. The experimental results show that the performance of object-based image retrieval can be improved by the proposed kernel.
C1 [Qi, Heng; Li, Keqiu; Shen, Yanming] Dalian Univ Technol, Sch Elect & Informat Engn, Dalian 116024, Peoples R China.
   [Qu, Wenyu] Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian 116023, Peoples R China.
C3 Dalian University of Technology; Dalian Maritime University
RP Li, KQ (corresponding author), Dalian Univ Technol, Sch Elect & Informat Engn, 2 Linggong Rd, Dalian 116024, Peoples R China.
EM keqiu@dlut.edu.cn
FU NSFC [60973115, 60973117, 61173160, 61173162, 61173165]; Intel-MoE Joint
   Research Fund; New Century Excellent Talents in University (NCET) of
   Ministry of Education of China
FX This work is supported by NSFC under grant nos. 60973115, 60973117,
   61173160, 61173162, and 61173165, Intel-MoE Joint Research Fund, and New
   Century Excellent Talents in University (NCET) of Ministry of Education
   of China.
CR [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], DEP COMPUT
   Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800
   Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   GOSSELIN P. H., 2007, P 6 ACM INT C IM VID, P226
   Gosselin PH, 2007, IEEE IMAGE PROC, P177
   Gosselin PH, 2008, COMPUT VIS IMAGE UND, V110, P403, DOI 10.1016/j.cviu.2007.09.018
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Grauman K., 2007, ADV NEURAL INFORM PR, V19, P505, DOI DOI 10.7551/MITPRESS/7503.003.0068
   Griffin G., 2007, CALTECH 256 OBJECT C
   Harchaoui Z., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383049
   Hsiao JH, 2009, P 18 ACM C INF KNOWL, P157
   Hsu C.-W., 2003, PRACTICAL GUIDE SUPP
   Kuo Yin-Hsi., 2009, ACM Multimedia, P65
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   LEBRUN J., 2008, P 19 INT C PATT REC, P1
   Lei Z, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P721, DOI 10.1109/ICIP.2001.958595
   Lyu SW, 2005, PROC CVPR IEEE, P223
   Perronnin F., 2007, P IEEE CVPR, P1
   Philbin J., 2008, P CVPR, P1
   Pontil M, 1998, IEEE T PATTERN ANAL, V20, P637, DOI 10.1109/34.683777
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Wallraven C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P257
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Zheng QF, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1404880.1404887
NR 28
TC 18
Z9 18
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2012
VL 8
IS 4
AR 54
DI 10.1145/2379790.2379796
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 052QA
UT WOS:000312211900006
DA 2024-07-18
ER

PT J
AU Weir, J
   Yan, WQ
   Kankanhalli, MS
AF Weir, Jonathan
   Yan, Weiqi
   Kankanhalli, Mohan S.
TI Image Hatching for Visual Cryptography
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Verification; Cryptography; visual cryptography; image
   hatching; data encryption; structural similarity
ID SECRET SHARING SCHEMES; MULTIPLE SECRETS; ILLUSTRATIONS
AB Image hatching (or nonphotorealistic line-art) is a technique widely used in the printing or engraving of currency. Diverse styles of brush strokes have previously been adopted for different areas of an image to create aesthetically pleasing textures and shading. Because there is no continuous tone within these types of images, a multilevel scheme is proposed, which uses different textures based on a threshold level. These textures are then applied to the different levels and are then combined to build up the final hatched image. The proposed technique allows a secret to be hidden using Visual Cryptography (VC) within the hatched images. Visual cryptography provides a very powerful means by which one secret can be distributed into two or more pieces known as shares. When the shares are superimposed exactly together, the original secret can be recovered without computation. Also provided is a comparison between the original grayscale images and the resulting hatched images that are generated by the proposed algorithm. This reinforces that the overall quality of the hatched scheme is sufficient. The Structural SIMilarity index (SSIM) is used to perform this comparison.
C1 [Weir, Jonathan; Yan, Weiqi] Queens Univ Belfast, Inst ECIT, Belfast BT3 9DT, Antrim, North Ireland.
   [Kankanhalli, Mohan S.] Natl Univ Singapore, Singapore 117548, Singapore.
C3 Queens University Belfast; National University of Singapore
RP Weir, J (corresponding author), Queens Univ Belfast, Inst ECIT, Belfast BT3 9DT, Antrim, North Ireland.
EM jweir07@qub.ac.uk
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
CR [Anonymous], MODERN DIGITAL HALFT
   ATENIESE G, 1996, THEORETICAL COMPUTER, V250, P1
   Blundo C, 2003, SIAM J DISCRETE MATH, V16, P224, DOI 10.1137/S0895480198336683
   CHANG C. Y., 2001, P INT C IM AC SPEE 2
   ELBER G, 1995, VISUAL COMPUT, V11, P290
   ELBER G, 1995, IEEE T VIS COMPUT GR, V1, P231, DOI 10.1109/2945.466718
   Elber G, 1998, IEEE T VIS COMPUT GR, V4, P71, DOI 10.1109/2945.675655
   Feng JB, 2008, PATTERN RECOGN, V41, P3572, DOI 10.1016/j.patcog.2008.05.031
   Hasluck PaulN., 1977, MANUAL TRADITIONAL W
   Hofmeister T, 2000, THEOR COMPUT SCI, V240, P471, DOI 10.1016/S0304-3975(99)00243-1
   HSU HC, 2004, NETWORKING SENSING C, V2, P996
   Ito R, 1999, IEICE T FUND ELECTR, VE82A, P2172
   Ivins WilliamMills., 1987, How Prints Look: Photographs with Commentary. Rev. and expandeded
   IVINS WM, 1992, PRINTS VISUAL COMMUN
   Memon N., 1998, Communications of the ACM, V41, P34, DOI 10.1145/278476.278485
   Ostromoukhov V, 1999, COMP GRAPH, P417, DOI 10.1145/311535.311604
   Praun E, 2001, COMP GRAPH, P581, DOI 10.1145/383259.383328
   SALISBURY M, 1996, P SIGGRAPH 96, P461, DOI DOI 10.1145/237170.237286
   SALISBURY MP, 1994, P SIGGRAPH 94, P101
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Shyu SH, 2006, PATTERN RECOGN, V39, P866, DOI 10.1016/j.patcog.2005.06.010
   Shyu SJ, 2007, PATTERN RECOGN, V40, P3633, DOI 10.1016/j.patcog.2007.03.012
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZM, 2009, IEEE T INF FOREN SEC, V4, P383, DOI 10.1109/TIFS.2009.2024721
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   WEIR J., 2008, MULTIMEDIA COMMUNICA
   Weir J, 2009, LECT NOTES COMPUT SC, V5703, P136, DOI 10.1007/978-3-642-03688-0_14
   YAN W.-Q., 2009, P IEEE INT S CIRC SY
   Yang CN, 2006, LECT NOTES COMPUT SC, V3989, P433
   Zhou Z, 2006, IEEE T IMAGE PROCESS, V15, P2441, DOI 10.1109/TIP.2006.875249
NR 30
TC 8
Z9 8
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 2
SU S
AR 32
DI 10.1145/2344436.2344438
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 011KE
UT WOS:000309162800002
DA 2024-07-18
ER

PT J
AU Wu, WM
   Arefin, A
   Kurillo, G
   Agarwal, P
   Nahrstedt, K
   Bajcsy, R
AF Wu, Wanmin
   Arefin, Ahsan
   Kurillo, Gregorij
   Agarwal, Pooja
   Nahrstedt, Klara
   Bajcsy, Ruzena
TI CZLoD: A Psychophysical Approach for 3D Tele-Immersive Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Experimentation; Measurement; Tele-immersive video;
   psychophysics; color-plus-depth; level-of-details; adaptation
ID QUALITY; JITTER
AB This article presents a psychophysical study that measures the perceptual thresholds of a new factor called Color-plus-Depth Level-of-Details (CZLoD) peculiar to polygon-based 3D tele-immersive video. The results demonstrate the existence of Just Noticeable Degradation and Just Unacceptable Degradation thresholds on the factor. In light of the results, we design and implement a real-time perception-based quality adaptor for 3D tele-immersive video. Our experimental results show that the adaptation scheme can reduce resource usage (e. g., CPU cycles) while considerably enhancing the overall perceived visual quality. Our analysis confirms the potential temporal and spatial performance benefits achievable with CZLoD adaptation.
C1 [Arefin, Ahsan] Univ Illinois, Siebel Ctr Comp Sci, Urbana, IL 61801 USA.
   [Kurillo, Gregorij; Bajcsy, Ruzena] Univ Calif Berkeley, CITRIS Lab, Berkeley, CA 94720 USA.
   [Agarwal, Pooja; Nahrstedt, Klara] Univ Illinois, Thomas M Siebel Ctr Comp Sci, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   University of California System; University of California Berkeley;
   University of Illinois System; University of Illinois Urbana-Champaign
RP Wu, WM (corresponding author), Univ Calif San Diego, 9500 Gilman Dr,MC 0811, La Jolla, CA 92093 USA.
EM wwu@ucsd.edu; merefin2@illinois.edu; gregorij@eecs.berkely.edu;
   pooja.agarwal.mit@gmail.com; klara@illinois.edu;
   bajcsy@eecs.berkeley.edu
FU Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [0964081] Funding Source: National Science Foundation
CR [Anonymous], 2010, POLYCOM TELEPRESENCE
   [Anonymous], 2000, DESIGN ANAL EXPT
   [Anonymous], 2006, CISCO TELEPRESENCE
   [Anonymous], 1997, PSYCHOPHYSICS FUNDAM
   APTEKER RT, 1995, IEEE MULTIMEDIA, V2, P32, DOI 10.1109/93.410510
   Baker H.Harlyn., 2005, ACM Trans. Multimedia Comput. Commun. Appl, V1, P190, DOI DOI 10.1145/1062253.1062258
   Chang YC, 1998, P SOC PHOTO-OPT INS, V3299, P173, DOI 10.1117/12.320108
   Dahlhaus R., 2003, Highly Structured Stochastic Systems
   De Silva DVSX, 2010, IEEE IMAGE PROC, P4013, DOI 10.1109/ICIP.2010.5653353
   Hacker S., 2000, MP3: The Definitive Guide, V1st
   Howard I. P., 2002, Seeing in depth, V2
   IAI S, 1993, GLOBECOM '93 COMMUNICATIONS FOR A CHANGING WORLD, CONFERENCE RECORD, P394, DOI 10.1109/GLOCOM.1993.318049
   ITU, 2009, METH SUBJ ASS QUAL T
   ITU, 1992, ITU T RECOMM T
   Jain A.K., 1981, ALGORITHMS CLUSTERIN
   Jin ZX, 2007, LECT NOTES COMPUT SC, V4551, P605
   Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394
   KIES J. K., 1997, THESIS VIRGINIA POLY
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   LUEBKE D., 2002, MORGAN KAUFMANN SERI
   Narayanan PJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P3, DOI 10.1109/ICCV.1998.710694
   Ott DE, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P147
   Pinson M, 2003, PROC SPIE, V5150, P573, DOI 10.1117/12.509908
   Simons DJ, 2005, TRENDS COGN SCI, V9, P16, DOI 10.1016/j.tics.2004.11.006
   Steinbach E, 2011, IEEE SIGNAL PROC MAG, V28, P87, DOI 10.1109/MSP.2010.938753
   Steinmetz R, 1996, IEEE J SEL AREA COMM, V14, P61, DOI 10.1109/49.481694
   Towles H., 2002, INT WORKSHOP IMMERSI
   Vasudevan G.K. R. B. Ramanarayan., 2010, Proceedings of the first annual ACM SIGMM conference on Multimedia systems, MMSys '10, P281
   Vasudevan R, 2011, IEEE T MULTIMEDIA, V13, P573, DOI 10.1109/TMM.2011.2123871
   Wu W., 2011, P 19 ACM INT C MULTI, P13
   Wu WM, 2008, INT CON DISTR COMP S, P647, DOI 10.1109/ICDCS.2008.47
   Wurmlin S., 2003, 3D video fragments: Dynamic point samples for real-time free-viewpoint video
NR 32
TC 4
Z9 5
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 3
SU S
AR 39
DI 10.1145/2348816.2348818
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021VG
UT WOS:000309912900002
DA 2024-07-18
ER

PT J
AU Snidaro, L
   Visentini, I
   Foresti, GL
AF Snidaro, Lauro
   Visentini, Ingrid
   Foresti, Gian Luca
TI Fusing Multiple Video Sensors for Surveillance
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Multicamera video surveillance
ID TRACKING
AB Real-time detection, tracking, recognition, and activity understanding of moving objects from multiple sensors represent fundamental issues to be solved in order to develop surveillance systems that are able to autonomously monitor wide and complex environments. The algorithms that are needed span therefore from image processing to event detection and behaviour understanding, and each of them requires dedicated study and research. In this context, sensor fusion plays a pivotal role in managing the information and improving system performance. Here we present a novel fusion framework for combining the data coming from multiple and possibly heterogeneous sensors observing a surveillance area.
C1 [Snidaro, Lauro; Visentini, Ingrid; Foresti, Gian Luca] Univ Udine, Dept Math & Comp Sci, I-33100 Udine, Italy.
C3 University of Udine
RP Snidaro, L (corresponding author), Univ Udine, Dept Math & Comp Sci, Via Margareth 3, I-33100 Udine, Italy.
EM lauro.snidaro@uniud.it
RI Snidaro, Lauro/AAJ-8072-2021; Snidaro, Lauro/F-6897-2017
OI Snidaro, Lauro/0000-0003-3828-9017
FU European Defence Agency in DAFNE (Distributed and Adaptive multisensor
   FusioN Engine) [A-0830-RT-GC]
FX This work was supported by the European Defence Agency in DAFNE
   (Distributed and Adaptive multisensor FusioN Engine) project
   A-0830-RT-GC.
CR Aghajan H, 2009, MULTI-CAMERA NETWORKS: PRINCIPLES AND APPLICATIONS, P1
   [Anonymous], P 11 INT C INF FUS C
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   BAKER K, 1989, J MARKET RES SOC, V31, P153
   Blum RickS., 2005, Signal Processing and Communications
   CHATEAU T, 2006, P EUR C COMP VIS
   Collins RT, 2001, P IEEE, V89, P1456, DOI 10.1109/5.959341
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   DURRANTWHYTE HF, 1988, INT J ROBOT RES, V7, P97, DOI 10.1177/027836498800700608
   Grabner H., 2006, P BMVC, V1, P4756
   GRABSER H., 2008, P INT C PATT REC
   Grossmann P, 1998, GEC J TECHNOL, V15, P27
   HALL DL, 1993, P SOC PHOTO-OPT INS, V1956, P98, DOI 10.1117/12.155076
   Hall DL, 1997, P IEEE, V85, P6, DOI [10.1109/5.554205, 10.1109/ISCAS.1998.705329]
   Jain A.K., 1999, 2 INT C AUDIO VISUAL, P182
   Jephcott J, 1998, J MARKET RES SOC, V40, P185
   Nguyen HT, 2006, INT J COMPUT VISION, V69, P277, DOI 10.1007/s11263-006-7067-x
   Oza NC, 2005, IEEE SYS MAN CYBERN, P2340
   Pérez P, 2004, P IEEE, V92, P495, DOI 10.1109/JPROC.2003.823147
   Petrovic N, 2008, LECT NOTES COMPUT SC, V5259, P775, DOI 10.1007/978-3-540-88458-3_70
   Ross Arun, 2004, 2004 12th European Signal Processing Conference (EUSIPCO), P1221
   SNIDARO L, 2009, INTELLIGENT VIDEO SU, P363
   Snidaro L, 2007, IEEE T SYST MAN CY B, V37, P1044, DOI 10.1109/TSMCB.2007.895331
   Snidaro L, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P364, DOI 10.1109/AVSS.2009.67
   Tomasi C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1441
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yin Z., 2008, APPL COMPUTER VISION, P1
NR 30
TC 7
Z9 9
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2012
VL 8
IS 1
AR 7
DI 10.1145/2071396.2071403
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 894AV
UT WOS:000300400200007
DA 2024-07-18
ER

PT J
AU Agarwal, P
   Prabhakaran, B
AF Agarwal, Parag
   Prabhakaran, Balakrishnan
TI Blind Robust Watermarking of 3d Motion Data
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Security; Watermarking; encoding; decoding; blind; spatial
AB The article addresses the problem of copyright protection for 3D motion-captured data by designing a robust blind watermarking mechanism. The mechanism segments motion capture data and identifies clusters of 3D points per segment. A watermark can be embedded and extracted within these clusters by using a proposed extension of 3D quantization index modulation. The watermarking scheme is blind in nature and the encoded watermarks are shown to be imperceptible, and secure. The resulting hiding capacity has bounds based on cluster size. The watermarks are shown to be robust against attacks such as uniform affine transformations (scaling, rotation, and translation), cropping, reordering, and noise addition. The time complexity for watermark embedding and extraction is estimated as O(n log n) and O(n(2) log n), respectively.
C1 [Agarwal, Parag; Prabhakaran, Balakrishnan] Univ Texas Dallas, Dept Comp Sci, Dallas, TX 75083 USA.
C3 University of Texas System; University of Texas Dallas
RP Agarwal, P (corresponding author), Univ Texas Dallas, Dept Comp Sci, Dallas, TX 75083 USA.
EM pxa016500@utdallas.edu
FU Funding Agency; Army Research Office [48645-MA]; National Science
   Foundation [0237954]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [0237954] Funding Source: National
   Science Foundation
FX This research was supported by the Funding Agency, Army Research Office
   grant 48645-MA and National Science Foundation under Grant No. 0237954.
CR AGARWAL P, 2006, P ACM MULT SEC WORKS
   AGARWAL P, 2007, P ACM MULT SEC WORKS
   Alface PR, 2005, IEEE IMAGE PROC, P633
   [Anonymous], P 22 ANN C COMP GRAP
   BENEDENS O, 2002, P INF HID NOORDW NET, P177
   Bhatkar S, 2006, P IEEE S SECUR PRIV, P48, DOI 10.1109/SP.2006.12
   BODO Y, 2003, P IEEE INT C IM PROC
   Bors AG, 2006, IEEE T IMAGE PROCESS, V15, P687, DOI 10.1109/TIP.2005.863116
   Bruderlin Armin., 1995, Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '95, P97, DOI DOI 10.1145/218380.218421
   Byeong-Seob Ko, 2005, IEEE Transactions on Multimedia, V7, P212, DOI 10.1109/TMM.2005.843366
   Cayre F, 2003, IEEE T SIGNAL PROCES, V51, P939, DOI 10.1109/TSP.2003.809380
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Cho JW, 2007, IEEE T SIGNAL PROCES, V55, P142, DOI 10.1109/TSP.2006.882111
   Choi KJ, 2000, J VISUAL COMP ANIMAT, V11, P223, DOI 10.1002/1099-1778(200012)11:5<223::AID-VIS236>3.0.CO;2-5
   COOPER S, 2007, ACM SIGGRAPH PAPERS
   Cotting D, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P233, DOI 10.1109/SMI.2004.1314510
   Cox I., 2001, Digital Watermarking
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   GOLAM A, 2000, P GRAPH INT C, V45, P52
   Gomes J, 2000, COMP ANIM CONF PROC, P62, DOI 10.1109/CA.2000.889039
   Harte T, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P661, DOI 10.1109/ICIP.2002.1039057
   Huang JW, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL III, PROCEEDINGS, P627
   JEONG IK, 2001, P VIRT SYST MULT VSM, P761
   JIN Y, 2007, P 14 INT C MULT MOD, P14
   JIN Y, 2007, P 7 INT C VIRT SYST
   KIM HJ, 2003, PAC RIM WORKSH DIG
   KIM TH, 2000, P EUROGRAPHICS, V19, P189
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Lee J, 1999, COMP GRAPH, P39
   Lemma AN, 2003, IEEE T SIGNAL PROCES, V51, P1088, DOI 10.1109/TSP.2003.809372
   Li C, 2007, MULTIMED TOOLS APPL, V35, P55, DOI 10.1007/s11042-007-0119-6
   Li W, 2006, IEEE T MULTIMEDIA, V8, P60, DOI 10.1109/TMM.2005.861291
   LIM IS, 2002, P IEEE INT MULT EXP, V9, P26
   LU CS, 2000, P 15 INT C PATT REC, V3, P286
   LU CS, 1916, P INT C PATT REC, V2, P552
   MALIK H, 2004, P IEEE INT C AC SPEE, V5, P385
   MANSOUR, 2001, P INT C MULT EXP
   Menache Alberto, 2000, Understanding motion capture for computer animation and video games
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Ohbuchi R, 2004, 2004 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P392, DOI 10.1109/CW.2004.70
   Ohbuchi R., 2001, Graphics Interface, P9
   PANKAJAKSHAN V, 2006, P 8 ACM MULT SEC WOR
   Parent Rick., 2001, COMPUTER ANIMATION
   PAWAR M, 2007, P 14 INT C MULT MOD, P446
   PETROVIC R, 2001, P INT C TEL MOD SAT, V1, P227
   RONDAO AP, 2007, P IEEE INT C IM PROC, P465
   SALVADOR S, 2007, P C INT DAT ANAL
   Su K, 2005, IEEE T MULTIMEDIA, V7, P43, DOI 10.1109/TMM.2004.840617
   UCCHEDDU F, 2004, P 2004 MULT SEC WORK, P143
   Wang CM, 2005, COMPUT GRAPH FORUM, V24, P591, DOI 10.1111/j.1467-8659.2005.00884.x
   Wang XY, 2006, IEEE T SIGNAL PROCES, V54, P4835, DOI 10.1109/TSP.2006.881258
   WU C, 2002, P SPIE C SEC WAT MUL, V3971, P382
   XIAI S, 2006, P 8 INT C SIGN PROC, V1, P16
   YAMAZAKI S, 2004, P PAC RIM WORKSH DIG, P177
   YEO I, 2003, IEEE T SPEECH AUDIO
   Zafeiriou S, 2005, IEEE T VIS COMPUT GR, V11, P596, DOI 10.1109/TVCG.2005.71
   Zhang J, 2001, XIV BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P179, DOI 10.1109/SIBGRAPI.2001.963053
   Zhang J, 2007, IEEE T CIRCUITS-II, V54, P205, DOI 10.1109/TCSII.2006.886247
NR 58
TC 2
Z9 2
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2010
VL 6
IS 1
AR 02
DI 10.1145/1671954.1671956
PG 32
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 563VL
UT WOS:000275163200002
DA 2024-07-18
ER

PT J
AU Li, MZ
   Claypool, M
   Kinicki, R
AF Li, Mingzhe
   Claypool, Mark
   Kinicki, Robert
TI Playout Buffer and Rate Optimization for Streaming over IEEE 802.11
   Wireless Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Performance; Multimedia networking; playout buffer;
   streaming rate; wireless networks
ID VIDEO
AB Most streaming rate selection and buffer optimization algorithms are developed for wired networks and can perform poorly over wireless networks. Wireless MAC layer behavior, such as rate adaptation, retransmissions, and medium sharing, can significantly degrade the effectiveness of current streaming algorithms. This article presents the Buffer and Rate Optimization for Streaming (BROS) algorithm to improve streaming performance. BROS uses a bandwidth estimation tool designed specifically for wireless networks and models the relationship between buffer size, streaming data rate, and available bandwidth distribution. BROS optimizes the streaming data rate and initial buffer size, resulting in a high data rate but with few frame losses and buffer underflow events, while still keeping a small initial buffer delay. BROS is implemented in the Emulated Streaming (EmuS) client-server system and evaluated on an IEEE 802.11 wireless testbed with various wireless conditions. The evaluation shows that BROS can effectively optimize the streaming rate and initial buffer size based on wireless network bandwidth conditions, thus achieving better performance than static rate or buffer selection and jitter removal buffers.
C1 [Li, Mingzhe; Claypool, Mark; Kinicki, Robert] Worcester Polytech Inst, Dept Comp Sci, Worcester, MA 01609 USA.
C3 Worcester Polytechnic Institute
RP Claypool, M (corresponding author), Worcester Polytech Inst, Dept Comp Sci, 100 Inst Rd, Worcester, MA 01609 USA.
EM claypool@cs.wpi.edu
RI Li, Mingzhe/JLK-8164-2023; Claypool, Mark/ABC-5316-2020
CR ANGRISANI L, 2006, P 1 INT S WIR PERV C, P1
   BAI G, 2004, P WIR NETW EM TECHN, P596
   BIRNEY B, 2004, REDUCING START UP LA
   BOLOT JC, 1994, IEEE INFOCOM SER, P1216, DOI 10.1109/INFCOM.1994.337568
   Cen S, 2003, IEEE ACM T NETWORK, V11, P703, DOI 10.1109/TNET.2003.818187
   Chen MH, 2005, IEEE WIREL COMMUN, V12, P32, DOI 10.1109/MWC.2005.1497856
   Conklin GJ, 2001, IEEE T CIRC SYST VID, V11, P269, DOI 10.1109/76.911155
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Delgrossi L., 1993, Proceedings ACM Multimedia 93, P99, DOI 10.1145/166266.166277
   DEMIRCIN MU, 2005, P IEEE INT C MULT EX, P1250
   FENG W, 1995, P IS T SPIE S MULT C, P234
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Fujimoto K, 2002, GLOB TELECOMM CONF, P2451
   Hsu CY, 1999, IEEE J SEL AREA COMM, V17, P756, DOI 10.1109/49.768193
   JACOBS S, 1996, P WORLD WID WEB CONS
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   Kazantzidis M, 2003, ITRE2003: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: RESEARCH AND EDUCATION, P326, DOI 10.1109/ITRE.2003.1270631
   Kohler E, 2006, ACM SIGCOMM COMP COM, V36, P27, DOI 10.1145/1151659.1159918
   Kuang TB, 2004, COMPUT COMMUN, V27, P538, DOI 10.1016/j.comcom.2003.08.019
   Lakshminarayanan K., 2004, IMC 04, P314
   Lambrecht CJV, 1996, P SOC PHOTO-OPT INS, V2668, P450, DOI 10.1117/12.235440
   Laoutaris N, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-10, CONFERENCE RECORD, P969, DOI 10.1109/ICC.2001.937381
   Li F, 2005, LECT NOTES COMPUT SC, V3431, P189
   LI K, 2001, P 8 INT WORKSH INT D, P181
   Li M., 2005, ACM Transactions on Internet Technology, V5, P601, DOI 10.1145/1111627.1111629
   Li M., 2005, Proceedings of the 15th International Workshop on Network and Operating Systems Support for Digital Audio and Video. NOSSDAV 2005, P33, DOI 10.1145/1065983.1065993
   Li Mingzhe, 2008, P 33 IEEE C LOC COMP
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   LIN S, 1984, IEEE COMMUN MAG, V22, P5, DOI 10.1109/MCOM.1984.1091865
   Moon SB, 1998, MULTIMEDIA SYST, V6, P17, DOI 10.1007/s005300050073
   MUNDUR P, 1999, NETWORK DELAY JITTER
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Prasad R, 2003, IEEE NETWORK, V17, P27, DOI 10.1109/MNET.2003.1248658
   RAMJEE R, 1994, IEEE INFOCOM SER, P680, DOI 10.1109/INFCOM.1994.337672
   Steinbach E, 2001, IEEE IMAGE PROC, P962, DOI 10.1109/ICIP.2001.959207
   Stockhammer T, 2004, IEEE T MULTIMEDIA, V6, P268, DOI 10.1109/TMM.2003.822795
   van Beek P, 2004, PROC SPIE, V5308, P647, DOI 10.1117/12.527629
   van Beek P., 2005, P IEEE INT C IM PROC, V2, P173
   Wang Z, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P65, DOI 10.1109/ICIP.2002.1038904
   Wang Z., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P327
   Yang F, 2004, IEEE J SEL AREA COMM, V22, P777, DOI 10.1109/JSAC.2004.826008
   YANG G, 2004, P IFIP IEEE MAN MULT, P26
   Yuang MC, 1997, IEEE J SEL AREA COMM, V15, P136, DOI 10.1109/49.552064
   Yuang MC, 1996, 1996 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS - CONVERGING TECHNOLOGIES FOR TOMORROW'S APPLICATIONS, VOLS. 1-3, P1365, DOI 10.1109/ICC.1996.533632
   Zink M, 2003, LECT NOTES COMPUT SC, V2707, P137
NR 45
TC 6
Z9 6
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2009
VL 5
IS 3
AR 26
DI 10.1145/1556134.1556143
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 504DV
UT WOS:000270595600009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Moncrieff, S
   Venkatesh, S
   West, G
AF Moncrieff, Simon
   Venkatesh, Svetha
   West, Geoff
TI Dynamic Privacy Assessment in a Smart House Environment Using Multimodal
   Sensing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Human Factors; Assisted living; privacy; audio; surveillance and
   monitoring; context aware
AB Surveillance applications in private environments such as smart houses require a privacy management policy if such systems are to be accepted by the occupants of the environment. This is due to the invasive nature of surveillance, and the private nature of the home. In this article, we propose a framework for dynamically altering the privacy policy applied to the monitoring of a smart house based on the situation within the environment. Initially the situation, or context, within the environment is determined; we identify several factors for determining environmental context, and propose methods to quantify the context using audio and binary sensor data. The context is then mapped to an appropriate privacy policy, which is implemented by applying data hiding techniques to control access to data gathered from various information sources. The significance of this work lies in the examination of privacy issues related to assisted-living smart house environments. A single privacy policy in such applications would be either too restrictive for an observer, for example, a carer, or too invasive for the occupants. We address this by proposing a dynamic method, with the aim of decreasing the invasiveness of the technology, while retaining the purpose of the system.
C1 [Moncrieff, Simon; Venkatesh, Svetha; West, Geoff] Curtin Univ Technol, Dept Comp, Perth, WA 6845, Australia.
C3 Curtin University
RP Moncrieff, S (corresponding author), Curtin Univ Technol, Dept Comp, GPO Box U1987, Perth, WA 6845, Australia.
OI Venkatesh, Svetha/0000-0001-8675-6631
FU ARC discovery [DP 0449437]
FX The authors acknowledge ARC discovery Grant DP 0449437 ( Homes That
   Sense and Support).
CR Altman I., 1975, ENV SOCIAL BEHAV
   [Anonymous], 1993, Discrete Time Processing of Speech Signals
   [Anonymous], 2004, Proceedings of the 2nd International Conference on Mobile Systems, Applications, and Services
   [Anonymous], 2003, P SIGCHI C HUM FACT, DOI DOI 10.1145/642611.642635
   [Anonymous], 1992, LECT WAVELETS
   [Anonymous], 2000, Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations
   BELLOTTI V, 1993, PROCEEDINGS OF THE THIRD EUROPEAN CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK ( ECSCW 93 ), P77
   BOYER JP, 2006, P 3 INT C SEC PERV C
   Chen JF, 2005, LECT NOTES COMPUT SC, V3468, P47, DOI 10.1385/1-59259-820-x:047
   DAS SK, 2004, P INT C SMART HOM HL
   Dufaux Frederic., 2006, C COMPUTER VISION PA, P160
   Ellis D. R., 2004, Proceedings of a Workshop on Equine Recurrent Laryngeal Neuropathy, Stratford-upon-Avon, UK, 7-10 September, 2003, P39, DOI 10.1145/1026653.1026659
   Fidaleo D.A., 2004, Proceedings of ACM Workshop on Video Surveillance Sensor Networks (VSSN), P46
   Helal S, 2003, PROCEEDINGS OF THE FIRST IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS (PERCOM 2003), P531, DOI 10.1109/PERCOM.2003.1192785
   HONG D, 2005, 7 INT C HUM COMP INT, V111, P1
   Lederer S, 2004, PERS UBIQUIT COMPUT, V8, P440, DOI 10.1007/s00779-004-0304-9
   MARTINEZPONTE I, 2005, P WORKSH INT KNOWL S
   MONCRIEFF S, 2007, ACM MULTIMEDIA 2007, P671
   Moncrieff S, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P2034
   Moncrieff S, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1230812.1230814
   Moncrieff S, 2007, PERVASIVE MOB COMPUT, V3, P74, DOI 10.1016/j.pmcj.2006.07.003
   Neustaedter C, 2003, LECT NOTES COMPUT SC, V2864, P297
   Nixon P.A., 2004, SMART ENV, P249
   Rabiner L. R., 1978, SIGNAL PROCESSING SE
   Senior A, 2005, IEEE SECUR PRIV, V3, P50, DOI 10.1109/MSP.2005.65
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   West G, 2005, P INT COMP SOFTW APP, P335
   Wickramasuriya J., 2004, MULTIMEDIA '04, P48, DOI DOI 10.1145/1027527.1027537
   WILSON DH, 2005, THESIS CARNEGIE MELL
   Zhang T, 1999, INT CONF ACOUST SPEE, P3001, DOI 10.1109/ICASSP.1999.757472
NR 30
TC 24
Z9 26
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2008
VL 5
IS 2
AR 10
DI 10.1145/1413862.1413863
PG 29
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 376AU
UT WOS:000261155900001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Thouin, F
   Coates, M
AF Thouin, Frederic
   Coates, Mark
TI Equipment Allocation in Video-on-Demand Network Deployments
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT ACM Multimedia Conference 2007
CY SEP 24-29, 2007
CL Augsburg, GERMANY
SP ACM
DE Design; Management; Video-on-demand; equipment allocation; resource
   allocation; optimization
ID LOCATION PROBLEM; SYSTEMS; COST
AB Video-on-Demand (VoD) services are very user-friendly, but also complex and resource demanding. Deployments involve careful design of many mechanisms where content attributes and usage models should be taken into account. We define, and propose a methodology to solve, the VoD Equipment Allocation Problem of determining the number and type of streaming servers with directly attached storage (VoD servers) to install at each potential location in a metropolitan area network topology such that deployment costs are minimized. We develop a cost model for VoD deployments based on streaming, storage and transport costs and train a parametric function that maps the amount of available storage to a worst-case hit ratio. We observe the impact of having to determine the amount of storage and streaming cojointly, and determine the minimum demand required to deploy replicas as well as the average hit ratio at each location. We observe that common video-on-demand server configurations lead to the installation of excessive storage, because a relatively high hit-ratio can be achieved with small amounts of storage so streaming requirements dominate.
C1 [Thouin, Frederic; Coates, Mark] McGill Univ, Montreal, PQ H3A 2T5, Canada.
C3 McGill University
RP Thouin, F (corresponding author), McGill Univ, Montreal, PQ H3A 2T5, Canada.
EM frederic.thouin@mail.mcgill.ca
CR Almeida JM, 2004, IEEE T MULTIMEDIA, V6, P356, DOI 10.1109/TMM.2003.822796
   [Anonymous], 1990, Discrete Location Theory
   COUCH K, 2005, CONVERGE NETWORK DIG
   FLETCHER R., 1987, PRACTICAL METHODS OP
   GUMMADI K, 2003, P ACM S OP SYST PRIN
   KARLSSON M, 2002, UNIFIED FRAMEWORK EV
   KIM SJ, 2003, P S TRENDS COMM
   Krishnan P, 2000, IEEE ACM T NETWORK, V8, P568, DOI 10.1109/90.879344
   Laoutaris N, 2005, COMPUT NETW, V47, P409, DOI 10.1016/j.comnet.2004.07.020
   MARKMAN J, 2006, 2007 IS SHOWTIME VID
   MASA M, 2003, P INT PERF COMP COMM
   Mundur P, 2004, IEEE T MULTIMEDIA, V6, P129, DOI 10.1109/TMM.2003.819757
   NGUYEN T, 2003, P AUSTR TEL NETW APP
   Tang WKS, 2004, IEEE T BROADCAST, V50, P16, DOI 10.1109/TBC.2003.822983
   THOUIN F, 2007, P INT TEL C ITC OTT
   Thouin F, 2007, IEEE NETWORK, V21, P42, DOI 10.1109/MNET.2007.334311
   VINOKUROV A, 2005, P EUR NEXT GEN INT D
   WANG B, 2002, P IEEE INF
   Wu LY, 2006, COMPUT OPER RES, V33, P1226, DOI 10.1016/j.cor.2004.09.012
   YANG M, 2003, P IEEE INT C COMM AN
   YU H, 2006, P ACM EUR LEUV BELG
   [No title captured]
   [No title captured]
NR 23
TC 8
Z9 8
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2008
VL 5
IS 1
AR 5
DI 10.1145/1404880.1404885
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 376AS
UT WOS:000261155700005
DA 2024-07-18
ER

PT J
AU Reddy, ALN
   Wyllie, J
   Wijayaratne, KBR
AF Reddy, A. L. N.
   Wyllie, Jim
   Wijayaratne, K. B. R.
TI Disk Scheduling in a Multimedia I/O System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; I/O systems; disk scheduling; multimedia applications;
   real-time; performance evaluation
ID FILE SYSTEM; DESIGN
AB This article provides a retrospective of our original paper by the same title in the Proceedings of the First ACM Conference on Multimedia, published in 1993. This article examines the problem of disk scheduling in a multimedia I/O system. In a multimedia server, the disk requests may have constant data rate requirements and need guaranteed service. We propose a new scheduling algorithm, SCAN-EDF, that combines the features of SCAN type of seek optimizing algorithm with an Earliest Deadline First (EDF) type of real-time scheduling algorithm. We compare SCAN-EDF with other scheduling strategies and show that SCAN-EDF combines the best features of both SCAN and EDF. We also investigate the impact of buffer space on the maximum number of video streams that can be supported.
   We show that by making the deadlines larger than the request periods, a larger number of streams can be supported. We also describe how we extended the SCAN-EDF algorithm in the PRISM multimedia architecture. PRISM is an integrated multimedia server, designed to satisfy the QOS requirements of multiple classes of requests. Our experience in implementing the extended SCAN-EDF algorithm in a generic operating system is discussed and performance metrics and results are presented to illustrate how the SCAN-EDF extensions and implementation strategies have succeeded in meeting the QOS requirements of different classes of requests.
C1 [Reddy, A. L. N.] Texas A&M Univ, Dept Elect Engn, College Stn, TX 77843 USA.
   [Wyllie, Jim] IBM Almaden Res Ctr, San Jose, CA 95120 USA.
   [Wijayaratne, K. B. R.] Adaptec Inc, Milpitas, CA 95035 USA.
C3 Texas A&M University System; Texas A&M University College Station;
   International Business Machines (IBM)
RP Reddy, ALN (corresponding author), Texas A&M Univ, Dept Elect Engn, 214 Zachry, College Stn, TX 77843 USA.
EM reddy@ee.tamu.edu; wyllie@almaden.ibm.com; ravi_wijayaratne@adaptec.com
CR ABBOTT RK, 1990, PROCEEDINGS : 11TH REAL-TIME SYSTEMS SYMPOSIUM, P113, DOI 10.1109/REAL.1990.128736
   ANDERSON DP, 1992, ACM T COMPUT SYST, V10, P311, DOI 10.1145/138873.138875
   ANDERSON DP, 1991, UCBCSD91646
   BLANQUEE J, 1999, FREEBSD C
   BOLOSKY WJ, 1997, P 16 ACM S OP SYST P, P212
   Bosch P, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P277, DOI 10.1109/MMCS.1999.778377
   Bruno J, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P400, DOI 10.1109/MMCS.1999.778459
   CARD R, 1998, LINUX KERNEL BOOK, pCH3
   CHANG E, 1994, P SOC PHOTO-OPT INS, V2185, P208, DOI 10.1117/12.171778
   Chang RI, 2000, REAL-TIME SYST, V19, P149, DOI 10.1023/A:1008192415994
   Chen H. J., 1995, Proceedings of the International Conference on Multimedia Computing and Systems (Cat. No.95TH8066), P65, DOI 10.1109/MMCS.1995.484909
   CHEN MS, 1994, P ACM MULT, P391
   CHEN SZ, 1991, REAL-TIME SYST, V3, P307, DOI 10.1007/BF00364960
   Chiueh TC, 1996, P SOC PHOTO-OPT INS, V2604, P133, DOI 10.1117/12.230049
   FREEDMAN CS, 1995, P 1995 ACM SIGMOD IN, P352
   Haskin RL, 1998, IBM J RES DEV, V42, P185, DOI 10.1147/rd.422.0185
   JAYANTA KD, 2004, P ACM MULT C, P25
   JEFFAY K, 1991, PROCEEDING : TWELFTH REAL-TIME SYSTEMS SYMPOSIUM, P129, DOI 10.1109/REAL.1991.160366
   KIM MY, 1986, IEEE T COMPUT, V35, P978, DOI 10.1109/TC.1986.1676699
   LEHOCZKY JP, 1990, PROCEEDINGS : 11TH REAL-TIME SYSTEMS SYMPOSIUM, P201, DOI 10.1109/REAL.1990.128748
   LIN TH, 1991, P SIMM C, P31
   LIU CL, 1973, J ACM, V20, P46, DOI 10.1145/321738.321743
   LOUGHER P, 1993, COMPUT J, V36, P32, DOI 10.1093/comjnl/36.1.32
   LUND K, 2003, P 11 ACM INT C MULT, P65
   Makaroff D, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P143, DOI 10.1145/266180.266354
   MARTIN C, 1996, FELLINI MULTIMEDIA S, pCH5
   Mokbel MF, 2004, PROC INT CONF DATA, P498, DOI 10.1109/ICDE.2004.1320022
   Molano A, 1997, REAL TIM SYST SYMP P, P155, DOI 10.1109/REAL.1997.641278
   Patterson D. A., 1988, SIGMOD Record, V17, P109, DOI 10.1145/971701.50214
   Plagemann T, 2000, COMPUT COMMUN, V23, P267, DOI 10.1016/S0140-3664(99)00180-2
   Reddy A. L. N., 1993, P ACM MULT C, P225
   REDDY ALN, 1989, IEEE T COMPUT, V38, P1680, DOI 10.1109/12.40846
   REDDY ALN, 1996, COMMUNICATIONS HDB, pCH106
   REDDY ALN, 1992, P INT S COMP ARCH, P308
   ROMPOGIANNAKIS T, 1998, P ACM MULT C, P297
   ROSE O, 1995, 101 U WURZB I COMP S
   Salem K., 1986, P IEEE DATA ENG C, P336
   Shenoy P, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P457, DOI 10.1145/319463.319686
   Shenoy P. J., 1998, Performance Evaluation Review, V26, P44, DOI 10.1145/277858.277871
   SHENOY PJ, 1998, P ACM SPIE MULT COMP, P124
   SHIH WK, 1992, MODIFIED RATE MONOTO
   WIJAYARATNE KBR, 2001, THESIS A M U TEXAS
   Wijayaratne R, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P487, DOI 10.1109/MMCS.1999.779250
   Wijayaratne R, 2000, MULTIMEDIA SYST, V8, P57, DOI 10.1007/s005300050005
   WIJAYARATNE R, 1999, ACM SPIE MULTIMED CO, V3654, P216
   WIJAYARATNE R, 2001, P ACM MULT 2001 OTT, P270
   YEE J, 1992, DISK SCHEDULING POLI
   YU PS, 1993, MULTIMEDIA SYSTEMS, V1, P99
NR 48
TC 21
Z9 25
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2005
VL 1
IS 1
BP 37
EP 59
DI 10.1145/1047936.1047941
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DU
UT WOS:000205012200005
DA 2024-07-18
ER

PT J
AU Fränti, P
   Fazal, N
AF Franti, Pasi
   Fazal, Nancy
TI Design Principles for Content Creation in Location-Based Games
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Content creation; quality evaluation; user-generated content (UGC);
   user-generated media (UGM); location-based game (LBG); orienteering;
   treasure hunting; O-Mopsi
ID USER-GENERATED CONTENT; MOBILE GAME; MOTIVATION; EDUCATION
AB Location-based games have been around since 2000 across various fields, including education, health, and entertainment. The main challenge facing such games is content generation. In contrast to normal games, content in location-based games is inherently dependent on location. The biggest challenge is the availability of the content globally. Other challenges include player engagement, enjoyable interactions with the real-world environment, safety, and customizability based on player performance and preference. While crowdsourcing has often been adopted as a tool for content creation, this approach requires quality control. Designing high-quality content requires detailed guidelines. In this paper, we introduce design principles for the creation of high-quality content that can survive for long periods of time. These principles are derived from ten years of experience running our in-house orienteering treasure-hunt game called O-Mopsi, which represents a case study in this paper. O-Mopsi allows players to visit pre-defined locations. The design principles are expected to be generalizable to other location-based games as well as to the creation of sightseeing tours more generally.
C1 [Franti, Pasi; Fazal, Nancy] Univ Eastern Finland, Sch Comp, Lansikatu 15, Kuopio 80110, Finland.
C3 University of Eastern Finland
RP Fränti, P (corresponding author), Univ Eastern Finland, Sch Comp, Lansikatu 15, Kuopio 80110, Finland.
EM franti@cs.uef.fi; fazal@cs.uef.fi
OI Franti, Pasi/0000-0002-9554-2827; Fazal, Nancy/0000-0002-7099-1327
CR Alavesa P, 2015, PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA (MUM 2015), P64, DOI 10.1145/2836041.2836047
   Alderman N., 2012, Zombies, Run!
   Andone I, 2017, PROCEEDINGS OF THE 19TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI '17), DOI 10.1145/3098279.3122145
   Arango-López J, 2021, UNIVERSAL ACCESS INF, V20, P465, DOI 10.1007/s10209-020-00769-w
   Bell M, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P123
   Benford S., 2004, INT C ADV COMP ENT T
   Bengs A, 2015, INTERACT DES ARCHIT, P85
   Bindoff I, 2016, JMIR SERIOUS GAMES, V4, DOI 10.2196/games.6258
   Budak C., 2011, P 20 INT C WORLD WID, P665, DOI DOI 10.1145/1963405.1963499
   Caserman P, 2020, JMIR SERIOUS GAMES, V8, DOI 10.2196/19037
   Casey S., 2007, ACE 07 P INT C ADV C, P9, DOI 10.1145/1255047.1255050
   Celino Irene, 2012, The Semantic Web. 11th International Semantic Web Conference (ISWC 2012). Proceedings, P34, DOI 10.1007/978-3-642-35173-0_3
   Chang Kuo Ping, 2014, P 1 ACM SIGCHI ANN S, P323, DOI [10., DOI 10.1145/2658537.2662975]
   Cochrane T, 2016, INT J MOB BLENDED LE, V8, P44, DOI 10.4018/IJMBL.2016100104
   Erlacher C., 2012, GI FORUM, P230
   Ferreira C, 2019, ENTERTAIN COMPUT, V30, DOI 10.1016/j.entcom.2019.100295
   FitzGerald E, 2012, J COMPUT ASSIST LEAR, V28, P195, DOI 10.1111/j.1365-2729.2012.00481.x
   Fränti P, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3115935
   Fulton Will, 2020, DIGITAL TRENDS
   Giannakos Michail N., 2018, International Journal of Child-Computer Interaction, V18, P27, DOI 10.1016/j.ijcci.2018.06.002
   Giannakos MN, 2013, C&C'13: PROCEEDINGS OF THE 9TH ACM CONFERENCE ON CREATIVITY & COGNITION 2013, P104
   Hajarnis Sanjeet, 2011, Interactive Storytelling. Proceedings 4th International Conference on Interactive Digital Storytelling, ICIDS 2011, P278, DOI 10.1007/978-3-642-25289-1_30
   Heljakka K., 2018, J DIGITAL MEDIA INTE, P75, DOI [10.34624/jdmi.v1i2.955, DOI 10.34624/JDMI.V1I2.955]
   Henriques E., 2015, International Journal of Cartography, V1, P45, DOI 10.1080/23729333.2015.1055110
   Hodson H., 2012, New Scientist, V216, P19
   Huizenga J, 2009, J COMPUT ASSIST LEAR, V25, P332, DOI [10.1111/J.1365-2729.2009.00316.x, 10.1111/j.1365-2729.2009.00316.x]
   Kasapakis V, 2018, COMPUT ENTERTAIN, V16, DOI 10.1145/3161570
   Kasapakis V, 2016, 2016 IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATION (ISCC), P79, DOI 10.1109/ISCC.2016.7543718
   Klopfer E., 2005, MYSTERY MUSEUM A COL
   Klopfer E, 2008, ETR&D-EDUC TECH RES, V56, P203, DOI 10.1007/s11423-007-9037-6
   Kultima Annakaisa., 2009, P MINDTREK2009, P58, DOI DOI 10.1145/1621841.1621854
   La Guardia D., 2012, INT C FUTURE ED, V2nd, P508, DOI [10.1007/s10055-010-0177-3, DOI 10.1007/S10055-010-0177-3]
   Laato S., 2019, P 11 INT C COMPUTER, P616, DOI DOI 10.5220/0007801206160627
   Laato S, 2019, COMPUTER SYSTEMS AND TECHNOLOGIES, P153, DOI 10.1145/3345252.3345286
   Laine TH, 2009, ICALT: 2009 IEEE INTERNATIONAL CONFERENCE ON ADVANCED LEARNING TECHNOLOGIES, P655, DOI 10.1109/ICALT.2009.57
   Lee Celina Wing Yi, 2016, Proceedings of the Association for Information Science and Technology, V53, DOI 10.1002/pra2.2016.14505301033
   Lobb R., 2013, CODERUNNER
   Lochrie M., 2010, P HCI 2010, V24, P474, DOI [10.14236/ewic/HCI2010.59, DOI 10.14236/EWIC/HCI2010.59]
   Ludia Inc, US
   Massung E., 2013, Proceedings of CHI '13, P371, DOI [DOI 10.1145/2470654.2470708, 10.1145/2470654.2470708]
   Matyas S., 2008, ACE 08 P 2008 INT C, P244, DOI 10.1145/1501750.1501806
   Matyas S, 2011, LECT NOTES COMPUT SC, V6972, P331, DOI 10.1007/978-3-642-24500-8_36
   Nery J, 2019, IBER CONF INF SYST, DOI 10.23919/cisti.2019.8760640
   Neustaedter C., 2012, ACM2012 C COMPUTER S, P235, DOI [10.1145/2141512.2141586, DOI 10.1145/2141512.2141586]
   Neustaedter C., 2011, 2011111602 CONN LAB
   Neustaedter C, 2013, PERS UBIQUIT COMPUT, V17, P335, DOI 10.1007/s00779-011-0497-7
   Nova N, 2005, IEEE INTERNATIONAL WORKSHOP ON WIRELESS AND MOBILE TECHNOLOGIES IN EDUCATION, PROCEEDINGS, P21, DOI 10.1109/WMTE.2005.2
   Oppermann L, 2016, LECT NOTES COMPUT SC, V9970, P475, DOI 10.1007/978-3-319-46152-6_18
   Pang C, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300571
   Pocket Tactics, 2021, BEST LOC BAS GAM MOB
   Polson D., 2007, INT C MOBILE TECHNOL, P614, DOI [10.1145/1378063.1378165, DOI 10.1145/1378063.1378165]
   Puja J-C, 2011, 2011 11th IEEE International Conference on Advanced Learning Technologies (ICALT 2011), P42, DOI 10.1109/ICALT.2011.20
   Ruiz-Ariza A., 2020, AUGMENTED REALITY ED, P247
   Scheider S., 2018, Geogames and Geoplay, P131
   Schito J., 2015, AGILE 2015 WORKSH GE, DOI [10.13140/RG.2.1.1322.3844/1, DOI 10.13140/RG.2.1.1322.3844/1]
   Sengupta L., 2021, AIMS APPL COMPUTING, V1, P1
   Sintoris C, 2010, INT J MOB HUM COMPUT, V2, P53, DOI 10.4018/jmhci.2010040104
   Söbke H, 2017, INT J SERIOUS GAMES, V4, P39, DOI 10.17083/ijsg.v4i2.182
   Tüzün H, 2009, COMPUT EDUC, V52, P68, DOI 10.1016/j.compedu.2008.06.008
   Ukpabi DC, 2018, TOUR MANAG PERSPECT, V28, P251, DOI 10.1016/j.tmp.2018.03.006
   Vassilakis K., 2018, EAI ENDORSED T CREAT, V5, P16, DOI [10.4108/eai.7-3-2019.156773, DOI 10.4108/EAI.7-3-2019.156773]
   Wake JD, 2009, INT J MOB BLENDED LE, V1, P12, DOI 10.4018/jmbl.2009090802
   Wetzel R., 2012, Proceedings of the International Conference on the Foundations of Digital Games, FDG'12, (New York, NY, USA), P238
   Zannettou S, 2017, PROCEEDINGS OF THE 2017 INTERNET MEASUREMENT CONFERENCE (IMC'17), P405, DOI 10.1145/3131365.3131390
   US
NR 65
TC 2
Z9 2
U1 4
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2023
VL 19
IS 5
SU S
AR 165
DI 10.1145/3583689
PG 30
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L0SK0
UT WOS:001020437100001
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, WJ
   Lin, LL
   Fan, ZJ
   Liu, JY
AF Wang, Wenjing
   Lin, Lilang
   Fan, Zejia
   Liu, Jiaying
TI Semi-supervised Learning for Mars Imagery Classification and
   Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Mars vision tasks; image classification; image segmentation;
   representation learning; unsupervised learning
AB With the progress of Mars exploration, numerous Mars image data are being collected and need to be analyzed. However, due to the severe train-test gap and quality distortion of Martian data, the performance of existing computer vision models is unsatisfactory. In this article, we introduce a semi-supervised framework for machine vision on Mars and try to resolve two specific tasks: classification and segmentation. Contrastive learning is a powerful representation learning technique. However, there is too much information overlap between Martian data samples, leading to a contradiction between contrastive learning and Martian data. Our key idea is to reconcile this contradiction with the help of annotations and further take advantage of unlabeled data to improve performance. For classification, we propose to ignore inner-class pairs on labeled data as well as neglect negative pairs on unlabeled data, forming supervised inter-class contrastive learning and unsupervised similarity learning. For segmentation, we extend supervised inter-class contrastive learning into an element-wise mode and use online pseudo labels for supervision on unlabeled areas. Experimental results show that our learning strategies can improve the classification and segmentation models by a large margin and outperform state-of-the-art approaches.
C1 [Wang, Wenjing; Lin, Lilang; Fan, Zejia; Liu, Jiaying] Peking Univ, Wangxuan Inst Comp Technol, 128 Zhongguancun North St, Beijing, Peoples R China.
C3 Peking University
RP Liu, JY (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, 128 Zhongguancun North St, Beijing, Peoples R China.
EM daooshee@pku.edu.cn; linlilang@pku.edu.cn; zejia@pku.edu.cn;
   liujiaying@pku.edu.cn
FU National Key Research and Development Program of China [2018AAA0102702];
   National Natural Science Foundation of China [62172020]
FX This work was supported by the National Key Research and Development
   Program of China under Grant No. 2018AAA0102702, and the National
   Natural Science Foundation of China under Contract No. 62172020.
CR Abcouwer N, 2021, AEROSP CONF PROC, DOI 10.1109/AERO50100.2021.9438337
   Azari A. R., 2020, ARXIV
   Bouman KL, 2016, PROC CVPR IEEE, P913, DOI 10.1109/CVPR.2016.105
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Caron Mathilde, 2020, C NEURAL INFORM PROC
   Castano A, 2008, MACH VISION APPL, V19, P467, DOI 10.1007/s00138-007-0081-3
   Castano R, 2007, J FIELD ROBOT, V24, P379, DOI 10.1002/rob.20192
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen X, 2021, AUTOPHAGY, V17, P2054, DOI 10.1080/15548627.2020.1810918
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DeVries T, 2017, PREPRINT
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Doran Gary, 2020, IEEE AEROSPACE C
   Dundar Murat, 2019, EARTH SPACE SCI OPEN, P23
   Estlin T, 2009, SMC-IT 2009: THIRD IEEE INTERNATIONAL CONFERENCE ON SPACE MISSION CHALLENGES FOR INFORMATION TECHNOLOGY, PROCEEDINGS, P257, DOI 10.1109/SMC-IT.2009.38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jin X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3417333
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kang B., 2020, 8 INT C LEARN REPR I
   Khosla P., 2020, C NEUR INF PROC SYST
   Kingma D. P., 2014, arXiv
   Lee D.-H., 2013, PSEUDOLABEL SIMPLE E, V3, P896
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu S., 2021, ARXIV
   Lu ZW, 2017, IEEE T PATTERN ANAL, V39, P486, DOI 10.1109/TPAMI.2016.2552172
   Momennasab Ali, 2021, ARXIV
   Olsson V, 2021, IEEE WINT CONF APPL, P1368, DOI 10.1109/WACV48630.2021.00141
   Ouali Yassine, 2020, IEEE C COMPUTER VISI
   Petkovic M, 2019, IEEE AERO EL SYS MAG, V34, P46, DOI 10.1109/MAES.2019.2915456
   Radosavovic I., 2020, IEEE C COMPUTER VISI
   Rothrock B., 2016, P AIAA SPACE, DOI [10.2514/6.2016-5539, DOI 10.2514/6.2016-5539]
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shallue CJ, 2018, ASTRON J, V155, DOI 10.3847/1538-3881/aa9e09
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Swan RM, 2021, IEEE COMPUT SOC CONF, P1982, DOI 10.1109/CVPRW53098.2021.00226
   Tanaka FHKS, 2019, ARXIV
   Tarvainen A, 2017, ADV NEUR IN, V30
   van den Oord A., 2018, ARXIV
   Wagstaff KL, 2018, AAAI CONF ARTIF INTE, P7867
   Walker SI, 2018, ASTROBIOLOGY, V18, P779, DOI 10.1089/ast.2017.1738
   Wei Colin, 2019, C NEURAL INFORM PROC
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wilhelm T, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12233981
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang YC, 2018, ICMLC 2020: 2020 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING, P145, DOI 10.1145/3383972.3383975
   Zhang Yuhang, 2022, IEEE Transactions on Multimedia
   Zhao Xiangyun, 2021, IEEE C COMPUTER VISI
NR 57
TC 5
Z9 5
U1 9
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2023
VL 19
IS 4
AR 144
DI 10.1145/3572916
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FX8
UT WOS:001011937600006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Le, TNH
   Chen, YH
   Lee, TY
AF Le, Thi-Ngoc-Hanh
   Chen, Ya-Hsuan
   Lee, Tong-Yee
TI Structure-aware Video Style Transfer with Map Art
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Style transfer video; coherence; map art; CNN; MAViNet
AB Changing the style of an image/video while preserving its content is a crucial criterion to access a new neural style transfer algorithm. However, it is very challenging to transfer a new map art style to a certain video in which "content" comprises a map background and animation objects. In this article, we present a novel comprehensive system that solves the problems in transferring map art style in such video. Our system takes as input an arbitrary video, a map image, and an off-the-shelf map art image. It then generates an artistic video without damaging the functionality of the map and the consistency in details. To solve this challenge, we propose a novel network, Map Art Video Network (MAViNet), the tailored objective functions, and a rich training set with rich animation contents and different map structures. We have evaluated our method on various challenging cases and many comparisons with those of the related works. Our method substantially outperforms state-of-the-art methods in terms of visual quality and meets the mentioned criteria in this research domain.
C1 [Le, Thi-Ngoc-Hanh; Chen, Ya-Hsuan; Lee, Tong-Yee] Natl Cheng Kung Univ, Tainan, Taiwan.
C3 National Cheng Kung University
RP Le, TNH (corresponding author), Natl Cheng Kung Univ, Tainan, Taiwan.
EM ngochanh.le1987@gmail.com; arielmiyabi@gmail.com;
   tonylee@mail.ncku.edu.tw
RI Hanh, Le Thi Ngoc/JQI-5639-2023
OI Hanh, Le Thi Ngoc/0000-0001-9667-9780
FU National Science and Technology Council, Taiwan, Republic of China
   [111-2221-E-006-112-MY3, 110-2221-E-006-135-MY3]
FX This work was supported in part by the National Science and Technology
   Council (under Nos. 111-2221-E-006-112-MY3 and 110-2221-E-006-135-MY3),
   Taiwan, Republic of China.
CR Gatys LA, 2015, Arxiv, DOI arXiv:1508.06576
   An J, 2021, PROC CVPR IEEE, P862, DOI 10.1109/CVPR46437.2021.00092
   [Anonymous], 2013, COMPUTATIONAL IMAGIN
   Cetinic E, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3475799
   Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chen Yi-Wen, 2018, P ASIAN C COMPUTER V, P615
   Deng YY, 2021, AAAI CONF ARTIF INTE, V35, P1210
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Dumoulin V, 2017, Arxiv, DOI [arXiv:1610.07629, DOI 10.48550/ARXIV.1610.07629]
   Fairburn Ed, 2021, ED FAIRBURN ORIGINAL
   Gao C., 2018, P AS C COMP VIS
   Gao C, 2019, LECT NOTES COMPUT SC, V11366, P637, DOI 10.1007/978-3-030-20876-9_40
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gooch Amy, 2001, NONPHOTOREALISTIC RE, DOI [10.1201/9781439864173, DOI 10.1201/9781439864173]
   Gupta A, 2017, IEEE I CONF COMP VIS, P4087, DOI 10.1109/ICCV.2017.438
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Champandard AJ, 2016, Arxiv, DOI arXiv:1603.01768
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Kingma D. P., 2014, arXiv
   Li XT, 2019, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR.2019.00393
   Liao J, 2017, Arxiv, DOI arXiv:1705.01088
   Lin Hang, 2022, Pers Ubiquitous Comput, P1, DOI 10.1007/s00779-022-01667-z
   Liu S., 2021, IEEE INT C COMPUTER, P6649
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Nair Vinod, 2010, ICML, DOI DOI 10.5555/3104322.3104425
   Odena A., 2016, DISTILL, V1, P3, DOI [10.23915/distill.00003., DOI 10.23915/DISTILL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Paszke A, 2019, ADV NEUR IN, V32
   Rebelo ADP, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3462634
   Pont-Tuset J, 2018, Arxiv, DOI arXiv:1704.00675
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Ruder M, 2018, INT J COMPUT VISION, V126, P1199, DOI 10.1007/s11263-018-1089-z
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   Shih CY, 2021, MULTIMED TOOLS APPL, V80, P4279, DOI 10.1007/s11042-020-09788-4
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh Saurabh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11234, DOI 10.1109/CVPR42600.2020.01125
   Strothotte T, 2002, NONPHOTOREALISTIC CO
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Xu K, 2021, IEEE T IMAGE PROCESS, V30, P2501, DOI 10.1109/TIP.2021.3052709
   Yeh Mao-Chuang, 2018, ARXIV
   Zhang H, 2019, LECT NOTES COMPUT SC, V11132, P349, DOI 10.1007/978-3-030-11018-5_32
   Zhang LZ, 2020, IEEE WINT CONF APPL, P231, DOI [10.1109/wacv45572.2020.9093632, 10.1109/WACV45572.2020.9093632]
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 51
TC 0
Z9 0
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 131
DI 10.1145/3572030
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700006
DA 2024-07-18
ER

PT J
AU Cao, GF
   Zhou, F
   Liu, KL
   Wang, AJ
   Fan, LD
AF Cao, Gaofeng
   Zhou, Fei
   Liu, Kanglin
   Wang, Anjie
   Fan, Leidong
TI A Decoupled Kernel Prediction Network Guided by Soft Mask for Single
   Image HDR Reconstruction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE High dynamic range image; inverse tone mapping; kernel prediction
   network; image reconstruction
ID DYNAMIC-RANGE EXPANSION; TONE; VISIBILITY
AB Recent works on single image high dynamic range (HDR) reconstruction fail to hallucinate plausible textures, resulting in information missing and artifacts in large-scale under/over-exposed regions. In this article, a decoupled kernel prediction network is proposed to infer an HDR image from a low dynamic range (LDR) image. Specifically, we first adopt a simplemodule to generate a preliminary result, which can precisely estimatewellexposed HDR regions. Meanwhile, an encoder-decoder backbone network with a soft mask guidance module is presented to predict pixel-wise kernels, which is further convolved with the preliminary result to obtain the final HDR output. Instead of traditional kernels, our predicted kernels are decoupled along the spatial and channel dimensions. The advantages of our method are threefold at least. First, ourmodel is guided by the soft mask so that it can focus on the most relevant information for under/over-exposed regions. Second, pixel-wise kernels are able to adaptively solve the different degradations for differently exposed regions. Third, decoupled kernels can avoid information redundancy across channels and reduce the solution space of our model. Thus, our method is able to hallucinate fine details in the under/over-exposed regions and renders visually pleasing results. Extensive experiments demonstrate that our model outperforms state-of-the-art ones.
C1 [Cao, Gaofeng; Wang, Anjie; Fan, Leidong] Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China.
   [Cao, Gaofeng] Peng Cheng Lab, 2199 Lishui Rd, Shenzhen 518055, Guangdong, Peoples R China.
   [Zhou, Fei] Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen, Peoples R China.
   [Zhou, Fei] Guangdong Key Lab Intelligent Informat Proc, 3688 Nanhai Ave, Shenzhen 518060, Guangdong, Peoples R China.
   [Liu, Kanglin] Peng Cheng Lab, 2 Xingke Ist St, Shenzhen, Guangdong, Peoples R China.
   [Wang, Anjie; Fan, Leidong] Shenzhen Key Lab Digital Creat Technol, 2199 Lishui Rd, Shenzhen 518055, Guangdong, Peoples R China.
C3 Peking University; Peng Cheng Laboratory; Shenzhen University; Peng
   Cheng Laboratory
RP Zhou, F (corresponding author), Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen, Peoples R China.; Zhou, F (corresponding author), Guangdong Key Lab Intelligent Informat Proc, 3688 Nanhai Ave, Shenzhen 518060, Guangdong, Peoples R China.
EM gaofengcao@pku.edu.cn; flying.zhou@163.com; max.liu.426@gmail.com;
   ajwang@stu.pku.edu.cn; fanleidong@stu.pku.edu.cn
OI Zhou, Fei/0000-0003-1216-2181; LIU, KANGLIN/0000-0002-6293-5464; cao,
   gaofeng/0000-0002-1556-7187; , fan lei dong/0000-0002-3082-695X
FU Guangdong Basic and Applied Basic Research Foundation [2021A1515011584];
   National Natural Science Foundation of China [62101290]
FX This work was supported by the Guangdong Basic and Applied Basic
   Research Foundation (No. 2021A1515011584) and National Natural Science
   Foundation of China (No. 62101290).
CR Akhil KA, 2021, IEEE COMPUT SOC CONF, P526, DOI 10.1109/CVPRW53098.2021.00064
   [Anonymous], 2017, Advanced high dynamic range imaging
   Ayd~n T. O., 2008, HUMAN VISION ELECT I, P6806
   Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073703, 10.1145/3072959.3073708]
   Banterle F, 2007, VISUAL COMPUT, V23, P467, DOI 10.1007/s00371-007-0124-9
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P2343, DOI 10.1111/j.1467-8659.2009.01541.x
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Cao GF, 2021, NEUROCOMPUTING, V464, P1, DOI 10.1016/j.neucom.2021.08.057
   Chen GN, 2021, IEEE COMPUT SOC CONF, P398, DOI 10.1109/CVPRW53098.2021.00050
   Chen XY, 2021, IEEE COMPUT SOC CONF, P354, DOI 10.1109/CVPRW53098.2021.00045
   Dang-Nguyen D.T., 2015, P ACM MULT SYST C, P219
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   Eilertsen G, 2021, IEEE INT CONF COMP V, P3981, DOI 10.1109/ICCVW54120.2021.00445
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   Grossberg MD, 2003, PROC CVPR IEEE, P602
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu W, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3199514
   Huo YQ, 2014, VISUAL COMPUT, V30, P507, DOI 10.1007/s00371-013-0875-4
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Khan Z, 2019, IEEE GLOB CONF SIG, DOI 10.1109/globalsip45357.2019.8969167
   Kingma Diederik, 2015, P 3 INT C LEARN REPR
   Kinoshita Y, 2017, INT WORK SIG DES, P49, DOI 10.1109/IWSDA.2017.8095734
   Kovaleski RP, 2014, 2014 27TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI), P49, DOI 10.1109/SIBGRAPI.2014.29
   Landis H., 2002, PRODUCTION READY GLO, V16, P11
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Marnerides D, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13340
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Reinhard E, 2005, IEEE T VIS COMPUT GR, V11, P13, DOI 10.1109/TVCG.2005.9
   Rempel AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239490
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Santos MS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392403
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Yan QS, 2022, INT J COMPUT VISION, V130, P76, DOI 10.1007/s11263-021-01535-y
   Yan QS, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108342
   Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Zhang DY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3398685
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zheng ZR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4429, DOI 10.1109/ICCV48922.2021.00441
   Zhihao Xia, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11841, DOI 10.1109/CVPR42600.2020.01186
NR 47
TC 4
Z9 4
U1 4
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 79
DI 10.1145/3550277
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300004
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhu, YC
   Min, XK
   Zhu, DD
   Zhai, GT
   Yang, XK
   Zhang, WJ
   Gu, K
   Zhou, JT
AF Zhu, Yucheng
   Min, Xiongkuo
   Zhu, Dandan
   Zhai, Guangtao
   Yang, Xiaokang
   Zhang, Wenjun
   Gu, Ke
   Zhou, Jiantao
TI Toward Visual Behavior and Attention Understanding for Augmented 360
   Degree Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Augmented reality; visual attention; visual behavior; virtual reality;
   saliency prediction
ID SALIENCY PREDICTION; DEEP AGENT; IMAGE; MOVEMENT; MODELS; COLOR; MAPS;
   HEAD
AB Augmented reality (AR) overlays digital content onto reality. In an AR system, correct and precise estimations of user visual fixations and head movements can enhance the quality of experience by allocating more computational resources for analyzing, rendering, and 3D registration on the areas of interest. However, there is inadequate research to help in understanding the visual explorations of the users when using an AR system or modeling AR visual attention. To bridge the gap between the saliency prediction on real-world scenes and on scenes augmented by virtual information, we construct the ARVR saliency dataset. The virtual reality (VR) technique is employed to simulate the real-world. Annotations of object recognition and tracking as augmented contents are blended into omnidirectional videos. The saliency annotations of head and eye movements for both original and augmented videos are collected and together constitute the ARVR dataset. We also design a model that is capable of solving the saliency prediction problem in AR. Local block images are extracted to simulate the viewport and offset the projection distortion. Conspicuous visual cues in the local block images are extracted to constitute the spatial features. The optical flow information is estimated as an important temporal feature. We also consider the interplay between virtual information and reality. The composition of the augmentation information is distinguished, and the joint effects of adversarial augmentation and complementary augmentation are estimated. The Markov chain is constructed with block images as graph nodes. In the determination of the edge weights, both the characteristics of the viewing behaviors and the visual saliency mechanisms are considered. The order of importance for block images is estimated through the state of equilibrium of the Markov chain. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method.
C1 [Zhu, Yucheng; Min, Xiongkuo; Zhu, Dandan; Zhai, Guangtao; Yang, Xiaokang; Zhang, Wenjun] Shanghai Jiao Tong Univ, Inst Image Commu & Infor Proce, Shanghai, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
   [Zhou, Jiantao] Univ Macau, State Key Lab Internet Things Smart City, Macau, Peoples R China.
   [Zhou, Jiantao] Univ Macau, Dept Comp & Informat Sci, Macau, Peoples R China.
C3 Shanghai Jiao Tong University; Beijing University of Technology;
   University of Macau; University of Macau
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commu & Infor Proce, Shanghai, Peoples R China.
EM zyc420@sjtu.edu.cn; minxiongkuo@sjtu.edu.cn; ddz@sjtu.edu.cn;
   zhaiguangtao@sjtu.edu.cn; xkyang@sjtu.edu.cn; zhangwenjun@sjtu.edu.cn;
   guke@bjut.edu; jtzhou@umac.mo
RI Zhai, Guangtao/X-5949-2019
OI Zhai, Guangtao/0000-0001-8165-9322; Zhu, Yucheng/0000-0002-3069-060X
FU National Natural Science Foundation of China [62101326, 61831015,
   U1908210, 61927809, 61771305]; National Key R&D Program of China
   [2021YFF0900503, 2019YFB1405900, 2019YFB1405902]; China Postdoctoral
   Science Foundation [2022M712090]
FX This work was supported by the National Natural Science Foundation of
   China (Grants No. 62101326, No. 61831015, No. U1908210, No. 61927809,
   and No. 61771305), National Key R&D Program of China (Grants No.
   2021YFF0900503, No. 2019YFB1405900, and No. 2019YFB1405902), and the
   China Postdoctoral Science Foundation (Grant No. 2022M712090).
CR Nguyen A, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P279, DOI 10.1145/3304109.3325820
   [Anonymous], 2023, ACM T MULTIM COMPUT, V19
   AZUMA R, 1993, COMMUN ACM, V36, P50, DOI 10.1145/159544.159581
   Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Battisti F, 2018, SIGNAL PROCESS-IMAGE, V69, P53, DOI 10.1016/j.image.2018.03.008
   Billinghurst M, 2002, COMMUN ACM, V45, P64, DOI 10.1145/514236.514265
   BRANDT T, 1973, EXP BRAIN RES, V16, P476, DOI 10.1007/BF00234474
   Canas JJ., 2006, International Encyclopedia of Ergonomics and Human Factors-3 Volume Set
   Carlson C., How I Made Wine Glasses from Sunflowers
   Carmigniani J, 2011, HANDBOOK OF AUGMENTED REALITY, P3, DOI 10.1007/978-1-4614-0064-6_1
   Chang KC, 2008, COMMUN MATH SCI, V6, P507
   Chao Fang-Yi, 2021, P IEEE WORKSHOP MULT, P6
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Dataset, 2017, LARG SCAL SCEN UND L
   De Abreu A., 2017, INT WORK QUAL MULTIM, P1
   Deng CW, 2020, IEEE T CYBERNETICS, V50, P1146, DOI 10.1109/TCYB.2018.2889376
   Djilali Yasser Abdelaziz Dahou, 2021, P IEEECVF INT C COMP, P3750
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Duan H, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3337066
   Guastello StephenJ., 2013, Human Factors Engineering and Ergonomics: A Systems Approach
   Guo CL, 2008, PROC CVPR IEEE, P2908
   Gutiérrez J, 2018, SIGNAL PROCESS-IMAGE, V69, P35, DOI 10.1016/j.image.2018.05.003
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2015, Arxiv, DOI arXiv:1510.07748
   Jia S, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103887
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Jiang QP, 2018, IEEE T CYBERNETICS, V48, P1276, DOI 10.1109/TCYB.2017.2690452
   John B, 2019, INT J SEMANT COMPUT, V13, P329, DOI 10.1142/S1793351X19400142
   Jost T, 2005, COMPUT VIS IMAGE UND, V100, P107, DOI 10.1016/j.cviu.2004.10.009
   Judd T., 2012, A benchmark of computational models of saliency to predict human fixations
   Kruijff Ernst, 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P3, DOI 10.1109/ISMAR.2010.5643530
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Lai WS, 2018, IEEE T VIS COMPUT GR, V24, P2610, DOI 10.1109/TVCG.2017.2750671
   Lebreton P, 2018, SIGNAL PROCESS-IMAGE, V69, P69, DOI 10.1016/j.image.2018.03.006
   Li J, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3511603
   Li Yunhao, 2021, P IEEE CVF INT C COM, P3742
   Linardos P, 2019, Arxiv, DOI arXiv:1907.01869
   Ling J, 2018, SIGNAL PROCESS-IMAGE, V69, P60, DOI 10.1016/j.image.2018.03.007
   Martin D, 2022, IEEE T VIS COMPUT GR, V28, P2003, DOI 10.1109/TVCG.2022.3150502
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Min XK, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996463
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Pan JT, 2018, Arxiv, DOI arXiv:1701.01081
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Rai Y, 2017, INT WORK QUAL MULTIM
   RAYNER K, 1995, STUD VIS INFORM PROC, V6, P3
   Cohen TS, 2018, Arxiv, DOI arXiv:1801.10130
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shenhav A, 2017, ANNU REV NEUROSCI, V40, P99, DOI 10.1146/annurev-neuro-072116-031526
   Startsev M, 2018, SIGNAL PROCESS-IMAGE, V69, P43, DOI 10.1016/j.image.2018.03.013
   Su Yi-Hao, 2017, Zenodo, Vv0.1.dev
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   Van Krevelen D. W. F., 2010, INT J VIRTUAL REALIT, V9, P1, DOI 10/ggxxt5
   Vive Pro, 2019, VIVE PROEYE HMDWITH
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Xu M, 2021, IEEE T IMAGE PROCESS, V30, P2087, DOI 10.1109/TIP.2021.3050861
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang K, 2019, IEEE T CIRC SYST VID, V29, P3544, DOI 10.1109/TCSVT.2018.2883305
   Zhu YC, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3410455
   Zhu YC, 2020, IEEE T MULTIMEDIA, V22, P2331, DOI 10.1109/TMM.2019.2957986
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
   Zhu Yucheng, 2021, P INT C VIS COMM IM, P1
   Zoya BylinskiiTilke Judd., MIT Saliency Benchmark
NR 69
TC 8
Z9 8
U1 3
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 99
DI 10.1145/3565024
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300024
DA 2024-07-18
ER

PT J
AU Lin, JY
   Tan, X
   Xu, K
   Ma, LZ
   Lau, RH
AF Lin, Jiaying
   Tan, Xin
   Xu, Ke
   Ma, Lizhuang
   Lau, Rynsonw. H.
TI Frequency-aware Camouflaged Object Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Camouflaged object detection; frequency learning
AB Camouflaged object detection (COD) is important as it has various potential applications. Unlike salient object detection (SOD), which tries to identify visually salient objects, COD tries to detect objects that are visually very similar to the surrounding background. We observe that recent COD methods try to fuse features from different levels using some context aggregation strategies originally developed for SOD. Such an approach, however, may not be appropriate for COD as these existing context aggregation strategies are good at detecting distinctive objects while weakening the features from less discriminative objects. To address this problem, we propose in this article to exploit frequency learning to suppress the confusing high-frequency texture information, to help separate camouflaged objects from their surrounding background, and a frequency-based method, called FBNet, for camouflaged object detection. Specifically, we design a frequency-aware context aggregation (FACA) module to suppress high-frequency information and aggregate multi-scale features from a frequency perspective, an adaptive frequency attention (AFA) module to enhance the features of the learned important frequency components, and a gradient-weighted loss function to guide the proposed method to pay more attention to contour details. Experimental results show that our model outperforms relevant state-of-the-art methods.
C1 [Lin, Jiaying; Tan, Xin; Xu, Ke; Lau, Rynsonw. H.] City Univ Hong Kong, Hong Kong, Peoples R China.
   [Tan, Xin] East China Normal Univ, Shanghai, Peoples R China.
   [Ma, Lizhuang] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
C3 City University of Hong Kong; East China Normal University; Shanghai
   Jiao Tong University
RP Xu, K; Lau, RH (corresponding author), City Univ Hong Kong, Hong Kong, Peoples R China.
RI wang, mengyi/KEI-9461-2024; Li, Shiyu/KHE-1376-2024; Sun,
   Peng/KDO-4243-2024; Tan, Xin/GRJ-0367-2022
OI Tan, Xin/0000-0001-9346-1196; LAU, Rynson W H/0000-0002-8957-8129; XU,
   Ke/0000-0001-5855-3810
FU Research Grants Council of Hong Kong [11205620]; City University of Hong
   Kong
FX This work is partially supported by a GRF (RGC Ref: 11205620) from the
   Research Grants Council of Hong Kong. Xin Tan is supported by the
   Postgraduate Studentship (Mainland Schemes) from City University of Hong
   Kong.
CR Abdulameer Hassan, 2018, ANIMAL CAMOUFL UNPUB, P7
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Ehrlich M, 2019, IEEE I CONF COMP VIS, P3483, DOI 10.1109/ICCV.2019.00358
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Feng ZY, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108777
   Geirhos R., 2019, P ICLR, P1
   Guan Huankang, 2022, P CVPR
   He K., 2017, P ICCV, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Li AX, 2021, PROC CVPR IEEE, P10066, DOI 10.1109/CVPR46437.2021.00994
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Lin JY, 2021, PROC CVPR IEEE, P13410, DOI 10.1109/CVPR46437.2021.01321
   Lin JY, 2020, PROC CVPR IEEE, P3694, DOI 10.1109/CVPR42600.2020.00375
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu W, 2015, Arxiv, DOI [arXiv:1506.04579, 10.48550/arXiv.1506.04579]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo WJ, 2016, ADV NEUR IN, V29
   Lv YQ, 2021, PROC CVPR IEEE, P11586, DOI 10.1109/CVPR46437.2021.01142
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Mei HY, 2020, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR42600.2020.00374
   Pan Y., 2011, Mod Appl Sci, V5, P152, DOI DOI 10.5539/MAS.V5N4P152
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qin ZQ, 2021, Arxiv, DOI arXiv:2012.11879
   Shiming Ge, 2018, Computational Visual Media, V4, P71, DOI 10.1007/s41095-017-0102-8
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Tan X, 2023, IEEE T PATTERN ANAL, V45, P3492, DOI 10.1109/TPAMI.2022.3181030
   Tan X, 2021, IEEE T IMAGE PROCESS, V30, P9085, DOI 10.1109/TIP.2021.3122004
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xu YY, 2019, IEEE I CONF COMP VIS, P3788, DOI 10.1109/ICCV.2019.00389
   Xue F, 2016, MULTIMED TOOLS APPL, V75, P4065, DOI 10.1007/s11042-015-2946-1
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Ye SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6423, DOI 10.1109/ICCV48922.2021.00638
   Yin JQ, 2011, PROCEDIA ENGINEER, V15, DOI 10.1016/j.proeng.2011.08.412
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang Qing, 2020, P AAAI
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhao Xiaoqi, 2020, P ECCV
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
   Zou Xueyan, 2020, P BMVC
NR 59
TC 4
Z9 5
U1 20
U2 38
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 61
DI 10.1145/3545609
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000011
DA 2024-07-18
ER

PT J
AU Zheng, N
   Song, XM
   Su, TY
   Liu, WF
   Yan, Y
   Nie, LQ
AF Zheng, Na
   Song, Xuemeng
   Su, Tianyu
   Liu, Weifeng
   Yan, Yan
   Nie, Liqiang
TI Egocentric Early Action Prediction via Adversarial Knowledge
   Distillation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Early action prediction; teacher-student knowledge distillation;
   egocentric video understanding; generative adversarial networks
AB Egocentric early action prediction aims to recognize actions from the first-person view by only observing a partial video segment, which is challenging due to the limited context information of the partial video. In this article, to tackle the egocentric early action prediction problem, we propose a novel multi-modal adversarial knowledge distillation framework. In particular, our approach involves a teacher network to learn the enhanced representation of the partial video by considering the future unobserved video segment, and a student network to mimic the teacher network to produce the powerful representation of the partial video and based on that predicting the action label. To promote the knowledge distillation between the teacher and the student network, we seamlessly integrate adversarial learning with latent and discriminative knowledge regularizations encouraging the learned representations of the partial video to be more informative and discriminative toward the action prediction. Finally, we devise a multi-modal fusion module toward comprehensively predicting the action label. Extensive experiments on two public egocentric datasets validate the superiority of our method over the state-of-the-art methods. We have released the codes and involved parameters to benefit other researchers.(1)
C1 [Zheng, Na; Song, Xuemeng; Su, Tianyu; Nie, Liqiang] Shandong Univ, N3 Floor,72 Binhai Highway, Qingdao 266237, Peoples R China.
   [Liu, Weifeng] China Univ Petr East China, 66 West Changjiang Rd, Qingdao 266580, Peoples R China.
   [Yan, Yan] IIT, 10 West 35th St, Chicago, IL 60616 USA.
C3 Shandong University; China University of Petroleum; Illinois Institute
   of Technology
RP Zheng, N; Nie, LQ (corresponding author), Shandong Univ, N3 Floor,72 Binhai Highway, Qingdao 266237, Peoples R China.
EM zhengnagrape@gmail.com; sxmustc@gmail.com; tyanyu.su@gmail.com;
   liuwf@upc.edu.cn; yyan34@iit.edu; nieliqiang@gmail.com
RI liu, weifeng/B-7909-2008
OI Zheng, Na/0000-0001-5468-6691
FU National Key Research and Development Project of New Generation
   Artificial Intelligence [2018AAA0102502]
FX This work was supported by the National Key Research and Development
   Project of New Generation Artificial Intelligence (2018AAA0102502).
CR Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Cai YJ, 2019, AAAI CONF ARTIF INTE, P8118
   Cao Y, 2018, PROC CVPR IEEE, P1287, DOI 10.1109/CVPR.2018.00140
   Chen XL, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1084, DOI 10.1145/3404835.3462946
   Chen XY, 2018, LECT NOTES COMPUT SC, V11206, P167, DOI 10.1007/978-3-030-01216-8_11
   Cheraghian A, 2021, PROC CVPR IEEE, P2534, DOI 10.1109/CVPR46437.2021.00256
   Chuang Gan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5589, DOI 10.1109/CVPR.2018.00586
   Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44
   Dessalene E, 2023, IEEE T PATTERN ANAL, V45, P6703, DOI 10.1109/TPAMI.2021.3055233
   Fathi A, 2011, IEEE I CONF COMP VIS, P407, DOI 10.1109/ICCV.2011.6126269
   Furnari A, 2019, IEEE I CONF COMP VIS, P6261, DOI 10.1109/ICCV.2019.00635
   Furnari A, 2021, IEEE T PATTERN ANAL, V43, P4021, DOI 10.1109/TPAMI.2020.2992889
   Furnari A, 2019, LECT NOTES COMPUT SC, V11133, P389, DOI 10.1007/978-3-030-11021-5_24
   Gammulle H, 2019, IEEE I CONF COMP VIS, P5561, DOI 10.1109/ICCV.2019.00566
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Garcia NC, 2018, LECT NOTES COMPUT SC, V11212, P106, DOI 10.1007/978-3-030-01237-3_7
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu HT, 2020, PROC CVPR IEEE, P3120, DOI 10.1109/CVPR42600.2020.00319
   Hu Tao, 2021, P IEEECVF INT C COMP, P14528
   Huang M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177757
   Huang Shao, 2017, ACM Transaction on Multimedia Computing, Communication, and Applications, V14, P1
   Huang Y, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409332
   Huang YF, 2019, IEEE I CONF COMP VIS, P6281, DOI 10.1109/ICCV.2019.00637
   Jiang Hao, 2021, P IEEE INT C COMPUTE, P11006
   Kazakos E, 2019, IEEE I CONF COMP VIS, P5491, DOI 10.1109/ICCV.2019.00559
   Khrulkov V, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14408, DOI 10.1109/ICCV48922.2021.01416
   Kim K, 2021, PROC CVPR IEEE, P8162, DOI 10.1109/CVPR46437.2021.00807
   Kong Y, 2020, IEEE T PATTERN ANAL, V42, P539, DOI 10.1109/TPAMI.2018.2882805
   Kong Y, 2017, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2017.390
   Li HX, 2019, PROC CVPR IEEE, P7924, DOI 10.1109/CVPR.2019.00812
   Li S, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3131344
   Li Y, 2018, LECT NOTES COMPUT SC, V11209, P639, DOI 10.1007/978-3-030-01228-1_38
   Li Y, 2015, PROC CVPR IEEE, P287, DOI 10.1109/CVPR.2015.7298625
   Li Yin, 2021, IEEE T PATTERN ANAL, V2021, P1
   Liu F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1526, DOI 10.1145/3343031.3350953
   Lu ML, 2019, IEEE T IMAGE PROCESS, V28, P3703, DOI 10.1109/TIP.2019.2901707
   Ma MH, 2016, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2016.209
   Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214
   Nagarajan T, 2019, IEEE I CONF COMP VIS, P8687, DOI 10.1109/ICCV.2019.00878
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Possas R, 2018, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2018.00625
   Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899
   Ren XF, 2010, PROC CVPR IEEE, P3137, DOI 10.1109/CVPR.2010.5540074
   Shahbazi M, 2021, PROC CVPR IEEE, P12162, DOI 10.1109/CVPR46437.2021.01199
   Shamai G, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3337067
   Shen Y, 2018, LECT NOTES COMPUT SC, V11206, P202, DOI 10.1007/978-3-030-01216-8_13
   Shi YG, 2018, LECT NOTES COMPUT SC, V11214, P305, DOI 10.1007/978-3-030-01249-6_19
   Song XM, 2018, ACM/SIGIR PROCEEDINGS 2018, P5, DOI 10.1145/3209978.3209996
   Sudhakaran S, 2018, Arxiv, DOI arXiv:1807.11794
   Sudhakaran S, 2019, PROC CVPR IEEE, P9946, DOI 10.1109/CVPR.2019.01019
   Valverde FR, 2021, PROC CVPR IEEE, P11607, DOI 10.1109/CVPR46437.2021.01144
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang CY, 2019, IEEE I CONF COMP VIS, P743, DOI 10.1109/ICCV.2019.00083
   Wang J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P515, DOI 10.1145/3077136.3080786
   Wang LC, 2019, IEEE I CONF COMP VIS, P6221, DOI 10.1109/ICCV.2019.00631
   Wang XH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8148, DOI 10.1109/ICCV48922.2021.00806
   Wang XH, 2019, PROC CVPR IEEE, P3551, DOI 10.1109/CVPR.2019.00367
   Wang XP, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355397
   Xiao TT, 2019, IEEE I CONF COMP VIS, P3918, DOI 10.1109/ICCV.2019.00402
   Yang X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P941, DOI 10.1145/3397271.3401150
   Ye J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3038917
   Yu C, 2019, IEEE I CONF COMP VIS, P9045, DOI 10.1109/ICCV.2019.00914
   Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121
   Yu Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3424116
   Zaki HFM, 2017, PROC CVPR IEEE, P1619, DOI 10.1109/CVPR.2017.176
   Zhang CR, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1135
   Zhang JY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P845, DOI 10.1145/3343031.3350897
NR 70
TC 4
Z9 4
U1 3
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 59
DI 10.1145/3544493
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000009
DA 2024-07-18
ER

PT J
AU Xiong, LZ
   Han, X
   Yang, CN
   Xia, ZH
AF Xiong, Lizhi
   Han, Xiao
   Yang, Ching-Nung
   Xia, Zhihua
TI RDH-DES: Reversible Data Hiding over Distributed Encrypted-Image Servers
   Based on Secret Sharing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Reversible data hiding; secret sharing; encrypted image; lightweight
   cryptography; multi-server and multi-message privacy
AB Reversible Data Hiding in Encrypted Image (RDHEI) schemes may redistribute the data hiding procedure to other parties and can preserve privacy of the cover image. Recently, cloud computing technology has led to the rapid growth of networked media, and many multimedia rights are owned by multiple parties, such as a film's producer and multiple distributors. Thus, the data hiding task could be distributed to multiple distributed servers. Multi-party data hiding has become an important demand for networked media. In addition, it is essential to preserve multi-server and multi-message privacy and data integrity. However, most of the RDHEI schemes involve only one data hider. That inspired us to design the secure multi-party embedding over distributed encrypted-image servers as a solution for multi-party RDHEI applications. In this article, we propose a novel Reversible Data Hiding over Distributed Encrypted-Image Servers (RDH-DES) based on secret sharing. The Chinese remainder theorem, secret sharing, and block-level scrambling are developed as a lightweight cryptography to generate the encrypted image shares. These shares are distributed to different image servers and are used to embed secret data in the proposed framework. Themarked encrypted image can be constructed through the marked encrypted shares from different parties, and the decryption and extraction can be completed by the receiver. The experimental results and theoretical analysis have demonstrated that the proposed scheme is secure and effective.
C1 [Xiong, Lizhi; Han, Xiao; Xia, Zhihua] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Engn Res Ctr Digital Forens, Minist Educ, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China.
   [Yang, Ching-Nung] Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, 1,Sec 2,Da Hsueh Rd, Hualien 974301, Taiwan.
C3 Nanjing University of Information Science & Technology; National Dong
   Hwa University
RP Xiong, LZ (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Engn Res Ctr Digital Forens, Minist Educ, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China.; Yang, CN (corresponding author), Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, 1,Sec 2,Da Hsueh Rd, Hualien 974301, Taiwan.
EM lzxiong16@163.com; hanxiao6027@qq.com; cnyang@gms.edu.tw;
   xia_zhihua@163.com
RI han, xiao/GXF-7623-2022; Xiong, Lizhi/KCK-1464-2024; Xia,
   Zhihua/C-8581-2011; Yang, Ching-Nung/HKV-1639-2023
FU National Natural Science Foundation of China [62172233]; Ministry of
   Science and Technology (MOST) [MOST 110-2221-E-259-005-MY2];
   Postgraduate Research & Practice Innovation Program of Jiangsu Province
   [KYCX21_1007]; Guangxi Key Laboratory of Trusted Software [KX202044]
FX This work was supported in part by the National Natural Science
   Foundation of China under grant 62172233, in part by the Ministry of
   Science and Technology (MOST) under grant MOST 110-2221-E-259-005-MY2,
   in part by the Postgraduate Research & Practice Innovation Program of
   Jiangsu Province under grant KYCX21_1007, and in part by the Guangxi Key
   Laboratory of Trusted Software under grant KX202044.
CR Abdulla AA, 2015, THESIS BUCKINGHAM U
   Abdulla AA, 2014, PROC SPIE, V9120, DOI 10.1117/12.2050518
   Adi PW, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING, COMPUTER SCIENCE AND INFORMATICS (EECSI), P235
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bas Patrick, 2022, IMAGE DATABASE BOWS
   Chen B, 2022, IEEE T DEPEND SECURE, V19, P978, DOI 10.1109/TDSC.2020.3011923
   Chen YC, 2019, IEEE T INF FOREN SEC, V14, P3332, DOI 10.1109/TIFS.2019.2914557
   Chen YC, 2014, J VIS COMMUN IMAGE R, V25, P1164, DOI 10.1016/j.jvcir.2014.04.003
   Coltuc D, 2011, IEEE T INF FOREN SEC, V6, P873, DOI 10.1109/TIFS.2011.2145372
   Dragoi IC, 2012, EUR SIGNAL PR CONF, P1688
   Dragoi IC, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549458
   Dragoi IC, 2015, IEEE T IMAGE PROCESS, V24, P1244, DOI 10.1109/TIP.2015.2395724
   Dragoi IC, 2014, IEEE T IMAGE PROCESS, V23, P1779, DOI 10.1109/TIP.2014.2307482
   Frattolillo F, 2019, J INF SECUR APPL, V47, P246, DOI 10.1016/j.jisa.2019.05.011
   Gao XB, 2011, IEEE T CIRC SYST VID, V21, P1061, DOI 10.1109/TCSVT.2011.2130410
   He WG, 2021, IEEE T MULTIMEDIA, V23, P52, DOI 10.1109/TMM.2020.2982042
   Hua ZY, 2018, SIGNAL PROCESS, V144, P134, DOI 10.1016/j.sigpro.2017.10.004
   Huang FJ, 2016, IEEE T INF FOREN SEC, V11, P2777, DOI 10.1109/TIFS.2016.2598528
   Jeon Cheol, 2012, P 2012 ACM RES APPL, P333, DOI DOI 10.1145/2401603.2401675
   Ke Y, 2020, IEEE T CIRC SYST VID, V30, P2353, DOI 10.1109/TCSVT.2019.2963393
   Li XL, 2013, IEEE T INF FOREN SEC, V8, P1091, DOI 10.1109/TIFS.2013.2261062
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Lin PY, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700291
   Luo X, 2014, WIRELESS OPTIC COMM
   Namasudra S, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3392665
   Naskar R, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487272
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Sahu AK, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3398039
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Singh AK, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3382772
   Sudibyo U, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY, COMPUTER, AND ELECTRICAL ENGINEERING (ICITACEE), P208, DOI 10.1109/ICITACEE.2017.8257704
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Thodi DM, 2004, IEEE IMAGE PROC, P1549
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wang JX, 2017, IEEE T CYBERNETICS, V47, P315, DOI 10.1109/TCYB.2015.2514110
   Wang YM, 2021, IEEE T MULTIMEDIA, V23, P1466, DOI 10.1109/TMM.2020.2999187
   Weber Allan G., 1997, 315 USC VIT SCH ENG
   Weng SW, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11010049
   Weng SW, 2017, J VIS COMMUN IMAGE R, V48, P317, DOI 10.1016/j.jvcir.2017.05.005
   Wu XT, 2018, SIGNAL PROCESS, V143, P269, DOI 10.1016/j.sigpro.2017.09.017
   [席国宝 Xi Guobao], 2006, [电子与信息学报, Journal of electronics & information technology], V28, P2378
   Xiang SJ, 2018, IEEE T CIRC SYST VID, V28, P3099, DOI 10.1109/TCSVT.2017.2742023
   Xiong LZ, 2019, J INF SECUR APPL, V47, P78, DOI 10.1016/j.jisa.2019.04.005
   Yan XH, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419750
   Yan XH, 2020, IEEE T INF FOREN SEC, V15, P3848, DOI 10.1109/TIFS.2020.3001735
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
   Zhang XP, 2016, IEEE T CIRC SYST VID, V26, P1622, DOI 10.1109/TCSVT.2015.2433194
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
NR 50
TC 8
Z9 8
U1 11
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 5
DI 10.1145/3512797
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400005
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Pan, YW
   Yao, T
   Huang, R
   Mei, T
   Chen, CW
AF Zhang, Yong
   Pan, Yingwei
   Yao, Ting
   Huang, Rui
   Mei, Tao
   Chen, Chang-Wen
TI Boosting Scene Graph Generation with Visual Relation Saliency
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Scene graph generation; relation saliency
ID LANGUAGE
AB The scene graph is a symbolic data structure that comprehensively describes the objects and visual relations in a visual scene, while ignoring the inherent perceptual saliency of each visual relation (i.e., relation saliency). However, humans often quickly allocate attention to important/salient visual relations in a scene. To align with such human perception of a scene, we explicitly model the perceptual saliency of visual relation in scene graph by upgrading each graph edge (i.e., visual relation) with an attribute of relation saliency. We present a new design, named as Saliency-guided Message Passing (SMP), that boosts the generation of such scene graph structure with the guidance from the visual relation saliency. Technically, an object interaction encoder is first utilized to strengthen object relation representations by jointly exploiting the appearance, semantic, and spatial relations in between. A branch is further leveraged to estimate the relation saliency of each visual relation by ordinal regression. Next, conditioned on the object and relation features (coupled with the estimated relation saliency), our SMP enhances scene graph generation by performing message passing over the objects and the most salient relations. Extensive experiments on VG-KR and VG150 datasets demonstrate the superiority of SMP for the scene graph generation. Moreover, we empirically validate the compelling generalizability of the learned scene graphs via SMP on downstream tasks like cross-model retrieval and image captioning.
C1 [Zhang, Yong; Huang, Rui] Chinese Univ Hong Kong, 2001 Longxiang Blvd, Hong Kong, Peoples R China.
   [Pan, Yingwei; Yao, Ting; Mei, Tao] JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
   [Chen, Chang-Wen] Hong Kong Polytech Univ, Hung Hom, 11 Yuk Choi Rd, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong; Hong Kong Polytechnic University
RP Yao, T (corresponding author), JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
EM yongzhang@link.cuhk.edu.cn; panyw.ustc@gmail.com;
   tingyao.ustc@gmail.com; ruihuang@cuhk.edu.cn; tmei@live.com;
   changwen.chen@polyu.edu.hk
RI Zhang, Yong/IZV-0577-2023
OI Zhang, Yong/0000-0001-5396-4549
FU National Key R&D Program of China [2020AAA0108600]
FX This work was supported by the National Key R&D Program of China under
   Grant No. 2020AAA0108600.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Cao W., 2019, arXiv preprint arXiv:1901.07884
   Chen L, 2019, IEEE I CONF COMP VIS, P4612, DOI 10.1109/ICCV.2019.00471
   Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Faghri Fartash, 2017, P BRIT MACHINE VISIO
   Feng Fangxiang, 2015, ACM T MULTIM COMPUT, V12, P22, DOI DOI 10.1145/2808205
   Frank Eibe, 2001, EUR C MACH LEARN, P145, DOI 10.1007/3-540-44795-413
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kaihua Tang, 2020, SCENE GRAPH GENERATI
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Ke L, 2019, IEEE I CONF COMP VIS, P8887, DOI 10.1109/ICCV.2019.00898
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li RJ, 2021, PROC CVPR IEEE, P11104, DOI 10.1109/CVPR46437.2021.01096
   Li YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3799, DOI 10.1145/3474085.3478331
   Li YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473140
   Li YA, 2021, AAAI CONF ARTIF INTE, V35, P8518
   Li YH, 2019, PROC CVPR IEEE, P12489, DOI 10.1109/CVPR.2019.01278
   Li YK, 2018, LECT NOTES COMPUT SC, V11205, P346, DOI 10.1007/978-3-030-01246-5_21
   Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142
   Lin H. T., 2007, ADV NEURAL INFORM PR, V19
   Lin X, 2020, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR42600.2020.00380
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Luo JJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5600, DOI 10.1145/3474085.3475703
   Lv JM, 2020, Arxiv, DOI arXiv:2003.07012
   Niu ZX, 2016, PROC CVPR IEEE, P4920, DOI 10.1109/CVPR.2016.532
   Pan YW, 2020, Arxiv, DOI arXiv:2007.02375
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qi MS, 2019, PROC CVPR IEEE, P3952, DOI 10.1109/CVPR.2019.00408
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Wang AQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3226037
   Wang WB, 2019, PROC CVPR IEEE, P8180, DOI 10.1109/CVPR.2019.00838
   Wenbin Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P222, DOI 10.1007/978-3-030-58601-0_14
   Wu J., 2018, ACM T MULTIM COMPUT, P1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yu Fan, 2020, P ACM INT C MULTIMED
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P606, DOI 10.1007/978-3-030-58592-1_36
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang CY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3412847
   Zhang J, 2019, PROC CVPR IEEE, P11527, DOI 10.1109/CVPR.2019.01180
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
NR 55
TC 3
Z9 3
U1 3
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 8
DI 10.1145/3514041
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400008
DA 2024-07-18
ER

PT J
AU Wu, JJ
   Jiang, JG
   Qi, MB
   Chen, CQ
   Zhang, JJ
AF Wu, Jingjing
   Jiang, Jianguo
   Qi, Meibin
   Chen, Cuiqun
   Zhang, Jingjing
TI An End-to-end Heterogeneous Restraint Network for RGB-D Cross-modal
   Person Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE RGB-D cross-modal person re-identification; end-to-end deep network;
   heterogeneous interactive learning; cross-modal relational branch;
   mutual contextual graph networks
AB The RGB-D cross-modal person re-identification (re-id) task aims to identify the person of interest across the RGB and depth image modes. The tremendous discrepancy between these two modalities makes this task difficult to tackle. Few researchers pay attention to this task, and the deep networks of existing methods still cannot be trained in an end-to-end manner. Therefore, this article proposes an end-to-end module for RGBD cross-modal person re-id. This network introduces a cross-modal relational branch to narrow the gaps between two heterogeneous images. It models the abundant correlations between any cross-modal sample pairs, which are constrained by heterogeneous interactive learning. The proposed network also exploits a dual-modal local branch, which aims to capture the common spatial contexts in two modalities. This branch adopts shared attentive pooling and mutual contextual graph networks to extract the spatial attention within each local region and the spatial relations between distinct local parts, respectively. Experimental results on two public benchmark datasets, that is, the BIWI and RobotPKU datasets, demonstrate that our method is superior to the state-of-the-art. In addition, we perform thorough experiments to prove the effectiveness of each component in the proposed method.
C1 [Wu, Jingjing; Chen, Cuiqun; Zhang, Jingjing] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Tunxi Rd 193, Hefei 230009, Anhui, Peoples R China.
   [Jiang, Jianguo; Qi, Meibin] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Anhui Prov Key Lab Ind Safety & Emergency Technol, Key Lab Knowledge Engn Big Data,Minist Educ, Tunxi Rd 193, Hefei, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology
RP Wu, JJ (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Tunxi Rd 193, Hefei 230009, Anhui, Peoples R China.
EM hfutwujingjing@mail.hfut.edu.cn; jgjiang@hfut.edu.cn; qimeibin@163.com;
   chencuiqun_hfut@163.com; jingiingz1994@163.com
RI li, yifei/IWU-7824-2023; ZHANG, XIAOLONG/IZQ-4553-2023; Li,
   Xiaoli/JVZ-4089-2024; Zhang, Jing/ISA-6627-2023
OI Zhang, Jing/0009-0003-5039-5688; Wu, Jingjing/0000-0002-3818-4277
FU National Natural Science Foundation of China [61876056, 61771180]
FX This work is supported by the National Natural Science Foundation of
   China Grant 61876056 and Grant 61771180.
CR [Anonymous], 2020, PROC CVPR IEEE, DOI DOI 10.1109/CVPR42600.2020.00643
   Baltieri D., 2013, Proceedings of the 21st ACM International Conference on Multimedia, MM'13, P557
   Bhuiyan Amran, 2019, 16 IEEE INT C ADV VI, P1
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan X, 2019, J VIS COMMUN IMAGE R, V60, P51, DOI 10.1016/j.jvcir.2019.01.010
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Jiang JG, 2020, NEUROCOMPUTING, V406, P59, DOI 10.1016/j.neucom.2020.03.109
   Lan XY, 2020, PATTERN RECOGN LETT, V130, P12, DOI 10.1016/j.patrec.2018.10.002
   Li YY, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3412384
   Li ZJ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3362988
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   LinWu Chunhua Shen, 2016, ARXIV160107255
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Liu H, 2017, CAAI T INTELL TECHNO, V2, P48, DOI 10.1016/j.trit.2017.04.001
   Liu JL, 2020, PROC CVPR IEEE, P2967, DOI 10.1109/CVPR42600.2020.00304
   Liu WY, 2016, PR MACH LEARN RES, V48
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Mogelmose A, 2013, I W BIOMETRIC FORENS
   Mogelmose A, 2013, IEEE COMPUT SOC CONF, P301, DOI 10.1109/CVPRW.2013.52
   Munaro M, 2014, ADV COMPUT VIS PATT, P161, DOI 10.1007/978-1-4471-6296-4_8
   Pala F, 2016, IEEE T CIRC SYST VID, V26, P788, DOI 10.1109/TCSVT.2015.2424056
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wojke N, 2018, IEEE WINT CONF APPL, P748, DOI 10.1109/WACV.2018.00087
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu AC, 2017, IEEE T IMAGE PROCESS, V26, P2588, DOI 10.1109/TIP.2017.2675201
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Xu XX, 2015, IEEE T NEUR NET LEAR, V26, P3150, DOI 10.1109/TNNLS.2015.2405574
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Zhang Can, 25 INT C PATT REC IC, P8679
   Zhang P, 2020, IEEE T CIRC SYST VID, V30, P4554, DOI 10.1109/TCSVT.2019.2939564
   Zhang Xiangyu, 2017, CoRRabs/1711.08184
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zhuo JX, 2017, COMM COM INF SC, V773, P280, DOI 10.1007/978-981-10-7305-2_25
NR 49
TC 1
Z9 2
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 109
DI 10.1145/3506708
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600020
DA 2024-07-18
ER

PT J
AU Wang, XQ
   Chen, C
   Lan, RS
   Liu, LC
   Liu, ZB
   Zhou, HY
   Luo, XN
AF Wang, Xiaoqin
   Chen, Chen
   Lan, Rushi
   Liu, Licheng
   Liu, Zhenbing
   Zhou, Huiyu
   Luo, Xiaonan
TI Binary Representation via Jointly Personalized Sparse Hashing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Binary representation; personalized hashing; manifold hashing;
   similarity search
AB Unsupervised hashing has attracted much attention for binary representation learning due to the requirement of economical storage and efficiency of binary codes. It aims to encode high-dimensional features in the Hamming space with similarity preservation between instances. However, most existing methods learn hash functions in manifold-based approaches. Those methods capture the local geometric structures (i.e., pairwise relationships) of data, and lack satisfactory performance in dealing with real-world scenarios that produce similar features (e.g., color and shape) with different semantic information. To address this challenge, in this work, we propose an effective unsupervised method, namely, Jointly Personalized Sparse Hashing (JPSH), for binary representation learning. To be specific, first, we propose a novel personalized hashing module, i.e., Personalized Sparse Hashing (PSH). Different personalized subspaces are constructed to reflect category-specific attributes for different clusters, adaptively mapping instances within the same cluster to the same Hamming space. In addition, we deploy sparse constraints for different personalized subspaces to select important features. We also collect the strengths of the other clusters to build the PSH module with avoiding over-fitting. Then, to simultaneously preserve semantic and pairwise similarities in our proposed JPSH, we incorporate the proposed PSH and manifold-based hash learning into the seamless formulation. As such, JPSH not only distinguishes the instances from different clusters but also preserves local neighborhood structures within the cluster. Finally, an alternating optimization algorithm is adopted to iteratively capture analytical solutions of the JPSH model. We apply the proposed representation learning algorithm JPSH to the similarity search task. Extensive experiments on four benchmark datasets verify that the proposed JPSH outperforms several state-of-the-art unsupervised hashing algorithms.
C1 [Wang, Xiaoqin; Chen, Chen; Lan, Rushi; Liu, Zhenbing; Luo, Xiaonan] Guilin Univ Elect Technol, Guangxi Key Lab Image & Graph Intelligent Proc, Guilin, Peoples R China.
   [Liu, Licheng] Hunan Univ, Coll Elect & Informat Engn, Changsha, Peoples R China.
   [Zhou, Huiyu] Univ Leicester, Sch Comp & Math Sci, Leicester, Leics, England.
C3 Guilin University of Electronic Technology; Hunan University; University
   of Leicester
RP Lan, RS (corresponding author), Guilin Univ Elect Technol, Guangxi Key Lab Image & Graph Intelligent Proc, Guilin, Peoples R China.; Liu, LC (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha, Peoples R China.
EM xqwang@guet.edu.cn; chenchen_guet@163.com; rslan@guet.edu.cn;
   lichenghnu@gmail.com; zbliu2011@163.com; hz143@leicester.ac.uk;
   luoxn@guet.edu.cn
RI Zhou, Huiyu/O-2692-2014; Liu, Licheng/GQB-5193-2022
OI Zhou, Huiyu/0000-0003-1634-9840; Wang, Xiaoqin/0000-0002-6813-7666; Lan,
   Rushi/0000-0002-9488-8236
FU Guanxi Natural Science Foundation [2019GXNSFFA245014, ZY20198016];
   National Natural Science Foundation of China [62172120, 61936002,
   62071174]; Natural Science Foundation of Hunan Province [2020JJ3014];
   Guangxi Key Laboratory of Image and Graphic Intelligent Processing
   [GIIP2001]
FX This work was supported in part by the Guanxi Natural Science Foundation
   under Grants No. 2019GXNSFFA245014, and No. ZY20198016, the National
   Natural Science Foundation of China under Grants No. 62172120, No.
   61936002, and No. 62071174, the Natural Science Foundation of Hunan
   Province under Grant No. 2020JJ3014, and Guangxi Key Laboratory of Image
   and Graphic Intelligent Processing No. GIIP2001.
CR [Anonymous], ADV NEURAL INFORM PR
   Chen M, 2015, IEEE IMAGE PROC, P4491, DOI 10.1109/ICIP.2015.7351656
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dong X, 2021, IEEE T CIRC SYST VID, V31, P3266, DOI 10.1109/TCSVT.2020.3035775
   Fernandez-Beltran R, 2021, IEEE GEOSCI REMOTE S, V18, P256, DOI 10.1109/LGRS.2020.2969491
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Hallac D, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P387, DOI 10.1145/2783258.2783313
   Hao YB, 2017, IEEE T IMAGE PROCESS, V26, P5531, DOI 10.1109/TIP.2017.2737329
   Hu D, 2019, IEEE T IMAGE PROCESS, V28, P1080, DOI 10.1109/TIP.2018.2875312
   Hu HT, 2020, PROC CVPR IEEE, P3120, DOI 10.1109/CVPR42600.2020.00319
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Indyk P, 2001, J ALGORITHM, V38, P84, DOI 10.1006/jagm.2000.1131
   Irie G, 2014, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR.2014.272
   Ji RR, 2017, IEEE T IMAGE PROCESS, V26, P5411, DOI 10.1109/TIP.2017.2735184
   Jiang QY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2248
   Jin L, 2019, IEEE T NEUR NET LEAR, V30, P1429, DOI 10.1109/TNNLS.2018.2869601
   Jin S, 2021, IEEE T IMAGE PROCESS, V30, P6130, DOI 10.1109/TIP.2021.3091895
   Kong D., 2014, Advances in Neural Information Processing Systems, P1655
   Kong DG, 2016, AAAI CONF ARTIF INTE, P1765
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai ZH, 2018, IEEE T IMAGE PROCESS, V27, P6147, DOI 10.1109/TIP.2018.2867956
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li J., 2017, SIAM International Conference on Data Mining, P444
   Li JD, 2018, AAAI CONF ARTIF INTE, P3514
   Li SY, 2022, IEEE T CIRC SYST VID, V32, P2441, DOI 10.1109/TCSVT.2021.3093258
   Li XL, 2017, AAAI CONF ARTIF INTE, P2203
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Lin MB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1635, DOI 10.1145/3240508.3240519
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu H, 2019, IEEE T PATTERN ANAL, V41, P941, DOI 10.1109/TPAMI.2018.2819978
   Liu H, 2016, AAAI CONF ARTIF INTE, P1258
   Liu W., 2010, PROC ICML, P679
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Rao TR, 2020, NEURAL PROCESS LETT, V51, P2043, DOI 10.1007/s11063-019-10033-9
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Shi YC, 2020, PROC CVPR IEEE, P6816, DOI 10.1109/CVPR42600.2020.00685
   Tang JH, 2018, IEEE T NEUR NET LEAR, V29, P6154, DOI 10.1109/TNNLS.2018.2816743
   Tian ZB, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107261
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Weinberger Kilian Q., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516
   Weiss Y., 2008, ADV NEURAL INFORM PR, V21, P1753
   Weng ZY, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107151
   Xia Y, 2015, PROC CVPR IEEE, P3332, DOI 10.1109/CVPR.2015.7298954
   Xiang XG, 2022, IEEE T IMAGE PROCESS, V31, P314, DOI 10.1109/TIP.2021.3131042
   Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1640, DOI 10.1109/TMM.2020.3001527
   Yao XX, 2019, IEEE I CONF COMP VIS, P1140, DOI 10.1109/ICCV.2019.00123
   Zhang WQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3274, DOI 10.1145/3394171.3414028
   Zhang Z, 2019, AAAI CONF ARTIF INTE, P5853
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1025, DOI 10.1145/2647868.2655035
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
   Zhu XF, 2014, IEEE T IMAGE PROCESS, V23, P3737, DOI 10.1109/TIP.2014.2332764
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 58
TC 1
Z9 1
U1 2
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 137
DI 10.1145/3558769
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shi, QHY
   Zhang, HB
   Li, Z
   Du, JX
   Lei, Q
   Liu, JH
AF Shi, Qinghongya
   Zhang, Hong-Bo
   Li, Zhe
   Du, Ji-Xiang
   Lei, Qing
   Liu, Jing-Hua
TI Shuffle-invariant Network for Action Recognition in Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Action recognition; key region detection; shuffle-invariant network;
   adversarial learning; critical feature sort loss
AB The local key features in video are important for improving the accuracy of human action recognition. However, most end-to-end methods focus on global feature learning from videos, while few works consider the enhancement of the local information in a feature. In this article, we discuss how to automatically enhance the ability to discriminate the local information in an action feature and improve the accuracy of action recognition. To address these problems, we assume that the critical level of each region for the action recognition task is different and will not change with the region location shuffle. We therefore propose a novel action recognition method called the shuffle-invariant network. In the proposed method, the shuffled video is generated by regular region cutting and random confusion to enhance the input data. The proposed network adopts the multitask framework, which includes one feature backbone network and three task branches: local critical feature shuffle-invariant learning, adversarial learning, and an action classification network. To enhance the local features, the feature response of each region is predicted by a local critical feature learning network. To train this network, an L1-based critical feature shuffle-invariant loss is defined to ensure that the ordered feature response list of these regions remains unchanged after region location shuffle. Then, the adversarial learning is applied to eliminate the noise caused by the region shuffle. Finally, the action classification network combines these two tasks to jointly guide the training of the feature backbone network and obtain more effective action features. In the testing phase, only the action classification network is applied to identify the action category of the input video. We verify the proposed method on the HMDB51 and UCF101 action datasets. Several ablation experiments are constructed to verify the effectiveness of each module. The experimental results show that our approach achieves the state-of-the-art performance.
C1 [Shi, Qinghongya; Zhang, Hong-Bo] Huaqiao Univ, Sch Comp Sci & Technol, 668 Jimei Ave, Xiamen, Peoples R China.
   [Li, Zhe; Du, Ji-Xiang] Huaqiao Univ, Fujian Key Lab Big Data Intelligence & Secur, 668 Jimei Ave, Xiamen, Peoples R China.
   [Lei, Qing; Liu, Jing-Hua] Huaqiao Univ, Xiamen Key Lab Comp Vis & Pattern Recognit, 668 Jimei Ave, Xiamen, Peoples R China.
C3 Huaqiao University; Huaqiao University; Huaqiao University
RP Zhang, HB (corresponding author), Huaqiao Univ, Sch Comp Sci & Technol, 668 Jimei Ave, Xiamen, Peoples R China.
EM 18014083014@stu.hqu.edu.cn; zhanghongbo@hqu.edu.cn;
   20014083023@stu.hqu.edu.cn; jxdu@hqu.edu.cn; leiqing@hqu.edu.cn;
   liujinghua@hqu.edu.cn
RI Zhang, Hong-Bo/GWC-9306-2022; shi, qinghongya/HNJ-1617-2023
OI shi, qinghongya/0000-0001-9433-2281
FU Natural Science Foundation of China [61871196, 62001176]; National Key
   Research and Development Program of China [2019YFC1604700]; Natural
   Science Foundation of Fujian Province of China [2019J01082, 2020J01085];
   Promotion Program for Young and Middle-aged Teacher in Science and
   Technology Research of Huaqiao University [ZQN-YX601]
FX This work was supported by the Natural Science Foundation of China (Nos.
   61871196, 62001176); National Key Research and Development Program of
   China (NO. 2019YFC1604700); Natural Science Foundation of Fujian
   Province of China (Nos. 2019J01082 and 2020J01085); and the Promotion
   Program for Young and Middle-aged Teacher in Science and Technology
   Research of Huaqiao University (ZQN-YX601).
CR [Anonymous], CORR
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Diba A., 2017, Temporal 3D ConvNets: New architecture and transfer learning for video classification
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Diba A, 2017, PROC CVPR IEEE, P1541, DOI 10.1109/CVPR.2017.168
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Hara K, 2017, IEEE INT CONF COMP V, P3154, DOI 10.1109/ICCVW.2017.373
   Jacquot Vincent, 2020, Conf Comput Vis Pattern Recognit Workshops, V2020
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Kamel A, 2019, IEEE T SYST MAN CY-S, V49, P1806, DOI 10.1109/TSMC.2018.2850149
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Li Dong, 2021, P IEEE CVF C COMP VI, P3310
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu X, 2021, PROC CVPR IEEE, P14887, DOI 10.1109/CVPR46437.2021.01465
   Liu ZY, 2020, AAAI CONF ARTIF INTE, V34, P11669
   Mavroudi E, 2018, IEEE WINT CONF APPL, P1558, DOI 10.1109/WACV.2018.00174
   Munro J, 2020, PROC CVPR IEEE, P119, DOI 10.1109/CVPR42600.2020.00020
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Rahmani H, 2017, IEEE I CONF COMP VIS, P5833, DOI 10.1109/ICCV.2017.621
   Ren B., 2020, ARXIV200205907
   Rohrbach M, 2016, INT J COMPUT VISION, V119, P346, DOI 10.1007/s11263-015-0851-8
   Simonyan K, 2014, ADV NEUR IN, V27
   Song YF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1625, DOI 10.1145/3394171.3413802
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tran D, 2019, IEEE I CONF COMP VIS, P5551, DOI 10.1109/ICCV.2019.00565
   Wang L., 2016, P ECCV
   Xikun Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14321, DOI 10.1109/CVPR42600.2020.01434
   Xu JF, 2018, INT C PATT RECOG, P1567, DOI 10.1109/ICPR.2018.8546165
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Yi Zhu, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P668, DOI 10.1007/978-3-319-46604-0_47
   Zhang HB, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19051005
   Zhang SH, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P882
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou Yizhou, 2020, P IEEE CVF C COMP VI, P9829
   Zhu S, 2020, IEEE-ACM T AUDIO SPE, V28, P1936, DOI 10.1109/TASLP.2020.3001684
NR 42
TC 19
Z9 19
U1 3
U2 38
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 69
DI 10.1145/3485665
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600003
DA 2024-07-18
ER

PT J
AU Tahir, M
   Halim, Z
   Rahman, AU
   Waqas, M
   Tu, SS
   Chen, S
   Han, Z
AF Tahir, Madiha
   Halim, Zahid
   Rahman, Atta Ur
   Waqas, Muhammad
   Tu, Shanshan
   Chen, Sheng
   Han, Zhu
TI Non-Acted Text and Keystrokes Database and Learning Methods to Recognize
   Emotions
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Affective computing; machine learning; affective states; pattern
   recognition; data-driven decision-making
ID DEEP; FUSION; MODELS
AB The modern computing applications are presently adapting to the convenient availability of huge and diverse data for making their pattern recognition methods smarter. Identification of dominant emotion solely based on the text data generated by humans is essential for the modern human-computer interaction. This work presents a multimodal text-keystrokes dataset and associated learning methods for the identification of human emotions hidden in small text. For this, a text-keystrokes data of 69 participants is collected in multiple scenarios. Stimuli are induced through videos in a controlled environment. After the stimuli induction, participants write their reviews about the given scenario in an unguided manner. Afterward, keystroke and in-text features are extracted from the dataset. These are used with an assortment of learning methods to identify emotion hidden in the short text. An accuracy of 86.95% is achieved by fusing text and keystroke features. Whereas, 100% accuracy is obtained for pleasure-displeasure classes of emotions using the fusion of keystroke/text features, tree-based feature selection method, and support vector machine classifier. The present work is also compared with four state-of-the-art techniques for the same task, where the results suggest that the present proposal performs better in terms of accuracy.
C1 [Tahir, Madiha; Halim, Zahid; Rahman, Atta Ur] Ghulam Ishaq Khan Inst Engn Sci & Technol, Fac Comp Sci & Engn, Machine Intelligence Res Grp MInG, Topi 23640, KP, Pakistan.
   [Waqas, Muhammad; Tu, Shanshan] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Trusted Comp, Beijing 100124, Peoples R China.
   [Chen, Sheng] Univ Southampton, Sch Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
   [Han, Zhu] Univ Houston, Dept Elect & Comp Engn, Houston, TX 77204 USA.
C3 GIK Institute Engineering Science & Technology; Beijing University of
   Technology; University of Southampton; University of Houston System;
   University of Houston
RP Halim, Z (corresponding author), Ghulam Ishaq Khan Inst Engn Sci & Technol, Fac Comp Sci & Engn, Machine Intelligence Res Grp MInG, Topi 23640, KP, Pakistan.
EM madiha.tahir@giki.edu.pk; zahid.halim@giki.edu.pk;
   atta.rahman@giki.edu.pk; engr.waqas2079@gmail.com; sstu@bjut.edu.cn;
   sqc@soton.ac.uk; zhan2@central.uh.edu
RI Waqas, Muhammad/AAZ-6651-2020; Chen, Sheng/F-7835-2011; Waqas,
   Muhammad/IYS-9931-2023; Tahir, Madiha/KCJ-9981-2024
OI Waqas, Muhammad/0000-0003-0814-7544; Chen, Sheng/0000-0001-6882-600X;
   Tahir, Madiha/0009-0001-2890-3661
FU Ghulam Ishaq Khan Institute of Engineering Sciences and Technology; GIK
   Institute graduate research fund under GA-4 scheme
FX The authors would like to thank the Ghulam Ishaq Khan Institute of
   Engineering Sciences and Technology, for providing research funding and
   facilities. This work was sponsored by the GIK Institute graduate
   research fund under GA-4 scheme.
CR Abaalkhail R, 2018, SEMANT WEB, V9, P441, DOI 10.3233/SW-170270
   Adolphs R, 2017, SOC COGN AFFECT NEUR, V12, P24, DOI 10.1093/scan/nsw153
   Aguado G, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10113754
   Akhtar MS, 2020, IEEE COMPUT INTELL M, V15, P64, DOI 10.1109/MCI.2019.2954667
   Alhuzali H., 2018, P 2 WORKSH COMP MOD, P25, DOI [10.18653/v1/W18-1104, DOI 10.18653/V1/W18-1104]
   Alzubaidi L, 2021, CANCERS, V13, DOI 10.3390/cancers13071590
   [Anonymous], 2005, P INTERSPEECH
   AVERILL JR, 1983, AM PSYCHOL, V38, P1145, DOI 10.1037/0003-066X.38.11.1145
   Bechtoldt MN, 2011, J APPL PSYCHOL, V96, P1087, DOI 10.1037/a0023683
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Calix RA, 2010, IEEE T MULTIMEDIA, V12, P544, DOI 10.1109/TMM.2010.2052026
   Chaturvedi I, 2018, INFORM FUSION, V44, P65, DOI 10.1016/j.inffus.2017.12.006
   Epp C, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P715
   Ghosh S, 2017, INT CONF AFFECT, P146, DOI 10.1109/ACII.2017.8273592
   Grover S, 2016, 2016 INTERNATIONAL CONFERENCE ON INVENTIVE COMPUTATION TECHNOLOGIES (ICICT), VOL 2, P240
   Gu Y, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5079, DOI 10.1109/ICASSP.2018.8462440
   Gupta N, 2013, COMPUT INTELL-US, V29, P489, DOI 10.1111/j.1467-8640.2012.00454.x
   Halim Z, 2020, KNOWL-BASED SYST, V208, DOI 10.1016/j.knosys.2020.106443
   Hassan MM, 2019, INFORM FUSION, V51, P10, DOI 10.1016/j.inffus.2018.10.009
   Kazemzadeh A, 2013, IEEE COMPUT INTELL M, V8, P34, DOI 10.1109/MCI.2013.2247824
   Khare SK, 2021, IEEE T NEUR NET LEAR, V32, P2901, DOI 10.1109/TNNLS.2020.3008938
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Kolakowska A, 2016, ACSIS-ANN COMPUT SCI, V8, P1621, DOI 10.15439/2016F263
   Kolakowska A, 2015, C HUM SYST INTERACT, P291, DOI 10.1109/HSI.2015.7170682
   Kowalska M., 2017, HDB COGNITION EMOTIO, P1, DOI [https://doi.org/10.1007/978-3-319-28099-8495-1, DOI 10.1002/0470013494.CH3, 10.1002/0470013494.ch3]
   Kratzwald B, 2018, DECIS SUPPORT SYST, V115, P24, DOI 10.1016/j.dss.2018.09.002
   Kumar KMA, 2015, PROCEDIA COMPUT SCI, V70, P296, DOI 10.1016/j.procs.2015.10.096
   Leong F. H., 2016, OPEN J SOCIAL SCI, V4, P9
   Li JP, 2020, IEEE T CYBERNETICS, V50, P3281, DOI [10.1109/TPAMI.2019.2929036, 10.1109/TCYB.2019.2904052]
   Liu H., 2018, P 15 INT C CONTR AUT, P55
   Liu J, 2020, INFORM FUSION, V53, P123, DOI 10.1016/j.inffus.2019.06.016
   Martin O., 2006, 22 INT C DATA ENG WO, P8, DOI [DOI 10.1109/ICDEW.2006.145, 10.1109/ICDEW.2006.145]
   Nahin AFMNH, 2014, BEHAV INFORM TECHNOL, V33, P987, DOI 10.1080/0144929X.2014.907343
   Nathanson D.L., 1992, Shame and pride
   Nguyen Cuong, 2020, INT C MACH LEARN, P7294, DOI DOI 10.48550/ARXIV.2002.12462
   Nwe TL, 2003, SPEECH COMMUN, V41, P603, DOI 10.1016/S0167-6393(03)00099-2
   Payne R.L., 2003, Emotions at Work: Theory, Research and Applications for Management
   Plutchik R, 2001, AM SCI, V89, P344, DOI 10.1511/2001.4.344
   Pons G, 2022, IEEE T CYBERNETICS, V52, P4764, DOI 10.1109/TCYB.2020.3036935
   Ramírez-Gallego S, 2018, INFORM FUSION, V42, P51, DOI 10.1016/j.inffus.2017.10.001
   Salmeron-Majadas S, 2018, IEEE ACCESS, V6, P39154, DOI 10.1109/ACCESS.2018.2854966
   Seo YS, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8020164
   Shikder R, 2017, PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON NETWORKING, SYSTEMS AND SECURITY (NSYSS), P96, DOI 10.1109/NSysS.2017.7885808
   Stojanovski D, 2015, IEEE INT CONF INNOV, P52, DOI 10.1109/INNOVATIONS.2015.7381514
   Subhashini R, 2015, PROCEDIA COMPUT SCI, V48, P530, DOI 10.1016/j.procs.2015.04.131
   Talarico J, 2009, COGNITION EMOTION, V23, P380, DOI 10.1080/02699930801993999
   Tripathi S., 2018, Multi-modal emotion recognition on iemocap with neural networks
   Valdivia A, 2018, INFORM FUSION, V44, P126, DOI 10.1016/j.inffus.2018.03.007
   Wolff S, 2007, BRIT J CLIN PSYCHOL, V46, P347, DOI 10.1348/014466507X173736
   Xu Z., 2020, P 28 ACM INT C MULT, P2955
   Xu ZW, 2023, IEEE T AFFECT COMPUT, V14, P357, DOI 10.1109/TAFFC.2021.3071131
   Zhalehpour S, 2017, IEEE T AFFECT COMPUT, V8, P300, DOI 10.1109/TAFFC.2016.2553038
   Zhang T, 2019, INT J COMPUT MATH, V96, P594, DOI 10.1080/00207160.2018.1455092
   Zheng WL, 2019, IEEE T CYBERNETICS, V49, P1110, DOI 10.1109/TCYB.2018.2797176
NR 54
TC 4
Z9 4
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 61
DI 10.1145/3480968
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 0A0YT
UT WOS:000773689400013
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Wu, L
   Ling, HF
   Shi, YX
   Zhang, BY
AF Wu, Lei
   Ling, Hefei
   Shi, Yuxuan
   Zhang, Baiyan
TI Instance Correlation Graph for Unsupervised Domain Adaptation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Unsupervised domain adaptation; Instance correlation graph; Residual
   graph convolutional network
ID KERNEL
AB In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application fields. Due to the expensive cost of manual labeling efforts, it is important to transfer knowledge from a label-rich source domain to an unlabeled target domain. The core problem is how to learn a domain-invariant representation to address the domain shift challenge, in which the training and test samples come from different distributions. First, considering the geometry of space probability distributions, we introduce an effective Hellinger Distance to match the source and target distributions on statistical manifold. Second, the data samples are not isolated individuals, and they are interrelated. The correlation information of data samples should not be neglected for domain adaptation. Distinguished from previous works, we pay attention to the correlation distributions over data samples. We design elaborately a Residual Graph Convolutional Network to construct the Instance Correlation Graph (ICG). The correlation information of data samples is exploited to reduce the domain shift. Therefore, a novel Instance Correlation Graph for Unsupervised Domain Adaptation is proposed, which is trained end-to-end by jointly optimizing three types of losses, i.e., Supervised Classification loss for source domain, Centroid Alignment loss to measure the centroid difference between source and target domain, ICG Alignment loss to match Instance Correlation Graph over two related domains. Extensive experiments are conducted on several hard transfer tasks to learn domain-invariant representations on three benchmarks: Office-31, Office-Home, and VisDA2017. Compared with other state-of-the-art techniques, our method achieves superior performance.
C1 [Wu, Lei; Ling, Hefei; Shi, Yuxuan; Zhang, Baiyan] Huazhong Univ Sci & Technol, Luoyu Rd 1037, Wuhan, Peoples R China.
C3 Huazhong University of Science & Technology
RP Ling, HF (corresponding author), Huazhong Univ Sci & Technol, Luoyu Rd 1037, Wuhan, Peoples R China.
EM leiwu@hust.edu.com; lhefei@hust.edu.cn; shiyx@hust.edu.cn;
   zhangbyxy@hust.edu.cn
FU Natural Science Foundation of China [61972169]; National key research
   and development program of China [2019QY(Y)0202]; Research Programme on
   Applied Fundamentals and Frontier Technologies of Wuhan
   [2020010601012182]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61972169, in part by the National key research and
   development program of China (2019QY(Y)0202), in part by the Research
   Programme on Applied Fundamentals and Frontier Technologies of Wuhan
   (2020010601012182).
CR Ajakan H., 2014, ARXIV14124446
   Amini MR, 2002, FR ART INT, V77, P390
   Amodei Dario, 2016, CONCRETE PROBLEMS AI
   [Anonymous], 2017, P 2017 ACM MULT C, DOI DOI 10.1145/3123266.3123292
   [Anonymous], 2018, IEEE INT CONF AUTOMA, DOI DOI 10.1109/FG.2018.00011
   Arjovsky Martin, 2019, ABS190702893 CORR
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Baktashmotlagh M, 2016, J MACH LEARN RES, V17
   Ben-David S., 2006, NEURIPS, V19
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Cao ZJ, 2018, AAAI CONF ARTIF INTE, P6698
   Carter K.M., 2009, Dimensionality Reduction on Statistical Manifolds
   Chastagnol Clement, 2020, ABS200613629 CORR
   Chen XY, 2019, PR MACH LEARN RES, V97
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Das D, 2018, IEEE IMAGE PROC, P3758, DOI 10.1109/ICIP.2018.8451152
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gebru T, 2017, IEEE I CONF COMP VIS, P1358, DOI 10.1109/ICCV.2017.151
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goyal Priya, 2017, CoRR abs/1706.02677
   Gretton A., 2012, NEURAL INFORM PROCES
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Kass R. E., 2011, GEOMETRICAL FDN ASYM, V908
   Kipf TN, 2016, ARXIV
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li JJ, 2019, IEEE T IMAGE PROCESS, V28, P6103, DOI 10.1109/TIP.2019.2924174
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu XF, 2019, PROC CVPR IEEE, P637, DOI 10.1109/CVPR.2019.00073
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long M., 2017, ICML
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Luo Z., 2017, Advances in Neural Information Processing Systems, P165
   Ma XH, 2019, PROC CVPR IEEE, P8258, DOI 10.1109/CVPR.2019.00846
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Peng X., 2017, ARXIV
   Pilanci M., 2020, IEEE T KNOWL DATA EN, V2020, P1
   Pinheiro PO, 2018, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2018.00835
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito Kuniaki, 2018, P INT C LEARN REPR
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887
   Shuhao Cui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12452, DOI 10.1109/CVPR42600.2020.01247
   Snell J, 2017, ADV NEUR IN, V30
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tran L, 2019, PROC CVPR IEEE, P1126, DOI 10.1109/CVPR.2019.00122
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Vaswani A, 2017, ADV NEUR IN, V30
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xie S., 2018, ICML
   Xu MH, 2020, AAAI CONF ARTIF INTE, V34, P6502
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yang BY, 2019, AAAI CONF ARTIF INTE, P5613
   Zellinger Werner, 2017, ARXIV170208811
   Zhang BY, 2022, IEEE T COGN DEV SYST, V14, P892, DOI 10.1109/TCDS.2021.3075280
   Zhang Y., 2019, PR MACH LEARN RES, P7404
   Zhang YB, 2019, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR.2019.00517
   Zhong EH, 2010, LECT NOTES ARTIF INT, V6323, P547, DOI 10.1007/978-3-642-15939-8_35
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 81
TC 5
Z9 5
U1 2
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 33
DI 10.1145/3486251
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300010
DA 2024-07-18
ER

PT J
AU Xu, S
   Liu, C
   Zhang, BC
   Lü, JH
   Guo, GD
   Doermann, D
AF Xu, Sheng
   Liu, Chang
   Zhang, Baochang
   Lu, Jinhu
   Guo, Guodong
   Doermann, David
TI BiRe-ID: Binary Neural Network for Efficient Person Re-ID
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; network binarization; network compression
AB Person re-identification (Re-ID) has been promoted by the significant success of convolutional neural networks (CNNs). However, the application of such CNN-based Re-ID methods depends on the tremendous consumption of computation and memory resources, which affects its development on resource-limited devices such as next generation AI chips. As a result, CNN binarization has attracted increasing attention, which leads to binary neural networks (BNNs). In this article, we propose a new BNN-based framework for efficient person Re-ID (BiRe-ID). In this work, we discover that the significant performance drop of binarized models for Re-ID task is caused by the degraded representation capacity of kernels and features. To address the issues, we propose the kernel and feature refinement based on generative adversarial learning (KR-GAL and FR-GAL) to enhance the representation capacity of BNNs. We first introduce an adversarial attention mechanism to refine the binarized kernels based on their real-valued counterparts. Specifically, we introduce a scale factor to restore the scale of 1-bit convolution. And we employ an effective generative adversarial learning method to train the attention-aware scale factor. Furthermore, we introduce a self-supervised generative adversarial network to refine the low-level features using the corresponding high-level semantic information. Extensive experiments demonstrate that our BiRe-ID can be effectively implemented on various mainstream backbones for the Re-ID task. In terms of the performance, our BiRe-ID surpasses existing binarization methods by significant margins, at the level even comparable with the real-valued counterparts. For example, on Market-1501, BiRe-ID achieves 64.0% mAP on ResNet-18 backbone, with an impressive 12.51x speedup in theory and 11.75x storage saving. In particular, the KR-GAL and FR-GAL methods show strong generalization on multiple tasks such as Re-ID, image classification, object detection, and 3D point cloud processing.
C1 [Xu, Sheng; Liu, Chang; Zhang, Baochang; Lu, Jinhu] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing, Peoples R China.
   [Zhang, Baochang] Shenzhen Acad Aerosp Technol, Shenzhen, Peoples R China.
   [Lu, Jinhu] Beihang Univ, State Key Lab Software Dev Environm, Beijing, Peoples R China.
   [Lu, Jinhu] Beihang Univ, Beijing Adv Innovat Ctr Big Data & Brain Comp, Beijing, Peoples R China.
   [Guo, Guodong] Baidu Res, Inst Deep Learning, Beijing, Peoples R China.
   [Guo, Guodong] Natl Engn Lab Deep Learning Technol & Applicat, Beijing, Peoples R China.
   [Doermann, David] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY USA.
C3 Beihang University; Shenzhen Academy of Aerospace Technology; Beihang
   University; Beihang University; Baidu; State University of New York
   (SUNY) System; State University of New York (SUNY) Buffalo
RP Zhang, BC (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Beijing, Peoples R China.; Zhang, BC (corresponding author), Shenzhen Acad Aerosp Technol, Shenzhen, Peoples R China.
EM shengxu@buaa.edu.cn; lc1503@buaa.edu.cn; bczhang@buaa.edu.cn;
   lvjinhu@buaa.edu.cn; guoguodong01@baidu.com; doermann@buffalo.edu
RI LU, JINHU/JBR-7772-2023; Lu, Jinhu/B-6851-2013
OI Lu, Jinhu/0000-0003-0275-8387; Xu, Sheng/0000-0002-7742-275X
FU Key Research and Development Program of Shandong Province
   [2019JZZY011101];  [62076016];  [61876015]
FX This study was supported by Grant No. 2019JZZY011101 from the Key
   Research and Development Program of Shandong Province to Dianmin Sun.
   This work was supported in part by the National Natural Science
   Foundation of China under Grants 62076016 and 61876015.
CR [Anonymous], 2012, Wkly Epidemiol Rec, V87, P1
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2018, PEER PEER NETW APPL, DOI DOI 10.1007/s12083-017-0556-6
   [Anonymous], 2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2016.170, 10.1109/CVPR.2016.170]
   [Anonymous], 1990, Adv Neural Inform Process Syst.
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bak S, 2018, LECT NOTES COMPUT SC, V11217, P193, DOI 10.1007/978-3-030-01261-8_12
   Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Courbariaux M, 2015, ADV NEUR IN, V28
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Denil M., 2013, ADV NEURAL INFORM PR
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ge YX, 2018, ADV NEUR IN, V31
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2019, IEEE I CONF COMP VIS, P4908, DOI 10.1109/ICCV.2019.00501
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234
   Hou RB, 2021, IEEE T NEUR NET LEAR, V32, P4460, DOI 10.1109/TNNLS.2020.3017939
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang Y, 2019, IEEE T IMAGE PROCESS, V28, P1391, DOI 10.1109/TIP.2018.2874715
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jin X, 2020, GUT, V69, P1002, DOI 10.1136/gutjnl-2020-320926
   Juefei-Xu F, 2017, PROC CVPR IEEE, P4284, DOI 10.1109/CVPR.2017.456
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Li H., 2017, PRUNING FILTERS EFFI, DOI DOI 10.48550/ARXIV.1608.08710
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Lin Mingbao, 2020, P C ADV NEUR INF PRO
   Lin SH, 2017, AAAI CONF ARTIF INTE, P1424
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin XF, 2017, ADV NEUR IN, V30
   Liu CL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P854
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Martinez JA, 2020, POLITICA EXTERIOR DE CHILE ANTE ARGENTINA, BOLIVIA Y PERU EN EL MARCO DEL MULTILATERALISMO: AMENAZA U OPORTUNIDAD? (1900-1930), P1
   McDonnell M.D., 2018, ARXIV180208530, P1
   Odena A, 2017, PR MACH LEARN RES, V70
   Qi CR, 2017, ADV NEUR IN, V30
   Qin HT, 2020, PROC CVPR IEEE, P2247, DOI 10.1109/CVPR42600.2020.00232
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ristani E, 2018, PROC CVPR IEEE, P6036, DOI 10.1109/CVPR.2018.00632
   Romero A., 2015, ICLR, P1, DOI DOI 10.48550/ARXIV.1412.6550
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Tschannen M, 2018, ADV NEUR IN, V31
   Wan DW, 2018, LECT NOTES COMPUT SC, V11206, P322, DOI 10.1007/978-3-030-01216-8_20
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang XD, 2018, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2018.00094
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang ZW, 2020, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR42600.2020.00212
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu FR, 2020, IEEE T IMAGE PROCESS, V29, P8930, DOI 10.1109/TIP.2020.3020648
   Xu S, 2020, P 4 INT WORKSHOP EMB, P19, DOI DOI 10.1145/3410338.3412340
   Xu S, 2021, PROC CVPR IEEE, P5678, DOI 10.1109/CVPR46437.2021.00563
   Xu S, 2020, CHIN CONTR CONF, P7458, DOI 10.23919/CCC50068.2020.9189610
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zechun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P143, DOI 10.1007/978-3-030-58568-6_9
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhou Shuchang, 2016, ARXIV160606160
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
NR 78
TC 5
Z9 5
U1 1
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 26
DI 10.1145/3473340
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300003
DA 2024-07-18
ER

PT J
AU Zhu, AQ
   Zhang, L
   Chen, JT
   Zhou, YC
AF Zhu, Anqi
   Zhang, Lin
   Chen, Juntao
   Zhou, Yicong
TI Pedestrian-Aware Panoramic Video Stitching Based on a Structured Camera
   Array
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Panoramic video stitching; extrinsic calibration; photometric alignment;
   seam-cutting; instance segmentation
ID IMAGE
AB The panorama stitching system is an indispensable module in surveillance or space exploration. Such a system enables the viewer to understand the surroundings instantly by aligning the surrounding images on a plane and fusing them naturally. The bottleneck of existing systems mainly lies in alignment and naturalness of the transition of adjacent images. When facing dynamic foregrounds, they may produce outputs with misaligned semantic objects, which is evident and sensitive to human perception. We solve three key issues in the existing workflow that can affect its efficiency and the quality of the obtained panoramic video and present Pedestrian360, a panoramic video system based on a structured camera array (a spatial surround-view camera system). First, to get a geometrically aligned 360. view in the horizontal direction, we build a unified multi-camera coordinate system via a novel refinement approach that jointly optimizes camera poses. Second, to eliminate the brightness and color difference of images taken by different cameras, we design a photometric alignment approach by introducing a bias to the baseline linear adjustment model and solving it with two-step least-squares. Third, considering that the human visual system is more sensitive to high-level semantic objects, such as pedestrians and vehicles, we integrate the results of instance segmentation into the framework of dynamic programming in the seam-cutting step. To our knowledge, we are the first to introduce instance segmentation to the seam-cutting problem, which can ensure the integrity of the salient objects in a panorama. Specifically, in our surveillance oriented system, we choose the most significant target, pedestrians, as the seam avoidance target, and this accounts for the name Pedestrian360. To validate the effectiveness and efficiency of Pedestrian360, a large-scale dataset composed of videos with pedestrians in five scenes is established. The test results on this dataset demonstrate the superiority of Pedestrian360 compared to its competitors. Experimental results show that Pedestrian360 can stitch videos at a speed of 12 to 26 fps, which depends on the number of objects in the shooting scene and their frequencies of movements.
C1 [Zhu, Anqi; Zhang, Lin; Chen, Juntao] Tongji Univ, Sch Software Engn, 1239 Siping Rd, Shanghai 200092, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Taipa Univ Rd, Macau, Peoples R China.
C3 Tongji University; University of Macau
RP Zhang, L (corresponding author), Tongji Univ, Sch Software Engn, 1239 Siping Rd, Shanghai 200092, Peoples R China.
EM 1931555@tongji.edu.cn; cslinzhang@tongji.edu.cn; 1931554@tongji.edu.cn;
   yicongzhou@um.edu.mo
RI Zhou, Yicong/A-8017-2009; Chen, Jun/AGG-5204-2022
OI Zhou, Yicong/0000-0002-4487-6384; 
FU National Key Research and Development Project [2020YFB2103900]; National
   Natural Science Foundation of China [61973235]; Shanghai Science and
   Technology Innovation Plan [20510760400]; Shanghai Municipal Science and
   Technology Major Project [2021SHZDZX0100]; Fundamental Research Funds
   for the Central Universities
FX This study was supported in part by the National Key Research and
   Development Project under Grant 2020YFB2103900, in part by the National
   Natural Science Foundation of China under Grant 61973235, in part by the
   Shanghai Science and Technology Innovation Plan under Grant 20510760400,
   in part by the Shanghai Municipal Science and Technology Major Project
   under Grant 2021SHZDZX0100, and in part by the Fundamental Research
   Funds for the Central Universities.
CR [Anonymous], 2011, PROC CVPR IEEE
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bai ZW, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9091336
   Brown M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1218
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Choi K, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18092956
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Gao Y., 2017, IEEE COMMUNICATIONS, V3, P1
   Han SR, 2012, PROC SPIE, V8290, DOI 10.1117/12.910275
   He B. T., 2016, SENSORS, V16, P1
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hedi A., 2012, IFAC P, P120
   Heng L, 2014, IEEE INT CONF ROBOT, P4912, DOI 10.1109/ICRA.2014.6907579
   Heng L, 2013, IEEE INT C INT ROBOT, P1793, DOI 10.1109/IROS.2013.6696592
   Hu J., 2015, MULTIMEDIA EXPO ICME, P1, DOI DOI 10.1109/ICME.2015.7177506
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Kang J, 2019, IEEE ACCESS, V7, P26440, DOI 10.1109/ACCESS.2019.2900672
   Kang L, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19235310
   Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lee J, 2016, BOUND VALUE PROBL, DOI 10.1186/s13661-016-0603-x
   Li HD, 2006, INT C PATT RECOG, P630
   Li J., 2019, IEEE J-STSP, V14, P209, DOI [10.1109/JSTSP.2019.2953950, DOI 10.1109/JSTSP.2019.2953950]
   Li JG, 2018, CHIN AUTOM CONGR, P715, DOI 10.1109/CAC.2018.8623120
   Li N, 2018, SIGNAL IMAGE VIDEO P, V12, P967, DOI 10.1007/s11760-018-1241-9
   Liao TL, 2019, SIGNAL IMAGE VIDEO P, V13, P1199, DOI 10.1007/s11760-019-01466-9
   Liao TL, 2020, IEEE T IMAGE PROCESS, V29, P724, DOI 10.1109/TIP.2019.2934344
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Liu HW, 2011, SCI REP-UK, V1, DOI 10.1038/srep00112
   Liu QX, 2020, MULTIMED TOOLS APPL, V79, P3107, DOI 10.1007/s11042-018-6337-2
   Liu S, 2018, IEEE T IMAGE PROCESS, V27, P5032, DOI 10.1109/TIP.2018.2836313
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Natroshvili K, 2017, IEEE INT VEH SYM, P82, DOI 10.1109/IVS.2017.7995702
   Plath N., 2009, P 26 ANN INT C MACH, P817, DOI DOI 10.1145/1553374.1553479
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sharma A, 2014, 2014 PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS (GRAPP 2014), P247
   Souly N, 2017, IEEE I CONF COMP VIS, P5689, DOI 10.1109/ICCV.2017.606
   Suen STY, 2006, PROC SPIE, V6069, DOI 10.1117/12.640261
   Tennoe M, 2013, IEEE INT SYM MULTIM, P76, DOI 10.1109/ISM.2013.21
   Ueshiba T., 2002, Sensors, V8, P4
   Uyttendaele M, 2001, PROC CVPR IEEE, P509
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Xu Y, 2014, IEEE T CIRC SYST VID, V24, P1061, DOI 10.1109/TCSVT.2013.2290576
   Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247
   Zhang BY, 2014, IEEE COMPUT SOC CONF, P676, DOI 10.1109/CVPRW.2014.103
   Zhang GF, 2016, IEEE T IMAGE PROCESS, V25, P3099, DOI 10.1109/TIP.2016.2535225
   Zhang L, 2019, IEEE IMAGE PROC, P4150, DOI [10.1109/ICIP.2019.8803453, 10.1109/icip.2019.8803453]
   Zhang LX, 2007, PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS, P45, DOI 10.1109/ICIG.2007.59
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zou WB, 2015, IEEE I CONF COMP VIS, P406, DOI 10.1109/ICCV.2015.54
NR 56
TC 4
Z9 4
U1 6
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 136
DI 10.1145/3460511
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800020
DA 2024-07-18
ER

PT J
AU Kumar, A
   Manikandan, R
   Kose, U
   Gupta, D
   Satapathy, SC
AF Kumar, Ambeshwar
   Manikandan, Ramachandran
   Kose, Utku
   Gupta, Deepak
   Satapathy, Suresh C.
TI Doctor's Dilemma: Evaluating an Explainable Subtractive Spatial
   Lightweight Convolutional Neural Network for Brain Tumor Diagnosis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Explainable artificial intelligence; subtractive spatial lightweight
   convolutional neural networks; brain tumor diagnosis; medical imaging;
   deep learning
ID ARTIFICIAL-INTELLIGENCE; COVID-19 CLASSIFICATION; BREAST-CANCER; DEEP
   CNN; SEGMENTATION; FUSION; ATTENTION; PLATFORM; SYSTEM
AB In Medicine Deep Learning has become an essential tool to achieve outstanding diagnosis on image data. However, one critical problem is that Deep Learning comes with complicated, black-box models so it is not possible to analyze their trust level directly. So, Explainable Artificial Intelligence (XAI) methods are used to build additional interfaces for explaining how the model has reached the outputs by moving from the input data. Of course, that's again another competitive problem to analyze if such methods are successful according to the human view. So, this paper comes with two important research efforts: (1) to build an explainable deep learning model targeting medical image analysis, and (2) to evaluate the trust level of this model via several evaluation works including human contribution. The target problem was selected as the brain tumor classification, which is a remarkable, competitive medical image-based problem for Deep Learning. In the study, MR-based pre-processed brain images were received by the Subtractive Spatial Lightweight Convolutional Neural Network (SSLW-CNN) model, which includes additional operators to reduce the complexity of classification. In order to ensure the explainable background, the model also included Class Activation Mapping (CAM). It is important to evaluate the trust level of a successful model. So, numerical success rates of the SSLW-CNN were evaluated based on the peak signal-to-noise ratio (PSNR), computational time, computational overhead, and brain tumor classification accuracy. The objective of the proposed SSLW-CNN model is to obtain faster and good tumor classification with lesser time. The results illustrate that the SSLW-CNN model provides better performance of PSNR which is enhanced by 8%, classification accuracy is improved by 33%, computation time is reduced by 19%, computation overhead is decreased by 23%, and classification time is minimized by 13%, as compared to state-of-the-art works. Because the model provided good numerical results, it was then evaluated in terms of XAI perspective by including doctor-model based evaluations such as feedback CAM visualizations, usability, expert surveys, comparisons of CAM with other XAI methods, and manual diagnosis comparison. The results show that the SSLW-CNN provides good performance on brain tumor diagnosis and ensures a trustworthy solution for the doctors.
C1 [Kumar, Ambeshwar; Manikandan, Ramachandran] SASTRA Deemed Univ, Sch Comp, Thanjavur 613401, Tamil Nadu, India.
   [Kose, Utku] Suleyman Demirel Univ, TR-32260 Isparta, Turkey.
   [Gupta, Deepak] Maharaja Agrasen Inst Technol, Delhi 110086, India.
   [Satapathy, Suresh C.] KIIT Univ, Bhubaneswar 751024, India.
C3 Shanmugha Arts, Science, Technology & Research Academy (SASTRA);
   Suleyman Demirel University; Maharaja Agrasen Institute of Technology;
   Kalinga Institute of Industrial Technology (KIIT)
RP Kumar, A (corresponding author), SASTRA Deemed Univ, Sch Comp, Thanjavur 613401, Tamil Nadu, India.
EM ambeshwar.kumar@gmail.com; srmanimt75@gmail.com; utkukose@sdu.edu.tr;
   deepakgupta@mait.ac.in; suresh.satapathyfcs@kiit.ac.in
RI kumar, Dr. Ambeshwar/AAG-8011-2021; Ramachandran,
   Manikandan/B-2783-2014; Kose, Utku/C-8683-2009; Gupta,
   Deepak/AAV-2728-2020
OI kumar, Dr. Ambeshwar/0000-0001-5393-6361; Ramachandran,
   Manikandan/0000-0001-6116-2132; Kose, Utku/0000-0002-9652-6415; Gupta,
   Deepak/0000-0002-3019-7161
CR Abd Ghani MK, 2020, NEURAL COMPUT APPL, V32, P625, DOI 10.1007/s00521-018-3882-6
   Abd-Ellah MK, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0332-4
   Abdel-Maksoud E, 2015, EGYPT INFORM J, V16, P71, DOI 10.1016/j.eij.2015.01.003
   Adadi A, 2018, IEEE ACCESS, V6, P52138, DOI 10.1109/ACCESS.2018.2870052
   Alom MZ, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8030292
   Alonso-Dos-Santos M, 2019, SUSTAINABILITY-BASEL, V11, DOI 10.3390/su11041079
   Alqazzaz S, 2019, COMPUT VIS MEDIA, V5, P209, DOI 10.1007/s41095-019-0139-y
   Arasi PRE, 2019, J MED SYST, V43, DOI 10.1007/s10916-019-1266-9
   Ari A, 2018, TURK J ELECTR ENG CO, V26, P2275, DOI 10.3906/elk-1801-8
   Arunkumar N, 2019, SOFT COMPUT, V23, P9083, DOI 10.1007/s00500-018-3618-7
   Arunkumar N., 2018, CONCURR COMP-PRACT E, V22
   Bahadure NB, 2017, INT J BIOMED IMAGING, V2017, DOI 10.1155/2017/9749108
   Arrieta AB, 2020, INFORM FUSION, V58, P82, DOI 10.1016/j.inffus.2019.12.012
   Bi XA, 2019, FRONT GENET, V10, DOI 10.3389/fgene.2019.00976
   Charniak E., 2019, INTRO DEEP LEARNING
   Chen YJ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1226, DOI 10.1145/3219819.3219974
   Cui SG, 2018, J HEALTHC ENG, V2018, DOI 10.1155/2018/4940593
   Daglarli Evren, 2020, ADV APPL DEEP LEARN
   Duran-Lopez L, 2020, IEEE ACCESS, V8, P128613, DOI 10.1109/ACCESS.2020.3008868
   Fu R., 2020, BRIT MACH VIS C
   Fukui H, 2019, PROC CVPR IEEE, P10697, DOI 10.1109/CVPR.2019.01096
   Gagana B., 2019, CLASS ACTIVATION MAP
   Gao F, 2018, COMPUT MED IMAG GRAP, V70, P53, DOI 10.1016/j.compmedimag.2018.09.004
   Gao J, 2020, NEURAL COMPUT, V32, P829, DOI 10.1162/neco_a_01273
   Gass SM, 2005, STUD SECOND LANG ACQ, V27, P1, DOI 10.1017/S0272263105050011
   Gibson E, 2018, COMPUT METH PROG BIO, V158, P113, DOI 10.1016/j.cmpb.2018.01.025
   Gupta T., 2017, MED IMAGE ANAL, P1
   Janssen CP, 2019, INT J HUM-COMPUT ST, V131, P99, DOI 10.1016/j.ijhcs.2019.05.006
   Kamarujjaman, 2019, PATTERN ANAL APPL, V22, P1561, DOI 10.1007/s10044-019-00806-2
   Kim K, 2011, INT J HUM-COMPUT INT, V27, P383, DOI 10.1080/10447318.2011.540493
   Korolev S, 2017, I S BIOMED IMAGING, P835, DOI 10.1109/ISBI.2017.7950647
   Kumar A, 2019, APPL SOFT COMPUT, V82, DOI 10.1016/j.asoc.2019.105528
   Kumar NAM, 2016, 2016 IEEE UTTAR PRADESH SECTION INTERNATIONAL CONFERENCE ON ELECTRICAL, COMPUTER AND ELECTRONICS ENGINEERING (UPCON), P1, DOI [10.1109/HMI.2016.7449165, 10.1109/UPCON.2016.7894614]
   LaLonde R., 2020, INT C MED IM COMP CO, P294, DOI [DOI 10.1007/978-3-030-59710-829, 10.1007/978-3-030-59710-8_29, DOI 10.1007/978-3-030-59710-8_29]
   Lamy JB, 2019, ARTIF INTELL MED, V94, P42, DOI 10.1016/j.artmed.2019.01.001
   Laukamp KR, 2019, EUR RADIOL, V29, P124, DOI 10.1007/s00330-018-5595-8
   Lu SY, 2021, NEURAL COMPUT APPL, V33, P10799, DOI 10.1007/s00521-020-05082-4
   Lu SY, 2019, J COMPUT SCI-NETH, V30, P41, DOI 10.1016/j.jocs.2018.11.008
   Lundervold AS, 2019, Z MED PHYS, V29, P102, DOI 10.1016/j.zemedi.2018.11.002
   Mohsen Heba, 2018, Future Computing and Informatics Journal, V3, P68, DOI 10.1016/j.fcij.2017.12.001
   Mortazi A, 2018, LECT NOTES COMPUT SC, V11046, P98, DOI 10.1007/978-3-030-00919-9_12
   Papanastasopoulos Z, 2020, PROC SPIE, V11314, DOI 10.1117/12.2549298
   Poma Y., 2020, Journal of Automation, Mobile Robotics and Intelligent Systems, V14, DOI [DOI 10.14313/JAMRIS/1-2020/12, 10.14313/JAMRIS/1-2020/12]
   Sajjad M, 2019, J COMPUT SCI-NETH, V30, P174, DOI 10.1016/j.jocs.2018.12.003
   Samek W., 2017, ARXIV PREPRINT ARXIV
   Samek W, 2019, LNAI, V11700
   Sarmento RM, 2020, FUTURE GENER COMP SY, V105, P135, DOI 10.1016/j.future.2019.11.033
   Seetha J., 2018, BIOMED PHARMACOL J, V11, P1457, DOI [DOI 10.13005/bpj/1511, 10.13005/bpj/1511]
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shakeel PM, 2019, IEEE ACCESS, V7, P5577, DOI 10.1109/ACCESS.2018.2883957
   Shrestha A, 2019, IEEE ACCESS, V7, P53040, DOI 10.1109/ACCESS.2019.2912200
   Siddique N., 2020, IMAGE VIDEO PROCESSI, P1
   Tiwari P, 2017, INT J COMPUT INT SYS, V10, P104, DOI 10.2991/ijcis.2017.10.1.8
   Varela-Santos S, 2021, INFORM SCIENCES, V545, P403, DOI 10.1016/j.ins.2020.09.041
   Varuna Shree N, 2018, Brain Inform, V5, P23, DOI 10.1007/s40708-017-0075-5
   Varuna Shree N, 2018, BRAIN INFORM, V5, P23, DOI DOI 10.1007/S40708-017-0075-5
   Vasconcelos FFX, 2020, ENG APPL ARTIF INTEL, V91, DOI 10.1016/j.engappai.2020.103585
   Wahler R., 2020, EYE TRACKING TOURISM, P85
   Wang SH, 2021, INFORM FUSION, V68, P131, DOI 10.1016/j.inffus.2020.11.005
   Wang SH, 2021, INFORM FUSION, V67, P208, DOI 10.1016/j.inffus.2020.10.004
   Woo WL, 2020, IEEE INSTRU MEAS MAG, V23, P71, DOI 10.1109/MIM.2020.9062691
   Xue Y, 2017, CONTRAST MEDIA MOL I, DOI 10.1155/2017/9512370
   Zhang C, 2019, INT J BIOMED IMAGING, V2019, DOI 10.1155/2019/7305832
   Zhang YD, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2020.102439
   Zhang YD, 2020, INFORM FUSION, V64, P149, DOI 10.1016/j.inffus.2020.07.006
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zou L, 2017, IEEE ACCESS, V5, P23626, DOI 10.1109/ACCESS.2017.2762703
NR 67
TC 11
Z9 11
U1 2
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 105
DI 10.1145/3457187
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA YA4BM
UT WOS:000738280600007
DA 2024-07-18
ER

PT J
AU Dong, XB
   Kim, S
   Jin, Z
   Hwang, JY
   Cho, S
   Teoh, ABJ
AF Dong, Xingbo
   Kim, Soohyong
   Jin, Zhe
   Hwang, Jung Yeon
   Cho, Sangrae
   Teoh, Andrew Beng Jin
TI Secure Chaff-less Fuzzy Vault for Face Identification Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Chaff-less fuzzy vault; face identification system; biometric template
   protection
ID BIOMETRIC ENCRYPTION; SCHEME
AB Biometric cryptosystems such as fuzzy vaults represent one of the most popular approaches for secret and biometric template protection. However, they are solely designed for biometric verification, where the user is required to input both identity credentials and biometrics. Several practical questions related to the implementation of biometric cryptosystems remain open, especially in regard to biometric template protection. In this article, we propose a face cryptosystem for identification (FCI) in which only biometric input is needed. Our FCI is composed of a one-to-N search subsystem for template protection and a one-to-one match chaff-less fuzzy vault (CFV) subsystem for secret protection. The first subsystem stores N facial features, which are protected by index-of-maximum (IoM) hashing, enhanced by a fusion module for search accuracy. When a face image of the user is presented, the subsystem returns the top k matching scores and activates the corresponding vaults in the CFV subsystem. Then, one-to-one matching is applied to the k vaults based on the probe face, and the identifier or secret associated with the user is retrieved from the correct matched vault. We demonstrate that coupling between the IoM hashing and the CFV resolves several practical issues related to fuzzy vault schemes. The FCI system is evaluated on three large-scale public unconstrained face datasets (LFW, VGG2, and IJB-C) in terms of its accuracy, computation cost, template protection criteria, and security.
C1 [Dong, Xingbo; Jin, Zhe] Monash Univ Malaysia, Sch Informat Technol, Subang Jaya, Selangor, Malaysia.
   [Kim, Soohyong; Cho, Sangrae] Elect & Telecommun Res Inst ETRI, Daejeon, South Korea.
   [Hwang, Jung Yeon] Sungshin Womens Univ, Dept Math, Seoul, South Korea.
   [Teoh, Andrew Beng Jin] Yonsei Univ, Sch Elect & Elect Engn, Seoul, South Korea.
C3 Monash University; Monash University Malaysia; Electronics &
   Telecommunications Research Institute - Korea (ETRI); Sungshin Women's
   University; Yonsei University
RP Dong, XB (corresponding author), Monash Univ Malaysia, Sch Informat Technol, Subang Jaya, Selangor, Malaysia.
EM Xingbo.Dong@monash.edu; lifewsky@etri.re.kr; Jin.Zhe@monash.edu;
   videmot@gmail.com; sangrae@etri.re.kr; bjteoh@yonsei.ac.kr
RI Teoh, Andrew Beng Jin/F-4422-2010; Dong, Xingbo/AAN-9103-2021
OI Dong, Xingbo/0000-0001-9782-6068
FU Institute for Information & Communications Technology Promotion (IITP) -
   Korean government (MSIT) [2016-0-00097, 2018-0-00189]
FX This work was supported by the Institute for Information &
   Communications Technology Promotion (IITP) via grants funded by the
   Korean government (MSIT) (No. 2016-0-00097, Development of
   Biometrics-based Key Infrastructure Technology for On-line
   Identification, and No. 2018-0-00189, Security Technology for Portal
   Device to Connect HumanInfrastructure-Service in Highly Trusted
   Intelligent Information Service).
CR [Anonymous], 2018, 2018 reform of EU data protection rules
   [Anonymous], 2011, CVPR 2011 WORKSH IEE, DOI DOI 10.1109/CVPRW.2011.5981720
   Bissessar D, 2016, STUD COMPUT INTELL, V621, P339, DOI 10.1007/978-3-319-26450-9_13
   BRASSARD G, 1988, J COMPUT SYST SCI, V37, P156, DOI 10.1016/0022-0000(88)90005-0
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Cavoukian A, 2012, REV POLICY RES, V29, P37, DOI 10.1111/j.1541-1338.2011.00537.x
   Cole O, 2017, IEEE INT CONF DISTR, P199, DOI 10.1109/DCOSS.2017.19
   Daugman J, 2006, P IEEE, V94, P1927, DOI 10.1109/JPROC.2006.884092
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dodis Y, 2004, LECT NOTES COMPUT SC, V3027, P523
   Franssen T, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P1069, DOI 10.1109/IIH-MSP.2008.315
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Guruswami V., 2001, Ph.D. dissertation
   Hao F, 2006, IEEE T COMPUT, V55, P1081, DOI 10.1109/TC.2006.138
   Jain AK, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/579416
   Jin Z, 2018, IEEE T INF FOREN SEC, V13, P393, DOI 10.1109/TIFS.2017.2753172
   Juels A, 2006, DESIGN CODE CRYPTOGR, V38, P237, DOI 10.1007/s10623-005-6343-z
   Juels A, 1999, 6TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P28, DOI 10.1145/319709.319714
   Kevenaar TAM, 2005, FOURTH IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P21, DOI 10.1109/AUTOID.2005.24
   Khalil-Hani M, 2013, FUTURE GENER COMP SY, V29, P800, DOI 10.1016/j.future.2012.02.002
   Lai YL, 2021, IEEE T DEPEND SECURE, V18, P58, DOI 10.1109/TDSC.2018.2874245
   Lai YL, 2019, INFORM SCIENCES, V502, P492, DOI 10.1016/j.ins.2019.05.064
   Lee DaeJong, 2013, [Journal of Korean Institute of Intelligent Systems, 한국지능시스템학회 논문지], V23, P113
   Lee H, 2012, FUTURE GENER COMP SY, V28, P218, DOI 10.1016/j.future.2010.11.006
   Li C, 2016, IEEE T INF FOREN SEC, V11, P543, DOI 10.1109/TIFS.2015.2505630
   Li L, 2018, LECT NOTES ARTIF INT, V11012, P710, DOI 10.1007/978-3-319-97304-3_54
   Li P, 2010, J NETW COMPUT APPL, V33, P207, DOI 10.1016/j.jnca.2009.12.003
   Liao SC, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014)
   Lifang Wu, 2010, 2010 Proceedings of 2nd International Symposium on Data, Privacy & E-Commerce (ISDPE 2010), P45, DOI 10.1109/ISDPE.2010.13
   Lim MH, 2015, IEEE SIGNAL PROC MAG, V32, P77, DOI 10.1109/MSP.2015.2423693
   Lu Haiping, 2009, 2009 16 INT C DIG SI, P1, DOI DOI 10.1109/ICDSP.2009.5201257
   Macmillan N. A., 2004, DETECTION THEORY USE, DOI [DOI 10.4324/9781410611147, 10.4324/9781410611147]
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   Merkle J., 2010, PERFORMANCE FUZZY VA
   MIHAILESCU P, 2007, FUZZY VAULT FINGERPR
   Nagar A, 2008, INT C PATT RECOG, P822
   Nandakumar K, 2007, IEEE T INF FOREN SEC, V2, P744, DOI 10.1109/TIFS.2007.908165
   Nandakumar K, 2015, IEEE SIGNAL PROC MAG, V32, P88, DOI 10.1109/MSP.2015.2427849
   Nunes ID, 2019, FUTURE GENER COMP SY, V98, P259, DOI 10.1016/j.future.2019.03.051
   Nyang D, 2007, LECT NOTES COMPUT SC, V4554, P491
   Osadchy M, 2019, IEEE T DEPEND SECURE, V16, P796, DOI 10.1109/TDSC.2018.2804949
   Pussewalage HSG, 2014, 2014 11TH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (FSKD), P956, DOI 10.1109/FSKD.2014.6980968
   Rathgeb C, 2011, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2011-3
   Scheirer WJ, 2007, 2007 BIOMETRICS SYMPOSIUM, P30
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Uludag U, 2004, P IEEE, V92, P948, DOI 10.1109/JPROC.2004.827372
   Wang DY, 2017, IEEE T PATTERN ANAL, V39, P1122, DOI 10.1109/TPAMI.2016.2582166
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang Y, 2007, IEEE I CONF COMP VIS, P2800
   Xi K, 2016, ACM T EMBED COMPUT S, V15, DOI 10.1145/2930669
   Xi K, 2010, HANDBOOK OF INFORMATION AND COMMUNICATION SECURITY, P129, DOI 10.1007/978-3-642-04117-4_7
   Yang WC, 2014, IEEE T INF FOREN SEC, V9, P1179, DOI 10.1109/TIFS.2014.2328095
   Yi Dong, 2014, ARXIV14117923
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang N., 2007, Tech. Rep. 07-49, P7
NR 55
TC 8
Z9 8
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 79
DI 10.1145/3442198
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400003
DA 2024-07-18
ER

PT J
AU Wang, R
   Liang, D
   Cao, XC
   Guo, YF
AF Wang, Rui
   Liang, Dong
   Cao, Xiaochun
   Guo, Yuanfang
TI Semantic Correspondence with Geometric Structure Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Curl; image correspondence; bilateral symmetry
ID GRAPH; RETRIEVAL; ALGORITHM
AB This article studies the correspondence problem for semantically similar images, which is challenging due to the joint visual and geometric deformations. We introduce the Flip-aware Distance Ratio method (FDR) to solve this problem from the perspective of geometric structure analysis. First, a distance ratio constraint is introduced to enforce the geometric consistencies between images with large visual variations, whereas local geometric jitters are tolerated via a smoothness term. For challenging cases with symmetric structures, our proposed method exploits Curl to suppress the mismatches. Subsequently, image correspondence is formulated as a permutation problem, for which we propose a Gradient Guided Simulated Annealing (GGSA) algorithm to perform a robust discrete optimization. Experiments on simulated and real-world datasets, where both visual and geometric deformations are present, indicate that our method significantly improves the baselines for both visually and semantically similar images.
C1 [Wang, Rui; Liang, Dong; Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing Shi 100093, Peoples R China.
   [Guo, Yuanfang] Beihang Univ, Sch Comp Sci & Engn, 37 Xueyuan Rode, Beijing 100191, Peoples R China.
   [Wang, Rui; Liang, Dong; Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, 65 Xingshikou Rode, Beijing 100093, Peoples R China.
   [Wang, Rui; Liang, Dong; Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Beihang University; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Liang, D (corresponding author), Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing Shi 100093, Peoples R China.; Liang, D (corresponding author), Univ Chinese Acad Sci, Sch Cyber Secur, 65 Xingshikou Rode, Beijing 100093, Peoples R China.; Liang, D (corresponding author), Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
EM wangrui@iie.ac.cn; dongliang.ucas@gmail.com; caoxiaochun@iie.ac.cn;
   eeandyguo@connect.ust.hk
RI wang, rui/JAC-6240-2023
OI , Rui Wang/0000-0002-4792-1945
FU National Natural Science Foundation of China [U1936208, 61733007]
FX Thiswork was supported in part by the National Natural Science
   Foundation of China under grants U1936208 and 61733007.
CR Afonso MV, 2014, IEEE T MULTIMEDIA, V16, P1, DOI 10.1109/TMM.2013.2281023
   Benavent X, 2013, IEEE T MULTIMEDIA, V15, P2009, DOI 10.1109/TMM.2013.2267726
   Berg AC, 2005, PROC CVPR IEEE, P26
   Bhat Goutam, 2020, P EUR C COMP VIS
   Bristow H, 2015, IEEE I CONF COMP VIS, P4024, DOI 10.1109/ICCV.2015.458
   Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222
   Cho MS, 2014, PROC CVPR IEEE, P2091, DOI 10.1109/CVPR.2014.268
   Cho MS, 2013, IEEE I CONF COMP VIS, P25, DOI 10.1109/ICCV.2013.11
   Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Duchenne O, 2011, IEEE I CONF COMP VIS, P1792, DOI 10.1109/ICCV.2011.6126445
   Feng Fangxiang, 2015, ACM T MULTIM COMPUT, V12, P22, DOI DOI 10.1145/2808205
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Ge SM, 2017, PROC CVPR IEEE, P426, DOI 10.1109/CVPR.2017.53
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   GRANVILLE V, 1994, IEEE T PATTERN ANAL, V16, P652, DOI 10.1109/34.295910
   Hassner T, 2012, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2012.6247842
   Huang SY, 2019, IEEE I CONF COMP VIS, P2010, DOI 10.1109/ICCV.2019.00210
   Hur J, 2015, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2015.7298745
   Jeon S, 2019, IEEE I CONF COMP VIS, P7293, DOI 10.1109/ICCV.2019.00739
   Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299
   Kim S., 2018, NeurIPS, P6126
   Kim S, 2020, IEEE T PATTERN ANAL, V42, P59, DOI 10.1109/TPAMI.2018.2878240
   Kim S, 2017, IEEE I CONF COMP VIS, P4539, DOI 10.1109/ICCV.2017.485
   Kim S, 2017, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2017.73
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LAWLER EL, 1963, MANAGE SCI, V9, P586, DOI 10.1287/mnsc.9.4.586
   Lee Junghyup, 2020, IEEE T PATTERN ANAL
   Lee SY, 2013, IEEE T MULTIMEDIA, V15, P1719, DOI 10.1109/TMM.2013.2271747
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Leordeanu M., 2009, NIPS, P1114
   Li CY, 2008, IEEE T MULTIMEDIA, V10, P447, DOI 10.1109/TMM.2008.917421
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670
   Novotny D, 2017, PROC CVPR IEEE, P2867, DOI 10.1109/CVPR.2017.306
   Pachauri D., 2013, Advances in Neural Information Processing Systems, V26, P1860
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Roberts R., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3137, DOI 10.1109/CVPR.2011.5995549
   Rocco I, 2018, PROC CVPR IEEE, P6917, DOI 10.1109/CVPR.2018.00723
   Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12
   SCHMIDT DC, 1976, J ACM, V23, P433, DOI 10.1145/321958.321963
   Suh YM, 2015, PROC CVPR IEEE, P5070, DOI 10.1109/CVPR.2015.7299142
   Suh Y, 2012, LECT NOTES COMPUT SC, V7574, P624, DOI 10.1007/978-3-642-33712-3_45
   Suna Kim, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P98, DOI 10.1007/978-3-642-37431-9_8
   Terada Y, 2014, PR MACH LEARN RES, V32, P847
   Truong P, 2020, PROC CVPR IEEE, P6257, DOI 10.1109/CVPR42600.2020.00629
   Ufer N, 2017, PROC CVPR IEEE, P5929, DOI 10.1109/CVPR.2017.628
   ULLMANN JR, 1976, J ACM, V23, P31, DOI 10.1145/321921.321925
   Vondrick C, 2018, LECT NOTES COMPUT SC, V11217, P402, DOI 10.1007/978-3-030-01261-8_24
   Wang R, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P606, DOI 10.1145/2964284.2967293
   Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620
   ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149
   Zhao WL, 2013, IEEE T IMAGE PROCESS, V22, P980, DOI 10.1109/TIP.2012.2226043
   ZhiyuWang Peng Cui, 2014, ACM T MULTIM COMPUT, V10, P21
   Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459
NR 59
TC 1
Z9 1
U1 2
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 83
DI 10.1145/3441576
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400007
DA 2024-07-18
ER

PT J
AU Alkhariji, L
   Alhirabi, N
   Alraja, MN
   Barhamgi, M
   Rana, O
   Perera, C
AF Alkhariji, Lamya
   Alhirabi, Nada
   Alraja, Mansour Naser
   Barhamgi, Mahmoud
   Rana, Omer
   Perera, Charith
TI Synthesising Privacy by Design Knowledge Toward Explainable Internet of
   Things Application Designing in Healthcare
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Internet of Things; privacy; privacy by design; privacy assistant;
   knowledge engineering; healthcare; explainable privacy
AB Privacy by Design (PbD) is the most common approach followed by software developers who aim to reduce risks within their application designs, yet it remains commonplace for developers to retain little conceptual understanding of what is meant by privacy. A vision is to develop an intelligent privacy assistant to whom developers can easily ask questions to learn how to incorporate different privacy-preserving ideas into their IoT application designs. This article lays the foundations toward developing such a privacy assistant by synthesising existing PbD knowledge to elicit requirements. It is believed that such a privacy assistant should not just prescribe a list of privacy-preserving ideas that developers should incorporate into their design. Instead, it should explain how each prescribed idea helps to protect privacy in a given application design context this approach is defined as "Explainable Privacy. "A total of 74 privacy patterns were analysed and reviewed using ten different PbD schemes to understand how each privacy pattern is built and how each helps to ensure privacy. Due to page limitations, we have presented a detailed analysis in Reference [3]. In addition, different real-world Internet of Things (Wi) use-cases, including a healthcare application, were used to demonstrate how each privacy pattern could be applied to a given application design. By doing so, several knowledge engineering requirements were identified that need to be considered when developing a privacy assistant. It was also found that, when compared to other IoT application domains, privacy patterns can significantly benefit healthcare applications. In conclusion, this article identifies the research challenges that must be addressed if one wishes to construct an intelligent privacy assistant that can truly augment software developers' capabilities at the design phase.
C1 [Alkhariji, Lamya; Alhirabi, Nada; Rana, Omer; Perera, Charith] Cardiff Univ, Cardiff, Wales.
   [Alraja, Mansour Naser] Dhofar Univ, Dept Management Informat Syst, Salalah 2509, Oman.
   [Barhamgi, Mahmoud] Claude Bernard Lyon 1 Univ, F-69622 Lyon, France.
C3 Cardiff University; Dhofar University; Universite Claude Bernard Lyon 1
RP Alkhariji, L (corresponding author), Cardiff Univ, Cardiff, Wales.
EM AlkharijiLa@cardiff.ac.uk; alhirabin@cardiff.ac.uk; malraja@du.edu.om;
   mahmoud.barhamgi@liris.cnrs.fr; RanaOF@cardiff.ac.uk;
   pereraC@cardiff.ac.uk
RI Rana, Omer/AAP-8523-2020; Alraja, Mansour/N-3186-2015; Alhirabi,
   Nada/JVN-0373-2024; Alraja, Mansour/IAP-8215-2023
OI Rana, Omer/0000-0003-3597-2646; Alraja, Mansour/0000-0003-3492-8838;
   Alhirabi, Nada/0000-0002-8751-1000; 
FU PETRAS2: National Centre of Excellence for IoT Systems Cyber Security
   [EP/S035362/1]; Quarriable Smart City Data Markets [EP/T517203/1]; PACE:
   Privacy-aware Cloud Ecosystems [EP/R033439/1]; The Research Council
   (TRC), Sultanate of Oman (Block Fund-Research Grant)
   [BFP/RGP/ICT/19/186]; EPSRC [EP/R033439/1] Funding Source: UKRI; SPF
   [EP/S035362/1] Funding Source: UKRI
FX Charith Perera's work is partially supported by PETRAS2: National Centre
   of Excellence for IoT Systems Cyber Security (Grant No. EP/S035362/1)
   and Quarriable Smart City Data Markets (Grant No. EP/T517203/1). Omer
   Rana's work is supported by PACE: Privacy-aware Cloud Ecosystems (Grant
   No. EP/R033439/1). Mansour Naser Alraja is supported by The Research
   Council (TRC), Sultanate of Oman (Block Fund-Research Grant) (Grant No.
   BFP/RGP/ICT/19/186).
CR Abdul-Ghani HA, 2019, J SENS ACTUAT NETW, V8, DOI 10.3390/jsan8020022
   Alkhariji Lamya, 2020, EXAMING INTERPLAY PR
   [Anonymous], 2005, J INTERNET LAW
   [Anonymous], 2015, PERS INF PROT EL DOC
   Anwar Malik Nadeem, 2020, Smart CitiesOpportunities and Challenges. Select Proceedings of ICSC 2019. Lecture Notes in Civil Engineering (LNCE 58), P387, DOI 10.1007/978-981-15-2545-2_33
   Budgen David., 2003, SOFTWARE DESIGN
   Bushmann F., 1996, Pattern-Oriented Software Architecture: A System of Patterns, V1, P476, DOI DOI 10.1192/BJP.108.452.101
   Cate FredH., 2006, Consumer Protection in the Age of the "Information Economy,", P341
   Cavoukian A., 2010, P 32 INT C DAT PROT
   Cavoukian A., 2012, Privacy and Drones : Unmanned Aerial Vehicles, (August), P1
   Cavoukian A., 2009, Privacy by design: The 7 foundational principles, V5
   Chen DT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198308
   Dewitte P, 2019, SAC '19: PROCEEDINGS OF THE 34TH ACM/SIGAPP SYMPOSIUM ON APPLIED COMPUTING, P1512, DOI 10.1145/3297280.3297595
   European Commission, 2016, OFFIC J EUR UNION
   Figueroa Asuncion Gomez-perez, 2009, DEMETRA EOOD, P1, DOI [10.1016/j.landurbplan.2011.04.007, DOI 10.1016/J.LANDURBPLAN.2011.04.007]
   Fisk G, 2015, 2015 IEEE SECURITY AND PRIVACY WORKSHOPS (SPW), P193, DOI 10.1109/SPW.2015.23
   Gangemi A., 2009, Ontology design patterns, P221, DOI [10.1007/978-3-540-92673-310, DOI 10.1007/978-3-540-92673-310]
   Hitzler Pascal, 2009, Foundations of semantic web technologies
   Hoepman JH, 2014, IFIP ADV INF COMM TE, V428, P446
   Hong J, 2017, IEEE PERVAS COMPUT, V16, P40, DOI 10.1109/MPRV.2017.2940957
   IBM, 2019, CARF REAL TIM VEH TR
   Intersoft Consulting, 2020, PRIVACY BY DESIGN
   ISO/IEC 29100, 2011, 29100 ISOIEC
   Li W, 2017, 2017 1ST IEEE SYMPOSIUM ON PRIVACY-AWARE COMPUTING (PAC), P176, DOI 10.1109/PAC.2017.26
   Martin Rost, 2011, DUD, P1
   Microsoft, 2019, MICR SEC DEV LIF THR
   OLEARY DE, 1995, IEEE EXPERT, V10, P48, DOI 10.1109/64.395352
   Perera C., 2016, Proceedings of the 6th International Conference on the Internet of Things, Germany, P83
   Perera C., 2020, PRIVACY PATTERNS INT
   Perera C, 2020, INFORM SCIENCES, V512, P238, DOI 10.1016/j.ins.2019.09.061
   Perera C, 2014, IEEE COMMUN SURV TUT, V16, P414, DOI 10.1109/SURV.2013.042313.00197
   Privacypatterns.eu, 2016, COLL PATT BETT PRIV
   Privacypatterns.org, 2015, PRIV PATT
   Rubinstein Ira S., 2013, Berkeley Technology Law Journal, V28, P1333
   Sakai K, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2501643.2501645
   Sivaram GSVS, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1556134.1556140
   Ul Hassan M, 2019, FUTURE GENER COMP SY, V97, P512, DOI 10.1016/j.future.2019.02.060
   Wang JJ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3209659
   Wong RY, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300492
   Wright D., 2014, International Review of Law, Computers Technology, V28, P277, DOI [DOI 10.1080/13600869.2014.913874, 10.1080/13600869.2014.913874]
   Wright D, 2011, COMMUN ACM, V54, P119, DOI 10.1145/1897816.1897848
   Wu DP, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978570
NR 42
TC 7
Z9 7
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 62
DI 10.1145/3434186
PG 29
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100005
DA 2024-07-18
ER

PT J
AU Zhang, CY
   Song, JY
   Zhu, XF
   Zhu, L
   Zhang, SC
AF Zhang, Chengyuan
   Song, Jiayu
   Zhu, Xiaofeng
   Zhu, Lei
   Zhang, Shichao
TI HCMSL: Hybrid Cross-modal Similarity Learning for Cross-modal Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modal retrieval; deep learning; intra-modal semantic correlation;
   hybrid cross-modal similarity
AB The purpose of cross-modal retrieval is to find the relationship between different modal samples and to retrieve other modal samples with similar semantics by using a certain modal sample. As the data of different modalities presents heterogeneous low-level feature and semantic-related high-level features, the main problem of cross-modal retrieval is how to measure the similarity between different modalities. In this article, we present a novel cross-modal retrieval method, named Hybrid Cross-Modal Similarity Learning model (HCMSL for short). It aims to capture sufficient semantic information from both labeled and unlabeled cross-modal pairs and intra-modal pairs with same classification label. Specifically, a coupled deep fully connected networks are used to map cross-modal feature representations into a common subspace. Weight-sharing strategy is utilized between two branches of networks to diminish cross-modal heterogeneity. Furthermore, two Siamese CNN models are employed to learn intra-modal similarity from samples of same modality. Comprehensive experiments on real datasets clearly demonstrate that our proposed technique achieves substantial improvements over the state-of-the-art cross-modal retrieval techniques.
C1 [Zhang, Chengyuan] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
   [Song, Jiayu; Zhang, Shichao] Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Hunan, Peoples R China.
   [Zhu, Xiaofeng] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Sichuan, Peoples R China.
   [Zhu, Lei] Hunan Agr Univ, Coll Informat & Intelligence, Changsha 410128, Hunan, Peoples R China.
C3 Hunan University; Central South University; University of Electronic
   Science & Technology of China; Hunan Agricultural University
RP Zhang, SC (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Hunan, Peoples R China.; Zhu, L (corresponding author), Hunan Agr Univ, Coll Informat & Intelligence, Changsha 410128, Hunan, Peoples R China.
EM cyzhangcse@hnu.edu.cn; jiayusong@csu.edu.cn; xfzhu0011@hotmail.com;
   leizhu@hunau.edu.cn; zhangsc@csu.edu.cn
RI Zhang, Shichao/JXW-9650-2024; Zhu, Xiaofeng/HII-5291-2022
OI Zhu, Xiaofeng/0000-0001-6840-0578
FU National Natural Science Foundation of China [61702560, 62072166,
   61836016, 61672177]; Science and Technology Plan of Hunan Province
   [2018JJ3691, 2016JC2011]
FX This work was supported in part by the National Natural Science
   Foundation of China (61702560, 62072166, 61836016, 61672177), project
   (2018JJ3691, 2016JC2011) of Science and Technology Plan of Hunan
   Province.
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   [Anonymous], 2019, P 2019 INT C MULT RE, DOI DOI 10.1145/3323873.3325019
   [Anonymous], 2011, P ICML
   [Anonymous], 2016, P 25 INT JOINT C ART
   [Anonymous], 2011, Advances in Neural Information Processing Systems
   [Anonymous], 2015, INT CONF MACH LEARN
   [Anonymous], 2009, CIVR
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Cao D, 2020, KNOWL-BASED SYST, V193, DOI 10.1016/j.knosys.2019.105428
   Cao D, 2020, INFORM SCIENCES, V514, P302, DOI 10.1016/j.ins.2019.11.033
   Cao D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1685, DOI 10.1145/3343031.3351067
   Cheng J, 2014, COMPUT VIS IMAGE UND, V124, P12, DOI 10.1016/j.cviu.2014.04.001
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Deng GW, 2018, PROC SPIE, V10828, DOI 10.1117/12.2502072
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Fang Leyuan, 2019, IEEE GEOSCI REM SENS, V16, P9
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Huang X, 2020, IEEE T CYBERNETICS, V50, P1047, DOI 10.1109/TCYB.2018.2879846
   Huang Xin, 2019, INF PROC MANAG, V56, P6
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Liao RJ, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P493, DOI 10.1145/2556195.2556238
   Lin ZJ, 2017, IEEE T CYBERNETICS, V47, P4342, DOI 10.1109/TCYB.2016.2608906
   Liong VE, 2017, IEEE I CONF COMP VIS, P4097, DOI 10.1109/ICCV.2017.439
   Liu Y, 2020, NONLINEAR DYNAM, V100, P2917, DOI 10.1007/s11071-020-05654-y
   Liu YL, 2016, RADIOENGINEERING, V25, P556, DOI 10.13164/re.2016.0556
   Liu YL, 2013, RADIOENGINEERING, V22, P1072
   Ouyang JL, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978572
   Ouyang JL, 2017, MULTIMED TOOLS APPL, V76, P2609, DOI 10.1007/s11042-015-3225-x
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Peng Yuxin, 2017, ABS171005106 CORR
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rashtchian Cyrus, 2010, P NAACL HLT WORKSH C
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang C, 2015, PROC INT C TOOLS ART, P234, DOI 10.1109/ICTAI.2015.45
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang JL, 2017, MULTIMED TOOLS APPL, V76, P20197, DOI 10.1007/s11042-017-4567-3
   Wang K, 2016, INFORM SCIENCES, V330, P199, DOI 10.1016/j.ins.2015.10.028
   Wang Y., 2020, ACM T MULTIMEDIA COM
   Wang YF, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P307, DOI 10.1145/2647868.2654901
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wang Y, 2017, IEEE T IMAGE PROCESS, V26, P1393, DOI 10.1109/TIP.2017.2655449
   Wang Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P79, DOI 10.1145/2733373.2806233
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wen X, 2019, IEEE INT CON MULTI, P478, DOI 10.1109/ICME.2019.00089
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Wu L, 2021, IEEE T NEUR NET LEAR, V32, P722, DOI 10.1109/TNNLS.2020.2979190
   Wu L, 2020, IEEE T IMAGE PROCESS, V29, P1233, DOI 10.1109/TIP.2019.2940684
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu L, 2019, IEEE T IMAGE PROCESS, V28, P1602, DOI 10.1109/TIP.2018.2878970
   Xu C, 2020, MULTIMED TOOLS APPL, V79, P5573, DOI 10.1007/s11042-019-08273-x
   Yakhnenko O., 2009, Proceedings of the 2009 SIAM International Conference on Data Mining, SDM '09, P283
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang Han Ling, 2008, P C IM SIGN PROC
   Zhang HanLing, 2009, P 2 INT S EL COMM SE
   Zhang J, 2020, IEEE T CYBERNETICS, V50, P489, DOI 10.1109/TCYB.2018.2868826
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
NR 68
TC 28
Z9 28
U1 13
U2 111
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 2
DI 10.1145/3412847
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900002
DA 2024-07-18
ER

PT J
AU Hu, HZ
   Zhou, WG
   Li, XZ
   Yan, N
   Li, HQ
AF Hu, Hezhen
   Zhou, Wengang
   Li, Xingze
   Yan, Ning
   Li, Houqiang
TI MV2Flow: Learning Motion Representation for Fast Compressed Video Action
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE MV2Flow; motion representation; compressed domain; action recognition
ID OPTICAL-FLOW
AB In video action recognition, motion is a very crucial clue, which is usually represented by optical flow. However, optical flow is computationally expensive to obtain, which becomes the bottleneck for the efficiency of traditional action recognition algorithms. In this article, we propose a network called MV2Flow to learn motion representation efficiently from the signals in the compressed domain. To learn the network, three losses are defined. First, we select the classical TV-L1 flow as proxy ground truth to guide the learning. Besides, an unsupervised image reconstruction loss is proposed to further refine it. Moreover, toward the task of action recognition, the above two losses are combined with a motion content loss. To evaluate our approach, extensive experiments on two benchmark datasets UCF-101 and HMDB-51 are conducted. The motion representation generated with our MV2Flow has shown comparable classification performance on action recognition with TV-L1 flow, while operating at an over 200x faster speed. Based on our MV2Flow and 2D-CNN-based network, we have achieved state-of-the-art performance in the compressed domain. With 3D-CNN-based network, we also achieve comparable accuracy with higher inference speed than methods in the decoded domain setting.
C1 [Hu, Hezhen; Li, Xingze; Yan, Ning] Univ Sci & Technol China, Hefei 230027, Peoples R China.
   [Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Inst Artificial Intelligence, Hefei Comprehens Natl Sci Ctr, Hefei, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Hu, HZ (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.
EM alexhu@mail.ustc.edu.cn; zhwg@ustc.edu.cn; lixingze@mail.ustc.edu.cn;
   nyan@mail.ustc.edu.cn; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
FU NSFC [61836011, 61632019, 61822208]; Youth Innovation Promotion
   Association CAS [2018497]
FX This work was supported in part to Dr. Wengang Zhou by NSFC under
   Contracts No. 61632019 and No. 61822208 and Youth Innovation Promotion
   Association CAS (Grant No. 2018497), and in part to Dr. Houqiang Li by
   NSFC under Contract No. 61836011.
CR Ahmadi A, 2016, IEEE IMAGE PROC, P1629, DOI 10.1109/ICIP.2016.7532634
   [Anonymous], 2012, CRCVTR1201
   [Anonymous], 2017, CONVNET ARCHITECTURE
   Black M.J, 2018, GCPR
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen YP, 2018, LECT NOTES COMPUT SC, V11205, P364, DOI 10.1007/978-3-030-01246-5_22
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan LJ, 2018, PROC CVPR IEEE, P6016, DOI 10.1109/CVPR.2018.00630
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gadot D, 2016, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2016.459
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Gao RH, 2018, PROC CVPR IEEE, P5937, DOI 10.1109/CVPR.2018.00622
   Güney F, 2017, LECT NOTES COMPUT SC, V10114, P207, DOI 10.1007/978-3-319-54190-7_13
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Huang WB, 2019, IEEE T IMAGE PROCESS, V28, P1773, DOI 10.1109/TIP.2018.2877936
   Jaderberg M, 2015, ADV NEUR IN, V28
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kantorov V, 2014, PROC CVPR IEEE, P2593, DOI 10.1109/CVPR.2014.332
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lai W.-S., 2017, PROC NEURAL INF PROC, P354
   LEGALL D, 1991, COMMUN ACM, V34, P46, DOI 10.1145/103085.103090
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Liu Kun, 2018, P AAAI C ART INT
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Meister S, 2018, AAAI CONF ARTIF INTE, P7251
   Memin E, 1998, IEEE T IMAGE PROCESS, V7, P703, DOI 10.1109/83.668027
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Piergiovanni AJ, 2019, PROC CVPR IEEE, P9937, DOI 10.1109/CVPR.2019.01018
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Ren Z, 2017, AAAI CONF ARTIF INTE, P1495
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shou Zheng, 2019, DMC NET GENERATING D
   Simonyan K, 2014, ADV NEUR IN, V27
   Song XL, 2020, IEEE T CIRC SYST VID, V30, P748, DOI 10.1109/TCSVT.2019.2896029
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Tu ZG, 2019, IEEE T CIRC SYST VID, V29, P1423, DOI 10.1109/TCSVT.2018.2830102
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P ECCV
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Wu CY, 2018, PROC CVPR IEEE, P6026, DOI 10.1109/CVPR.2018.00631
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Yeo CH, 2008, IEEE T CIRC SYST VID, V18, P1006, DOI 10.1109/TCSVT.2008.927112
   Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297
   Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219
NR 58
TC 12
Z9 12
U1 1
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 3
SU S
AR 102
DI 10.1145/3422360
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2RB
UT WOS:000612586300010
DA 2024-07-18
ER

PT J
AU Huang, Y
   Yang, XS
   Gao, JY
   Sang, JT
   Xu, CS
AF Huang, Yi
   Yang, Xiaoshan
   Gao, Junyu
   Sang, Jitao
   Xu, Changsheng
TI Knowledge-driven Egocentric Multimodal Activity Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Egocentric videos; wearable sensors; graph neural networks
ID 1ST-PERSON VISION; VIDEOS
AB Recognizing activities from egocentric multimodal data collected by wearable cameras and sensors, is gaining interest, as multimodal methods always benefit from the complementarity of different modalities. However, since high-dimensional videos contain rich high-level semantic information while low-dimensional sensor signals describe simple motion patterns of the wearer, the large modality gap between the videos and the sensor signals raises a challenge for fusing the raw data. Moreover, the lack of large-scale egocentric multimodal datasets due to the cost of data collection and annotation processes makes another challenge for employing complex deep learning models. To jointly deal with the above two challenges, we propose a knowledge-driven multimodal activity recognition framework that exploits external knowledge to fuse multimodal data and reduce the dependence on large-scale training samples. Specifically, we design a dual-GCLSTM (Graph Convolutional LSTM) and a multi-layer GCN (Graph Convolutional Network) to collectively model the relations among activities and intermediate objects. The dual-GCISTM is designed to fuse temporal multimodal features with top-down relation-aware guidance. In addition, we apply a co-attention mechanism to adaptively attend to the features of different modalities at different timesteps. The multi-layer GCN aims to learn relation-aware classifiers of activity categories. Experimental results on three publicly available egocentric multimodal datasets show the effectiveness of the proposed model.
C1 [Huang, Yi; Yang, Xiaoshan; Gao, Junyu; Xu, Changsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Huang, Yi; Yang, Xiaoshan; Gao, Junyu; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Huang, Yi; Yang, Xiaoshan; Gao, Junyu; Sang, Jitao; Xu, Changsheng] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Sang, Jitao] Beijing Jiaotong Univ, Sch Comp & Informat Technol, 506 Room 9 Teaching Bldg Beijing Jiaotong Univ, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory; Beijing Jiaotong University
RP Huang, Y (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, 95 Zhongguancun East Rd, Beijing, Peoples R China.; Huang, Y (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, 95 Zhongguancun East Rd, Beijing, Peoples R China.; Huang, Y (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
EM yi.huang@nlpria.ac.cn; xiaoshan.yang@nlpria.ac.cn;
   gaojunyu2015@ia.ac.cn; jtsang@bjtu.edu.cn; csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023; Gao, Junyu/HDO-5516-2022
FU National Key Research and Development Program of China [2018AAA0100604];
   National Natural Science Foundation of China [61720106006, 62072455,
   61702511, 61751211, 61620106003, 61532009, U1836220, U1705262,
   61872424]; Key Research Program of Frontier Sciences of CAS
   [QYZDJSSWJSC039]; Research Program of National Laboratory of Pattern
   Recognition [Z-2018007]
FX This work was supported by National Key Research and Development Program
   of China (No. 2018AAA0100604), National Natural Science Foundation of
   China (No. 61720106006, 62072455, 61702511, 61751211, 61620106003,
   61532009, U1836220, U1705262, 61872424) and Key Research Program of
   Frontier Sciences of CAS (QYZDJSSWJSC039). This work was also supported
   by Research Program of National Laboratory of Pattern Recognition (No.
   Z-2018007).
CR [Anonymous], 2013, People's Web Meets NLP
   Baradel F, 2018, LECT NOTES COMPUT SC, V11217, P106, DOI 10.1007/978-3-030-01261-8_7
   Bernal EA, 2018, IEEE T MULTIMEDIA, V20, P107, DOI 10.1109/TMM.2017.2726187
   Betancourt A, 2015, IEEE T CIRC SYST VID, V25, P744, DOI 10.1109/TCSVT.2015.2409731
   Bulling A, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2499621
   Cai MJ, 2016, ROBOTICS: SCIENCE AND SYSTEMS XII
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen C, 2017, MULTIMED TOOLS APPL, V76, P4405, DOI 10.1007/s11042-015-3177-1
   Chen YT, 2019, IEEE T MULTIMEDIA, V21, P704, DOI 10.1109/TMM.2018.2865860
   Cornacchia M, 2017, IEEE SENS J, V17, P386, DOI 10.1109/JSEN.2016.2628346
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fang Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1661
   Fathi A, 2012, LECT NOTES COMPUT SC, V7572, P314, DOI 10.1007/978-3-642-33718-5_23
   Gao Jingyue, 2019, AAAI
   Gao JY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P690, DOI 10.1145/3240508.3240566
   Guan WL, 2021, IEEE T CYBERNETICS, V51, P4501, DOI 10.1109/TCYB.2019.2951207
   Ha S, 2016, IEEE IJCNN, P381, DOI 10.1109/IJCNN.2016.7727224
   Ha S, 2015, IEEE SYS MAN CYBERN, P3017, DOI 10.1109/SMC.2015.525
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hsieh P.-J., 2016, Multimedia and Expo (ICME), 2016 IEEE International Conference on, P1
   Hussein F, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3063532
   Jelodar AB, 2019, IEEE T MULTIMEDIA, V21, P1813, DOI 10.1109/TMM.2018.2885228
   Jin WK, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321505
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kingma Diederik P., 2013, P 3 INT C LEARN REPR
   Kipf TN, 2017, INT C LEARN REPR
   Kumano S, 2017, IEEE T MULTIMEDIA, V19, P107, DOI 10.1109/TMM.2016.2608002
   Li Yaguang, 2018, INT C LEARN REPR
   Liu M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P970, DOI 10.1145/3123266.3123341
   Liu SP, 2012, MED SCI SPORT EXER, V44, P2138, DOI 10.1249/MSS.0b013e31825e825a
   Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289
   Ma MH, 2016, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2016.209
   Marino K, 2017, PROC CVPR IEEE, P20, DOI 10.1109/CVPR.2017.10
   Mettes P, 2017, IEEE I CONF COMP VIS, P4453, DOI 10.1109/ICCV.2017.476
   Mohamed Abduallah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14412, DOI 10.1109/CVPR42600.2020.01443
   Morerio P, 2013, 2013 16TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1502
   Murad A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17112556
   Nakamura K, 2017, PROC CVPR IEEE, P6817, DOI 10.1109/CVPR.2017.721
   Nie L., 2016, Synthesis Lectures on Information Concepts Retrieval & Services, V8, P1, DOI 10.2200/S00714ED1V01Y201603ICR048
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Oppenheim A. V., 1999, DISCRETE TIME SIGNAL, DOI DOI 10.1049/EP.1977.0078
   Pirsiavash H, 2012, PROC CVPR IEEE, P2847, DOI 10.1109/CVPR.2012.6248010
   Possas R, 2018, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2018.00625
   Qian SS, 2018, IEEE T MULTIMEDIA, V20, P2733, DOI 10.1109/TMM.2018.2815785
   Sadeghi F, 2015, PROC CVPR IEEE, P1456, DOI 10.1109/CVPR.2015.7298752
   Salakhutdinov R, 2011, PROC CVPR IEEE, P1481, DOI 10.1109/CVPR.2011.5995720
   Seo Y, 2018, LECT NOTES COMPUT SC, V11301, P362, DOI 10.1007/978-3-030-04167-0_33
   Shen ZJ, 2019, IEEE IMAGE PROC, P3317, DOI [10.1109/icip.2019.8803460, 10.1109/ICIP.2019.8803460]
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Soltanian M, 2019, IEEE T MULTIMEDIA, V21, P157, DOI 10.1109/TMM.2018.2844101
   Song H, 2018, IEEE T MULTIMEDIA, V20, P1088, DOI 10.1109/TMM.2017.2763322
   Song SB, 2016, IEEE COMPUT SOC CONF, P378, DOI 10.1109/CVPRW.2016.54
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Nguyen THC, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010072
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang JD, 2019, PATTERN RECOGN LETT, V119, P3, DOI 10.1016/j.patrec.2018.02.010
   Wang L, 2017, IEEE T MULTIMEDIA, V19, P646, DOI 10.1109/TMM.2016.2617079
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Wei YW, 2020, IEEE T IMAGE PROCESS, V29, P1, DOI 10.1109/TIP.2019.2923608
   Yao SC, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P351, DOI 10.1145/3038912.3052577
   Ye J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3038917
   Yuan XL, 2016, IEEE T MULTIMEDIA, V18, P2002, DOI 10.1109/TMM.2016.2602758
   Zhang TH, 2018, IEEE INT CONF ROBOT, P5628
   Zhu Y, 2018, PROC CVPR IEEE, P9436, DOI 10.1109/CVPR.2018.00983
NR 70
TC 12
Z9 13
U1 6
U2 34
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 133
DI 10.1145/3409332
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800017
DA 2024-07-18
ER

PT J
AU Zhu, YC
   Zhai, GT
   Min, XK
   Zhou, JT
AF Zhu, Yucheng
   Zhai, Guangtao
   Min, Xiongkuo
   Zhou, Jiantao
TI Learning a Deep Agent to Predict Head Movement in 360-Degree Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE VR; omnidirectional; 360 degree; panoramic; saliency; head movement
   prediction; deep reinforcement learning (DRL)
ID SALIENCY PREDICTION; COMPONENTS; SCANPATHS; MAPS; EYE
AB Virtual reality adequately stimulates senses to trick users into accepting the virtual environment. To create a sense of immersion, high-resolution images are required to satisfy human visual system, and low latency is essential for smooth operations, which put great demands on data processing and transmission. Actually, when exploring in the virtual environment, viewers only perceive the content in the current field of view. Therefore, if we can predict the head movements that are important behaviors of viewers, more processing resources can be allocated to the active field of view. In this article, we propose a model to predict the trajectory of head movement. Deep reinforcement learning is employed to mimic the decision making. In our framework, to characterize each state, features for viewport images are extracted by convolutional neural networks. In addition, the spherical coordinate maps and visited maps are generated for each viewport image, which facilitate the multiple dimensions of the state information by considering the impact of historical head movement and position information. To ensure the accurate simulation of visual behaviors during the watching of panoramas, we stipulate that the model imitates the behaviors of human demonstrators. To allow the model to generalize to more conditions, the intrinsic motivation is employed to guide the agent's action toward reducing uncertainty, which can enhance robustness during the exploration. The experimental results demonstrate the effectiveness of the proposed stepwise head movement predictor.
C1 [Zhu, Yucheng; Zhai, Guangtao; Min, Xiongkuo] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai, Peoples R China.
   [Zhou, Jiantao] Univ Macau, State Key Lab Internet Things Smart City, Macau, Peoples R China.
   [Zhou, Jiantao] Univ Macau, Dept Comp & Informat Sci, Macau, Peoples R China.
C3 Shanghai Jiao Tong University; University of Macau; University of Macau
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai, Peoples R China.
EM zyc420@sjtu.edu.cn; zhaiguangtao@sjtu.edu.cn; minxiongkuo@sjtu.edu.cn;
   jtzhou@umac.mo
RI Zhai, Guangtao/X-5949-2019; Min, Xiongkuo/A-7097-2019
OI Zhai, Guangtao/0000-0001-8165-9322; Min, Xiongkuo/0000-0001-5693-0416
FU National Natural Science Foundation of China [61901260, 61971476,
   61831015, 61521062, 61527804]; Macau Science and Technology Development
   Fund [FDCT/022/2017/A1, FDCT/077/2018/A2]; Research Committee at the
   University of Macau [MYRG2018-00029-FST]
FX This work was supported in part by the National Natural Science
   Foundation of China under grants 61901260, 61971476, 61831015, 61521062,
   and 61527804; in part by the Macau Science and Technology Development
   Fund under grants FDCT/022/2017/A1 and FDCT/077/2018/A2; and in part by
   the Research Committee at the University of Macau under grant
   MYRG2018-00029-FST.
CR Attias H, 2000, ADV NEUR IN, V12, P209
   BAJCSY R, 1988, P IEEE, V76, P996, DOI 10.1109/5.5968
   Battisti F, 2018, SIGNAL PROCESS-IMAGE, V69, P53, DOI 10.1016/j.image.2018.03.008
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cohen T.S., 2018, P 6 INT C LEARN REPR, P1
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   De Abreu A., 2017, PROC IEEE 9 INT C QU, P1, DOI 10.1109/QoMEX.2017.7965634
   Duan HY, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351786
   Fremerey S, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P403, DOI 10.1145/3204949.3208134
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Gandhi NJ, 2008, EXP BRAIN RES, V189, P35, DOI 10.1007/s00221-008-1401-1
   Goodchild MF, 2007, GEOJOURNAL, V69, P211, DOI 10.1007/s10708-007-9111-y
   Gu K, 2015, IEEE SIGNAL PROC LET, V22, P1552, DOI 10.1109/LSP.2015.2413944
   Guastello StephenJ., 2013, Human Factors Engineering and Ergonomics: A Systems Approach
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153
   Itti L, 2009, Advances in neural information processing systems, V49, P1295, DOI [10.1016/j.visres.2008.09.007, DOI 10.1016/J.VISRES.2008.09.007]
   Kantz H., 2004, NONLINEAR TIME SERIE, DOI DOI 10.1017/CB09780511755798
   Kapoor J, 2017, 2017 INTERNATIONAL CONFERENCE ON EMERGING TRENDS IN COMPUTING AND COMMUNICATION TECHNOLOGIES (ICETCCT), P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai WS, 2018, IEEE T VIS COMPUT GR, V24, P2610, DOI 10.1109/TVCG.2017.2750671
   Le Meur O, 2013, BEHAV RES METHODS, V45, P251, DOI 10.3758/s13428-012-0226-9
   Lebreton P, 2018, SIGNAL PROCESS-IMAGE, V69, P69, DOI 10.1016/j.image.2018.03.006
   Li BJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02116
   Ling J, 2018, SIGNAL PROCESS-IMAGE, V69, P60, DOI 10.1016/j.image.2018.03.007
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Nasrabadi AT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1689, DOI 10.1145/3123266.3123414
   Oudeyer Pierre-Yves, 2007, Front Neurorobot, V1, P6, DOI 10.3389/neuro.12.006.2007
   Ozcinar C, 2018, INT WORK QUAL MULTIM, P1
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218
   RIZZOLATTI G, 1987, NEUROPSYCHOLOGIA, V25, P31, DOI 10.1016/0028-3932(87)90041-8
   Schmidt T, 2006, PERCEPT PSYCHOPHYS, V68, P489, DOI 10.3758/BF03193692
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Startsev M, 2018, SIGNAL PROCESS-IMAGE, V69, P43, DOI 10.1016/j.image.2018.03.013
   Su Y.-C., 2017, ADV NEURAL INFORM PR, P529
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   TOBII VR, 2019, DISC NEW POSS EYE TR
   Upenik Evgeniy, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P73, DOI 10.1109/ICMEW.2017.8026231
   van Waveren JMP, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P37, DOI 10.1145/2993369.2993375
   Wang W, 2011, PROC CVPR IEEE, P441, DOI 10.1109/CVPR.2011.5995423
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1705, DOI 10.1109/TMM.2013.2268053
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 50
TC 16
Z9 16
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 130
DI 10.1145/3410455
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA QB4CR
UT WOS:000614088800014
DA 2024-07-18
ER

PT J
AU Duan, MX
   Li, KL
   Ouyang, AJ
   Win, KN
   Li, KQ
   Tian, Q
AF Duan, Mingxing
   Li, Kenli
   Ouyang, Aijia
   Win, Khin Nandar
   Li, Keqin
   Tian, Qi
TI EGroupNet: A Feature-enhanced Network for Age Estimation with Novel Age
   Group Schemes
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Age estimation; age groups; correlations; enhancement; multi-task
   learning
ID NEURAL-NETWORK; FACE
AB Although age estimation is easily affected by smiling, race, gender, and other age-related attributes, most of the researchers did not pay attention to the correlations among these attributes. Moreover, many researchers perform age estimation from a wide range of age; however, conducting an age prediction over a narrow age range may achieve better results. This article proposes a hierarchic approach referred to as EGroupNet for age prediction. The method includes two main stages, i.e., feature enhancement via excavating the correlations among age-related attributes and age estimation based on different age group schemes. First, we apply the multi-task learning model to learn multiple face attributes simultaneously to obtain discriminative features of different attributes. Second, we project the outputs of fully connected layers of several subnetworks into a highly correlated matrix space via the correlation learning process. Third, we classify these enhanced features into narrow age groups using two Extreme Learning Machine models. Finally, we make predictions based on the results of the age groups mergence. We conduct a large number of experiments on MORPH-II, LAP-2016 dataset, and Adience benchmark. The mean absolute errors of the two different settings on MORPH-II are 2.48 and 2.13 years, respectively; the normal score (e) on the LAP-2016 dataset is 0.3578; and the accuracy of age prediction on Adience benchmark is 0.6978.
C1 [Duan, Mingxing; Li, Kenli] Hunan Univ, Coll Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
   [Ouyang, Aijia] Zunyi Normal Univ, Coll Informat Engn, Zunyi 563006, Guizhou, Peoples R China.
   [Win, Khin Nandar] Hunan Univ, Sch Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
   [Li, Keqin] SUNY Coll New Paltz, Dept Comp Sci, New Paltz, NY 12561 USA.
   [Tian, Qi] Univ Texas San Antonio, Comp Sci, San Antonio, TX USA.
C3 Hunan University; Hunan University; State University of New York (SUNY)
   System; SUNY New Paltz; University of Texas System; University of Texas
   at San Antonio (UTSA)
RP Duan, MX (corresponding author), Hunan Univ, Coll Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
EM duanmingxing@hnu.edu.cn; lkl@hnu.edu.cn; ouyangaijia@163.com;
   knandarwin@hnu.edu.cn; lik@newpaltz.edu; qitian@cs.utsa.edu
FU National Outstanding Youth Science Program of National Natural Science
   Foundation of China [61625202]; International (Regional) Cooperation and
   Exchange Program of National Natural Science Foundation of China
   [61661146006]; National Key R&D Program of China [2016YT80201900];
   National Youth Science Program of National Natural Science Foundation of
   China [61902119]; China Postdoctoral Science Foundation [2019M652758,
   2019TQ0087]
FX This work was supported in part by the National Outstanding Youth
   Science Program of National Natural Science Foundation of China under
   Grant No. 61625202, in part by the International (Regional) Cooperation
   and Exchange Program of National Natural Science Foundation of China
   under Grant No. 61661146006, in part by the National Key R&D Program of
   China under Grant No. 2016YT80201900, in part by the National Youth
   Science Program of National Natural Science Foundation of China under
   Grant 61902119, and in part by the Project funded by China Postdoctoral
   Science Foundation under Grants 2019M652758 and 2019TQ0087.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2017, IEEE T CYBERNETICS
   [Anonymous], 1999, P C COMP VIS PATT RE
   [Anonymous], 2008, P 2008 23 INT S COMP
   [Anonymous], 2014, EUR C COMP VIS
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.532
   [Anonymous], 2017, P IEEE INT C COMP VI, DOI DOI 10.1109/ICCV.2017.182
   [Anonymous], 2001, J Appl Sci Eng, DOI DOI 10.6180/JASE.2001.4.3.05
   Chang KY, 2015, IEEE T IMAGE PROCESS, V24, P785, DOI 10.1109/TIP.2014.2387379
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   Chen K, 2013, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2013.319
   Chen SX, 2017, PROC CVPR IEEE, P742, DOI 10.1109/CVPR.2017.86
   Dibeklioglu H, 2015, IEEE T IMAGE PROCESS, V24, P1928, DOI 10.1109/TIP.2015.2412377
   Duan MX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355542
   Duan MX, 2018, NEUROCOMPUTING, V275, P448, DOI 10.1016/j.neucom.2017.08.062
   Duan MX, 2018, IEEE T INF FOREN SEC, V13, P758, DOI 10.1109/TIFS.2017.2766583
   Duan Mingxing, 2017, IEEE T NEUR NET LEAR, V29, P2337
   Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646
   Escalera S, 2016, IEEE COMPUT SOC CONF, P706, DOI 10.1109/CVPRW.2016.93
   Fu SY, 2014, IEEE T PATTERN ANAL, V36, P2483, DOI 10.1109/TPAMI.2014.2321570
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Gao B.B., 2016, IEEE T IMAGE PROCESS, P1
   GENG X, 2016, IEEE TKDE, V28, P1734, DOI DOI 10.1109/TKDE.2016.2545658
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Geng X, 2010, AAAI CONF ARTIF INTE, P451
   Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   Guo J, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL V, P71
   Gürpinar F, 2016, IEEE COMPUT SOC CONF, P785, DOI 10.1109/CVPRW.2016.103
   Hajizadeh M.A., 2011, 2011 7th IEEE Iranian Machine Vision and Image Processing (MVIP), P1
   Han H, 2015, IEEE T PATTERN ANAL, V37, P1148, DOI 10.1109/TPAMI.2014.2362759
   Hu ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3087, DOI 10.1109/TIP.2016.2633868
   Huang D, 2017, PROC CVPR IEEE, P4058, DOI 10.1109/CVPR.2017.432
   Iqbal M. T. B., 2017, IEEE TIFS
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuang-Yu Chang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3396, DOI 10.1109/ICPR.2010.829
   KWON YH, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P762, DOI 10.1109/CVPR.1994.323894
   Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Li CL, 2015, J ELECTR COMPUT ENG, V2015, DOI 10.1155/2015/370615
   Li WH, 2019, PROC CVPR IEEE, P1145, DOI 10.1109/CVPR.2019.00124
   Liu FY, 2015, PATTERN RECOGN, V48, P2983, DOI 10.1016/j.patcog.2015.04.019
   Liu KH, 2015, IEEE T INF FOREN SEC, V10, P2408, DOI 10.1109/TIFS.2015.2462732
   Liu X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P258, DOI 10.1109/ICCVW.2015.42
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lou ZY, 2018, IEEE T PATTERN ANAL, V40, P365, DOI 10.1109/TPAMI.2017.2679739
   Malli RC, 2016, IEEE COMPUT SOC CONF, P714, DOI 10.1109/CVPRW.2016.94
   Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47
   Niu XX, 2012, PATTERN RECOGN, V45, P1318, DOI 10.1016/j.patcog.2011.09.021
   Ramanathan N., 2006, 2006 IEEE COMP SOC C, V1, P387
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3
   Rothe R, 2016, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2016.599
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Shen W, 2017, CIVIL, ARCHITECTURE AND ENVIRONMENTAL ENGINEERING, VOLS 1 AND 2, P1083
   Shen W, 2018, PROC CVPR IEEE, P2304, DOI 10.1109/CVPR.2018.00245
   Tan ZC, 2018, IEEE T PATTERN ANAL, V40, P2610, DOI 10.1109/TPAMI.2017.2779808
   Tan Zichang, 2019, P INT JOINT C ART IN, P3548
   Tang JX, 2015, IEEE T GEOSCI REMOTE, V53, P1174, DOI 10.1109/TGRS.2014.2335751
   Thukral P, 2012, INT CONF ACOUST SPEE, P1529, DOI 10.1109/ICASSP.2012.6288182
   Uriclcr M., 2016, P COMP VIS PATT REC
   Wan J, 2018, IEEE T CYBERNETICS, V48, P2531, DOI 10.1109/TCYB.2017.2741998
   Wang JW, 2013, 2013 IEEE 10TH INTERNATIONAL CONFERENCE ON AND 10TH INTERNATIONAL CONFERENCE ON AUTONOMIC AND TRUSTED COMPUTING (UIC/ATC) UBIQUITOUS INTELLIGENCE AND COMPUTING, P17, DOI 10.1109/UIC-ATC.2013.19
   Wang XL, 2015, IEEE WINT CONF APPL, P534, DOI 10.1109/WACV.2015.77
   Xie G. S., 2015, IEEE TCSVT, V27, P1
   Zhang C, 2019, PROC CVPR IEEE, P12579, DOI 10.1109/CVPR.2019.01287
   Zhang Y, 2010, PROC CVPR IEEE, P2622, DOI 10.1109/CVPR.2010.5539975
NR 69
TC 21
Z9 21
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 42
DI 10.1145/3379449
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600003
DA 2024-07-18
ER

PT J
AU Huang, XW
   Qian, SS
   Fang, Q
   Sang, JT
   Xu, CS
AF Huang, Xiaowen
   Qian, Shengsheng
   Fang, Quan
   Sang, Jitao
   Xu, Changsheng
TI Meta-path Augmented Sequential Recommendation with Contextual
   Co-attention Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE User modeling; sequential recommendation; self-attention; co-attention;
   meta-path; heterogeneous information network
ID PREDICTION
AB It is critical to comprehensively and efficiently learn user preferences for an effective sequential reconunender system. Existing sequential recommendation methods mainly focus on modeling local preference from users' historical behaviors, which largely ignore the global context information from the heterogeneous information network. This prevents a comprehensive user preference representation. 'lb address these issues, we propose a joint learning approach to incorporate global context with local preferences efficiently. The proposed approach introduces meta-paths from a heterogeneous information network to capture the global context information, and the position-based self-attention mechanism is adopted to model the local preference representation efficiently. Compared with the methods that only consider the local preference, our proposed method takes the advantages of incorporating global context information, which extracts structural features that captures relevant semantics to construct users' global preference representation for the sequential recommendation. We further adopt a co-attention mechanism to model complex interactions between global context and users' historical behaviors for better user representations. Quantitative and qualitative experimental evaluations are conducted on nine large-scale Amazon datasets and a multi-modal Zhihu dataset. The promising results demonstrate the effectiveness of the proposed model.
C1 [Huang, Xiaowen; Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, 95 Zhongguancun Rd, Beijing, Peoples R China.
   [Huang, Xiaowen; Qian, Shengsheng; Fang, Quan] Univ Chinese Acad Sci, Sch Artificial Intelligence, 80 Zhongguancun Rd, Beijing, Peoples R China.
   [Sang, Jitao] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing, Peoples R China.
   [Sang, Jitao] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Beijing Jiaotong University; Beijing Jiaotong University; Peng Cheng
   Laboratory
RP Huang, XW (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, 95 Zhongguancun Rd, Beijing, Peoples R China.; Huang, XW (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, 80 Zhongguancun Rd, Beijing, Peoples R China.
EM aowen.huang77@gmail.com; shengsheng.qian@nlpr.ia.ac.cn;
   qfang@nlpr.ia.ac.cn; jtsang@bjtu.edu.cn; csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023
FU National Key Research and Development Program of China [2017YFB1002804];
   National Natural Science Foundation of China [61720106006, 61802405,
   61872424, 61702509, 61832002]; Key Research Program of Frontier
   Sciences, CAS [QYZDJSSW-JSC039]; K. C. Wong Education Foundation
FX This work was supported in part by the National Key Research and
   Development Program of China (Grant No. 2017YFB1002804), the National
   Natural Science Foundation of China under Grants No. 61720106006, No.
   61802405, No. 61872424, No. 61702509, and No. 61832002, the Key Research
   Program of Frontier Sciences, CAS, Grant No. QYZDJSSW-JSC039, and the K.
   C. Wong Education Foundation.
CR [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   [Anonymous], 2017, INT C MACH LEARN
   [Anonymous], Attention-over-Attention Neural Networks for Reading Comprehension
   Chen BL, 2019, IEEE INT C BIOINFORM, P193, DOI [10.1109/BIBM47256.2019.8983256, 10.1109/bibm47256.2019.8983256]
   Cheng C., 2013, 23 INT JOINT C ART I
   Chenguang Shi, 2018, Multidimensional Systems and Signal Processing, V29, P1203, DOI 10.1007/s11045-017-0494-8
   Chou SY, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P115, DOI 10.1145/2959100.2959156
   Dong YX, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P135, DOI 10.1145/3097983.3098036
   Dong YX, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/2783258.2783329
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Feng SS, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2069
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Hao YC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P221, DOI 10.18653/v1/P17-1021
   He RN, 2016, IEEE DATA MINING, P191, DOI [10.1109/ICDM.2016.88, 10.1109/ICDM.2016.0030]
   Hu BB, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1531, DOI 10.1145/3219819.3219965
   Hu L, 2016, ACM T INFORM SYST, V35, DOI 10.1145/2976737
   Hu M., 2017, CoRR
   Huang SR, 2016, IEEE T MULTIMEDIA, V18, P287, DOI 10.1109/TMM.2015.2510333
   Huang XW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P548, DOI 10.1145/3343031.3350893
   Huang XW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P447, DOI 10.1145/3240508.3240609
   Kang WC, 2018, IEEE DATA MINING, P197, DOI 10.1109/ICDM.2018.00035
   Le Quoc V., 2014, P INT C MACH LEARN I
   Lin Z., 2017, PROC INT C LEARN REP
   Liu DR, 2009, INFORM SCIENCES, V179, P3505, DOI 10.1016/j.ins.2009.06.004
   Liu J, 2017, LECT NOTES ARTIF INT, V10235, P131, DOI 10.1007/978-3-319-57529-2_11
   Liu Q, 2016, AAAI CONF ARTIF INTE, P194
   Liu Q, 2017, IEEE T KNOWL DATA EN, V29, P1254, DOI 10.1109/TKDE.2017.2661760
   Ma RF, 2018, ACM/SIGIR PROCEEDINGS 2018, P195, DOI 10.1145/3209978.3210026
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Mikolov T., 2010, 11 ANN C INT SPEECH, P1045
   Mikolov T, 2011, INT CONF ACOUST SPEE, P5528
   Mikolov Tomas., 2011, Proc. ASRU, P196
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Rendle S., 2010, P 19 INT C WORLD WID, P811, DOI DOI 10.1145/1772690.1772773
   Sang JT, 2018, IEEE T MULTIMEDIA, V20, P3439, DOI 10.1109/TMM.2018.2839530
   Shen T., 2017, Disan: Directional self-attention network for rnn/cnn-free language understanding
   Shi Chuan, 2018, IEEE T KNOWL DATA EN
   Shi Z., 2015, P 24 ACM INT C INFOR, P453, DOI DOI 10.1145/2806416.2806528
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Sun Y., 2013, SIGKDD Explorations, V14, P20, DOI DOI 10.1145/2481244.2481248
   Sun YB, 2012, ELECTRON J QUAL THEO, P1
   Sun YT, 2013, J NANOMATER, V2013, DOI 10.1155/2013/249369
   Sunt YZ, 2011, PROC VLDB ENDOW, V4, P992
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Tang JX, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P565, DOI 10.1145/3159652.3159656
   Tay Y, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2309, DOI 10.1145/3219819.3220086
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang PF, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P403, DOI 10.1145/2766462.2767694
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Xiong C., 2016, Dynamic coattention networks for question answering
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu ZX, 2017, IEEE T MULTIMEDIA, V19, P1933, DOI 10.1109/TMM.2017.2688928
   Yang YJ, 2018, IEEE T MULTIMEDIA, V20, P1888, DOI 10.1109/TMM.2017.2779043
   Yu X, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P283, DOI 10.1145/2556195.2556259
   Zhai SF, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1295, DOI 10.1145/2939672.2939759
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhang YF, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1449, DOI 10.1145/3132847.3132892
   Zhang YY, 2014, AAAI CONF ARTIF INTE, P1369
   Zhao H, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P635, DOI 10.1145/3097983.3098063
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zheng L, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P425, DOI 10.1145/3018661.3018665
   Zhou C, 2018, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON UBIQUITOUS INFORMATION MANAGEMENT AND COMMUNICATION (IMCOM 2018), DOI 10.1145/3164541.3164545
NR 63
TC 3
Z9 3
U1 3
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 52
DI 10.1145/3382180
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600013
DA 2024-07-18
ER

PT J
AU Wang, X
   Liu, W
   Chen, J
   Wang, XB
   Yan, CG
   Mei, T
AF Wang, Xiao
   Liu, Wu
   Chen, Jun
   Wang, Xiaobo
   Yan, Chenggang
   Mei, Tao
TI Listen, Look, and Find the One: Robust Person Search with Multimodality
   Index
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person search; multimodality index; associative network
AB Person search with one portrait, which attempts to search the targets in arbitrary scenes using one portrait image at a time, is an essential yet unexplored problem in the multimedia field. Existing approaches, which predominantly depend on the visual information of persons, cannot solve problems when there are variations in the person's appearance caused by complex environments and changes in pose, makeup, and clothing. In contrast to existing methods, in this article, we propose an associative multimodality index for person search with face, body, and voice information. In the offline stage, an associative network is proposed to learn the relationships among face, body, and voice information. It can adaptively estimate the weights of each embedding to construct an appropriate representation. The multimodality index can be built by using these representations, which exploit the face and voice as long-term keys and the body appearance as a short-term connection. In the online stage, through the multimodality association in the index, we can retrieve all targets depending only on the facial features of the query portrait. Furthermore, to evaluate our multimodality search framework and facilitate related research, we construct the Cast Search in Movies with Voice (CSMV) dataset, a large-scale benchmark that contains 127K annotated voices corresponding to tracklets from 192 movies. According to extensive experiments on the CSM-V dataset, the proposed multimodality person search framework outperforms the state-of-the-art methods.
C1 [Wang, Xiao; Chen, Jun] Wuhan Univ, NERCMS, Sch Comp Sicence, Wuhan, Peoples R China.
   [Liu, Wu; Wang, Xiaobo; Mei, Tao] AI Res JD Com, Beijing, Peoples R China.
   [Yan, Chenggang] Hangzhou Dianzi Univ, Hangzhou, Peoples R China.
C3 Wuhan University; Hangzhou Dianzi University
RP Chen, J (corresponding author), Wuhan Univ, NERCMS, Sch Comp Sicence, Wuhan, Peoples R China.; Liu, W (corresponding author), AI Res JD Com, Beijing, Peoples R China.
EM hebeiwangxiao@whu.edu.cn; liuwu1@jd.com; chenj.whu@gmail.com;
   wangxiaobo8@jd.com; cgyan@hdu.edu.cn; tmei@jd.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
FU National Key R&D Program of China [2017YFC0803700]; National Nature
   Science Foundation of China [61876135, 61801335, U1611461]; Hubei
   Province Technological Innovation Major Project [2018AAA062];
   Fundamental Research Funds for the Central Universities [2019XD-A12]
FX This work is supported by National Key R&D Program of China
   (No.2017YFC0803700), National Nature Science Foundation of China (No.
   61876135, 61801335, U1611461), Hubei Province Technological Innovation
   Major Project (2018AAA062), the Fundamental Research Funds for the
   Central Universities (No. 2019XD-A12).
CR [Anonymous], 2017, ARXIV170509422
   [Anonymous], 2017, IEEE T SYST MAN CY-S
   [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.61
   [Anonymous], 2017, IEEE DECIS CONTR P
   [Anonymous], 2016, SCI REP, DOI DOI 10.1016/J.IJPSYCHO.2016.07.146
   [Anonymous], 2018, T INFORM FORENSICS S
   Arandjelovic R., 2017, ARXIV170508168
   Best-Rowden L, 2018, IEEE T PATTERN ANAL, V40, P148, DOI 10.1109/TPAMI.2017.2652466
   Chen D, 2021, IEEE T KNOWL DATA EN, V33, P569, DOI 10.1109/TKDE.2019.2931687
   Chen D, 2017, IEEE T PARALL DISTR, V28, P1091, DOI 10.1109/TPDS.2016.2613054
   Choi JI, 2010, MOBICOM 10 & MOBIHOC 10: PROCEEDINGS OF THE 16TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING AND THE 11TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P1
   Chung J., VOXCELEB2 DEEP SPEAK, P1086
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   Gan C, 2019, IEEE I CONF COMP VIS, P7052, DOI 10.1109/ICCV.2019.00715
   Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17
   Guo Y, 2016, INT J AEROSPACE ENG, V2016, DOI 10.1155/2016/2942686
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Horiguchi S., 2018, P ACM MULT, P1
   Ke HJ, 2020, SOFTWARE PRACT EXPER, V50, P596, DOI 10.1002/spe.2668
   Li SQ, 2016, INT CONF CLOUD COMPU, P480, DOI 10.1109/CCIS.2016.7790306
   Liu J., 2019, P BIGCOMP, P1
   Liu K, 2018, AAAI CONF ARTIF INTE, P7138
   Liu W, 2018, NEUROINFORMATICS, V16, P457, DOI 10.1007/s12021-018-9362-4
   Liu W, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1618, DOI 10.1145/3123266.3123422
   Liu Wu, 2013, P ACM INT C MULT, P887
   Liu Y., 2018, ARXIV181107548
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   LOY CC, 2018, WIDER FACE PEDESTRIA
   Min DZ, 2018, PROCEEDINGS OF 2018 THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ARTIFICIAL INTELLIGENCE (CSAI 2018) / 2018 THE 10TH INTERNATIONAL CONFERENCE ON INFORMATION AND MULTIMEDIA TECHNOLOGY (ICIMT 2018), P425, DOI 10.1145/3297156.3297229
   Munjal B, 2019, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2019.00090
   Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Normalization Batch., 2015, CoRR.-
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sagonas Christos, 2013, P IEEE INT C COMP VI, P1
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Wang J, 2018, REFR SCI T, P765, DOI 10.18462/iir.gl.2018.1271
   Xie ZB, 2017, IEEE ACCESS, V5, P3454, DOI 10.1109/ACCESS.2017.2671338
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang SF, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P1, DOI 10.1109/BTAS.2017.8272675
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zhu X., 2017, COMPLEXITY, V2017, P1
NR 48
TC 5
Z9 5
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 47
DI 10.1145/3380549
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600008
DA 2024-07-18
ER

PT J
AU Ye, ZD
   Peng, YX
AF Ye, Zhaoda
   Peng, Yuxin
TI Sequential Cross-Modal Hashing Learning via Multi-scale Correlation
   Mining
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modal hashing; correlation mining; multi-scale; sequential hash
   learning
ID RETRIEVAL; CODES
AB Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space through hash function, and achieves fast and flexible cross-modal retrieval. Most existing cross-modal hashing methods learn hash function by mining the correlation among multimedia data, but ignore the important property of multimedia data: Each modality of multimedia data has features of different scales, such as texture, object, and scene features in the image, which can provide complementary information for boosting retrieval task. The correlations among the multi-scale features are more abundant than the correlations between single features of multimedia data, which reveal finer underlying structures of the multimedia data and can be used for effective hashing function learning. Therefore, we propose the Multi-scale Correlation Sequential Cross-modal Hashing (MCSCH) approach, and its main contributions can be summarized as follows: (1) Multi-scale feature guided sequential hashing learning method is proposed to share the information from features of different scales through an RNN-based network and generate the hash codes sequentially. The features of different scales are used to guide the hash codes generation, which can enhance the diversity of the hash codes and weaken the influence of errors in specific features, such as false object features caused by occlusion. (2) Multi-scale correlation mining strategy is proposed to align the features of different scales in different modalities and mine the correlations among aligned features. These correlations reveal the finer underlying structure of multimedia data and can help to boost the hash function learning. (3) Correlation evaluation network evaluates the importance of the correlations to select the worthwhile correlations, and increases the impact of these correlations for hash function learning. Experiments on two widely-used 2-media datasets and a 5-media dataset demonstrate the effectiveness of our proposed MCSCH approach.
C1 [Ye, Zhaoda; Peng, Yuxin] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Peng, YX (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
FU National Natural Science Foundation of China [61925201, 61771025]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61925201 and Grant 61771025.
CR [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   Baeza-Yates R., 1999, MODERN INFORM RETRIE, V463
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Cao Y, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P197, DOI 10.1145/2911996.2912000
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Hu MQ, 2018, IEEE T IMAGE PROCESS, V27, P545, DOI 10.1109/TIP.2017.2749147
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu DR, 2013, ALLERGY ASTHMA CL IM, V9, DOI 10.1186/1710-1492-9-30
   Liu XL, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540990
   Long MS, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P579, DOI 10.1145/2911451.2911493
   Luo X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2518
   McEnnis D., 2005, P INT C MUSIC INFORM, P600, DOI 10.5281/zenodo.1416648
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Peng YX, 2016, IEEE T CIRC SYST VID, V26, P583, DOI 10.1109/TCSVT.2015.2400779
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   SALTON G, 1986, COMMUN ACM, V29, P648, DOI 10.1145/6138.6149
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song Y, 2018, IEEE T CONTROL NETW, V5, P1261, DOI 10.1109/TCNS.2017.2698266
   Tian XM, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240139
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Wang Heng, 2013, P IEEE C COMP VIS PA
   Wang J., 2016, J NANOMATER, V2016, P1
   Wei Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P791, DOI 10.1145/2623330.2623688
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Yang Y, 2019, IEEE T KNOWL DATA EN, V31, P757, DOI 10.1109/TKDE.2018.2842190
   Ye Zhaoda, 2018, P ACM INT C MULT ACM
   Yuwono B., 1997, Database Systems for Advanced Applications '97. Proceedings of the Fifth International Conference, P41, DOI 10.1142/9789812819536_0005
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang Jian, 2018, P AAAI C ART INT AAA
   Zhang Jian, 2017, ARXIV171200358
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zheng F, 2018, IEEE T PATTERN ANAL, V40, P1059, DOI 10.1109/TPAMI.2016.2645565
   Zhuang YT, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P901, DOI 10.1145/2647868.2655059
NR 51
TC 15
Z9 16
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 105
DI 10.1145/3356338
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800005
DA 2024-07-18
ER

PT J
AU Atrey, PK
   Trehan, BL
   Saini, MK
AF Atrey, Pradeep K.
   Trehan, Baku L.
   Saini, Mukesh K.
TI Watch Me from Distance (WMD): A Privacy-Preserving Long-Distance Video
   Surveillance System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Privacy; surveillance; context decoupling; trust model
AB Preserving the privacy of people in video surveillance systems is quite challenging, and a significant amount of research has been done to solve this problem in recent times. Majority of existing techniques are based on detecting bodily cues such as face and/or silhouette and obscuring them so that people in the videos cannot be identified. We observe that merely hiding bodily cues is not enough for protecting identities of the individuals in the videos. An adversary, who has prior contextual knowledge about the surveilled area, can identify people in the video by exploiting the implicit inference channels such as behavior, place, and time. This article presents an anonymous surveillance system, called Watch Me from Distance (WMD), which advocates for outsourcing of surveillance video monitoring (similar to call centers) to the long-distance sites where professional security operators watch the video and alert the local site when any suspicious or abnormal event takes place. We find that long-distance monitoring helps in decoupling the contextual knowledge of security operators. Since security operators at the remote site could turn into adversaries, a trust computation model to determine the credibility of the operators is presented as an integral part of the proposed system. The feasibility study and experiments suggest that the proposed system provides more robust measures of privacy yet maintains surveillance effectiveness.
C1 [Atrey, Pradeep K.] SUNY Albany, Coll Engn & Appl Sci, Albany, NY 12222 USA.
   [Trehan, Baku L.] Univ Winnipeg, Dept Appl Comp Sci, Winnipeg, MB, Canada.
   [Saini, Mukesh K.] Indian Inst Technol, Dept Comp Sci & Engn, Ropar, India.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Albany; University of Winnipeg; Indian Institute of Technology
   System (IIT System); Indian Institute of Technology (IIT) - Ropar
RP Atrey, PK (corresponding author), SUNY Albany, Coll Engn & Appl Sci, Albany, NY 12222 USA.
EM patrey@albany.edu; bakultrehan@gmail.com; mukesh@iitrpr.ac.in
RI Saini, Mukesh Kumar/C-5923-2012
CR Alamri A, 2016, MULTIMED TOOLS APPL, V75, P13333, DOI 10.1007/s11042-015-3074-7
   [Anonymous], 1996, CSCW '96: Proceedings of the 1996 ACM conference on Computer supported cooperative work
   Atrey PK, 2011, MULTIMED TOOLS APPL, V51, P697, DOI 10.1007/s11042-010-0649-1
   Barhm Mukhtaj S., 2011, Modern Approaches in Applied Intelligence. Proceedings 24th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems (IEA/AIE 2011), P511, DOI 10.1007/978-3-642-21827-9_52
   Bhatt CA, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457452
   Boyle M., 2000, CSCW 2000. ACM 2000 Conference on Computer Supported Cooperative Work, P1, DOI 10.1145/358916.358935
   Chaudhari J., 2007, IEEEWorkshop on Signal Processing Applications for Public Security and Forensics, P1
   Chen DT, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/75427
   Cheung S.-C.S., 2009, PROTECTING PRIVACY V, P11
   DASIC P., 2016, B TRANSILVANIA U BRA, V9, P83
   Dee HM, 2008, MACH VISION APPL, V19, P329, DOI 10.1007/s00138-007-0077-z
   Dufaux F., 2011, SOC PHOTOOPT INSTRUM, V8063, P2
   Fidaleo D.-A., 2004, VSSN '04: Proceedings of the ACM 2nd international workshop on Video surveillance sensor networks, P46
   Hampapur I, 2005, IEEE SIGNAL PROC MAG, V22, P38
   Hossain MA, 2009, IEEE T INSTRUM MEAS, V58, P1525, DOI 10.1109/TIM.2009.2014507
   Khan IR, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON FRONTIERS OF SENSORS TECHNOLOGIES (ICFST), P435, DOI 10.1109/ICFST.2017.8210551
   Kitahara I, 2004, INT C PATT RECOG, P404, DOI 10.1109/ICPR.2004.1333788
   Oh S., 2011, P CVPR, P3153, DOI DOI 10.1109/CVPR.2011.5995586
   Oommen BJ, 2005, ARTIF INTELL, V164, P1, DOI 10.1016/j.artint.2002.02.001
   Saini M, 2011, P INT C MULT EXP, P1
   Saini M, 2012, IEEE T MULTIMEDIA, V14, P555, DOI 10.1109/TMM.2012.2186957
   Saini M, 2010, IEEE INT CON MULTI, P60, DOI 10.1109/ICME.2010.5583334
   Schiff J, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P977
   Sohn H, 2011, IEEE T CIRC SYST VID, V21, P170, DOI 10.1109/TCSVT.2011.2106250
   Song B, 2016, MULTIMED TOOLS APPL, V75, P13375, DOI 10.1007/s11042-015-2816-x
   Song BA, 2014, 2014 INTERNATIONAL SYMPOSIUM ON BIOMETRICS AND SECURITY TECHNOLOGIES (ISBAST), P256, DOI 10.1109/ISBAST.2014.7013131
   Wallace E., 1988, 1498 POL SCI DEV BRA
   Wickramasuriya J., 2004, MULTIMEDIA '04, P48, DOI DOI 10.1145/1027527.1027537
   Zhang W., 2005, P IEEE INT C IM PROC, V3
NR 29
TC 7
Z9 7
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 37
DI 10.1145/3312574
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IM3ZV
UT WOS:000477935400008
DA 2024-07-18
ER

PT J
AU Bai, YL
   Yang, KY
   Mei, T
   Ma, WY
   Zhao, TJ
AF Bai, Yalong
   Yang, Kuiyuan
   Mei, Tao
   Ma, Wei-Ying
   Zhao, Tiejun
TI Automatic Data Augmentation from Massive Web Images for Deep Visual
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Dataset construction; deep convolutional neural network; dataset
   augmentation
ID CONSTRUCTION; OBJECT
AB Large-scale image datasets and deep convolutional neural networks (DCNNs) are the two primary driving forces for the rapid progress in generic object recognition tasks in recent years. While lots of network architectures have been continuously designed to pursue lower error rates, few efforts are devoted to enlarging existing datasets due to high labeling costs and unfair comparison issues. In this article, we aim to achieve lower error rates by augmenting existing datasets in an automatic manner. Our method leverages both the web and DCNN, where the web provides massive images with rich contextual information, and DCNN replaces humans to automatically label images under the guidance of web contextual information. Experiments show that our method can automatically scale up existing datasets significantly from billions of web pages with high accuracy. The performance on object recognition tasks and transfer learning tasks have been significantly improved by using the automatically augmented datasets, which demonstrates that more supervisory information has been automatically gathered from the web. Both the dataset and models trained on the dataset have been made publicly available.
C1 [Bai, Yalong; Zhao, Tiejun] Harbin Inst Technol, Harbin 150090, Heilongjiang, Peoples R China.
   [Yang, Kuiyuan] DeepMotion, 9 North 4th Ring West Rd, Beijing 100190, Peoples R China.
   [Mei, Tao] JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
   [Ma, Wei-Ying] Bytedance, 43 Beisanhuan West Rd, Beijing 100080, Peoples R China.
C3 Harbin Institute of Technology
RP Bai, YL (corresponding author), Harbin Inst Technol, Harbin 150090, Heilongjiang, Peoples R China.
EM ylbai@outlook.com; kuiyuanyang@deepmotion.ai; tmei@jd.com;
   weiyingma@bytedance.com; tjzhao@hit.edu.cn
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307; Bai, Yalong/0000-0002-8416-9027
CR [Anonymous], 2009, TECHNICAL REPORT
   [Anonymous], 2013, Tech. rep.
   [Anonymous], 2011, Technical Report CNS-TR-2011-001
   [Anonymous], ARXIV170208513
   [Anonymous], 1995, COMMUNICATIONS ACM
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], ARXIV151204785
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], WEBVISION VISUAL UND
   [Anonymous], 2014, INT J COMPUT VISION
   [Anonymous], ARXIV16070175
   [Anonymous], 2015, ARXIV151106789
   Arjovsky M., 2017, ARXIV170107875
   Bai YL, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P441, DOI 10.1145/2733373.2806243
   Collins B, 2008, LECT NOTES COMPUT SC, V5302, P86, DOI 10.1007/978-3-540-88682-2_8
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ewerth R, 2012, IEEE T MULTIMEDIA, V14, P1008, DOI 10.1109/TMM.2012.2186956
   Fei Wu, 2015, IEEE Transactions on Big Data, V1, P109, DOI 10.1109/TBDATA.2015.2497270
   Griffin Gregory, 2007, CALTECH 256 OBJECT C
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XF, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1230812.1230816
   Hua X.-S., 2013, Proceedings of the 21st ACM International Conference on Multimedia, Oct. 21-25, ACM, P243
   Khosla A., 2011, P CVPR WORKSHOP FINE, V2, P1
   Krishna R., 2016, CoRR
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li W, 2014, LECT NOTES COMPUT SC, V8693, P437, DOI 10.1007/978-3-319-10602-1_29
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Navigli R, 2012, ARTIF INTELL, V193, P217, DOI 10.1016/j.artint.2012.07.001
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Simonyan K., 2014, 14091556 ARXIV
   Sukhbaatar Sainbayar, 2014, Training convolutional networks with noisy labels
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Wang S, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700292
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yu W, 2015, IEEE T MULTIMEDIA, V17, P2000, DOI 10.1109/TMM.2015.2480340
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang L, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490823
   Zhou BL, 2014, ADV NEUR IN, V27
NR 47
TC 1
Z9 1
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
AR 69
DI 10.1145/3204941
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GT7CI
UT WOS:000444677600002
DA 2024-07-18
ER

PT J
AU Li, YS
   Deng, YH
   Tang, XY
   Cai, WT
   Liu, XG
   Wang, G
AF Li, Yusen
   Deng, Yunhua
   Tang, Xueyan
   Cai, Wentong
   Liu, Xiaoguang
   Wang, Gang
TI Cost-Efficient Server Provisioning for Cloud Gaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cloud gaming; service cost; server provisioning; software distribution;
   heuristic algorithms
ID APPROXIMATIONS; PERFORMANCE; TIME
AB Cloud gaming has gained significant popularity recently due to many important benefits such as removal of device constraints, instant-on, and cross-platform. The properties of intensive resource demands and dynamic workloads make cloud gaming appropriate to be supported by an elastic cloud platform. Facing a large user population, a fundamental problem is how to provide satisfactory cloud gaming service at modest cost. We observe that the software storage cost could be substantial compared to the server running cost in cloud gaming using elastic cloud resources. Therefore, in this article, we address the server provisioning problem for cloud gaming to optimize both the server running cost and the software storage cost. We find that the distribution of game software among servers and the selection of server types both trigger tradeoffs between the software storage cost and the server running cost in cloud gaming. We formulate the problem with a stochastic model and employ queueing theory to conduct a solid theoretical analysis of the system behaviors under different request dispatching policies. We then propose several classes of algorithms to approximate the optimal solution. The proposed algorithms are evaluated by extensive experiments using real-world parameters. The results show that the proposed Ordered and Genetic algorithms are computationally efficient, nearly cost-optimal, and highly robust to dynamic changes.
C1 [Li, Yusen; Liu, Xiaoguang; Wang, Gang] Nankai Univ, Comp Sci & Informat Secur Dept, 38 Tongyan Rd, Tianjin 300350, Peoples R China.
   [Deng, Yunhua] Huawei Technol, Huawei Ind Base Bantian, Shenzhen 518129, Peoples R China.
   [Tang, Xueyan; Cai, Wentong] Nanyang Technol Univ, Sch Comp Sci & Engn, 50 Nanyang Ave, Singapore 639798, Singapore.
C3 Nankai University; Huawei Technologies; Nanyang Technological University
RP Li, YS (corresponding author), Nankai Univ, Comp Sci & Informat Secur Dept, 38 Tongyan Rd, Tianjin 300350, Peoples R China.
EM liyusen@nbjl.nankai.edu.cn; dengyunhua1@huawei.com; asxytang@ntu.edu.sg;
   aswtcai@ntu.edu.sg; liuxg@nbjl.nankai.edu.cn; wgzwp@nbjl.nankai.edu.cn
RI wang, gang/ITT-0670-2023; Cai, Wentong/A-3720-2011; Tang,
   Xueyan/A-3703-2011
OI Cai, Wentong/0000-0002-0183-3835; 
FU NSF of China [61373018, 61602266, 11550110491]; NSF of Tianjin
   [16JCYBJC41900, 17JCYBJC15300]; National Research Foundation, Prime
   Minister's Office, Singapore under its IDM Futures Funding Initiative;
   Singapore Ministry of Education Academic Research Fund Tier 2
   [MOE2013-T2-2-067]
FX This work is supported by NSF of China (under grant 61373018, grant
   61602266 and grant 11550110491) and NSF of Tianjin (under grant
   16JCYBJC41900 and grant 17JCYBJC15300). This research is also supported
   by the National Research Foundation, Prime Minister's Office, Singapore
   under its IDM Futures Funding Initiative, and by Singapore Ministry of
   Education Academic Research Fund Tier 2 under Grant MOE2013-T2-2-067.
CR Adan I., 2001, QUEUEING THEORY
   Ahmadi H, 2014, MULTIMEDIA SYST, V20, P485, DOI 10.1007/s00530-014-0381-1
   [Anonymous], ENTERPRISE SERVICE C
   [Anonymous], 2014, P 13 ANN WORKSH NETW
   Askar SS, 2015, MATH PROBL ENG, V2015, DOI 10.1155/2015/232456
   Basiri M, 2018, IEEE T CIRC SYST VID, V28, P972, DOI 10.1109/TCSVT.2016.2632121
   BAYS C, 1977, COMMUN ACM, V20, P191, DOI 10.1145/359436.359453
   BOXMA OJ, 1979, OPER RES, V27, P1115, DOI 10.1287/opre.27.6.1115
   Cai W, 2016, P IEEE, V104, P687, DOI 10.1109/JPROC.2016.2539418
   Cezik MT, 2008, MANAGE SCI, V54, P310, DOI 10.1287/mnsc.1070.0824
   Chan W. C., 2000, PERFORMANCE ANAL TEL
   Choy S, 2014, MULTIMEDIA SYST, V20, P503, DOI 10.1007/s00530-014-0367-z
   Claypool M, 2014, MULTIMEDIA SYST, V20, P471, DOI 10.1007/s00530-014-0362-4
   Davis L., 1991, Handbook of Genetic Algorithms
   Deng Yunhua, 2016, P 24 ACM INT C MULT, P918
   Gandhi A, 2010, PERFORM EVALUATION, V67, P1155, DOI 10.1016/j.peva.2010.08.009
   Hemmati M., 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, P7
   HOKSTAD P, 1978, OPER RES, V26, P510, DOI 10.1287/opre.26.3.510
   Hong HJ, 2015, IEEE T CLOUD COMPUT, V3, P42, DOI 10.1109/TCC.2014.2338295
   Hong Y.-J., 2011, P ACM SIGMETRICS JOI, P147
   Huang Chun-Ying, 2013, P 4 ACM MULT SYST C, P36, DOI DOI 10.1145/2483977.2483981
   Jiao L, 2014, IEEE INFOCOM SER, P28, DOI 10.1109/INFOCOM.2014.6847921
   Koole G., 2000, P QNETS, V23, P1
   Lee KW, 2005, COMPUT NETW, V49, P84, DOI 10.1016/j.comnet.2005.04.006
   Lee K, 2014, MOBISYS'14: PROCEEDINGS OF THE 12TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P347, DOI 10.1145/2594368.2601474
   Lee Y.-T., 2011, APPL COMPUTATIONAL M, P123, DOI [10.1145/1943552.1943569, DOI 10.1145/1943552.1943569]
   Lee Y.-T., 2012, Network and Systems Support for Games (NetGames), 2012 11th Annual Workshop on, P1
   Li YS, 2016, IEEE T PARALL DISTR, V27, P157, DOI 10.1109/TPDS.2015.2393868
   Li YS, 2015, IEEE T CIRC SYST VID, V25, P2052, DOI 10.1109/TCSVT.2015.2450152
   Li YS, 2014, PROCEEDINGS OF THE 26TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES (SPAA'14), P2, DOI 10.1145/2612669.2612675
   Li Yusen, 2017, P 2017 ACM MULT C
   Lin MH, 2013, IEEE ACM T NETWORK, V21, P1378, DOI 10.1109/TNET.2012.2226216
   Murata T, 1996, COMPUT IND ENG, V30, P1061, DOI 10.1016/0360-8352(96)00053-8
   Poole D., 2010, Linear algebra: A modern introduction
   Shea R., 2015, P 6 ACM MULT SYST C, P97
   Tang J, 2015, INT CON DISTR COMP S, P215, DOI 10.1109/ICDCS.2015.30
   TAVARE S, 1979, BIOMETRICS, V35, P831, DOI 10.2307/2530117
   Wang F, 2012, IEEE INFOCOM SER, P199, DOI 10.1109/INFCOM.2012.6195578
   Wang QS, 2017, INT CON DISTR COMP S, P1565, DOI 10.1109/ICDCS.2017.16
   Wang WW, 2013, ADV MATER RES-SWITZ, V602-604, P13, DOI 10.4028/www.scientific.net/AMR.602-604.13
   Wu D, 2014, IEEE T CIRC SYST VID, V24, P1405, DOI 10.1109/TCSVT.2014.2302543
   Wu Y, 2011, INT CON DISTR COMP S, P268, DOI 10.1109/ICDCS.2011.50
   Xhafa F, 2012, INT CON ADV INFO NET, P7, DOI 10.1109/AINA.2012.64
   Xie J., 2010, PARALLEL DISTRIBUTED, P1
   Zhao YH, 2014, IEEE INFOCOM SER, P298, DOI 10.1109/INFOCOM.2014.6847951
   Zheng HY, 2016, IEEE T PARALL DISTR, V27, P271, DOI 10.1109/TPDS.2015.2388473
   Zheng HY, 2013, INT CON DISTR COMP S, P500, DOI 10.1109/ICDCS.2013.44
NR 47
TC 14
Z9 15
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
SU S
AR 55
DI 10.1145/3190838
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG9GD
UT WOS:000455314500003
DA 2024-07-18
ER

PT J
AU Petrangeli, S
   Van Der Hooft, J
   Wauters, T
   De Turck, F
AF Petrangeli, Stefano
   Van Der Hooft, Jeroen
   Wauters, Tim
   De Turck, Filip
TI Quality of Experience-Centric Management of Adaptive Video Streaming
   Services: Status and Challenges
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Quality of experience; HTTP adaptive streaming; MPEG-DASH
ID OPTIMIZATION; PERFORMANCE; FRAMEWORK; DELIVERY
AB Video streaming applications currently dominate Internet traffic. Particularly, HTTP Adaptive Streaming ( HAS) has emerged as the dominant standard for streaming videos over the best-effort Internet, thanks to its capability of matching the video quality to the available network resources. In HAS, the video client is equipped with a heuristic that dynamically decides the most suitable quality to stream the content, based on information such as the perceived network bandwidth or the video player buffer status. The goal of this heuristic is to optimize the quality as perceived by the user, the so-called Quality of Experience (QoE). Despite the many advantages brought by the adaptive streaming principle, optimizing users' QoE is far from trivial. Current heuristics are still suboptimal when sudden bandwidth drops occur, especially in wireless environments, thus leading to freezes in the video playout, the main factor influencing users' QoE. This issue is aggravated in case of live events, where the player buffer has to be kept as small as possible in order to reduce the playout delay between the user and the live signal. In light of the above, in recent years, several works have been proposed with the aim of extending the classical purely client-based structure of adaptive video streaming, in order to fully optimize users' QoE. In this article, a survey is presented of research works on this topic together with a classification based on where the optimization takes place. This classification goes beyond client-based heuristics to investigate the usage of server-and network-assisted architectures and of new application and transport layer protocols. In addition, we outline the major challenges currently arising in the field of multimedia delivery, which are going to be of extreme relevance in future years.
C1 [Petrangeli, Stefano; Van Der Hooft, Jeroen; Wauters, Tim; De Turck, Filip] Univ Ghent, Imec, IDLab, Dept Informat Technol, Technol Pk Zwijnaarde 15, B-9052 Ghent, Belgium.
C3 IMEC; Ghent University
RP Petrangeli, S (corresponding author), Univ Ghent, Imec, IDLab, Dept Informat Technol, Technol Pk Zwijnaarde 15, B-9052 Ghent, Belgium.
EM stefano.petrangel@ugent.be; jeroen.vanderhooft@ugent.be;
   tim.wauters@ugent.be; filip.deturck@ugent.be
OI Petrangeli, Stefano/0000-0002-5492-7747; De Turck,
   Filip/0000-0003-4824-1199
FU imec ICON PRO-FLOW project [150223]
FX This research was performed partially within the imec ICON PRO-FLOW
   project (150223).
CR Ahlehagh H, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P1357
   Ahlgren B, 2012, IEEE COMMUN MAG, V50, P26, DOI 10.1109/MCOM.2012.6231276
   Ahmad A, 2016, COMPUT NETW, V110, P168, DOI 10.1016/j.comnet.2016.09.022
   Akhshabi S, 2012, SIGNAL PROCESS-IMAGE, V27, P271, DOI 10.1016/j.image.2011.10.003
   Akhshabi Saamer, 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video-NOSSDAV'13
   [Anonymous], IEEE COMMUNICATIONS
   [Anonymous], 2017, ACM T MULTIMEDIA COM
   [Anonymous], P 26 INT WORKSH NETW
   [Anonymous], IEEE T COMPUT
   [Anonymous], P 7 INT C MULT SYST
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], IEEE COMMUNICATIONS
   [Anonymous], 2017, MULTIMEDIA TOOLS APP
   [Anonymous], P 2017 ACM MULT C MM
   [Anonymous], 2017, P 27 NOSSDAV, DOI DOI 10.1145/3083165.3083180
   [Anonymous], MID 2015 UPD CONV VI
   [Anonymous], ABS160600264 CORR
   [Anonymous], P 7 INT C MULT SYST
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], 2016, P 7 INT C MULT SYST
   [Anonymous], 2015, 12 USENIX S NETW SYS
   [Anonymous], 2013, 2013 IEEE INT C MULT
   [Anonymous], 2015, P IFIP NETW C
   [Anonymous], 2014, P 2014 WORKSHOP DESI
   [Anonymous], 2015, P 6 ACM MULTIMEDIA S
   [Anonymous], 2300952017 ISOIEC
   [Anonymous], P 26 INT WORKSH NETW
   [Anonymous], 2014, 2014 IEEE NETW OP MA
   [Anonymous], USE CASES DRAFT REQU
   [Anonymous], CISC VIS NEYW IND FO
   [Anonymous], ACM T MULTIM COMPUT
   [Anonymous], SIGCOMM COMPUT COMMU
   Bagci KT, 2016, IEEE IMAGE PROC, P1519, DOI 10.1109/ICIP.2016.7532612
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Bhat D, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P62, DOI 10.1145/3083187.3083196
   Bottger Timm, 2016, ABS160605519 CORR
   Bouten N, 2016, INT CONF NETW SER, P82, DOI 10.1109/CNSM.2016.7818403
   Casas P, 2014, 2014 IFIP WIRELESS DAYS (WD)
   Cetinkaya C, 2014, IEEE I C CONS ELECT, P74, DOI 10.1109/ICCE-Berlin.2014.7034284
   Chen J., 2013, Proceedings of the 19th Annual International Conference on Mobile Computing Networking, P389
   Cherif W., 2015, P 25 ACM WORKSHOP NE, P25
   Claeys M, 2016, INT CONF NETW SER, P100, DOI 10.1109/CNSM.2016.7818405
   Claeys M, 2016, IEEE T NETW SERV MAN, V13, P308, DOI 10.1109/TNSM.2016.2546459
   Colonnese S, 2015, AD HOC NETW, V24, P74, DOI 10.1016/j.adhoc.2014.07.023
   Cui Y, 2017, IEEE INTERNET COMPUT, V21, P72, DOI 10.1109/MIC.2017.44
   De Vriendt J, 2013, 2013 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2013), P1288
   Deng Rui, 2017, MULTIMED TOOLS APPL, P1
   Dimopoulos G., 2016, P 2016 INT MEAS C SA, P513, DOI DOI 10.1145/2987443.2987459
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   El Essaili A, 2015, IEEE T CIRC SYST VID, V25, P988, DOI 10.1109/TCSVT.2014.2367355
   Foukas X, 2017, IEEE COMMUN MAG, V55, P94, DOI 10.1109/MCOM.2017.1600951
   Ge C, 2016, PROCEEDINGS OF THE 2016 3RD ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ACM-ICN '16), P237, DOI 10.1145/2984356.2988522
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   Han B., 2013, IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS), P375
   Han B, 2016, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'16), P129, DOI 10.1145/2999572.2999606
   He HL, 2016, IEEE T VEH TECHNOL, V65, P7917, DOI 10.1109/TVT.2016.2543747
   Hossfeld Tobias, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P111, DOI 10.1109/QoMEX.2014.6982305
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Huysegems R, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P541, DOI 10.1145/2733373.2806264
   James C, 2016, I S MOD ANAL SIM COM, P331, DOI 10.1109/MASCOTS.2016.75
   Jin YC, 2015, IEEE T CIRC SYST VID, V25, P1914, DOI 10.1109/TCSVT.2015.2402892
   Kleinrouweler JW, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P36, DOI 10.1145/2910017.2910599
   Krishnamoorthi V, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P551, DOI 10.1145/2733373.2806270
   Krishnappa D.K., 2015, Proceedings of the 6th ACM Multimedia Systems Conference. ACM, P37, DOI DOI 10.1145/2713168.2713175
   Krishnappa DK, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2716310
   Langley A, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P183, DOI 10.1145/3098822.3098842
   Lederer S., 2012, P 3 MULT SYST C, P89
   Lederer Stefan., 2013, MULTIMEDIA EXPO ICME, P1
   Lee D.H., 2014, P NETWORK OPERATING, P31
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liotou E., 2016, COMMUNICATIONS ICC 2, P1
   Liu YN, 2013, IEEE ICC, P3629, DOI 10.1109/ICC.2013.6655116
   Lohmar T., 2012, 22nd ACM Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV), P21
   Lu Z, 2014, IEEE ICC, P1723, DOI 10.1109/ICC.2014.6883571
   Ma G, 2017, IEEE J SEL AREA COMM, V35, P1076, DOI 10.1109/JSAC.2017.2680958
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   Nam H, 2014, IEEE GLOB COMM CONF, P1317, DOI 10.1109/GLOCOM.2014.7036990
   Nguyen Duc V., 2016, 2016 IEEE International Conference on Consumer Electronics (ICCE), P189, DOI 10.1109/ICCE.2016.7430575
   Niamut OA, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P46, DOI 10.1145/2910017.2910606
   Petrangeli S, 2017, J NETW COMPUT APPL, V94, P78, DOI 10.1016/j.jnca.2017.07.009
   Petrangeli S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818361
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Rainer B, 2016, IEEE J SEL AREA COMM, V34, P2130, DOI 10.1109/JSAC.2016.2577365
   Rainer B, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2648429
   Ramakrishnan A, 2016, IEEE INT SYM MULTIM, P238, DOI [10.1109/ISM.2016.47, 10.1109/ISM.2016.0054]
   Reichl P, 2015, INT WORK QUAL MULTIM, DOI 10.1109/QoMEX.2015.7148138
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Schwarzmann S, 2016, 2016 28TH INTERNATIONAL TELETRAFFIC CONGRESS (ITC 28), VOL 3, P13, DOI 10.1109/ITC-28.2016.310
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Sieber C, 2015, PROCEEDINGS OF THE 2015 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM), P1261, DOI 10.1109/INM.2015.7140478
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Song M, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700282
   Spiteri Kevin, 2016, P 35 ANN IEEE INT C, P1
   Stohr D, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1113, DOI 10.1145/3123266.3123426
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Tavakoli S, 2016, IEEE J SEL AREA COMM, V34, P2141, DOI 10.1109/JSAC.2016.2577361
   Thomas Emmanuel, 2017, Motion Imaging Journal, V126, P22, DOI 10.5594/JMI.2016.2632338
   Toni L, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700294
   van der Hooft J, 2018, J NETW SYST MANAG, V26, P51, DOI 10.1007/s10922-017-9407-2
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   van der Hooft J, 2015, PROCEEDINGS OF THE 2015 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM), P131, DOI 10.1109/INM.2015.7140285
   Wang C, 2015, 2015 IEEE 8TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING, P917, DOI 10.1109/CLOUD.2015.125
   Wang Z, 2014, IEEE INFOCOM SER, P91, DOI 10.1109/INFOCOM.2014.6847928
   Wei Pu, 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P65, DOI 10.1109/PV.2012.6229745
   Wei S, 2015, IEEE INT WORKSH MULT
   Wei Sheng., 2014, NETWORK OPERATING SY, P37
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiao MB, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P22, DOI 10.1145/2964284.2964313
   Yu YT, 2015, CONSUM COMM NETWORK, P588, DOI 10.1109/CCNC.2015.7158039
   Zhao MC, 2015, IEEE T CIRC SYST VID, V25, P451, DOI 10.1109/TCSVT.2014.2357094
NR 110
TC 27
Z9 27
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 31
DI 10.1145/3165266
PG 29
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TD
UT WOS:000434634700004
OA Green Published
DA 2024-07-18
ER

PT J
AU Ceballos, R
   Ionascu, B
   Park, W
   Eid, M
AF Ceballos, Rodrigo
   Ionascu, Beatrice
   Park, Wanjoo
   Eid, Mohamad
TI Implicit Emotion Communication: EEG Classification and Haptic Feedback
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Affective computing; affective haptics; multimodal interaction; tactile
   gestures
ID VALENCE; RECOGNITION
AB Today, ubiquitous digital communication systems do not have an intuitive, natural way of communicating emotion, which, in turn, affects the degree to which humans can emotionally connect and interact with one another. To address this problem, a more natural, intuitive, and implicit emotion communication system was designed and created that employs asymmetry-based EEG emotion classification for detecting the emotional state of the sender and haptic feedback (in the form of tactile gestures) for displaying emotions for a receiver. Emotions are modeled in terms of valence (positive/negative emotions) and arousal (intensity of the emotion). Performance analysis shows that the proposed EEG subject-dependent emotion classification model with Free Asymmetry features allows for more flexible feature-generation schemes than other existing algorithms and attains an average accuracy of 92.5% for valence and 96.5% for arousal, outperforming previous-generation schemes in high feature space. As for the haptic feedback, a tactile gesture authoring tool and a haptic jacket were developed to design tactile gestures that can intensify emotional reactions in terms of valence and arousal. Experimental study demonstrated that subject-independent emotion transmission through tactile gestures is effective for the arousal dimension of an emotion but is less effective for valence. Consistency in subject-dependent responses for both valence and arousal suggests that personalized tactile gestures would be more effective.
C1 [Ceballos, Rodrigo; Ionascu, Beatrice; Park, Wanjoo; Eid, Mohamad] New York Univ Abu Dhabi, Appl Interact Multimedia Res Lab AIMLab, Lab 5, Expt Res Bldg C1,POB 129188, Abu Dhabi, U Arab Emirates.
C3 New York University Abu Dhabi
RP Ceballos, R (corresponding author), New York Univ Abu Dhabi, Appl Interact Multimedia Res Lab AIMLab, Lab 5, Expt Res Bldg C1,POB 129188, Abu Dhabi, U Arab Emirates.
EM rceballos98@gmail.com; beatrice.nsc@gmail.com; wanjoo@nyu.edu;
   mohamad.eid@nyu.edu
OI Park, Wanjoo/0000-0003-1467-4156
FU New York University Abu Dhabi
FX This work is supported by the New York University Abu Dhabi.
CR AlZoubi O, 2009, LECT NOTES ARTIF INT, V5866, P52, DOI 10.1007/978-3-642-10439-8_6
   Appelhans BM, 2006, REV GEN PSYCHOL, V10, P229, DOI 10.1037/1089-2680.10.3.229
   Arafsha F, 2015, MULTIMED TOOLS APPL, V74, P3035, DOI 10.1007/s11042-013-1767-3
   Bailenson JN, 2007, HUM-COMPUT INTERACT, V22, P325
   Ball T, 2009, NEUROIMAGE, V46, P708, DOI 10.1016/j.neuroimage.2009.02.028
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Brown L, 2011, IEEE ENG MED BIO, P2188, DOI 10.1109/IEMBS.2011.6090412
   BUCK R, 1994, J PRAGMATICS, V22, P265, DOI 10.1016/0378-2166(94)90112-0
   Cha J., 2009, MM '09: Proceedings of the seventeen ACM international conference on Multimedia, P1135, DOI [10.1145/1631272.1631535, DOI 10.1145/1631272.1631535]
   Chakraborty M, 2012, PROC TECH, V4, P830, DOI 10.1016/j.protcy.2012.05.136
   Clynes M, 1977, SENTICS TOUCH EMOTIO
   Duan RN, 2013, I IEEE EMBS C NEUR E, P81, DOI 10.1109/NER.2013.6695876
   Gatti E, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P247, DOI 10.1109/WHC.2013.6548416
   Haans Antal, 2006, Virtual Reality, P149
   Israr A., 2011, 2011 IEEE World Haptics Conference (WHC 2011), P457, DOI 10.1109/WHC.2011.5945529
   JIBRIL TANIMU AHMED., 2013, Asian Social Science, V9, P201, DOI [10.5539/ass.v9n4p201, DOI 10.5539/ASS.V9N4P201]
   Kim MK, 2013, COMPUT MATH METHOD M, V2013, DOI 10.1155/2013/573734
   Krohn FranklinB., 2004, J TECH WRIT COMMUN, V34, P321, DOI DOI 10.2190/9EQH-DE81-CWG1-QLL9
   Lang P. J., 2005, A6 U FLOR CTR RES PS
   Lemmens P, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P7, DOI 10.1109/WHC.2009.4810832
   Lentini Rodrigo, 2016, WORLD ACAD SCI ENG T, V10, P1862
   Mazzoni A, 2015, PROCEEDINGS OF THE 2015 7TH INTERNATIONAL CONFERENCE ON INTELLIGENT TECHNOLOGIES FOR INTERACTIVE ENTERTAINMENT, P64, DOI 10.4108/icst.intetain.2015.259625
   McDaniel Troy, 2014, Universal Access in Human-Computer Interaction. Design and Development Methods for Universal Access. 8th International Conference, UAHCI 2014, Held as Part of HCI International 2014. Proceedings: LNCS 8513, P419, DOI 10.1007/978-3-319-07437-5_40
   Nummenmaa L, 2014, P NATL ACAD SCI USA, V111, P646, DOI 10.1073/pnas.1321664111
   Petrantonakis PC, 2011, IEEE T INF TECHNOL B, V15, P737, DOI 10.1109/TITB.2011.2157933
   Rahman A. S. M. M., 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P228, DOI 10.1109/ISM.2010.40
   Raisamo J, 2013, IEEE T HAPTICS, V6, P517, DOI [10.1109/ToH.2013.25, 10.1109/TOH.2013.25]
   Roter DL, 2006, J GEN INTERN MED, V21, pS28, DOI 10.1111/j.1525-1497.2006.00306.x
   Salminen K, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1555
   Schmidt LA, 2001, COGNITION EMOTION, V15, P487, DOI 10.1080/0269993004200187
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P211, DOI 10.1109/T-AFFC.2011.37
   Sun K, 2009, IEEE INT CON MULTI, P566, DOI 10.1109/ICME.2009.5202559
   Tsetserukou D., 2010, CHI EA '10: Proceedings of the 28th of the international conference extended abstracts on Human factors in computing systems, P3031, DOI DOI 10.1145/1753846.1753911
   Wang XW, 2014, NEUROCOMPUTING, V129, P94, DOI 10.1016/j.neucom.2013.06.046
   Westerink JHDM, 2008, PHILIPS RES BOOK SER, V8, P149, DOI 10.1007/978-1-4020-6593-4_14
   Xu Zhe, 2002, Proceedings of Third International Symposium on Communication Systems Networks and Digital Signal Processing, P164
   Yool Y, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P235, DOI 10.1109/WHC.2015.7177719
   Zheng W. L., 2016, ARXIV160102197
NR 38
TC 7
Z9 7
U1 2
U2 38
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2018
VL 14
IS 1
AR 3
DI 10.1145/3152128
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW9HO
UT WOS:000425646500003
DA 2024-07-18
ER

PT J
AU Yakubu, AM
   Maddage, NC
   Atrey, PK
AF Yakubu, Abukari M.
   Maddage, Namunu C.
   Atrey, Pradeep K.
TI Securing Speech Noise Reduction in Outsourced Environment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Encrypted domain; secret sharing; homomorphic encryption; secure noise
   reduction
AB Cloud data centers (CDCs) are becoming a cost-effective method for processing and storage of multimedia data including images, video, and audio. Since CDCs are physically located in different jurisdictions, and are managed by external parties, data security is a growing concern. Data encryption at CDCs is commonly practiced to improve data security. However, to process the data at CDCs, data must often be decrypted, which raises issues in security. Thus, there is a growing demand for data processing techniques in encrypted domain in such an outsourced environment. In this article, we analyze encrypted domain speech content processing techniques for noise reduction. Noise contaminates speech during transmission or during the acquisition process by recording. As a result, the quality of the speech content is degraded. We apply Shamir's secret sharing as the cryptosystem to encrypt speech data before uploading it to a CDC. We then propose finite impulse response digital filters to reduce white and wind noise in the speech in the encrypted domain. We prove that our proposed schemes meet the security requirements of efficiency, accuracy, and checkability for both semi-honest and malicious adversarial models. Experimental results show that our proposed filtering techniques for speech noise reduction in the encrypted domain produce similar results when compared to plaintext domain processing.
C1 [Yakubu, Abukari M.] Univ Winnipeg, 515 Portage Ave, Winnipeg, MB R3B 2E9, Canada.
   [Maddage, Namunu C.] NextGmultimedia, Melbourne, Vic, Australia.
   [Atrey, Pradeep K.] SUNY Albany, 1400 Washington Ave, Albany, NY 12118 USA.
   [Maddage, Namunu C.] NextGmultimedia Pty Ltd, 15 Addison Way,Roxburgh Pk, Melbourne, Vic 3064, Australia.
C3 University of Winnipeg; State University of New York (SUNY) System;
   State University of New York (SUNY) Albany
RP Yakubu, AM (corresponding author), Univ Winnipeg, 515 Portage Ave, Winnipeg, MB R3B 2E9, Canada.
EM myabukari@gmail.com; namunu_chinthaka@yahoo.com; patrey@albany.edu
RI Yakubu, Abukari/ACP-8483-2022
OI Mohammed Yakubu, Abukari/0000-0002-4381-0914
FU Natural Sciences and Engineering Research Council of Canada [371714];
   University at Albany, SUNY [640075]
FX This work is supported in parts by the Natural Sciences and Engineering
   Research Council of Canada, Discovery Grant #371714, and the University
   at Albany, SUNY Grant #640075.
CR [Anonymous], EURASIP J INF SECUR
   [Anonymous], 2016, P 8 IEEE INT WORKSH
   [Anonymous], INFORM SECURITY CRYP
   Backes M, 2010, LECT NOTES COMPUT SC, V6345, P508, DOI 10.1007/978-3-642-15497-3_31
   Barni M, 2009, LECT NOTES COMPUT SC, V5789, P424, DOI 10.1007/978-3-642-04444-1_26
   Brookes Mike, 2011, VOICEBOX
   Carnegie Mellon University (CMU), 2007, CARN MELL U SPEECH D
   Damgård I, 2002, LECT NOTES COMPUT SC, V2501, P125
   Fujisaki E, 1998, LECT NOTES COMPUT SC, V1403, P32, DOI 10.1007/BFb0054115
   Gamal T.E., 1984, P WORKSH THEOR APPL, V196, P10, DOI DOI 10.1007/3-540-39568-7_2
   Gautschi Walter, 2011, Numerical analysis
   Hohenberger S, 2005, LECT NOTES COMPUT SC, V3378, P264
   Hu XJ, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2886777
   Hu Y, 2008, IEEE T AUDIO SPEECH, V16, P229, DOI 10.1109/TASL.2007.911054
   Industrial Noise Control INC, 2010, NOIS CONTR CONC
   ITU-T, 2001, PERC EV SPEECH QUAL, P862
   Khan LA, 2010, DIGIT INVEST, V7, P65, DOI 10.1016/j.diin.2009.10.001
   Lathey A, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2656205
   Loizou PC, 2011, STUD COMPUT INTELL, V346, P623
   Lu Wenjun, 2009, P IS T SPIE ELECT IM
   MALIK MB, 2012, PROCEEDINGS OF 3RD I, P26, DOI DOI 10.1109/ICCCT.2012.15
   MELCHOR CA, 2008, P IEEE ISIT TOR CAN, P1858
   Paillier P, 1999, LECT NOTES COMPUT SC, V1592, P223
   Pathak MA, 2013, IEEE T AUDIO SPEECH, V21, P397, DOI 10.1109/TASL.2012.2215602
   Piva A, 2010, IEEE T INF FOREN SEC, V5, P13, DOI 10.1109/TIFS.2009.2038761
   Piva Alessandro, 2006, P IS T SPIE ELECT IM
   Prins Jeroen P., 2007, EURASIP J INF SECUR, V2007, P20
   Troncoso-Pastoriza JR, 2013, IEEE SIGNAL PROC MAG, V30, P29, DOI 10.1109/MSP.2012.2228533
   Troncoso-Pastoriza JR, 2011, IEEE T INF FOREN SEC, V6, P469, DOI 10.1109/TIFS.2011.2109385
   Richard G. L., 2004, UNDERSTANDING DIGITA, P121
   Shashanka Madhusudana V. S., 2006, P IEEE INT C AC SPEE, V3
   Sohn H, 2010, LECT NOTES COMPUT SC, V6297, P622, DOI 10.1007/978-3-642-15702-8_57
   Stallings W., 2006, Cryptography and Network Security, V4th
   Stolfo Salvatore J., 1989, P WORKSH SPEECH NAT, P353
   Yakubu Abukari Mohammed., 2016, Proceedings of the IEEE International Conference on Multimedia and Expo (ICME) (Seattle, WA, USA), P1
   Yang R, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2344436.2344441
   Yao A. C., 1982, 23rd Annual Symposium on Foundations of Computer Science, P160, DOI 10.1109/SFCS.1982.38
   Zhaohong Wang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P7401, DOI 10.1109/ICASSP.2014.6855038
   Zheng PJ, 2013, IEEE T IMAGE PROCESS, V22, P2455, DOI 10.1109/TIP.2013.2253474
NR 39
TC 1
Z9 1
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 51
DI 10.1145/3105970
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300005
DA 2024-07-18
ER

PT J
AU Seidenari, L
   Baecchi, C
   Uricchio, T
   Ferracani, A
   Bertini, M
   Del Bimbo, A
AF Seidenari, Lorenzo
   Baecchi, Claudio
   Uricchio, Tiberio
   Ferracani, Andrea
   Bertini, Marco
   Del Bimbo, Alberto
TI Deep Artwork Detection and Retrieval for Automatic Context-Aware Audio
   Guides
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; computer vision; object detection; image retrieval;
   mobile computing; cultural heritage; audio guide
AB In this article, we address the problem of creating a smart audio guide that adapts to the actions and interests of museum visitors. As an autonomous agent, our guide perceives the context and is able to interact with users in an appropriate fashion. To do so, it understands what the visitor is looking at, if the visitor is moving inside the museum hall, or if he or she is talking with a friend. The guide performs automatic recognition of artworks, and it provides configurable interface features to improve the user experience and the fruition of multimedia materials through semi-automatic interaction.
   Our smart audio guide is backed by a computer vision system capable of working in real time on a mobile device, coupled with audio and motion sensors. We propose the use of a compact Convolutional Neural Network (CNN) that performs object classification and localization. Using the same CNN features computed for these tasks, we perform also robust artwork recognition. To improve the recognition accuracy, we perform additional video processing using shape-based filtering, artwork tracking, and temporal filtering. The system has been deployed on an NVIDIA Jetson TK1 and a NVIDIA Shield Tablet K1 and tested in a real-world environment (Bargello Museum of Florence).
C1 [Seidenari, Lorenzo; Baecchi, Claudio; Uricchio, Tiberio; Ferracani, Andrea; Bertini, Marco; Del Bimbo, Alberto] Univ Florence, MICC, Florence, Italy.
C3 University of Florence
RP Seidenari, L (corresponding author), Univ Florence, MICC, Florence, Italy.
EM lorenzo.seidenari@unifi.it; claudio.baecchi@unifi.it;
   tiberio.uricchio@unifi.it; andrea.ferracani@unifi.it;
   marco.bertini@unifi.it; alberto.delbimbo@unifi.it
RI Seidenari, Lorenzo/AAA-1848-2020; Bertini, Marco/X-1325-2019
OI Seidenari, Lorenzo/0000-0003-4816-0268; Bertini,
   Marco/0000-0002-1364-218X; URICCHIO, TIBERIO/0000-0003-1025-4541
FU "Social Museum and Smart Tourism" project [CTN01_00034_231545]
FX This work is partially supported by the "Social Museum and Smart
   Tourism" project (CTN01_00034_231545).
CR [Anonymous], IJCV
   [Anonymous], P ACM MULTIMEDIA MM
   [Anonymous], P IEEE INT C AC SPEE
   [Anonymous], 2014, P IEEE COMP VIS PATT
   [Anonymous], 2015, P IEEE INT C COMP VI
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P INT WORKSH MULT AS
   [Anonymous], P WORKSH WEAR SYST A
   [Anonymous], P MUS WEB MW 04
   [Anonymous], P ANN C INT SPEECH C
   [Anonymous], T AM I ELECT ENG 1
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P ACM INT C MULT RET
   [Anonymous], P ACM MULT MM 16
   [Anonymous], 1990, P SIGCHI C HUM FACT
   [Anonymous], P DIG HER INT C DIGI
   [Anonymous], P INT JOINT C NEUR N
   [Anonymous], ETHNOGRAPHIE EXPOSIO
   [Anonymous], 2012, Analysis and Prediction of Museum Visitors' Behavioral Pattern Types
   [Anonymous], P INT C US MOD UM 07
   [Anonymous], P IEEE CAN C EL COMP
   [Anonymous], P IEEE COMP VIS PATT
   [Anonymous], P IEEE INT C MULT EX
   Bangor A, 2009, J USABILITY STUD, V4, P114
   Brooke J., 1996, USABILITY EVALUATION, P189, DOI DOI 10.1201/9781498710411-35
   COHEN E, 1985, ANN TOURISM RES, V12, P5, DOI 10.1016/0160-7383(85)90037-4
   Drugman T, 2016, IEEE SIGNAL PROC LET, V23, P252, DOI 10.1109/LSP.2015.2495219
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Girshick R., 2015, IEEE I CONF COMP VIS, DOI [DOI 10.1109/ICCV.2015.169, 10.1109/ICCV.2015.169]
   Karaman S, 2016, MULTIMED TOOLS APPL, V75, P3787, DOI 10.1007/s11042-014-2192-y
   Mousazadeh S, 2011, IEEE T AUDIO SPEECH, V19, P916, DOI 10.1109/TASL.2010.2070494
   Picard D, 2015, IEEE SIGNAL PROC MAG, V32, P95, DOI 10.1109/MSP.2015.2409557
   Redmon J., 2016, P IEEE COMP VIS PATT
   Ren S., 2017, IEEETransactionsonPatternAnalysisandMachineIntelligence, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sauro J, 2012, QUANTIFYING THE USER EXPERIENCE: PRACTICAL STATISTICS FOR USER RESEARCH, P1
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Trochim W. M., 2006, RES METHODS KNOWLEDG
   Wang YW, 2009, INTERDISCIPL SCI REV, V34, P139, DOI 10.1179/174327909X441072
   Woo KH, 2000, ELECTRON LETT, V36, P180, DOI 10.1049/el:20000192
   Zhou K, 2016, DESTECH TRANS COMP
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 42
TC 27
Z9 27
U1 4
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
SU S
AR 35
DI 10.1145/3092832
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FP1UO
UT WOS:000417400400003
DA 2024-07-18
ER

PT J
AU Calagari, K
   Elgamal, T
   Diab, K
   Templin, K
   Didyk, P
   Matusik, W
   Hefeeda, M
AF Calagari, Kiana
   Elgamal, Tarek
   Diab, Khaled
   Templin, Krzysztof
   Didyk, Piotr
   Matusik, Wojciech
   Hefeeda, Mohamed
TI Depth Personalization and Streaming of Stereoscopic Sports Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D Video streaming; 3d video; stereoscopic retargeting; depth
   personalization; video storage systems
AB Current three-dimensional displays cannot fully reproduce all depth cues used by a human observer in the real world. Instead, they create only an illusion of looking at a three-dimensional scene. This leads to a number of challenges during the content creation process. To assure correct depth reproduction and visual comfort, either the acquisition setup has to be carefully controlled or additional postprocessing techniques have to be applied. Furthermore, these manipulations need to account for a particular setup that is used to present the content, for example, viewing distance or screen size. This creates additional challenges in the context of personal use when stereoscopic content is shown on TV sets, desktop monitors, or mobile devices. We address this problem by presenting a new system for streaming stereoscopic content. Its key feature is a computationally efficient depth adjustment technique which can automatically optimize viewing experience for videos of field sports such as soccer, football, and tennis. Additionally, the method enables depth personalization to allow users to adjust the amount of depth according to their preferences. Our stereoscopic video streaming system was implemented, deployed, and tested with real users.
C1 [Calagari, Kiana; Diab, Khaled] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   [Elgamal, Tarek; Hefeeda, Mohamed] Qatar Comp Res Inst, Doha, Qatar.
   [Didyk, Piotr] Univ Saarland, Saarbrucken, Germany.
   [Templin, Krzysztof; Matusik, Wojciech] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
C3 Simon Fraser University; Qatar Foundation (QF); Hamad Bin Khalifa
   University-Qatar; Qatar Computing Research Institute; Saarland
   University; Massachusetts Institute of Technology (MIT)
RP Calagari, K (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM kcalagar@sfu.ca; tgamal@qf.org.qa; kdiab@sfu.ca; ktemplin@csail.mit.edu;
   pdidyk@mmci.uni-saarland.de; wojciech@csail.mit.edu; mhefeeda@qf.org.qa
OI Didyk, Piotr/0000-0003-0768-8939
FU Qatar Computing Research Institute (QCRI)
FX This research was supported in part by Qatar Computing Research
   Institute (QCRI).
CR [Anonymous], 2014, PROC NETW OPERATING
   [Anonymous], 2012, 2300912012 ISOIEC
   [Anonymous], BT2021 ITUR
   Baicheng Xin, 2012, Proceedings of the 2012 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC), P286, DOI 10.1109/ICSPCC.2012.6335735
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   BURT P, 1980, PERCEPTION, V9, P671, DOI 10.1068/p090671
   Calagari K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P337, DOI 10.1145/2647868.2654899
   Carballeira P, 2012, IEEE J-STSP, V6, P583, DOI 10.1109/JSTSP.2012.2193378
   COUTANT BE, 1993, OPHTHAL PHYSL OPT, V13, P3, DOI 10.1111/j.1475-1313.1993.tb00419.x
   Diab K., 2014, P ACM C MULT SYST MM, P59
   Didyk P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964991
   Displaybank Co, 2010, SPECIAL REPORT
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Fehn C, 2003, CONF REC ASILOMAR C, P1529
   Freeman J, 2000, PROC SPIE, V3959, P530, DOI 10.1117/12.387207
   Gürler CG, 2011, P IEEE, V99, P694, DOI 10.1109/JPROC.2010.2100010
   Holliman N, 2004, P SOC PHOTO-OPT INS, V5291, P117, DOI 10.1117/12.525853
   Jian-Guang Lou, 2005, 13th Annual ACM International Conference on Multimedia, P161
   Johanson M., 2001, Proceedings. The Second IEEE Workshop on Internet Applications. WIAPP 2001, P12, DOI 10.1109/WIAPP.2001.941865
   Kimata H., 2011, 2011 IEEE 15th International Symposium on Consumer Electronics, P576, DOI 10.1109/ISCE.2011.5973896
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Mendiburu Bernard, 2012, 3D movie making: stereoscopic digital cinema from script to screen
   Oskam Thomas, 2011, ACM T GRAPHIC, V30
   Pajak D, 2014, COMPUT GRAPH FORUM, V33, P195, DOI 10.1111/cgf.12293
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   Pehlivan S, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2169, DOI 10.1109/ICME.2006.262685
   Redert A, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P313
   Shibata T, 2011, J VISION, V11, DOI 10.1167/11.8.11
   Smolic A, 2011, P IEEE, V99, P607, DOI 10.1109/JPROC.2010.2098350
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Tam WJ, 2011, IEEE T BROADCAST, V57, P335, DOI 10.1109/TBC.2011.2125070
   Thorpe J. R., 2011, P SOC MOT PICT TEL E, P1
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wu W., 2011, P 19 ACM INT C MULTI, P13
   Yang QX, 2010, PROC CVPR IEEE, P1458, DOI 10.1109/CVPR.2010.5539797
NR 36
TC 3
Z9 3
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2016
VL 12
IS 3
AR 41
DI 10.1145/2890103
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YO
UT WOS:000379425400007
DA 2024-07-18
ER

PT J
AU Gaddam, VR
   Eg, R
   Langseth, R
   Griwodz, C
   Halvorsen, P
AF Gaddam, Vamsidhar Reddy
   Eg, Ragnhild
   Langseth, Ragnar
   Griwodz, Carsten
   Halvorsen, Pal
TI The Cameraman Operating My Virtual Camera is Artificial: Can the Machine
   Be as Good as a Human?
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Measurement; Performance; Interactive immersion;
   panorama video; zoom; panning; real-time; visual servoing; virtual
   camera; quality of experience; user studies
ID VIDEO
AB In this article, we argue that the energy spent in designing autonomous camera control systems is not spent in vain. We present a real-time virtual camera system that can create smooth camera motion. Similar systems are frequently benchmarked with the human operator as the best possible reference; however, we avoid a priori assumptions in our evaluations. Our main question is simply whether we can design algorithms to steer a virtual camera that can compete with the user experience for recordings from an expert operator with several years of experience? In this respect, we present two low-complexity servoing methods that are explored in two user studies. The results from the user studies give a promising answer to the question pursued. Furthermore, all components of the system meet the real-time requirements on commodity hardware. The growing capabilities of both hardware and network in mobile devices give us hope that this system can be deployed to mobile users in the near future. Moreover, the design of the presented system takes into account that services to concurrent users must be supported.
C1 [Gaddam, Vamsidhar Reddy] Simula Res Lab, Oslo, Norway.
   Univ Oslo, N-0316 Oslo, Norway.
C3 University of Oslo
RP Gaddam, VR (corresponding author), Simula Res Lab, Oslo, Norway.
EM vamsidhg@ifi.uio.no
RI Eg, Ragnhild/AAQ-5857-2021
OI Eg, Ragnhild/0000-0002-9550-0424; Halvorsen, Pal/0000-0003-2073-7029
FU Norwegian Research Council [174867]
FX This work has been performed in the context of the iAD center for
   Research-based Innovation (project number 174867) funded by the
   Norwegian Research Council.
CR ADearden, 2007, P ART AMB INT S, P227
   Ahmed A., 2005, Proc. of Asia-Pacific Symposium on Information Visu-alisation, P27
   [Anonymous], P ACM SIGGRAPH
   [Anonymous], P911 ITUT
   [Anonymous], 2020, INT TELECOMMUNICATIO
   [Anonymous], P SPIE IS T ELECT IM
   Ariki Y, 2006, IEEE INT SYM MULTIM, P851
   Carr P, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P74, DOI 10.1109/DICTA.2009.62
   Carr Peter., 2013, Proceedings of the 21st ACM International Conference on Multimedia, MM '13, P193, DOI DOI 10.1145/2502081.2502086
   Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309
   Chen F, 2010, COMPUT VIS IMAGE UND, V114, P667, DOI 10.1016/j.cviu.2010.01.005
   Chen KT, 2009, MATH COMPUT SCI ENG, P491, DOI 10.1145/1631272.1631339
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   Christie M, 2005, LECT NOTES COMPUT SC, V3638, P40
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Farias MCQ, 2007, IEEE T SIGNAL PROCES, V55, P2954, DOI 10.1109/TSP.2007.893963
   Fehn C, 2006, IEEE INT SYM MULTIM, P291
   Foote Eric., 2013, Proceedings of the 21st ACM International Conference on Multimedia (New York, NY, USA), MM'13, Association for Computing Machinery, P163
   Gaddam VR, 2014, P NOSSDAV, P19
   Goldmann Lutz, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P52, DOI 10.1109/QOMEX.2010.5518189
   Goorts P, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 3, P378
   Grau O, 2004, IEEE T CIRC SYST VID, V14, P370, DOI 10.1109/TCSVT.2004.823397
   Grau Oliver, 2007, 3DTV Conference, 2007, P1
   Halvorsen P., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, Protland, OR, P48
   Hutchinson S, 1996, IEEE T ROBOTIC AUTOM, V12, P651, DOI 10.1109/70.538972
   Jenkin Michael, 1998, VISION INTERFACE REA
   Kaiser R., 2011, 2011 Conference for Visual Media Production, P21, DOI 10.1109/CVMP.2011.9
   Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394
   Lee JS, 2013, MULTIMED TOOLS APPL, V67, P31, DOI 10.1007/s11042-012-1011-6
   Mavlankar A, 2010, SIGNALS COMMUN TECHN, P431, DOI 10.1007/978-3-642-12802-8_19
   Ni P., 2011, P 19 ACM INT C MULT, P463, DOI DOI 10.1145/2072298.2072359
   Papadakis N., 2010, Proceedings 2010 Conference on Visual Media Production (CVMP 2010). 7th European Conference on Visual Media Production, P97, DOI 10.1109/CVMP.2010.20
   Ren JC, 2010, MACH VISION APPL, V21, P855, DOI 10.1007/s00138-009-0212-0
   Tennoe Marius, 2013, P IEEE INT S MULT
   Wang J., 2004, P 12 ANN ACM INT C M, P32
   Wu W., 2009, MM 09, P481, DOI DOI 10.1145/1631272.1631338
   Xinding Sun, 2005, IEEE Transactions on Multimedia, V7, P981, DOI 10.1109/TMM.2005.854388
   Xu M, 2005, IEE P-VIS IMAGE SIGN, V152, P232, DOI 10.1049/ip-vis:20041257
   Xu W, 2013, MULTIMEDIA SYST, V19, P407, DOI 10.1007/s00530-013-0316-2
   Yokoi T., 2005, P IEEE INT C MULT EX
   Yu X., 2003, PROC 11 ACM INT C MU, P11
NR 41
TC 9
Z9 11
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2015
VL 11
IS 4
AR 56
DI 10.1145/2744411
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CK0ZF
UT WOS:000355933700009
DA 2024-07-18
ER

PT J
AU Hu, YT
   Kautz, J
   Yu, YZ
   Wang, WP
AF Hu, Yongtao
   Kautz, Jan
   Yu, Yizhou
   Wang, Wenping
TI Speaker-Following Video Subtitles
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Performance; Speaker detection; speaker-following
   subtitle placement; video viewing experience
ID RECOGNITION; TIME
AB We propose a new method for improving the presentation of subtitles in video (e.g., TV and movies). With conventional subtitles, the viewer has to constantly look away from the main viewing area to read the subtitles at the bottom of the screen, which disrupts the viewing experience and causes unnecessary eyestrain. Our method places on-screen subtitles next to the respective speakers to allow the viewer to follow the visual content while simultaneously reading the subtitles. We use novel identification algorithms to detect the speakers based on audio and visual information. Then the placement of the subtitles is determined using global optimization. A comprehensive usability study indicated that our subtitle placement method outperformed both conventional fixed-position subtitling and another previous dynamic subtitling method in terms of enhancing the overall viewing experience and reducing eyestrain.
C1 [Hu, Yongtao; Yu, Yizhou; Wang, Wenping] Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
   [Kautz, Jan] UCL, London WC1E 6BT, England.
C3 University of Hong Kong; University of London; University College London
RP Hu, YT (corresponding author), Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
EM herohuyongtao@gmail.com
CR Miro XA, 2012, IEEE T AUDIO SPEECH, V20, P356, DOI 10.1109/TASL.2011.2125954
   [Anonymous], 2003, ACM Multimedia Conference
   [Anonymous], 2004, COUPLING APPROACHES
   [Anonymous], 2006, BMVC
   Chun BK, 2006, LECT NOTES COMPUT SC, V4292, P576
   DIMOU A, 2005, P 5 EURASIP C SPEECH
   Driver J, 1996, NATURE, V381, P66, DOI 10.1038/381066a0
   Fernando WAC, 2001, ELECTRON COMMUN ENG, V13, P117, DOI 10.1049/ecej:20010302
   Glass J. R, 2004, P 6 INT C MULT INT A, P152, DOI DOI 10.1145/1027933.1027960
   Gordan M, 2002, EURASIP J APPL SIG P, V2002, P1248, DOI 10.1155/S1110865702207039
   Hong R., 2010, Proceedings of the 18th ACM international conference on Multimedia, P421
   Just Marcel., 1987, The Psychology of Reading and Language Comprehension
   Kuo CH, 2010, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2010.5540148
   Kurlander D., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P225, DOI 10.1145/237170.237260
   MCCONKIE GW, 1989, PERCEPT PSYCHOPHYS, V46, P245, DOI 10.3758/BF03208086
   Monaci G., 2011, P 19 EUR SIGN PROC C
   NOCK HJ, 2003, LECT NOTES COMPUTER, V2728, P565
   Park S.-H., 2008, P INT C GAM RES DEV
   Park SH, 2008, ICHIT 2008: INTERNATIONAL CONFERENCE ON CONVERGENCE AND HYBRID INFORMATION TECHNOLOGY, PROCEEDINGS, P119, DOI 10.1109/ICHIT.2008.186
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   RAYNER K, 1975, COGNITIVE PSYCHOL, V7, P65, DOI 10.1016/0010-0285(75)90005-5
   Saenko K, 2005, IEEE I CONF COMP VIS, P1424, DOI 10.1109/ICCV.2005.251
   Sethi I. K., 1995, Proceedings of the SPIE - The International Society for Optical Engineering, V2420, P329, DOI 10.1117/12.205299
   Uricar Michal, 2012, Proceedings of the International Conference on Computer Vision Theory and Applications. VISAPP 2012, P547
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wallace MT, 2004, EXP BRAIN RES, V158, P252, DOI 10.1007/s00221-004-1899-9
   Wikipedia, 2012, VIS SPAN
NR 27
TC 20
Z9 20
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2014
VL 11
IS 2
AR 32
DI 10.1145/2632111
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6DM
UT WOS:000348308800009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Stensland, HK
   Gaddam, VR
   Tennoe, M
   Helgedagsrud, E
   Naess, M
   Alstad, HK
   Mortensen, A
   Langseth, R
   Ljodal, S
   Landsverk, O
   Griwodz, C
   Halvorsen, P
   Stenhaug, M
   Johansen, D
AF Stensland, Hakon Kvale
   Gaddam, Vamsidhar Reddy
   Tennoe, Marius
   Helgedagsrud, Espen
   Naess, Mikkel
   Alstad, Henrik Kjus
   Mortensen, Asgeir
   Langseth, Ragnar
   Ljodal, Sigurd
   Landsverk, Oystein
   Griwodz, Carsten
   Halvorsen, Pal
   Stenhaug, Magnus
   Johansen, Dag
TI Bagadus: An Integrated Real-Time System for Soccer Analytics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Measurement; Performance; Real-time panorama video;
   system integration; camera array; sensor tracking; video annotation;
   sport analytics; soccer system
ID TRACKING; PEOPLE; PLAYERS
AB The importance of winning has increased the role of performance analysis in the sports industry, and this underscores how statistics and technology keep changing the way sports are played. Thus, this is a growing area of interest, both from a computer system view in managing the technical challenges and from a sport performance view in aiding the development of athletes. In this respect, Bagadus is a real-time prototype of a sports analytics application using soccer as a case study. Bagadus integrates a sensor system, a soccer analytics annotations system, and a video processing system using a video camera array. A prototype is currently installed at Alfheim Stadium in Norway, and in this article, we describe how the system can be used in real-time to playback events. The system supports both stitched panorama video and camera switching modes and creates video summaries based on queries to the sensor system. Moreover, we evaluate the system from a systems point of view, benchmarking different approaches, algorithms, and trade-offs, and show how the system runs in real time.
C1 [Stensland, Hakon Kvale; Gaddam, Vamsidhar Reddy; Tennoe, Marius; Helgedagsrud, Espen; Naess, Mikkel; Alstad, Henrik Kjus; Mortensen, Asgeir; Langseth, Ragnar; Ljodal, Sigurd; Landsverk, Oystein; Griwodz, Carsten; Halvorsen, Pal] Univ Oslo, N-0316 Oslo, Norway.
   [Stensland, Hakon Kvale; Gaddam, Vamsidhar Reddy; Tennoe, Marius; Helgedagsrud, Espen; Naess, Mikkel; Alstad, Henrik Kjus; Mortensen, Asgeir; Langseth, Ragnar; Ljodal, Sigurd; Landsverk, Oystein; Griwodz, Carsten; Halvorsen, Pal] Simula Res Lab, Oslo, Norway.
   [Stenhaug, Magnus; Johansen, Dag] Univ Tromso, N-9001 Tromso, Norway.
C3 University of Oslo; UiT The Arctic University of Tromso
RP Stensland, HK (corresponding author), Univ Oslo, N-0316 Oslo, Norway.
EM haakonks@ifi.uio.no
OI Halvorsen, Pal/0000-0003-2073-7029
FU iAD Centre for Research-Based Innovation [174867]; Norwegian Research
   Council
FX This work has been performed in the context of the iAD Centre for
   Research-Based Innovation (project number 174867) funded by the
   Norwegian Research Council.
CR Ali M. A., 2012, Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2012), P913, DOI 10.1109/ICASSP.2012.6288033
   [Anonymous], 2007, P IEEE C COMP VIS PA
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   Ben Shitrit H, 2011, IEEE I CONF COMP VIS, P137, DOI 10.1109/ICCV.2011.6126235
   Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Brutzer S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1937, DOI 10.1109/CVPR.2011.5995508
   Cairos Technologies, 2013, VIS TRACK
   Cairos Technologies, 2013, GOAL LIN TECHN GLT S
   Camargus, 2013, PREM STAD VID TECHN
   Chao-Ho Chen, 2011, Proceedings of the 2011 2nd International Conference on Innovations in Bio-Inspired Computing and Applications (IBICA 2011), P1, DOI 10.1109/IBICA.2011.5
   Dizikes Peter, 2013, SPORTS ANAL REAL GAM
   Fehn C, 2006, IEEE INT SYM MULTIM, P291
   Fuentes LM, 2006, IMAGE VISION COMPUT, V24, P1165, DOI 10.1016/j.imavis.2005.06.006
   Guthier B., 2012, Proceedings of the 2012 IEEE International Conference on Imaging Systems and Techniques (IST), P303, DOI 10.1109/IST.2012.6295505
   Halvorsen P., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, Protland, OR, P48
   Interplay Sports, 2013, ULT VID AN SCOUT SOF
   Iwase S, 2004, INT C PATT RECOG, P751, DOI 10.1109/ICPR.2004.1333881
   Jin H.L., 2008, 2008 IEEE C COMPUTER, P1
   Johansen D., 2012, 2012 Seventh International Conference on Digital Information Management (ICDIM 2012), P205, DOI 10.1109/ICDIM.2012.6360105
   Johansen D., 2009, P 17 ACM INT C MULTI, P989
   Jubiao Li, 2010, 2010 Second Pacific-Asia Conference on Circuits,Communications and System (PACCS 2010), P417, DOI 10.1109/PACCS.2010.5626602
   KaewTraKulPong P, 2002, VIDEO-BASED SURVEILLANCE SYSTEMS: COMPUTER VISION AND DISTRIBUTED PROCESSING, P135
   Kang Jinman., 2003, JOINT IEEE INT WORKS, P172
   Kao Wen-Chung, 2011, IEEE T INSTRUM MEAS, V60, P1206
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   McKeegan Noel, 2007, ADIDAS INTELLIGENT F
   Ozawa Tomohiro, 2012, P AUGMENTED HUMAN IN, P20
   Prozone, 2013, PROZ SPORTS INTR PRO
   Saegrov Simen, 2012, P 6 INT C DISTR SMAR
   Seo Y., 1997, International Conference on Image Analysis and Processing, P196, DOI DOI 10.1007/3-540-63508-4
   Siebel NT, 2002, LECT NOTES COMPUT SC, V2353, P373
   Stats, 2013, STATS SPORTVU FOOTB
   Valter DS., 2006, International Journal of Performance Analysis in Sport, V6, P108, DOI 10.1080/24748668.2006.11868359
   Xiong Y., 2009, Proc. Ist Int. Conf. Internet Multimedia Compute. Service, P219
   Xiong Yingen, 2010, IEEE T CONSUM ELECTR, V56, P2
   Xu M, 2004, IEEE IMAGE PROC, P2909
   Zhengyou Zhang, 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P666, DOI 10.1109/ICCV.1999.791289
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
   ZXY, 2013, ZXY SPORT TRACK
NR 41
TC 29
Z9 31
U1 0
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2014
VL 10
IS 1
SU S
SI SI
AR 14
DI 10.1145/2541011
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA2EF
UT WOS:000330907200006
DA 2024-07-18
ER

PT J
AU Swaminathan, V
AF Swaminathan, Viswanathan
TI Are We in the Middle of a Video Streaming Revolution?
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Video streaming; HTTP streaming; video delivery; multimedia
   systems
AB It has been roughly 20 years since the beginning of video streaming over the Internet. Until very recently, video streaming experiences left much to be desired. Over the last few years, this has significantly improved making monetization of streaming, possible. Recently, there has been an explosion of commercial video delivery services over the Internet, sometimes referred to as over-the-top (OTT) delivery. All these services invariably use streaming technologies. Initially, streaming had all the promise, then for a long time, it was download and play, later progressive download for short content, and now it is streaming again. Did streaming win the download versus streaming contest? Did the best technology win? The improvement in streaming experience has been possible through a variety of new streaming technologies, some proprietary and others extensions to standard protocols. The primary delivery mechanism for entertainment video, both premium content like movies and user generated content (UGC), tends to be HTTP streaming. Is HTTP streaming the panacea for all problems? The goal of this article is to give an industry perspective of what fundamentally changed in video streaming that makes it commercially viable now. This article outlines how a blend of technology choices between download and streaming makes the current wave of ubiquitous streaming possible for entertainment video delivery. After identifying problems that still need to be solved, the article concludes with the lessons learnt from the video streaming evolution.
RP Swaminathan, V (corresponding author), 345 Pk Ave, San Jose, CA 95110 USA.
EM vishy@adobe.com
CR [Anonymous], 2300912012 MPEG ISOI
   HDS, 2011, AD HTTP DYN STREAM
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   KUSCHNIG R., 2011, P 2 ANN ACM C MULT S, P200
   Muller C., 2012, 4th ACM Workshop on Mobile Video (MoVID), P37, DOI DOI 10.1145/2151677.2151686
   PANTOS R., 2013, APPLE HTTP LIVE STRE
   PARMAR H., 2012, REAL TIME TRANSPORT
   SANDVINE, 2013, 2H2012 SANDV GLOB IN
   Schulzrinne H., 2003, RTP TRANSPORT PROTOC
   Schulzrinne H., 1998, 2326 RFC
   SMOOTH, 2009, MICR SMOOTH STREAM
   Wang B, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1352012.1352020
   Xie BC, 2011, IEEE INT SYMP ELEC, P16, DOI 10.1109/ISEMC.2011.6038277
NR 13
TC 5
Z9 11
U1 1
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 40
DI 10.1145/2490826
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700010
DA 2024-07-18
ER

PT J
AU Yeh, LY
   Huang, JL
AF Yeh, Lo-Yao
   Huang, Jiun-Long
TI A Conditional Access System with Efficient Key Distribution and
   Revocation for Mobile Pay-TV Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Digital video broadcasting; nonrepudiation; multimedia
   delivery; sophisticated access control
AB Current mobile pay-TV systems have two types of Conditional Access Systems (CAS): group-key-based and public-key systems. The best feature of group-key-based systems is the ability to enjoy the broadcast nature in delivery multimedia contents, while the major advantage of public-key systems is consolidating the security foundation to withstand various attacks, such as collusion attacks. However, the problems of group-key-based systems include collusion attacks, lack of nonrepudiation, and troublesome key distribution. Even worse, the benefit of broadcast efficiency is confined to a group size of no more than 512 subscribers. For public-key systems, the poor delivery scalability is the major shortcoming because the unique private key feature is only suitable for one-to-one delivery. In this article, we introduce a scalable access control scheme to integrate the merits of broadcasting regardless of group size and sound security assurance, including fine-grained access control and collusion attack resistance. For subscriber revocation, a single message is broadcast to the other subscribers to get the updated key, thus significantly boosting subscriber revocation scalability. Due to mobile subscribers' dynamic movements, this article also analyzes the benefit of retransmission cases in our system. Through the performance evaluation and functionality comparison, the proposed scheme should be a decent candidate to enhance the security strength and transmission efficiency in a mobile pay-TV system.
C1 [Yeh, Lo-Yao; Huang, Jiun-Long] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Huang, JL (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
EM jlhuang@cs.nctu.edu.tw
FU National Science Council [NSC 101-2221-E-492-030, NSC
   101-2221-E-492-025]
FX This work was supported in part by the National Science Council under
   grant NSC 101-2221-E-492-030 and NSC 101-2221-E-492-025.
CR CONDITIONAL-ACCESS BROADCASTING SYSTEM, 1992, COND ACC BROADC SYST
   Daemen J, 2020, The design of rijndael: the advanced encryption standard (AES), V2nd, DOI DOI 10.1007/978-3-662-60769-53
   DIGITAL VIDEO BROADCASTING (DVB-H), 2004, 302304 DVBH ETSI
   DIGITAL VIDEO BROADCASTING (DVB-SH), 2007, 102585 DVBSH ETSI
   DIGITAL VIDEO BROADCASTING (DVB-SPP), 2007, 102474 DVBSPP ETSI
   FIAT A., 1994, P 13 ANN INT CRYPT C
   Goyal V., 2006, P 2006 INT C PRIVACY, P1
   Huang YL, 2004, IEEE T MULTIMEDIA, V6, P760, DOI 10.1109/TMM.2004.834861
   Qi Q, 2010, IEEE T CONSUM ELECTR, V56, P2276, DOI 10.1109/TCE.2010.5681100
   Roh H, 2011, IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE 2011), P717, DOI 10.1109/ICCE.2011.5722826
   Sahai A, 2005, LECT NOTES COMPUT SC, V3494, P457, DOI 10.1007/11426639_27
   Staddon J., 2002, P IEEE S SEC PRIV
   Sun HM, 2008, IEEE T MULTIMEDIA, V10, P1109, DOI 10.1109/TMM.2008.2001381
   Sun HM, 2009, IEEE T MULTIMEDIA, V11, P947, DOI 10.1109/TMM.2009.2021790
   Waldvogel M, 1999, IEEE J SEL AREA COMM, V17, P1614, DOI 10.1109/49.790485
   Wang SY, 2008, IEEE T MULTIMEDIA, V10, P480, DOI 10.1109/TMM.2008.917417
   Yeung SF, 2005, IEEE T MULTIMEDIA, V7, P330, DOI 10.1109/TMM.2005.843361
   Yu SC, 2011, IEEE T PARALL DISTR, V22, P673, DOI 10.1109/TPDS.2010.130
   Zhu HJ, 2009, IEEE T VEH TECHNOL, V58, P2529, DOI 10.1109/TVT.2008.2007983
   Zhu WT, 2008, IEEE T MULTIMEDIA, V10, P1214, DOI 10.1109/TMM.2008.2001376
NR 20
TC 6
Z9 6
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2013
VL 9
IS 3
AR 18
DI 10.1145/2487268.2487271
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 175GL
UT WOS:000321218800003
DA 2024-07-18
ER

PT J
AU Wu, C
   Li, BC
   Zhao, SQ
AF Wu, Chuan
   Li, Baochun
   Zhao, Shuqiao
TI Diagnosing Network-Wide P2P Live Streaming Inefficiencies
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurement; Peer-to-peer streaming; streaming inefficiency
AB Large-scale live peer-to-peer (P2P) streaming applications have been successfully deployed in today's Internet. While they can accommodate hundreds of thousands of users simultaneously with hundreds of channels of programming, there still commonly exist channels and times where and when the streaming quality is unsatisfactory. In this paper, based on more than two terabytes and one year worth of live traces from UUSee, a large-scale commercial P2P live streaming system, we show an in-depth network-wide diagnosis of streaming inefficiencies, commonly present in typical mesh-based P2P live streaming systems. As the first highlight of our work, we identify an evolutionary pattern of low streaming quality in the system, and the distribution of streaming inefficiencies across various streaming channels and in different geographical regions. We then carry out an extensive investigation to explore the causes to such streaming inefficiencies over different times and across different channels/regions at specific times, by investigating the impact of factors such as the number of peers, peer upload bandwidth, inter-peer bandwidth availability, server bandwidth consumption, and many more. The original discoveries we have brought forward include the two-sided effects of peer population on the streaming quality in a streaming channel, the significant impact of inter-peer bandwidth bottlenecks at peak times, and the inefficient utilization of server capacities across concurrent channels. Based on these insights, we identify problems within the existing P2P live streaming design and discuss a number of suggestions to improve real-world streaming protocols operating at a large scale.
C1 [Wu, Chuan] Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Li, Baochun] Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 1A1, Canada.
   [Zhao, Shuqiao] UUSee Inc, Beijing, Peoples R China.
C3 University of Hong Kong; University of Toronto
RP Wu, C (corresponding author), Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM cwu@cs.hku.hk; bli@eecg.toronto.edu; zhaosq@uusee.com
RI baochun, Li/AAD-3188-2022; Wu, Chuan/E-9919-2010
CR Agarwal S., 2008, P 16 INT WORKSH QUAL
   ALESSANDRIA E., 2009, P ANN JOINT C IEEE C
   [Anonymous], [No title captured]
   [Anonymous], P WORKSH REC ADV PEE
   CHEN C., 2007, P IFIP INT C NETW PA
   Gao P., 2008, P 2 INT C NETW GRID
   Hei X., 2006, P WORKSH INT PROT TV
   Hei XJ, 2007, IEEE J SEL AREA COMM, V25, P1640, DOI 10.1109/JSAC.2007.071204
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Li B, 2007, IEEE J SEL AREA COMM, V25, P1627, DOI 10.1109/JSAC.2007.071203
   SILVERSTON T, 2006, P 2 C FUT NETW TECHN
   Silverston T., 2007, P 17 INT WORKSH NETW
   WU C., 2009, P ANN JOINT C IEEE C
   Wu C, 2007, IEEE J SEL AREA COMM, V25, P1612, DOI 10.1109/JSAC.2007.071202
   Yue Lu, 2009, International Journal of Internet Protocol Technology, V4, P11, DOI 10.1504/IJIPT.2009.024166
   Zhou FF, 2008, PROCEEDINGS OF THE 11TH JOINT CONFERENCE ON INFORMATION SCIENCES
NR 16
TC 3
Z9 3
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2012
VL 8
IS 1
SI SI
AR 13
DI 10.1145/2089085.2089090
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 898YN
UT WOS:000300778400005
OA Green Published
DA 2024-07-18
ER

PT J
AU Kuo, WK
   Wu, KW
AF Kuo, Wen-Kuang
   Wu, Kuo-Wei
TI Traffic Prediction and QoS Transmission of Real-Time Live VBR Videos in
   WLANs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Performance; VBR video; traffic prediction; WLAN
ID MPEG-4; TRACES; SIZE
AB As the demand for broadband multimedia wireless services is increasing, improving quality of service (QoS) of the widely deployed IEEE 802.11 wireless LANs (WLANs) has become crucial. To support the QoS required by a wide range of applications, the IEEE 802.11 working group has defined a new standard-the IEEE 802.11e. Substantial studies have been performed on traffic scheduling for variable bit rate (VBR) video transport over 802.11e WLANs. However, within those studies, relatively little attention has been devoted to the QoS transmission of real-time live VBR videos. In this paper, we present a novel traffic scheduling algorithm for IEEE 802.11e that aims at achieving high channel utilization while still guaranteeing QoS requirements for real-time live VBR videos. The novel characteristic of this algorithm, compared to published literatures, is that it predicts the bandwidth requirements for future traffic using a novel traffic predictor designed to provide simple yet accurate online prediction. Analyses using real life MPEG video traces indicate that the proposed traffic predictor significantly outperforms previously published technique with respect to the prediction error. The proposed traffic predictor can also be used independently to estimate any MPEG traffic. The performance of the proposed traffic scheduling algorithm is also investigated by comparing several existing scheduling algorithms. Simulation results demonstrate that the proposed traffic scheduling algorithm surpasses other mechanisms in terms of channel utilization, buffer usage, video quality and packet loss rate.
C1 [Kuo, Wen-Kuang; Wu, Kuo-Wei] Natl Cheng Kung Univ, Inst Comp & Commun Engn, Dept Elect & Engn, Tainan 70101, Taiwan.
C3 National Cheng Kung University
RP Kuo, WK (corresponding author), Natl Cheng Kung Univ, Inst Comp & Commun Engn, Dept Elect & Engn, Tainan 70101, Taiwan.
EM wkuo@ee.ncku.edu.tw; n2893122@mail.ncku.edu.tw
CR Abdennour A, 2006, IEEE T BROADCAST, V52, P184, DOI 10.1109/TBC.2006.872994
   Adas AM, 1998, IEEE ACM T NETWORK, V6, P635, DOI 10.1109/90.731200
   [Anonymous], 2005, 80211E IEEE, V802, P11
   [Anonymous], 2006, VIDEO TRACES NETWORK
   [Anonymous], 2016, IEEE Standard 802.11-2020
   Fitzek FHP, 2001, IEEE NETWORK, V15, P40, DOI 10.1109/65.967596
   Higuchi Y, 2007, IEEE WCNC, P2082
   *IEEE, 1999, 80211 IEEE S
   Ikkurthy P, 2002, C LOCAL COMPUT NETW, P421, DOI 10.1109/LCN.2002.1181814
   KRUNZ K, 1997, P ACM SIGM INT C MEA, P192
   KWONG RH, 1992, IEEE T SIGNAL PROCES, V40, P1633, DOI 10.1109/78.143435
   Lagkas TD, 2007, IEEE T VEH TECHNOL, V56, P1761, DOI 10.1109/TVT.2007.897223
   Lie A, 2008, MULTIMEDIA SYST, V14, P33, DOI 10.1007/s00530-007-0110-0
   Lim LW, 2004, CCNC 2004: 1ST IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE, PROCEEDINGS, P199, DOI 10.1109/CCNC.2004.1286858
   Moussa N, 2006, IEEE DTIS: 2006 INTERNATIONAL CONFERENCE ON DESIGN & TEST OF INTEGRATED SYSTEMS IN NANOSCALE TECHNOLOGY, PROCEEDINGS, P341, DOI 10.1109/DTIS.2006.1708711
   N SS, 2007, IEEE T VEH TECHNOL, V56, P2346, DOI 10.1109/TVT.2007.897646
   Narasimha R, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON PERSONAL WIRELESS COMMUNICATIONS, P290, DOI 10.1109/ICPWC.2002.1177295
   Seeling P, 2004, IEEE COMMUN SURV TUT, V6, P58, DOI 10.1109/COMST.2004.5342293
   Skyrianoglou D, 2006, IEEE T WIREL COMMUN, V5, P3558, DOI [10.1109/TWC.2006.256978, 10.1109/TWC.2006.04669]
   Tseng YH, 2007, IEEE T MULTIMEDIA, V9, P642, DOI 10.1109/TMM.2006.888019
   van der Schaar M, 2006, IEEE T MOBILE COMPUT, V5, P755, DOI 10.1109/TMC.2006.81
   Yoo SJ, 2002, IEEE T BROADCAST, V48, P10, DOI 10.1109/11.992849
   Zhao L, 2002, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P54, DOI 10.1109/ITCC.2002.1000359
   2011, ACM T MULTIMEDIA COM, V7
NR 24
TC 5
Z9 6
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2011
VL 7
IS 4
AR 36
DI 10.1145/2043612.2043614
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 856ZS
UT WOS:000297684000002
DA 2024-07-18
ER

PT J
AU Yin, H
   Liu, XN
   Zhan, TY
   Sekar, V
   Qiu, F
   Lin, CA
   Zhang, H
   Li, B
AF Yin, Hao
   Liu, Xuening
   Zhan, Tongyu
   Sekar, Vyas
   Qiu, Feng
   Lin, Chuang
   Zhang, Hui
   Li, Bo
TI LiveSky: Enhancing CDN with P2P
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Measurement; Performance; Content delivery networks;
   peer-to-peer; live streaming
ID PEER-TO-PEER
AB We present the design and deployment experiences with LiveSky, a commercial hybrid CDN-P2P live streaming system, which inherits the best of both CDN and P2P. We address several key challenges, including: 1) ease of integration with existing CDN infrastructure, 2) dynamic resource scaling while guaranteeing quality-of-service, 3) providing good user experience, ensuring network friendliness and upload fairness. LiveSky has been used for several large-scale live streaming events in China. Our evaluation results from real-world indicate that such a hybrid CDN-P2P system provides quality and performance comparable to a CDN and effectively scales the system capacity.
C1 [Yin, Hao; Liu, Xuening; Zhan, Tongyu; Lin, Chuang] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Sekar, Vyas; Zhang, Hui] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
   [Qiu, Feng] Beijing Blue IT Technol Co Ltd, Dept Res & Dev, Beijing, Peoples R China.
   [Li, Bo] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Tsinghua University; Carnegie Mellon University; Hong Kong University of
   Science & Technology
RP Yin, H (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM h-yin@tsinghua.edu.cn
RI Li, Bo/AAA-8968-2020; Wang, Zejun/KBB-8454-2024
OI Li, Bo/0000-0002-7294-6888; Sun, Liangliang/0000-0002-9533-098X
FU NSFC [60873254]; China Postdoctoral Science Foundation [20090460317];
   Tsinghua-ChinaCache CDN Research Institute; NSF [ANI-0331653]; RGC
   [615608, 616207]; NSFC/RGC [N_HKUST603/07]
FX This work was funded by Project 60873254 supported by NSFC, Project
   20090460317 supported by China Postdoctoral Science Foundation, and the
   Tsinghua-ChinaCache CDN Research Institute Project. V. Sekar and H.
   Zhang were supported in part by NSF award ANI-0331653. B. Li's research
   was supported in part by grants from RGC under the contracts 615608 and
   616207, and by a grant from NSFC/RGC under the contract N_HKUST603/07.
CR Ali S., 2006, P INT WORKSH REC ADV
   [Anonymous], 2020, MARACAIBO
   [Anonymous], 2007, CHEMIA
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   Choffnes DR, 2008, ACM SIGCOMM COMP COM, V38, P363, DOI 10.1145/1402946.1403000
   Chu YH, 2000, PERF E R SI, V28, P1, DOI 10.1145/345063.339337
   DABEK F, 2004, P 2004 C APPL TECHN, P15
   Darlagiannis V, 2007, MULTIMEDIA SYST, V13, P19, DOI 10.1007/s00530-007-0078-9
   Dischinger M, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P3
   GANNES L, 2009, OBAMA INAUGRATION LI
   HEI X, 2009, IEEE T MULTIMEDIA, V8, P1672
   Huang C., 2008, P NOSSDAV C BRAUNSCH, P75
   HUANG G, 2007, P ACM SIGCOMM PEER T
   Huang Y, 2008, ACM SIGCOMM COMP COM, V38, P375, DOI 10.1145/1402946.1403001
   KIRKPATRICK M, 2008, P ANN JOINT C IEEE C
   Li B, 2007, IEEE J SEL AREA COMM, V25, P1627, DOI 10.1109/JSAC.2007.071203
   Liu B, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON VEHICULAR ELECTRONICS AND SAFETY, PROCEEDINGS, P49
   LIU F, 2009, P INT WORKSH PEER TO
   Liu JC, 2008, P IEEE, V96, P11, DOI 10.1109/JPROC.2007.909921
   Pakkala D., 2005, P 14 IST MOB WIR COM
   RODRIGUEZ P, 1936, ACM SIGCOMM COMMUNIC, V36, P75
   Rosenberg J., 2003, RFC 3489, DOI DOI 10.17487/RFC3489
   Small T, 2007, IEEE J SEL AREA COMM, V25, P35, DOI 10.1109/JSAC.2007.070105
   Thorup M, 2005, J ACM, V52, P1, DOI 10.1145/1044731.1044732
   VANCE A, 2009, NEWS SITES STRUGGLE
   Venkataraman V, 2006, PROCEEDINGS OF THE 2006 IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, P2, DOI 10.1109/ICNP.2006.320193
   Wu CA, 2009, IEEE INFOCOM SER, P2731, DOI 10.1109/INFCOM.2009.5062221
   Xie HY, 2008, ACM SIGCOMM COMP COM, V38, P351, DOI 10.1145/1402946.1402999
   Xie S, 2007, IEEE T MULTIMEDIA, V9, P1661, DOI 10.1109/TMM.2007.907469
   XU D, 2011, MULTIMEDIA SYST, V11, P383
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   Zhou FF, 2008, PROCEEDINGS OF THE 11TH JOINT CONFERENCE ON INFORMATION SCIENCES
NR 32
TC 38
Z9 41
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2010
VL 6
IS 3
SI SI
AR 16
DI 10.1145/1823746.1823750
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 649UL
UT WOS:000281799300005
DA 2024-07-18
ER

PT J
AU Wu, JW
   Trivedi, MM
AF Wu, Junwen
   Trivedi, Mohan M.
TI An Eye Localization, Tracking and Blink Pattern Recognition System:
   Algorithm and Evaluation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Verification; Eye blink
   detection; human computer interface; particle filtering; video
   processing
ID REAL-TIME EYE
AB This study is to investigate the fundamental problems of, (1) facial feature detection and localization, especially eye features; and (2) eye dynamics, including tracking and blink detection. We first describe our contribution to eye localization. Following that, we discuss a simultaneous eye tracking and blink detection system. Facial feature detection is solved in a general object detection framework and its performance for eye localization is presented. A binary tree representation based on feature dependency partitions the object feature space in a coarse to fine manner. In each compact feature subspace, independent component analysis (ICA) is used to get the independent sources, whose probability density functions (PDFs) are modeled by Gaussian mixtures. When applying this representation for the task of eye detection, a subwindow is used to scan the entire image and each obtained image patch is examined using Bayesian criteria to determine the presence of an eye subject. After the eyes are automatically located with binary tree-based probability learning, interactive particle filters are used for simultaneously tracking the eyes and detecting the blinks. The particle filters use classification-based observation models, in which the posterior probabilities are evaluated by logistic regressions in tensor subspaces. Extensive experiments are used to evaluate the performance from two aspects, (1) blink detection rate and the accuracy of blink duration in terms of the frame numbers; (2) eye tracking accuracy. We also present an experimental setup for obtaining the benchmark data in tracking accuracy evaluation. The experimental evaluation demonstrates the capability of this approach.
C1 [Wu, Junwen; Trivedi, Mohan M.] Univ Calif San Diego, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Wu, JW (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.
EM juwu@ucsd.edu; mtrivedi@ucsd.edu
RI Wu, Junwen/KOC-5338-2024
CR [Anonymous], 1961, Adaptive control processes: a guided tour, DOI DOI 10.1515/9781400874668
   [Anonymous], P EUR C COMP VIS
   [Anonymous], 2004, P IEEE C COMP VIS PA
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Chang C, 2005, IEEE SIGNAL PROC LET, V12, P242, DOI 10.1109/LSP.2004.842254
   Cohn JF, 2003, BEHAV RES METH INS C, V35, P420, DOI 10.3758/BF03195519
   Duda R., 1973, Pattern Classification and Scene Analysis
   FLETCHER L, 2003, IEEE INTELL SYST
   Ghinea G, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314304
   GORODNICHY D, 2003, P INT ASS SCI TECHN
   Grauman Kristen, 2003, Universal Access in the Information Society, V2, P359, DOI [10.1007/s10209-003-0062-x, DOI 10.1007/s10209-003-0062-x]
   Guan YT, 2006, STAT COMPUT, V16, P193, DOI 10.1007/s11222-006-6966-6
   Gustafsson F, 2002, IEEE T SIGNAL PROCES, V50, P425, DOI 10.1109/78.978396
   HE X, 2005, P C NEUR INF PROC SY
   He X., 2005, IEEE T PATT ANAL MAC, V27
   Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696, DOI 10.1109/34.1000242
   HUANG K, 2001, P 4 IEEE INT WORKSH
   HUANG K, 2003, P 1 INT WORKSH IN VE
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Ji Q, 2002, REAL-TIME IMAGING, V8, P357, DOI 10.1006/rtim.2002.0279
   KOJIMA N, 2001, P IEEE INT VEH EL C
   LEE MW, 2002, P IEEE WORKSH MOT VI
   MORIYAMA T, 2002, P INT C PATT REC ICP
   Morris T, 2002, J NETW COMPUT APPL, V25, P129, DOI 10.1006/jnca.2002.0130
   NGUYEN K, 2002, P EYE TRACK RES APPL
   Nishiyama K, 2005, SIGNAL PROCESS, V85, P2412, DOI 10.1016/j.sigpro.2005.07.030
   ONOE Y, 1998, P 14 INT C PATT REC
   Pantic M., 2006, PROC MULTIMODAL INTE, P239
   PHILLIPS PJ, 2000, IEEE T PATT ANAL MAC, V22
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   RUI Y, 2001, P IEEE COMP SOC C CO
   RURAINSKY, 2003, P INT WORKSH VER LOW
   SHEN C, 2005, P IEEE INT C IM PROC
   SMITH P, 2000, P 15 IEEE INT C PATT
   Trivedi MM, 2005, IEEE T SYST MAN CY A, V35, P145, DOI 10.1109/TSMCA.2004.838480
   Vasilescu M Alex O, 2002, P EUR C COMP VIS
   Veeraraghavan H., 2001, Detecting driver fatigue through the use of advanced face monitoring techniques
   WANG P, 2005, P IEEE WORKSH FAC RE
   Welch Greg., 2004, INTRO KALMAN FILTER
   Zhu SC, 2003, IEEE T PATTERN ANAL, V25, P691, DOI 10.1109/TPAMI.2003.1201820
NR 40
TC 6
Z9 8
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR 10
PY 2010
VL 6
IS 2
AR 8
DI 10.1145/1671962.1671964
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 579PA
UT WOS:000276382700002
DA 2024-07-18
ER

PT J
AU Ivanov, YV
   Bleakley, CJ
AF Ivanov, Yuri V.
   Bleakley, C. J.
TI Real-Time H.264 Video Encoding in Software with Fast Mode Decision and
   Dynamic Complexity Control
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; H/264/AVC; rate distortion; real time; mode decision; fast
   mode decision; complexity; complexity control
ID OPTIMIZED MOTION ESTIMATION; ALLOCATION
AB This article presents a novel real-time algorithm for reducing and dynamically controlling the computational complexity of an H.264 video encoder implemented in software. A fast mode decision algorithm, based on a Pareto-optimal macroblock classification scheme, is combined with a dynamic complexity control algorithm that adjusts the MB class decisions such that a constant frame rate is achieved. The average coding efficiency of the proposed algorithm was found to be similar to that of conventional encoding operating at half the frame rate. The proposed algorithm was found to provide lower average bitrate and distortion than static complexity scaling.
C1 [Bleakley, C. J.] Univ Coll Dublin, Sch Informat & Comp Sci, Dublin 4, Ireland.
   [Ivanov, Yuri V.] Movidius Ltd, Dublin 1, Ireland.
C3 University College Dublin
RP Bleakley, CJ (corresponding author), Univ Coll Dublin, Sch Informat & Comp Sci, Dublin 4, Ireland.
EM chris.bleakley@ucd.ie
RI Bleakley, Chris/H-4056-2019
OI Bleakley, Chris/0000-0001-8149-9015
FU University College Dublin
FX This research was supported by University College Dublin.
CR AHHAD A, 2004, P IEEE INT C AC SPEE, V3, P173
   Akyol E, 2007, IEEE IMAGE PROC, P77
   [Anonymous], 2001, ITU T VCEG M
   Ates HF, 2008, IEEE T CIRC SYST VID, V18, P159, DOI 10.1109/TCSVT.2008.918114
   Berger G, 2007, IEEE T BROADCAST, V53, P584, DOI 10.1109/TBC.2007.896960
   CHANG A, 2001, P IEEE INT S CIRC SY, V3, P817
   DEEPAK S, 2001, IEEE T MULTIMEDIA, V3, P41
   EVERETT H, 1963, OPER RES, V11, P399, DOI 10.1287/opre.11.3.399
   FENG B, 2006, P IEEE 3 INT C CONS, V2, P745
   Goto K, 2006, IEEE WRK SIG PRO SYS, P101, DOI 10.1109/SIPS.2006.352563
   Han KH, 2004, TENCON IEEE REGION, pA347
   Hong Z, 2005, 19TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS, VOL 1, PROCEEDINGS, P775
   HSU KW, 2005, P 48 S CIRC SYST, P1489
   Hu Y, 2006, 2006 IEEE International Conference on Multimedia and Expo - ICME 2006, Vols 1-5, Proceedings, P1949, DOI 10.1109/ICME.2006.262939
   *JVT, 1950, JVTG050D35 ITUT H264
   Kaminsky E, 2008, J VIS COMMUN IMAGE R, V19, P56, DOI 10.1016/j.jvcir.2007.05.002
   Kannangara CS, 2006, IEEE T CIRC SYST VID, V16, P202, DOI 10.1109/TCSVT.2005.859026
   KANT S, 2006, PAPERS INT C CONSUME, P93
   Kim C, 2005, IEEE INT SYMP CIRC S, P308
   Kossentini F, 1997, IEEE J SEL AREA COMM, V15, P1752, DOI 10.1109/49.650048
   Kuo TY, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P675, DOI 10.1109/ICME.2004.1394282
   Li GL, 2005, IEEE INT SYMP CIRC S, P5481
   SAPONARA S, 2002, ISOIECJCT1SC29WG1198
   Shen JS, 2007, IEEE INT SYM MULTIM, P427, DOI 10.1109/ISM.Workshops.2007.78
   Stottrup-Andersen J, 2004, IEEE IMAGE PROC, P111
   Wang CN, 2004, IEEE T CIRC SYST VID, V14, P429, DOI 10.1109/TCSVT.2004.825550
   WANG H, 2005, P INT C CONTR AUT IC, P1040
   WE Z, 2006, P IEEE INT S CIRC SY, P4
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WU M, 2007, P SPIE, V6508
   YIN P, 2004, P IEEE INT C IM PROC, P853
   Yu AC, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P169
   Zhao YF, 2003, SIGNAL PROCESS-IMAGE, V18, P801, DOI 10.1016/S0923-5965(03)00072-9
NR 33
TC 10
Z9 13
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2010
VL 6
IS 1
AR 5
DI 10.1145/1671954.1671959
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 563VL
UT WOS:000275163200005
OA Green Published
DA 2024-07-18
ER

PT J
AU Komogortsev, OV
   Khan, JI
AF Komogortsev, Oleg V.
   Khan, Javed I.
TI Predictive real-time perceptual compression based on eye-gaze-position
   analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 12th ACM Multimedia Conference
CY OCT 10-16, 2004
CL New York, NY
SP ACM
DE algorithms; performance; design; reliability; experimentation; human
   factors; verification; real-time multimedia compression; human visual
   system
ID VIDEO COMPRESSION; IMAGE; SYSTEM
AB This article designs a real-time perceptual compression system (RTPCS) based on eye-gaze-position analysis. Our results indicate that the eye-gaze-position containment metric provides more efficient and effective evaluation of an RTPCS than the eye fixation containment. The presented RTPCS is designed for a network communication scenario with a feedback loop delay. The proposed RTPCS uses human visual system properties to compensate for the delay and to provide high ratios of multimedia compression.
C1 [Komogortsev, Oleg V.] Texas State Univ San Marcos, Dept Comp Sci, San Marcos, TX 78666 USA.
   [Khan, Javed I.] Kent State Univ, Dept Comp Sci, Kent, OH 44242 USA.
C3 Texas State University System; Texas State University San Marcos;
   University System of Ohio; Kent State University; Kent State University
   Kent; Kent State University Salem
RP Komogortsev, OV (corresponding author), Texas State Univ San Marcos, Dept Comp Sci, 601 Univ Dr, San Marcos, TX 78666 USA.
EM ok11@txstate.edu; javed@kent.edu
RI Khan, Javed/AAJ-3455-2021
CR [Anonymous], EYE MOVEMENTS VISION
   Babcock JS, 2003, P SOC PHOTO-OPT INS, V5007, P218, DOI 10.1117/12.477770
   Cormen T.H., 1990, Introduction to Algorithms
   Daly S, 1998, P SOC PHOTO-OPT INS, V3299, P180, DOI 10.1117/12.320110
   Daly S, 2001, J ELECTRON IMAGING, V10, P30, DOI 10.1117/1.1333679
   Duchowski A., 2003, Eye Tracking Methodology: Theory and Practice
   DUCHOWSKI AT, 1995, P SOC PHOTO-OPT INS, V2411, P128, DOI 10.1117/12.207556
   Duchowski AT, 2000, IEEE T IMAGE PROCESS, V9, P1437, DOI 10.1109/83.855439
   Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120
   Irwin D.E., 1992, Eye movements and visual cognition: Scene perception and reading, P146, DOI [10.1007/978-1-4612-2852-3_9, DOI 10.1007/978-1-4612-2852-3_9]
   Khan JI, 2002, DARPA ACTIVE NETWORKS CONFERENCE AND EXPOSITION, PROCEEDINGS, P409, DOI 10.1109/DANCE.2002.1003511
   KOMOGORTSEV O, 2006, P 2006 S EYE TRACK R, P101, DOI DOI 10.1145/1117309
   Komogortsev Oleg., 2004, Proc. of the 12th Annual ACM Int. Conf. on Multimedia, P220, DOI [10.1145/1027527.1027577, DOI 10.1145/1027527.1027577]
   Kortum P, 1996, P SOC PHOTO-OPT INS, V2657, P350, DOI 10.1117/12.238732
   Kuyel T, 1998, P SOC PHOTO-OPT INS, V3299, P603, DOI 10.1117/12.320151
   Lee S, 2001, IEEE T IMAGE PROCESS, V10, P977, DOI 10.1109/83.931092
   Peters R. J., 2006, Proceedings. ETRA 2006. Symposium on Eye Tracking Research and Applications, P27, DOI 10.1145/1117309.1117315
   Shebilske W.L., 1983, Eye Movements and Psychological Functions: International Views, P303
   STELMACH LB, 1994, P SOC PHOTO-OPT INS, V2179, P90, DOI 10.1117/12.172660
   Yanoff M., 2009, Ophthalmology, V3rd
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 27
TC 5
Z9 5
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2008
VL 4
IS 3
AR 23
DI 10.1145/1386109.1386116
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 351OI
UT WOS:000259433300007
DA 2024-07-18
ER

PT J
AU Jung, B
   Song, J
   Lee, Y
AF Jung, Byunghee
   Song, Junehwa
   Lee, Yoonjoon
TI A narrative-based abstraction framework for story-oriented video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE design; human factors; video abstraction; narrative structure; film;
   story-oriented; story understanding; video abstraction system; online
   review services
ID SEGMENTATION
AB This article proposes a novel video abstraction framework for online review services of story-oriented videos such as dramas. Among the many genres of TV programs, a drama is one of the most popularly watched on the Web. The abstracts generated by the proposed framework not only give a summary of a video but also effectively help viewers understand the overall story. In addition, our method is duration-flexible. We get clues about human understanding of a story from scenario writing rules and editorial techniques that are popularly used in the process of video production to explicitly express a narrative, and propose a new video abstraction model, called a Narrative Abstraction Model. The model effectively captures the narrative structure embedded in a story-oriented video and articulates the progress of the story in a weighted directed graph, called a Narrative Structure Graph (NSG). The model provides a basis for a flexible framework for abstract generation using the NSG as the. intermediary representation of a video. Different abstracts can be appropriately generated based upon different user requirements. To show the effectiveness of the proposed model and method, we developed a video abstraction system realizing the framework, and successfully applied it to large volumes of TV dramas. The evaluation results show that the proposed framework is a feasible solution for online review services.
C1 Korea Adv Inst Sci & Technol, Dept Elect Engn & Comp Sci, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Jung, B (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn & Comp Sci, 373-1 Guseong Dong, Taejon 305701, South Korea.
EM bhjung@dbserver.kaist.ac.kr; junesong@cs.kaist.ac.kr;
   yjlee@cs.kaist.ac.kr
RI Song, Junehwa/CAJ-3787-2022; Song, Junehwa/C-1575-2011; Lee, Yoon
   Joon/C-1881-2011
OI Song, Junehwa/0000-0002-7941-6636; 
CR Adams B, 2002, IEEE T MULTIMEDIA, V4, P472, DOI 10.1109/TMM.2002.802016
   [Anonymous], 1995, PROC ICJAI, DOI DOI 10.1145/217279.215068
   [Anonymous], 2018, WORKSHOP IIPHDW
   [Anonymous], 1991, Grammar of the film language
   ARAI H, 1987, FUNDAMENTAL TECHNIQU
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   Bordwell David., 1996, Film Art: An Introduction
   Boreczky JS, 1996, P SOC PHOTO-OPT INS, V2670, P170, DOI 10.1117/12.234794
   BRANIGAN Edward., 1992, NARRATIVE COMPREHENS
   Brooks K. M., 1996, Proceedings ACM Multimedia 96, P317, DOI 10.1145/244130.244233
   Chatman, 1978, STORY DISCOURSE NARR
   Hanjalic A, 2001, J ELECTRON IMAGING, V10, P871, DOI 10.1117/1.1406506
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   He LW, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P489, DOI 10.1145/319463.319691
   Jung B, 2004, 12 ANN ACM INT C MUL, P828, DOI [10.1145/1027527.1027720, DOI 10.1145/1027527.1027720]
   JUNG B, 2004, P INT WORKSH IM AN M
   *KBS, KBS ACC STAT
   Kender JR, 1998, PROC CVPR IEEE, P367, DOI 10.1109/CVPR.1998.698632
   Kupiec Julian, 1995, P 18 ANN INT ACM SIG, P68, DOI DOI 10.1145/215206.215333
   Lienhart R, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P685, DOI 10.1109/MMCS.1999.779282
   MENG JH, 1995, P SOC PHOTO-OPT INS, V2419, P14, DOI 10.1117/12.206359
   NANG J, 2002, P 3 IEEE PAC RIM C M, P311
   Omoigui N., 1999, P SIGCHI C HUMAN FAC, P136
   Patel NV, 1996, P SOC PHOTO-OPT INS, V2670, P373, DOI 10.1117/12.234776
   Pfeiffer S, 1996, J VIS COMMUN IMAGE R, V7, P345, DOI 10.1006/jvci.1996.0030
   Sharff Stefan., 1982, The Elements of Cinema: Toward a Theory of Cinesthetic Impact
   Smith MA, 1997, PROC CVPR IEEE, P775, DOI 10.1109/CVPR.1997.609414
   Song J, 1999, IEEE T CIRC SYST VID, V9, P1100, DOI 10.1109/76.795061
   Song JH, 2000, IEEE T CIRC SYST VID, V10, P767, DOI 10.1109/76.856453
   Sundaram H, 2002, IEEE T MULTIMEDIA, V4, P482, DOI 10.1109/TMM.2002.802017
   Sundaram Hari., 2002, COMUNICA O APRESENTA, DOI [DOI 10.1145/641007.641042, 10.1145/641007.641042]
   Syeda-Mahmood T., 2001, PROC ACM MULITMEDIA, V9, P119, DOI DOI 10.1145/500141.500161
   Uchihashi S, 1999, INT CONF ACOUST SPEE, P3041, DOI 10.1109/ICASSP.1999.757482
   Vasconcelos N, 1998, PROC CVPR IEEE, P361, DOI 10.1109/CVPR.1998.698631
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   YEO BL, 1995, P INT C MULT COMP SY, P81
   YEO BL, 1995, P IEEE INT C IM PROC, V2, P260
   Yeung M, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P296, DOI 10.1109/MMCS.1996.534991
   Yeung MM, 1997, IEEE T CIRC SYST VID, V7, P771, DOI 10.1109/76.633496
   YEUNG MM, 1995, P SOC PHOTO-OPT INS, V2417, P399, DOI 10.1117/12.206067
   Yoshitaka A, 1997, IEEE VISLANG, P310, DOI 10.1109/VL.1997.626599
NR 42
TC 11
Z9 12
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 2
AR 11
DI 10.1145/1230812.1230817
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230JA
UT WOS:000250871600005
DA 2024-07-18
ER

PT J
AU Xu, H
   Chua, TS
AF Xu, Huaxin
   Chua, Tat-Seng
TI Fusion of AV features and external information sources for event
   detection in team sports video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; design; experimentation; sports video; semantic; event
   detection; event modeling
AB The use of AV features alone is insufficient to induce high-level semantics. This article proposes a framework that utilizes both internal AV features and various types of external information sources for event detection in team sports video. Three schemes are also proposed to tackle the asynchronism between the fusion of AV and external information. The framework is extensible as it can provide increasing functionalities given more detailed external information and domain knowledge. By demonstrating its effectiveness on soccer and American football, we believe that with the availability of appropriate domain knowledge, the framework is applicable to other team sports.
C1 Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
C3 National University of Singapore
RP Xu, H (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
EM xuhuaxin@comp.nus.edu.sg
CR ASSFALG J, 2002, P IEEE INT C MULT EX
   Babaguchi N, 2004, IEEE T MULTIMEDIA, V6, P575, DOI [10.1109/TMM.2004.830811, 10.1109/tmm.2004.830811]
   Babaguchi N, 2003, IEEE IMAGE PROC, P13
   BERTINI M, 2003, P 5 ACM SIGMM INT WO, P215, DOI DOI 10.1145/973264.973298
   CHUA TS, 1998, P INT C ADV MULT CON, P148
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Duan L.Y., 2003, Proc. ACM Int. Conf. Multimedia, P33
   Galley M., 2003, P 41 ANN M ASS COMP
   HAN M, 2002, P ACM MULT
   HAUPTMAN A, 2003, TRECVID 03       NOV
   KOH CK, 2000, DETECTION SEGMENTATI
   LEE MH, 2003, P IEEE INT C MULT EX
   Li BX, 2001, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P132, DOI 10.1109/IVL.2001.990867
   PAN H, 2002, P IEEE INT C AC SPEE
   TAN YP, 2000, IEEE T CIRCUITS VIDE, V10, P1
   WU Y, 2004, ACM MULTIMEDIA   OCT
   XIAO J, 2004, INT J ARTIFICIAL INT, V13, P813
   XIE L, 2003, P IEEE INT C MULT EX
   XIE L, 2002, P IEEE INT C AC SPEE
   XU H, 2004, P 6 ACM SIGMM INT WO
   XU H, 2005, P IEEE INT C MULT EX
   XU M, 2003, THESIS NATL U SINGAP
   XU P, 2001, P IEEE INT C MULT EX
   YANG H, 2004, P WWW 04, P304
   YANG J, 2004, P 3 INT C IM VID RET
   Zhang D., 2002, ACM Multimedia, P315
   Zhang H., 1995, Multimedia Tools and Applications, V1, P89, DOI 10.1007/BF01261227
   ZHANG Y, 2000, P ACM MULT 2000 WORK, P201
   Zhou W., 2000, ACM Workshops on Multimedia, P213
NR 29
TC 28
Z9 34
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2006
VL 2
IS 1
BP 44
EP 67
DI 10.1145/1126004.1126007
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IW
UT WOS:000250871200003
DA 2024-07-18
ER

PT J
AU Zhang, WG
   Qi, ZB
   Wang, SH
   Su, C
   Su, L
   Huang, QM
AF Zhang, Weigang
   Qi, Zhaobo
   Wang, Shuhui
   Su, Chi
   Su, Li
   Huang, Qingming
TI Temporal Dynamic Concept Modeling Network for Explainable Video Event
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Event recognition; temporal concept receptive field; dynamic convolution
AB Recently, with the vigorous development of deep learning and multimedia technology, intelligent urban computing has received more and more extensive attention from academia and industry. Unfortunately, most of the related technologies are black-box paradigms that lack interpretability. Among them, video event recognition is a basic technology. Event contains multiple concepts and their rich interactions, which can assist us to construct explainable event recognition methods. However, the crucial concepts needed to recognize events have various temporal existing patterns, and the relationship between events and the temporal characteristics of concepts has not been fully exploited. This brings great challenges for concept-based event categorization. To address the above issues, we introduce the temporal concept receptive field, which is the length of the temporal window size required to capture key concepts for concept-based event recognition methods. Accordingly, we introduce the temporal dynamic convolution (TDC) to model the temporal concept receptive field dynamically according to different events. Its core idea is to combine the results of multiple convolution layers with the learned coefficients from two complementary perspectives. These convolution layers contain a variety of kernel sizes, which can provide temporal concept receptive fields of different lengths. Similarly, we also propose the cross-domain temporal dynamic convolution (CrTDC) with the help of the rich relationship between different concepts. Different coefficients can help us to capture suitable temporal concept receptive field sizes and highlight crucial concepts to obtain accurate and complete concept representations for event analysis. Based on the TDC and CrTDC, we introduce the temporal dynamic concept modeling network (TDCMN) for explainable video event recognition. We evaluate TDCMN on large-scale and challenging datasets FCVID, ActivityNet, and CCV. Experimental results show that TDCMN significantly improves the event recognition performance of concept-based methods, and the explainability of our method inspires us to construct more explainable models from the perspective of the temporal concept receptive field.
C1 [Zhang, Weigang; Qi, Zhaobo] Harbin Inst Technol, Weihai 264209, Peoples R China.
   [Wang, Shuhui] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Wang, Shuhui] Peng Cheng Lab, Beijing 100190, Peoples R China.
   [Su, Chi] SmartMore, Beijing, Peoples R China.
   [Su, Li] Univ Chinese Acad Sci, Beijing 101478, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Inst Comp Technol, Chinese Acad Sci, Beijing 101478, Peoples R China.
   [Huang, Qingming] Peng Cheng Lab, Beijing 101478, Peoples R China.
C3 Harbin Institute of Technology; Chinese Academy of Sciences; Institute
   of Computing Technology, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Institute of Computing
   Technology, CAS
RP Qi, ZB (corresponding author), Harbin Inst Technol, Weihai 264209, Peoples R China.; Huang, QM (corresponding author), Univ Chinese Acad Sci, Inst Comp Technol, Chinese Acad Sci, Beijing 101478, Peoples R China.; Huang, QM (corresponding author), Peng Cheng Lab, Beijing 101478, Peoples R China.
EM wgzhang@hit.edu.cn; qizb@hit.edu.cn; wangshuhui@ict.ac.cn;
   chi.su@smartmore.com; suli@ucas.ac.cn; qmhuang@ucas.ac.cn
RI 丽, 苏/JVO-8581-2024
OI Zhang, Weigang/0000-0003-0042-7074
FU Technology and Innovation Major Project of the Ministry of Science and
   Technology of China [2020AAA0108400, 2020AAA0108402]; National Natural
   Science Foundation of China [61976069, U21B2038, 62236008, 62022083,
   61836002, 61931008]; Beijing Nova Program [Z201100006820023];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by Technology and Innovation Major
   Project of the Ministry of Science and Technology of China under Grant
   2020AAA0108400 and 2020AAA0108402, in part by National Natural Science
   Foundation of China under Grant 61976069, U21B2038, 62236008, 62022083,
   61836002, and 61931008, in part by the Beijing Nova Program under Grant
   Z201100006820023, and in part by the Fundamental Research Funds for the
   Central Universities.
CR Ahmad K, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3199668
   Ahmad K, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3306240
   [Anonymous], 2016, P INT JOINT C ARTIFI
   Bhattacharya S, 2014, PROC CVPR IEEE, P2243, DOI 10.1109/CVPR.2014.287
   Burkov E, 2018, ADV NEUR IN, V31
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang XJ, 2016, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2016.208
   Chang XJ, 2017, IEEE T PATTERN ANAL, V39, P1617, DOI 10.1109/TPAMI.2016.2608901
   Chang X, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P581, DOI 10.1145/2733373.2806218
   Chen Jiawei., 2014, Proceedings of International Conference on Multimedia Retrieval, P1
   Chen Weidong, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P4416, DOI 10.1145/3503161.3547761
   Chen WD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3514250
   Chen WD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4053, DOI 10.1145/3474085.3475534
   Chen XD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11793, DOI 10.1109/ICCV48922.2021.01160
   Dang Ha TheHien, 2017, GUID REC FIELD AR CO
   De Brabandere B, 2016, ADV NEUR IN, V29
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan HH, 2017, IEEE I CONF COMP VIS, P736, DOI 10.1109/ICCV.2017.86
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409332
   Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31
   Jiang Y., 2011, P ACM INT C MULT RET
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Jiang YG, 2012, IEEE T IMAGE PROCESS, V21, P3080, DOI 10.1109/TIP.2012.2188038
   Kang Sunghun, 2018, PROCEEDINGS OF THE E, P386
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kloft M, 2011, J MACH LEARN RES, V12, P953
   Korbar B, 2019, IEEE I CONF COMP VIS, P6241, DOI 10.1109/ICCV.2019.00633
   Lai KT, 2014, LECT NOTES COMPUT SC, V8691, P675, DOI 10.1007/978-3-319-10578-9_44
   Li C, 2017, IEEE T IMAGE PROCESS, V26, P2149, DOI 10.1109/TIP.2017.2670782
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Li YG, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3378026
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZF, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2807705
   Lioutas Vasileios, 2020, INT C MACHINE LEARNI, V119, P6172
   Liu K, 2021, IEEE T CIRC SYST VID, V31, P647, DOI 10.1109/TCSVT.2020.2984569
   Liu K, 2019, WORLD WIDE WEB, V22, P807, DOI 10.1007/s11280-018-0642-6
   Liu K, 2018, AAAI CONF ARTIF INTE, P7138
   Liu Wu, 2022, RECENT ADV MONOCULAR, DOI [10.1145/3524497, DOI 10.1145/3524497]
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Meng Yue, 2021, P INT C LEARNING REP
   Nagel Markus, 2015, P BRIT MACH VIS C
   Pei WJ, 2017, PROC CVPR IEEE, P820, DOI 10.1109/CVPR.2017.94
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Qi ZB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3857, DOI 10.1145/3394171.3413954
   Qi ZB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3798, DOI 10.1145/3394171.3413618
   Qi ZB, 2023, IEEE T PATTERN ANAL, V45, P6715, DOI 10.1109/TPAMI.2021.3059923
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh B, 2015, IEEE I CONF COMP VIS, P4561, DOI 10.1109/ICCV.2015.518
   Smith JR, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P445
   Soltanian M, 2020, IEEE T MULTIMEDIA, V22, P1769, DOI 10.1109/TMM.2019.2959426
   Soltanian M, 2019, IEEE T MULTIMEDIA, V21, P157, DOI 10.1109/TMM.2018.2844101
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16229, DOI 10.1109/ICCV48922.2021.01594
   Wu WH, 2019, IEEE I CONF COMP VIS, P6231, DOI 10.1109/ICCV.2019.00632
   Wu ZX, 2019, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2019.00137
   Wu ZX, 2019, ADV NEUR IN, V32
   Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Xie WL, 2019, IEEE T MULTIMEDIA, V21, P1425, DOI 10.1109/TMM.2018.2879749
   Xu ZW, 2014, PROC CVPR IEEE, P97, DOI 10.1109/CVPR.2014.20
   Yan Y, 2015, AAAI CONF ARTIF INTE, P3841
   Yang B, 2019, ADV NEUR IN, V32
   Yang XS, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2962719
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Yu F., 2015, ARXIV
   Yue Meng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P86, DOI 10.1007/978-3-030-58571-6_6
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2518, DOI 10.1109/TMM.2019.2907453
   Zhang LG, 2019, Arxiv, DOI arXiv:1906.11367
   Zhao RW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231739
   Zhao SC, 2018, IEEE T CYBERNETICS, V48, P3218, DOI 10.1109/TCYB.2017.2762344
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 84
TC 0
Z9 0
U1 3
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 219
DI 10.1145/3568312
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200041
DA 2024-07-18
ER

PT J
AU Zhang, JW
   Yu, Y
   Tang, SH
   Wu, JM
   Li, W
AF Zhang, Jiwei
   Yu, Yi
   Tang, Suhua
   Wu, Jianming
   Li, Wei
TI Variational Autoencoder with CCA for Audio-Visual Cross-modal Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modal retrieval; audio-visual correlation learning
ID CANONICAL CORRELATION-ANALYSIS
AB Cross-modal retrieval is to utilize one modality as a query to retrieve data from another modality, which has become a popular topic in information retrieval, machine learning, and databases. Finding a method to effectively measure the similarity between different modality data is the major challenge of cross-modal retrieval. Although several research works have calculated the correlation between different modality data via learning a common subspace representation, the encoder's ability to extract features from multi-modal information is not satisfactory. In this article, we present a novel variational autoencoder architecture for audio-visual cross-modal retrieval by learning paired audio-visual correlation embedding and category correlation embedding as constraints to reinforce the mutuality of audio-visual information. On the one hand, audio encoder and visual encoder separately encode audio data and visual data into two different latent spaces. Further, two mutual latent spaces are respectively constructed by canonical correlation analysis. On the other hand, probabilistic modeling methods are used to deal with possible noise and missing information in the data. Additionally, in this way, the cross-modal discrepancies from intra-modal and inter-modal information are simultaneously eliminated in the joint embedding subspace. We conduct extensive experiments over two benchmark datasets. The experimental results confirm that the proposed architecture is effective in learning audio-visual correlation and is appreciably better than the existing cross-modal retrieval methods.
C1 [Zhang, Jiwei; Yu, Yi] Natl Inst Informat, Digital Content & Media Sci Res Div, 2-1-2 Hitotsubashi,Chiyoda Ku, Tokyo 1018430, Japan.
   [Tang, Suhua] Univ Elect Commun, Grad Sch Informat & Engn, Dept Comp & Network Engn, I-5-1 Chofugaoka,Chofu Shi, Tokyo 1828585, Japan.
   [Wu, Jianming] KDDI Res Inc, 2-1-15 Ohara, Fujimino, Saitama 3568502, Japan.
   [Li, Wei] Fudan Univ, Sch Comp Sci, 220 Handan Rd, Shanghai 3568502, Peoples R China.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; KDDI Corporation; KDDI Research,
   Inc.; Fudan University
RP Yu, Y (corresponding author), Natl Inst Informat, Digital Content & Media Sci Res Div, 2-1-2 Hitotsubashi,Chiyoda Ku, Tokyo 1018430, Japan.
EM jiweizhang@nii.ac.jp; yiyu@nii.ac.jp; shtang@uec.ac.jp;
   swordwu@gmail.com; weili-fudan@fudan.edu.cn
RI WU, Jianming/JGM-5915-2023
OI WU, Jianming/0000-0002-1720-8516; zhang, jiwei/0000-0003-4494-8648;
   Tang, Suhua/0000-0002-5784-8411; Li, Wei/0000-0002-4486-8341
CR Andrew G., 2013, ICML, P1247
   [Anonymous], 2012, P 20 ACM INT C MULTI
   Chen PH, 2020, IEEE T IMAGE PROCESS, V29, P8292, DOI 10.1109/TIP.2020.3009820
   Chuang Gan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P758, DOI 10.1007/978-3-030-58621-8_44
   Chuang Gan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10475, DOI 10.1109/CVPR42600.2020.01049
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gan C, 2019, IEEE I CONF COMP VIS, P7052, DOI 10.1109/ICCV.2019.00715
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Gu W, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/3323873.3325045
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He L, 2017, IEEE INT CON MULTI, P1153, DOI 10.1109/ICME.2017.8019549
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huang ZC, 2020, Arxiv, DOI arXiv:2004.00849
   KaiyeWang Qiyue Yin, 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06215
   Kingma D. P., 2014, arXiv
   Lai PL, 2000, IEEE IJCNN, P614
   LEURGANS SE, 1993, J ROY STAT SOC B MET, V55, P725
   Mandal D, 2020, IEEE T MULTIMEDIA, V22, P2345, DOI 10.1109/TMM.2019.2954741
   MITCHELL TJ, 1992, 1992 WINTER SIMULATION CONFERENCE PROCEEDINGS, P565, DOI 10.1145/167293.167638
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rasiwasia N, 2014, JMLR WORKSH CONF PRO, V33, P823
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang W, 2016, VLDB J, V25, P79, DOI 10.1007/s00778-015-0391-4
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu JL, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P917, DOI 10.1145/3077136.3080678
   Xu X, 2019, WORLD WIDE WEB, V22, P657, DOI 10.1007/s11280-018-0541-x
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Yu Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3281746
   Yu Y, 2019, IEEE T NEUR NET LEAR, V30, P1250, DOI 10.1109/TNNLS.2018.2856253
   Zeng DH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3387164
   Zeng DH, 2018, IEEE INT SYM MULTIM, P143, DOI 10.1109/ISM.2018.00-21
   Zeng Donghuo, 2021, ARXIV
   Zhai DM, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168767
   Zhang Jian, 2018, P 32 AAAI C ARTIFICI
   Zhang L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3406
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhao H, 2018, LECT NOTES COMPUT SC, V11205, P587, DOI 10.1007/978-3-030-01246-5_35
   Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
   Zhu Y, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4300, DOI 10.1109/ICASSP39728.2021.9414296
NR 52
TC 2
Z9 2
U1 2
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 130
DI 10.1145/3575658
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Guo, YY
   Nie, LQ
   Cheng, H
   Cheng, ZY
   Kankanhalli, M
   Del Bimbo, A
AF Guo, Yangyang
   Nie, Liqiang
   Cheng, Harry
   Cheng, Zhiyong
   Kankanhalli, Mohan
   Del Bimbo, Alberto
TI On Modality Bias Recognition and Reduction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Modality bias; Out-of-Distribution; large margin loss
AB Making each modality in multi-modal data contribute is of vital importance to learning a versatile multi-modal model. Existing methods, however, are often dominated by one or few of modalities during model training, resulting in sub-optimal performance. In this article, we refer to this problem as modality bias and attempt to study it in the context of multi-modal classification systematically and comprehensively. After stepping into several empirical analyses, we recognize that one modality affects the model prediction more just because this modality has a spurious correlation with instance labels. To primarily facilitate the evaluation on the modality bias problem, we construct two datasets, respectively, for the colored digit recognition and video action recognition tasks in line with the Out-of-Distribution (OoD) protocol. Collaborating with the benchmarks in the visual question answering task, we empirically justify the performance degradation of the existing methods on these OoD datasets, which serves as evidence to justify the modality bias learning. In addition, to overcome this problem, we propose a plug-and-play loss function method, whereby the feature space for each label is adaptively learned according to the training set statistics. Thereafter, we apply this method on 10 baselines in total to test its effectiveness. From the results on four datasets regarding the above three tasks, our method yields remarkable performance improvements compared with the baselines, demonstrating its superiority on reducing the modality bias problem.
C1 [Guo, Yangyang; Kankanhalli, Mohan] Natl Univ Singapore, 13 Comp Dr, Singapore 117417, Singapore.
   [Nie, Liqiang] Harbin Inst Technol Shenzhen, Harbin 518055, Peoples R China.
   [Cheng, Harry] Shandong Univ, Qingdao 266237, Peoples R China.
   [Cheng, Zhiyong] Shandong Artificial Intelligence Inst, Jinan 250014, Peoples R China.
   [Del Bimbo, Alberto] Univ Florence, I-50134 Florence, Italy.
C3 National University of Singapore; Harbin Institute of Technology;
   Shandong University; University of Florence
RP Guo, YY (corresponding author), Natl Univ Singapore, 13 Comp Dr, Singapore 117417, Singapore.
EM guoyang.eric@gmail.com; nieliqiang@gmail.com; xaCheng1996@gmail.com;
   jason.zy.cheng@gmail.com; mohan@comp.nus.edu.sg;
   alberto.delbimbo@unifi.it
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015; Guo,
   Yangyang/0000-0001-8691-5372; Cheng, Harry/0000-0001-7436-0162; DEL
   BIMBO, ALBERTO/0000-0002-1052-8322
FU National Research Foundation, Singapore under its Strategic Capability
   Research Centres Funding Initiative
FX This research is supported by the National Research Foundation,
   Singapore under its Strategic Capability Research Centres Funding
   Initiative. Any opinions, findings and conclusions or recommendations
   expressed in this material are those of the author(s) and do not reflect
   the views of National Research Foundation, Singapore.
CR Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bai H, 2021, P IEEE CVF INT C COM, P8320
   Bai HY, 2021, AAAI CONF ARTIF INTE, V35, P6705
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Boyu Lu, 2019, IEEE Transactions on Biometrics, Behavior, and Identity Science, V1, P42, DOI 10.1109/TBIOM.2018.2890577
   Cadene Remi, 2019, ADV NEURAL INFORM PR, P839
   Caliskan A, 2017, SCIENCE, V356, DOI 10.1126/science.aal4230
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen CF, 2021, PROC CVPR IEEE, P6161, DOI 10.1109/CVPR46437.2021.00610
   Chen Yunliang, 2021, P IEEECVF INT C COMP, P14980, DOI DOI 10.48550/ARXIV.2108.08504
   Cheng L, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P2158
   Clark C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4069
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dixon L, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P67, DOI 10.1145/3278721.3278729
   Dou Q, 2019, ADV NEUR IN, V32
   Engstrom L, 2020, PR MACH LEARN RES, V119
   Fukui A., 2016, P C EMP METH NAT LAN, P457
   Gat Itai, 2020, ADV NEURAL INFORM PR
   Gong SX, 2021, PROC CVPR IEEE, P3413, DOI 10.1109/CVPR46437.2021.00342
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guo YY, 2022, IEEE T IMAGE PROCESS, V31, P227, DOI 10.1109/TIP.2021.3128322
   Guo YY, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P75, DOI 10.1145/3331184.3331186
   Guo Yangyang, 2021, P INT JOINT C ART IN, P708, DOI DOI 10.1109/MWSCAS47672.2021.9531788
   Hama K, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3425663
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2019, PR MACH LEARN RES, V97
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jing CC, 2020, AAAI CONF ARTIF INTE, V34, P11181
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kim E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14972, DOI 10.1109/ICCV48922.2021.01472
   Kortylewski A, 2018, IEEE COMPUT SOC CONF, P2174, DOI 10.1109/CVPRW.2018.00283
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee Hyuck, 2021, Advances in Neural Information Processing Systems, V34
   Li YC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414838
   Li Y, 2019, PROC CVPR IEEE, P9564, DOI 10.1109/CVPR.2019.00980
   Li ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14950, DOI 10.1109/ICCV48922.2021.01470
   Li Zhiheng, 2021, P IEEECVF INT C COMP
   Long Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10797, DOI 10.1109/CVPR42600.2020.01081
   Mu Norman, 2021, ARXIV200616241, P8340
   Noori FM, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377882
   Park JH, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2799
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Perez Ethan, 2022, arXiv, DOI DOI 10.48550/ARXIV.2202.03286
   Pernici F., 2019, CVPR WORKSH, P46
   Ramakrishnan S., 2018, INT C NEURAL INF PRO, V31, P1541
   Recht B, 2019, PR MACH LEARN RES, V97
   Selvaraju RR, 2019, IEEE I CONF COMP VIS, P2591, DOI 10.1109/ICCV.2019.00268
   Shah Deven Santosh, 2020, P 58 ANN M ASS COMPU, P5248, DOI [10.18653/v1/2020.acl-main.468, DOI 10.18653/V1/2020.ACL-MAIN.468]
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Teney Damien, 2021, P INT C COMP VIS, P1417
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang Haohan, 2019, P INT C LEARNING REP
   Wang M, 2019, IEEE I CONF COMP VIS, P692, DOI 10.1109/ICCV.2019.00078
   Wang TL, 2019, IEEE I CONF COMP VIS, P5309, DOI 10.1109/ICCV.2019.00541
   Wu JL, 2019, ADV NEUR IN, V32
   Zhang GH, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4134
   Zhang Y., 2018, 6 INT C LEARN REPR I
   Zhang YS, 2021, AAAI CONF ARTIF INTE, V35, P3447
   Zhuang YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3366710
NR 65
TC 2
Z9 2
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 103
DI 10.1145/3565266
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300003
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Yan, H
   Zhang, HJ
   Shi, JY
   Ma, JH
   Xu, XF
AF Yan, Han
   Zhang, Haijun
   Shi, Jianyang
   Ma, Jianghong
   Xu, Xiaofei
TI Toward Intelligent Fashion Design: A Texture and Shape Disentangled
   Generative Adversarial Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fashion design; generative adversarial network; disentanglement;
   texture; shape
AB Texture and shape in fashion, constituting essential elements of garments, characterize the body and surface of the fabric and outline the silhouette of clothing, respectively. The selection of texture and shape plays a critical role in the design process, as they largely determine the success of a new design for fashion items. In this research, we propose a texture and shape disentangled generative adversarial network (TSD-GAN) to perform "intelligent" design with the transformation of texture and shape in fashion items. Our TSD-GAN aims to learn how to disentangle the features of texture and shape of different fashion items in an unsupervised manner. Specifically, a fashion attribute encoder is developed to decompose the input fashion items into independent representations of texture and shape. Then, to learn the coarse or fine styles hidden in the features of texture and shape, a texture mapping network and a shape mapping network are proposed to disentangle the features into different hierarchical representations. The different hierarchical representations of texture and shape are then fed into a multi-factor-based generator to generate mixed-style fashion items. In addition, a multi-discriminator framework is developed to distinguish the authenticity and texture similarity between the generated images and the real images. Experimental results on different fashion categories demonstrate that our proposed TSD-GAN may be useful for assisting designers to accomplish the design process by transforming the texture and shape of fashion items.
C1 [Yan, Han; Zhang, Haijun; Shi, Jianyang; Ma, Jianghong; Xu, Xiaofei] Harbin Inst Technol, Dept Comp Sci, Shenzhen 518055, Guangdong, Peoples R China.
C3 Harbin Institute of Technology
RP Zhang, HJ (corresponding author), Harbin Inst Technol, Dept Comp Sci, Shenzhen 518055, Guangdong, Peoples R China.
EM 20b351014@stu.hit.edu.cn; hjzhang@hit.edu.cn; 19b951026@stu.hit.edu.cn;
   majianghong@hit.edu.cn; xiaofei@hit.edu.cn
RI Zhang, Haijun/N-8470-2015; Xu, Xiaofei/IQS-7571-2023
OI Ma, Jianghong/0000-0002-0524-3584; Shi, Jianyang/0000-0002-9151-6058
FU National Natural Science Foundation of China [61972112, 61832004];
   Guangdong Basic and Applied Basic Research Foundation [2021B1515020088];
   Shenzhen Science and Technology Program [JCYJ20210324131203009];
   HITSZ-J&A Joint Laboratory of Digital Design and Intelligent Fabrication
   [HITSZ-JA-2021A01]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants No. 61972112 and No. 61832004, the
   Guangdong Basic and Applied Basic Research Foundation under Grant No.
   2021B1515020088, the Shenzhen Science and Technology Program under Grant
   No. JCYJ20210324131203009, and the HITSZ-J&A Joint Laboratory of Digital
   Design and Intelligent Fabrication under Grant No. HITSZ-J&A-2021A01.
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Adobe Creative Team, 2012, AD ILL CS6 CLASSR BO
   Alharbi Y, 2020, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR42600.2020.00518
   Chen G, 2018, ART DES COMMUN HIGH, V17, P51, DOI 10.1386/adch.17.1.51_1
   Cho W, 2019, PROC CVPR IEEE, P10631, DOI 10.1109/CVPR.2019.01089
   Collins E, 2020, PROC CVPR IEEE, P5770, DOI 10.1109/CVPR42600.2020.00581
   Cui YR, 2018, COMPUT GRAPH FORUM, V37, P109, DOI 10.1111/cgf.13552
   Deng YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2719, DOI 10.1145/3394171.3414015
   Guan SY, 2020, Arxiv, DOI arXiv:2007.01758
   Guan WL, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P482, DOI 10.1145/3477495.3532038
   Guan WL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2299, DOI 10.1145/3474085.3475392
   Haoye Dong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8117, DOI 10.1109/CVPR42600.2020.00814
   Harkonen E., 2020, Advances in Neural Information Processing Systems, V33, P9841
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Ho TT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3396237
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Ji YZ, 2018, NEUROCOMPUTING, V322, P130, DOI 10.1016/j.neucom.2018.09.061
   Jiang SH, 2022, IEEE T NEUR NET LEAR, V33, P4538, DOI 10.1109/TNNLS.2021.3057892
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Kotovenko D, 2019, IEEE I CONF COMP VIS, P4421, DOI 10.1109/ICCV.2019.00452
   Kukiev Boburmirzo, 2019, EUR J RES REFLECT ED, V7
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Liu GH, 2013, PATTERN RECOGN, V46, P188, DOI 10.1016/j.patcog.2012.06.001
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Shelly G. B., 2009, Adobe Photoshop CS4: Comprehensive Concepts and Techniques
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Yan HZ, 2024, IEEE T COMPUT SOC SY, V11, P3079, DOI [10.1109/TCSS.2022.3161996, 10.1109/TMM.2022.3146010]
   Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang FF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478642
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y., 2022, P ACM SIGGRAPH, P1
   Zhang Z, 2022, Arxiv, DOI arXiv:2105.14211
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 38
TC 7
Z9 7
U1 10
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 107
DI 10.1145/3567596
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300007
DA 2024-07-18
ER

EF