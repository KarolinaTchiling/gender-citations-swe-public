FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Xu, ZM
   Wei, LL
   Lang, CY
   Feng, SH
   Wang, T
   Bors, AG
   Liu, HZ
AF Xu, Zheming
   Wei, Lili
   Lang, Congyan
   Feng, Songhe
   Wang, Tao
   Bors, Adrian G.
   Liu, Hongzhe
TI SSR-Net: A Spatial Structural Relation Network for Vehicle
   Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Vehicle re-identification; Graph Convolution Network; attention
   mechanism; deep learning
AB Vehicle re-identification (Re-ID) represents the task aiming to identify the same vehicle from images captured by different cameras. Recent years have seen various feature learning-based approaches merely focusing on feature representations including global features or local features to obtain more subtle details to identify highly similar vehicles. However, few such methods consider the spatial geometrical structure relationship among local regions or between the global and local regions. By contrast, in this study, we propose a Spatial Structural Relation Network (SSR-Net) that explores the above-mentioned two kinds of relations simultaneously to learn more discriminative features by modeling the spatial structure information and global context information. In this article, we propose to adopt a Graph Convolution Network (GCN), for modeling spatial structural relationships among characteristic features. The GCN model aggregating the local and global features is shown to be more discriminative and robust to several car image transformations. To improve the performance of our proposed network, we jointly combine the classification loss with metric learning loss. Extensive experiments conducted on the public VehicleID and VeRi-776 datasets validate the effectiveness of our approach in comparison with recent works.
C1 [Xu, Zheming; Wei, Lili; Lang, Congyan; Feng, Songhe; Wang, Tao] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing Key Lab Traff Data Anal & Min, 3 Shangyuancun, Beijing 100044, Peoples R China.
   [Bors, Adrian G.] Univ York, York, England.
   [Liu, Hongzhe] Beijing Union Univ, Beijing Key Lab Informat Serv Engn, Beijing, Peoples R China.
C3 Beijing Jiaotong University; University of York - UK; Beijing Union
   University
RP Lang, CY (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing Key Lab Traff Data Anal & Min, 3 Shangyuancun, Beijing 100044, Peoples R China.
EM 21112016@bjtu.edu.cn; 20112014@bjtu.edu.cn; cylang@bjtu.edu.cn;
   shfeng@bjtu.edu.cn; twang@bjtu.edu.cn; adrian.bors@york.ac.uk;
   liuhongzhe@buu.edu.cn
RI Bors, Adrian G./T-3618-2019
OI Bors, Adrian G./0000-0001-7838-0021; Liu, Hongzhe/0000-0003-2314-5272;
   Xu, Zheming/0000-0001-7558-9305; Wei, Lili/0000-0002-3905-8651
FU National Natural Science Foundation of China [62072027, 61872032,
   62076021]
FX This work was supported by the National Natural Science Foundation of
   China under Grant nos. 62072027, 61872032, and 62076021.
CR [Anonymous], 2006, PROC IEEE COMPUT SOC
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Bruna J, 2014, Arxiv, DOI [arXiv:1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Chen H., 2019, P CVPR WORKSH LONG B, P184
   Chu RH, 2019, IEEE I CONF COMP VIS, P8281, DOI 10.1109/ICCV.2019.00837
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Elsayed GF, 2019, Arxiv, DOI arXiv:1908.07644
   Gidaris S, 2019, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2019.00011
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Guo HY, 2019, IEEE T IMAGE PROCESS, V28, P4328, DOI 10.1109/TIP.2019.2910408
   Guo HY, 2018, AAAI CONF ARTIF INTE, P6853
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YG, 2019, IEEE IMAGE PROC, P3108, DOI [10.1109/icip.2019.8803323, 10.1109/ICIP.2019.8803323]
   Henaff M, 2015, Arxiv, DOI arXiv:1506.05163
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hou JH, 2019, IEEE T VEH TECHNOL, V68, P8512, DOI 10.1109/TVT.2019.2927353
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Ji HXY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3641, DOI 10.1109/ICCV48922.2021.00364
   Jiang B, 2019, Arxiv, DOI arXiv:1907.08822
   Jiang N, 2018, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2018.8451776
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11165
   Khorramshahi Pirazh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P369, DOI 10.1007/978-3-030-58568-6_22
   Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623
   Kuma Ratnesh, 2019, P INT JOINT C NEURAL, P1
   Li YG, 2018, Arxiv, DOI [arXiv:1707.01926, DOI 10.48550/ARXIV.1707.01926]
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu XB, 2020, IEEE T IMAGE PROCESS, V29, P2638, DOI 10.1109/TIP.2019.2950796
   Liu XB, 2018, IEEE INT CON MULTI
   Liu XC, 2019, J COMPUT SCI TECH-CH, V34, P634, DOI 10.1007/s11390-019-1932-x
   Liu XC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P907, DOI 10.1145/3394171.3413578
   Liu XC, 2016, IEEE INT CON MULTI
   Liu XC, 2019, PROC CVPR IEEE, P3561, DOI 10.1109/CVPR.2019.00368
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Liu Z, 2021, Arxiv, DOI [arXiv:2103.14030, DOI 10.48550/ARXIV.2103.14030]
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Lou YH, 2019, IEEE T IMAGE PROCESS, V28, P3794, DOI 10.1109/TIP.2019.2902112
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Park J, 2018, Arxiv, DOI [arXiv:1807.06514, 10.48550/arXiv.1807.06514, DOI 10.48550/ARXIV.1807.06514]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sharma S, 2016, Arxiv, DOI arXiv:1511.04119
   Shaw P, 2018, Arxiv, DOI arXiv:1803.02155
   Shen YT, 2017, IEEE I CONF COMP VIS, P1918, DOI 10.1109/ICCV.2017.210
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tang Z, 2019, IEEE I CONF COMP VIS, P211, DOI 10.1109/ICCV.2019.00030
   Tsai-Shien Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P330, DOI 10.1007/978-3-030-58536-5_20
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang ZD, 2017, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2017.49
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu MH, 2021, IEEE COMPUT SOC CONF, P4072, DOI 10.1109/CVPRW53098.2021.00460
   Wu SJ, 2020, Arxiv, DOI arXiv:1808.10024
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan SJ, 2019, IEEE I CONF COMP VIS, P4393, DOI 10.1109/ICCV.2019.00449
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang JR, 2020, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR42600.2020.00335
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890
   Zhang XT, 2019, Arxiv, DOI arXiv:1906.01210
   Zhang YH, 2017, IEEE INT CON MULTI, P1386, DOI 10.1109/ICME.2017.8019491
   Zhang Y, 2010, INT J MACH LEARN CYB, V1, P43, DOI 10.1007/s13042-010-0001-0
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhao JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P205, DOI 10.1109/ICCV48922.2021.00027
   Zhao JN, 2021, PROC CVPR IEEE, P2225, DOI 10.1109/CVPR46437.2021.00226
   Zheming Xu, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12665), P356, DOI 10.1007/978-3-030-68821-9_32
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zhou Y., 2017, 1 AS AUSTR C PREC PA, P1, DOI DOI 10.5244/C.31.186
   Zhou Y, 2018, IEEE WINT CONF APPL, P653, DOI 10.1109/WACV.2018.00077
   Zhou Y, 2018, IEEE T IMAGE PROCESS, V27, P3275, DOI 10.1109/TIP.2018.2819820
   Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679
   Zhu JQ, 2020, IEEE T INTELL TRANSP, V21, P410, DOI 10.1109/TITS.2019.2901312
   Zhu JQ, 2018, INT C PATT RECOG, P3285, DOI 10.1109/ICPR.2018.8545514
   Zhu YC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P646, DOI 10.1145/3394171.3413607
NR 90
TC 1
Z9 1
U1 4
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 216
DI 10.1145/3578578
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200038
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Li, C
   Song, L
   Xie, R
   Zhang, WJ
AF Li, Chen
   Song, Li
   Xie, Rong
   Zhang, Wenjun
TI Local Bidirection Recurrent Network for Efficient Video Deblurring with
   the Fused Temporal Merge Module
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video deblurring; local bidirection; fused temporal merge
AB Video deblurring methods exploit the correlation between consecutive blurry inputs to generate sharp frames. However, designing an effective and efficient method is a challenging problem for video deblurring. To guarantee the effectiveness and further improve the deblurring performance, we adopt the recurrent-based method as the baseline and reconsider the recurrent mechanism as well as the temporal feature alignment in the state-of-the-art methods. For the recurrentmechanism, we add the local backward connection to the global forward recurrent backbone to effectively exploit accurate future information. For the temporal alignment, we adopt a fused temporal merge module that exploits the superiority of flow-based and kernel-based methods with progressive correlation volumes estimation. In addition, we evaluate our method with both synthetic datasets (GoPro, DVD) and a realistic dataset (BSD). The experimental results demonstrate that our method achieves significant performance improvement with a slight computational cost increase against the state-of-the-art video deblurring methods. The extended ablation studies verify the effectiveness of our model.
C1 [Li, Chen; Xie, Rong] Inst Image Commun & Network Engn, Dongchuan Rd, Shanghai 200240, Shanghai, Peoples R China.
   [Song, Li; Zhang, Wenjun] Shanghai Jiao Tong Univ, Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Song, Li; Zhang, Wenjun] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr CMIC, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Song, L (corresponding author), Shanghai Jiao Tong Univ, Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Dongchuan Rd, Shanghai 200240, Peoples R China.; Song, L (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr CMIC, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM lcjurrivh@sjtu.edu.cn; song_li@sjtu.edu.cn; xierong@sjtu.edu.cn;
   zhangwenjun@sjtu.edu.cn
OI Li, Chen/0000-0003-4059-2058; Song, Li/0000-0002-7124-5182
FU Fundamental Research Funds for the Central Universities; 111 Project,
   China [B07022, 150633]; Shanghai Key Laboratory of Digital Media
   Processing and Transmissions, China
FX This work was supported by the Fundamental Research Funds for the
   Central Universities, 111 Project, China, under grants B07022 and
   (Sheitc) 150633, and Shanghai Key Laboratory of Digital Media Processing
   and Transmissions, China.
CR Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Chan KCK, 2022, PROC CVPR IEEE, P5962, DOI 10.1109/CVPR52688.2022.00588
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Cho S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185560
   Delbracio M, 2015, IEEE T COMPUT IMAG, V1, P270, DOI 10.1109/TCI.2015.2501245
   He JL, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3382506
   Hu Mengshun, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P847, DOI 10.1145/3503161.3547874
   Kim TH, 2018, IEEE T PATTERN ANAL, V40, P2374, DOI 10.1109/TPAMI.2017.2761348
   Kim TH, 2017, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2017.435
   Kim TH, 2015, PROC CVPR IEEE, P5426, DOI 10.1109/CVPR.2015.7299181
   Kim TH, 2013, IEEE I CONF COMP VIS, P3160, DOI 10.1109/ICCV.2013.392
   Kim TH, 2014, PROC CVPR IEEE, P2766, DOI 10.1109/CVPR.2014.348
   Kingma D. P., 2014, arXiv
   Li C, 2022, NEUROCOMPUTING, V483, P195, DOI 10.1016/j.neucom.2022.02.013
   Li DX, 2021, PROC CVPR IEEE, P7717, DOI 10.1109/CVPR46437.2021.00763
   Li XG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282445
   Li YP, 2010, PROC CVPR IEEE, P2424, DOI 10.1109/CVPR.2010.5539938
   Lin Jing., 2022, PROC 39 INT C MACH L, P13334
   Matsushita Y, 2006, IEEE T PATTERN ANAL, V28, P1150, DOI 10.1109/TPAMI.2006.141
   Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251
   Nah S, 2019, PROC CVPR IEEE, P8094, DOI 10.1109/CVPR.2019.00829
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pan JS, 2020, PROC CVPR IEEE, P3040, DOI 10.1109/CVPR42600.2020.00311
   Ren WQ, 2017, IEEE I CONF COMP VIS, P1086, DOI 10.1109/ICCV.2017.123
   Son H, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3453720
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wieschollek P, 2017, IEEE I CONF COMP VIS, P231, DOI 10.1109/ICCV.2017.34
   Wulff J, 2014, LECT NOTES COMPUT SC, V8694, P236, DOI 10.1007/978-3-319-10599-4_16
   Yi P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4409, DOI 10.1109/ICCV48922.2021.00439
   Zhang HC, 2014, IEEE T PATTERN ANAL, V36, P1628, DOI 10.1109/TPAMI.2013.241
   Zhang HC, 2022, LECT NOTES COMPUT SC, V13676, P581, DOI 10.1007/978-3-031-19787-1_33
   Zhihang Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P191, DOI 10.1007/978-3-030-58539-6_12
   Zhou SC, 2019, IEEE I CONF COMP VIS, P2482, DOI 10.1109/ICCV.2019.00257
NR 36
TC 2
Z9 2
U1 3
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2023
VL 19
IS 5
SU S
AR 170
DI 10.1145/3587468
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L0SK0
UT WOS:001020437100006
DA 2024-07-18
ER

PT J
AU Siekkinen, M
   Kämäräinen, T
AF Siekkinen, Matti
   Kamarainen, Teemu
TI Neural Network Assisted Depth Map Packing for Compression Using Standard
   Hardware Video Codecs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Depth map; video encoding; neural network; game engine
AB Depth maps are needed by various graphics rendering and processing operations. Depth map streaming is often necessary when such operations are performed in a distributed system and it requires in most cases fast performing compression, which is why video codecs are often used. Hardware implementations of standard video codecs enable relatively high resolution and frame rate combinations, even on resource constrained devices, but unfortunately those implementations do not currently support RGB+depth extensions. However, they can be used for depth compression by first packing the depth maps into RGB or YUV frames. We investigate depth map compression using a combination of depth map packing followed by encoding with a standard video codec. We show that the precision at which depth maps are packed has a large and nontrivial impact on the resulting error caused by the combination of the packing scheme and lossy compression when the bitrate is constrained. Consequently, we propose a variable precision packing scheme assisted by a neural network model that predicts the optimal precision for each depth map given a bitrate constraint. We demonstrate that the model yields near optimal predictions and that it can be integrated into a game engine with very low overhead using modern hardware.
C1 [Siekkinen, Matti] Aalto Univ, Dept Comp Sci, POB 15400, FI-00076 Espoo, Finland.
   [Siekkinen, Matti; Kamarainen, Teemu] Univ Helsinki, Dept Comp Sci, POB 4, Helsinki 00014, Finland.
C3 Aalto University; University of Helsinki
RP Siekkinen, M (corresponding author), Aalto Univ, Dept Comp Sci, POB 15400, FI-00076 Espoo, Finland.; Siekkinen, M (corresponding author), Univ Helsinki, Dept Comp Sci, POB 4, Helsinki 00014, Finland.
EM matti.siekkinen@aalto.fi; teemu.kamarainen@helsinki.fi
RI Siekkinen, Matti/H-2447-2018
OI Kamarainen, Teemu/0000-0003-4685-6763
CR AMD, 2022, ADV MED FRAM AMF SDK
   Amish F, 2019, J REAL-TIME IMAGE PR, V16, P1559, DOI 10.1007/s11554-016-0664-1
   Boyce JM, 2021, P IEEE, V109, P1521, DOI 10.1109/JPROC.2021.3062590
   Ekong Sam, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10073, P246, DOI 10.1007/978-3-319-50832-0_24
   Garus P, 2022, IEEE T CIRC SYST VID, V32, P3250, DOI 10.1109/TCSVT.2021.3100006
   Gunkel SNB, 2021, MMSYS '21: PROCEEDINGS OF THE 2021 MULTIMEDIA SYSTEMS CONFERENCE, P65, DOI 10.1145/3458305.3459595
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Intel, 2022, INT MED SDK
   Koniaris B, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203193
   Lagarde Sebastien, 2018, PHOTOGRAMMETRY UNITY
   Lin JR, 2022, IEEE T MULTIMEDIA, V24, P1707, DOI 10.1109/TMM.2021.3070106
   Liu YP, 2015, LECT NOTES COMPUT SC, V9314, P442, DOI 10.1007/978-3-319-24075-6_43
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Nvidia, 2022, NVIDIA TENSORRT DCOU
   nvidia, 2022, Nvidia tensorrt
   Nvidia, 2022, NVIDIA VIDEO CODEC SDK-ENCODER Programming Guide
   Nvidia, 2015, DEPTH PREC VIS
   OneirosVR, 2019, ARCHVIZPRO
   Pece Fabrizio, 2011, EGVE EUROVR, P59
   Saldanha M, 2020, IEEE T CIRC SYST VID, V30, P850, DOI 10.1109/TCSVT.2019.2898122
   Sanchez G, 2020, IEEE T CIRCUITS-I, V67, P415, DOI 10.1109/TCSI.2019.2929977
   Shen LQ, 2018, IEEE T IMAGE PROCESS, V27, P4195, DOI 10.1109/TIP.2018.2837379
   Shi S, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2719921
   Stengel M, 2021, MMSYS '21: PROCEEDINGS OF THE 2021 MULTIMEDIA SYSTEMS CONFERENCE, P159, DOI 10.1145/3458305.3463379
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Unity, 2022, NAT START KIT 2
   Unity, 2022, UN REND STREAM
   Unity, 2021, UN ASS STOR ARCHVIZP, V6
   Unity, 2022, Unity Real-Time Development Platform
   Unity Technologies, 2021, NEW HDRP SCEN TEMPL
   Unity Technologies, 2020, UN COR PLATF
   Upchurch Paul, 2012, J GRAPHICS TOOLS, V16, P40
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wilson AD, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ACM ISS 2017), P100, DOI 10.1145/3132272.3134144
NR 35
TC 0
Z9 0
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2023
VL 19
IS 5
SU S
AR 174
DI 10.1145/3588440
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L0SK0
UT WOS:001020437100010
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Pan, YW
   Li, YH
   Yao, T
   Mei, T
AF Pan, Yingwei
   Li, Yehao
   Yao, Ting
   Mei, Tao
TI Bottom-up and Top-down Object Inference Networks for Image Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image captioning; attention mechanism; cross-modal reasoning
ID LANGUAGE
AB A bottom-up and top-down attention mechanism has led to the revolutionizing of image captioning techniques, which enables object-level attention for multi-step reasoning over all the detected objects. However, when humans describe an image, they often apply their own subjective experience to focus on only a few salient objects that areworthy of mention, rather than all objects in this image. The focused objects are further allocated in linguistic order, yielding the "object sequence of interest" to compose an enriched description. In this work, we present the Bottom-up and Top-down Object inference Network (BTO-Net), which novelly exploits the object sequence of interest as top-down signals to guide image captioning. Technically, conditioned on the bottom-up signals (all detected objects), an LSTM-based object inference module is first learned to produce the object sequence of interest, which acts as the top-down prior to mimic the subjective experience of humans. Next, both of the bottom-up and top-down signals are dynamically integrated via an attention mechanism for sentence generation. Furthermore, to prevent the cacophony of intermixed cross-modal signals, a contrastive learning-based objective is involved to restrict the interaction between bottom-up and top-down signals, and thus leads to reliable and explainable cross-modal reasoning. Our BTO-Net obtains competitive performances on the COCO benchmark, in particular, 134.1% CIDEr on the COCO Karpathy test split. Source code is available at https://github.com/YehLi/BTO-Net.
C1 [Pan, Yingwei; Li, Yehao; Yao, Ting; Mei, Tao] JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
RP Yao, T (corresponding author), JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
EM panyw.ustc@gmail.com; yehaoli.sysu@gmail.com; tingyao.ustc@gmail.com;
   tmei@live.com
RI Pan, Yingwei/T-7649-2019
OI Pan, Yingwei/0000-0002-4344-8898; Yao, Ting/0000-0001-7587-101X
FU National Key R&D Program of China [2020AAA0108600]
FX This work was supported by the National Key R&D Program of China under
   Grant No. 2020AAA0108600.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Banerjee S., 2005, P WORKSHOP INTRINSIC, P65
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Cornia M, 2019, PROC CVPR IEEE, P8299, DOI 10.1109/CVPR.2019.00850
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Devlin J, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P100
   Ding Y, 2022, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR52688.2022.00503
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   He C, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3292058
   Herdade S, 2019, ADV NEUR IN, V32
   Hou JY, 2020, AAAI CONF ARTIF INTE, V34, P10973
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Jiang XZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1265, DOI 10.1145/3394171.3413826
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma Diederik, 2015, P 3 INT C LEARN REPR
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Li X., 2020, P EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58577-8_8
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3799, DOI 10.1145/3474085.3478331
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Li YA, 2021, AAAI CONF ARTIF INTE, V35, P8518
   Li Yehao, 2022, P IEEE CVF C COMPUTE, P17990
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo J., 2022, arXiv
   Luo JJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5600, DOI 10.1145/3474085.3475703
   Mao Junhua, 2014, NIPSWORKSHOP DEEP LE
   Pan YW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P7070, DOI 10.1145/3503161.3551581
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Qin Y, 2019, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2019.00856
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rohrbach A, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4035
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shi Z., 2020, Association for Computational Linguistics, P7454
   Shi Z, 2021, ACL-IJCNLP 2021: THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 2, P269
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998
   Sutskever I, 2014, ADV NEUR IN, V27
   Toutanova K, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P252, DOI 10.3115/1073445.1073478
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wang J, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P940
   Wei HY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3439734
   Wu YH, 2016, Arxiv, DOI [arXiv:1609.08144, DOI 10.48550/ARXIV.1609.08144]
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2177, DOI 10.1109/ICCV48922.2021.00220
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2022, LECT NOTES COMPUT SC, V13685, P328, DOI 10.1007/978-3-031-19806-9_19
   Yao T, 2022, Arxiv, DOI arXiv:2207.04976
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu J, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107563
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
   Zhou YM, 2019, IEEE WINT CONF APPL, P283, DOI 10.1109/WACV.2019.00036
NR 70
TC 0
Z9 0
U1 8
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 161
DI 10.1145/3580366
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300010
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, JY
   Mou, LT
   Ma, L
   Huang, TJ
   Gao, W
AF Wang, Jingyao
   Mou, Luntian
   Ma, Lei
   Huang, Tiejun
   Gao, Wen
TI AMSA: Adaptive Multimodal Learning for Sentiment Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Sentiment analysis; multimodal fusion; self-adaptive mechanism;
   transformer; patch-based selection
ID FUSION
AB Efficient recognition of emotions has attracted extensive research interest, which makes new applications in many fields possible, such as human-computer interaction, disease diagnosis, service robots, and so forth. Although existingwork on sentiment analysis relying on sensors or unimodal methods performswell for simple contexts like business recommendation and facial expression recognition, it does far below expectations for complex scenes, such as sarcasm, disdain, and metaphors. In this article, we propose a novel two-stage multimodal learning framework, called AMSA, to adaptively learn correlation and complementarity between modalities for dynamic fusion, achieving more stable and precise sentiment analysis results. Specifically, a multiscale attention model with a slice positioning scheme is proposed to get stable quintuplets of sentiment in images, texts, and speeches in the first stage. Then a Transformer-based self-adaptive network is proposed to assign weights flexibly for multimodal fusion in the second stage and update the parameters of the loss function through compensation iteration. To quickly locate key areas for efficient affective computing, a patch-based selection scheme is proposed to iteratively remove redundant information through a novel loss function before fusion. Extensive experiments have been conducted on both machine weakly labeled and manually annotated datasets of self-made Video-SA, CMU-MOSEI, and CMU-MOSI. The results demonstrate the superiority of our approach through comparison with baselines.
C1 [Wang, Jingyao] Univ Chinese Acad Sci, Inst Software Chinese Acad Sci, Beijing, Peoples R China.
   [Mou, Luntian] Beijing Univ Technol, Inst Artificial Intelligence, Beijing Key Lab Multimedia & Intelligent Software, Beijing Inst Artificial Intelligence,Fac Informt, Beijing, Peoples R China.
   [Ma, Lei; Huang, Tiejun; Gao, Wen] Peking Univ, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; University of
   Chinese Academy of Sciences, CAS; Beijing University of Technology;
   Peking University
RP Wang, JY (corresponding author), Univ Chinese Acad Sci, Inst Software Chinese Acad Sci, Beijing, Peoples R China.
EM jingyao_wang0728@163.com; ltmou@pku.edu.cn; lei.ma@pku.edu.cn;
   tjhuang@pku.edu.cn; wgao@pku.edu.cn
RI Huang, Tiejun/D-6161-2011; Mou, Luntian/ACX-6553-2022
OI Mou, Luntian/0000-0002-1551-4448; Ma, Lei/0000-0001-6024-3854
FU Natural Science Foundation of China [61672068]
FX Luntian Mou work was supported in part by the Natural Science Foundation
   of China under Grant 61672068.
CR Ahmad K, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3199668
   [Anonymous], 2015, P 2015 C EMP METH NA, DOI [10.18653/v1/d15-1303, 10.18653/v1/D15-1303]
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Bai X, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108102
   Basiri ME, 2021, FUTURE GENER COMP SY, V115, P279, DOI 10.1016/j.future.2020.08.005
   Chauhan DS, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4351
   Chen JJ, 2021, CAMB J EDUC, V51, P327, DOI 10.1080/0305764X.2020.1831440
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI 10.1145/3136755.3136801
   Chen YX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P117, DOI 10.1145/3240508.3240533
   Chung Jessica Elan, 2011, 25 AAAI C ARTIFICIAL
   Church KW, 2017, NAT LANG ENG, V23, P155, DOI 10.1017/S1351324916000334
   Datar Mayur, 2006, AAAI 06
   Hazer-Rau D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082308
   Hu D., 2021, arXiv
   Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524
   Kanakaraj M, 2015, IEEE INT C SEMANT CO, P169, DOI 10.1109/ICOSC.2015.7050801
   Krishnamoorthy S, 2018, KNOWL INF SYST, V56, P373, DOI 10.1007/s10115-017-1134-1
   Kumar S, 2019, INFORM FUSION, V52, P41, DOI 10.1016/j.inffus.2018.11.001
   Li CZ, 2019, AAAI CONF ARTIF INTE, P996
   Li LH, 2017, AAAI CONF ARTIF INTE, P4133
   Li R. F., 2021, P 59 ANN M ASS COMPU, P6319, DOI [DOI 10.18653/V1/2021.ACL-LONG.494, 10.18653/v1/2021.acl-long.494]
   Li XD, 2014, KNOWL-BASED SYST, V69, P14, DOI 10.1016/j.knosys.2014.04.022
   Liu NN, 2013, COMPUT VIS IMAGE UND, V117, P493, DOI 10.1016/j.cviu.2012.10.009
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mai ND, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21155135
   Mai SJ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P481
   Mao QR, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5139
   Mazloom M., 2016, P ACM MULT, P197, DOI [10.1145/2964284.2967210, 10]
   Mou LT, 2021, EXPERT SYST APPL, V173, DOI 10.1016/j.eswa.2021.114693
   Mou Luntian, 2021, IEEE T MULTIMEDIA, V2021, P1
   Mun J, 2017, AAAI CONF ARTIF INTE, P4233
   Naseem U, 2020, FUTURE GENER COMP SY, V113, P58, DOI 10.1016/j.future.2020.06.050
   Ou XY, 2017, ACM T INTEL SYST TEC, V8, DOI 10.1145/3057733
   Palvanov A, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061343
   Pandeya YR, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21144927
   Perez-Rosas V., 2013, P ANN M ASS COMP LIN, P973
   Poria S, 2017, IEEE DATA MINING, P1033, DOI 10.1109/ICDM.2017.134
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Qiao TT, 2018, AAAI CONF ARTIF INTE, P7300
   Truong QT, 2019, AAAI CONF ARTIF INTE, P305
   Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401
   Rao YH, 2014, WORLD WIDE WEB, V17, P723, DOI 10.1007/s11280-013-0221-9
   ReadFace, 2020, READFACE WEBP 36KR
   Sennhauser L, 2018, Arxiv, DOI arXiv:1811.02611
   Shoumy NJ, 2020, J NETW COMPUT APPL, V149, DOI 10.1016/j.jnca.2019.102447
   Sindagi VA, 2018, PATTERN RECOGN LETT, V107, P3, DOI 10.1016/j.patrec.2017.07.007
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Nguyen TL, 2019, NEURAL NETWORKS, V118, P208, DOI 10.1016/j.neunet.2019.06.010
   Wane XY, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P2364
   Wang C, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108498
   Wang SF, 2015, IEEE T AFFECT COMPUT, V6, P410, DOI 10.1109/TAFFC.2015.2432791
   Wu J, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3271485
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xie YB, 2021, Arxiv, DOI arXiv:2012.12007
   Xu HY, 2020, Arxiv, DOI arXiv:1909.05645
   Xu N, 2019, AAAI CONF ARTIF INTE, P371
   Xu N, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2399, DOI 10.1145/3132847.3133142
   Yadav A, 2020, ARTIF INTELL REV, V53, P4335, DOI 10.1007/s10462-019-09794-5
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Yildirim Ezgi, 2015, Turkiye Bilisim Vakfi Bilgisayar Bilimleri ve Muhendisligi Dergisi, V7, P43
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   Yu WM, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3718
   Zadeh A, 2017, Arxiv, DOI [arXiv:1707.07250, DOI 10.48550/ARXIV.1707.07250]
   Zadeh A, 2016, Arxiv, DOI arXiv:1606.06259
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zhang YX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4595
   [赵国朕 Zhao Guozhen], 2016, [计算机研究与发展, Journal of Computer Research and Development], V53, P80
   Zhao SC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3363560
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
NR 71
TC 0
Z9 0
U1 13
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 135
DI 10.1145/3572915
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700010
DA 2024-07-18
ER

PT J
AU Zhang, YS
   Chen, N
   Qi, SR
   Xue, MF
   Hua, ZY
AF Zhang, Yushu
   Chen, Nuo
   Qi, Shuren
   Xue, Mingfu
   Hua, Zhongyun
TI Detection of Recolored Image by Texture Features in Chrominance
   Components
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image recoloring; forgery detection; texture; chrominance component;
   discriminability
ID COLOR TRANSFER; CLASSIFICATION
AB Image recoloring is an emerging editing technique that can change the color style of an image by modifying pixel values without altering the original image content. With the rapid proliferation of social network and image editing techniques, recolored images (RIs) have raised new security issues in society. Existing detection methods have good performance in detecting RIs for certain categories of recoloring techniques. However, the performance on the handcrafted recoloring scenario is still poor due to the influence of human prior knowledge. To deal with this problem, we explore a solution from the perspective of chrominance texture artifacts to improve the generalization ability. The results of the analysis show that natural images (NIs) and RIs have textural disparities in different color components, especially in the chrominance components (i.e., Cb, Cr, and H). Based on such new prior knowledge of statistical discriminability, we propose a feature set to capture texture features in chrominance components for identifying RIs. Extensive experimental results show that the proposed method can accurately identify RIs with certain categories of recoloring techniques, and outperforms existing methods in the scenario of handcrafted recoloring.
C1 [Zhang, Yushu; Chen, Nuo; Qi, Shuren; Xue, Mingfu] Nanjing Univ Aeronaut & Astro naut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Zhang, Yushu] Guilin Univ Elect Tech nology, Guangxi Key Lab Trusted Software, Guilin 541004, Peoples R China.
   [Hua, Zhongyun] Harbin Inst Technol Shenzhen, Coll Comp Sci & Technol, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology
RP Qi, SR (corresponding author), Nanjing Univ Aeronaut & Astro naut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM yushu@nuaa.edu.cn; yournuo@nuaa.edu.cn; shurenqi@nuaa.edu.cn;
   mingfu.xue@nuaa.edu.cn; huazhongyun@hit.edu.cn
RI Hua, Zhongyun/F-1887-2016; Wang, Jiachen/KFT-0161-2024; Qi,
   Shuren/JAX-8354-2023; lin, lin/KFB-9548-2024; ZHOU, YUE/KCJ-8790-2024;
   zhang, zheng/KHY-8870-2024; Chen, Nuo/JZD-0344-2024; ZHANG,
   JING/KHY-1073-2024; li, lan/KCJ-5061-2024; jing, wang/KCZ-2144-2024
OI zhang, yushu/0000-0001-8183-8435; Xue, Mingfu/0000-0003-2408-503X
FU Nanjing University of Aeronautics and Astronautics Graduate Research and
   Practice Innovation Program Project [xcxjh20211606]; National Natural
   Science Foundation of China [62072237]; Guangxi Key Laboratory of
   Trusted Software [KX202027]; Basic Research Program of Jiangsu Province
   [BK20201290]
FX This work was supported in part by the Nanjing University of Aeronautics
   and Astronautics Graduate Research and Practice Innovation Program
   Project under Grant No. xcxjh20211606, in part by the National Natural
   Science Foundation of China under Grant No. 62072237, in part by Guangxi
   Key Laboratory of Trusted Software under Grant No. KX202027, and in part
   by Basic Research Program of Jiangsu Province under Grant No.
   BK20201290.
CR Afifi M., 2019, EUROGRAPHICS SHORT P, P33
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   Beigpour S, 2011, IEEE I CONF COMP VIS, P327, DOI 10.1109/ICCV.2011.6126259
   Carvalho T, 2016, IEEE T INF FOREN SEC, V11, P720, DOI 10.1109/TIFS.2015.2506548
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Chen XW, 2014, PROC CVPR IEEE, pCP5, DOI 10.1109/CVPR.2014.365
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Cho J, 2017, IEEE COMPUT SOC CONF, P1058, DOI 10.1109/CVPRW.2017.143
   Cozzolino D, 2020, IEEE T INF FOREN SEC, V15, P144, DOI 10.1109/TIFS.2019.2916364
   Endo Y, 2016, COMPUT GRAPH FORUM, V35, P189, DOI 10.1111/cgf.12822
   Fan ZG, 2003, IEEE T IMAGE PROCESS, V12, P230, DOI 10.1109/TIP.2002.807361
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goljan M, 2014, IEEE INT WORKS INFOR, P185, DOI 10.1109/WIFS.2014.7084325
   Grognot M., 2015, 2015 40th International Conference on Infrared, Millimeter and Terahertz Waves (IRMMW-THz), P1, DOI 10.1109/IRMMW-THz.2015.7327838
   Guo YF, 2018, IEEE T INF FOREN SEC, V13, P1932, DOI 10.1109/TIFS.2018.2806926
   Gupta R. K., 2012, P 20 ACM INT C MULT, P369, DOI DOI 10.1145/2393347.2393402
   He MM, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3292482
   Ho JS, 2010, IEEE INT CON MULTI, P1475, DOI 10.1109/ICME.2010.5582951
   Iizuka S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925974
   Ironi R., 2005, RENDERING TECHNIQUES, P201
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Lee J, 2020, VISUAL COMPUT, V36, P2129, DOI 10.1007/s00371-020-01921-6
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li HD, 2020, SIGNAL PROCESS, V174, DOI 10.1016/j.sigpro.2020.107616
   Li YJ, 2017, Arxiv, DOI [arXiv:1705.08086, DOI 10.48550/ARXIV.1705.08086, 10.48550/ARXIV.1705.08086]
   Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28
   Lin A.S., 2017, arXiv
   Lin S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461988
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Luo WQ, 2007, INT CONF ACOUST SPEE, P217
   O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pitie A., 2007, 4 EUR C VIS MED PROD, DOI DOI 10.1049/CP:20070055
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Pitié F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Quan Weize, 2019, ARXIV
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ren RY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3506853
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun Hongyi, 2022, IEEE T MULTIMEDIA
   Swaminathan A, 2008, IEEE T INF FOREN SEC, V3, P101, DOI 10.1109/TIFS.2007.916010
   Tai YW, 2005, PROC CVPR IEEE, P747
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Verdoliva L, 2020, IEEE J-STSP, V14, P910, DOI 10.1109/JSTSP.2020.3002101
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wen LY, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183518
   Wu HW, 2022, IEEE T MULTIMEDIA, V24, P4016, DOI 10.1109/TMM.2021.3111491
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Yan YY, 2019, IEEE T INF FOREN SEC, V14, P5, DOI 10.1109/TIFS.2018.2834155
   Yerushalmy I, 2011, INT J COMPUT VISION, V92, P71, DOI 10.1007/s11263-010-0403-1
   Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913
   Yu Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499026
   Yu YX, 2021, J VIS COMMUN IMAGE R, V80, DOI 10.1016/j.jvcir.2021.103295
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhuo L, 2018, ASIAPAC SIGN INFO PR, P733, DOI 10.23919/APSIPA.2018.8659761
NR 60
TC 4
Z9 4
U1 2
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 121
DI 10.1145/3571076
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300021
DA 2024-07-18
ER

PT J
AU Park, JH
   Kim, S
   Lee, JC
   Ko, JH
AF Park, Jae Hyun
   Kim, Sanghoon
   Lee, Joo Chan
   Ko, Jong Hwan
TI Scalable Color Quantization for Task-centric Image Compression
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Color quantization; deep learning; precision scalable
AB Conventional image compression techniques targeted for the perceptual quality are not generally optimized for classification tasks using deep neural networks (DNNs). To compress images for DNN inference tasks, recent studies have proposed task-centric image compression methods with quantization techniques optimized for DNN inference. Among them, color quantization was proposed to reduce the amount of data per pixel by limiting the number of distinct colors (color space) in an image. However, quantizing images into various color space sizes requires training and inference of multiple DNNs, each of which is dedicated to each color space. To overcome this limitation, we propose a scalable color quantization method, where images with variable color space sizes can be extracted from a master image generated by a single DNN model. This scalability is enabled by weighted color grouping that constructs a color palette using critical color components for the classification task. We also propose an adaptive training method that can jointly optimize imageswith various color-space sizes. The results show that the proposed method supports dynamic changes of the color space size between 1-6 bit color space per pixel, while even increasing the inference accuracy at a low bit precision up to 20.2% and 46.6% compared to other task- and human-centric color quantizations, respectively.
C1 [Park, Jae Hyun; Lee, Joo Chan] Sungkyunkwan Univ, Dept Artificial Intelligence, Suwon, South Korea.
   [Kim, Sanghoon] Sungkyunkwan Univ, Dept Semicond & Display Engn, Suwon, South Korea.
   [Ko, Jong Hwan] Sungkyunkwan Univ, Coll Informat & Commun Engn, Suwon, South Korea.
C3 Sungkyunkwan University (SKKU); Sungkyunkwan University (SKKU);
   Sungkyunkwan University (SKKU)
RP Ko, JH (corresponding author), Sungkyunkwan Univ, Coll Informat & Commun Engn, Suwon, South Korea.
EM xoxc4565@skku.edu; wha933@g.skku.edu; maincold2@skku.edu; jhko@skku.edu
OI Park, Jae Hyun/0000-0002-1424-372X; Lee, Joo Chan/0000-0001-9398-9089
FU Ministry of Science and ICT (MSIT) of Korea, under the Institute of
   Information and Communication Technology Planning Evaluation (IITP)
   [IITP-2019-0-00421, IITP-2021-0-02052, IITP-2020-0-01821,
   IITP-2021-0-02068]
FX This research was supported by the Ministry of Science and ICT (MSIT) of
   Korea, under the Institute of Information and Communication Technology
   Planning Evaluation (IITP) grants for the AI Graduate School program
   (IITP-2019-0-00421), Information Technology Research Center (ITRC)
   program (IITP-2021-0-02052), ICT Creative Consilience program
   (IITP-2020-0-01821), and Artificial Intelligence Innovation Hub program
   (IITP-2021-0-02068).
CR Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   [Anonymous], 2022, ISOIEC JTC 1SC29WG1
   Burger W., 2016, DIGITAL IMAGE PROCES, DOI [10.1007/978-1-4471-6684-9, DOI 10.1007/978-1-4471-6684-9]
   Chamain LD, 2021, IEEE DATA COMPR CONF, P163, DOI 10.1109/DCC50243.2021.00024
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Deng YN, 1999, ISCAS '99: PROCEEDINGS OF THE 1999 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 4, P21, DOI 10.1109/ISCAS.1999.779933
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Gao Changsheng, 2021, IEEE T MULTIM
   Gervautz M, 1988, NEW TRENDS COMPUTER, P219, DOI DOI 10.1007/978-3-642-83492-920
   GOLDBERG N, 1991, IMAGE VISION COMPUT, V9, P303, DOI 10.1016/0262-8856(91)90035-N
   GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heckbert P., 1982, Computer Graphics, V16, P297, DOI 10.1145/965145.801294
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hou Yunzhong, 2020, P IEEE CVF C COMP VI, P10116
   Houle G., 1986, GLOBECOM '86: IEEE Global Telecommunications Conference. Communications Broadening Technology Horizons. Conference Record (Cat. No.86CH2298-9), P1138
   Ioffe S, 2015, Arxiv, DOI [arXiv:1502.03167, DOI 10.48550/ARXIV.1502.03167]
   Jinyoung Choi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P309, DOI 10.1007/978-3-030-58565-5_19
   KIEFER J, 1952, ANN MATH STAT, V23, P462, DOI 10.1214/aoms/1177729392
   Kim SH, 2021, IEEE ACCESS, V9, P119418, DOI 10.1109/ACCESS.2021.3108449
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Le Ya, 2015, CS 231N, P3
   Liu Z, 2018, DES AUT CON, DOI 10.1145/3195970.3196022
   Loshchilov Ilya, 2016, arXiv
   ORCHARD MT, 1991, IEEE T SIGNAL PROCES, V39, P2677, DOI 10.1109/78.107417
   Patwa N, 2020, IEEE IMAGE PROC, P1281, DOI [10.1109/ICIP40778.2020.9191247, 10.1109/icip40778.2020.9191247]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith LN, 2019, PROC SPIE, V11006, DOI 10.1117/12.2520589
   Wang Q, 2020, IEEE SIGNAL PROC LET, V27, P1150, DOI 10.1109/LSP.2020.3004967
   Weber Maurice, 2021, Pattern Recognition. 42nd DAGM German Conference, DAGM GCPR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12544), P130, DOI 10.1007/978-3-030-71278-5_10
   Weber M, 2020, Arxiv, DOI arXiv:1910.03472
   WU XL, 1992, ACM T GRAPHIC, V11, P348, DOI 10.1145/146443.146475
   Xiang ZG, 1997, ACM T GRAPHIC, V16, P260, DOI 10.1145/256157.256159
   Yu HC, 2021, Arxiv, DOI arXiv:1911.07346
   Yu JH, 2019, IEEE I CONF COMP VIS, P1803, DOI 10.1109/ICCV.2019.00189
   Zhang YC, 2020, Arxiv, DOI arXiv:2002.07136
NR 38
TC 3
Z9 3
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 82
DI 10.1145/3551389
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300007
DA 2024-07-18
ER

PT J
AU Mai, SJ
   Xing, SL
   He, JX
   Zeng, Y
   Hu, HF
AF Mai, Sijie
   Xing, Songlong
   He, Jiaxuan
   Zeng, Ying
   Hu, Haifeng
TI Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph
   Convolution and Graph Pooling
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Graph pooling; multimodal graph; multimodal sequence analysis; sentiment
   analysis
ID NEURAL-NETWORK; SENTIMENT; FUSION; LANGUAGE; SPEECH; TEXT
AB Multimodal sequence analysis aims to draw inferences from visual, language, and acoustic sequences. A majority of existing works focus on the aligned fusion of three modalities to explore inter-modal interactions, which is impractical in real-world scenarios. To overcome this issue, we seek to focus on analyzing unaligned sequences, which is still relatively underexplored and also more challenging. We propose Multimodal Graph, whose novelty mainly lies in transforming the sequential learning problem into graph learning problem. The graph-based structure enables parallel computation in time dimension (as opposed to recurrent neural network) and can effectively learn longer intra- and inter-modal temporal dependency in unaligned sequences. First, we propose multiple ways to construct the adjacency matrix for sequence to perform sequence to graph transformation. To learn intra-modal dynamics, a graph convolution network is employed for each modality based on the defined adjacency matrix. To learn inter-modal dynamics, given that the unimodal sequences are unaligned, the commonly considered word-level fusion does not pertain. To this end, we innovatively devise graph pooling algorithms to automatically explore the associations between various time slices from different modalities and learn high-level graph representation hierarchically. Multimodal Graph outperforms state-of-the-art models on three datasets under the same experimental setting.
C1 [Mai, Sijie; Xing, Songlong; He, Jiaxuan; Zeng, Ying; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
EM maisj@mail2.sysu.edu.cn; xingslong@mail2.sysu.edu.cn;
   hejx25@mail2.sysu.edu.cn; zengy268@mail2.sysu.edu.cn;
   huhaif@mail.sysu.edu.cn
OI He, Jiaxuan/0000-0002-1419-3670; Xing, Songlong/0000-0002-2734-1695;
   Mai, Sijie/0000-0001-9763-375X; Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [62076262]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62076262.
CR Bai SJ, 2018, Arxiv, DOI [arXiv:1803.01271, DOI 10.48550/ARXIV.1803.01271]
   Bai SJ, 2019, P INT C LEARNING REP
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Casanova Arantxa, 2018, INT C LEARNING REPRE
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI 10.1145/3136755.3136801
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Franceschi L, 2019, PR MACH LEARN RES, V97
   Fu SC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3412846
   Gao Difei, 2020, CVPR, P12746
   Gkoumas D, 2021, INFORM FUSION, V66, P184, DOI 10.1016/j.inffus.2020.09.005
   GOUDREAU MW, 1994, IEEE T NEURAL NETWOR, V5, P511, DOI 10.1109/72.286928
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou M, 2019, ADV NEUR IN, V32
   Huang FR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3388861
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Kampman O, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P606
   Kingma D, 2014, ICLR P, V2014, P1
   Kipf TN, 2016, ARXIV
   Misraa AK, 2020, Arxiv, DOI arXiv:2010.01666
   Li QC, 2021, INFORM FUSION, V65, P58, DOI 10.1016/j.inffus.2020.08.006
   Liang PP, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P150
   Liang PP, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1569
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Mai SJ, 2022, IEEE T AFFECT COMPUT, V13, P320, DOI 10.1109/TAFFC.2020.3000510
   Mai SJ, 2021, AAAI CONF ARTIF INTE, V35, P4294
   Mai SJ, 2021, IEEE-ACM T AUDIO SPE, V29, P1424, DOI 10.1109/TASLP.2021.3068598
   Mai SJ, 2020, AAAI CONF ARTIF INTE, V34, P164
   Mai SJ, 2020, IEEE T MULTIMEDIA, V22, P122, DOI 10.1109/TMM.2019.2925966
   Mai SJ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P481
   Mai Sijie, 2022, IEEE Transactions on Affective Computing
   Micheli A, 2009, IEEE T NEURAL NETWOR, V20, P498, DOI 10.1109/TNN.2008.2010350
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   OLSON DR, 1977, HARVARD EDUC REV, V47, P257, DOI 10.17763/haer.47.3.8840364413869005
   Pandey A, 2019, INT CONF ACOUST SPEE, P6875, DOI [10.1109/ICASSP.2019.8683634, 10.1109/icassp.2019.8683634]
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.178, 10.1109/ICDM.2016.0055]
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Tang JH, 2019, IEEE T PATTERN ANAL, V41, P2027, DOI 10.1109/TPAMI.2019.2906603
   Tsai YHH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1823, DOI 10.18653/v1/2020.emnlp-main.143
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang SP, 2017, IEEE T MULTIMEDIA, V19, P1454, DOI 10.1109/TMM.2017.2663324
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wöllmer M, 2013, IEEE INTELL SYST, V28, P46, DOI 10.1109/MIS.2013.34
   Xu K., 2018, 7 INT C LEARNING REP
   Yang JN, 2021, Arxiv, DOI arXiv:2010.11985
   Yang KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P521, DOI 10.1145/3394171.3413690
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Ying R, 2018, ADV NEUR IN, V31
   Yu F., 2015, ARXIV
   Yuan H., 2020, P INT C LEARNING REP
   Yuan J, 2008, J ACOUST SOC AM, V124, P2078, DOI 10.1121/1.2968700
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438
   Zhang MH, 2018, ADV NEUR IN, V31
   Zhao SC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3363560
NR 70
TC 3
Z9 3
U1 6
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 54
DI 10.1145/3542927
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000004
DA 2024-07-18
ER

PT J
AU Le, TNH
   Yeh, CK
   Lin, YC
   Lee, TY
AF Thi-Ngoc-Hanh Le
   Yeh, Chih-Kuo
   Lin, Ying-Chi
   Lee, Tong-Yee
TI Animating Still Natural Images Using Warping
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Animation; still images; cycle warping; preserve-curve-warping
ID FLUID ANIMATION; VIDEO; PICTURES
AB From a single still image, a looping video could be generated by imparting subtle motion to objects in the image. The results are a hybrid of photography and video. They contain gentle motion in some objects, while the rest of the image remains still. Existing techniques are successful in animating such images. However, there are still some drawbacks that need to be investigated, such as too-large computation time necessary to retrieve the matched videos or the challenges of controlling the desired motion not only in terms of a single region but also in terms of consistency in regions. In this work, we address these issues by proposing an interactive system with a novel warping method. The key idea of our approach is to utilize user's annotations to impart motion to certain objects. With two proposed phases in terms of preserve-curve-warping and cycle warping, a looping video is generated. We demonstrate the effectiveness of our method via various experimental challenging results and evaluations. We show that with a simple and lightweight method, our system is able to deal with animating a still image's problems and results in realistic motion and appealing videos. In addition, using our proposed system, it is easy to create plausible animation using simple user annotations without referencing the video database or machine learning models and allows ordinary users with minimal expertise to produce compelling results.
C1 [Thi-Ngoc-Hanh Le; Lin, Ying-Chi; Lee, Tong-Yee] Natl Cheng Kung Univ, Tainan, Taiwan.
   [Yeh, Chih-Kuo] Zhaoqing Univ, Sch Comp Sci & Software, Zhaoqing, Peoples R China.
C3 National Cheng Kung University; Zhaoqing University
RP Le, TNH (corresponding author), Natl Cheng Kung Univ, Tainan, Taiwan.
EM ngochanh.le1987@gmail.com; simpson.ycg@gmail.com; yclin0017@gmail.com;
   tonylee@mail.ncku.edu.tw
RI Yeh, Chih-Kuo/JBS-2228-2023
FU Ministry of Science and Technology, Taiwan, Republic of China
   [111-2221-E-006-112-MY3, 110-2221-E-006-135-MY3]; Key Area Research
   Program of Universities in Guangdong Province (Nature science), China
   [2020KQNCX095]
FX The authors would like to thank the reviewers for the many constructive
   comments that help improve the paper. This work was supported in part by
   the Ministry of Science and Technology (under nos.
   111-2221-E-006-112-MY3 and 110-2221-E-006-135-MY3), Taiwan, Republic of
   China and the Key Area Research Program of Universities in Guangdong
   Province (Nature science), China (under no. 2020KQNCX095).
CR [Anonymous], 2004, ADOBE PHOTOSHOP
   Arfken GB., 1999, MATH METHODS PHYS CO, V7
   Bhat KS, 2004, ACM T GRAPHIC, V23, P360, DOI 10.1145/1015706.1015729
   Chuang YY, 2005, ACM T GRAPHIC, V24, P853, DOI 10.1145/1073204.1073273
   Endo Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356523
   Eng Eirik, 1996, LINUX J, V1996, P31
   FREEMAN WT, 1991, COMP GRAPH, V25, P27, DOI 10.1145/127719.122721
   Gao RH, 2018, PROC CVPR IEEE, P5937, DOI 10.1109/CVPR.2018.00622
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   Guennebaud G., 2010, Eigen
   Gui Y, 2012, J ZHEJIANG U-SCI C, V13, P510, DOI 10.1631/jzus.C1100342
   Holynski Aleksander, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P5806, DOI 10.1109/CVPR46437.2021.00575
   Kim KR, 2019, IEEE I CONF COMP VIS, P273, DOI 10.1109/ICCV.2019.00036
   Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15
   Lai YC, 2017, IEEE T VIS COMPUT GR, V23, P2535, DOI 10.1109/TVCG.2016.2622269
   Liao Z, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461950
   Lin CY, 2019, MULTIMED TOOLS APPL, V78, P6637, DOI 10.1007/s11042-018-6332-7
   Logacheva Elizaveta, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P256, DOI 10.1007/978-3-030-58592-1_16
   Ma WC, 2017, PROC CVPR IEEE, P4636, DOI 10.1109/CVPR.2017.493
   Mortensen E. N., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P191, DOI 10.1145/218380.218442
   Mottaghi R, 2016, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2016.383
   Okabe M, 2018, VISUAL COMPUT, V34, P347, DOI 10.1007/s00371-016-1337-6
   Okabe M, 2011, COMPUT GRAPH FORUM, V30, P1973, DOI 10.1111/j.1467-8659.2011.02062.x
   Okabe M, 2009, COMPUT GRAPH FORUM, V28, P677, DOI 10.1111/j.1467-8659.2009.01408.x
   Prashnani E, 2017, COMPUT GRAPH FORUM, V36, P303, DOI 10.1111/cgf.12940
   Press W., 1989, Numerical Recipes in Pascal: the Art of Scientific Computing, V1
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Weisstein E.W., 2005, MATHWORLD A WOLFRAM
   Wikipedia contributors, 2020, LINEAR INTERPOLATION
   Yagi T, 2018, PROC CVPR IEEE, P7593, DOI 10.1109/CVPR.2018.00792
   Zhang Jin, 2007, VISUALIZATION INFORM, V23
NR 31
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 4
DI 10.1145/3511894
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400004
DA 2024-07-18
ER

PT J
AU Zhang, XL
   Shen, ML
   Li, XM
   Wang, XJ
AF Zhang, Xianlin
   Shen, Mengling
   Li, Xueming
   Wang, Xiaojie
TI AABLSTM: A Novel Multi-task Based CNN-RNN Deep Model for Fashion
   Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Attribute detection; landmark localization; multi-task mechanism;
   CNN-RNN; deep architecture
AB With the rapid growth of online commerce and fashion-related applications, visual clothing analysis and recognition has become a hotspot in computer vision. In this paper, we propose a novel AABLSTM network, which is based on deep CNN-RNN, to solve the visual fashion analysis of clothing category classification, attribute detection, and landmark localization. The designed fashion model is leveraged with the multi-task driven mechanism as follows: firstly, a bidirectional LSTM (Bi-LSTM) branch is proposed for efficiently mining the semantic association between related attributes so as to improve the precision of clothing category classification and attribute detection; then, an imitated hourglass sub-network of "down-up sampling" is constructed for boosting the accuracy of fashion landmark localization; and finally, a specially designed multi-loss function is constructed to better optimize the network training. Extensive experimental results on large-scale fashion datasets demonstrate the superior performance of our approach.
C1 [Zhang, Xianlin] Beijing Univ Posts & Telecommun, Sch Digital Media & Amp Design Art, 10 Xitu Cheng Rd, Beijing 100876, Peoples R China.
   [Shen, Mengling] China Unicom Online Informat Technol Co Ltd, Beijing, Peoples R China.
   [Li, Xueming] Beijing Univ Posts & Telecommun, Sch Digital Media & Amp Design Art, Beijing Key Lab Network Syst & Network Culture, 10 Xitu Cheng Rd, Beijing 100876, Peoples R China.
   [Wang, Xiaojie] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, 10 Xitu Cheng Rd, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beijing University of
   Posts & Telecommunications; Beijing University of Posts &
   Telecommunications
RP Zhang, XL (corresponding author), Beijing Univ Posts & Telecommun, Sch Digital Media & Amp Design Art, 10 Xitu Cheng Rd, Beijing 100876, Peoples R China.
EM zxlin@bupt.edu.cn; shenm19@chinaunicom.cn; lixm@bupt.edu.cn;
   xjwang@bupt.edu.cn
RI WU, SHAN/KGM-5484-2024; Wang, Xiaoman/JYP-1144-2024; Liu,
   Joanne/JRY-7564-2023; wang, yi/KBB-3614-2024; li, yurong/JMQ-8540-2023;
   Wang, Yuchen/JPW-9345-2023; Yu, Shicheng/KHU-3059-2024; Li,
   Jiawei/JOJ-9277-2023; chen, chen/JGD-3057-2023; Zhang, Ge/KGL-7634-2024;
   Xiaojie, Wang/T-5052-2019; cheng, chen/JHS-9462-2023; Jia,
   Li/JVN-3095-2024; zhang, jingxing/KCY-4726-2024
OI , Xianlin/0000-0003-3905-2062
FU NVIDIA Corporation
FX We gratefully acknowledge the support of the NVIDIA Corporation with the
   donation of the TITAN X GPU used for our deep learning.
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   Al-Halah Z, 2017, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2017.50
   [Anonymous], 2013, ICMR
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen Q, 2015, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR.2015.7299169
   Chen T, 2018, IEEE T PATTERN ANAL, V40, P2522, DOI 10.1109/TPAMI.2017.2756936
   Chen X., 2014, Advances in neural information processing systems, P1736, DOI DOI 10.1109/CVPR.2018.00742
   Corbière C, 2017, IEEE INT CONF COMP V, P2268, DOI 10.1109/ICCVW.2017.266
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fang HS, 2018, AAAI CONF ARTIF INTE, P6821
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong YC, 2014, Arxiv, DOI arXiv:1312.4894
   Gupta S, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3436494
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu J, 2020, IEEE T PATTERN ANAL, V42, P2011, DOI 10.1109/TPAMI.2019.2913372
   Huang CQ, 2019, IEEE T CYBERNETICS, V49, P3744, DOI 10.1109/TCYB.2018.2850745
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang SQ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231738
   Jin JR, 2016, INT C PATT RECOG, P2452, DOI 10.1109/ICPR.2016.7900004
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li PZ, 2019, IEEE IMAGE PROC, P3038, DOI [10.1109/icip.2019.8803394, 10.1109/ICIP.2019.8803394]
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Liu JY, 2019, LECT NOTES COMPUT SC, V11131, P30, DOI 10.1007/978-3-030-11015-4_4
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Liu ZW, 2016, LECT NOTES COMPUT SC, V9906, P229, DOI 10.1007/978-3-319-46475-6_15
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126
   Menglin Jia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P316, DOI 10.1007/978-3-030-58452-8_19
   Qi SY, 2018, PROC CVPR IEEE, P5899, DOI 10.1109/CVPR.2018.00618
   Ramakrishna V, 2014, LECT NOTES COMPUT SC, V8690, P33, DOI 10.1007/978-3-319-10605-2_3
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Takagi M, 2017, IEEE INT CONF COMP V, P2247, DOI 10.1109/ICCVW.2017.263
   Tompson J, 2014, ADV NEUR IN, V27
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Xianwang., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM'11, P1353
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Yan SJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P172, DOI 10.1145/3123266.3123276
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang SY, 2021, IEEE T CIRC SYST VID, V31, P1016, DOI 10.1109/TCSVT.2020.2990531
   Zhang SY, 2018, NEUROCOMPUTING, V282, P98, DOI 10.1016/j.neucom.2017.12.027
   Zhao RW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231739
   Ziegler T, 2020, IEEE INT CONF AUTON, P81, DOI [10.1109/ICARSC49921.2020.9096071, 10.1109/icarsc49921.2020.9096071]
NR 61
TC 1
Z9 1
U1 4
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 19
DI 10.1145/3519029
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400019
DA 2024-07-18
ER

PT J
AU Hao, SJ
   Han, X
   Guo, YR
   Wang, M
AF Hao, Shijie
   Han, Xu
   Guo, Yanrong
   Wang, Meng
TI Decoupled Low-Light Image Enhancement
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image enhancement; low-light images; deep neural networks; decoupling
   degeneration
ID CONTRAST ENHANCEMENT; REPRESENTATION; FUSION
AB The visual quality of photographs taken under imperfect lightness conditions can be degenerated by multiple factors, e.g., low lightness, imaging noise, color distortion, and so on. Current low-light image enhancement models focus on the improvement of low lightness only, or simply deal with all the degeneration factors as a whole, therefore leading to sub-optimal results. In this article, we propose to decouple the enhancement model into two sequential stages. The first stage focuses on improving the scene visibility based on a pixel-wise non-linear mapping. The second stage focuses on improving the appearance fidelity by suppressing the rest degeneration factors. The decoupled model facilitates the enhancement in two aspects. On the one hand, the whole low-light enhancement can be divided into two easier subtasks. The first one only aims to enhance the visibility. It also helps to bridge the large intensity gap between the low-light and normal-light images. In this way, the second subtask can be described as the local appearance adjustment. On the other hand, since the parameter matrix learned from the first stage is aware of the lightness distribution and the scene structure, it can be incorporated into the second stage as the complementary information. In the experiments, our model demonstrates the state-of-the-art performance in both qualitative and quantitative comparisons, compared with other low-light image enhancement models. In addition, the ablation studies also validate the effectiveness of our model in multiple aspects, such as model structure and loss function.
C1 [Hao, Shijie; Han, Xu; Guo, Yanrong; Wang, Meng] Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, 193 Tunxi Rd, Hefei, Peoples R China.
   [Hao, Shijie; Han, Xu; Guo, Yanrong; Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, 193 Tunxi Rd, Hefei, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology
RP Hao, SJ (corresponding author), Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, 193 Tunxi Rd, Hefei, Peoples R China.; Hao, SJ (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, 193 Tunxi Rd, Hefei, Peoples R China.
EM hfut.hsj@gmail.com; xuhan@mail.hfut.edu.cn; yrguo@hfut.edu.cn;
   eric.mengwang@gmail.com
RI Wang, Meng/ITR-8699-2023
FU National Nature Science Foundation of China [62172137, 62072152,
   61725203]; Fundamental Research Funds for the Central Universities
   [PA2020GDKC0023, PA2019GDZC0095]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant No. 62172137, 62072152, and 61725203,
   and in part by the Fundamental Research Funds for the Central
   Universities under Grant No. PA2020GDKC0023 and PA2019GDZC0095.
CR Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Dong X, 2011, IEEE INT CON MULTI
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Gehler P., 2011, P ADV NEUR INF PROC, P765
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Hao SJ, 2019, MULTIMED TOOLS APPL, V78, P3817, DOI 10.1007/s11042-018-6257-1
   Hu YM, 2017, PROC CVPR IEEE, P330, DOI 10.1109/CVPR.2017.43
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Li Chongyi, 2021, ARXIVCSCV210410729
   Li Fei, 2021, IET IMAGE PROCESS
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li M, 2021, IET IMAGE PROCESS, V15, P2020, DOI 10.1049/ipr2.12173
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2011, CONF REC ASILOMAR C, P723, DOI 10.1109/ACSSC.2011.6190099
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sandoub G, 2021, IET IMAGE PROCESS, V15, P1759, DOI 10.1049/ipr2.12148
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Venkatanath N, 2015, NATL CONF COMMUN
   Vonikakis V, 2008, IET IMAGE PROCESS, V2, P19, DOI 10.1049/iet-ipr:20070012
   Wang QH, 2016, IEEE IMAGE PROC, P4077, DOI 10.1109/ICIP.2016.7533126
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P3461, DOI 10.1109/TIP.2021.3062184
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Zhang Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P582, DOI 10.1145/3240508.3240595
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
NR 45
TC 10
Z9 10
U1 4
U2 66
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 92
DI 10.1145/3498341
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Stacchio, L
   Angeli, A
   Lisanti, G
   Calanca, D
   Marfia, G
AF Stacchio, Lorenzo
   Angeli, Alessia
   Lisanti, Giuseppe
   Calanca, Daniela
   Marfia, Gustavo
TI Toward a Holistic Approach to the Socio-historical Analysis of
   Vernacular Photos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Family photo albums; historical images; image dating; socio-historical
   context classification; multimedia application
AB Although one of the most popular practices in photography since the end of the 19th century, an increase in scholarly interest in family photo albums dates back to the early 1980s. Such collections of photos may reveal sociological and historical insights regarding specific cultures and times. They are, however, in most cases scattered among private homes and only available on paper or photographic film, thus making their collection and analysis by historians, socio-cultural anthropologists, and cultural theorists very cumbersome. Computer-based methodologies could aid such a process in various ways, speeding up the cataloging step, for example, with the use of modern computer vision techniques. We here investigate such an approach, introducing the design and development of a multimedia application that may automatically catalog vernacular pictures drawn from family photo albums. To this aim, we introduce the IMAGO dataset, which is composed of photos belonging to family albums assembled at the University of Bologna's Rimini campus since 2004. Exploiting the proposed application, IMAGO has offered the opportunity of experimenting with photos taken between the years 1845 and 2009. In particular, it has been possible to estimate their socio-historical content, i.e., the dates and contexts of the images, without resorting to any other sources of information. Exceeding our initial expectations, such an approach has revealed its merit not only in terms of performance but also in terms of the foreseeable implications for the benefit of socio-historical research. To the best of our knowledge, this contribution is among the few that move along this path at the intersection of socio-historical studies, multimedia computing, and artificial intelligence.
C1 [Stacchio, Lorenzo] Univ Bologna, Dept Life Qual Studies, Rimini, Italy.
   [Angeli, Alessia; Lisanti, Giuseppe] Univ Bologna, Dept Comp Sci & Engn, Bologna, Italy.
   [Calanca, Daniela; Marfia, Gustavo] Univ Bologna, Dept Arts, Rimini, Italy.
C3 University of Bologna; University of Bologna; University of Bologna
RP Stacchio, L (corresponding author), Univ Bologna, Dept Life Qual Studies, Rimini, Italy.
EM stacchio2@unibo.it; alessia.angeli2@unibo.it; giuseppe.lisanti@unibo.it;
   daniela.calanca@unibo.it; gustavo.marfia@unibo.it
RI Marfia, Gustavo/D-1347-2010; Stacchio, Lorenzo/JEO-9407-2023; Lisanti,
   Giuseppe/AAG-8699-2020
OI Stacchio, Lorenzo/0000-0002-9341-7651; Lisanti,
   Giuseppe/0000-0002-0785-9972
FU University of Bologna; Alma Attrezzature 2017 grant; AEFFE S.p.a.;
   Golinelli Foundation
FX This work was supported by the University of Bologna with the Alma
   Attrezzature 2017 grant and by AEFFE S.p.a. and the Golinelli Foundation
   with the funding of two Ph.D. scholarships.
CR Amazon, 2021, AM SAGEMAKER GROUND
   [Anonymous], 2016, P IEEE WINT C APPL C, DOI [DOI 10.1109/WACV.2016, 10.1109/WACV.2016]
   [Anonymous], 2015, Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCV-W)
   Arnold T, 2019, DIGIT SCHOLARSH HUM, V34, P3, DOI 10.1093/llc/fqz013
   Barba S., 2011, SURVEYS 3D CATALOGUI
   Bentein K, 2015, SYMB OSLO, V89, P104, DOI 10.1080/00397679.2015.1095012
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Borcoci E., 2010, 2010 Third International Conference on Communication Theory, Reliability, and Quality of Service (CTRQ), P162, DOI 10.1109/CTRQ.2010.35
   Bosi L., 2014, Methodological practices in social movement research, P117
   Bourdieu P, 1996, THEOR CULT SOC, V13, P19, DOI 10.1177/026327696013003002
   Cabrera MA, 2001, HIST THEORY, V40, P82, DOI 10.1111/0018-2656.00183
   Cabrera MiguelA., 2004, POSTSOCIAL HIST INTR
   Calanca D., 2004, RIV STORIA STORIOGRA, V5, P203
   Calanca D, 2006, STORIA FUTURO, V12, P134
   Calanca D., 2011, ALMATOURISM J TOUR C, V2, P1
   Calanca D., 2005, STORIA FUTURO, P8
   Chen JJ, 2018, IEEE T BIG DATA, V4, P148, DOI 10.1109/TBDATA.2018.2839919
   Chen X., 2020, IEEE T MULTIMEDIA
   Clemens E., 2002, Methods of social movement research, P201
   Coburn E, 2010, IFLA J-INT FED LIBR, V36, P16, DOI 10.1177/0340035209359561
   Criscenti L., 2005, ITALIA NOVECENTO FOT
   Culurciello E., 2021, NEURAL NETWORK ARCHI
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Enns KJ., 2015, J AGR ED, V56, P69, DOI [DOI 10.5032/JAE.2015, DOI 10.5032/JAE.2015.03069, 10.5032/jae.2015.03069]
   Fernando B, 2014, IEEE IMAGE PROC, P2589, DOI 10.1109/ICIP.2014.7025524
   Franzosi R, 1998, INT REV SOC HIST, V43, P81, DOI 10.1017/S002085900011510X
   Google, 2021, PLATF DAT LAB SERV
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He KM, 2015, Arxiv, DOI [arXiv:1512.03385, 10.48550/arxiv.1512.03385]
   Huang G, 2018, Arxiv, DOI [arXiv:1608.06993, DOI 10.48550/ARXIV.1608.06993]
   Phan TH, 2020, Arxiv, DOI [arXiv:2006.01413, 10.48550/arXiv.2006.01413]
   Lemley J, 2017, IEEE CONSUM ELECTR M, V6, P48, DOI 10.1109/MCE.2016.2640698
   Li YH, 2020, IEEE T MULTIMEDIA, V22, P1285, DOI 10.1109/TMM.2019.2939711
   Lincoln M., 2020, PREPRINT, DOI [10.1184/R1/12791807.v2, DOI 10.1184/R1/12791807.V2]
   Mitman Gregg., 2016, DOCUMENTING WORLD FI
   MoMA, 2020, Vernacular photography
   Müller E, 2017, LECT NOTES COMPUT SC, V10193, P619, DOI 10.1007/978-3-319-56608-5_57
   Nguyen T., 2018, YOLO FACE IMPLEMENTA
   Palermo F, 2012, LECT NOTES COMPUT SC, V7577, P499
   Paris S., 2007, ACM SIGGRAPH 2007 courses on - SIGGRAPH'07 p, P1, DOI [DOI 10.1145/1281500.1281602, 10.1145/1281500.1281602]
   Peres MichaelR., 2014, The Concise Focal Encyclopedia of Photography
   Qiu XH, 2014, 2014 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ENSEMBLE LEARNING (CIEL), P21
   Raghu M, 2021, ADV NEUR IN, V34
   Rainer B, 2017, IEEE T MULTIMEDIA, V19, P849, DOI 10.1109/TMM.2016.2629761
   Redmon J, 2019, YOLO: Real Time Object Detection
   Roccetti Marco, 2020, GoodTechs '20: Proceedings of the 6th EAI International Conference on Smart Objects and Technologies for Social Good, P216, DOI 10.1145/3411170.3411254
   Rosner D, 2014, COMMUN ACM, V57, P82, DOI 10.1145/2602695.2602701
   Sandbye M, 2014, J AESTHET CULT, V6, DOI 10.3402/jac.v6.25419
   Schreiber C, 2014, EDUC RES-UK, V56, P137, DOI 10.1080/00131881.2014.898911
   Scott John, 2009, A dictionary of sociology
   Sejnowski TJ, 2018, DEEP LEARNING REVOLUTION, P1
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Serafinelli E., 2018, Digital Life on Instagram: New Social Communication Photography
   Sorcinelli P., 2004, RIV STORIA STORIOGRA, V5, P200
   Szegedy C, 2015, Arxiv, DOI arXiv:1512.00567
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaccaro Federico, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P311, DOI 10.1145/3372278.3390732
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XT, 2018, Arxiv, DOI [arXiv:1809.00219, DOI 10.48550/ARXIV.1809.00219]
   Wevers M, 2020, DIGIT SCHOLARSH HUM, V35, P194, DOI 10.1093/llc/fqy085
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Zhang K, 2019, Image Restoration Toolbox
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang W, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3279952
NR 66
TC 2
Z9 2
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 146
DI 10.1145/3507918
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800016
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Morillo, P
   Navarro-Pérez, JJ
   Orduña, JM
   Fernández, M
AF Morillo, Pedro
   Navarro-Perez, Jose J.
   Orduna, Juan M.
   Fernandez, Marcos
TI Evaluation of an Intervention Program Based on Mobile Apps to Learn
   Sexism Prevention in Teenagers
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Ambivalent sexism; mobile apps; real user evaluation; usability outcomes
ID AMBIVALENT SEXISM; DATING VIOLENCE; SAY NO; INVENTORY; EDUCATION;
   BEHAVIOR; SCALES; GIRL; RAPE
AB The fight against sexism is nowadays one of the flagship social movements in western countries. Adolescence is a crucial period, and some empirical studies have focused on the socialization of teenagers, proving that the socialization with the surrounding environment prevent sexist practices. In a previous work, we developed and tested the effectiveness of a mobile app, called Liad@s, with the goals of helping teenagers to prevent sexism and build healthy couple relationships. In this article, we carry out a study where (using a real situation) we compare the effectiveness of the Liad@s app in front of traditional interventions like a workshop about sexism for teenagers. Also, we evaluate the usability of the app and the user satisfaction with this application. In this study, our primary hypothesis is that the effectiveness of using our mobile application, in terms of knowledge acquired about sexism, would be at least as good as attending the workshop. Our secondary hypothesis is that the user satisfaction with the mobile application would be higher than the one with the workshop, causing a preference for the app. The results of this study show significant differences in learning appeared between gender and between the two different procedures when separately evaluating the data collected from both hostile sexism (HS) and benevolent sexism (BS) questionnaires. These results validate our primary hypothesis. Also, most of the population under study preferred the mobile app in front of the traditional workshop, validating also our secondary hypothesis.
C1 [Morillo, Pedro; Navarro-Perez, Jose J.; Orduna, Juan M.; Fernandez, Marcos] Univ Valencia, Avda Univ S-N, Valencia 46100, Spain.
C3 University of Valencia
RP Morillo, P (corresponding author), Univ Valencia, Avda Univ S-N, Valencia 46100, Spain.
EM Pedro.Morillo@uv.es; J.Javier.Navarro@uv.es; Juan.Orduna@uv.es;
   Marcos.Fernandez@uv.es
RI Navarro-Perez, Jose-Javier JJNP/J-7711-2016; Fernández,
   Marcos/JDC-9198-2023
OI Orduna, Juan M./0000-0002-2932-0214; Fernandez Marin,
   Marcos/0000-0002-0307-0392
FU Spanish MCIU; EU ERDF programs [RTI2018-098156-B-C55]; Valencian
   Regional government [GV2017/208]
FX This work has been supported by Spanish MCIU and EU ERDF programs under
   grant RTI2018-098156-B-C55. It has been also supported by the Valencian
   Regional government under grant GV2017/208.
CR Alhusen J, 2015, J TECHNOL HUMAN SERV, V33, P263, DOI 10.1080/15228835.2015.1037414
   Allport GW., 1954, NATURE PREJUDICE
   Anderson T.W., 2011, International Encyclopedia of Statistical Science, V1, P52, DOI [10.1007/978-3-642-04898-2_118, DOI 10.1007/978-3-642-04898-2_118]
   Begue Laurent, 2017, Front Psychol, V8, P466, DOI 10.3389/fpsyg.2017.00466
   Bendixen M, 2017, SCAND J PSYCHOL, V58, P541, DOI 10.1111/sjop.12392
   Bohner G, 2010, SEX ROLES, V62, P568, DOI 10.1007/s11199-009-9665-x
   Bowey JT, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1530, DOI 10.1145/3025453.3025563
   Brayboy LM, 2017, J PEDIATR ADOL GYNEC, V30, P23, DOI 10.1016/j.jpag.2016.06.011
   Creswell J.W., 2012, ED RES PLANNING COND, DOI DOI 10.1017/CBO9781107415324.004
   de la Barrera U, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0250384
   de Lemus S, 2008, INT J CLIN HLTH PSYC, V8, P537
   Emmons RA, 2003, J PERS SOC PSYCHOL, V84, P377, DOI 10.1037/0022-3514.84.2.377
   Gabbiadini A, 2019, J SPORT MED PHYS FIT, V59, P407, DOI [10.23736/S0022-4707.18.08260-9, 10.23736/s0022-4707.18.08260-9]
   Fernández EG, 2016, REV LAT COMUN SOC, V71, P818, DOI 10.4185/RLCS-2016-1122
   Glick P, 1996, J PERS SOC PSYCHOL, V70, P491, DOI 10.1037/0022-3514.70.3.491
   Gorton K., 2007, Feminist Theory, V8, P333, DOI DOI 10.1177/1464700107082369
   Greeno CG, 2002, FAM PROCESS, V41, P733, DOI 10.1111/j.1545-5300.2002.00733.x
   Hammond SP, 2018, BRIT J SOC WORK, V48, P2058, DOI 10.1093/bjsw/bcx144
   Harrison R., 2013, J INTERACTION SCI, V1, P1, DOI [10.1186/2194-0827-1-1, DOI 10.1186/2194-0827-1-1]
   Hornbak Kasper, 2011, Foundations and Trends in Human-Computer Interaction, V5, P299, DOI 10.1561/1100000043
   Hutson-Comeaux SL, 2002, SEX ROLES, V47, P1, DOI 10.1023/A:1020657301981
   Jouriles EN, 2017, J ADOLESCENT HEALTH, V61, P115, DOI 10.1016/j.jadohealth.2017.01.020
   Kahne JE, 2008, AM EDUC RES J, V45, P738, DOI 10.3102/0002831208316951
   Katsaliaki K, 2015, SIMULAT GAMING, V46, P647, DOI 10.1177/1046878114552166
   Kirk Roger E., 2011, SIMPLE RANDOM SAMPLE, P1328, DOI [10. 1007/978- 3-642- 04898-2_518, DOI 10.1007/978-3-642-04898-2_518]
   Kitzinger C, 1999, DISCOURSE SOC, V10, P293, DOI 10.1177/0957926599010003002
   Kreager DA, 2013, J MARRIAGE FAM, V75, P565, DOI 10.1111/jomf.12018
   Lansford JE, 2012, AGGRESSIVE BEHAV, V38, P298, DOI 10.1002/ab.21433
   Leaper C, 2014, ADV CHILD DEV BEHAV, V47, P189, DOI 10.1016/bs.acdb.2014.04.001
   Malinov YD, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P501, DOI 10.1109/VR50410.2021.00074
   MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095
   McKellar K, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON DIGITAL HEALTH (DH'17), P43, DOI 10.1145/3079452.3079497
   Mendez-Lopez M, 2022, ANAT SCI EDUC, V15, P535, DOI 10.1002/ase.2089
   Navarro-Pérez JJ, 2020, PSYCHOSOC INTERV, V29, P59, DOI 10.5093/pi2020a3
   Navarro-Pérez JJ, 2018, CONVERGENCIA, P119, DOI 10.29101/crcs.v25i76.4442
   Navarro-Pérez JJ, 2019, REV PSICODIDACT, V24, P9, DOI 10.1016/j.psicod.2018.07.002
   O'Byrne R, 2008, J COMMUNITY APPL SOC, V18, P168, DOI 10.1002/casp.922
   Plan International Ltd, 2019, SHEB
   POLLATSEK A, 1995, J EXP PSYCHOL LEARN, V21, P785, DOI 10.1037/0278-7393.21.3.785
   Ramiro-Sánchez T, 2018, INT J CLIN HLTH PSYC, V18, P245, DOI 10.1016/j.ijchp.2018.04.002
   Robertson J, 2012, COMMUN ACM, V55, P6, DOI 10.1145/2160718.2160721
   Semenzin S, 2020, SOC MEDIA SOC, V6, DOI 10.1177/2056305120984453
   SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591
   Sjoberg DIK, 2005, IEEE T SOFTWARE ENG, V31, P733, DOI 10.1109/TSE.2005.97
   Valkenburg PM, 2007, DEV PSYCHOL, V43, P267, DOI 10.1037/0012-1649.43.2.267
   Verheijen GP, 2018, AGGRESSIVE BEHAV, V44, P257, DOI 10.1002/ab.21748
   West JH, 2017, JMIR MHEALTH UHEALTH, V5, DOI 10.2196/mhealth.7410
   Witmer BG, 2005, PRESENCE-TELEOP VIRT, V14, P298, DOI 10.1162/105474605323384654
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
NR 49
TC 1
Z9 1
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 45
DI 10.1145/3471139
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 0A0YT
UT WOS:000773689400002
DA 2024-07-18
ER

PT J
AU Xu, QL
   Del Molino, AG
   Lin, J
   Fang, F
   Subbaraju, V
   Li, LY
   Lim, JH
AF Xu, Qianli
   Del Molino, Ana Garcia
   Lin, Jie
   Fang, Fen
   Subbaraju, Vigneshwaran
   Li, Liyuan
   Lim, Joo-Hwee
TI Lifelog Image Retrieval Based on Semantic Relevance Mapping
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Lifelog; image retrieval; summarization; semantic mapping
AB Lifelog analytics is an emerging research area with technologies embracing the latest advances in machine learning, wearable computing, and data analytics. However, state-of-the-art technologies are still inadequate to distill voluminous multimodal lifelog data into high quality insights. In this article, we propose a novel semantic relevance mapping (SRM) method to tackle the problem of lifelog information access. We formulate lifelog image retrieval as a series of mapping processes where a semantic gap exists for relating basic semantic attributes with high-level query topics. The SRM serves both as a formalism to construct a trainable model to bridge the semantic gap and an algorithm to implement the training process on real-world lifelog data. Based on the SRM, we propose a computational framework of lifelog analytics to support various applications of lifelog information access, such as image retrieval, summarization, and insight visualization. Systematic evaluations are performed on three challenging benchmarking tasks to show the effectiveness of our method.
C1 [Xu, Qianli; Del Molino, Ana Garcia; Lin, Jie; Fang, Fen; Li, Liyuan; Lim, Joo-Hwee] ASTAR, Inst Infocomm Res, 1 Fusionopolis Way,21-01 Connexis South Tower, Singapore 138632, Singapore.
   [Del Molino, Ana Garcia] ByteDance AI Lab, Singapore, Singapore.
   [Subbaraju, Vigneshwaran] A STAR Human Centr Artificial Intelligence Progra, 1 Fusionopolis Way 16-16 Connexis North Tower, Singapore 138632, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R)
RP Xu, QL (corresponding author), ASTAR, Inst Infocomm Res, 1 Fusionopolis Way,21-01 Connexis South Tower, Singapore 138632, Singapore.
EM qxu@i2r.a-star.edu.sg; a.g.delmolino@gmail.com; lin-j@i2r.a-star.edu.sg;
   fang_fen@i2r.a-star.edu.sg; vigneshwaran_subbaraju@scei.a-star.edu.sg;
   lyli@i2r.a-star.edu.sg; joohwee@i2r.a-star.edu.sg
OI Subbaraju, Vigneshwaran/0000-0002-4276-5939
CR Amar C. B., 2018, ICCE COMP GRAPH VIS, P1
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Androutsopoulos I., 1995, Natural Language Engineering, V1, P29, DOI [DOI 10.1017/S135132490000005X, 10.1017/S0269888900005476]
   [Anonymous], 2005, P 21 C UNCERTAINTY A, DOI DOI 10.3115/1690219.1690283
   Bappaditya, 2017, CLEF WORKING NOTES, P1
   Berant J., 2013, P 2013 C EMPIRICAL M, P1533
   Bolaños M, 2015, IEEE INT CONF MULTI
   Bolaños M, 2017, IEEE T HUM-MACH SYST, V47, P77, DOI 10.1109/THMS.2016.2616296
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chang SF, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2491844
   Chen Hsin-Hsi, 2018, WORKING NOTES CLEF 2
   Chevallet, 2016, 12 NTCIR C EV INF AC, P361
   Choe EK, 2015, IEEE COMPUT GRAPH, V35, P28, DOI 10.1109/MCG.2015.51
   Dang-Nguyen D.T., 2017, WORKING NOTES CLEF 2, P15
   del Molino A. G., 2017, NTCIR 13, P33
   del Molino AG, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P10, DOI 10.1145/3240508.3240624
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dimiccoli M, 2017, COMPUT VIS IMAGE UND, V155, P55, DOI 10.1016/j.cviu.2016.10.005
   Doherty Aiden R., 2008, 2008 Ninth International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS), P20, DOI 10.1109/WIAMIS.2008.32
   Doherty AR, 2010, SENSORS-BASEL, V10, P1423, DOI [10.3390/100301423, 10.3390/s100301423]
   Ezzarka, 2018, WORKING NOTES CLEF 2
   Garcia, 2018, WORKING NOTES CLEF 2
   Gurrin, 2017, WIND DESIGN CIVIL ST, P1
   Gurrin, 2016, P 12 NTCIR C EV INF, P386
   Gurrin, 2017, NTCIR 13
   Gurrin, 2018, WORKING NOTES CLEF 2
   Gurrin Cathal, 2014, Foundations and Trends in Information Retrieval, V8, P5, DOI 10.1561/1500000033
   Gurrin C, 2017, PROC NTCIR, P1
   Gurrin Cathal, 2018, WORKING NOTES CLEF 2
   Harvey M, 2016, PERVASIVE MOB COMPUT, V27, P14, DOI 10.1016/j.pmcj.2015.12.002
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ionescu Bogdan, 2017, WORKING NOTES CLEF 2
   Jones Gareth J. F., 2010, ACM AH 10
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Lee ML, 2007, ASSETS'07: PROCEEDINGS OF THE NINTH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P131
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu DR, 2013, ALLERGY ASTHMA CL IM, V9, DOI 10.1186/1710-1492-9-30
   Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350
   Meyer J, 2014, IEEE PERVAS COMPUT, V13, P10, DOI 10.1109/MPRV.2014.25
   Noreen S, 2016, FRONT PSYCHOL, V7, DOI 10.3389/fpsyg.2016.00379
   Ong YS, 2019, IEEE T EM TOP COMP I, V3, P411, DOI 10.1109/TETCI.2019.2928344
   Papapanagiotou V, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2790230
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roig G, 2013, IEEE I CONF COMP VIS, P2312, DOI 10.1109/ICCV.2013.287
   Sellen A, 2010, COMMUN ACM, V53, P70, DOI 10.1145/1735223.1735243
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Do TT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3314051
   Toda H., 2017, NTCIR 13, P12
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Xu, 2017, NTCIR 13, P28
   Xu QL, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-58076-6
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 58
TC 2
Z9 2
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 92
DI 10.1145/3446209
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400016
DA 2024-07-18
ER

PT J
AU Li, JS
   Zhao, J
   Lang, CY
   Li, YD
   Wei, YC
   Guo, GD
   Sim, T
   Yan, SC
   Feng, JS
AF Li, Jianshu
   Zhao, Jian
   Lang, Congyan
   Li, Yidong
   Wei, Yunchao
   Guo, Guodong
   Sim, Terence
   Yan, Shuicheng
   Feng, Jiashi
TI Multi-human Parsing with a Graph-based Generative Adversarial Model
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human parsing; multi-human parsing; human-centric image analysis;
   generative adversarial networks; graph convolution network
AB Human parsing is an important task in human-centric image understanding in computer vision and multimedia systems. However, most existing works on human parsing mainly tackle the single-person scenario, which deviates from real-world applications where multiple persons are present simultaneously with interaction and occlusion. To address such a challenging multi-human parsing problem, we introduce a novel multi-human parsing model named MI-I-Parser, which uses a graph-based generative adversarial model to address the challenges of close-person interaction and occlusion in multi-human parsing. To validate the effectiveness of the new model, we collect a new dataset named Multi-Human Parsing (MHP), which contains multiple persons with intensive person interaction and entanglement. Experiments on the new MHP dataset and existing datasets demonstrate that the proposed method is effective in addressing the multi-human parsing problem compared with existing solutions in the literature.
C1 [Li, Jianshu; Sim, Terence] Natl Univ Singapore, 13 Comp Dr, Singapore 117417, Singapore.
   [Feng, Jiashi] Natl Univ Singapore, 4 Engn Dr 3, Singapore 117583, Singapore.
   [Zhao, Jian] Inst North Elect Equipment, Beijing Haidian Dist, Peoples R China.
   [Lang, Congyan; Li, Yidong] Beijing Jiaotong Univ, Beijing 100044, Peoples R China.
   [Wei, Yunchao] Univ Technol Sydney, 15 Broadway, Ultimo, NSW 2007, Australia.
   [Guo, Guodong] Baidu IDL, Beijing 100085, Peoples R China.
   [Yan, Shuicheng] Yitu Technol, Beijing, Peoples R China.
C3 National University of Singapore; National University of Singapore;
   Beijing Jiaotong University; University of Technology Sydney
RP Li, JS (corresponding author), Natl Univ Singapore, 13 Comp Dr, Singapore 117417, Singapore.; Zhao, J (corresponding author), Inst North Elect Equipment, Beijing Haidian Dist, Peoples R China.
EM jianshu@u.nus.edu; zhaojian90@u.nus.edu; cylang@bjtu.edu.cn;
   ydli@bjtu.edu.cn; wychao1987@gmail.com; guodong.guo@mail.wvu.edu;
   tsim@comp.nus.edu.sg; eleyans@nus.edu.sg; elefjia@nus.edu.sg
RI Yan, Shuicheng/HCI-1431-2022; zhao, jian/HTM-3920-2023; Feng,
   Jiashi/AGX-6209-2022
FU National Research Foundation Singapore; National Science Foundation of
   China [62006244]; NUS [R-263-000-C08-133, R-263-000-C67-646, ECRA
   R-263-000-C87133]; MOE Tier-I [R-263-000-C21-112]
FX The work of Jianshu Li was partially supported by National Research
   Foundation Singapore. The work of Jian Zhao was partially supported by
   the National Science Foundation of China 62006244. The work of Jiashi
   Feng was partially supported by NUS startup R-263-000-C08-133, MOE
   Tier-I R-263-000-C21-112, NUS IDS R-263-000-C67-646 and ECRA
   R-263-000-C87133.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2017, ARXIV170802551
   [Anonymous], P 30 C NEURAL INFORM
   [Anonymous], 2016, ARXIV161105424
   [Anonymous], 2015, ARXIV150902636
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Arnab A, 2017, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2017.100
   Arnab A, 2016, LECT NOTES COMPUT SC, V9906, P524, DOI 10.1007/978-3-319-46475-6_33
   Bojchevski A., 2018, ARXIV180300816
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Chu X, 2015, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2015.383
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Defferrard M., 2016, P 30 INT C NEURAL IN, V29, P3844
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468
   Gadde R, 2016, LECT NOTES COMPUT SC, V9905, P597, DOI 10.1007/978-3-319-46448-0_36
   Gan C, 2016, AAAI CONF ARTIF INTE, P3487
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang H, 2017, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR.2017.366
   King DB, 2015, ACS SYM SER, V1214, P1
   Kipf T.N., 2016, BAYESIAN DEEP LEARNI
   Kipf T. N., 2017, 8 INT C LEARN REPR, P1
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248
   Ladicky Lubor, 2011, BRIT MACH VIS C, P12
   Lafferty John D, 2001, CONDITIONAL RANDOM F
   Li JS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P45, DOI 10.1145/3240508.3240515
   Li Qizhu, 2017, BMVC
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Li YH, 2015, 2015 INTERNATIONAL WORKSHOP ON PATTERN RECOGNITION IN NEUROIMAGING (PRNI) 2015, P41, DOI 10.1109/PRNI.2015.18
   Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin Z., 2017, PROC INT C LEARN REP
   Liu S, 2015, PROC CVPR IEEE, P1419, DOI 10.1109/CVPR.2015.7298748
   Manessi Franco, 2017, ARXIV170406199
   Neven D, 2017, FAST SCENE UNDERSTAN
   Radford A., 2015, ARXIV151106434
   Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471
   Sun, 2015, P INT C ADV NEUR INF, P91
   Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201
   Wang HW, 2018, AAAI CONF ARTIF INTE, P2508
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Zhang N, 2015, PROC CVPR IEEE, P4804, DOI 10.1109/CVPR.2015.7299113
   Zhanpeng Zhang CCLPingLuo, 2016, ARXIV160906426V2
   Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509
NR 57
TC 14
Z9 14
U1 4
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 29
DI 10.1145/3418217
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200009
OA Bronze
DA 2024-07-18
ER

PT J
AU Abu Ul Fazal, M
   Ferguson, S
   Johnston, A
AF Abu Ul Fazal, Muhammad
   Ferguson, Sam
   Johnston, Andrew
TI Evaluation of Information Comprehension in Concurrent Speech-based
   Designs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Concurrent speech; audio streams; auditory display; concurrent
   streaming; voice-based interaction; concurrent speech-based information
   comprehension; audio spatial location; intermittent & continuous speech
   presentation; comprehension depth; speech perception; listening
   comprehension
ID COCKTAIL PARTY; DISCOURSE
AB In human-computer interaction, particularly in multimedia delivery, information is communicated to users sequentially, whereas users are capable of receiving information from multiple sources concurrently. This mismatch indicates that a sequential mode of communication does not utilise human perception capabilities as efficiently as possible. This article reports an experiment that investigated various speech-based (audio) concurrent designs and evaluated the comprehension depth of information by comparing comprehension performance across several different formats of questions (main/detailed, implied/stated). The results showed that users, besides answering the main questions, were also successful in answering the implied questions, as well as the questions that required detailed information, and that the pattern of comprehension depth remained similar to that seen to a baseline condition, where only one speech source was presented. However, the participants answered more questions correctly that were drawn from the main information, and performance remained low where the questions were drawn from detailed information. The results are encouraging to explore the concurrent methods further for communicating multiple information streams efficiently in human-computer interaction, including multimedia.
C1 [Abu Ul Fazal, Muhammad; Ferguson, Sam; Johnston, Andrew] Univ Technol, Fac Engn & IT, Creat & Cognit Studios, Sch Comp Sci, Sydney, NSW 2007, Australia.
C3 University of Technology Sydney
RP Abu Ul Fazal, M (corresponding author), Univ Technol, Fac Engn & IT, Creat & Cognit Studios, Sch Comp Sci, Sydney, NSW 2007, Australia.
EM fazalsidhu@gmail.com; Samuel.Ferguson@uts.edu.au;
   Andrew.Johnston@uts.edu.au
RI Johnston, Andrew J/M-6858-2013
OI Abu ul Fazal, Muhammad/0000-0001-7811-8146; , Dr MOHAMMED
   FAZAL/0009-0007-3654-1377
FU School of Computer Science, Faculty of Engineering and IT, University of
   Technology, Sydney, Australia
FX This research was supported by the School of Computer Science, Faculty
   of Engineering and IT, University of Technology, Sydney, Australia.
CR Fazal MA, 2019, PROCEEDINGS OF 3RD INTERNATIONAL CONFERENCE ON INFORMATION SYSTEM AND DATA MINING (ICISDM 2019), P143, DOI 10.1145/3325917.3325922
   Fazal MA, 2018, 2018 CONFERENCE ON INTERACTION WITH SOUND (AUDIO MOSTLY): SOUND IN IMMERSION AND EMOTION (AM'18), DOI 10.1145/3243274.3243284
   Abu ul Fazal M, 2017, ADV INTELL SYST, V506, P101, DOI 10.1007/978-3-319-43982-2_9
   Abu ul Fazal Muhammad, 2019, THESIS
   Abu ul Fazal Muhammad, 2018, P 145 CONV AUD ENG S
   [Anonymous], 1999, Auditory Scene Analysis: The Perceptual Organization of Sound, DOI DOI 10.7551/MITPRESS/1486.001.0001
   Aydelott J, 2015, J ACOUST SOC AM, V138, P964, DOI 10.1121/1.4927410
   Aydelott J, 2012, LANG COGNITIVE PROC, V27, P1108, DOI 10.1080/01690965.2011.589735
   Beattie David, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3130901
   Beattie D, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P451, DOI 10.1145/2750858.2807519
   Best V, 2006, J ACOUST SOC AM, V120, P1506, DOI 10.1121/1.2234849
   Biatov K., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P211
   Brayda Luca., 2015, Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers, P957
   Bregman AS., 1994, AUDITORY SCENE ANAL
   Brookshire RH., 1997, The Discourse Comprehension Test - 2nd ed
   Brungart D S., 2005, ACM Transactions on Applied Perception, V2, P430, DOI [10.1145/1101530.1101538, DOI 10.1145/1101530.1101538]
   Chernyshov G, 2016, IEEE INT SYM WRBL CO, P58, DOI 10.1145/2971763.2971789
   Church K, 2014, ACM T COMPUT-HUM INT, V21, DOI 10.1145/2552193
   Conway ARA, 2001, PSYCHON B REV, V8, P331, DOI 10.3758/BF03196169
   Csapó A, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543586
   Dix A., 2003, HUM FAC ER
   Doherty J, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487273
   Elhilali M, 2008, J ACOUST SOC AM, V124, P3751, DOI 10.1121/1.3001672
   Feng W.-c., 2012, Proceedings of the 22nd International Workshop on Network and Operating System Support for Digital Audio and Video, P57
   Griffiths TD, 2004, NAT REV NEUROSCI, V5, P887, DOI 10.1038/nrn1538
   Guerreiro J., 2016, ACM SIGACCESS Accessibility and Computing, P12, DOI DOI 10.1145/2961108.2961110
   Guerreiro J, 2016, ACM T ACCESS COMPUT, V8, DOI 10.1145/2822910
   Guerreiro Joao, 2013, P 10 INT CROSS DISC, P8
   Guerreiro Joao., 2014, Proceedings of the 16th International ACM SIGACCESS Conference on Computers Accessibility ASSETS '14, P169, DOI DOI 10.1145/2661334.2661367
   Gulliver SR, 2006, ACM T MULTIM COMPUT, V2, P241, DOI 10.1145/1201730.1201731
   Hinde Alistair F., 2016, THESIS
   Hines A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1173, DOI 10.1145/2647868.2655025
   Iyer N., 2013, P M ACOUSTICS, V19, P050158, DOI [10.1121/1.4800507, DOI 10.1121/1.4800507]
   Kortum P, 2008, MORG KAUF SER INTER, P1, DOI 10.1016/B978-0-12-374017-5.00001-8
   LAWSON EA, 1966, Q J EXP PSYCHOL, V18, P260, DOI 10.1080/14640746608400038
   Li GP, 2005, Seventh International Conference on Electronic Commerce, Vols 1 and 2, Selected Proceedings, P66
   Matassa A., 2015, Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers - UbiComp'15, P923, DOI DOI 10.1145/2800835.2806201
   McDermott JH, 2009, CURR BIOL, V19, pR1024, DOI 10.1016/j.cub.2009.09.005
   McGookin D.K., 2004, ACM Transactions on Applied Perception (TAP), V1, P130, DOI DOI 10.1145/1024083.1024087
   Moffat D, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3165287
   MORAY N, 1959, Q J EXP PSYCHOL, V11, P56, DOI 10.1080/17470215908416289
   Nelson Cowan, 1995, OXFORD PSYCHOL SER, V26
   Obermeyer JA, 2018, AM J SPEECH-LANG PAT, V27, P392, DOI 10.1044/2017_AJSLP-16-0200
   Patel D, 2018, PROCEEDINGS OF CHINESE CHI 2018: SIXTH INTERNATIONAL SYMPOSIUM OF CHINESE CHI (CHINESE CHI 2018), P160, DOI 10.1145/3202667.3202696
   Qudah B, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823754
   Rivenez M, 2006, J ACOUST SOC AM, V119, P4027, DOI 10.1121/1.2190162
   Sato D, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2769
   Schmandt Chris., 1995, C COMPANION HUMAN FA, P218
   Vazquez Alvarez Y., 2010, Proceedings of the 12th International Conference on Human Computer Interaction with Mobile Devices and Services, P253
   Welland RJ, 2002, J SPEECH LANG HEAR R, V45, P1175, DOI 10.1044/1092-4388(2002/095)
   WILLIAMS SM, 1994, SFI S SCI C, V18, P95
   Wu TT, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2983642
   Xu CS, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198303
   Zimmermann R., 2008, PROCEEDING 16 ACM IN, P299
NR 54
TC 0
Z9 0
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 129
DI 10.1145/3409463
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800013
DA 2024-07-18
ER

PT J
AU Tanwar, VK
   Raman, B
   Rajput, AS
   Bhargava, R
AF Tanwar, Vishesh Kumar
   Raman, Balasubramanian
   Rajput, Amitesh Singh
   Bhargava, Rama
TI <i>CryptoLesion</i>: A Privacy-preserving Model for Lesion Segmentation
   Using Whale Optimization over Cloud
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Privacy preserving; cloud computing; permutation ordered binary number
   system; melanoma detection; whale optimization
ID IMAGE ENCRYPTION SCHEME; ALGORITHM; SYSTEM
AB The low-cost, accessing flexibility, agility, and mobility of cloud infrastructures have attracted medical organizations to store their high-resolution data in encrypted form. Besides storage, these infrastructures provide various image processing services for plain (non-encrypted) images. Meanwhile, the privacy and security of uploaded data depend upon the reliability of the service provider(s). The enforcement of laws towards privacy policies in health-care organizations, for not disclosing their patient's sensitive and private medical information, restrict them to utilize these services. To address these privacy concerns for melanoma detection, we propose CryptoLesion, a privacy-preserving model for segmenting lesion region using whale optimization algorithm (WOA) over the cloud in the encrypted domain (ED). The user's image is encrypted using a permutation ordered binary number system and a random stumble matrix. The task of segmentation is accomplished by dividing an encrypted image into a pre-defined number of clusters whose optimal centroids are obtained by WOA in ED, followed by the assignment of each pixel of an encrypted image to the unique centroid. The qualitative and quantitative analysis of CryptoLesion is evaluated over publicly available datasets provided in The International Skin Imaging Collaboration Challenges in 2016, 2017, 2018, and PH2 dataset. The segmented results obtained by CryptoLesion are found to be comparable with the winners of respective challenges. CryptoLesion is proved to be secure from a probabilistic viewpoint and various cryptographic attacks. To the best of our knowledge, CryptoLesion is first moving towards the direction of lesion segmentation in ED.
C1 [Tanwar, Vishesh Kumar; Bhargava, Rama] Indian Inst Technol Roorkee, Dept Math, Room 209, Roorkee, Uttar Pradesh, India.
   [Tanwar, Vishesh Kumar; Rajput, Amitesh Singh] Indian Inst Technol Roorkee, Dept Comp & Engn, Machine Vis Lab, Room 216, Roorkee, Uttar Pradesh, India.
   [Raman, Balasubramanian; Rajput, Amitesh Singh] Indian Inst Technol Roorkee, Dept Comp & Engn, Room 227, Roorkee, Uttar Pradesh, India.
   [Rajput, Amitesh Singh] BITS Pilani, Dept Comp Sci & Informat Syst, Room 6121-X,New Acad Block, Pilani, Rajasthan, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Roorkee; Indian
   Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee; Birla Institute of Technology & Science
   Pilani (BITS Pilani)
RP Tanwar, VK (corresponding author), Indian Inst Technol Roorkee, Dept Math, Room 209, Roorkee, Uttar Pradesh, India.; Tanwar, VK (corresponding author), Indian Inst Technol Roorkee, Dept Comp & Engn, Machine Vis Lab, Room 216, Roorkee, Uttar Pradesh, India.
EM vtanwar@ma.iitr.ac.in; balarfcs@iitr.ac.in; arajput@cs.iitr.ac.in
RI Tanwar, Dr. Vishesh Kumar/AFJ-6309-2022; Tanwar, Dr. Vishesh
   Kumar/AGI-9932-2022
OI Tanwar, Dr. Vishesh Kumar/0000-0002-4802-7582; 
FU University Grant Commission, INDIA [21/12/2014(ii)EU-V, 2121440593]
FX This research was supported by University Grant Commission, INDIA
   reference grant number: 21/12/2014(ii)EU-V, 2121440593.
CR Abd El Aziz M, 2017, EXPERT SYST APPL, V83, P242, DOI 10.1016/j.eswa.2017.04.023
   Ahn E, 2017, IEEE J BIOMED HEALTH, V21, P1685, DOI 10.1109/JBHI.2017.2653179
   [Anonymous], 2016, P 8 IEEE INT WORKSH
   [Anonymous], 2004, ABSTRACT ALGEBRA
   [Anonymous], 2015, IEEE INT C ELECT COM, DOI DOI 10.1109/ICECCT.2015.7226064
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Barata C, 2014, IEEE SYST J, V8, P965, DOI 10.1109/JSYST.2013.2271540
   Berseth M., 2017, Isic 2017-skin lesion analysis towards melanoma detection
   Bertalmio M., 2001, PROC CVPR IEEE, V1, P1, DOI DOI 10.1109/CVPR.2001.990497
   Bindu ChHima., 2012, INT J ADV SCI TECHNO, V38, P67
   Brakerski Zvika, 2014, ACM Transactions on Computation Theory, V6, DOI 10.1145/2633600
   Chang V, 2018, MULTIMED TOOLS APPL, V77, P17693, DOI 10.1007/s11042-017-5186-8
   Chu K.-Y., 2013, Proceedings of the 21st ACM international conference on Multimedia, P597
   Codella NCF, 2018, I S BIOMED IMAGING, P168, DOI 10.1109/ISBI.2018.8363547
   Fan HD, 2017, COMPUT BIOL MED, V85, P75, DOI 10.1016/j.compbiomed.2017.03.025
   Fu C, 2012, OPT EXPRESS, V20, P2363, DOI 10.1364/OE.20.002363
   Ghamisi P, 2012, EXPERT SYST APPL, V39, P12407, DOI 10.1016/j.eswa.2012.04.078
   Hsu CY, 2012, IEEE T IMAGE PROCESS, V21, P4593, DOI 10.1109/TIP.2012.2204272
   Jackway PT, 2000, ELECTRON LETT, V36, P1194, DOI 10.1049/el:20000873
   Jahanifar M, 2019, IEEE J BIOMED HEALTH, V23, P509, DOI 10.1109/JBHI.2018.2839647
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   José-García A, 2016, APPL SOFT COMPUT, V41, P192, DOI 10.1016/j.asoc.2015.12.001
   Ke ZT, 2009, ENVIRONMENTAL VIBRATIONS: PREDICTION, MONITORING, MITIGATION AND EVALUATION, VOLS I AND II, P717
   Koohbanani Navid Alemi, 2018, ABS180910243 ARXIV
   Lakshmi VS, 2019, MULTIMED TOOLS APPL, V78, P20609, DOI 10.1007/s11042-019-7378-x
   Lathey A, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2656205
   Lathey A, 2013, IEEE INT C SEMANT CO, P310, DOI 10.1109/ICSC.2013.60
   Lei T, 2018, IEEE T FUZZY SYST, V26, P3027, DOI 10.1109/TFUZZ.2018.2796074
   Li DM, 2019, J INF SECUR APPL, V47, P59, DOI 10.1016/j.jisa.2019.03.020
   Li M, 2018, SIGNAL PROCESS-IMAGE, V62, P164, DOI 10.1016/j.image.2018.01.002
   Li YX, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020556
   Liu SX, 2017, BEVERAGES, V3, DOI 10.3390/beverages3020023
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma HP, 2019, SWARM EVOL COMPUT, V44, P365, DOI 10.1016/j.swevo.2018.04.011
   Mahmood A. B., 2011, 2011 6th International Conference for Internet Technology and Secured Transactions (ICITST), P596
   Martins P, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3124441
   Marwan Mbarek, 2017, P 1 INT C REAL TIM I, P378
   Mendoncya T., 2013, P INT C IEEE ENG MED
   Mirjalili S, 2016, ADV ENG SOFTW, V95, P51, DOI 10.1016/j.advengsoft.2016.01.008
   Mishra N., 2016, ARXIV160107843
   Mostafa A, 2017, MULTIMED TOOLS APPL, V76, P24931, DOI 10.1007/s11042-017-4638-5
   Pennisi A, 2016, COMPUT MED IMAG GRAP, V52, P89, DOI 10.1016/j.compmedimag.2016.05.002
   Preishuber M, 2018, IEEE T INF FOREN SEC, V13, P2137, DOI 10.1109/TIFS.2018.2812080
   Rahulamathavan Y, 2013, IEEE T AFFECT COMPUT, V4, P83, DOI 10.1109/T-AFFC.2012.33
   Rajput AS, 2018, MULTIMED TOOLS APPL, V77, P24223, DOI 10.1007/s11042-018-5729-7
   SaghaianNejadEsfahani SM, 2012, IEEE IMAGE PROC, P253, DOI 10.1109/ICIP.2012.6466843
   Selva, 2019, MATLAB CENTRAL FILE
   Singh P, 2018, IEEE T CIRC SYST VID, V28, P2116, DOI 10.1109/TCSVT.2017.2716828
   Sreekumar A., 2009, Hack, V2009, P33
   Tajeddin Neda Zamani, 2016, 2016 23rd Iranian Conference on Biomedical Engineering and 2016 1st International Iranian Conference on Biomedical Engineering (ICBME), P134, DOI 10.1109/ICBME.2016.7890944
   Tanwar VK, 2018, IEEE SYS MAN CYBERN, P2073, DOI 10.1109/SMC.2018.00357
   Teng L, 2012, OPT COMMUN, V285, P4048, DOI 10.1016/j.optcom.2012.06.004
   Xianjun Hu, 2014, Internet of Vehicles - Technologies and Services. First International Conference (IOV). Proceedings: LNCS 8662, P386, DOI 10.1007/978-3-319-11167-4_38
   Yan WQ, 2016, LECT NOTES COMPUT SC, V9431, P775, DOI 10.1007/978-3-319-29451-3_61
   Yi X, 2012, INFORM SYST, V38, P97, DOI 10.1016/j.is.2012.06.001
   Yu L, 2016, IEEE ACCESS, V4, P941, DOI 10.1109/ACCESS.2016.2539369
   Yuan YD, 2017, IEEE T MED IMAGING, V36, P1876, DOI 10.1109/TMI.2017.2695227
NR 57
TC 5
Z9 5
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 50
DI 10.1145/3380743
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600011
DA 2024-07-18
ER

PT J
AU Nie, WZ
   Wang, WJ
   Liu, AA
   Su, YT
   Nie, J
AF Nie, Weizhi
   Wang, Weijie
   Liu, Anan
   Su, Yuting
   Nie, Jie
TI HGAN: Holistic Generative Adversarial Networks for Two-dimensional
   Image-based Three-dimensional Object Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D retrieval; generative adversarial networks; characteristic view;
   image-based
AB In this article, we propose a novel method to address the two-dimensional (2D) image-based 3D object retrieval problem. First, we extract a set of virtual views to represent each 3D object. Then, a soft-attention model is utilized to find the weight of each view to select one characteristic view for each 3D object. Second, we propose a novel Holistic Generative Adversarial Network (HGAN) to solve the cross-domain feature representation problem and make the feature space of virtual characteristic view more inclined to the feature space of the real picture. This will effectively mitigate the distribution discrepancies across the 2D image domains and 3D object domains. Finally, we utilize the generative model of the HGAN to obtain the "virtual real image" of each 3D object and make the characteristic view of the 3D object and real picture possess the same feature space for retrieval. To demonstrate the performance of our approach, We established a new dataset that includes pairs of 2D images and 3D objects, where the 3D objects are based on the ModelNet40 dataset. The experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods.
C1 [Nie, Weizhi; Wang, Weijie; Liu, Anan; Su, Yuting] Tianjin Univ, Tianjin, Peoples R China.
   [Nie, Jie] Ocean Univ China, Qingdao, Peoples R China.
C3 Tianjin University; Ocean University of China
RP Nie, WZ (corresponding author), Tianjin Univ, Tianjin, Peoples R China.
EM weizhinie@tju.edu.cn; weijiewang@163.com; anan0422@gmail.com;
   ytsu@tju.edu.cn; niejie@ouc.edu.cn
RI Nie, Weizhi/ABF-5316-2021; Nie, Jie/ABG-9228-2021
OI Nie, Jie/0000-0003-4952-7666; nie, weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China [61872267, 61502337,
   61772359, 61472275]
FX This work was supported in part by the National Natural Science
   Foundation of China (61872267, 61502337, 61772359, and 61472275).
CR [Anonymous], P EUR WORKSH 3D OBJ
   [Anonymous], PATTERN RECOGNITION
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], UNIFIED FRAMEWORK OB
   [Anonymous], P EUR C 3D OBJ RETR
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], P ASS ADV ART INT C
   [Anonymous], 2014, COMPUT SCI
   [Anonymous], 2018, IEEE T IMAGE PROCESS
   [Anonymous], J ZHEJIANG U SCI C
   Arjovsky M., 2017, INT C MACH LEARN, P214
   Aubry M, 2015, IEEE I CONF COMP VIS, P2875, DOI 10.1109/ICCV.2015.329
   Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487
   Bosche F, 2008, AUTOMAT CONSTR, V17, P499, DOI 10.1016/j.autcon.2007.09.001
   Cho K, 2014, COMPUT SCI
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Furuya Takahiko, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P543, DOI 10.1109/3DV.2014.72
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Guétat G, 2006, IEEE T INF TECHNOL B, V10, P362, DOI 10.1109/TITB.2005.863875
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Li B, 2014, EUR WORKSH 3D OBJ RE, P121
   Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071
   Lin ZJ, 2017, IEEE T CYBERNETICS, V47, P4342, DOI 10.1109/TCYB.2016.2608906
   Liu AA, 2019, IEEE T CIRC SYST VID, V29, P868, DOI 10.1109/TCSVT.2018.2810191
   Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Massa F, 2016, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2016.648
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shao TJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366155
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Simonyan K., 2014, 14091556 ARXIV
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tatsuma A., 2012, Signal Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Pacific, P1
   Vondrick Carl, 2016, ADV NEURAL INFORM PR
   Wong HS, 2007, IEEE T MULTIMEDIA, V9, P1026, DOI 10.1109/TMM.2007.898915
   Wu BT, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3946
   Wu JJ, 2016, ADV NEUR IN, V29
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang XL, 2017, IEEE INT CONF COMMUN, P92
   Yeh JS, 2005, BIOINFORMATICS, V21, P3056, DOI 10.1093/bioinformatics/bti458
   Zaremba W., 2014, ARXIV
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhen Y., 2012, P INT C NEUR INF PRO, P1376
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 52
TC 14
Z9 14
U1 2
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 101
DI 10.1145/3344684
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800001
DA 2024-07-18
ER

PT J
AU van der Hooft, J
   Vega, MT
   Petrangeli, S
   Wauters, T
   De Turck, F
AF van der Hooft, Jeroen
   Vega, Maria Torres
   Petrangeli, Stefano
   Wauters, Tim
   De Turck, Filip
TI Tile-based Adaptive Streaming for Virtual Reality Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Virtual reality; HTTP adaptive streaming; tile-based rate adaptation
AB The increasing popularity of head-mounted devices and 360 degrees video cameras allows content providers to provide virtual reality (VR) video streaming over the Internet, using a two-dimensional representation of the immersive content combined with traditional HTTP adaptive streaming (HAS) techniques. However, since only a limited part of the video (i.e., the viewport) is watched by the user, the available bandwidth is not optimally used. Recent studies have shown the benefits of adaptive tile-based video streaming; rather than sending the whole 360 degrees video at once, the video is cut into temporal segments and spatial tiles, each of which can be requested at a different quality level. This allows prioritization of viewable video content and thus results in an increased bandwidth utilization. Given the early stages of research, there are still a number of open challenges to unlock the full potential of adaptive tile-based VR streaming. The aim of this work is to provide an answer to several of these open research questions. Among others, we propose two tile-based rate adaptation heuristics for equirectangular VR video, which use the great-circle distance between the viewport center and the center of each of the tiles to decide upon the most appropriate quality representation. We also introduce a feedback loop in the quality decision process, which allows the client to revise prior decisions based on more recent information on the viewport location. Furthermore, we investigate the benefits of parallel TCP connections and the use of HTTP/2 as an application layer optimization. Through an extensive evaluation, we show that the proposed optimizations result in a significant improvement in terms of video quality (more than twice the time spent on the highest quality layer), compared to non-tiled HAS solutions.
C1 [van der Hooft, Jeroen; Vega, Maria Torres; Wauters, Tim; De Turck, Filip] Univ Ghent, IMEC, IDLab, Dept Informat Technol, Ghent, Belgium.
   [Petrangeli, Stefano] Adobe Res, San Jose, CA USA.
C3 IMEC; Ghent University; Adobe Systems Inc.
RP van der Hooft, J (corresponding author), Univ Ghent, IMEC, IDLab, Dept Informat Technol, Ghent, Belgium.
EM jeroen.vanderhooft@ugent.be; maria.torresvega@ugent.be;
   petrange@adobe.com; tim.wauters@ugent.be; filip.deturck@ugent.be
RI Vega, Maria Torres/AAL-1171-2020
OI Vega, Maria Torres/0000-0002-5656-6607; De Turck,
   Filip/0000-0003-4824-1199
FU Agency for Innovation by Science and Technology in Flanders (VLAIO);
   Research Foundation-Flanders (FWO); PRO-FLOW project [150223]; FWO
   [G025615N]
FX Jeroen van der Hooft is funded by grant of the Agency for Innovation by
   Science and Technology in Flanders (VLAIO). Maria Torres Vega is funded
   by grant of the Research Foundation-Flanders (FWO). This research was
   performed partially within the imec PRO-FLOW (150223) project and the
   FWO-funded "Optimized source coding for multiple terminals in
   self-organising networks" (G025615N) project.
CR Ahmadi H, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P170, DOI 10.1145/3126686.3126743
   [Anonymous], P IFIP IEEE INT S IN
   [Anonymous], 2017, JCTVCAC0038
   [Anonymous], 2014, 230091 ISOIEC
   [Anonymous], P INT WORKSH QUAL EX
   [Anonymous], 2015, 7540 RFC
   [Anonymous], P NETW OP SYST SUPP
   Berg J., 2016, P ACM INT C MULT SYS
   Budagavi M, 2015, IEEE IMAGE PROC, P750, DOI 10.1109/ICIP.2015.7350899
   Fan C, 2017, SIXTEENTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, P67
   Feuvre J., 2016, P ACM INT C MULT SYS
   Fielding R., 2014, Hypertext Transfer Protocol: Message Syntax and Routing, DOI [10.17487/rfc7230, DOI 10.17487/RFC7230]
   Ghosh A., 2017, COMPUT RES REPOS
   Hosseini M, 2016, IEEE INT SYM MULTIM, P107, DOI [10.1109/ISM.2016.45, 10.1109/ISM.2016.0028]
   Hu H., 2017, COMPUT RES REPOS
   Juluri P, 2015, IEEE INT CONF COMM, P1765, DOI 10.1109/ICCW.2015.7247436
   Kellerer H., 2004, KNAPSACK PROBLEMS, P317, DOI DOI 10.1007/978-3-540-24777-7
   Kuzyakov E., 2016, Next-generation video encoding techniques for 360 video and VR-Engineering at Meta
   Mok R. K. P., 2011, 2011 IFIP/IEEE International Symposium on Integrated Network Management (IM 2011), P485, DOI 10.1109/INM.2011.5990550
   Nasrabadi AT, 2017, P IEEE VIRT REAL ANN, P347, DOI 10.1109/VR.2017.7892319
   OpenSignal, 2018, STAT MOB NETW BELG
   Opensignal, 2018, STAT MOB NETW US
   Petrangeli S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P306, DOI 10.1145/3123266.3123453
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Rai Y., 2017, 2017 9 INT C QUAL MU, P1, DOI 10.1109/QoMEX.2017.7965659
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Skupin R, 2017, CONSUM COMM NETWORK, P613, DOI 10.1109/CCNC.2017.7983191
   Son J, 2018, PROCEEDINGS OF THE 28TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'18), P61, DOI 10.1145/3210445.3210455
   da Costa RIT, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P270, DOI 10.1145/3204949.3204966
   Team Pixvana, 2016, INTRO FOVAS FIELD VI
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Xu ZQ, 2018, PROCEEDINGS OF 2018 IEEE WORLD SYMPOSIUM ON COMMUNICATION ENGINEERING (WSCE), P1, DOI 10.1109/WSCE.2018.8690540
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Zare A., 2016, P 24 ACM INT C MULT, P601, DOI DOI 10.1145/2964284.2967292
   Zare A, 2017, IEEE IMAGE PROC, P1432, DOI 10.1109/ICIP.2017.8296518
NR 38
TC 24
Z9 25
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 110
DI 10.1145/3362101
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800010
OA Green Published
DA 2024-07-18
ER

PT J
AU Cheung, M
   She, J
   Sun, WW
   Zhou, JT
AF Cheung, Ming
   She, James
   Sun, Weiwei
   Zhou, Jiantao
TI Detecting Online Counterfeit-goods Seller using Connection Discovery
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Counterfeit seller detection; social network; deep learning
ID INFLUENTIAL USERS; SOCIAL NETWORKS
AB With the advancement of social media and mobile technology, any smartphone user can easily become a seller on social media and e-commerce platforms, such as Instagram and Carousell in Hong Kong or Taobao in China. A seller shows images of their products and annotates their images with suitable tags that can be searched easily by others. Those images could be taken by the seller, or the seller could use images shared by other sellers. Among sellers, some sell counterfeit goods, and these sellers may use disguising tags and language, which make detecting them a difficult task. This article proposes a framework to detect counterfeit sellers by using deep learning to discover connections among sellers from their shared images. Based on 473K shared images from Taobao, Instagram, and Carousel, it is proven that the proposed framework can detect counterfeit sellers. The framework is 30% better than approaches using object recognition in detecting counterfeit sellers. To the best of our knowledge, this is the first work to detect online counterfeit sellers from their shared images.
C1 [Cheung, Ming; She, James] HKUST NIE Social Media Lab, Hong Kong, Peoples R China.
   [Sun, Weiwei; Zhou, Jiantao] Univ Macau, Dept Comp & Informat Sci, Fac Sci & Technol, State Key Lab Internet Things Smart City, Taipa, Macao, Peoples R China.
   [Cheung, Ming; She, James] HKUST, Sai Kung, Room 2430,Clear Water Bay, Hong Kong, Peoples R China.
   [Sun, Weiwei; Zhou, Jiantao] Univ Macau, Fac Sci & Technol, E11 Ave Univ, Taipa, Macao, Peoples R China.
C3 University of Macau; Hong Kong University of Science & Technology;
   University of Macau
RP Cheung, M (corresponding author), HKUST NIE Social Media Lab, Hong Kong, Peoples R China.; Cheung, M (corresponding author), HKUST, Sai Kung, Room 2430,Clear Water Bay, Hong Kong, Peoples R China.
FU HKUST-NIE Social Media Lab., HKUST; Macau Science and Technology
   Development Fund [FDCT/022/2017/A1, FDCT/077/2018/A2]; Research
   Committee at the University of Macau [MYRG2016-00137-FST,
   MYRG2018-00029-FST]
FX This work was supported in part by HKUST-NIE Social Media Lab., HKUST,
   and the Macau Science and Technology Development Fund under Grants No.
   FDCT/022/2017/A1 and No. FDCT/077/2018/A2, and in part by the Research
   Committee at the University of Macau under Grants No. MYRG2016-00137-FST
   and No. MYRG2018-00029-FST.
CR [Anonymous], IEEE T BIG DATA
   [Anonymous], 1986, MONOGR STAT APPL PRO
   [Anonymous], 2007, P 18 ANN ACM SIAM S
   [Anonymous], P 31 INF SYST INT C
   [Anonymous], 2014, ABS14053531 CORR
   [Anonymous], P THEIEEE C COMP COM
   [Anonymous], 2014, ARXIV14032802
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Chen C, 2013, AM J TRANSPLANT, V13, P162
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   McCallum A., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P169, DOI 10.1145/347090.347123
   Mukherjee A, 2012, P 21 INT C WORLD WID, P191
   Mukherjee A., 2013, P 7 INT C WEBL SOC M, P409
   Probst F, 2013, BUS INFORM SYST ENG+, V5, P179, DOI 10.1007/s12599-013-0263-7
   Sarna G, 2017, INT J MACH LEARN CYB, V8, P677, DOI 10.1007/s13042-015-0463-1
   Savage D, 2014, SOC NETWORKS, V39, P62, DOI 10.1016/j.socnet.2014.05.002
   Tan EH, 2012, INT CON DISTR COMP S, P305, DOI 10.1109/ICDCS.2012.40
   Ting SL, 2014, INT J PROD RES, V52, P4456, DOI 10.1080/00207543.2013.861947
   Trusov M, 2010, J MARKETING RES, V47, P643, DOI 10.1509/jmkr.47.4.643
   van Rijsbergen C. J, 1979, Information Retrieval, V2nd
   Wang AH, 2010, SECRYPT 2010: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SECURITY AND CRYPTOGRAPHY, P142
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang QY, 2013, IEEE INT CONGR BIG, P141, DOI 10.1109/BigData.Congress.2013.27
   Zheng XH, 2015, NEUROCOMPUTING, V159, P27, DOI 10.1016/j.neucom.2015.02.047
NR 30
TC 4
Z9 6
U1 2
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 35
DI 10.1145/3311785
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IM3ZV
UT WOS:000477935400006
DA 2024-07-18
ER

PT J
AU Trabelsi, R
   Varadarajan, J
   Zhang, L
   Jabri, I
   Pei, Y
   Smach, F
   Bouallegue, A
   Moulin, P
AF Trabelsi, Rim
   Varadarajan, Jagannadan
   Zhang, Le
   Jabri, Issam
   Pei, Yong
   Smach, Fethi
   Bouallegue, Ammar
   Moulin, Pierre
TI Understanding the Dynamics of Social Interactions: A Multi-Modal
   Multi-View Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Interaction recognition; active/passive subjects; multi-modal data;
   RGBD; skeleton; CNN
AB In this article, we deal with the problem of understanding human-to-human interactions as a fundamental component of social events analysis. Inspired by the recent success of multi-modal visual data in many recognition tasks, we propose a novel approach to model dyadic interaction by means of features extracted from synchronized 3D skeleton coordinates, depth, and Red Green Blue (RGB) sequences. From skeleton data, we extract new view-invariant proxemic features, named Unified Proxemic Descriptor (uProD), which is able to incorporate intrinsic and extrinsic distances between two interacting subjects. A novel key frame selection method is introduced to identify salient instants of the interaction sequence based on the joints' energy. From Red Green Blue Depth (RGBD) videos, more holistic CNN features are extracted by applying an adaptive pre-trained Convolutional Neural Networks (CNNs) on optical flow frames. For better understanding the dynamics of interactions, we expand the boundaries of dyadic interactions analysis by proposing a fundamentally new modeling for non-treated problem aiming to discern the active from the passive interactor. Extensive experiments have been carried out on four multi-modal and multi-view interactions datasets. The experimental results demonstrate the superiority of our proposed techniques against the state-of-the-art approaches.
C1 [Trabelsi, Rim] Singapore LISTIC, Adv Digital Sci Ctr, Singapore, Singapore.
   [Trabelsi, Rim] Univ Savoie Mont Blanc, Polytech Annecy Chambery, Chambery, France.
   [Trabelsi, Rim] Univ Gabes, Natl Engn Sch Gabes, Hatem Bettaher IResCoMath Res Unit, Gabes, Tunisia.
   [Varadarajan, Jagannadan] Grab, Singapore, Singapore.
   [Zhang, Le] Agcy Sci Technol & Res, Inst Infocomm Res, Singapore, Singapore.
   [Jabri, Issam] Al Yamamah Univ, Coll Engn & Architecture, Riyadh, Saudi Arabia.
   [Pei, Yong] SAP Asia Pte Ltd, Singapore, Singapore.
   [Smach, Fethi] Grp Credit Agr, Montrouge, France.
   [Bouallegue, Ammar] Univ Tunis El Manar, SysCom Lab, Natl Engn Sch Tunis, Tunis, Tunisia.
   [Moulin, Pierre] Univ Illinois, Urbana, IL USA.
C3 Universite Savoie Mont Blanc; Universite de Gabes; Agency for Science
   Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research
   (I2R); Al-Yamamah University; Universite de Tunis-El-Manar; Ecole
   Nationale d'Ingenieurs de Tunis (ENIT); University of Illinois System;
   University of Illinois Urbana-Champaign
RP Trabelsi, R (corresponding author), 1 Create Way,14-02 Create Tower, Singapour 138602, Singapore.
EM rim.trabelsi@enit.rnu.tn
RI Jabri, Issam/JZZ-2097-2024; Trabelsi, Rim/AAF-3243-2020
OI Zhang, Le/0000-0002-6930-8674; Trabelsi, Rim/0000-0001-9481-8003
FU Singapore Agency for Science, Technology and Research (A*STAR) through
   the ARAP program
FX This work was funded by the research grant from Singapore Agency for
   Science, Technology and Research (A*STAR) through the ARAP program.
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   [Anonymous], 2015, CORR
   [Anonymous], BRIT MACH VIS C BMVC
   Burgoon JudeeK., 2007, INTERPERSONAL ADAPTA
   Chen CY, 2017, IEEE T PATTERN ANAL, V39, P908, DOI 10.1109/TPAMI.2016.2564404
   Chen LL, 2013, PATTERN RECOGN LETT, V34, P1995, DOI 10.1016/j.patrec.2013.02.006
   Coppola C, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P5055, DOI 10.1109/IROS.2016.7759742
   Coppola  Claudio, 2017, AUTOMATIC DETECTION
   Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446
   Evangelidis G., 2014, INT C PATT REC ICPR
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fiske, 1991, STRUCTURES SOCIAL LI
   FISKE AP, 1992, PSYCHOL REV, V99, P689, DOI 10.1037/0033-295X.99.4.689
   Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128
   HALL ET, 1963, AM ANTHROPOL, V65, P1003, DOI 10.1525/aa.1963.65.5.02a00020
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Huang DA, 2014, LECT NOTES COMPUT SC, V8695, P489, DOI 10.1007/978-3-319-10584-0_32
   Kalimeri K, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P23
   Kong  Yu, 2012, EUR C COMP VIS ECCV, V7572
   Kooij JFP, 2016, COMPUT VIS IMAGE UND, V144, P106, DOI 10.1016/j.cviu.2015.06.009
   Lefter I, 2017, INT CONF AFFECT, P21, DOI 10.1109/ACII.2017.8273574
   Liu J, 2017, IEEE C COMP VIS PATT
   Ma MH, 2016, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2016.209
   Marcos-Ramiro A, 2015, IEEE T MULTIMEDIA, V17, P1721, DOI 10.1109/TMM.2015.2464152
   Morency L.-P., 2008, ICMI '08: Proceedings of the 10th international conference on Multimodal interfaces, P181, DOI DOI 10.1145/1452392.1452426
   Ohn-Bar E., 2013, CVPR WORKSHOPS
   Park S, 2013, INT CONF AFFECT, P423, DOI 10.1109/ACII.2013.76
   Patron-Perez Alonso., 2010, BMVC, V1, P2
   Postma  Eric, 2016, MEASURING CAUSAL DYN
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Simonyan  Karen, 2014, NEUR INF PROC SYST C
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Vinciarelli A, 2015, COGN COMPUT, V7, P397, DOI 10.1007/s12559-015-9326-z
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang PC, 2016, IEEE T HUM-MACH SYST, V46, P498, DOI 10.1109/THMS.2015.2504550
   Wolf C, 2014, COMPUT VIS IMAGE UND, V127, P14, DOI 10.1016/j.cviu.2014.06.014
   Xu N, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1195, DOI 10.1145/2733373.2806315
   Yang XD, 2017, IEEE T PATTERN ANAL, V39, P1028, DOI 10.1109/TPAMI.2016.2565479
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Yonetani R, 2016, PROC CVPR IEEE, P2629, DOI 10.1109/CVPR.2016.288
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
NR 45
TC 4
Z9 4
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 15
DI 10.1145/3300937
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100015
DA 2024-07-18
ER

PT J
AU Zhang, W
   Yao, T
   Zhu, SA
   El Saddik, A
AF Zhang, Wei
   Yao, Ting
   Zhu, Shiai
   El Saddik, Abdulmotaleb
TI Deep Learning-Based Multimedia Analytics: A Review
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Review
DE Multimedia analytics; deep learning; neural networks
AB The multimedia community has witnessed the rise of deep learning-based techniques in analyzing multimedia content more effectively. In the past decade, the convergence of deep-learning and multimedia analytics has boosted the performance of several traditional tasks, such as classification, detection, and regression, and has also fundamentally changed the landscape of several relatively new areas, such as semantic segmentation, captioning, and content generation. This article aims to review the development path of major tasks in multimedia analytics and take a look into future directions. We start by summarizing the fundamental deep techniques related to multimedia analytics, especially in the visual domain, and then review representative high-level tasks powered by recent advances. Moreover, the performance review of popular benchmarks gives a pathway to technology advancement and helps identify both milestone works and future directions.
C1 [Zhang, Wei; Yao, Ting] JD AI Res, 8 Beichen West Rd, Beijing, Peoples R China.
   [Zhu, Shiai] Ant Financial Grp, Hangzhou, Zhejiang, Peoples R China.
   [El Saddik, Abdulmotaleb] Univ Ottawa, 800 King Edward, Ottawa, ON, Canada.
C3 University of Ottawa
RP Zhu, SA (corresponding author), Ant Financial Grp, Hangzhou, Zhejiang, Peoples R China.
EM wzhang.cu@gmail.com; tingyao.ustc@gmail.com; zshiai@gmail.com;
   elsaddik@uottawa.ca
RI /D-4159-2009
OI /0000-0002-7690-8547
FU National Natural Science Foundation of China [61602463]; Open Project
   Program of the National Laboratory of Pattern Recognition (NLPR)
FX This work was partially funded by National Natural Science Foundation of
   China No. 61602463, and the Open Project Program of the National
   Laboratory of Pattern Recognition (NLPR).
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], 2016, ARXIV160307285
   [Anonymous], IEEE I CONF COMP VIS
   [Anonymous], MULTIMEDIA TOOLS APP
   [Anonymous], 2017, 31 AAAI C ART INT AA
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], EUR C COMP VIS
   [Anonymous], 2017, ABS170701083 CORR
   [Anonymous], ARXIV161108387
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], 2017, IEEE C COMP VIS PATT
   [Anonymous], 2016, ARXIV160604621
   [Anonymous], N AM CHAPTER ASS COM
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322
   [Anonymous], 2014, NIPS
   [Anonymous], 2017, ARXIV170701691
   [Anonymous], 2016, ARXIV E PRINTS
   [Anonymous], 2016, ENET DEEP NEURAL NET
   [Anonymous], 2016, ARXIV160401685
   [Anonymous], 2017, ARXIV170302719
   [Anonymous], ARXIV170403915
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P 30 C NEURAL INFORM
   [Anonymous], 2014, ARXIV14127062
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2016, P INT JOINT C ARTIFI
   [Anonymous], THUMOS CHALL WORKSH
   [Anonymous], 2016, ARXIV160207360
   [Anonymous], 2014, CORR
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], 2016, ARXIV161009585
   [Anonymous], 2013, P IEEE COMP SOC C CO
   [Anonymous], ON LINE LEARNING NEU
   [Anonymous], 2016, INT C LEARNING REPRE
   [Anonymous], ARXIV161200370
   [Anonymous], ARXIV161107675
   [Anonymous], ARXIV14064729
   [Anonymous], 2013, ICML WORKSH DEEP LEA
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], ARXIV171010196V2
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2016, ARXIV161201105CS
   [Anonymous], 2015, ARXIV151202325
   [Anonymous], 2017, Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   [Anonymous], 2004, P ACL WORKSH TEXT SU
   [Anonymous], 2017, ARXIV171109224
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Arjovsky M., 2017, ARXIV170107875
   Badrinarayanan V., 2015, SEGNET DEEP CONVOLUT
   Ballas Nicolas, 2015, Comput. Sci
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339
   Bengio Y., 2014, TECHNICAL REPORT
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen L.-C., 2017, IEEE C COMP VIS PATT
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Clevert D.A, 2015, 4 INT C LEARN REPR I
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Denton E.L., 2015, CoRR, P1486
   Devlin J, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P100
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong Li, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11210), P306, DOI 10.1007/978-3-030-01231-1_19
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gamal M., 2018, SHUFFLESEG REAL TIME
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Glorot X., 2010, P INT C ART INT STAT, P249
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves A., 2013, Generating sequences with recurrent neural networks
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072
   Han J, 1995, LECT NOTES COMPUT SC, V930, P195
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kendall Alex, 2017, ADV NEURAL INFORM PR, V30, DOI DOI 10.5555/3295222.3295309
   Kim T, 2017, PR MACH LEARN RES, V70
   King DB, 2015, ACS SYM SER, V1214, P1
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li Q, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/2911996.2912001
   Li Q, 2017, J ELECTR COMPUT ENG, V2017, DOI 10.1155/2017/3451358
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Li Yehao., 2016, P 24 ACM INT C MULT, P928
   Lin G., 2016, RefineNet: Multi-path refinement networks for high-resolution semantic segmentation
   Lin M., 2013, P 2 INT C LEARNING R
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mao Junhua., 2014, Explain images with multimodal recurrent neural networks
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Pan YW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1789, DOI 10.1145/3123266.3127905
   Pan YW, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1341, DOI 10.1145/3077136.3084144
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Pan YW, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P717, DOI 10.1145/2600428.2609568
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Qiu ZF, 2017, PROC CVPR IEEE, P4085, DOI 10.1109/CVPR.2017.435
   Radford A., 2015, ARXIV151106434
   Redmon J., 2016, P IEEE C COMP VIS PA, P779, DOI DOI 10.1109/CVPR.2016.91
   Reed S, 2016, PR MACH LEARN RES, V48
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rumelhart D.E., 1988, Nature, P696
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212
   Simonyan K., 2014, 14091556 ARXIV
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xie S., 2016, ARXIV161105431
   Xu K., 2015, COMPUTER SCI, P2048
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Yang Y., 2011, P C EMP METH NAT LAN, P444
   Yang YB, 2018, PROC CVPR IEEE, P2413, DOI 10.1109/CVPR.2018.00256
   Yang ZL, 2016, ADV NEUR IN, V29
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yao T, 2017, PROC CVPR IEEE, P5263, DOI 10.1109/CVPR.2017.559
   Yao T, 2015, IEEE I CONF COMP VIS, P28, DOI 10.1109/ICCV.2015.12
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zhang H, 2017, ARXIV161203242
   Zhang YH, 2018, PROC CVPR IEEE, P6810, DOI 10.1109/CVPR.2018.00712
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Y., 2018, STUDY INFLUENCE EXTE
NR 155
TC 13
Z9 13
U1 1
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 2
DI 10.1145/3279952
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100002
DA 2024-07-18
ER

PT J
AU D'Aronco, S
   Mena, S
   Frossard, P
AF D'Aronco, Stefano
   Mena, Sergio
   Frossard, Pascal
TI Distributed Rate Allocation in Switch-Based Multiparty Videoconferencing
   System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Videoconference; rate allocation; QoS optimization
AB Multiparty videoconferences, or more generally multiparty video calls, are gaining a lot of popularity as they offer a rich communication experience. These applications have, however, large requirements in terms of both network and computational resources and have to deal with sets of heterogenous clients. The multiparty videoconferencing systems are usually either based on expensive central nodes, called Multipoint Control Units (MCU), with transcoding capabilities, or on a peer-to-peer architecture where users cooperate to distribute more efficiently the different video streams. Whereas the first class of systems requires an expensive central hardware, the second one depends completely on the redistribution capacity of the users, which sometimes might neither provide sufficient bandwidth nor be reliable enough. In this work, we propose an alternative solution where we use a central node to distribute the video streams, but at the same time we maintain the hardware complexity and the computational requirements of this node as low as possible, for example, it has no video decoding capabilities. We formulate the rate allocation problem as an optimization problem that aims at maximizing the Quality of Service (QoS) of the videoconference. We propose two different distributed algorithms for solving the optimization problem: the first algorithm is able to find an approximate solution of the problem in a one-shot execution, whereas the second algorithm, based on Lagrangian relaxation, performs iterative updates of the optimization variables in order to gradually increase the value of the objective function. The two algorithms, though being disjointed, nicely complement each other. If executed in sequence, they allow us to achieve both a quick approximate rate reallocation, in case of a sudden change of the system conditions, and a precise refinement of the variables, which avoids problems caused by possible faulty approximate solutions. We have further implemented our solution in a network simulator where we show that our rate allocation algorithm is able to properly optimize users' QoS. We also illustrate the benefits of our solution in terms of network usage and overall utility when compared to a baseline heuristic method operating on the same system architecture.
C1 [D'Aronco, Stefano; Frossard, Pascal] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Mena, Sergio] Cisco Syst, San Jose, CA USA.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Cisco Systems Inc
RP D'Aronco, S (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM stefano.daronco@epfl.ch; semena@cisco.com; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019
FU Swiss Commission for Technology and Innovation [CTI-13175.1 PFES-ES];
   Swiss National Science Foundation [CHISTERA FNS 20CH21 151569]
FX This work has been supported by the Swiss Commission for Technology and
   Innovation under grant CTI-13175.1 PFES-ES and by the Swiss National
   Science Foundation under grant CHISTERA FNS 20CH21 151569.
CR [Anonymous], COUPLED CON IN PRESS
   [Anonymous], 1998, J OPERATIONAL RES SO
   [Anonymous], 2012, RTP MED CONG AV TECH
   [Anonymous], 2010, MODELING TOOLS NETWO
   [Anonymous], 2006, INTERNETWORKING TCP
   [Anonymous], 2012, SURV OPER RES MANAG
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chen Minghua, 2008, P SIGMETRICS PERF EV
   Chen Xiangwen, 2011, P INT C MULT
   Cisco, 2012, CISC VID TEL ARCH DE
   Cisco, 2016, CISC VIS NETW IND GL
   DAronco Stefano, 2016, P 7 INT C MULT SYST
   Eleftheriadis Alex, 2011, CISC VIS NETW IND GL
   Grozev Boris, 2015, P 25 WORKSH NETW OP
   Kurdoglu E, 2016, IEEE T MULTIMEDIA, V18, P90, DOI 10.1109/TMM.2015.2496872
   Li Jin, 2005, P SIGCOMM AS WORKSH
   Liu JC, 2004, IEEE T WIREL COMMUN, V3, P656, DOI 10.1109/TWC.2003.821216
   Palomar Daniel Perez, 2006, IEEE J SEL AREA COMM, V24, P8
   Ponec Miroslav, 2009, P INT C MULT EXP
   Sakhnov Kirill, 2009, P WORLD C ENG
   Schrijver A., 1998, THEORY LINEAR INTEGE
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Xu Yang, 2012, P C INT MEAS C
   Yang Yang Richard, 2000, P INT C NETW PROT
   Zhu Xiaoqing, 2015, NADA UNIFIE IN PRESS
NR 25
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
SU S
AR 41
DI 10.1145/3092835
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FP1UO
UT WOS:000417400400009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lin, SY
   Lin, YY
   Chen, CS
   Hung, YP
AF Lin, Shih-Yao
   Lin, Yen-Yu
   Chen, Chu-Song
   Hung, Yi-Ping
TI Recognizing Human Actions with Outlier Frames by Observation Filtering
   and Completion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human action recognition; outlier filtering; observation completion;
   early prediction; gap filling and conditional random fields
ID ACTION RECOGNITION; DEPTH; ENSEMBLE; TRACKING; MODELS; ROBUST
AB This article addresses the problem of recognizing partially observed human actions. Videos of actions acquired in the real world often contain corrupt frames caused by various factors. These frames may appear irregularly, and make the actions only partially observed. They change the appearance of actions and degrade the performance of pretrained recognition systems. In this article, we propose an approach to address the corrupt-frame problem without knowing their locations and durations in advance. The proposed approach includes two key components: outlier filtering and observation completion. The former identifies and filters out unobserved frames, and the latter fills up the filtered parts by retrieving coherent alternatives from training data. Hidden Conditional Random Fields (HCRFs) are then used to recognize the filtered and completed actions. Our approach has been evaluated on three datasets, which contain both fully observed actions and partially observed actions with either real or synthetic corrupt frames. The experimental results show that our approach performs favorably against the other state-of-the-art methods, especially when corrupt frames are present.
C1 [Lin, Shih-Yao] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 106, Taiwan.
   [Lin, Shih-Yao] 4005 Miranda Ave, Palo Alto, CA 94304 USA.
   [Lin, Yen-Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
   [Lin, Yen-Yu; Chen, Chu-Song] 128,Sect 2,Acad Rd, Taipei 11529, Taiwan.
   [Chen, Chu-Song] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
   [Hung, Yi-Ping] Tainan Natl Univ Arts, Tainan 720, Taiwan.
C3 National Taiwan University; Academia Sinica - Taiwan; Academia Sinica -
   Taiwan
RP Lin, SY (corresponding author), Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 106, Taiwan.; Lin, SY (corresponding author), 4005 Miranda Ave, Palo Alto, CA 94304 USA.
EM shihyaolin@iis.sinica.edu.tw; yylin@citi.sinica.edu.tw;
   song@iis.sinica.edu.tw; hung@csie.ntu.edu.tw
OI Lin, Yen-Yu/0000-0002-7183-6070
FU Ministry of Science and Technology of the Republic of China
   [104-2628-E-001-001-MY2, 105-2221-E-001-030-MY2, 105-2218-E-001-006,
   104-2221-E-002-050-MY3]
FX This work was supported in part by the Ministry of Science and
   Technology of the Republic of China under grants 104-2628-E-001-001-MY2,
   105-2221-E-001-030-MY2, 105-2218-E-001-006, and 104-2221-E-002-050-MY3.
CR Chaaraoui AA, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P91, DOI 10.1109/ICCVW.2013.19
   Andre E, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2502433
   [Anonymous], IJCV
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], P INT C OPT INSTR TE
   [Anonymous], UT-Interaction Dataset, ICPR contest on Semantic Description of Human Activities (SDHA)
   [Anonymous], 2017, P C COMP VIS PATT RE
   [Anonymous], 2014, ROBOTICS SCI SYSTEMS
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P C COMP VIS PATT RE
   [Anonymous], P INT C MACH LEARN
   [Anonymous], BIOM FOR IWBF 2013 I
   [Anonymous], P INT C AC SPEECH SI
   [Anonymous], ARXIV160203346
   [Anonymous], ARXIV160407528
   [Anonymous], 2016, ARXIV161108050
   [Anonymous], P C COMP VIS PATT RE
   [Anonymous], P C COMP VIS PATT RE
   Banerjee P, 2014, LECT NOTES COMPUT SC, V8690, P711, DOI 10.1007/978-3-319-10605-2_46
   Cao Y, 2013, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2013.343
   Carneiro G, 2013, IEEE T PATTERN ANAL, V35, P2592, DOI 10.1109/TPAMI.2013.96
   Chang KY, 2009, PROC CVPR IEEE, P533, DOI 10.1109/CVPRW.2009.5206612
   Chen Z, 2011, PATTERN RECOGN, V44, P2902, DOI 10.1016/j.patcog.2011.04.022
   Chia-Chih Chen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3425, DOI 10.1109/CVPR.2011.5995555
   Davis JW, 2006, IMAGE VISION COMPUT, V24, P455, DOI 10.1016/j.imavis.2006.01.012
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Fothergill S., 2012, P SIGCHI C HUM FACT, P1737, DOI DOI 10.1145/2207676.2208303
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Gowayyed M.A., 2013, Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, P1351
   James M., 1967, PROC BERKELEY S MATH, V1, P281, DOI DOI 10.1007/S11665-016-2173-6
   Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li CJ, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1236471.1236475
   Li Kang, 2014, IEEE Trans Pattern Anal Mach Intell, V36, P1644, DOI 10.1109/TPAMI.2013.2297321
   Li Q, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/2911996.2912001
   Li WQ, 2008, IEEE T CIRC SYST VID, V18, P1499, DOI 10.1109/TCSVT.2008.2005597
   Li X, 2017, PATTERN RECOGN, V61, P433, DOI 10.1016/j.patcog.2016.08.016
   Lin YY, 2014, PROC CVPR IEEE, P2617, DOI 10.1109/CVPR.2014.335
   Liu L, 2016, IEEE T CYBERNETICS, V46, P158, DOI 10.1109/TCYB.2015.2399172
   Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359
   Maji S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3177, DOI 10.1109/CVPR.2011.5995631
   MARTENS J, 2011, P 28 INT C MACH LEAR, P1033
   Hoai M, 2014, INT J COMPUT VISION, V107, P191, DOI 10.1007/s11263-013-0683-3
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Oshin O., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P111, DOI 10.1109/FG.2011.5771382
   Peng J., 2009, NIPS
   Piyathilaka L, 2013, C IND ELECT APPL, P567
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   Schindler Konrad, 2008, PROC C COMPUTER VISI, P1
   Shen W, 2012, PROC CVPR IEEE, P1784, DOI 10.1109/CVPR.2012.6247875
   Shu G, 2012, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2012.6247879
   Song Y, 2013, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR.2013.457
   Song Y, 2012, PROC CVPR IEEE, P2120, DOI 10.1109/CVPR.2012.6247918
   Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591
   Sutton C., 2007, INTRO CONDITIONAL RA
   Tang NC, 2015, IEEE T IMAGE PROCESS, V24, P709, DOI 10.1109/TIP.2014.2385591
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang L, 2007, 2007 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN SECURITY AND DEFENSE APPLICATIONS, P1
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Weinland D, 2010, LECT NOTES COMPUT SC, V6313, P635
   Xia L, 2013, PROC CVPR IEEE, P2834, DOI 10.1109/CVPR.2013.365
   Yu G., 2012, Acm international conference on multimedia, P1049
   Yu G, 2015, IEEE T CIRC SYST VID, V25, P87, DOI 10.1109/TCSVT.2014.2319594
   Zhang B, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2750780
   Zhang JG, 2010, PATTERN RECOGN, V43, P197, DOI 10.1016/j.patcog.2009.05.015
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2401, DOI 10.1109/TIP.2011.2128332
   Zhao X., 2014, ACM T MULTIMEDIA COM, V11, P1
NR 75
TC 1
Z9 1
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
AR 28
DI 10.1145/3089250
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD5TE
UT WOS:000407591900006
DA 2024-07-18
ER

PT J
AU Zhang, HW
   Shang, XD
   Luan, HB
   Wang, M
   Chua, TS
AF Zhang, Hanwang
   Shang, Xindi
   Luan, Huanbo
   Wang, Meng
   Chua, Tat-Seng
TI Learning from Collective Intelligence: Feature Learning Using Social
   Images and Tags
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Representation learning; visual-semantic embedding; cross-media analysis
ID KNOWLEDGE
AB Feature representation for visual content is the key to the progress of many fundamental applications such as annotation and cross-modal retrieval. Although recent advances in deep feature learning offer a promising route towards these tasks, they are limited in application domains where high-quality and large-scale training data are expensive to obtain. In this article, we propose a novel deep feature learning paradigm based on social collective intelligence, which can be acquired from the inexhaustible social multimedia content on the Web, in particular, largely social images and tags. Differing from existing feature learning approaches that rely on high-quality image-label supervision, our weak supervision is acquired by mining the visual-semantic embeddings from noisy, sparse, and diverse social image collections. The resultant image word embedding space can be used to (1) fine-tune deep visual models for low-level feature extractions and (2) seek sparse representations as high-level cross-modal features for both image and text. We offer an easy-to-use implementation for the-proposed paradigm, which is fast and compatible with any state-of-the-art deep architectures. Extensive experiments on several benchmarks demonstrate that the cross-modal features learned by our paradigm significantly outperforms others in various applications such as content based retrieval, classification, and image captioning.
C1 [Zhang, Hanwang; Shang, Xindi; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Luan, Huanbo] Tsinghua Univ, Tsinghua Yuan 1, Beijing 100084, Peoples R China.
   [Wang, Meng] Hefei Univ Technol, Hefei 230009, Peoples R China.
C3 National University of Singapore; Tsinghua University; Hefei University
   of Technology
RP Luan, HB (corresponding author), Tsinghua Univ, Tsinghua Yuan 1, Beijing 100084, Peoples R China.
EM hanwangzhang@gmail.com; xindi1992@gmail.com; luanhuanbo@gmail.com;
   eric.mengwang@gmail.com; dcscts@nus.edu.sg
RI Shang, Xindi/AAE-7259-2019; Wang, Meng/ITR-8699-2023
OI Shang, Xindi/0000-0002-9308-2927; Zhang, Hanwang/0000-0001-7374-8739
FU NUS-Tsinghua Extreme Search (NExT) project [R-252-300001-490]
FX This work was supported by the NUS-Tsinghua Extreme Search (NExT)
   project under Grant No. R-252-300001-490.
CR Andersen R, 2006, ANN IEEE SYMP FOUND, P475
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], 2011, P 1 INT C MULT RETR
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2013, P NIPS
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P 37 INT ACM SIGIR C
   [Anonymous], 2014, P 31 INT C INT C MAC
   [Anonymous], 2016, P CVPR
   [Anonymous], P IEEE
   [Anonymous], 2012, P 20 ACM INT C MULTI
   Bruni E, 2014, J ARTIF INTELL RES, V49, P1, DOI 10.1613/jair.4135
   Chen XL, 2013, IEEE I CONF COMP VIS, P1409, DOI 10.1109/ICCV.2013.178
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Corrado G., 2012, P 25 INT C NEUR INF, P1223
   D'Aspremont A, 2008, SIAM J OPTIMIZ, V19, P1171, DOI 10.1137/060676386
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Fang C, 2015, PROC CVPR IEEE, P577, DOI 10.1109/CVPR.2015.7298656
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Geng X, 2015, IEEE I CONF COMP VIS, P4274, DOI 10.1109/ICCV.2015.486
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Guadarrama S., 2014, Robotics: science and systems, V2, P6
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hua X.-S., 2013, Proceedings of the 21st ACM International Conference on Multimedia, Oct. 21-25, ACM, P243
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Karpathy A, 2014, ADV NEUR IN, V27
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Krishna R., 2016, CoRR
   McFee B, 2012, P 21 INT C WORLD WID, P909, DOI [DOI 10.1145/2187980, 10.1145/2187980.2188222]
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Morin F, 2005, P AISTATS, V5, P246
   Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Page L., 1999, PAGERANK CITATION RA
   Pan YW, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P717, DOI 10.1145/2600428.2609568
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Recht B., 2011, ADV NEURAL INFORM PR, P693
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sadeghi F, 2015, PROC CVPR IEEE, P1456, DOI 10.1109/CVPR.2015.7298752
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56
   Vedantam R, 2015, IEEE I CONF COMP VIS, P2542, DOI 10.1109/ICCV.2015.292
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu F, 2016, IEEE T IMAGE PROCESS, V25, P630, DOI 10.1109/TIP.2015.2507401
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yuan ZQ, 2014, IEEE T MULTIMEDIA, V16, P1624, DOI 10.1109/TMM.2014.2322338
   Zhang HW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1079, DOI 10.1145/2733373.2806286
   Zhang HW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P187, DOI 10.1145/2647868.2654915
   Zhang HW, 2014, IEEE T IMAGE PROCESS, V23, P2996, DOI 10.1109/TIP.2014.2325784
NR 54
TC 82
Z9 88
U1 1
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2017
VL 13
IS 1
AR 1
DI 10.1145/2978656
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM7WJ
UT WOS:000395522700001
DA 2024-07-18
ER

PT J
AU Jiang, YJ
   Tang, SY
   Zhang, LP
   Xiong, MZ
   Yip, YJ
AF Jiang, Yijing
   Tang, Shanyu
   Zhang, Liping
   Xiong, Muzhou
   Yip, Yau Jim
TI Covert Voice over Internet Protocol Communications with Packet Loss
   Based on Fractal Interpolation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Covert communications; VoIP; fractal interpolation; steganography;
   packet loss
ID STEGANOGRAPHY; SPEECH; CAPACITY
AB The last few years have witnessed an explosive growth in the research of information hiding in multimedia objects, but few studies have taken into account packet loss in multimedia networks. As one of the most popular real-time services in the Internet, Voice over Internet Protocol (VoIP) contributes to a large part of network traffic for its advantages of real time, high flow, and low cost. So packet loss is inevitable in multimedia networks and affects the performance of VoIP communications. In this study, a fractal-based VoIP steganographic approach was proposed to realize covert VoIP communications in the presence of packet loss. In the proposed scheme, secret data to be hidden were divided into blocks after being encrypted with the block cipher, and each block of the secret data was then embedded into VoIP streaming packets. The VoIP packets went through a packet-loss system based on Gilbert model which simulates a real network situation. And a prediction model based on fractal interpolation was built to decide whether a VoIP packet was suitable for data hiding. The experimental results indicated that the speech quality degradation increased with the escalating packet-loss level. The average variance of speech quality metrics (PESQ score) between the "no-embedding" speech samples and the "with-embedding" stego-speech samples was about 0.717, and the variances narrowed with the increasing packet-loss level. Both the average PESQ scores and the SNR values of stego-speech samples and the data-retrieving rates had almost the same varying trends when the packet-loss level increased, indicating that the success rate of the fractal prediction model played an important role in the performance of covert VoIP communications.
C1 [Jiang, Yijing; Tang, Shanyu; Zhang, Liping; Xiong, Muzhou] China Univ Geosci, Sch Comp Sci, Wuhan 430074, Peoples R China.
   [Jiang, Yijing; Tang, Shanyu; Yip, Yau Jim] Univ Salford, Salford M5 4WT, Lancs, England.
C3 China University of Geosciences; University of Salford
RP Tang, SY (corresponding author), China Univ Geosci, Sch Comp Sci, Wuhan 430074, Peoples R China.; Tang, SY (corresponding author), Univ Salford, Salford M5 4WT, Lancs, England.
EM yijingjiang2012@gmail.com; shanyu.tang@gmail.com; carolyn321@163.com;
   mzxiong@foxmail.com; y.j.yip@salford.ac.uk
RI Zhang, Liqun/JDN-3523-2023; Zhang, Li/GWM-7501-2022
FU National Natural Science Foundation of China [61272469, 61303237]
FX This work is supported by the National Natural Science Foundation of
   China, under Grant 61272469 and Grant 61303237.
CR [Anonymous], TRENDS NETWORK COMMU
   [Anonymous], 2003, P INT S INT SIGN PRO
   [Anonymous], P 6 INT C INT INF HI
   [Anonymous], INT J INFORM COMMUNI
   [Anonymous], 2009, IEEE INT C COMM 2009
   [Anonymous], P IEEE INT C INN INF
   [Anonymous], P 20 ACM MULT C ACM
   BARNSLEY MF, 1986, CONSTR APPROX, V2, P303, DOI 10.1007/BF01893434
   Cetin O, 2012, IMAGING SCI J, V60, P75, DOI 10.1179/1743131X11Y.0000000004
   Chang PC, 2002, CONF REC ASILOMAR C, P1199
   Cvejic N, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P336, DOI 10.1109/mmsp.2002.1203314
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Guerchi D., 2008, INT J MATH COMPUT SC, V3, P1
   Huang Y, 2011, IET INFORM SECUR, V5, P26, DOI 10.1049/iet-ifs.2010.0032
   Huang YF, 2011, IEEE T INF FOREN SEC, V6, P296, DOI 10.1109/TIFS.2011.2108649
   Huang YF, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P1512, DOI 10.1109/IIH-MSP.2008.174
   Huang YF, 2012, IEEE T INF FOREN SEC, V7, P1865, DOI 10.1109/TIFS.2012.2218599
   Ito A, 2010, IEICE T FUND ELECTR, VE93A, P1279, DOI 10.1587/transfun.E93.A.1279
   Krätzer C, 2006, IEEE INT SYMP CIRC S, P2397
   Lee YK, 2000, IEE P-VIS IMAGE SIGN, V147, P288, DOI 10.1049/ip-vis:20000341
   LELAND WE, 1994, IEEE ACM T NETWORK, V2, P1, DOI 10.1109/90.282603
   Liu LH, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P406, DOI 10.1109/IIH-MSP.2008.297
   Marvel LM, 1999, IEEE T IMAGE PROCESS, V8, P1075, DOI 10.1109/83.777088
   Mazurczyk W, 2008, COMM COM INF SC, V12, P65
   Mazurczyk W, 2014, SECUR COMMUN NETW, V7, P2602, DOI 10.1002/sec.388
   Mazurczyk W, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543587
   Mazurczyk W, 2012, SECUR COMMUN NETW, V5, P1394, DOI 10.1002/sec.502
   Mazurczyk W, 2008, LECT NOTES COMPUT SC, V5332, P1001
   Miao R, 2011, P 46 IEEE INT C COMM, P1, DOI DOI 10.1109/ICC.2011.5962657
   ShanYu T, 2014, SCI CHINA INFORM SCI, V57, DOI 10.1007/s11432-014-5063-2
   Simmons G. J., 1984, Advances in Cryptology. Proceedings of Crypto 83, P51
   Song CM, 2006, NAT PHYS, V2, P275, DOI 10.1038/nphys266
   Song CM, 2005, NATURE, V433, P392, DOI 10.1038/nature03248
   Takahashi T, 2007, 2007 THIRD INTERNATIONAL CONFERENCE ON SECURITY AND PRIVACY IN COMMUNICATION NETWORKS AND WORKSHOPS, P371, DOI 10.1109/SECCOM.2007.4550357
   Tian H, 2011, COMPUT COMMUN, V34, P2236, DOI 10.1016/j.comcom.2011.07.003
   Tian H, 2008, PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE FOR YOUNG COMPUTER SCIENTISTS, VOLS 1-5, P647, DOI 10.1109/ICYCS.2008.394
   Wang CY, 2007, IEEE INT SYM MULTIM, P255, DOI 10.1109/ISM.2007.33
   Willinger W, 1997, IEEE ACM T NETWORK, V5, P71, DOI 10.1109/90.554723
   Wu ZJ, 2006, LECT NOTES COMPUT SC, V4113, P1139, DOI 10.1007/11816157_141
   Xiu CB, 2014, CHAOS SOLITON FRACT, V68, P89, DOI 10.1016/j.chaos.2014.07.013
   Xu TT, 2009, INT CONF WIRE COMMUN, P752
   Yang CH, 2008, IEEE T INF FOREN SEC, V3, P488, DOI 10.1109/TIFS.2008.926097
NR 42
TC 13
Z9 15
U1 1
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2016
VL 12
IS 4
AR 54
DI 10.1145/2961053
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EH
UT WOS:000382877500008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhang, LM
   Li, XL
   Nie, LQ
   Yan, Y
   Zimmermann, R
AF Zhang, Luming
   Li, Xuelong
   Nie, Liqiang
   Yan, Yan
   Zimmermann, Roger
TI Semantic Photo Retargeting Under Noisy Image Labels
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithm; Performance; Experimentations; Retargeting; semantic;
   graphlet; aesthetics evaluation; image label
ID SCENE
AB With the popularity of mobile devices, photo retargeting has become a useful technique that adapts a high-resolution photo onto a low-resolution screen. Conventional approaches are limited in two aspects. The first factor is the de-emphasized role of semantic content that is many times more important than low-level features in photo aesthetics. Second is the importance of image spatial modeling: toward a semantically reasonable retargeted photo, the spatial distribution of objects within an image should be accurately learned. To solve these two problems, we propose a new semantically aware photo retargeting that shrinks a photo according to region semantics. The key technique is a mechanism transferring semantics of noisy image labels (inaccurate labels predicted by a learner like an SVM) into different image regions. In particular, we first project the local aesthetic features (graphlets in this work) onto a semantic space, wherein image labels are selectively encoded according to their noise level. Then, a category-sharing model is proposed to robustly discover the semantics of each image region. The model is motivated by the observation that the semantic distribution of graphlets from images tagged by a common label remains stable in the presence of noisy labels. Thereafter, a spatial pyramid is constructed to hierarchically encode the spatial layout of graphlet semantics. Based on this, a probabilistic model is proposed to enforce the spatial layout of a retargeted photo to be maximally similar to those from the training photos. Experimental results show that (1) noisy image labels predicted by different learners can improve the retargeting performance, according to both qualitative and quantitative analysis, and (2) the category-sharing model stays stable even when 32.36% of image labels are incorrectly predicted.
C1 [Zhang, Luming] Hefei Univ Technol, Dept CSIE, Hefei, Peoples R China.
   [Li, Xuelong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, State Key Lab Transient Opt & Photon, Ctr OPT IMagery Anal & Learning OPTIMAL, Xian 710119, Shaanxi, Peoples R China.
   [Nie, Liqiang; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore, Singapore.
   [Yan, Yan] Univ Trento, Dept Informat Engn & Comp Sci, Via Sommar 9, I-38123 Trento, Italy.
C3 Hefei University of Technology; Chinese Academy of Sciences; Xi'an
   Institute of Optics & Precision Mechanics, CAS; State Key Laboratory of
   Transient Optics & Photonics; National University of Singapore;
   University of Trento
RP Zhang, LM (corresponding author), Hefei Univ Technol, Dept CSIE, Hefei, Peoples R China.
EM zglumg@gmail.com; xuelong_li@opt.ac.cn; zglumg@nus.edu.sg
RI zhang, lu/GRO-2969-2022; Li, Xuelong/ABF-3381-2020; Li,
   Xuelong/Z-3785-2019; Lei, Ming/JAD-1050-2023; li, xiang/GWM-6319-2022;
   Zimmermann, Roger/D-7944-2015
OI Zimmermann, Roger/0000-0002-7410-2590; Li, Xuelong/0000-0002-0019-4197
CR Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   [Anonymous], P CVPR
   [Anonymous], P SPIE
   [Anonymous], 2014, MEDIAT INFLAMM, DOI DOI 10.3171/2014.11.JNS14770
   [Anonymous], P NIPS
   [Anonymous], P EMMCVPR
   [Anonymous], P CVPR
   [Anonymous], SIGGRAPH ASIA COURSE
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   [Anonymous], ACM TOG
   [Anonymous], 2010, ACM MULTIMEDIA
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   [Anonymous], SIGGRAPH ASIA COURSE
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Kapoor A., 2007, P ICCV, P1
   Lafferty J., 2001, CONDITIONAL RANDOM F, V2001, P282
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536
   Liu Y, 2013, PROC CVPR IEEE, P2075, DOI 10.1109/CVPR.2013.270
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Song DJ, 2010, IEEE T IMAGE PROCESS, V19, P174, DOI 10.1109/TIP.2009.2032939
   Tang JH, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2501643.2501651
   Verbeek J., 2007, PROC IEEE C COMPUTER, P1, DOI DOI 10.1109/CVPR.2007.383098
   Vezhnevets A, 2012, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2012.6247757
   Vezhnevets A, 2012, PROC CVPR IEEE, P3162, DOI 10.1109/CVPR.2012.6248050
   Vezhnevets A, 2011, IEEE I CONF COMP VIS, P643, DOI 10.1109/ICCV.2011.6126299
   Vezhnevets A, 2010, PROC CVPR IEEE, P3249, DOI 10.1109/CVPR.2010.5540060
   Wang XC, 2011, IEEE T IMAGE PROCESS, V20, P2627, DOI 10.1109/TIP.2011.2114354
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Xiang SM, 2009, KNOWL INF SYST, V19, P159, DOI 10.1007/s10115-008-0161-3
   Xiong XJ, 2000, INT C PATT RECOG, P897, DOI 10.1109/ICPR.2000.903688
   Xu J, 2014, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2014.408
   Yin YF, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2658981
   Yuan J, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2534409
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
NR 45
TC 9
Z9 9
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2016
VL 12
IS 3
AR 37
DI 10.1145/2886775
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YO
UT WOS:000379425400003
DA 2024-07-18
ER

PT J
AU Antaris, S
   Rafailidis, D
AF Antaris, Stefanos
   Rafailidis, Dimitrios
TI Similarity Search over the Cloud Based on Image Descriptors' Dimensions
   Value Cardinalities
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Measurement; Experimentation; Content-based image retrieval;
   large-scale similarity search; distributed databases; multimedia cloud
   computing
ID NEIGHBOR; QUANTIZATION
AB In recognition that inmodern applications billions of images are stored into distributed databases in different logical or physical locations, we propose a similarity search strategy over the cloud based on the dimensions value cardinalities of image descriptors. Our strategy has low preprocessing requirements by dividing the computational cost of the preprocessing steps into several nodes over the cloud and locating the descriptors with similar dimensions value cardinalities logically close. New images are inserted into the distributed databases over the cloud efficiently, by supporting dynamical update in real-time. The proposed insertion algorithm has low computational complexity, depending exclusively on the dimensionality of descriptors and a small subset of descriptors with similar dimensions value cardinalities. Finally, an efficient query processing algorithm is proposed, where the dimensions of image descriptors are prioritized in the searching strategy, assuming that dimensions of high value cardinalities have more discriminative power than the dimensions of low ones. The computation effort of the query processing algorithm is divided into several nodes over the cloud infrastructure. In our experiments with seven publicly available datasets of image descriptors, we show that the proposed similarity search strategy outperforms competitive methods of single node, parallel and cloud-based architectures, in terms of preprocessing cost, search time and accuracy.
C1 [Antaris, Stefanos; Rafailidis, Dimitrios] Aristotle Univ Thessaloniki, Sch Informat, Thessaloniki 54124, Greece.
C3 Aristotle University of Thessaloniki
RP Antaris, S (corresponding author), Aristotle Univ Thessaloniki, Sch Informat, Aristotle Univ Campus, Thessaloniki 54124, Greece.
EM santaris@csd.auth.gr; draf@csd.auth.gr
CR [Anonymous], P 6 INT WORKSH CONT
   [Anonymous], 2011, ICML
   [Anonymous], 2004, P 20 ACM S COMP
   [Anonymous], P BRIT MACH VIS C BM
   [Anonymous], P INT C IM VID RETR
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2009, NEURIPS
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], METRIC BASED SIMILAR
   [Anonymous], ACM T MULTIMEDIA COM
   Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038
   Batko M, 2008, FUTURE GENER COMP SY, V24, P834, DOI 10.1016/j.future.2007.07.012
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bozkaya T, 1999, ACM T DATABASE SYST, V24, P361, DOI 10.1145/328939.328959
   Chatzichristofis SA, 2008, LECT NOTES COMPUT SC, V5008, P312
   Cheng H, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P1207
   Fu AW, 2000, VLDB J, V9, P154, DOI 10.1007/PL00010672
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   He JF, 2011, PROC CVPR IEEE, P753, DOI 10.1109/CVPR.2011.5995518
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Huang Z., 2011, SIGMOD, P1021
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jia Y, 2010, PROC CVPR IEEE, P3392, DOI 10.1109/CVPR.2010.5540006
   Joly A, 2011, PROC CVPR IEEE, P873, DOI 10.1109/CVPR.2011.5995709
   Knees P, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2542205.2542206
   Liang Lei, 2010, 2010 9th IEEE International Conference on Cognitive Informatics (ICCI), P127, DOI 10.1109/COGINF.2010.5599753
   Liu XL, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540990
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mingdong Zhu, 2012, Web-Age Information Management. Proceedings of the 13th International Conference, WAIM 2012, P222, DOI 10.1007/978-3-642-32281-5_22
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Novak D, 2012, INFORM PROCESS MANAG, V48, P855, DOI 10.1016/j.ipm.2010.12.004
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Ratnasamy S, 2001, ACM SIGCOMM COMP COM, V31, P161, DOI 10.1145/964723.383072
   Silpa-Anan C., 2008, P IEEE C COMPUTER VI, P1
   Stehling R. O., 2002, Proceedings of the Eleventh International Conference on Information and Knowledge Management. CIKM 2002, P102, DOI 10.1145/584792.584812
   Tiakas E, 2013, IEEE T MULTIMEDIA, V15, P1415, DOI 10.1109/TMM.2013.2247989
   Tian YH, 2010, COMPUTER, V43, P27, DOI 10.1109/MC.2010.188
   Wang J., 2010, P 2010 INT C MANAGEM, P591, DOI DOI 10.1145/1807167.1807232
   Wang J, 2013, IEEE I CONF COMP VIS, P2128, DOI 10.1109/ICCV.2013.265
   Wang J, 2012, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.2012.6247790
   Wang JD, 2014, IEEE T PATTERN ANAL, V36, P388, DOI 10.1109/TPAMI.2013.125
   Wang ZH, 2011, 2011 SECOND INTERNATIONAL CONFERENCE ON EDUCATION AND SPORTS EDUCATION (ESE 2011), VOL V, P1
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Zhang LN, 2013, CANCER GENE THER, V20, P1, DOI 10.1038/cgt.2012.84
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 50
TC 4
Z9 4
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2015
VL 11
IS 4
AR 51
DI 10.1145/2716315
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CK0ZF
UT WOS:000355933700004
DA 2024-07-18
ER

PT J
AU Szeliski, R
   Snavely, N
   Seitz, SM
AF Szeliski, Richard
   Snavely, Noah
   Seitz, Steven M.
TI Navigating the Worldwide Community of Photos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Image-based rendering; image-based modeling; visualization
AB The last decade has seen an explosion in the number of photographs available on the Internet. The sheer volume of interesting photos makes it a challenge to explore this space. Various Web and social media sites, along with search and indexing techniques, have been developed in response. One natural way to navigate these images in a 3D geo-located context. In this article, we reflect on our work in this area, with a focus on techniques that build partial 3D scene models to help find and navigate interesting photographs in an interactive, immersive 3D setting. We also discuss how finding such relationships among photographs opens up exciting new possibilities for multimedia authoring, visualization, and editing.
C1 [Szeliski, Richard] Microsoft Res, Redmond, WA 98052 USA.
   [Snavely, Noah] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
   [Seitz, Steven M.] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
C3 Microsoft; Cornell University; University of Washington; University of
   Washington Seattle
RP Szeliski, R (corresponding author), Microsoft Res, 1 Microsoft Way, Redmond, WA 98052 USA.
EM szeliski@microsoft.com; snavely@cs.cornell.edu; seitz@cs.washington.edu
CR Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293
   Goesele M., 2007, P 11 INT C COMP VIS
   Kushal A, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P57, DOI 10.1109/3DIMPVT.2012.62
   Seitz S. M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P21, DOI 10.1145/237170.237196
   SIMON I., 2007, 11 INT C COMP VIS
   Simon I, 2008, LECT NOTES COMPUT SC, V5303, P541, DOI 10.1007/978-3-540-88688-4_40
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Snavely N, 2010, P IEEE, V98, P1370, DOI 10.1109/JPROC.2010.2049330
   Snavely N, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360614
   Szeliski R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P251, DOI 10.1145/258734.258861
   Uyttendaele M, 2004, IEEE COMPUT GRAPH, V24, P52, DOI 10.1109/MCG.2004.1297011
NR 11
TC 0
Z9 0
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 47
DI 10.1145/2492208
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700017
DA 2024-07-18
ER

PT J
AU Hong, RC
   Tang, JH
   Tan, HK
   Ngo, CW
   Yan, SC
   Chua, TS
AF Hong, Richang
   Tang, Jinhui
   Tan, Hung-Khoon
   Ngo, Chong-Wah
   Yan, Shuicheng
   Chua, Tat-Seng
TI Beyond Search: Event-Driven Summarization for Web Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithm; Design; Experimentation; Event evolution; key-shot threading;
   key-shot tagging; Web video summarization
ID SIMILARITY
AB The explosive growth of Web videos brings out the challenge of how to efficiently browse hundreds or even thousands of videos at a glance. Given an event-driven query, social media Web sites usually return a large number of videos that are diverse and noisy in a ranking list. Exploring such results will be time-consuming and thus degrades user experience. This article presents a novel scheme that is able to summarize the content of video search results by mining and threading "key" shots, such that users can get an overview of main content of these videos at a glance. The proposed framework mainly comprises four stages. First, given an event query, a set of Web videos is collected associated with their ranking order and tags. Second, key-shots are established and ranked based on near-duplicate keyframe detection and they are threaded in a chronological order. Third, we analyze the tags associated with key-shots. Irrelevant tags are filtered out via a representativeness and descriptiveness analysis, whereas the remaining tags are propagated among key-shots by random walk. Finally, summarization is formulated as an optimization framework that compromises relevance of key-shots and user-defined skimming ratio. We provide two types of summarization: video skimming and visual-textual storyboard. We conduct user studies on twenty event queries for over hundred hours of videos crawled from YouTube. The evaluation demonstrates the feasibility and effectiveness of the proposed solution.
C1 [Hong, Richang; Tang, Jinhui; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Tan, Hung-Khoon; Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
C3 National University of Singapore; City University of Hong Kong; National
   University of Singapore
RP Tang, JH (corresponding author), Natl Univ Singapore, Sch Comp, COM1,13 Comp Dr, Singapore 117417, Singapore.
EM hongrc@comp.nus.edu.sg; tangjh@comp.nus.edu.sg; hktan@cs.cityu.edu.hk;
   cwngo@cs.cityu.edu.hk; eleyans@nus.edu.sg; chuats@comp.nus.edu.sg
RI Tang, Jinhui/KBR-0891-2024; Yan, Shuicheng/HCI-1431-2022
OI Ngo, Chong Wah/0000-0003-4182-8261
CR [Anonymous], 2007, P 7 ACM SIGCOMM C IN
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI [10.1145/1899412.1899418, DOI 10.1145/1899412.1899418]
   BENOIT H, 2006, INTERACTIVE VIDEO AL, P27
   CAPRA RG, 2008, P JOINT C DIG LIB JC
   CHEN BW, 2003, IEEE T MULTIMEDIA, V9, P295
   Cheng X, 2007, P 7 ACM SIGCOMM C IN, P1
   CHUA T. S, 2010, SCHOLARPEDIA, V5, P9546
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   DUYGULU P, 2003, P 11 ACM INT C MULT
   HONG R, 2010, P INT C MULT MOD MMM
   HONG R, 2010, P ACM INT C MULT ACM
   HONG R, 2009, P ACM MULT WORKSH SO
   HONG R, 2010, P ACM INT C IM VID R
   HSU WH, 2007, P ACM 14 INT C MULT
   JING Y, 2008, P 17 INT WORLD WID W
   KE Y, 2004, P 12 ACM INT C MULT
   Li XL, 2010, IEEE T SYST MAN CY B, V40, P1170, DOI 10.1109/TSMCB.2009.2035629
   Li XL, 2010, IEEE T KNOWL DATA EN, V22, P145, DOI 10.1109/TKDE.2009.64
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   NEO SY, 2007, P 14 ACM INT C MULT
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   PEDRO JS, 2007, P ACM INT C IM VID R
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Shen JL, 2008, IEEE T CIRC SYST VID, V18, P1587, DOI 10.1109/TCSVT.2008.2005607
   Shen JL, 2009, ACM T INFORM SYST, V27, DOI 10.1145/1508850.1508856
   SIERSDORFER S, 2009, P 32 ANN ACM SIGIR C
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wu X, 2006, IEEE SIGNAL PROC MAG, V23, P59
   WU X, 2007, P 15 INT ACM C MULT
   Yang H., 2003, ACM MULTIMEDIA
   Yang Y, 2010, IEEE T CIRC SYST VID, V20, P1745, DOI 10.1109/TCSVT.2010.2087452
   Yang Y, 2010, IEEE T IMAGE PROCESS, V19, P2761, DOI 10.1109/TIP.2010.2049235
   ZHANG DQ, 2004, P 12 ACM INT C MULT
   Zhao WL, 2007, IEEE T MULTIMEDIA, V9, P1037, DOI 10.1109/TMM.2007.898928
   Zhao WL, 2009, IEEE T IMAGE PROCESS, V18, P412, DOI 10.1109/TIP.2008.2008900
   Zhu XQ, 2003, MULTIMEDIA SYST, V9, P31, DOI 10.1007/s00530-003-0076-5
   2011, ACM T MULTIMEDIA COM, V7
NR 40
TC 32
Z9 33
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2011
VL 7
IS 4
AR 35
DI 10.1145/2043612.2043613
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 856ZS
UT WOS:000297684000001
OA Green Published
DA 2024-07-18
ER

PT J
AU Sang, JT
   Xu, CS
AF Sang, Jitao
   Xu, Changsheng
TI Browse by Chunks: Topic Mining and Organizing on Web-Scale Social Media
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Performance; Hierarchical topic
   model; search result clustering; semisupervised learning; social media;
   topic mining; video retrieval
AB The overwhelming amount of Web videos returned from search engines makes effective browsing and search a challenging task. Rather than conventional ranked list, it becomes necessary to organize the retrieved videos in alternative ways. In this article, we explore the issue of topic mining and organizing of the retrieved web videos in semantic clusters. We present a framework for clustering-based video retrieval and build a visualization user interface. A hierarchical topic structure is exploited to encode the characteristics of the retrieved video collection and a semi-supervised hierarchical topic model is proposed to guide the topic hierarchy discovery. Carefully designed experiments on web-scale video dataset collected from video sharing websites validate the proposed method and demonstrate that clustering-based video retrieval is practical to facilitate users for effective browsing.
C1 [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   China Singapore Inst Digital Media, Singapore, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
FU National Natural Science Foundation of China [90920303]; 973 Program
   [2012CB316304]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 90920303) and 973 Program (Project No. 2012CB316304).
CR [Anonymous], P ACM INT C MULT
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Blei DM, 2010, J ACM, V57, DOI 10.1145/1667053.1667056
   Blei DM, 2004, ADV NEUR IN, V16, P17
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   CAO J, 2010, P ACM MULT C MM, P1639
   Carpineto C, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541884
   CHANDRAMOULI K, 2008, VISUAL INFORM ENG, P452
   Cheung SCS, 2005, IEEE T MULTIMEDIA, V7, P524, DOI 10.1109/TMM.2005.846906
   CUTTING DR, 1992, SIGIR 92 : PROCEEDINGS OF THE FIFTEENTH ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P318
   Druck G, 2008, Association for Computing Machinery, P595, DOI [10.1145/1390334.1390436, DOI 10.1145/1390334.1390436]
   Gong ZG, 2005, LECT NOTES COMPUT SC, V3588, P166
   Hindle A., 2010, Proceedings of the 26th International Conference on Software Maintenance, P1, DOI DOI 10.1109/ICSM.2010.5609670
   Jing F., 2006, PROC MM 06, P377, DOI DOI 10.1145/1180639.1180720
   KUMMAMURU K, 1998, P 21 INT ACM SIGIR C, P46
   LIU JS, 1994, J AM STAT ASSOC, V89, P958, DOI 10.2307/2290921
   Liu L., 2008, P 17 INT C WORLD WID, P1009
   Liu L, 2008, INT CONF ACOUST SPEE, P2145
   MILLER GA, 1990, INTRO WORLDNET ON LI, V3
   RAMACHANDRAN C, 2009, P MULT C MM
   STEINBACH M, 2000, P ACM SIGKDD INT C K, P35
   TAN P, 2005, INTRO DATA MINING, V19
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Wu A. G., 2007, P ACM MM, P218
   Yuan JS, 2008, IEEE T CIRC SYST VID, V18, P1597, DOI 10.1109/TCSVT.2008.2005616
   Yuan JS, 2010, IEEE T MULTIMEDIA, V12, P705, DOI 10.1109/TMM.2010.2051868
   Zamir O., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P46, DOI 10.1145/290941.290956
NR 27
TC 9
Z9 11
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 30
DI 10.1145/2037676.2037687
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 857NR
UT WOS:000297725800011
DA 2024-07-18
ER

PT J
AU Marshall, D
   Mcloone, S
   Ward, T
AF Marshall, Damien
   Mcloone, Seamus
   Ward, Tomas
TI Optimizing Consistency by Maximizing Bandwidth Usage in Distributed
   Interactive Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurements; Performance; Adaptive algorithms; multiplayer games;
   information management techniques; consistency
ID LATENCY
AB A key factor determining the success of a Distributed Interactive Application (DIA) is the maintenance of a consistent shared virtual world. To help maintain consistency, a number of Information Management techniques have been developed. However, unless carefully tuned to the underlying network, they can negatively impact on consistency. This work presents a novel adaptive algorithm for optimizing consistency by maximizing available bandwidth usage in DIAs. This algorithm operates by estimating bandwidth from trends in network latency, and modifying data transmission rates to match the estimated value. Results presented within demonstrate that this approach can help optimise consistency levels in a DIA.
C1 [Marshall, Damien; Mcloone, Seamus; Ward, Tomas] Natl Univ Ireland, Maynooth, Kildare, Ireland.
C3 Maynooth University
RP Marshall, D (corresponding author), Natl Univ Ireland, Maynooth, Kildare, Ireland.
EM Damien.marshall@itcarlow.ie
RI Ward, Tomas E/G-9221-2011; Ward, Tomas/AEN-2410-2022
OI Ward, Tomas/0000-0002-6173-6607
FU Science Foundation Ireland; Enterprise Ireland [IRCSET/SC/04/CS0289]
FX This work is supported by Science Foundation Ireland and Enterprise
   Ireland, Grant no. IRCSET/SC/04/CS0289.
CR ABOOBAKER N, 2002, P IFIP IEEE INT C MA, P89
   [Anonymous], P NOSSDAV 04 CORK IR
   [Anonymous], 127811995 IEEE
   [Anonymous], P ACM S VIRT REAL SO
   BALAN R, 2005, P 6 ACM IFIP USENIX, P390
   BANSAL D, 2001, P ACM SIGCOOM C APPL, P253
   BEIGBEDER T, 2003, P ACM SIGCOMM 3 WORK, P144
   BERNIER Y, 2001, P GAM DEV C
   Cai WT, 1999, THIRTEENTH WORKSHOP ON PARALLEL AND DISTRIBUTED SIMULATION - PROCEEDINGS, P82, DOI 10.1109/PADS.1999.766164
   Chen L, 2005, 11TH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL I, PROCEEDINGS, P795
   Cheng L, 2001, JOINT 4TH IEEE INTERNATIONAL CONFERENCE ON ATM (ICATM'01) AND HIGH SPEED INTELLIGENT INTERNET SYMPOSIUM, P222, DOI 10.1109/ICATM.2001.932090
   Claypool M, 2003, IEEE IPCCC, P263
   Claypool M, 2006, COMMUN ACM, V49, P40, DOI 10.1145/1167838.1167860
   Cottrell L., 2003, P PAM WORKSH
   DAS TK, 1997, P ACM S VIRT REAL SO, P157
   Delaney D, 2006, PRESENCE-TELEOP VIRT, V15, P465, DOI 10.1162/pres.15.4.465
   DOURISH P, 1995, P EUR C COMP SUPP CO, P213
   Dube P, 2005, GLOB TELECOMM CONF, P920
   Feng WC, 2002, IMW 2002: PROCEEDINGS OF THE SECOND INTERNET MEASUREMENT WORKSHOP, P151, DOI 10.1145/637201.637223
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Frecon E., 1998, Distributed Systems Engineering, V5, P91, DOI 10.1088/0967-1846/5/3/002
   Funkhouser T. A., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P85, DOI 10.1145/199404.199418
   Goel A, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1386109.1386113
   Jain M., 2002, P PASSIVE ACTIVE MEA, P14, DOI DOI 10.1109/JSAC.2003.814505
   Jehaes T., 2003, P 2 WORKSH NETW SYST, P63, DOI [10.1145/963900.963906, DOI 10.1145/963900.963906]
   Krasic C., 2001, Interactive Distributed Multimedia Systems. 8th International Workshop, IDMS 2001. Proceedings (Lecture Notes in Computer Science Vol.2158), P213
   Kravets R, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P33, DOI 10.1109/MMCS.1998.693623
   Lang Tanja, 2004, ACM Int. Conf. Proc. Ser., P233, DOI DOI 10.1145/1067343.1067373
   LEITH DJ, 2007, P WORKSH PROT FAST L, P73
   Li J, 2006, I C WIREL COMM NETW, P1019
   Liebeherr J, 2007, IEEE INFOCOM SER, P1127, DOI 10.1109/INFCOM.2007.135
   Macedonia MR, 1997, IEEE MULTIMEDIA, V4, P48, DOI 10.1109/93.580395
   Marshall Damien, 2007, Proceedings of the IET China-Ireland International Conference on Information and Communications Technologies 2007, CIICT'07, P54
   Marshall D, 2006, CGAMES'2006: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON COMPUTER GAMES: ARTIFICIAL INTELLIGENCE AND MOBILE SYSTEMS, P88
   Marshall D, 2006, IEEE ACM DIS SIM, P77
   McCoy A, 2007, ACM T MODEL COMPUT S, V17, DOI 10.1145/1276927.1276929
   MCGOVERN P, 2006, P 15 IST MOB WIR COM
   Mukherjee B, 2000, 2000 INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P165, DOI 10.1109/ICNP.2000.896301
   PALAZZI CE, 2005, COMPUTERS ENTERTAINM, V3, DOI DOI 10.1145/1063723.1063730
   Prasad R, 2003, IEEE NETWORK, V17, P27, DOI 10.1109/MNET.2003.1248658
   Rejaie R, 1999, IEEE INFOCOM SER, P1337, DOI 10.1109/INFCOM.1999.752152
   Roehle B, 1997, IEEE SPECTRUM, V34, P32, DOI 10.1109/6.576006
   Singhal S., 1999, Networked Virtual Environments
   Trefftz H, 2003, PRESENCE-TELEOP VIRT, V12, P37, DOI 10.1162/105474603763835323
   VAGHI I, 1999, P ACM S VIRT REAL SO, P42
   Widmer J, 2001, IEEE NETWORK, V15, P28, DOI 10.1109/65.923938
   XUE L, 2002, P 4 INT WORKSH DISTR, P135
   Yasui Takahiro., 2005, NETGAMES 05, P1
   Yu Y, 2007, IEEE IC COMP COM NET, P563
   Zhang XY, 2004, TENTH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED SYSTEMS, PROCEEDINGS, P445, DOI 10.1109/ICPADS.2004.1316125
NR 50
TC 4
Z9 4
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2010
VL 6
IS 4
AR 30
DI 10.1145/1865106.1865114
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 688VA
UT WOS:000284880900008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU He, WB
   Nahrstedt, K
   Liu, X
AF He, Wenbo
   Nahrstedt, Klara
   Liu, Xue
TI End-to-End Delay Control of Multimedia Applications over Multihop
   Wireless Links
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Design; End-to-end delay QoS; wireless ad hoc networks
ID QUALITY-OF-SERVICE; FRAMEWORK
AB The proliferation of multimedia applications over mobile, resource-constrained wireless networks has raised the need for techniques that adapt these applications both to clients' Quality of Service (QoS) requirements and to network resource constraints. This article investigates the upper-layer adaptation mechanisms to achieve end-to-end delay control for multimedia applications. The proposed adaptation approach spans application layer, middleware layer and network layer. In application layer, the requirement adaptor dynamically changes the requirement levels according to end-to-end delay measurement and acceptable QoS requirements for the end-users. In middleware layer, the priority adaptor is used to dynamically adjust the service classes for applications using feedback control theory. In network layer, the service differentiation scheduler assigns different network resources (e. g., bandwidth) to different service classes. With the coordination of these three layers, our approach can adaptively assign resources to multimedia applications. To evaluate the impact of our adaptation scheme, we built a real IEEE 802.11 ad hoc network testbed. The test-bed experiments show that the proposed upper-layer adaptation for end-to-end delay control successfully adjusts multimedia applications to meet delay requirements in many scenarios.
C1 [He, Wenbo; Nahrstedt, Klara] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
   [Liu, Xue] McGill Univ, Sch Comp Sci, Montreal, PQ H3A 2A7, Canada.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   McGill University
RP He, WB (corresponding author), Univ Illinois, Dept Comp Sci, 201 N Goodwin Ave, Urbana, IL 61801 USA.
RI He, Wenbo/KEI-0262-2024
OI He, Wenbo/0000-0001-8606-2920
CR Aad I, 2001, IEEE INFOCOM SER, P209, DOI 10.1109/INFCOM.2001.916703
   ABDELZAHER T, 2002, IEEE T PARALL DISTRI, V13
   AHN GS, 2002, P ANN JOINT C IEEE C
   Banchs A, 2002, INT WORKSH QUAL SERV, P237, DOI 10.1109/IWQoS.2002.1006592
   BARRY MG, 2001, P ANN JOINT C IEEE C
   CHEN WT, 2002, P IEEE VLSI TEST S
   Chung K.L, 2000, COURSE PROBABILITY T
   DIAO Y, 2002, P AM CONTR C ACC
   Diao YX, 2005, IEEE J SEL AREA COMM, V23, P2213, DOI 10.1109/JSAC.2005.857206
   DOVROLIS C, 1999, IEEE NETW, V13
   FELLER W., 1971, An Introduction to Probability Theory and Its Applications, V2
   Gahng-Seop Ahn, 2002, IEEE Transactions on Mobile Computing, V1, P192, DOI 10.1109/TMC.2002.1081755
   Grilo A, 2002, 13TH IEEE INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR AND MOBILE RADIO COMMUNICATIONS, VOL 1-5, PROCEEDINGS, P511, DOI 10.1109/PIMRC.2002.1046753
   HE W, 2006, P 12 IEEE REAL TIM E
   HELLERSTEIN JL, 2004, P AM CONTR C
   Hellerstein Joseph L, 2004, Feedback control of computing systems
   IEEE Computer Society, 80211 IEEE COMP SOC
   KARAMANOLIS C, 2005, P USENIX WORKSH HOT, P49
   Lee SB, 2000, J PARALLEL DISTR COM, V60, P374, DOI 10.1006/jpdc.1999.1613
   Li BC, 2005, INT CON DISTR COMP S, P471, DOI 10.1109/ICDCS.2005.30
   Li BC, 1999, IEEE J SEL AREA COMM, V17, P1632, DOI 10.1109/49.790486
   Ljung L, 1999, PRENTICE HALL INFORM, P503
   LU CY, 2001, P IEEE REAL TIM TECH
   Lu Y, 2004, IEEE T PARALL DISTR, V15, P440, DOI 10.1109/TPDS.2004.1278101
   LUO H, 2000, P ACM ANN INT C MOB
   MANGOLD S, 2002, P C EUR WIR
   Sheu ST, 2001, IEEE J SEL AREA COMM, V19, P2065, DOI 10.1109/49.957320
   Sobrinho JL, 1999, IEEE J SEL AREA COMM, V17, P1353, DOI 10.1109/49.779919
   VAIDYA NH, 2000, P 6 ANN INT C MOB CO, P167
   XUE Q, 2004, ACM MULTIMEDIA
   YANG Y, 2004, P INT C MOB AD HOC S
   Zhang Yuting., 2005, P 1 ACMUSENIX INT C, P2
NR 32
TC 11
Z9 11
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2008
VL 5
IS 2
AR 16
DI 10.1145/1413862.1413869
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 376AU
UT WOS:000261155900007
DA 2024-07-18
ER

PT J
AU Ursu, MF
   Thomas, M
   Kegel, I
   Williams, D
   Tuomola, M
   Lindstedt, I
   Wright, T
   Leurdijk, A
   Zsombori, V
   Sussner, J
   Myrestam, ULF
   Hall, N
AF Ursu, Marian F.
   Thomas, Maureen
   Kegel, Ian
   Williams, Doug
   Tuomola, Mika
   Lindstedt, Inger
   Wright, Terence
   Leurdijk, Andra
   Zsombori, Vilmos
   Sussner, Julia
   Myrestam, Ulf
   Hall, Nina
TI Interactive TV Narratives: Opportunities, Progress, and Challenges
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Experimentation; Human Factors; Languages; Theory; Interactive;
   nonlinear; television; media; screen media; narrativity; computational
   narrativity; digital storytelling; entertainment; shapeshifting
ID SHAPESHIFTING TV
AB This article is motivated by the question whether television should do more than simply offer interactive services alongside ( and separately from) traditional linear programs, in the context of its dominance being seriously challenged and threatened by interactive forms of screen media entertainment. It suggests: yes. Interactive narrativity, that is, the ability to interact with ( and influence) stories whilst they are being told, represents one clear development path for interactive television. The capabilities of computing technology are ripe for exploring this new form of storytelling, from creation to commercial distribution. The article starts by looking at the relationship between narrativity and interactivity in the current context of screen media, and identifies clear signs of interest from certain European public broadcasters in interactive TV narratives. It then presents in detail four recent experimental interactive TV productions in the genres of drama, news, and documentary, developed in collaboration with public broadcasters, which illustrate the potential and richness of this new form of storytelling, but also highlight new technological capabilities necessary for such productions. A number of essential technological requirements are then discussed in more detail in the final part. The article suggests that the ShapeShifting Media Technology, employed in the implementation of the four productions, has made significant advances both at the technological and the creative ends in supporting the development of interactive TV narrativity, but, however, that further developments are required before being able to answer questions such as "Would end users want such a form of screen media entertainment?" and "Would it be effective for both end users and producers?"
C1 [Ursu, Marian F.; Zsombori, Vilmos] Univ London, Dept Comp, Narrat & Interact Media Grp, London SE14 6NW, England.
   [Thomas, Maureen; Sussner, Julia] Univ Cambridge, Dept Architecture, Cambridge CB2 1PX, England.
   [Kegel, Ian; Williams, Doug] Future Content Grp, BT, Ipswich IPS 3RE, Suffolk, England.
   [Tuomola, Mika] Univ Art & Design Helsinki, FIN-00560 Helsinki, Finland.
   [Lindstedt, Inger] Univ Malmo, Sch Arts & Commun, S-20506 Malmo, Sweden.
   [Wright, Terence] Univ Ulster, Sch Art & Design, Belfast BT15 1ED, Antrim, North Ireland.
   [Leurdijk, Andra] TNO, NL-2600 GB Delft, Netherlands.
C3 University of London; University of Cambridge; Aalto University; Malmo
   University; Ulster University; Netherlands Organization Applied Science
   Research
RP Ursu, MF (corresponding author), Univ London, Dept Comp, Narrat & Interact Media Grp, London SE14 6NW, England.
RI Akalugwu, Kenneth/F-4815-2014
FU NM2: New Media, New Millenium [FP6 IST-004124]; TA2: Together Anytime,
   Together Anywhere [FP7 214793]; European Union's research Framework
   Programmes 6 and 7
FX This research was supported by the integrated projects NM2: New Media,
   New Millenium (FP6 IST-004124) and TA2: Together Anytime, Together
   Anywhere (FP7 214793), partly funded by the European Union's research
   Framework Programmes 6 and 7.
CR *2K GAM, 2007, BIOSH
   ARDEVOL E, 2002, ANTHRO ACTION, V9, P32
   Atkins B., 2003, More than a game: The computer game as fictional form
   Atkins B., 2007, VIDEOGAME PLAYER TEX
   Barbash Ilisha., 1997, CROSS CULTURAL FILMM
   Barkin G, 2000, SOC SCI COMPUT REV, V18, P125, DOI 10.1177/089443930001800202
   *BBC, 2008, DOC MULT
   *BBC, 2008, DRAM MULT
   *BBC, 2008, PURP PLAN DEL PUBL B
   Bocconi S., 2004, P 1 ACM WORKSHOP STO, P9
   BONNETT M, 2008, SOFIAS DIARY
   BROCK J, 1986, CASUALTY
   BUGHIN JR, 2004, USING MOBILE PHONES
   BUSHOFF B, 2005, DEV INTERACTIVE NARR
   CHUNG DS, 2007, CONVERGENCE, V1
   DOWNES EJ, 2000, NEW MEDIA SOC, V6
   EVANS R, 1999, HOLBY CITY
   *FORR, 2006, GEN X GEN Y CANT LIV
   Freytag G., 1898, Technique of the Drama, V2nd
   GRIGSBY M, 1975, PEOPLES LAND ESKIMOS
   Gunter B., 2003, NEWS NET
   HYAMS L, 2007, DUBPLATE DRAMA
   JARNHED R, 2007, TRUTH MARIKA
   Jenkins H., 2006, Convergence Culture
   Jenkins Henry., 2004, 1 PERSON, P118
   JENNINGS H, 1943, I WAS FIREMAN
   Jennings Humphrey, 1939, SPARE TIME
   Jensen J.F., 2005, Proc. the Second Australasian Conference on Interactive entertaINPent, P89
   KIOUSIS S, 2002, NEW MEDIA SOC, V3
   *KPMG, 2007, IMP DIG GEN AP
   Larsson H, 2008, LECT NOTES COMPUT SC, V5066, P30, DOI 10.1007/978-3-540-69478-6_4
   Leurdijk A., 2007, From Public Service Broadcasting to Public Service Media, P71
   LIMONARD S, 2007, IST004124
   Malinowski B., 1922, ARGONAUTS W PACIFIC
   Mateas M, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P236
   MATEAS M, 2005, P DIG GAM RES ASS 20
   MATEAS M, 2005, P AAAI 1 ANN ART INT
   MCCRUM R, 2002, OBSERVER        1029
   MEVISSEN F, 2007, IST004124
   Murray Janet H., 1997, Hamlet on the Holodeck: The Future of Narrative in Cyberspace
   MURRAY JH, 2005, DIGRA 2005 VANC CAN
   *PACKETVISION, IPTV SERV PROV IMPL
   PALS N, 2007, IST004124
   Parker P., 1998, The art and science of screenwriting
   Patterson T.E., 2007, YOUNG PEOPLE NEWS
   PEAKE M., 1992, GORMENGHAST TRILOGY
   PELLINEN T, 2000, AQUARIUM
   PETER B, 2007, INDEPENDENT     0314
   Ramis Harold., 1993, GROUNDHOG DAY
   Rieser Martin., 2002, NEW SCREEN MEDIA CIN
   Ryan Marie-Laure., 2001, NARRATIVE VIRTUAL RE
   SAARINEN L, 2007, IST004124
   *SAG, 2007, WRIT INT FICT RES 20
   SCOTT B, 2005, TELEVISION NEW MEDIA, V1
   *SEG TRUVIDEO, 2005, FAHR
   *SHAPESHIFTING MED, 2005, SHAPESHIFTING MED PO
   SINGER A, 2002, FORM LECT U MANCH
   Stan L, 2003, PROBL POST-COMMUNISM, V50, P51, DOI 10.1080/10758216.2003.11656063
   SURMAN D, 2008, VIDEOGAMES HDB
   Sussner J, 2006, DIGIT CREAT, V17, P243, DOI 10.1080/14626260601074243
   Thomas M., 2003, ARCHITECTURES ILLUSI
   TUOMOLA ML, 2006, ACCIDENTAL LOVERS
   Tykwer Tom., 1998, LOLA RENNT
   Ursu MF, 2008, MULTIMEDIA SYST, V14, P115, DOI 10.1007/s00530-008-0119-z
   Ursu MF, 2007, LECT NOTES COMPUT SC, V4471, P96
   Voytilla S., 1999, MYTH MOVIES
   *W3C, 2005, SYNCHR MULT INT LANG
   WAND E, 2002, NEW SCREEN MEDIA CIN
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WILSON A, 2000, GORMENGHAST
   WOLF D, 2004, LAW ORDER CRIMINAL I
   WRIGHT T, 1994, EUROPEAN IMAGERY COL
   Wright Terence., 2004, VISUAL STUD, V19, P97, DOI DOI 10.1080/1472586042000204870
   ZIMMERMAN E, 2008, NEW MEDIA MARKETS, V26, P4
   ZIMMERMAN E, 2005, DEV INTERACTIVE NARR, P452
   ZIMMERMAN E, 2008, SCREEN DIGEST    JAN
   2008, TIMESONLINE     0529
   2008, WHATS ON TV     0509
   2008, TELEGRAPH       0428
NR 79
TC 32
Z9 36
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 4
AR 25
DI 10.1145/1412196.1412198
PG 39
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science
GA 376AP
UT WOS:000261155300002
DA 2024-07-18
ER

PT J
AU Goh, K
   Li, BT
   Chang, EY
AF Goh, Kingshy
   Li, Beitao
   Chang, Edward Y.
TI Semantics and Feature Discovery via Confidence-Based Ensemble
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Performance; Classification confidence; image annotation;
   semantics discovery
AB Providing accurate and scalable solutions to map low-level perceptual features to high-level semantics is essential for multimedia information organization and retrieval. In this paper, we propose a confidence-based dynamic ensemble (CDE) to overcome the shortcomings of the traditional static classifiers. In contrast to the traditional models, CDE can make dynamic adjustments to accommodate new semantics, to assist the discovery of useful low-level features, and to improve class-prediction accuracy. We depict two key components of CDE: a multi-level function that asserts class-prediction confidence, and the dynamic ensemble method based upon the confidence function. Through theoretical analysis and empirical study, we demonstrate that CDE is effective in annotating large-scale, real-world image datasets.
C1 [Goh, Kingshy; Li, Beitao; Chang, Edward Y.] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara
RP Goh, K (corresponding author), Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
EM kingshy@gmail.com; beitao_li@yahoo.com; echang@ece.ucsb.edu
CR [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], 1982, ESTIMATION DEPENDENC
   [Anonymous], P ACM MULT
   BENITEZ AB, 2002, P IEEE INT C MULT
   Bouchaffra D, 1999, IEEE T PATTERN ANAL, V21, P923, DOI 10.1109/34.790432
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655
   Chang E, 2003, IEEE T CIRC SYST VID, V13, P26, DOI 10.1109/TCSVT.2002.808079
   Chang SF, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P531, DOI 10.1109/ICIP.1998.727321
   CHOW CK, 1970, IEEE T INFORM THEORY, V16, P41, DOI 10.1109/TIT.1970.1054406
   Collobert R, 2001, J MACH LEARN RES, V1, P143, DOI 10.1162/15324430152733142
   Fan Jianping., 2004, ACM International Conference on Multimedia, P540, DOI [DOI 10.1145/1027527, DOI 10.1145/1027527.1027660]
   GOH K, 2004, ONE 2 CLASS SVMS MUL
   GOH KS, 2001, P 10 INT C INF KNOWL, P395
   Hastie T, 1998, ADV NEUR IN, V10, P507
   He X., 2002, PROC ACM MULTIMEDIA, P343
   Heng Tao Shen, 2000, Proceedings ACM Multimedia 2000, P39, DOI 10.1145/354384.376098
   HO TK, 1994, IEEE T PATTERN ANAL, V16, P66, DOI 10.1109/34.273716
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Liu WY, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P326
   Naphade MR, 2002, PROC SPIE, V4676, P264
   Platt JC, 2000, ADV NEUR IN, P61
   Platt JC, 2000, ADV NEUR IN, V12, P547
   PODDAR P, 1993, P INT C NEUR NETW, V1, P287
   Rodriguez C, 1998, INT C PATT RECOG, P1101, DOI 10.1109/ICPR.1998.711886
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Srihari R. K., 2000, Information Retrieval, V2, P245, DOI 10.1023/A:1009962928226
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Wu G., 2003, ICML, P816, DOI DOI 10.5555/3041838.3041941
   WU H, 2002, VIS DATAB, P327
NR 30
TC 4
Z9 4
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2005
VL 1
IS 2
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DV
UT WOS:000205012300004
DA 2024-07-18
ER

PT J
AU Li, H
   Wang, JW
   Xiong, N
   Zhang, Y
   Vasilakos, AV
   Luo, XY
AF Li, Hao
   Wang, Jinwei
   Xiong, Neal
   Zhang, Yi
   Vasilakos, Athanasios V.
   Luo, Xiangyang
TI A Siamese Inverted Residuals Network Image Steganalysis Scheme based on
   Deep Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Urban scenes; multimedia computing; steganalysis; siamese network;
   Inverted Residuals
ID STEGANOGRAPHY; CNN
AB With the rapid proliferation of urbanization, massive data in social networks are collected and aggregated in real time, making it possible for criminals to use images as a cover to spread secret information on the Internet. How to determine whether these images contain secret information is a huge challenge for multimedia computing security. The steganalysis method based on deep learning can effectively judge whether the pictures transmitted on the Internet in urban scenes contain secret information, which is of great significance to safeguarding national and social security. Image steganalysis based on deep learning has powerful learning ability and classification ability, and its detection accuracy of steganography images has surpassed that of traditional steganalysis based on manual feature extraction. In recent years, it has become a hot topic of the information hiding technology. However, the detection accuracy of existing deep learning based steganalysis methods still needs to be improved, especially when detecting arbitrary-size and multi-source images, their detection efficientness is easily affected by cover mismatch. In this manuscript, we propose a steganalysis method based on Inverse Residuals structured Siamese network (abbreviated as SiaIRNet method, Siamese-Inverted-Residuals-Network Based method). The SiaIRNet method uses a siamese convolutional neural network (CNN) to obtain the residual features of subgraphs, including three stages of preprocessing, feature extraction, and classification. Firstly, a preprocessing layer with high-pass filters combined with depth-wise separable convolution is designed to more accurately capture the correlation of residuals between feature channels, which can help capture rich and effective residual features. Then, a feature extraction layer based on the Inverse Residuals structure is proposed, which improves the ability of the model to obtain residual features by expanding channels and reusing features. Finally, a fully connected layer is used to classify the cover image and the stego image features. Utilizing three general datasets, BossBase-1.01, BOWS2, and ALASKA#2, as cover images, a large number of experiments are conducted comparing with the state-of-the-art steganalysis methods. The experimental results show that compared with the classical SID method and the latest SiaStegNet method, the detection accuracy of the proposed method for 15 arbitrary-size images is improved by 15.96% and 5.86% on average, respectively, which verifies the higher detection accuracy and better adaptability of the proposed method to multi-source and arbitrary-size images in urban scenes.
C1 [Li, Hao; Zhang, Yi] State Key Lab Math Engn & Adv Comp, 62 Sci Ave, Zhengzhou City 450001, Peoples R China.
   [Wang, Jinwei] Nanjing Univ Informat Sci & Technol, 219 Ningliu Rd, Nanjing 210044, Peoples R China.
   [Xiong, Neal] Sul Ross State Univ, Dept Comp Sci & Math, 1404 East Highway 90, Alpine, TX 79830 USA.
   [Vasilakos, Athanasios V.] Univ Agder UiA, Ctr Res CAIR, Jon Lilletunsvei 9, N-4630 Grimstad, Norway.
   [Vasilakos, Athanasios V.] Fuzhou Univ, Coll Math & Comp Sci, Xueyuan Rd, Fuzhou 350116, Fujian, Peoples R China.
   [Luo, Xiangyang] State Key Lab Math Engn & Adv Comp, 62 Sci Ave, Zhengzhou City 450001, Henan Province, Peoples R China.
   [Luo, Xiangyang] Key Lab Cyberspace Situat Awareness Henan Prov, 62 Sci Ave, Zhengzhou City 450001, Henan Province, Peoples R China.
C3 Nanjing University of Information Science & Technology; Texas State
   University System; University of Agder; Fuzhou University
RP Luo, XY (corresponding author), State Key Lab Math Engn & Adv Comp, 62 Sci Ave, Zhengzhou City 450001, Henan Province, Peoples R China.; Luo, XY (corresponding author), Key Lab Cyberspace Situat Awareness Henan Prov, 62 Sci Ave, Zhengzhou City 450001, Henan Province, Peoples R China.
EM li15575963101hao@163.com; wjwei2004@163.com; neal.xiong@sulross.edu;
   tzyy4001@sina.com; thanos.vasilakos@uia.no; luoxy_ieu@sina.com
RI xiong, naixue/M-4277-2019; vasilakos, athanasios/J-2824-2017
OI xiong, naixue/0000-0002-0394-4635; Vasilakos,
   Athanasios/0000-0003-1902-9877
FU National Natural Science Foundation of China [U1804263, 62172435];
   Zhongyuan Science and Technology Innovation Leading Talent Project,
   China [214200510019]; National Key Research and Development Program of
   China [2022YFB3102900]
FX This work was supported by the National Natural Science Foundation of
   China (No. U1804263, 62172435) and the Zhongyuan Science and Technology
   Innovation Leading Talent Project, China (No. 214200510019), the
   National Key Research and Development Program of China (No.
   2022YFB3102900).
CR Anwar S, 2017, ACM J EMERG TECH COM, V13, DOI 10.1145/3005348
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bas Patrick, 2022, BOWS 2
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Chen jun-fu, 2021, Journal of Software, P551, DOI 10.13328/j.cnki.jos.006135
   Cheng Wenhuang, 2021, ACM T MULTIM COMPUT, V17, p3s
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cogranne Remi, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P178, DOI 10.1007/978-3-642-24178-9_13
   Cogranne R, 2019, IH&MMSEC '19: PROCEEDINGS OF THE ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P125
   Cogranne Remi, 2022, DOCUMENTATION ALASKA
   Dumitrescu S, 2003, LECT NOTES COMPUT SC, V2578, P355
   Filler Tomas, 2009, P SPIE INT SOC OPTIC, V7254, P107
   Fridrich J., 2001, IEEE Multimedia, V8, P22, DOI 10.1109/93.959097
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Furon Teddy, 2021, TRUSTW AI 21 1 INT W
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Holub V., 2013, P 1 ACM WORKSH INF H, P59, DOI DOI 10.1145/2482513.2482514
   Holub V, 2013, IEEE T INF FOREN SEC, V8, P1996, DOI 10.1109/TIFS.2013.2286682
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Ker AD, 2007, IEEE SIGNAL PROC LET, V14, P525, DOI 10.1109/LSP.2006.891319
   Ker AD, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P107, DOI 10.1145/1411328.1411349
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Li B, 2014, IEEE IMAGE PROC, P4206, DOI 10.1109/ICIP.2014.7025854
   Li H, 2017, Arxiv, DOI arXiv:1608.08710
   Mazurczyk W, 2018, COMMUN ACM, V61, P86, DOI 10.1145/3158416
   Mingxing Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10778, DOI 10.1109/CVPR42600.2020.01079
   Pevny T, 2015, PROC SPIE, V9409, DOI 10.1117/12.2083216
   Qian YL, 2016, IEEE IMAGE PROC, P2752, DOI 10.1109/ICIP.2016.7532860
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tan SQ, 2021, IEEE T INF FOREN SEC, V16, P131, DOI 10.1109/TIFS.2020.3005304
   Tang WX, 2016, IEEE T INF FOREN SEC, V11, P734, DOI 10.1109/TIFS.2015.2507159
   Tong C, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3381088
   Tsang CF., 2018, ELECT IMAG, V2018, p121, DOI [DOI 10.2352/ISSN.2470-1173.2018.07.MWSF-121, 10.2352/ISSN.2470-1173.2018.07.MWSF-121]
   Xu G., 2016, P 4 ACM WORKSH INF H, P103, DOI DOI 10.1145/2909827.2930798
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   Yedroudj M, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2092, DOI 10.1109/ICASSP.2018.8461438
   You WK, 2021, IEEE T INF FOREN SEC, V16, P291, DOI 10.1109/TIFS.2020.3013204
   Yous Y, 2021, PROCEEDINGS OF THE 2021 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, IH&MMSEC 2021, P149, DOI 10.1145/3437880.3460397
   Yousfi Y, 2019, IH&MMSEC '19: PROCEEDINGS OF THE ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P138, DOI 10.1145/3335203.3335727
   Zhang R, 2020, IEEE T INF FOREN SEC, V15, P1138, DOI 10.1109/TIFS.2019.2936913
   Zhang Y, 2021, INFORM SCIENCES, V564, P306, DOI 10.1016/j.ins.2021.02.058
NR 44
TC 0
Z9 0
U1 7
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 214
DI 10.1145/3579166
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200037
DA 2024-07-18
ER

PT J
AU Wei, H
   Chen, R
AF Wei, Hao
   Chen, Rui
TI A Multi-Level Consistency Network for High-Fidelity Virtual Try-On
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image-based virtual try-on; flow-guided deformation; structural and
   textural consistency
AB The 2D virtual try-on task aims to transfer a target clothing image to the corresponding region of a person image. Although an extensive amount of research has been conducted due to its immense applications, this task still remains a great challenge to handle some complicated issues (e.g., non-rigid shapes, large occlusions and arbitrary poses). To this end, we propose a novel network with structural and textural consistency-preserving mechanism for producing high-fidelity try-on images. Specifically, we first generate the semantic layout of a clothing-agnostic person to obtain the segmentation map, which is used as the transforming conditions of the target clothes. Based on a recurrent network structure, the transform lookup is performed to iteratively update a dense flow. Then, we adopt a thin-plate-spline-based warping method to estimate the coarse offset flow for all key-point positions. Guided by this sparse flow, a multi-scale deformable convolution module is designed to further iteratively predict the fine offsets for densely sampled positions, by which the clothing item and person shape can be accurately aligned. Finally, we develop a refinement module to effectively fuse the global and local features, which can render accurate geometric structures of the body parts and maintain texture sharpness of the clothes. Extensive experiments on benchmark datasets demonstrate that our method outperforms other state-of-the-art methods in terms of quantitative and qualitative try-on results. The code is available on: https://github.com/TJU- WEIHAO/MLCN.
C1 [Wei, Hao; Chen, Rui] Tianjin Univ, Sch Microelect, Tianjin, Peoples R China.
C3 Tianjin University
RP Wei, H (corresponding author), Tianjin Univ, Sch Microelect, Tianjin, Peoples R China.
EM weihao@tju.edu.cn; ruichen@tju.edu.cn
OI Chen, Rui/0000-0002-8003-4643
CR [Anonymous], 1977, Construction theory of functions of several variables, DOI [DOI 10.1007/BFB0086566, 10.1007/BFb0086566]
   Brock A., 2018, PREPRINT
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X
   Feng ZL, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3326332
   Fincato M, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491226
   Gao X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P563, DOI 10.1145/3474085.3475210
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883
   Han XT, 2019, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2019.00458
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jandial Surgan, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2171, DOI 10.1109/WACV45572.2020.9093458
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Lee HJ, 2019, IEEE INT CONF COMP V, P3129, DOI 10.1109/ICCVW.2019.00381
   Li Honglin, 2021, ACM T MULTIM COMPUT, V17, P3
   MatiurRahman Minar, 2020, P IEEECVF C COMPUTER
   Minar M. R., 2020, CVPR WORKSH
   Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Raj A, 2018, LECT NOTES COMPUT SC, V11216, P679, DOI 10.1007/978-3-030-01258-8_41
   Reed S, 2016, PR MACH LEARN RES, V48
   Sekine M., 2014, INT C 3D BODY SCANNI
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14030, DOI 10.1109/ICCV48922.2021.01379
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3425636
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Zanfir M, 2018, PROC CVPR IEEE, P5391, DOI 10.1109/CVPR.2018.00565
   Zhang FF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478642
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou Zhenglong, 2012, SIGGRAPH ASIA 2012 T
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 45
TC 0
Z9 0
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 156
DI 10.1145/3580500
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300005
OA Bronze
DA 2024-07-18
ER

PT J
AU Dou, P
   Zeng, Y
   Wang, ZQ
   Hu, HF
AF Dou, Peng
   Zeng, Ying
   Wang, Zhuoqun
   Hu, Haifeng
TI Multiple Temporal Pooling Mechanisms for Weakly Supervised Temporal
   Action Localization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Weakly supervised temporal action localization; multiple instance
   learning; temporal pooling
AB Recent action localization works learn in a weakly supervised manner to avoid the expensive cost of human labeling. Those works are mostly based on the Multiple Instance Learning framework, where temporal pooling is an indispensable part that usually relies on the guidance of snippet-level Class Activation Sequences (CAS). However, we observe that previous works only leverage a simple convolutional neural network for the generation of CAS, which ignores the weak discriminative foreground action segments and the background ones, and meanwhile, the relationship between different actions has not been considered. To solve this problem, we propose multiple temporal pooling mechanisms (MTP) for a more sufficient information utilization. Specifically, with the design of the Foreground Variance Branch, Dual Foreground Attention Branch and Hybrid Attention Fine-tuning Branch, MTP can leverage more effective information from different aspects and generate different CASs to guide the learning of temporal pooling. Moreover, different loss functions are designed for a better optimization of individual branches, aiming to effectively distinguish the action from the background. Our method shows excellent results on the THUMOS14 and ActivityNet1.2 datasets.
C1 [Dou, Peng; Zeng, Ying; Wang, Zhuoqun; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM doup@mail2.sysu.edu.cn; zengy268@mail2.sysu.edu.cn;
   wangzhq53@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn
RI Peng/AAP-6413-2021
OI Wang, Zhuoqun/0000-0001-8633-0235; Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [62076262, 61673402,
   61273270, 60802069]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076262, Grant 61673402, Grant
   61273270, and Grant 60802069.
CR Buch S., 2019, P BRIT MACHINE VISIO
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gong G., 2020, 2020 IEEE CVF C COMP
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Huang LJ, 2020, AAAI CONF ARTIF INTE, V34, P11053
   Huang LJ, 2021, IEEE T IMAGE PROCESS, V30, P5154, DOI 10.1109/TIP.2021.3078324
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Islam A, 2021, AAAI CONF ARTIF INTE, V35, P1637
   Islam A, 2020, IEEE WINT CONF APPL, P536, DOI 10.1109/WACV45572.2020.9093620
   Ji Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P853, DOI 10.1145/3474085.3475261
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, ARXIV170506950
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu Z., 2021, ACSNET ACTION CONTEX
   Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Luo W, 2021, PROC CVPR IEEE, P9964, DOI 10.1109/CVPR46437.2021.00984
   Min Kyle, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P283, DOI 10.1007/978-3-030-58568-6_17
   Moniruzzaman M, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2166, DOI 10.1145/3394171.3413687
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Pardo A, 2021, IEEE WINT CONF APPL, P3318, DOI 10.1109/WACV48630.2021.00336
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   PengDou Wei Zhou, 2021, PATTERN RECOGN, P459
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Qin XL, 2020, IEEE SIGNAL PROC LET, V27, P1520, DOI 10.1109/LSP.2020.3018914
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shou Z., 2018, P EUROPEAN C COMPUTE
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson G.A., 2016, Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang BL, 2021, IEEE SIGNAL PROC LET, V28, P503, DOI 10.1109/LSP.2021.3061289
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu M., 2020, CVPR, P10156
   Yang XT, 2019, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2019.00035
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yin C, 2019, ELECTRON LETT, V55, P1126, DOI 10.1049/el.2019.2088
   Yu T, 2019, IEEE I CONF COMP VIS, P5521, DOI 10.1109/ICCV.2019.00562
   Yuan Yuan, 2019, ICLR 2019 7 INT C LE
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Zeng RH, 2022, IEEE T PATTERN ANAL, V44, P6209, DOI 10.1109/TPAMI.2021.3090167
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zeng RH, 2019, IEEE T IMAGE PROCESS, V28, P5797, DOI 10.1109/TIP.2019.2922108
   Zhao PS, 2022, IEEE SIGNAL PROC LET, V29, P194, DOI 10.1109/LSP.2021.3132287
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhekun Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P729, DOI 10.1007/978-3-030-58526-6_43
NR 59
TC 2
Z9 2
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 108
DI 10.1145/3567828
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300008
DA 2024-07-18
ER

PT J
AU Duan, W
   Yu, Y
   Zhang, XL
   Tang, SH
   Li, W
   Oyama, K
AF Duan, Wei
   Yu, Yi
   Zhang, Xulong
   Tang, Suhua
   Li, Wei
   Oyama, Keizo
TI Melody Generation from Lyrics with Local Interpretability
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Melody generation from lyrics; GAN; local interpretability; mutual
   information; Transformer
AB Melody generation aims to learn the distribution of real melodies to generate new melodies conditioned on lyrics, which has been a very interesting topic in the area of artificial intelligence and music. However, a challenging issue still limits the quality and reliability of melody generation conditioned on lyrics: how to enhance the interpretability between the input lyrics and generated melodies so humans can understand their relationships. To solve this issue, in this article, we propose a model for melody generation from lyrics with local interpretability, which contains two significant contributions: (i) Mutual information between input lyrics and generated melody is exploited to instruct the training of the network, which avoids the loss of content consistency during the training stage. (ii) Transformer is explored to efficiently extract semantic features from lyrics sequences, which provides more interpretable correlations between different syllables in lyrics. Experiments on a large-scale dataset with paired lyrics-melodies demonstrate that the proposed approach can generate higher-quality melodies from lyrics compared with existing methods.
C1 [Duan, Wei; Yu, Yi] SOKENDAI, Natl Inst Informat, Digital Content & Media Sci Res Div, 2-1-2 Hitotsubashi,Chiyoda Ku, Tokyo 1018430, Japan.
   [Zhang, Xulong; Li, Wei] Fudan Univ, Sch Comp Sci, 2005 Songhu Rd, Shanghai 200438, Peoples R China.
   [Tang, Suhua] Univ Elect Commun, Grad Sch Informat & Engn, Dept Comp & Network Engn, I-5-1 Chofugaoka, Chofu, Tokyo 1828585, Japan.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; Fudan University; University of
   Electro-Communications - Japan
RP Duan, W (corresponding author), SOKENDAI, Natl Inst Informat, Digital Content & Media Sci Res Div, 2-1-2 Hitotsubashi,Chiyoda Ku, Tokyo 1018430, Japan.
EM weiduan@nii.ac.jp; yiyu@nii.ac.jp; xlzhang14@fudan.edu.cn;
   shtang@uec.ac.jp; weili-fudan@fudan.edu.cn; oyama@nii.ac.jp
RI Zhang, Xulong/AAH-5645-2019
OI Zhang, Xulong/0000-0001-7005-992X; Duan, Wei/0000-0002-9007-1667; Li,
   Wei/0000-0002-4486-8341; Tang, Suhua/0000-0002-5784-8411
CR Ackerman M, 2017, LECT NOTES COMPUT SC, V10198, P1, DOI 10.1007/978-3-319-55750-2_1
   Bao HB, 2019, LECT NOTES ARTIF INT, V11838, P499, DOI 10.1007/978-3-030-32233-5_39
   Bounliphone W., 2016, 4 INT C LEARNING REP
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen X, 2016, ADV NEUR IN, V29
   Dhariwal P, 2020, Arxiv, DOI arXiv:2005.00341
   Dong HW, 2018, AAAI CONF ARTIF INTE, P34
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang CA, 2019, 7 INT C LEARNING REP
   Jolicoeur-Martineau Alexia, 2018, The relativistic discriminator: A key element missing from standard GAN
   Kusner MJ, 2016, Arxiv, DOI arXiv:1611.04051
   Lee HP, 2019, NAACL HLT 2019: THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES: PROCEEDINGS OF THE DEMONSTRATIONS SESSION, P84
   Madhumani Gurunath Reddy, 2020, ARXIV
   Monteith Kristine, 2012, ICCC, P87
   Nichols Eric, 2009, 35 INT C INT COMPUTE
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Ren Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1198, DOI 10.1145/3394171.3413721
   Rudin C., 2021, arXiv:2103.11251 cs
   Sheng ZH, 2021, AAAI CONF ARTIF INTE, V35, P13798
   Togelius Julian, 2015, 6 INT C COMP CREAT I, P204
   Toivanen Jukka M., 2013, Proceedings of the Fourth International Conference on Computational Creativity, P87
   Vaswani A, 2017, ADV NEUR IN, V30
   Yu Y, 2020, Arxiv, DOI arXiv:2009.08616
   Yu Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3424116
   Zeqian Ju, 2021, ARXIV
   Zhu HY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2837, DOI 10.1145/3219819.3220105
NR 27
TC 0
Z9 0
U1 3
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 124
DI 10.1145/3572031
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300024
DA 2024-07-18
ER

PT J
AU Tang, H
   Ding, L
   Wu, SS
   Ren, B
   Sebe, N
   Rota, P
AF Tang, Hao
   Ding, Lei
   Wu, Songsong
   Ren, Bin
   Sebe, Nicu
   Rota, Paolo
TI Deep Unsupervised Key Frame Extraction for Efficient Video
   Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Key frame extraction; density peaks clustering; LSTM; weight fusion;
   unsupervised learning; video classification
ID ACTION RECOGNITION; REPRESENTATION; ROBUST
AB Video processing and analysis have become an urgent task, as a huge amount of videos (e.g., YouTube, Hulu) are uploaded online every day. The extraction of representative key frames from videos is important in video processing and analysis since it greatly reduces computing resources and time. Although great progress has been made recently, large-scale video classification remains an open problem, as the existing methods have not well balanced the performance and efficiency simultaneously. To tackle this problem, this work presents an unsupervised method to retrieve the key frames, which combines the convolutional neural network and temporal segment density peaks clustering. The proposed temporal segment density peaks clustering is a generic and powerful framework, and it has two advantages compared with previous works. One is that it can calculate the number of key frames automatically. The other is that it can preserve the temporal information of the video. Thus, it improves the efficiency of video classification. Furthermore, a long short-term memory network is added on the top of the convolutional neural network to further elevate the performance of classification. Moreover, a weight fusion strategy of different input networks is presented to boost performance. By optimizing both video classification and key frame extraction simultaneously, we achieve better classification performance and higher efficiency. We evaluate our method on two popular datasets (i.e., HMDB51 and UCF101), and the experimental results consistently demonstrate that our strategy achieves competitive performance and efficiency compared with the state-of-the-art approaches.
C1 [Tang, Hao] Swiss Fed Inst Technol, Dept Informat Technol & Elect Engn, CH-8092 Zurich, Switzerland.
   [Ding, Lei; Ren, Bin; Sebe, Nicu; Rota, Paolo] Univ Trento, Dept Informat Engn & Comp Sci DISI, I-38123 Trento, Italy.
   [Wu, Songsong] Guangdong Univ Petrochem Technol, Sch Comp Sci, Maoming 525000, Peoples R China.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; University of
   Trento; Guangdong University of Petrochemical Technology
RP Tang, H (corresponding author), Swiss Fed Inst Technol, Dept Informat Technol & Elect Engn, CH-8092 Zurich, Switzerland.
EM hao.tang@vision.ee.ethz.ch; lei.ding@unitn.it; sswuai@126.com;
   bin.ren@unitn.it; sebe@disi.unitn.it; paolo.rota@unitn.it
RI Rota, Paolo/AAG-1352-2020; Sebe, Niculae/KEC-2000-2024; Ding,
   Lei/IVV-0541-2023
OI Rota, Paolo/0000-0003-0663-5659; Sebe, Niculae/0000-0002-6597-7248;
   Ding, Lei/0000-0003-0653-8373; Wu, Songsong/0000-0001-9347-5395; Tang,
   Hao/0000-0002-2077-1246; Ren, Bin/0000-0002-9790-1504
FU PRIN project PREVUE [2017N2RK7K]; EU [951911]
FX This work was supported by the PRIN project PREVUE (Prot. 2017N2RK7K)
   and by the EU H2020 project AI4Media under grant 951911.
CR Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Cai ZW, 2014, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2014.83
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cerneková Z, 2006, IEEE T CIRC SYST VID, V16, P82, DOI 10.1109/TCSVT.2005.856896
   Choutas V, 2018, PROC CVPR IEEE, P7024, DOI 10.1109/CVPR.2018.00734
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duta IC, 2017, PROC CVPR IEEE, P3205, DOI 10.1109/CVPR.2017.341
   Ejaz N, 2012, J VIS COMMUN IMAGE R, V23, P1031, DOI 10.1016/j.jvcir.2012.06.013
   Feichtenhofer C., 2016, P INT C NEUR INF PRO, P3468
   Feichtenhofer C, 2018, PROC CVPR IEEE, P7844, DOI 10.1109/CVPR.2018.00818
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   Gao RH, 2018, PROC CVPR IEEE, P5937, DOI 10.1109/CVPR.2018.00622
   Gharbi H, 2017, INT CONF ACOUST SPEE, P1502, DOI 10.1109/ICASSP.2017.7952407
   Guan GL, 2013, IEEE T CIRC SYST VID, V23, P729, DOI 10.1109/TCSVT.2012.2214871
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Mingyi, 2014, P ICME
   James M., 1967, PROC BERKELEY S MATH, V1, P281, DOI DOI 10.1007/S11665-016-2173-6
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kar A, 2017, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR.2017.604
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kuanar SK, 2013, J VIS COMMUN IMAGE R, V24, P1212, DOI 10.1016/j.jvcir.2013.08.003
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Kulhare S, 2016, INT C PATT RECOG, P835, DOI 10.1109/ICPR.2016.7899739
   Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Liu H, 2016, CAAI T INTELL TECHNO, V1, P125, DOI 10.1016/j.trit.2016.10.001
   Liu H, 2015, IEEE IMAGE PROC, P4674, DOI 10.1109/ICIP.2015.7351693
   Long X, 2018, AAAI CONF ARTIF INTE, P7202
   Mei SH, 2015, PATTERN RECOGN, V48, P522, DOI 10.1016/j.patcog.2014.08.002
   Meng JJ, 2016, PROC CVPR IEEE, P1039, DOI 10.1109/CVPR.2016.118
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Ni BB, 2015, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2015.7298993
   Panagiotakis C, 2009, IEEE T CIRC SYST VID, V19, P447, DOI 10.1109/TCSVT.2009.2013517
   Panda R, 2014, INT C PATT RECOG, P3481, DOI 10.1109/ICPR.2014.599
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38
   de Souza CR, 2017, PROC CVPR IEEE, P2594, DOI 10.1109/CVPR.2017.278
   de Souza CR, 2016, LECT NOTES COMPUT SC, V9911, P697, DOI 10.1007/978-3-319-46478-7_43
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shou Z, 2019, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2019.00136
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9909, P71, DOI 10.1007/978-3-319-46454-1_5
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Tang H, 2019, NEUROCOMPUTING, V331, P424, DOI 10.1016/j.neucom.2018.11.038
   Tang Hao, 2015, P ACM MM
   Tran D, 2017, Arxiv, DOI [arXiv:1708.05038, DOI 10.48550/ARXIV.1708.05038]
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Vázquez-Martín R, 2013, PATTERN RECOGN LETT, V34, P770, DOI 10.1016/j.patrec.2012.12.009
   Vondrick C, 2016, 30 C NEURAL INFORM P, V29
   Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang HX, 2017, PATTERN RECOGN, V63, P268, DOI 10.1016/j.patcog.2016.10.014
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang LM, 2016, INT J COMPUT VISION, V119, P254, DOI 10.1007/s11263-015-0859-0
   Wang LM, 2014, IEEE T IMAGE PROCESS, V23, P810, DOI 10.1109/TIP.2013.2295753
   Wang XL, 2016, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2016.291
   Wang YL, 2018, PROC CVPR IEEE, P5314, DOI 10.1109/CVPR.2018.00557
   Wang YB, 2017, PROC CVPR IEEE, P2097, DOI 10.1109/CVPR.2017.226
   Wu CY, 2018, PROC CVPR IEEE, P6026, DOI 10.1109/CVPR.2018.00631
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Zhou YZ, 2018, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2018.00054
   Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219
   Zhu Y, 2018, PROC CVPR IEEE, P9436, DOI 10.1109/CVPR.2018.00983
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 77
TC 8
Z9 8
U1 7
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 119
DI 10.1145/3571735
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300019
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhang, YS
   Tan, Q
   Qi, SR
   Xue, MF
AF Zhang, Yushu
   Tan, Qing
   Qi, Shuren
   Xue, Mingfu
TI PRNU-based Image Forgery Localization with Deep Multi-scale Fusion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image forgery localization; multi-scale analysis; photo-response
   non-uniformity; deep learning
ID CAMERA IDENTIFICATION; DOMAIN
AB Photo-response non-uniformity (PRNU), as a class of device fingerprint, plays a key role in the forgery detection/localization for visual media. The state-of-the-art PRNU-based forensics methods generally rely on the multi-scale trace analysis and result fusion, with Markov random field model. However, such handcrafted strategies are difficult to provide satisfactory multi-scale decision, exhibiting a high false-positive rate. Motivated by this, we propose an end-to-end multi-scale decision fusion strategy, where a mapping from multi-scale forgery probabilities to binary decision is achieved by a supervised deep fully connected neural network. As the first time, the deep learning technology is employed in PRNU-based forensics for more flexible and reliable integration of multi-scale information. The benchmark experiments exhibit the state-ofthe-art accuracy performance of our method in both pixel-level and image-level, especially for false positives. Additional robustness experiments also demonstrate the benefits of the proposed method in resisting noise and compression attacks.
C1 [Zhang, Yushu; Tan, Qing; Qi, Shuren; Xue, Mingfu] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Zhang, Yushu] Guilin Univ Elect Technol, Guangxi Key Lab Trusted Software, Guilin 541004, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Guilin University of
   Electronic Technology
RP Qi, SR (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM yushu@nuaa.edu.cn; tanqing@nuaa.edu.cn; shurenqi@nuaa.edu.cn;
   mingfu.xue@nuaa.edu.cn
RI Qi, Shuren/JAX-8354-2023
OI Xue, Mingfu/0000-0003-2408-503X; zhang, yushu/0000-0001-8183-8435
FU Nanjing University of Aeronautics and Astronautics Graduate Research and
   Practice Innovation Program Project [xcxjh20211606]; National Natural
   Science Foundation of China [62072237]; Guangxi Key Laboratory of
   Trusted Software [KX202027]; Basic Research Program of Jiangsu Province
   [BK20201290]
FX This work was supported in part by the Nanjing University of Aeronautics
   and Astronautics Graduate Research and Practice Innovation Program
   Project under Grant No. xcxjh20211606, in part by the National Natural
   Science Foundation of China under Grant No. 62072237, in part by Guangxi
   Key Laboratory of Trusted Software under Grant No. KX202027, and in part
   by Basic Research Program of Jiangsu Province under Grant No.
   BK20201290.
CR Amerini I, 2011, IEEE T INF FOREN SEC, V6, P1099, DOI 10.1109/TIFS.2011.2129512
   Amerini I, 2009, 2009 16TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, VOLS 1 AND 2, P511
   [Anonymous], 2003, 2003 C COMPUTER VISI, DOI [DOI 10.1109/CVPRW.2003.10093, DOI 10.1109/CVPRW.2003.10093.27.T.-T]
   Böhme R, 2016, ARTECH H COMP SEC LI, P231
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Chen M, 2008, IEEE T INF FOREN SEC, V3, P74, DOI 10.1109/TIFS.2007.916285
   Chierchia Giovanni, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6231, DOI 10.1109/ICASSP.2014.6854802
   Chierchia G., 2011, 2011 17 INT C DIGITA, P1
   Christlein V, 2012, IEEE T INF FOREN SEC, V7, P1841, DOI 10.1109/TIFS.2012.2218597
   Cooper AJ, 2013, FORENSIC SCI INT, V226, P132, DOI 10.1016/j.forsciint.2012.12.018
   Cozzolino D, 2015, IEEE T INF FOREN SEC, V10, P2284, DOI 10.1109/TIFS.2015.2455334
   Cozzolino D, 2020, IEEE T INF FOREN SEC, V15, P144, DOI 10.1109/TIFS.2019.2916364
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dang-Nguyen D.T., 2015, P ACM MULT SYST C, P219
   Ertam F, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING (UBMK), P755, DOI 10.1109/UBMK.2017.8093521
   Ferreira A, 2016, IEEE T IMAGE PROCESS, V25, P4729, DOI 10.1109/TIP.2016.2593583
   Fridrich J., 2013, Digital Image Forensics, P179
   Gupta A, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176650
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126
   Jafari M, 2019, LECT NOTES COMPUT SC, V11902, P529, DOI 10.1007/978-3-030-34110-7_44
   Kang XG, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-19
   Kang XG, 2012, IEEE T INF FOREN SEC, V7, P393, DOI 10.1109/TIFS.2011.2168214
   Korus P, 2017, IEEE T INF FOREN SEC, V12, P809, DOI 10.1109/TIFS.2016.2636089
   Lawgaly A, 2014, IEEE IMAGE PROC, P5357, DOI 10.1109/ICIP.2014.7026084
   Li HD, 2018, IEEE T CIRC SYST VID, V28, P31, DOI 10.1109/TCSVT.2016.2599849
   Lin XF, 2016, IEEE T INF FOREN SEC, V11, P126, DOI 10.1109/TIFS.2015.2478748
   Lukás J, 2006, PROC SPIE, V6072, DOI 10.1117/12.640109
   Lukás J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602
   Naskar R, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487272
   Pedamonti D, 2018, Arxiv, DOI [arXiv:1804.02763, 10.48550/arXiv.1804.02763]
   Quan YJ, 2021, IEEE T INF FOREN SEC, V16, P190, DOI 10.1109/TIFS.2020.3009583
   Shen XJ, 2017, IET IMAGE PROCESS, V11, P44, DOI 10.1049/iet-ipr.2016.0238
   Sheng Lu, 2019, 2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM). Proceedings, P230, DOI 10.1109/AIAM48774.2019.00053
   Singh P, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3077140
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Taspinar S, 2020, IEEE T INF FOREN SEC, V15, P3270, DOI 10.1109/TIFS.2020.2985544
   Verdoliva L, 2014, IEEE INT WORKS INFOR, P149, DOI 10.1109/WIFS.2014.7084319
   Wang JW, 2017, MULTIMED TOOLS APPL, V76, P23721, DOI 10.1007/s11042-016-4153-0
   Wei Y, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3408299
   Wen LY, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183518
   Zeng H, 2016, J FORENSIC SCI, V61, P520, DOI 10.1111/1556-4029.13017
   Zhang C, 2016, INT CONF ACOUST SPEE, P5300, DOI 10.1109/ICASSP.2016.7472689
   Zhang WW, 2019, MULTIMED TOOLS APPL, V78, P20113, DOI 10.1007/s11042-019-7288-y
   Zhuang PY, 2021, IEEE T INF FOREN SEC, V16, P2986, DOI 10.1109/TIFS.2021.3070444
NR 45
TC 2
Z9 2
U1 6
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 67
DI 10.1145/3548689
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000017
DA 2024-07-18
ER

PT J
AU Cheng, YH
   Zhu, XG
   Qian, JC
   Wen, F
   Liu, PL
AF Cheng, Yuhao
   Zhu, Xiaoguang
   Qian, Jiuchao
   Wen, Fei
   Liu, Peilin
TI Cross-modal Graph Matching Network for Image-text Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image-text retrieval; relation reasoning; graph matching; cross-modal
   matching
ID LANGUAGE
AB Image-text retrieval is a fundamental cross-modal task whose main idea is to learn image-text matching. Generally, according to whether there exist interactions during the retrieval process, existing image-text retrieval methods can be classified into independent representation matching methods and cross-interaction matching methods. The independent representation matching methods generate the embeddings of images and sentences independently and thus are convenient for retrieval with hand-crafted matching measures (e.g., cosine or Euclidean distance). As to the cross-interaction matching methods, they achieve improvement by introducing the interaction-based networks for inter-relation reasoning, yet suffer the low retrieval efficiency. This article aims to develop a method that takes the advantages of cross-modal inter-relation reasoning of cross-interaction methods while being as efficient as the independent methods. To this end, we propose a graph-based Cross-modal Graph Matching Network (CGMN), which explores both intra- and inter-relations without introducing network interaction. In CGMN, graphs are used for both visual and textual representation to achieve intra-relation reasoning across regions and words, respectively. Furthermore, we propose a novel graph node matching loss to learn fine-grained cross-modal correspondence and to achieve inter-relation reasoning. Experiments on benchmark datasets MS-COCO, Flickr8K, and Flickr30K showthat CGMN outperforms state-of-the-art methods in image retrieval. Moreover, CGMM is much more efficient than state-of-the-art methods using interactive matching. The code is available at https://github.com/cyh-sj/CGMN.
C1 [Cheng, Yuhao; Zhu, Xiaoguang; Qian, Jiuchao; Wen, Fei; Liu, Peilin] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University
RP Wen, F (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
EM cyh958859352@sjtu.edu.cn; Zhuxiaoguang178@sjtu.edu.cn;
   jcqian@sjtu.edu.cn; wenfei@sjtu.edu.cn; liupeilin@sjtu.edu.cn
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   [Anonymous], 2015, P 3 INT C LEARN REPR
   Bai YS, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P384, DOI 10.1145/3289600.3290967
   Chen Liqun, 2020, INT C MACHINE LEARNI, P1542
   Chen TL, 2020, AAAI CONF ARTIF INTE, V34, P10583
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Diao Haiwen, 2021, ARXIV PREPRINT ARXIV
   Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Fey Matthias, 2020, P INT C LEARN REPR
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Hu Zhibin, 2019, P 28 INT JOINT C ART
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Z, 2019, IEEE I CONF COMP VIS, P5753, DOI 10.1109/ICCV.2019.00585
   Jingna Mao, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348279
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Karpathy Andrej, 2014, ARXIV PREPRINT ARXIV
   Kipf T.N., 2017, P INT C LEARN REPR S
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee K.-H., 2018, P EUR C COMP VIS, P201
   Lee K, 2019, PR MACH LEARN RES, V97
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li X., 2020, P EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58577-8_8
   Li YZ, 2019, PR MACH LEARN RES, V97
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Chunxiao, 2020, P IEEE C COMP VIS PA
   Liu FY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3396520
   Liu XX, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409388
   Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205
   Lu JS, 2019, ADV NEUR IN, V32
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208
   Peng L, 2022, IEEE T PATTERN ANAL, V44, P318, DOI 10.1109/TPAMI.2020.3004830
   Plummer BA, 2018, LECT NOTES COMPUT SC, V11216, P258, DOI 10.1007/978-3-030-01258-8_16
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2017, ARXIV171010903, DOI DOI 10.48550/ARXIV.1710.10903
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang X, 2019, PROC CVPR IEEE, P3622, DOI 10.1109/CVPR.2019.00679
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wehrmann P, 2020, AAAI CONF ARTIF INTE, V34, P12313
   Wen KY, 2021, IEEE T CIRC SYST VID, V31, P2866, DOI 10.1109/TCSVT.2020.3030656
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Xu K, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3156
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yu Wei, 2020, INT C LEARN REPR
   Yuan J, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3394955
   Zanfir A, 2018, PROC CVPR IEEE, P2684, DOI 10.1109/CVPR.2018.00284
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
NR 59
TC 33
Z9 33
U1 10
U2 60
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 95
DI 10.1145/3499027
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600006
DA 2024-07-18
ER

PT J
AU Lin, K
   Jia, CM
   Zhang, XF
   Wang, SS
   Ma, SW
   Gao, W
AF Lin, Kai
   Jia, Chuanmin
   Zhang, Xinfeng
   Wang, Shanshe
   Ma, Siwei
   Gao, Wen
TI NR-CNN: Nested-Residual Guided CNN In-loop Filtering for Video Coding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE In-loop filter; deep learning based video coding; rate and distortion
   optimization; video coding
ID INTRA-PREDICTION
AB Recently, deep learning for video coding, such as deep predictive coding, deep transform coding, and deep in-loop filtering, has been an emerging research area. The coding gain of hybrid coding framework could be extensively promoted by the data-driven models. However, previous deep coding tools especially deep in-loop filtering mainly consider the performance improvement while pay less attention to the reliability, usability, and adaptivity of the networks. In this article, a nested-residual guided convolutional neural network (NR-CNN) structure with cascaded global shortcut and configurable residual blocks is proposed for in-loop filtering. By taking advantage of the correlation between different color components, we further extend the NR-CNN by utilizing luminance as textural and structural guidance for chrominance filtering, which significantly improves the filtering performance. To fully exploit the proposed network into codec integration, we subsequently introduce an efficient and adaptive framework consisting of an adaptive granularity optimization and a parallel inference pipeline for deep learning based filtering. The former contributes to the coding performance improvement through an adaptive decision-making based on rate-distortion analysis at various granularities. The latter reduces the running time of network inference. The extensive experimental results show the superiority of the proposed method, achieving 8.2%, 14.9%, and 13.2% BD-rate savings on average under random access (RA) configuration. Meanwhile, the proposed method also obtains better subjective quality.
C1 [Lin, Kai; Jia, Chuanmin; Gao, Wen] Peking Univ, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
   [Zhang, Xinfeng] Univ Chinese Acad Sci, Beijing 100871, Peoples R China.
   [Wang, Shanshe; Ma, Siwei] Peking Univ, Informat Technol R&D Innovat Ctr, Peng Cheng Lab, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
C3 Peking University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Peking University
RP Jia, CM (corresponding author), Peking Univ, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
EM kailin@pku.edu.cn; cmjia@pku.edu.cn; xfzhang@ucas.ac.cn;
   sswang@pku.edu.cn; swma@pku.edu.cn; wgao@pku.edu.cn
RI Zhang, Xinfeng/X-8148-2019
OI Lin, Kai/0000-0003-0898-0771
FU National Natural Science Foundation of China [62101007, 62031013,
   62088102, 62071449]; National Postdoctoral Program for Innovative
   Talents [BX2021009]; High performance Computing Platform of Peking
   University
FX This work was supported in part by the National Natural Science
   Foundation of China (62101007, 62031013, 62088102, 62071449), National
   Postdoctoral Program for Innovative Talents (BX2021009), and also
   supported by High performance Computing Platform of Peking University,
   which are gratefully acknowledged.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Chen Jie, 2019, N2727 AVS
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Galpin Fu, 2016, JVETC0040
   Gao W, 2021, IEEE T CIRC SYST VID, V31, P4147, DOI 10.1109/TCSVT.2021.3104305
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   He XY, 2018, IEEE IMAGE PROC, P216, DOI 10.1109/ICIP.2018.8451086
   Hu YY, 2018, IEEE DATA COMPR CONF, P413, DOI 10.1109/DCC.2018.00066
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huo S, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351609
   Jia CM, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Jia W, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460820
   Jian Yunrui, 2020, AVSM5373
   Karczewicz M., 2016, P PICT COD S PCS, P1
   Kim S., 2020, JVET S2002 TEL JUL 2
   Kingma D. P., 2014, arXiv
   Kuanar S, 2021, SIGNAL PROCESS-IMAGE, V99, DOI 10.1016/j.image.2021.116409
   Kuanar S, 2019, CIRC SYST SIGNAL PR, V38, P5081, DOI 10.1007/s00034-019-01110-4
   Kuanar S, 2018, PICT COD SYMP, P164, DOI 10.1109/PCS.2018.8456278
   Kuo C W, 2020, AVSM5595
   Li D., 2019, IEEE INT SYMP CIRC S, P1
   Li JH, 2018, IEEE T IMAGE PROCESS, V27, P3236, DOI 10.1109/TIP.2018.2817044
   Li JR, 2020, IEEE DATA COMPR CONF, P203, DOI 10.1109/DCC47342.2020.00028
   Lin K., 2019, 2019 PICTURE CODING, P1
   Ma D., 2021, IEEE T MULTIMEDIA
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Ma SW, 2015, IEEE SIGNAL PROC MAG, V32, P172, DOI 10.1109/MSP.2014.2371951
   Misra K, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954547
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Pu Fangjun, 2018, JVETK0309
   Song XD, 2018, IEEE IMAGE PROC, P1133, DOI 10.1109/ICIP.2018.8451589
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tsai CY, 2013, IEEE J-STSP, V7, P934, DOI 10.1109/JSTSP.2013.2271974
   Wang M, 2020, IEEE T IMAGE PROCESS, V29, P2931, DOI 10.1109/TIP.2019.2955238
   Wang YB, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Wennersten P, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P688, DOI 10.1109/TCSVT.2003.815168
   Xia SL, 2020, PSYCHOL HEALTH MED, V25, P309, DOI 10.1080/13548506.2019.1643032
   Xu M, 2018, IEEE T IMAGE PROCESS, V27, P5044, DOI 10.1109/TIP.2018.2847035
   Zhang J, 2015, IEEE INT SYM MULTIM, P301, DOI 10.1109/ISM.2015.90
   Zhang J, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954503
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P3983, DOI 10.1109/TIP.2018.2830640
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang XF, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P437, DOI 10.1109/PCS.2012.6213380
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3827, DOI 10.1109/TIP.2018.2815841
   Zhao L, 2019, IEEE T IMAGE PROCESS, V28, P4832, DOI 10.1109/TIP.2019.2913545
   Zhao ZH, 2019, IEEE T CIRC SYST VID, V29, P3291, DOI 10.1109/TCSVT.2018.2876399
   Zhu H., 2020, P 2020 IEEE INT C MU, P1
NR 53
TC 5
Z9 5
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 102
DI 10.1145/3502723
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600013
DA 2024-07-18
ER

PT J
AU Xu, YF
   Sheng, KK
   Dong, WM
   Wu, BY
   Xu, CS
   Hu, BG
AF Xu, Yifan
   Sheng, Kekai
   Dong, Weiming
   Wu, Baoyuan
   Xu, Changsheng
   Hu, Bao-Gang
TI Towards Corruption-Agnostic Robust Domain Adaptation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Domain adaptation; corruption robustness; transfer learning
AB Great progress has been achieved in domain adaptation in decades. Existing works are always based on an ideal assumption that testing target domains are independent and identically distributed with training target domains. However, due to unpredictable corruptions (e.g., noise and blur) in real data, such as web images and real-world object detection, domain adaptation methods are increasingly required to be corruption robust on target domains. We investigate a new task, corruption-agnostic robust domain adaptation (CRDA), to be accurate on original data and robust against unavailable-for-training corruptions on target domains. This task is non-trivial due to the large domain discrepancy and unsupervised target domains. We observe that simple combinations of popular methods of domain adaptation and corruption robustness have suboptimal CRDA results. We propose a newapproach based on two technical insights into CRDA, as follows: (1) an easy-to-plug module called domain discrepancy generator (DDG) that generates samples that enlarge domain discrepancy to mimic unpredictable corruptions; (2) a simple but effective teacher-student scheme with contrastive loss to enhance the constraints on target domains. Experiments verify that DDG maintains or even improves its performance on original data and achieves better corruption robustness than baselines. Our code is available at: https://github.com/YifanXu74/CRDA.
C1 [Xu, Yifan; Dong, Weiming; Xu, Changsheng; Hu, Bao-Gang] Chinese Acad Sci, NLPR, Inst Automat, 95 East Zhongguancun Rd, Beijing 100190, Peoples R China.
   [Xu, Yifan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, 95 East Zhongguancun Rd, Beijing 100190, Peoples R China.
   [Sheng, Kekai] Tencent Inc, Youtu Lab, 397 Tianlin Rd, Shanghai 201103, Peoples R China.
   [Dong, Weiming] CASIA LLvis Joint Lab, 95 East Zhongguancun Rd, Beijing 100190, Peoples R China.
   [Wu, Baoyuan] Chinese Univ Hong Kong, Shenzhen Res Inst Big Data, 2001 Longxiang Rd, Shenzhen 518172, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Tencent; Shenzhen Research Institute of Big Data; The Chinese University
   of Hong Kong, Shenzhen
RP Xu, YF (corresponding author), Chinese Acad Sci, NLPR, Inst Automat, 95 East Zhongguancun Rd, Beijing 100190, Peoples R China.; Xu, YF (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, 95 East Zhongguancun Rd, Beijing 100190, Peoples R China.
EM yifan.xu@nlpr.ia.ac.cn; saulsheng@tencent.com; weiming.dong@ia.ac.cn;
   wubaoyuan1987@gmail.com; csxu@nlpr.ia.ac.cn; hubg@nlpr.ia.ac.cn
RI Wu, Baoyuan/C-8429-2013; xu, cj/HJZ-3488-2023; xu, yifan/GWZ-7154-2022;
   DONG, Weiming/AAG-7678-2020; xu, ye/JYO-6282-2024
OI DONG, Weiming/0000-0001-6502-145X; xu, ye/0009-0007-9798-2723; Xu,
   Yifan/0000-0003-2467-888X; xu, chang sheng/0000-0001-8343-9665; Hu,
   Bao-Gang/0000-0002-6916-5394
FU National Natural Science Foundation of China [U20B2070, 61832016,
   62036012, 61720106006]; CASIA-Tencent Youtu joint research project
FX This work was supported by National Natural Science Foundation of China
   under nos. U20B2070, 61832016, 62036012 and 61720106006, and by
   CASIA-Tencent Youtu joint research project.
CR Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Chen C, 2019, AAAI CONF ARTIF INTE, P3296
   Chen TL, 2020, PROC CVPR IEEE, P696, DOI 10.1109/CVPR42600.2020.00078
   Chen T, 2020, PR MACH LEARN RES, V119
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   DanHendrycks Mantas Mazeika, 2019, P ADV NEURAL INFORM
   DanHendrycks Norman Mu, 2020, P INT C LEARN REPR
   Dodge S, 2017, 2017 26TH INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATION AND NETWORKS (ICCCN 2017)
   Dodge Samuel, 2017, ARXIV170308119
   Ganin Y, 2016, J MACH LEARN RES, V17
   Geirhos Robert, 2018, Advances in Neural Information Processing Systems, P7549
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Han ZY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2269
   Hendrycks D, 2021, PROC CVPR IEEE, P15257, DOI 10.1109/CVPR46437.2021.01501
   Hendrycks Dan, 2021, C COMP VIS PATT REC
   Hendrycks Dan, 2019, ARXIV190312261
   Hosseini H, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P101, DOI 10.1109/ICMLA.2017.0-172
   Jordan M. I., 2019, P ADV NEUR INF PROC, P1
   Kang Daniel, 2019, ARXIV190808016
   Li R., 2020, P IEEE CVF C COMP VI, P9641, DOI DOI 10.1109/CVPR42600.2020.00966
   Li S, 2020, AAAI CONF ARTIF INTE, V34, P11386
   Liang J., 2020, INT C MACH LEARN, P6028, DOI DOI 10.48550/ARXIV.2002.08546
   Liu P, 2021, COMPUT VIS MEDIA, V7, P217, DOI 10.1007/s41095-021-0202-3
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lopes R.G., 2019, ARXIV190602611
   Matsuura T, 2020, AAAI CONF ARTIF INTE, V34, P11749
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2017, PR MACH LEARN RES, V70
   Sheng Kekai, 2021, ARXIV210313561
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Shu R., 2018, P 6 INT C LEARN REPR
   Sun Y., 2020, P 37 INT C MACHINE L, P9229
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vasiljevic Igor, 2016, arXiv
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Vladu Adrian, 2018, PROC 6 INT C LEARN R
   Volpi Riccardo, 2018, NEURIPS
   Wei DM, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278040
   Xu YF, 2022, COMPUT VIS MEDIA, V8, P33, DOI 10.1007/s41095-021-0247-3
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zeyi Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P124, DOI 10.1007/978-3-030-58536-5_8
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400
   Zhang YB, 2019, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR.2019.00517
NR 48
TC 4
Z9 4
U1 4
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 99
DI 10.1145/3501800
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, JW
   Su, QC
   Li, WH
   Liu, ZR
   Zhang, T
   Liu, S
   Zhong, P
   Jiang, WC
   Wang, JX
AF Huang, Jiawei
   Su, Qichen
   Li, Weihe
   Liu, Zhuoran
   Zhang, Tao
   Liu, Sen
   Zhong, Ping
   Jiang, Wanchun
   Wang, Jianxin
TI Opportunistic Transmission for Video Streaming over Wild Internet
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE DASH; video streaming; ABR
AB The video streaming system employs adaptive bitrate (ABR) algorithms to optimize a user's quality of experience. However, it is hard for ABR algorithms to choose the right bitrate consistently under highly dynamic bandwidth fluctuations in wild Internet. In this article, we propose a building block on the client side named Opportunistic Chunk Replacement Mechanism (OCRM) to help existing ABR algorithms make full use of the available bandwidth to improve the network utilization and viewing experience of users. Specifically, the servers take advantages of the spare bandwidth to opportunistically transmit high-quality chunks (called opportunistic chunks) with low priority to the client, without incurring any extra delay. Then, the client player replaces the low-quality chunks with the opportunistic ones that have high quality. We compare OCRM with state-of-the-art ABR algorithms by using trace-driven experiments spanning a wide variety of quality of experience metrics and network conditions. The test results show that OCRM effectively achieves high network utilization and improves the user's viewing experience by up to 35%.
C1 [Huang, Jiawei; Su, Qichen; Li, Weihe; Liu, Zhuoran; Liu, Sen; Zhong, Ping; Jiang, Wanchun; Wang, Jianxin] Cent South Univ, Changsha, Peoples R China.
   [Zhang, Tao] Changsha Univ, Changsha, Peoples R China.
C3 Central South University; Changsha University
RP Huang, JW (corresponding author), Cent South Univ, Changsha, Peoples R China.
EM jiaweihuang@csu.edu.cn; csu_qichensu@csu.edu.cn; weiheli@csu.edu.cn;
   zoranliu@csu.edu.cn; tzhang@ccsu.edu.cn; sen.liu@csu.edu.cn;
   ping.zhong@csu.edu.cn; jiangwc@csu.edu.cn; jxwang@csu.edu.cn
RI Liu, Zhuoran/IUO-1250-2023; Wang, Jianxin/V-2800-2018; 钟,
   萍/JFA-6264-2023
OI 钟, 萍/0000-0003-3393-8874
FU National Natural Science Foundation of China [62132022, 61872387,
   62002066, 61972421]; Key Research and Development Program of Hunan
   [2022WK2005]; Natural Science Foundation of Hunan Province, China
   [2021JJ30867]; China Postdoctoral Science Foundation [2021M690705]
FX This work was supported by the National Natural Science Foundation of
   China (62132022, 61872387, 62002066, 61972421); Key Research and
   Development Program of Hunan (2022WK2005); Natural Science Foundation of
   Hunan Province, China (2021JJ30867); and China Postdoctoral Science
   Foundation (2021M690705).
CR Akamai, DASH JS
   Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   [Anonymous], 2019, CISCO VISUAL NETWORK
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   Federal Communications Commission, Measuring Broadband America
   Halvorsen P., HSDPA DATASET
   Han B, 2016, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'16), P129, DOI 10.1145/2999572.2999606
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huang TC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P429, DOI 10.1145/3343031.3351014
   ISO/IEC MPEG, 2012, INF TECHN DYN AD S 1
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Li Weihe, 2020, P 2020 IEEE GLOBECOM, P1
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Lim M, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P321, DOI 10.1145/3339825.3397043
   Liu CH, 2012, SIGNAL PROCESS-IMAGE, V27, P288, DOI 10.1016/j.image.2011.10.001
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Qin YY, 2018, CONEXT'18: PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P366, DOI 10.1145/3281411.3281439
   Sani Y, 2017, IEEE COMMUN SURV TUT, V19, P2985, DOI 10.1109/COMST.2017.2725241
   Spiteri K, 2020, IEEE ACM T NETWORK, V28, P1698, DOI 10.1109/TNET.2020.2996964
   Spiteri K, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3336497
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   van der Hooft J., 4G LTE BANDWIDTH LOG
   Wang B, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1122, DOI 10.1145/3123266.3123284
   Wu JY, 2016, IEEE T MOBILE COMPUT, V15, P2345, DOI 10.1109/TMC.2015.2497238
   Yan FY, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P495
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
NR 26
TC 3
Z9 3
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 140
DI 10.1145/3488722
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800010
DA 2024-07-18
ER

PT J
AU Galteri, L
   Seidenari, L
   Bongini, P
   Bertini, M
   Del Bimbo, A
AF Galteri, Leonardo
   Seidenari, Lorenzo
   Bongini, Pietro
   Bertini, Marco
   Del Bimbo, Alberto
TI LANBIQUE: LANguage-based Blind Image QUality Evaluation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image quality enhancement; image captioning; image quality evaluation;
   GAN; generative models evaluation
AB Image quality assessment is often performed with deep networks that are fine-tuned to regress a human provided quality score of a given image. Usually, this approach may lack generalization capabilities and, while being highly precise on similar image distribution, it may yield lower correlation on unseen distortions. In particular, they show poor performances, whereas images corrupted by noise, blur, or compression have been restored by generative models. As a matter of fact, evaluation of these generative models is often performed providing anecdotal results to the reader. In the case of image enhancement and restoration, reference images are usually available. Nevertheless, using signal based metrics often leads to counterintuitive results: Highly natural crisp images may obtain worse scores than blurry ones. However, blind reference image assessment may rank images reconstructed with GANs higher than the original undistorted images. To avoid time-consuming human-based image assessment, semantic computer vision tasks may be exploited instead.
   In this article, we advocate the use of language generation tasks to evaluate the quality of restored images. We refer to our assessment approach as LANguage-based Blind Image QUality Evaluation (LANBIQUE). We show experimentally that image captioning, used as a downstream task, may serve as a method to score image quality, independently of the distortion process that affects the data. Captioning scores are better aligned with human rankings with respect to classic signal based or No-reference image quality metrics. We show insights on how the corruption, by artefacts, of local image structure may steer image captions in the wrong direction.
C1 [Galteri, Leonardo; Seidenari, Lorenzo; Bongini, Pietro; Bertini, Marco; Del Bimbo, Alberto] MICC Univ Firenze, Florence, Italy.
RP Galteri, L (corresponding author), MICC Univ Firenze, Florence, Italy.
EM leonardo.galteri@unifi.it; lorenzo.seidenari@unifi.it;
   p.bongini@unifi.it; marco.bertini@unifi.it; alberto.delbimbo@unifi.it
RI Seidenari, Lorenzo/AAA-1848-2020; Galteri, Leonardo/HSI-0092-2023
OI Seidenari, Lorenzo/0000-0003-4816-0268; Galteri,
   Leonardo/0000-0002-7247-9407; DEL BIMBO, ALBERTO/0000-0002-1052-8322;
   Bertini, Marco/0000-0002-1364-218X
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Banerjee S., 2005, P WORKSHOP INTRINSIC, P65
   Blau Y, 2019, PR MACH LEARN RES, V97
   Bongini P, 2019, LECT NOTES COMPUT SC, V11752, P214, DOI 10.1007/978-3-030-30645-8_20
   Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chang HB, 2014, IEEE T SIGNAL PROCES, V62, P718, DOI 10.1109/TSP.2013.2290508
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Galteri L, 2017, IEEE I CONF COMP VIS, P4836, DOI 10.1109/ICCV.2017.517
   Galteri Leonardo, 2021, ACM MULTIMEDIA ASIA, P1
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   ITU, 2012, REC ITUR BT 500 13 M
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim DW, 2019, IEEE COMPUT SOC CONF, P2086, DOI 10.1109/CVPRW.2019.00261
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Ko H, 2020, IEEE T IMAGE PROCESS, V29, P5964, DOI 10.1109/TIP.2020.2987180
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Lukin VV, 2015, PROC SPIE, V9394, DOI 10.1117/12.2085465
   Mameli F, 2021, INT C PATT RECOG, P9326, DOI 10.1109/ICPR48806.2021.9413095
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Odena A., 2016, DISTILL, V1, pe3, DOI 10.23915/distill.00003.-URL
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Pei SC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440172
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Salimans T, 2016, ADV NEUR IN, V29
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shmelkov K, 2018, LECT NOTES COMPUT SC, V11206, P218, DOI 10.1007/978-3-030-01216-8_14
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tomosada H, 2021, INT C PATT RECOG, P3675, DOI 10.1109/ICPR48806.2021.9412584
   Tran Linh Duy, 2020, P ACCV
   Vaccaro F, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1221, DOI 10.1145/3474085.3475683
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang Z, 2002, INT CONF ACOUST SPEE, P3313
   Winkler S, 2009, INT WORK QUAL MULTIM, P139, DOI 10.1109/QOMEX.2009.5246961
   Yoo J, 2018, PROC CVPR IEEE, P6684, DOI 10.1109/CVPR.2018.00699
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 54
TC 0
Z9 0
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 116
DI 10.1145/3538649
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000003
OA Green Published
DA 2024-07-18
ER

PT J
AU Han, N
   Chen, JJ
   Zhang, H
   Wang, HW
   Chen, H
AF Han, Ning
   Chen, Jingjing
   Zhang, Hao
   Wang, Huanwen
   Chen, Hao
TI Adversarial Multi-Grained Embedding Network for Cross-Modal Text-Video
   Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-grained fusion; spatial-temporal object relationships; text-video
   retrieval
AB Cross-modal retrieval between texts and videos has received consistent research interest in the multimedia community. Existing studies follow a trend of learning a joint embedding space to measure the distance between text and video representations. In common practice, video representation is constructed by feeding clips into 3D convolutional neural networks for a coarse-grained global visual feature extraction. In addition, several studies have attempted to align Lite local objects of video with the text. However, these representations share a drawback of neglecting rich fine-grained relation features capturing spatial-temporal object interactions that benefits mapping textual entities in the real-world retrieval system. To tackle this problem, we propose an adversarial multi-grained embedding network (AME-Net), a novel cross-modal retrieval framework that adopts both fine-grained local relation and coarse-grained global features in bridging text-video modalities. Additionally, with the newly proposed visual representation, we also integrate an adversarial learning strategy into AME-Net, to further narrow the domain gap between text and video representations. In summary, we contribute AME-Net with an adversarial learning strategy for learning a better joint embedding space, and experimental results on MSR-VTT and YouCook2 datasets demonstrate that our proposed framework consistently outperforms the state-of-the-art method.
C1 [Han, Ning; Wang, Huanwen; Chen, Hao] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Chen, Jingjing] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Zhang, Hao] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Hunan University; Fudan University; City University of Hong Kong
RP Chen, JJ (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM ninghan@hnu.edu.cn; chenjingjing@fudan.edu.cn; zhanghaoinf@gmail.com;
   huanwenwang@hnu.edu.cn; chenhao@hnu.edu.cn
RI chen, JJ/HGB-6029-2022
OI han, ning/0000-0002-0654-6026
FU National Key R&D Program of China [2018YFB1402600]; National Natural
   Science Foundation of China [62072116, 61772190]; Shanghai Pujiang
   Program [20PJ1401900]
FX This work was supported by the National Key R&D Program of China (No.
   2018YFB1402600), the National Natural Science Foundation of China (Nos.
   62072116 and 61772190), and the Shanghai Pujiang Program (No.
   20PJ1401900).
CR [Anonymous], 2017, ARXIV170704555
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen JJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1020, DOI 10.1145/3240508.3240627
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Devlin J., 2018, BERT PRE TRAINING DE
   Diba A, 2019, IEEE I CONF COMP VIS, P6191, DOI 10.1109/ICCV.2019.00629
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Faghri Fartash, 2017, ARXIV170705612
   Feng Fangxiang, 2015, ACM T MULTIM COMPUT, V12, P22, DOI DOI 10.1145/2808205
   Feng ZR, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1005
   Gabeur Valentin, 2020, P EUR C COMP VIS ECC, V5
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ghosh P, 2020, IEEE WINT CONF APPL, P565, DOI 10.1109/WACV45572.2020.9093361
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kipf Thomas N., 2017, 5 INT C LEARN REPRES
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu Y., 2019, ROBERTA ROBUSTLY OPT
   Mavroudi Effrosyni, 2019, ARXIV PREPRINT ARXIV
   Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Miech Antoine, 2018, ARXIV180402516
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Perronnin F, 2007, PROC CVPR IEEE, P2272
   Qian XF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P84, DOI 10.1145/3343031.3351058
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Shang XD, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1300, DOI 10.1145/3123266.3123380
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10635, DOI 10.1109/CVPR42600.2020.01065
   Simonyan K, 2014, ADV NEUR IN, V27
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vaswani A, 2017, ADV NEUR IN, V30
   Velikovic P., 2017, Graph attention networks, P1
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang H, 2019, PROC CVPR IEEE, P11564, DOI 10.1109/CVPR.2019.01184
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wray M, 2019, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2019.00054
   Wu JC, 2019, PROC CVPR IEEE, P9956, DOI 10.1109/CVPR.2019.01020
   Xiong Y, 2019, IEEE I CONF COMP VIS, P4591, DOI 10.1109/ICCV.2019.00469
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xun Yang, 2020, SIGIR '20: Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, P1339, DOI 10.1145/3397271.3401151
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Yutian Guo, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P9, DOI 10.1145/3372278.3390709
   Zhou Luowei, 2018, BMVC, P50
   Zhu B, 2019, PROC CVPR IEEE, P11469, DOI 10.1109/CVPR.2019.01174
NR 71
TC 8
Z9 8
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 63
DI 10.1145/3483381
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400015
DA 2024-07-18
ER

PT J
AU Mugnai, D
   Pernici, F
   Turchini, F
   Del Bimbo, A
AF Mugnai, Daniele
   Pernici, Federico
   Turchini, Francesco
   Del Bimbo, Alberto
TI Fine-Grained Adversarial Semi-Supervised Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fine-grained visual categorization; deep neural networks;
   semi-supervised learning; adversarial learning
ID CNNS
AB In this article, we exploit Semi-Supervised Learning (SSL) to increase the amount of training data to improve the performance of Fine-Grained Visual Categorization (FGVC). This problem has not been investigated in the past in spite of prohibitive annotation costs that FGVC requires. Our approach leverages unlabeled data with an adversarial optimization strategy in which the internal features representation is obtained with a second-order pooling model. This combination allows one to back-propagate the information of the parts, represented by second-order pooling, onto unlabeled data in an adversarial training setting. We demonstrate the effectiveness of the combined use by conducting experiments on six state-of-the-art fine-grained datasets, which include Aircrafts, Stanford Cars, CUB-200-2011, Oxford Flowers, Stanford Dogs, and the recent Semi-Supervised iNaturalist-Aves. Experimental results clearly show that our proposed method has better performance than the only previous approach that examined this problem; it also obtained higher classification accuracy with respect to the supervised learning methods with which we compared.
C1 [Mugnai, Daniele; Pernici, Federico; Turchini, Francesco; Del Bimbo, Alberto] Univ Florence, Via S Marta 3, I-50139 Florence, Italy.
C3 University of Florence
RP Mugnai, D (corresponding author), Univ Florence, Via S Marta 3, I-50139 Florence, Italy.
EM daniele.mugnai@unifi.it; federico.pernici@unifi.it;
   francesco.turchini@unifi.it; alberto.delbinibo@unifi.it
OI Pernici, Federico/0000-0001-7036-6655
FU European Commission under the European Horizon 2020 Program [951911 -
   AI4Media]
FX This work was partially supported by the European Commission under the
   European Horizon 2020 Program under Grant 951911 - AI4Media
CR Anderson Connor, 2020, ARXIV200613190
   Athiwaratkun Ben, 2018, ICLR
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259
   Berthelot D, 2019, ADV NEUR IN, V32
   Cascante-Bonilla P, 2021, AAAI CONF ARTIF INTE, V35, P6912
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chen Ting, 2020, ADV NEURAL INFORM PR
   Chen Wei, 2021, ARXIV210111282
   Chen Wei, 2020, 31 BRIT MACH VIS C 2
   Cui Cheng, 2020, ARXIV200610702
   Cui Q., 2019, DEEP LEARNING FINE G
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Daume H., 2010, P ACL
   Daume III Hal, 2010, P 2010 WORKSH DOM AD
   Dean J., 2015, NIPS DEEP LEARNING R
   Donahue J, 2013, PROC CVPR IEEE, P668, DOI 10.1109/CVPR.2013.92
   Dosovitskiy Alexey, 2020, ARXIV201011929
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Grandvalet Y., 2005, CAP, V367, P281
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   He ZW, 2019, IEEE I CONF COMP VIS, P6667, DOI 10.1109/ICCV.2019.00677
   Higham NJ, 2008, OTHER TITL APPL MATH, V104, P1, DOI 10.1137/1.9780898717778
   Hu Tao, 2019, See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification
   Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339
   Javanmardi M, 2018, I S BIOMED IMAGING, P554, DOI 10.1109/ISBI.2018.8363637
   Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81
   Kato H, 2019, PROC CVPR IEEE, P9770, DOI 10.1109/CVPR.2019.01001
   Khosla A., 2011, P CVPR WORKSH FIN GR
   Korsch Dimitri, 2020, ARXIV200702080
   Korsch Dimitri, 2019, GERM C PATT REC
   Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2009, Tech. Rep.
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105
   Li Y, 2019, IEEE T PATTERN ANAL, V41, P639, DOI 10.1109/TPAMI.2018.2810288
   Li YF, 2019, FRONT COMPUT SCI-CHI, V13, P669, DOI 10.1007/s11704-019-8452-2
   Lin T. -Y., 2017, P BRIT MACH VIS C BM
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu B, 2019, IEEE INT CONF COMP V, P1317, DOI 10.1109/ICCVW.2019.00167
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu WY, 2016, PR MACH LEARN RES, V48
   Liu Y, 2020, ARXIV PREPRINT ARXIV
   Masana Marc, 2020, ARXIV201015277
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Mugnai Daniele, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12664), P102, DOI 10.1007/978-3-030-68799-1_8
   Nartey OT, 2020, IEEE ACCESS, V8, P2109, DOI 10.1109/ACCESS.2019.2962258
   Netzer Y., 2011, READING DIGITS NATUR
   Ngiam J, 2018, Arxiv, DOI arXiv:1811.07056
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Oliver A, 2018, ADV NEUR IN, V31
   Ouali Yassine, 2020, ARXIV200605278
   Pernici Federico, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P477, DOI 10.1109/ICMEW.2017.8026276
   Pernici F., 2019, CVPR WORKSH, P46
   Pernici F, 2022, IEEE T NEUR NET LEAR, V33, P4373, DOI 10.1109/TNNLS.2021.3056762
   Pernici F, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.102983
   Pernici F, 2018, PROC CVPR IEEE, P2324, DOI 10.1109/CVPR.2018.00247
   Pernici Federico, 2020, 25 INT C PATTERN REC
   Pu N., 2021, P IEEE CVF C COMP VI, P7901
   Real E, 2019, AAAI CONF ARTIF INTE, P4780
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814
   Shinohara Y, 2016, INTERSPEECH, P2369, DOI 10.21437/Interspeech.2016-879
   Simon M, 2017, IEEE I CONF COMP VIS, P4970, DOI 10.1109/ICCV.2017.531
   Simon Marcel, 2018, IEEE T PATTERN ANAL
   Sohn Kihyuk, 2020, Advances in Neural Information Processing Systems, P596, DOI DOI 10.48550/ARXIV.2001.07685
   Srinivas Aravind, 2019, ABS190509272 CORR
   Su JC, 2021, PROC CVPR IEEE, P12961, DOI 10.1109/CVPR46437.2021.01277
   Su Jong-Chyi, 2021, SEMISUPERVISED INATU, V2, P6
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Tarvainen A, 2017, ADV NEUR IN, V30
   Touvron Hugo, 2019, ADV NEURAL INFORM PR
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedaldi A., 2013, Technical report
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang M, 2021, NEUROCOMPUTING, V429, P215, DOI 10.1016/j.neucom.2020.10.081
   Wang QL, 2021, IEEE T PATTERN ANAL, V43, P2582, DOI 10.1109/TPAMI.2020.2974833
   Wang YY, 2013, IEEE T NEUR NET LEAR, V24, P1763, DOI 10.1109/TNNLS.2013.2263512
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie Q., 2020, ADV NEURAL INFORM PR, V33, P6256, DOI DOI 10.48550/ARXIV.1904.12848
   Yalniz Ismet Zeki, 2019, Billion-scale semi-supervised learning for image classification
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yun Sangdoo, 2008, 2019 IEEECVF INT C C, P6022
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang Jian, 2019, ARXIV190209941
   Zhang LB, 2019, IEEE I CONF COMP VIS, P8330, DOI 10.1109/ICCV.2019.00842
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zheng H., 2019, Advances in Neural Information Processing Systems
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhu X., 2003, P 20 INT C MACH LEAR, V3, P58, DOI DOI 10.1109/18.850663
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 97
TC 4
Z9 4
U1 1
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 34
DI 10.1145/3485473
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300011
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Wu, DC
   Hsu, YT
AF Wu, Da-Chun
   Hsu, Yu-Tsung
TI Authentication of LINE Chat History Files by Information Hiding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE LINE chat history; authentication; information hiding; tab code; half
   and full spaces
AB With the prevalence of smartphones, message exchanges via mobile chatting programs like LINE have become popular. The messages in the form of chat records in a LINE chat history, after being downloaded for legal uses, might be tampered with illicitly. A novel method for authenticating the chat history against such attacks is proposed. The signal used for authenticating each chat-record segment is created by concatenating the ID label of the segment and a digest yielded by hashing the segment content. The signal is then encoded by three types of spacing code, namely half space, full space, and tab, and embedded into the blank-space areas created by the tab codes in the chat records of the segment. Authentication of a history file is accomplished by extracting the authentication signals embedded in the file and comparing them with the original signals computed directly from the file. The embedded signals are invisible, arousing no suspicion from the hacker. The signals are fragile, because any modification of the records can be detected by the authentication process. Experiments for testing four types of tampering with text files of four languages have been conducted, yielding correct authentication results that show the feasibility of the proposed method.
C1 [Wu, Da-Chun; Hsu, Yu-Tsung] Natl Kaohsiung Univ Sci & Technol, Dept Comp & Commun Engn, 1 Univ Rd, Kaohsiung 824005, Taiwan.
C3 National Kaohsiung University of Science & Technology
RP Wu, DC (corresponding author), Natl Kaohsiung Univ Sci & Technol, Dept Comp & Commun Engn, 1 Univ Rd, Kaohsiung 824005, Taiwan.
EM dcwu@nkust.edu.tw; blue510144@gmail.com
FU National Science Council, Taiwan [NSC 101-3113-P-009-006, NSC
   102-2221-E-327-023]
FX This work was supported in part by the National Science Council, Taiwan,
   under grants NSC 101-3113-P-009-006 and NSC 102-2221-E-327-023.
CR [Anonymous], 2008, J INFORM TECHNOLOGY
   [Anonymous], 2021, INTR JSON
   [Anonymous], 2007, 2007 3 IEEE IFIP INT
   [Anonymous], 2009, P 2 IEEE INT C COMP, DOI DOI 10.1109/IC4.2009.4909228
   Artz D, 2001, IEEE INTERNET COMPUT, V5, P75, DOI 10.1109/4236.935180
   Bender W, 1996, IBM SYST J, V35, P313, DOI 10.1147/sj.353.0313
   Chotikakamthorn N., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P250, DOI 10.1109/ICIP.1999.822894
   Cox IJ, 1999, SIGNAL PROC SERIES, P461
   Craver S, 1998, IEEE J SEL AREA COMM, V16, P573, DOI 10.1109/49.668979
   Hsu Yu-Tsung, 2019, P 2019 CRYPT INF SEC
   Kothari L, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTER, COMMUNICATIONS AND ELECTRONICS (COMPTELIX), P448, DOI 10.1109/COMPTELIX.2017.8004011
   Lin CC, 2004, J SYST SOFTWARE, V73, P405, DOI 10.1016/S0164-1212(03)00239-5
   LINE Corporation, 2019, LINE FREE CALLS MESS
   LINE Corporation, 2019, RESP LAW ENF AG
   Liu TY, 2007, IEEE T INF FOREN SEC, V2, P24, DOI 10.1109/TIFS.2006.890310
   Nagarhalli Tatwadarshi P, 2014, P NAT C ROL ENG NAT, P103
   NIST, 2019, SHA 3 PROJ HASH FUNC
   Samphaiboon N, 2011, MULTIMED TOOLS APPL, V52, P569, DOI 10.1007/s11042-009-0432-3
   Sandcastle Documented Class Library, 2019, SHA3 SHAKE256 METH
   Schild Tomas, 2019, SEARCH CHARACTER S U
   Song L, 2017, LECT NOTES COMPUT SC, V10402, P428, DOI 10.1007/978-3-319-63715-0_15
   Unicode Inc, 2019, UN WORLD STAND TEXT
   Wikipedia, 2021, FACEBOOK MESSENGER
   Winstein K., 2019, LEXICAL STEGANOGRAPH
   Zhu RJ, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-00527-1
NR 25
TC 3
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 22
DI 10.1145/3474225
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900022
DA 2024-07-18
ER

PT J
AU Alahmadi, M
   Pocta, P
   Melvin, H
AF Alahmadi, Mohannad
   Pocta, Peter
   Melvin, Hugh
TI An Adaptive Bitrate Switching Algorithm for Speech Applications in
   Context of WebRTC
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE E-model; Opus codec; speech coding; WebRTC; QoE
ID FRAMEWORK; VOICE
AB Web Real-Time Communication (WebRTC) combines a set of standards and technologies to enable high-quality audio, video, and auxiliary data exchange in web browsers and mobile applications. It enables peer-to-peer multimedia sessions over IP networks without the need for additional plugins. The Opus codec, which is deployed as the default audio codec for speech and music streaming in WebRTC, supports a wide range of bitrates. This range of bitrates covers narrowband, wideband, and super-wideband up to fullband bandwidths. Users of IP-based telephony always demand high-quality audio. In addition to users' expectation, their emotional state, content type, and many other psychological factors; network quality of service; and distortions introduced at the end terminals could determine their quality of experience. To measure the quality experienced by the end user for voice transmission service, the E-model standardized in the ITU-T Rec. G.107 (a narrowband version), ITU-T Rec. G.107.1 (a wideband version), and the most recent ITU-T Rec. G.107.2 extension for the super-wideband E-model can be used. In this work, we present a quality of experience model built on the E-model to measure the impact of coding and packet loss to assess the quality perceived by the end user in WebRTC speech applications. Based on the computed Mean Opinion Score, a real-time adaptive codec parameter switching mechanism is used to switch to the most optimum codec bitrate under the present network conditions. We present the evaluation results to show the effectiveness of the proposed approach when compared with the default codec configuration in WebRTC.
C1 [Alahmadi, Mohannad; Melvin, Hugh] Natl Univ Ireland, Sch Comp Sci, Univ Rd, Galway, Ireland.
   [Pocta, Peter] Univ Zilina, Fac Elect Engn & Informat Technol, Dept Multimedia & Informat Commun Technol, Univ 8215-1, SK-01026 Zilina, Slovakia.
C3 Ollscoil na Gaillimhe-University of Galway; University of Zilina
RP Alahmadi, M (corresponding author), Natl Univ Ireland, Sch Comp Sci, Univ Rd, Galway, Ireland.
EM m.alahmadi1@nuigalway.ie; peter.pocta@feit.uniza.sk;
   hugh.melvin@nuigalway.ie
RI Pocta, Peter/A-6228-2010
OI Pocta, Peter/0000-0001-6791-1325
CR Aktas Ismet, 2012, Internet of Thing, Smart Spaces, and Next Generation Networking. 12th International Conference (NEW2AN 2012) and 5th Conference (ruSMART 2012). Proceedings, P347, DOI 10.1007/978-3-642-32686-8_32
   Al-Ahmadi M, 2019, 2019 30TH IRISH SIGNALS AND SYSTEMS CONFERENCE (ISSC)
   Alvestrand H., 2018, IDENTIFIERS WEBRTCS
   [Anonymous], 2019, G11322019 ITUT
   [Anonymous], 2015, P83412015 ITUT
   [Anonymous], 2009, P83312009 ITUT
   [Anonymous], 2007, G113 ITUT
   [Anonymous], 2019, P86312019 ITUT
   [Anonymous], 2004, ITU-T Rec. P, P563
   [Anonymous], 2019, SG12C3362019 ITUT
   [Anonymous], 2011, P8632011 ITUT
   [Anonymous], 1996, P8001996 ITUT
   [Anonymous], 2017, ITU T P 501 AMENDMEN
   [Anonymous], 2019, SG12C3352019 ITUT
   [Anonymous], 2009, G107 ITUTG
   [Anonymous], 2019, SG12C3342019 ITUT
   [Anonymous], 2017, SG12C222017 ITUT
   [Anonymous], 2001, P8622001 ITUT
   [Anonymous], 2001, P8332001 ITUT
   [Anonymous], 2019, G10712019 ITUT
   [Anonymous], 2007, P86232007 ITUT
   [Anonymous], 2015, P8342015 ITUT
   Ashara Amit, 2016, IMPLEMENTING OPUS VO
   Assem H, 2013, 2013 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2013), P1276
   Bergkvist A., 2018, WEBRTC 1 0 REAL TIME
   Bruhn S, 2015, INT CONF ACOUST SPEE, P5703, DOI 10.1109/ICASSP.2015.7179064
   Casetti C, 2000, IEEE ICC, P821, DOI 10.1109/ICC.2000.853613
   Fosser Eirik, 2016, THESIS NTNU
   de Carvalho LSG, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480753
   García B, 2019, COMPUTING, V101, P1585, DOI 10.1007/s00607-018-0669-7
   Halinger Gerhard, 2008, Proc. 14th GI/ITG Conf. Meas. Modelling Evaluation Comput. Commun. Syst, P1
   Han Y, 2015, IEEE INT SYM BROADB
   Handley M., 2006, 4566 RFC IETF
   Hemminger Stephen., 2005, LINUX C, P18
   Janssen J, 2002, IEEE INTERNET COMPUT, V6, P48, DOI 10.1109/MIC.2002.1003131
   Jokisch Oliver, 2016, ELEKTRONISCHE SPRACH, P254
   Kaskinen Tanu, 2019, PULSEAUDIO VOLUME CO
   Kim DS, 2007, BELL LABS TECH J, V12, P221, DOI 10.1002/bltj.20228
   Levent-Levi Tashi, 2019, ID WEBRTCS STAT API
   Maruschke M, 2015, LECT NOTES ARTIF INT, V9319, P348, DOI 10.1007/978-3-319-23132-7_43
   McGowan J. W, 2005, US Patent, Patent No. [6,931,017, 6931017]
   Meszaros M, 2018, LECT NOTES ARTIF INT, V11096, P408, DOI 10.1007/978-3-319-99579-3_43
   Mittag Gabriel, 2018, PROC INT C QUALITY M, P1
   Mkwawa IH, 2010, GLOB TELECOMM CONF
   Möller S, 2006, IEEE T AUDIO SPEECH, V14, P1969, DOI 10.1109/TASL.2006.883262
   Möller S, 2010, EURASIP J AUDIO SPEE, DOI 10.1155/2010/782731
   Nandakumar Suhas, 2018, ANNOTATED EXAMPLE SD
   Ng Leng, 2005, P 2005 2 AS PAC C MO
   Raake A., 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P182, DOI 10.1109/QOMEX.2010.5516264
   RABASSA A., 2010, P 13 INT S PERF EV C, P364
   Rämö A, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P2552
   Rosenberg Jonathan, 2002, document RFC 3261, DOI [10.174 87/RFC3261, DOI 10.17487/RFC3261]
   Salsano Stefano, 2012, TECH REP
   Shaw Mary, 1995, ACM SOFTWARE ENG NOT, V20, P27
   Spittka Julian, 2015, RFC7587 IETF
   Valin J.-M., 2012, Definition of the opus audio codec
   Valin Jean-Marc, 2010, CONSTRAINED ENERGY L
   Varela M, 2006, MEASUREMENT OF SPEECH AND AUDIO QUALITY IN NETWORKS, P45
   Vos K, 2010, SILK SPEECH CODEC DR
   Wältermann M, 2010, INT CONF ACOUST SPEE, P4654, DOI 10.1109/ICASSP.2010.5495199
   Zhou Juejia, 2010, P 2010 IEEE WIR COMM, P1
   1999, G108 ITUT
NR 62
TC 2
Z9 2
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 133
DI 10.1145/3458751
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800017
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Nguyen, PA
   Ngo, CW
AF Phuong-Anh Nguyen
   Chong-Wah Ngo
TI Interactive Search vs. Automatic Search: An Extensive Study on Video
   Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video retrieval; ad hoc video search; user study; interactive search;
   automatic search
ID EVENTS; FUSION; IMAGE
AB This article conducts user evaluation to study the performance difference between interactive and automatic search. Particularly, the study aims to provide empirical insights of how the performance landscape of video search changes, with tens of thousands of concept detectors freely available to exploit for query formulation. We compare three types of search modes: free-to-play (i.e., search from scratch), non-free-to-play (i.e., search by inspecting results provided by automatic search), and automatic search including concept-free and concept-based retrieval paradigms. The study involves a total of 40 participants; each performs interactive search over 15 queries of various difficulty levels using two search modes on the IACC.3 dataset provided by TRECVid organizers. The study suggests that the performance of automatic search is still far behind interactive search. Furthermore, providing users with the result of automatic search for exploration does not show obvious advantage over asking users to search from scratch. The study also analyzes user behavior to reveal insights of how users compose queries, browse results, and discover new query terms for search, which can serve as guideline for future research of both interactive and automatic search.
C1 [Phuong-Anh Nguyen; Chong-Wah Ngo] City Univ Hong Kong, Tat Chee Ave, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Nguyen, PA (corresponding author), City Univ Hong Kong, Tat Chee Ave, Hong Kong, Peoples R China.
EM panguyen2-c@my.cityu.edu.hk; cscwngo@cityu.edu.hk
RI Nguyen, Phuong Anh/AAQ-4427-2021
OI Nguyen, Phuong Anh/0000-0003-1289-3785
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 11250716]
FX The work described in this article was supported by a grant from the
   Research Grants Council of the Hong Kong Special Administrative Region,
   China (CityU 11250716).
CR [Anonymous], 2012, LREC
   [Anonymous], 2007, PROC INT C MULTIMEDI, DOI DOI 10.1145/1291233.1291447
   [Anonymous], 2009, PROC 1STWORKSHOPWEB
   [Anonymous], 2016, MULTIMEDIA MODELING
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Awad George, 2016, 2016 TREC VIDEO RETR
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Barthel KU, 2018, LECT NOTES COMPUT SC, V10705, P413, DOI 10.1007/978-3-319-73600-6_43
   Blasi Saverio G., 2018, 2018 TREC VIDEO RETR
   Blazek Adam, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P243, DOI 10.1007/978-3-319-14442-9_22
   Carpineto C, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2071389.2071390
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Cobârzan C, 2017, MULTIMED TOOLS APPL, V76, P5539, DOI 10.1007/s11042-016-3661-2
   de Boer M, 2016, MULTIMED TOOLS APPL, V75, P9025, DOI 10.1007/s11042-015-2757-4
   de Boer MHT, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3131288
   de Boer MHT, 2016, INT J MULTIMED INF R, V5, P203, DOI 10.1007/s13735-016-0112-9
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Ferrari, 2019, ABS19031083020 CORR
   Habibian A, 2017, IEEE T PATTERN ANAL, V39, P2089, DOI 10.1109/TPAMI.2016.2627563
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Hauptmann Alexander., 2007, CIVR 07, P627
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Heilbron Fabian Caba, 2015, C COMP VIS PATT REC
   Huet Benoit, 2017, 2017 TREC VIDEO RETR
   Huet Benoit, 2017, 2017 TREC VIDEO RETR
   Jiang L, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P27, DOI 10.1145/2671188.2749399
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kobayashi Tetsunori, 2017, 2017 TREC VIDEO RETR
   Kobayashi Tetsunori, 2016, 2016 TREC VIDEO RETR
   Koelma Dennis C., 2017, 2017 TREC VIDEO RETR
   Leibetseder A, 2018, LECT NOTES COMPUT SC, V10705, P425, DOI 10.1007/978-3-319-73600-6_45
   Li XR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1786, DOI 10.1145/3343031.3350906
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045
   Lokoc Jakub, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8326, P415, DOI 10.1007/978-3-319-04117-9_49
   Lokoc J, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3295663
   Lokoc J, 2018, IEEE T MULTIMEDIA, V20, P3361, DOI 10.1109/TMM.2018.2830110
   Lokoc J, 2018, LECT NOTES COMPUT SC, V10705, P419, DOI 10.1007/978-3-319-73600-6_44
   Lokoc J, 2017, LECT NOTES ARTIF INT, V10604, P754, DOI 10.1007/978-3-319-69179-4_53
   Lu YJ, 2017, LECT NOTES COMPUT SC, V10133, P463, DOI 10.1007/978-3-319-51814-5_42
   Lu YJ, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P127, DOI 10.1145/2911996.2912015
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   Miyao Shinichi, 2017, 2017 TREC VIDEO RETR
   Moumtzidou A, 2018, LECT NOTES COMPUT SC, V10705, P444, DOI 10.1007/978-3-319-73600-6_48
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   Ngo, 2017, 2017 TREC VIDEO RETR
   Ngo Chong-Wah, 2014, 2014 TREC VIDEO RETR
   Patras Ioannis, 2017, 2017 TREC VIDEO RETR
   Nguyen PA, 2019, LECT NOTES COMPUT SC, V11296, P609, DOI 10.1007/978-3-030-05716-9_54
   Nguyen PA, 2018, LECT NOTES COMPUT SC, V10705, P407, DOI 10.1007/978-3-319-73600-6_42
   Primus MJ, 2018, LECT NOTES COMPUT SC, V10705, P438, DOI 10.1007/978-3-319-73600-6_47
   Quenot Georges, 2019, 2019 TREC VIDEO RETR
   Rossetto L, 2019, LECT NOTES COMPUT SC, V11296, P616, DOI 10.1007/978-3-030-05716-9_55
   Rossetto L, 2018, LECT NOTES COMPUT SC, V10705, P403, DOI 10.1007/978-3-319-73600-6_41
   Rossetto L, 2017, LECT NOTES COMPUT SC, V10133, P469, DOI 10.1007/978-3-319-51814-5_43
   Rossi L, 2021, INFECTION, V49, P287, DOI 10.1007/s15010-020-01550-0
   Rujikietgumjorn S, 2018, LECT NOTES COMPUT SC, V10705, P431, DOI 10.1007/978-3-319-73600-6_46
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schoeffmann K, 2019, INT WORK CONTENT MUL
   Shah Mubarak, 2012, ABS12120402 201 CORR
   Snoek C.G. M., 2009, Concept-based video retrieval
   Snoek CGM, 2008, IEEE MULTIMEDIA, V15, P86, DOI 10.1109/MMUL.2008.21
   Truong TD, 2018, LECT NOTES COMPUT SC, V10705, P451, DOI 10.1007/978-3-319-73600-6_49
   Tzelepis C, 2016, IMAGE VISION COMPUT, V53, P35, DOI 10.1016/j.imavis.2015.09.005
   Wang Dong., 2007, the 15th ACM International Conference on Multimedia, P285
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yilmaz E., 2008, P 31 ANN INT ACM SIG
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zisserman, 2017, ABS17050695020 CORR
NR 72
TC 5
Z9 5
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 47
DI 10.1145/3429457
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000009
OA Green Published
DA 2024-07-18
ER

PT J
AU Tang, XCA
   Liu, MZ
   Zhong, H
   Ju, YZ
   Li, WL
   Xu, Q
AF Tang, Xiaochuan
   Liu, Mingzhe
   Zhong, Hao
   Ju, Yuanzhen
   Li, Weile
   Xu, Qiang
TI MILL: Channel Attention-based Deep Multiple Instance Learning for
   Landslide Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Landslide recognition; landslide classification; landslide detection;
   multiple instance learning; channel attention
ID CONVOLUTIONAL NEURAL-NETWORKS; INVENTORY; EARTHQUAKE
AB Landslide recognition is widely used in natural disaster risk management. Traditional landslide recognition is mainly conducted by geologists, which is accurate but inefficient. This article introduces multiple instance learning (MIL) to perform automatic landslide recognition. An end-to-end deep convolutional neural network is proposed, referred to as Multiple Instance Learning-based Landslide classification (MILL). First, MILL uses a large-scale remote sensing image classification dataset to build pre-train networks for landslide feature extraction. Second, MILL extracts instances and assign instance labels without pixel-level annotations. Third, MILL uses a new channel attention-based MIL pooling function to map instance-level labels to bag-level label. We apply MIL to detect landslides in a loess area. Experimental results demonstrate that MILL is effective in identifying landslides in remote sensing images.
C1 [Tang, Xiaochuan; Liu, Mingzhe; Zhong, Hao; Ju, Yuanzhen; Li, Weile; Xu, Qiang] Chengdu Univ Technol, State Key Lab Geohazrd Prevent & Geoenvironm Prot, Chengdu 610059, Peoples R China.
   [Tang, Xiaochuan] Univ Elect Sci & Technol China, Chengdu 611731, Peoples R China.
C3 Chengdu University of Technology; University of Electronic Science &
   Technology of China
RP Liu, MZ (corresponding author), Chengdu Univ Technol, State Key Lab Geohazrd Prevent & Geoenvironm Prot, Chengdu 610059, Peoples R China.
EM tangxiaochuan18@cdut.edu.cn; liumz@cdut.edu.cn; zhonghao@cdut.edu.cn;
   415384857@qq.com; liweile08@mail.cdut.edu.cn; xq@cdut.edu.cn
RI Liu, Mingzhe/KIL-5041-2024; Liu, Mingzhe/IAP-1680-2023; Xu,
   Qiang/AAE-3255-2022; Li, Weile/AAD-2385-2019
OI Liu, Mingzhe/0000-0001-7054-997X; Tang, Xiaochuan/0000-0003-0579-4797
FU National Natural Science Foundation of China [U19A2086]; Team Project of
   Independent Research of SKLGP [SKLGP2019Z014]; Comprehensive Remote
   Sensing Identification and Investigation of Geological Hazards in the
   Sichuan Meizoseismal Area, Ministry of Natural Resources of the People's
   Republic of China [0733-20180876/2]; Key Research and Development
   Project of Sichuan Province [2020YFG0169]
FX This work is supported by National Natural Science Foundation of China
   (Grant No. U19A2086), the Team Project of Independent Research of SKLGP
   (Grant No. SKLGP2019Z014), the Comprehensive Remote Sensing
   Identification and Investigation of Geological Hazards in the Sichuan
   Meizoseismal Area, Ministry of Natural Resources of the People's
   Republic of China (Grant No. 0733-20180876/2), and Key Research and
   Development Project of Sichuan Province (Grant No. 2020YFG0169).
CR Aksoy B, 2012, COMPUT GEOSCI-UK, V38, P87, DOI 10.1016/j.cageo.2011.05.010
   [Anonymous], 1958, LANDSLIDES ENG PRACT
   Cano A, 2017, KNOWL-BASED SYST, V136, P46, DOI 10.1016/j.knosys.2017.08.022
   Carbonneau MA, 2018, PATTERN RECOGN, V77, P329, DOI 10.1016/j.patcog.2017.10.009
   Chau KT, 2004, COMPUT GEOSCI-UK, V30, P429, DOI 10.1016/j.cageo.2003.08.013
   Dai KR, 2020, IEEE GEOSC REM SEN M, V8, P136, DOI 10.1109/MGRS.2019.2954395
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Bui DT, 2016, LANDSLIDES, V13, P361, DOI 10.1007/s10346-015-0557-6
   Galli M, 2008, GEOMORPHOLOGY, V94, P268, DOI 10.1016/j.geomorph.2006.09.023
   Gariano SL, 2016, EARTH-SCI REV, V162, P227, DOI 10.1016/j.earscirev.2016.08.011
   Ghorbanzadeh O, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11020196
   Gorelick N, 2017, REMOTE SENS ENVIRON, V202, P18, DOI 10.1016/j.rse.2017.06.031
   Guzzetti F, 1999, GEOMORPHOLOGY, V31, P181, DOI 10.1016/S0169-555X(99)00078-1
   Guzzetti F, 2006, GEOMORPHOLOGY, V81, P166, DOI 10.1016/j.geomorph.2006.04.007
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ilse M, 2018, PR MACH LEARN RES, V80
   Jaboyedoff M, 2012, NAT HAZARDS, V61, P5, DOI 10.1007/s11069-010-9634-2
   Ji SP, 2020, LANDSLIDES, V17, P1337, DOI 10.1007/s10346-020-01353-2
   Kanezaki A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1543, DOI 10.1109/ICASSP.2018.8462533
   Kingma D. P., 2014, arXiv
   Li ZL, 2020, IEEE T GEOSCI REMOTE, V58, P3685, DOI 10.1109/TGRS.2019.2960889
   Li ZB, 2016, REMOTE SENS ENVIRON, V187, P76, DOI 10.1016/j.rse.2016.10.008
   Liu MX, 2018, MED IMAGE ANAL, V43, P157, DOI 10.1016/j.media.2017.10.005
   Liu MZ, 2020, NEUROCOMPUTING, V391, P199, DOI 10.1016/j.neucom.2018.12.081
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Martha TR, 2010, GEOMORPHOLOGY, V116, P24, DOI 10.1016/j.geomorph.2009.10.004
   Moosavi V, 2014, GEOMORPHOLOGY, V204, P646, DOI 10.1016/j.geomorph.2013.09.012
   Nichol J, 2005, LAND DEGRAD DEV, V16, P243, DOI 10.1002/ldr.648
   Pesci A, 2011, ISPRS J PHOTOGRAMM, V66, P327, DOI 10.1016/j.isprsjprs.2010.12.002
   Pourghasemi HR, 2016, ENVIRON EARTH SCI, V75, DOI 10.1007/s12665-015-4950-1
   Qiao MY, 2017, PATTERN RECOGN, V64, P407, DOI 10.1016/j.patcog.2016.08.026
   Sato HP, 2007, LANDSLIDES, V4, P113, DOI 10.1007/s10346-006-0069-5
   Shi Wenzhong, 2020, IEEE T GEOSCI ELECT
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang RL, 2018, PATTERN RECOGN LETT, V109, P120, DOI 10.1016/j.patrec.2018.01.013
   Wang SJ, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101549
   Wei XS, 2017, IEEE T NEUR NET LEAR, V28, P975, DOI 10.1109/TNNLS.2016.2519102
   Wieczorek GF., 1984, B ASS ENG GEOLOGISTS, V21, P337, DOI [DOI 10.2113/GSEEGEOSCI.XXI.3.337, 10.2113/gseegeosci.xxi.3.337]
   Wu J, 2018, IEEE T KNOWL DATA EN, V30, P1065, DOI 10.1109/TKDE.2017.2788430
   Xu C, 2015, GEOMORPHOLOGY, V248, P77, DOI 10.1016/j.geomorph.2015.07.002
   Yi YN, 2020, IEEE J-STARS, V13, P6166, DOI 10.1109/JSTARS.2020.3028855
   Zhao W, 2017, IEEE J-STARS, V10, P1758, DOI 10.1109/JSTARS.2017.2661802
   Zhou YZ, 2017, IEEE INT CONF COMP V, P318, DOI 10.1109/ICCVW.2017.46
NR 46
TC 11
Z9 13
U1 13
U2 117
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 76
DI 10.1145/3454009
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100018
DA 2024-07-18
ER

PT J
AU Li, YD
   Liu, WH
   Jin, Y
   Cao, YZH
AF Li, Yidong
   Liu, Wenhua
   Jin, Yi
   Cao, Yuanzhouhan
TI SPGAN: Face Forgery Using Spoofing Generative Adversarial Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Imitated dataset; inter-intra attention mechanism; style classification
   constraint; imitation style transfer
AB Current face spoof detection schemes mainly rely on physiological cues such as eye blinking, mouth movements, and micro-expression changes, or textural attributes of the face images [9]. But none of these methods represent a viable mechanism for makeup-induced spoofing, especially since makeup has been widely used. Compared with face alteration techniques such as plastic surgery, makeup is non-permanent and cost efficient, which makes makeup-induced spoofing become a realistic threat to the integrity of a face recognition system. To solve this problem, we propose a generative model to construct spoofing face images (confusing face images) for improving the accuracy and robustness of automatic face recognition. Our network structure is composed of two separate parts, with one using inter-attention mechanism to obtain interested face region, and another using intra-attention to translate imitation style with preserving imitation style-excluding details. These two attention mechanisms can precisely learn imitation style, where inter-attention pays more attention to imitation regions of image and intra-attention learns face attributes with long distance in image. To effectively discriminate generated images, we introduce an imitation style discriminator. Our model (SP-GAN) generates face images that transfer the imitation style from target to subject image and preserve the imitation-excluding features. Experimental results demonstrate the performance of our model in improving quality of imitated face images.
C1 [Li, Yidong; Liu, Wenhua; Jin, Yi; Cao, Yuanzhouhan] Beijing Jiaotong Univ, 3 Shangyuancun, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Jin, Y (corresponding author), Beijing Jiaotong Univ, 3 Shangyuancun, Beijing 100044, Peoples R China.
EM ydli@bjtu.edu.cn; 16112077@bjtu.edu.cn; yjin@bjtu.edu.cn;
   yuanzhouhan.cao@gmail.com
OI Jin, Yi/0000-0001-8408-3816
CR Anjos Andre, 2011, 2011 International Joint Conference on Biometrics (IJCB), DOI 10.1109/IJCB.2011.6117503
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bousmalis Konstantinos, 2017, PROC CVPR IEEE, DOI [10.1109/CVPR, DOI 10.1109/CVPR]
   Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012
   CHE T, 2017, INT C LEARN REPR
   Chen CJ, 2016, INFORM FUSION, V32, P80, DOI 10.1016/j.inffus.2015.09.005
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen W., 2018, ARXIV PREPRINT ARXIV
   Cheng J., 2016, LONG SHORT TERM MEMO, DOI DOI 10.18653/V1/D16-1053
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Ding ST, 2020, NEUROCOMPUTING, V398, P520, DOI 10.1016/j.neucom.2019.04.095
   Dong H., 2017, UNSUPERVISED IMAGE T
   Gammulle H, 2019, IEEE WINT CONF APPL, P200, DOI 10.1109/WACV.2019.00027
   Gao Z, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377876
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Gulrajani I, 2017, ADV NEUR IN, V30
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kiasari MA, 2018, NEURAL NETWORKS, V100, P1, DOI 10.1016/j.neunet.2018.01.002
   Lample Guillaume, 2017, P ANN C NEUR INF PRO, P5967
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li C, 2015, PROC CVPR IEEE, P4621, DOI 10.1109/CVPR.2015.7299093
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li MJ, 2018, LECT NOTES COMPUT SC, V11213, P186, DOI 10.1007/978-3-030-01240-3_12
   Li Mu, 2016, 161005586 ARXIV
   Li Mu, 2016, 160806434 ARXIV
   Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618
   Lin ZH, 2017, Arxiv, DOI [arXiv:1703.03130, DOI 10.48550/ARXIV.1703.03130]
   Liu XW, 2020, NEURAL PROCESS LETT, V51, P211, DOI 10.1007/s11063-019-10080-2
   Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI DOI 10.48550/ARXIV.1411.4038
   Mao XF, 2018, NEUROCOMPUTING, V293, P55, DOI 10.1016/j.neucom.2018.02.092
   Mathew Salvaris, 2018, 14062661 ARXIV
   Metz Luke, 2017, Unrolled generative adversarial networks
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Miyato T, 2018, INT C LEARN REPR
   Parikh AP., 2016, EMNLP
   Patel K, 2016, IEEE T INF FOREN SEC, V11, P2268, DOI 10.1109/TIFS.2016.2578288
   Paulus Romain, 2017, ARXIV170504304
   Perarnau G., 2016, NIPS WORKSH ADV TRAI
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Radford A., 2015, ARXIV
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rusu Andrei A., 2016, ARXIV180907480
   Salimans Tim, 2018, ICLR
   Shao R, 2019, PROC CVPR IEEE, P10015, DOI 10.1109/CVPR.2019.01026
   Shen W, 2017, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2017.135
   Sonderby C. K., 2017, ICLR
   Tong WS, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P211, DOI 10.1109/PG.2007.31
   Tzeng Eric, 2016, ARXIV PREPRINT ARXIV
   Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645
   Wan SH, 2020, IEEE T MULTIMEDIA, V22, P1756, DOI 10.1109/TMM.2020.2976573
   Yamagishi J, 2017, IEEE J-STSP, V11, P585, DOI 10.1109/JSTSP.2017.2698143
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614
   Zhou Shuchang, 2017, 170504932 ARXIV
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
   Zhu YK, 2017, INT CONF ACOUST SPEE, P5335, DOI 10.1109/ICASSP.2017.7953175
   Zisserman Andrew, 2015, BMVC
NR 65
TC 4
Z9 4
U1 2
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 19
DI 10.1145/3432817
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900019
DA 2024-07-18
ER

PT J
AU Liang, HR
   Wu, J
   Zheng, X
   Zhang, MS
   Li, JH
   Jolfaei, A
AF Liang, Haoran
   Wu, Jun
   Zheng, Xi
   Zhang, Mengshi
   Li, Jianhua
   Jolfaei, Alireza
TI Fog-based Secure Service Discovery for Internet of Multimedia Things: A
   Cross-blockchain Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Internet of Multimedia Things (IoMT); blockchain; privacy-preserving;
   fog computing
ID PROXIMITY DETECTION; EFFICIENT
AB The Internet of Multimedia Things (IoMT) has become the backbone of innumerable multimedia applications in various fields. The wide application of IoMT not only makes our life convenient but also brings challenges to service discovery. Service discovery aims to leverage location information and trust evidence scattered in a variety of multimedia applications to find trusted IoMT devices that can provide specific service in target areas. However, the eavesdropping and tampering to these sensitive IoMT data during the trust propagation process invalidate the service discovery process. To address these challenges, we propose Secure Service Discovery (SSD) for IoMT using cross-blockchain-enabled fog computing. To resist the tampering and eavesdropping during the trust propagation process, a scalable cross-blockchain structure consisting of multiple parallel blockchains is first proposed based on fog, in which different parallel blockchains can be orchestrated to propagate encrypted location information and trust evidence of different applications. Moreover, to enable a cross-blockchain structure to leverage encrypted location information and trust evidence to find trusted IoMT devices in preset areas, a novel privacy-preserving range query is proposed to query and aggregate trust evidence. Security analysis and simulations are carried out to demonstrate the effectiveness and security of the proposed SSD.
C1 [Liang, Haoran; Wu, Jun; Li, Jianhua] Shanghai Jiao Tong Univ, Inst Cyber Sci & Technol, Shanghai Key Lab Integrated Adm Technol Informat, 800 Dongchuan Rd, Shanghai 20040, Peoples R China.
   [Zheng, Xi; Jolfaei, Alireza] Macquarie Univ, N Ryde, NSW 2109, Australia.
   [Zhang, Mengshi] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
C3 Shanghai Jiao Tong University; Macquarie University; University of Texas
   System; University of Texas Austin
RP Liang, HR (corresponding author), Shanghai Jiao Tong Univ, Inst Cyber Sci & Technol, Shanghai Key Lab Integrated Adm Technol Informat, 800 Dongchuan Rd, Shanghai 20040, Peoples R China.
EM hrliang@sjtu.edu.cn; junwuhn@sjtu.edu.cn; james.zheng@mq.edu.au;
   mengshi.zhang@utexas.edu; lijh888@sjtu.edu.cn; alireza.jolfaei@mq.edu.au
RI Jolfaei, Alireza/GQH-6907-2022; wu, jd/IST-2336-2023; Tang,
   Wei/IZQ-1283-2023; Zhang, Mengshi/HKN-1629-2023; Zheng,
   Xi/AAV-8387-2020; LI, WEI/ISS-1208-2023; Wu, Jun/HJP-1242-2023; Li,
   Jiaai/JCO-0168-2023; li, jian/IAQ-2794-2023; wu, jun/ISB-8607-2023
OI Zheng, Xi/0000-0002-2572-2355; Jolfaei, Alireza/0000-0001-7818-459X
FU National Natural Science Foundation of China [61972255]; Australian
   Research Council [LP190100676]; Australian Research Council
   [LP190100676] Funding Source: Australian Research Council
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant No. 61972255 and the Australian Research
   Council under Grant No. LP190100676.
CR Abdallah M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3212804
   Adewuyi AA, 2019, IEEE INTERNET THINGS, V6, P5432, DOI 10.1109/JIOT.2019.2902022
   Arnold V. I., 1968, ERGODIC PROBLEMS CLA
   Atrey PK, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3312574
   Awan KA, 2019, IEEE ACCESS, V7, P52191, DOI 10.1109/ACCESS.2019.2912469
   Çavusoglu U, 2018, NONLINEAR DYNAM, V92, P1745, DOI 10.1007/s11071-018-4159-4
   Dai HN, 2019, IEEE INTERNET THINGS, V6, P8076, DOI 10.1109/JIOT.2019.2920987
   Gai KK, 2019, IEEE INTERNET THINGS, V6, P7992, DOI 10.1109/JIOT.2019.2904303
   Grigorev A, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3360050
   Huang JQ, 2019, IEEE T IND INFORM, V15, P3680, DOI 10.1109/TII.2019.2903342
   Javaid U, 2020, IEEE INTERNET THINGS, V7, P11815, DOI 10.1109/JIOT.2020.3002711
   Li LC, 2016, IEEE INTERNET THINGS, V3, P206, DOI 10.1109/JIOT.2015.2469605
   Li LP, 2020, IEEE T IND INFORM, V16, P2091, DOI 10.1109/TII.2019.2927296
   Liang HF, 2019, PROCEEDINGS OF 2019 IEEE 3RD INTERNATIONAL ELECTRICAL AND ENERGY CONFERENCE (CIEEC), P683, DOI 10.1109/CIEEC47146.2019.CIEEC-2019273
   Liang HR, 2019, IEEE COMMUN MAG, V57, P77, DOI 10.1109/MCOM.001.1900143
   Lin X, 2019, IEEE T IND INFORM, V15, P6367, DOI 10.1109/TII.2019.2917307
   Lu YL, 2020, IEEE T VEH TECHNOL, V69, P4298, DOI 10.1109/TVT.2020.2973651
   Mu B, 2016, TSINGHUA SCI TECHNOL, V21, P270
   Pahins CAL, 2020, IEEE T VIS COMPUT GR, V26, P3314, DOI 10.1109/TVCG.2019.2914446
   Rani R, 2019, IEEE INTERNET THINGS, V6, P8421, DOI 10.1109/JIOT.2019.2917763
   Wang T, 2020, IEEE T IND INFORM, V16, P3531, DOI 10.1109/TII.2019.2920277
   Wang T, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3324926
   Xu CH, 2019, IEEE T PARALL DISTR, V30, P870, DOI 10.1109/TPDS.2018.2871449
   Xu GQ, 2019, INFORM SCIENCES, V476, P505, DOI 10.1016/j.ins.2018.05.022
   Yang Z, 2019, IEEE INTERNET THINGS, V6, P1495, DOI 10.1109/JIOT.2018.2836144
   Yao HP, 2019, IEEE T IND INFORM, V15, P3602, DOI 10.1109/TII.2019.2902563
   Yu RZ, 2018, IEEE COMMUN MAG, V56, P48, DOI 10.1109/MCOM.2018.1701140
   Zhao Sicheng, 2020, ACM T MULTIM COMPUT, V15, P93
   Zhu H, 2018, IEEE INTERNET THINGS, V5, P2947, DOI 10.1109/JIOT.2017.2766701
NR 29
TC 7
Z9 7
U1 0
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 3
SU S
AR 96
DI 10.1145/3415151
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2RB
UT WOS:000612586300004
DA 2024-07-18
ER

PT J
AU Ho, TT
   Virtusio, JJ
   Chen, YY
   Hsu, CM
   Hua, KL
AF Ho, Trang-Thi
   Virtusio, John Jethro
   Chen, Yung-Yao
   Hsu, Chih-Ming
   Hua, Kai-Lung
TI Sketch-guided Deep Portrait Generation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image synthesis; generative adversarial networks; semantic keypoints;
   perceptual loss; convolutional autoencoder
AB Generating a realistic human class image from a sketch is a unique and challenging problem considering that the human body has a complex structure that must be preserved. Additionally, input sketches often lack important details that are crucial in the generation process, hence making the problem more complicated. In this article, we present an effective method for synthesizing realistic images from human sketches. Our framework incorporates human poses corresponding to locations of key semantic components (e.g., arm, eyes, nose), seeing that its a strong prior for generating human class images. Our sketch-image synthesis framework consists of three stages: semantic keypoint extraction, coarse image generation, and image refinement. First, we extract the semantic keypoints using Part Affinity Fields (PAFs) and a convolutional autoencoder. Then, we integrate the sketch with semantic keypoints to generate a coarse image of a human. Finally, in the image refinement stage, the coarse image is enhanced by a Generative Adversarial Network (GAN) that adopts an architecture carefully designed to avoid checkerboard artifacts and to generate photo-realistic results. We evaluate our method on 6,300 sketch-image pairs and show that our proposed method generates realistic images and compares favorably against state-of-the-art image synthesis methods.
C1 [Ho, Trang-Thi; Virtusio, John Jethro] Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
   [Chen, Yung-Yao] Natl Taiwan Univ Sci & Technol, Dept Elect & Comp Engn, Taipei, Taiwan.
   [Hsu, Chih-Ming] Natl Taipei Univ Technol, Dept Mech Engn, Taipei, Taiwan.
   [Hua, Kai-Lung] Natl Taiwan Univ Sci & Technol, Dept CSIE, Taipei, Taiwan.
   [Hua, Kai-Lung] Natl Taiwan Univ Sci & Technol, Ctr Cyber Phys Syst Innovat, Taipei, Taiwan.
C3 National Taiwan University of Science & Technology; National Taiwan
   University of Science & Technology; National Taipei University of
   Technology; National Taiwan University of Science & Technology; National
   Taiwan University of Science & Technology
RP Ho, TT (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
EM hothitrang.dhdn@gmail.com; jetvirtusio@hotmail.com;
   yungyaochen@mail.ntust.edu.tw; jmshiu@ntut.edu.tw; hua@mail.ntust.edu.tw
RI Chen, Yung-Yao/IQU-8095-2023
OI Chen, Yung-Yao/0000-0001-6852-8862; Hua, Kai-Lung/0000-0002-7735-243X;
   Thi Ho, Trang/0000-0001-7541-3932
FU Center for Cyber-physical System Innovation and Center of Intelligent
   Robots from The Featured Areas Research Center Program within Ministry
   of Education (MOE) in Taiwan; Ministry of Science and Technology of
   Taiwan [MOST1082221-E-011-116, MOST108-2218-E-011-026]
FX This work was financially supported by the Center for Cyber-physical
   System Innovation and Center of Intelligent Robots from The Featured
   Areas Research Center Program within the framework of the Higher
   Education Sprout Project by the Ministry of Education (MOE) in Taiwan
   and Ministry of Science and Technology of Taiwan under Grants No.
   MOST1082221-E-011-116 and No. MOST108-2218-E-011-026.
CR [Anonymous], 2016, ARXIV161205360
   [Anonymous], 2014, ACM T GRAPHIC
   [Anonymous], ARXIV 1611 08050 CS, DOI DOI 10.1109/CVPR.2017.143
   Berthelot David, 2017, CoRR
   Cao Y, 2011, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2011.5995460
   Cao Yang, 2010, P 18 ACM INT C MULT, P1605
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Chen WL, 2018, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR.2018.00981
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eitz M, 2011, IEEE T VIS COMPUT GR, V17, P1624, DOI 10.1109/TVCG.2010.266
   Eitz M, 2011, IEEE COMPUT GRAPH, V31, P56, DOI 10.1109/MCG.2011.67
   Eitz M, 2010, COMPUT GRAPH-UK, V34, P482, DOI 10.1016/j.cag.2010.07.002
   Gatys L., 2015, NIPS
   Ghosh A, 2019, IEEE I CONF COMP VIS, P1171, DOI 10.1109/ICCV.2019.00126
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gulrajani I., 2017, Advances in neural information processing systems, P5769
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Heusel M., 2017, Advances in Neural Information Processing Systems, P6627, DOI [DOI 10.48550/ARXIV.1706.08500, 10.48550/arXiv.1706.08500]
   Hu R, 2013, COMPUT VIS IMAGE UND, V117, P790, DOI 10.1016/j.cviu.2013.02.005
   Hu R, 2010, IEEE IMAGE PROC, P1025, DOI 10.1109/ICIP.2010.5649331
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   James S., 2014, Proceedings of International Conference on Multimedia Retrieval, P313, DOI DOI 10.1145/2578726.2578766GLASGOW,UNITEDKINGDOM
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lassner C, 2017, IEEE I CONF COMP VIS, P853, DOI 10.1109/ICCV.2017.98
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lempitsky Victor, 2016, ARXIV160708022
   Li K, 2016, INT PARALL DISTRIB P, P1, DOI 10.1109/IPDPS.2016.126
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Lin YL, 2013, IEEE I CONF COMP VIS, P3495, DOI 10.1109/ICCV.2013.434
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lu Y., 2017, ARXIV171108972
   Lu YY, 2018, LECT NOTES COMPUT SC, V11220, P213, DOI 10.1007/978-3-030-01270-0_13
   Lu Yongyi, 2017, ARXIVABS171108972
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Odena A., 2016, DISTILL, V1, pe3, DOI 10.23915/distill.00003.-URL
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Photoshop Gravity, 2016, CREATE FILTER GALLER
   Rui Hu, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3661, DOI 10.1109/ICIP.2011.6116513
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Si CY, 2018, PROC CVPR IEEE, P118, DOI 10.1109/CVPR.2018.00020
   Simo-Serra Edgar, 2016, ACM Trans.Graph., V35
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun WC, 2019, PROCEEDINGS OF THE 2019 5TH INTERNATIONAL CONFERENCE ON COMPUTER AND TECHNOLOGY APPLICATIONS (ICCTA 2019), P117, DOI 10.1145/3323933.3324091
   Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361
   Wang Changhu., 2010, proceedings of the International Conference on World Wide Web, P1309
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu B., 2015, Empirical evaluation of rectified activations in convolutional network, DOI DOI 10.48550/ARXIV.1505.00853
   Yang BX, 2020, LECT NOTES COMPUT SC, V11961, P790, DOI 10.1007/978-3-030-37731-1_64
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 60
TC 15
Z9 15
U1 3
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 88
DI 10.1145/3396237
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200014
DA 2024-07-18
ER

PT J
AU Guo, CY
   Zhang, ZX
   Li, JJ
   Jiang, XS
   Zhang, J
   Zhang, L
AF Guo, Changyong
   Zhang, Zhaoxin
   Li, Jinjiang
   Jiang, Xuesong
   Zhang, Jun
   Zhang, Lei
TI Robust Visual Tracking Using Kernel Sparse Coding on Multiple Covariance
   Descriptors
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual tracking; covariance descriptor; Riemannian manifold; reproducing
   kernel Hilbert space; kernel sparse coding
ID OBJECT TRACKING; FACE RECOGNITION; REPRESENTATION
AB In this article, we aim to improve the performance of visual tracking by combing different features of multiple modalities. The core idea is to use covariance matrices as feature descriptors and then use sparse coding to encode different features. The notion of sparsity has been successfully used in visual tracking. In this context, sparsity is used along appearance models often obtained from intensity/color information. In this work, we step outside this trend and propose to model the target appearance by local covariance descriptors (CovDs) in a pyramid structure. The proposed pyramid structure not only enables us to encode local and spatial information of the target appearance but also inherits useful properties of CovDs such as invariance to affme transforms. Since CovDs lie on a Riemannian manifold, we further propose to perform tracking through sparse coding by embedding the Riemannian manifold into an infinite-dimensional Hilbert space. Embedding the manifold into a Hilbert space allows us to perform sparse coding efficiently using the kernel trick. Our empirical study shows that the proposed tracking framework outperforms the existing state-ofthe-art methods in challenging scenarios.
C1 [Guo, Changyong; Zhang, Zhaoxin] Harbin Inst Technol, 2 Wenhuaxi Rd, Weihai 264209, Shandong, Peoples R China.
   [Li, Jinjiang] Shandong Technol & Business Univ, Yantai, Shandong, Peoples R China.
   [Jiang, Xuesong] Harbin Inst Technol, Harbin, Heilongjiang, Peoples R China.
   [Zhang, Jun] Hefei Univ Technol, Hefei, Anhui, Peoples R China.
   [Zhang, Lei] Univ Pittsburgh, 3362 Fifth Ave, Pittsburgh, PA USA.
C3 Harbin Institute of Technology; Shandong Technology & Business
   University; Harbin Institute of Technology; Hefei University of
   Technology; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); University of Pittsburgh
RP Zhang, ZX (corresponding author), Harbin Inst Technol, 2 Wenhuaxi Rd, Weihai 264209, Shandong, Peoples R China.
EM hit_gcy@163.com; heart@hit.edu.cn; lijinjiang@gmail.com;
   xsjiang@hit.edu.cn; zhangjun@hfut.edu.cn; LEZ37@pitt.edu
RI wang, yatong/KDN-3824-2024; wang, yan/GSE-6489-2022
OI M.Khalaf, Nourhan/0009-0006-9717-9239
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2001, Sequential Monte Carlo methods in practice
   [Anonymous], 2018, P EUR C COMP VIS ECC
   [Anonymous], 2008, SIAM J OPTIMIZ
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247881
   [Anonymous], 1983, SOV MATH DOKL
   [Anonymous], 2014, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2013.230
   [Anonymous], 2011, Proc. NIPS, DOI DOI 10.1109/TPAMI.2013.57
   Caseiro R, 2013, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2013.13
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Danelljan Martin, 2018, ARXIV181107628
   Gao SH, 2010, LECT NOTES COMPUT SC, V6314, P1
   Geusebroek JM, 2003, IEEE T IMAGE PROCESS, V12, P938, DOI 10.1109/TIP.2003.812429
   Harandi M., 2015, P IEEE C COMP VIS PA, P1
   Harandi M, 2014, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2014.132
   Harandi MT, 2014, LECT NOTES COMPUT SC, V8695, P408, DOI 10.1007/978-3-319-10584-0_27
   Harandi MT, 2012, LECT NOTES COMPUT SC, V7573, P216, DOI 10.1007/978-3-642-33709-3_16
   Hu WM, 2012, IEEE T PATTERN ANAL, V34, P2420, DOI 10.1109/TPAMI.2012.42
   Jayasumana S, 2013, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2013.17
   Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Jiang F, 2015, J MACH LEARN RES, V16, P227
   Kim S, 2006, C IND ELECT APPL, P804
   Lan XY, 2014, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2014.156
   Li AN, 2016, IEEE T PATTERN ANAL, V38, P335, DOI 10.1109/TPAMI.2015.2417577
   Li H, 2011, PROCEEDINGS OF THE 5TH CONFERENCE ON CHINA'S ECONOMIC OPERATION RISK MANAGEMENT, P77
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Liu B., 2011, P IEEE C COMP VIS PA
   Liu BY, 2010, LECT NOTES COMPUT SC, V6314, P624
   Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Mei Xue, 2011, P IEEE C COMP VIS PA
   Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z
   Pérez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Porikli F., 2006, 2006 IEEE COMPUTER S, V1, P728, DOI [10.1109/CVPR.2006.94, DOI 10.1109/CVPR.2006.94]
   Qi YK, 2019, IEEE T PATTERN ANAL, V41, P1116, DOI 10.1109/TPAMI.2018.2828817
   Qi YK, 2018, IEEE T IMAGE PROCESS, V27, P3857, DOI 10.1109/TIP.2018.2797482
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Salti S, 2012, IEEE T IMAGE PROCESS, V21, P4334, DOI 10.1109/TIP.2012.2206035
   Sra S., 2012, ADV NEURAL INFORM PR, V1, P144
   Tang PQ, 2014, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MANAGEMENT AND ENGINEERING (CME 2014), P1107
   Tosato D, 2013, IEEE T PATTERN ANAL, V35, P1972, DOI 10.1109/TPAMI.2012.263
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Varma M, 2002, LECT NOTES COMPUT SC, V2352, P255
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang N., 2013, P INT C COMP VIS ICC
   Wang N., 2013, Advances in Neural Information Processing Systems, P809
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu Y, 2012, IEEE T IMAGE PROCESS, V21, P2824, DOI 10.1109/TIP.2011.2182521
   Xie Y, 2014, IEEE T CYBERNETICS, V44, P539, DOI 10.1109/TCYB.2013.2259230
   Yang HX, 2011, NEUROCOMPUTING, V74, P3823, DOI 10.1016/j.neucom.2011.07.024
   Yi Wu, 2011, Advances in Visual Computing. Proceedings 7th International Symposium, ISVC 2011, P738
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang L, 2015, PATTERN RECOGN LETT, V62, P17, DOI 10.1016/j.patrec.2015.04.010
   Zhang L, 2018, NANO-MICRO LETT, V10, DOI 10.1007/s40820-017-0178-9
   Zhang L, 2012, IEEE T SIGNAL PROCES, V60, P1684, DOI 10.1109/TSP.2011.2179539
   Zhang Shao-Yan, 2013, ISRN Radiol, V2013, P874570, DOI 10.5402/2013/874570
   Zhang SP, 2018, IEEE T INTELL TRANSP, V19, P187, DOI 10.1109/TITS.2017.2766093
   Zhang SP, 2017, IEEE T NEUR NET LEAR, V28, P2357, DOI 10.1109/TNNLS.2016.2586194
   Zhang SP, 2017, IEEE T CIRC SYST VID, V27, P421, DOI 10.1109/TCSVT.2016.2539860
   Zhang SP, 2015, AAAI CONF ARTIF INTE, P3165
   Zhang SP, 2015, IEEE T CIRC SYST VID, V25, P1749, DOI 10.1109/TCSVT.2015.2406194
   Zhang SP, 2015, SIGNAL PROCESS, V110, P132, DOI 10.1016/j.sigpro.2014.08.027
   Zhang SP, 2014, INFORM SCIENCES, V281, P635, DOI 10.1016/j.ins.2013.12.052
   Zhang SP, 2013, PATTERN RECOGN, V46, P1772, DOI 10.1016/j.patcog.2012.10.006
   Zhong BN, 2019, IEEE T IMAGE PROCESS, V28, P2331, DOI 10.1109/TIP.2018.2885238
   Zhong B, 2014, PATTERN RECOGN, V47, P1395, DOI 10.1016/j.patcog.2013.10.002
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zhou QQ, 2019, IEEE T MULTIMEDIA, V21, P1183, DOI 10.1109/TMM.2018.2875360
   Zhu HY, 2017, MULTIMED TOOLS APPL, V76, P4599, DOI 10.1007/s11042-016-3538-4
NR 73
TC 2
Z9 2
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 20
DI 10.1145/3360308
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300002
DA 2024-07-18
ER

PT J
AU Shen, LQ
   An, P
   Feng, GR
AF Shen, Liquan
   An, Ping
   Feng, Guorui
TI Low-Complexity Scalable Extension of the High-Efficiency Video Coding
   (SHVC) Encoding System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE SHVC; scalable video coding; mode decision; inter-layer correlation;
   low-complexity
ID FAST MODE DECISION; MOTION ESTIMATION; INTRA PREDICTION; ALGORITHM;
   QUALITY; LEVEL; SIZE
AB The scalable extension of the high-efficiency video coding (SHVC) system adopts a hierarchical quadtree-based coding unit (CU) that is suitable for various texture and motion properties of videos. Currently, the test model of SHVC identifies the optimal CU size by performing an exhaustive quadtree depth-level search, which achieves a high compression efficiency at a heavy cost in terms of the computational complexity. However, many interactive multimedia applications, such as remote monitoring and video surveillance, which are sensitive to time delays, have insufficient computational power for coding high-definition (HD) and ultra-highdefinition (UHD) videos. Therefore, it is important, yet challenging, to optimize the SHVC coding procedure and accelerate video coding. In this article, we propose a fast CU quadtree depth-level decision algorithm for inter-frames on enhancement layers that is based on an analysis of inter-layer, spatial, and temporal correlations. When motion/texture properties of coding regions can be identified early, a fast algorithm can be designed for adapting CU depth-level decision procedures to video contents and avoiding unnecessary computations during CU depth-level traversal. The proposed algorithm determines the motion activity level at the treeblock size of the hierarchical quadtree by utilizing motion vectors from its corresponding blocks at the base layer. Based on the motion activity level, neighboring encoded CUs that have larger correlations are preferentially selected to predict the optimal depth level of the current treeblock. Finally, two parameters, namely, the motion activity level and the predicted CU depth level, are used to identify a subset of candidate CU depth levels and adaptively optimize CU depth-level decision processes. The experimental results demonstrate that the proposed scheme can run approximately three times faster than the most recent SHVC reference software, with a negligible loss of compression efficiency. The proposed scheme is efficient for all types of scalable video sequences under various coding conditions and outperforms state-of-the-art fast SHVC and HEVC algorithms. Our scheme is a suitable candidate for interactive HD/UHD video applications that are expected to operate in real-time and power-constrained scenarios.
C1 [Shen, Liquan] Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai 200444, Peoples R China.
   [An, Ping; Feng, Guorui] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
C3 Shanghai University; Shanghai University
RP Shen, LQ (corresponding author), Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai 200444, Peoples R China.
EM jsslq@163.com; anping@shu.edu.cn; grfeng@shu.edu.cn
RI Shen, Liquan/D-4832-2012
FU National Natural Science Foundation of China [61671282, 61373151,
   61601278, 61525305]; Shanghai Pujiang Program [15pjd015]; Shanghai
   Science and Technology Innovation Plan [18010500200]; Shanghai Shuguang
   Program [17SG37]
FX This work is supported by the National Natural Science Foundation of
   China under grants No. 61671282, 61373151, 61601278, and 61525305 and
   sponsored by Shanghai Pujiang Program (15pjd015), Shanghai Science and
   Technology Innovation Plan (18010500200), and Shanghai Shuguang Program
   (17SG37).
CR [Anonymous], 2011, JCTVCE090
   [Anonymous], 2014, JCTVCP1009
   [Anonymous], JCTVCF045
   [Anonymous], 2013, JCTVCO0022
   [Anonymous], 2011, JCTVCF092
   Bailleul R, 2014, I SYMP CONSUM ELECTR, P195
   Bjontegaard G., 2001, 13 VCEG M33 M
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Correa G, 2015, IEEE INT SYMP CIRC S, P1114, DOI 10.1109/ISCAS.2015.7168833
   De Cock J, 2009, IEEE T MULTIMEDIA, V11, P1209, DOI 10.1109/TMM.2009.2030606
   Fan HF, 2016, IEEE T MULTIMEDIA, V18, P537, DOI 10.1109/TMM.2016.2515365
   Fu B, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2754167
   Fu GL, 2018, IEEE SIGNAL PROC LET, V25, P1665, DOI 10.1109/LSP.2018.2867895
   Ge QY, 2014, PROCEEDINGS OF 2014 IEEE WORKSHOP ON ADVANCED RESEARCH AND TECHNOLOGY IN INDUSTRY APPLICATIONS (WARTIA), P1366, DOI 10.1109/WARTIA.2014.6976537
   Grois D, 2014, IEEE T CIRC SYST VID, V24, P1025, DOI 10.1109/TCSVT.2014.2302557
   Hefeeda M, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1324287.1324289
   Heindel A, 2015, 2015 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING, P559, DOI 10.1109/ChinaSIP.2015.7230465
   Hilmi B, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE), P90, DOI 10.1109/ICCE.2012.6161754
   Díaz-Honrubia AJ, 2016, IEEE T CIRC SYST VID, V26, P154, DOI 10.1109/TCSVT.2015.2473299
   Jiménez-Moreno A, 2016, IEEE T MULTIMEDIA, V18, P563, DOI 10.1109/TMM.2016.2524995
   Kang M, 2016, INT FORUM DIGITAL TV, P349
   Kim HS, 2016, IEEE T CIRC SYST VID, V26, P130, DOI 10.1109/TCSVT.2015.2444672
   Lee B, 2016, IEEE T MULTIMEDIA, V18, P1257, DOI 10.1109/TMM.2016.2557075
   Lee B, 2012, IEEE T BROADCAST, V58, P285, DOI 10.1109/TBC.2012.2184154
   Lee JH, 2016, J REAL-TIME IMAGE PR, V12, P407, DOI 10.1007/s11554-014-0484-0
   Lei JJ, 2017, IEEE T BROADCAST, V63, P48, DOI 10.1109/TBC.2016.2623241
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P361, DOI 10.1109/TMM.2017.2745709
   Li G. H., 2011, 10 INT C ENV EL ENG, P1
   Li H, 2006, IEEE T CIRC SYST VID, V16, P889, DOI 10.1109/TCSVT.2006.877404
   Li Q., 2018, MATH PROBL ENG, P1
   Li XN, 2017, MULTIMED TOOLS APPL, V76, P8011, DOI 10.1007/s11042-016-3460-9
   Lu X, 2018, IEEE IMAGE PROC, P1792, DOI 10.1109/ICIP.2018.8451844
   Miao D, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2897395
   Pan ZQ, 2014, IEEE T BROADCAST, V60, P405, DOI 10.1109/TBC.2014.2321682
   Schwarz H., 2014, IEEE T CIRCUITS SYST, V17, P1103
   SHEN J, 2017, PLOS ONE, V23, DOI DOI 10.1371/JOURNAL.PONE.0171018
   Shen LQ, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700298
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen LQ, 2012, IEEE T IMAGE PROCESS, V21, P2582, DOI 10.1109/TIP.2011.2177849
   Shi ZB, 2012, IEEE T CIRC SYST VID, V22, P1813, DOI 10.1109/TCSVT.2012.2223031
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tohidypour H. R., 2014, 2014 IEEE International Conference on Consumer Electronics (ICCE), P191, DOI 10.1109/ICCE.2014.6775967
   Tohidypour H. R., 2013, P 6 BALK C INF BCI T, P61
   Tohidypour HR, 2016, IEEE T MULTIMEDIA, V18, P182, DOI 10.1109/TMM.2015.2510332
   Wang DY, 2014, LECT NOTES COMPUT SC, V8588, P693, DOI 10.1007/978-3-319-09333-8_75
   Wang H, 2014, P INT CONF BUS INTEL, P1, DOI 10.1109/BIFE.2013.1
   Wei-Ju Chiang, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P381, DOI 10.1109/ICMEW.2017.8026217
   Xu M, 2015, IEEE T IMAGE PROCESS, V24, P4250, DOI 10.1109/TIP.2015.2462747
   Yeh CH, 2010, IEEE T CIRC SYST VID, V20, P563, DOI 10.1109/TCSVT.2010.2041825
   Zhang YH, 2013, IEEE IMAGE PROC, P2000, DOI 10.1109/ICIP.2013.6738412
   Zuo XG, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P394, DOI 10.1109/VCIP.2014.7051589
NR 52
TC 6
Z9 6
U1 2
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 44
DI 10.1145/3313185
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IM3ZV
UT WOS:000477935400015
DA 2024-07-18
ER

PT J
AU He, C
   Hu, HF
AF He, Chen
   Hu, Haifeng
TI Image Captioning With Visual-Semantic Double Attention
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual-semantic double attention; image captioning; semantic attention
AB In this article, we propose a novel Visual-Semantic Double Attention (VSDA) model for image captioning. In our approach, VSDA consists of two parts: a modified visual attention model is used to extract sub-region image features, then a new SEmantic Attention (SEA) model is proposed to distill semantic features. Traditional attribute-based models always neglect the distinctive importance of each attribute word and fuse all of them into recurrent neural networks, resulting in abundant irrelevant semantic features. In contrast, at each timestep, our model selects the most relevant word that aligns with current context. In other words, the real power of VSDA lies in the ability of not only leveraging semantic features but also eliminating the influence of irrelevant attribute words to make the semantic guidance more precise. Furthermore, our approach solves the problem that visual attention models cannot boost generating non-visual words. Considering that visual and semantic features are complementary to each other, our model can leverage both of them to strengthen the generations of visual and non-visual words. Extensive experiments are conducted on famous datasets: MS COCO and Flickr30k. The results show that VSDA outperforms other methods and achieves promising performance.
C1 [He, Chen; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP He, C (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM hech35@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn
RI He, Chen/JLM-5059-2023
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; Natural Science Foundation of Guangdong Province
   [2017A030311029, 2016B010123005, 2017B090909005]; Science and Technology
   Program of Guangzhou of China [201704020180, 201604020024]; Fundamental
   Research Funds for the Central Universities of China
FX This work was supported in part by the National Natural Science
   Foundation of China (61673402, 61273270, 60802069), the Natural Science
   Foundation of Guangdong Province (2017A030311029, 2016B010123005,
   2017B090909005), the Science and Technology Program of Guangzhou of
   China (201704020180, 201604020024), and the Fundamental Research Funds
   for the Central Universities of China.
CR [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2017, ICCV
   [Anonymous], 2015, ARXIV150501809
   [Anonymous], 2004, P WORKSH TEXT SUMM B
   [Anonymous], 2017, P IEEE INT C COMP VI
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2013, P 2013 C EMP METH NA
   [Anonymous], 2014, ARXIV14128419
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gu J., 2017, P IEEE INT C COMP VI, P1222
   He C, 2019, NEURAL PROCESS LETT, V49, P177, DOI 10.1007/s11063-018-9807-7
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu JS, 2016, ADV NEUR IN, V29
   Mao JH, 2015, IEEE I CONF COMP VIS, P2533, DOI 10.1109/ICCV.2015.291
   Mao Junhua, 2014, CoRR
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Rennie Steven J., 2017, P IEEE C COMP VIS PA, V1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
NR 32
TC 8
Z9 10
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
AR 26
DI 10.1145/3292058
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9JB
UT WOS:000459798800010
DA 2024-07-18
ER

PT J
AU Hu, SH
   Xu, M
   Zhang, HM
   Xiao, CX
   Gui, C
AF Hu, Shenghong
   Xu, Min
   Zhang, Haimin
   Xiao, Chunxia
   Gui, Chao
TI Affective Content-aware Adaptation Scheme on QoE Optimization of
   Adaptive Streaming over HTTP
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE HTTP Adaptive Streaming; affective content analysis; Quality of
   Experience; MPEG-DASH
ID QUALITY
AB The article presents a novel affective content-aware adaptation scheme (ACAA) to optimize Quality of Experience (QoE) for dynamic adaptive video streaming over HTTP (DASH). Most of the existing DASH adaptation schemes conduct video bit-rate adaptation based on an estimation of available network resources, which ignore user preference on affective content (AC) embedded in video data streaming over the network. Since the personal demands to AC is very different among all viewers, to satisfy individual affective demand is critical to improve the QoE in commercial video services. However, the results of video affective analysis cannot be applied into a current adaptive streaming scheme directly. Correlating the AC distributions in user's viewing history to each being streamed segment, the affective relevancy can be inferred as an affective metric for the AC related segment. Further, we have proposed an ACAA scheme to optimize QoE for user desired affective content while taking into account both network status and affective relevancy. We have implemented the ACAA scheme over a realistic trace-based evaluation and compared its performance in terms of network performance, QoE with that of Probe and Adaptation (PANDA), buffer-based adaptation (BBA), and Model Predictive Control (MPC). Experimental results show that ACAA can preserve available buffer time for future being delivered affective content preferred by viewer's individual preference to achieve better QoE in affective contents than those normal contents while remain the overall QoE to be satisfactory.
C1 [Hu, Shenghong; Gui, Chao] Hubei Univ Econ, Sch Informat & Commun Engn, Wuhan 430205, Peoples R China.
   [Hu, Shenghong] Hubei Univ Econ, Res Ctr Hubei Financial Dev & Financial Secur, Wuhan 430205, Peoples R China.
   [Hu, Shenghong] Univ Technol Sydney, Sydney, NSW, Australia.
   [Hu, Shenghong; Xu, Min; Zhang, Haimin] Univ Technol Sydney, Fac Engn & Informat Technol, POB 2007, Sydney, NSW, Australia.
   [Xiao, Chunxia] Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
C3 Hubei University of Economics; Hubei University of Economics; University
   of Technology Sydney; University of Technology Sydney; Wuhan University
RP Xu, M (corresponding author), Univ Technol Sydney, Fac Engn & Informat Technol, POB 2007, Sydney, NSW, Australia.
EM wuhanhush@126.com; min.xu@uts.edu.au; haimin.zhang@student.uts.edu.au;
   cxxiao@whu.edu.au; gui_chao@126.com
RI Hu, Sheng/HNS-0538-2023; HU, SHENG/KEH-9483-2024
OI Xu, Min/0000-0001-9581-8849; Zhang, Haimin/0000-0002-0021-3634
FU Natural Science Foundation of China [61672390, 61572012]; National Key
   Research and Development Program of China [2017YFB1002600]; Humanities
   and Social Science Project of Ministry of Education [18YJCZH050];
   Natural Science Foundation of Hubei [2018CFB721]; Educational Commission
   Planning Project of Hubei [2018CFB721]; Wuhan Science and Technology
   Plan [2017010201010109]; Key Technological Innovation Projects of Hubei,
   China [2018AAA062]; China Scholarship Council
FX This work was supported by the Natural Science Foundation of China,
   under grant 61672390 and grant 61572012; the National Key Research and
   Development Program of China, under grant 2017YFB1002600; the Humanities
   and Social Science Project of Ministry of Education, under grant
   18YJCZH050; the Natural Science Foundation of Hubei, under grant
   2018CFB721; the Educational Commission Planning Project of Hubei, under
   grant 2018CFB721; the Wuhan Science and Technology Plan, under grant
   2017010201010109; the Key Technological Innovation Projects of Hubei,
   under grant 2018AAA062, China; and the China Scholarship Council.
CR [Anonymous], 2016, Proceedings of the 24th ACM international conference on Multimedia, DOI DOI 10.1145/2964284.2967196
   [Anonymous], 2019, CISCO VISUAL NETWORK
   Blender Foundation, 2008, BIGBUCKBUNNY
   Chang SF, 2005, P IEEE, V93, P148, DOI 10.1109/JPROC.2004.839600
   Chen X, 2017, IEEE T CIRC SYST VID, V27, P19, DOI 10.1109/TCSVT.2016.2539758
   Ciubotaru B, 2014, IEEE T BROADCAST, V60, P50, DOI 10.1109/TBC.2013.2290238
   Deng LH, 2014, SCI WORLD J, DOI 10.1155/2014/498134
   Ekman P., 1982, EMOTION HUMAN FACE
   Gao GY, 2018, IEEE T MULTIMEDIA, V20, P3399, DOI 10.1109/TMM.2018.2838330
   Hu SH, 2017, IEEE INT CON MULTI, P493, DOI 10.1109/ICME.2017.8019541
   Hu SH, 2014, IEEE GLOB COMM CONF, P1336, DOI 10.1109/GLOCOM.2014.7036993
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Kua J, 2017, IEEE COMMUN SURV TUT, V19, P1842, DOI 10.1109/COMST.2017.2685630
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Nie YW, 2013, IEEE T VIS COMPUT GR, V19, P1664, DOI 10.1109/TVCG.2012.176
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Rassool R., 2017, IEEE INT S BROADB MU, P1, DOI [10.1109/BMSB.2017.7986143, DOI 10.1109/BMSB.2017.7986143]
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Seufert M., 2015, IEEE COMMUNICATIONS, V17, DOI DOI 10.1109/C0MST.2014.2360940
   Spiteri K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P123, DOI 10.1145/3204949.3204953
   Tavakoli S, 2016, IEEE J SEL AREA COMM, V34, P2141, DOI 10.1109/JSAC.2016.2577361
   Timmerer C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1231, DOI 10.1145/2647868.2654849
   Thang TC, 2014, IEEE J SEL AREA COMM, V32, P693, DOI 10.1109/JSAC.2014.140403
   Wang SF, 2015, IEEE T AFFECT COMPUT, V6, P410, DOI 10.1109/TAFFC.2015.2432791
   Wijnants M, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1267, DOI 10.1145/2964284.2964300
   Xu M., 2006, P ACM MULT 06 SAN BA, P921
   Xu M., 2008, Proceeding of the 16th ACM International Conference on Multimedia, P677, DOI DOI 10.1145/1459359.1459457
   Xu M, 2014, MULTIMED TOOLS APPL, V70, P757, DOI 10.1007/s11042-012-1046-8
   Xu YD, 2014, IEEE T MULTIMEDIA, V16, P813, DOI 10.1109/TMM.2014.2300041
   Yin XQ, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P325, DOI 10.1145/2785956.2787486
   Zhang HM, 2018, IEEE T MULTIMEDIA, V20, P2824, DOI 10.1109/TMM.2018.2808760
   Zhang L, 2017, IEEE T IMAGE PROCESS, V26, P4114, DOI 10.1109/TIP.2017.2712283
   Zhao SC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3233184
   Zhao SC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P369, DOI 10.1145/3123266.3130858
   Zhao SC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4669
   Zhao SC, 2020, IEEE T AFFECT COMPUT, V11, P574, DOI [10.1109/TAFFC.2018.2818685, 10.1109/TAFFC.2016.2628787]
NR 38
TC 8
Z9 8
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2019
VL 15
IS 3
SU S
SI SI
AR 100
DI 10.1145/3328997
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LR5EU
UT WOS:000535718800016
DA 2024-07-18
ER

PT J
AU Zahran, AH
   Quinlan, JJ
   Ramakrishnan, KK
   Sreenan, CJ
AF Zahran, Ahmed H.
   Quinlan, Jason J.
   Ramakrishnan, K. K.
   Sreenan, Cormac J.
TI ASAP: Adaptive Stall-Aware Pacing for Improved DASH Video Experience in
   Cellular Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adaptive bitrate video streaming; DASH; QoE; separable programming
ID RATE ADAPTATION; HTTP; FAIRNESS
AB The dramatic growth of video traffic represents a practical challenge for cellular network operators in providing a consistent streaming Quality of Experience (QoE) to their users. Satisfying this objective has so-far proved elusive, due to the inherent characteristics of wireless networks and varying channel conditions as well as variability in the video bitrate that can degrade streaming performance. In this article, we propose stall-aware pacing as a novel MPEG DASH video traffic management solution that reduces playback stalls and seeks to maintain a consistent QoE for cellular users, even those with diverse channel conditions. These goals are achieved by leveraging both network and client state information to optimize the pacing of individual video flows. We evaluate the performance of two versions of stall-aware pacing techniques extensively, including stall-aware pacing (SAP) and adaptive stall-aware pacing (ASAP), using real video content and clients, operating over a simulated LTE network. We implement state-of-the-art client adaptation and traffic management strategies for direct comparisons with SAP and ASAP. Our results, using a heavily loaded base station, show that SAP reduces the number of stalls and the average stall duration per session by up to 95%. Additionally, SAP ensures that clients with good channel conditions do not dominate available wireless resources, evidenced by a reduction of up to 40% in the standard deviation of the QoE metric across clients. We also show that ASAP achieves additional performance gains by adaptively pacing video streams based on the application buffer state.
C1 [Zahran, Ahmed H.; Quinlan, Jason J.; Sreenan, Cormac J.] Univ Coll Cork, Dept Comp Sci, Western Gateway Bld, Cork, Ireland.
   [Ramakrishnan, K. K.] Univ Calif Riverside, Dept Comp Sci & Engn, Room 332 Winston Chung Hall,900 Univ Ave, Riverside, CA 92521 USA.
C3 University College Cork; University of California System; University of
   California Riverside
RP Zahran, AH (corresponding author), Univ Coll Cork, Dept Comp Sci, Western Gateway Bld, Cork, Ireland.
EM a.zahran@cs.ucc.ie; j.quinlan@cs.ucc.ie; kk@cs.ucr.edu; cjs@cs.ucc.ie
RI Quinlan, Jason/AAF-9605-2019
OI Quinlan, Jason/0000-0001-7810-9768; Zahran, Ahmed/0000-0003-3405-0324;
   Ramakrishnan, Kadangode/0000-0003-1849-5155
FU Science Foundation Ireland (SFI) [13/IA/1892]; NSF [CNS-1619441]
FX This publication has emanated from research conducted with the financial
   support of Science Foundation Ireland (SFI) under Grant No.: 13/IA/1892.
   This work was supported in part by NSF grant CNS-1619441.
CR Akhshabi Saamer, 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video-NOSSDAV'13
   [Anonymous], J COMPUT APPL MATH
   [Anonymous], 2012, 2012 USENIX ANN TECH
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], P ACM MULT SYST C MM
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], IEEE J SELECTED AREA
   Bouten N, 2015, COMPUT NETW, V81, P96, DOI 10.1016/j.comnet.2015.02.007
   Chen J., 2013, Proceedings of the 19th Annual International Conference on Mobile Computing Networking, P389
   Cofano G, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P24, DOI 10.1145/2910017.2910597
   De Cicco L, 2014, IEEE ACM T NETWORK, V22, P526, DOI 10.1109/TNET.2013.2253797
   De Vleeschauwer D, 2013, IEEE INFOCOM SER, P989
   De Vriendt J, 2013, 2013 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2013), P1288
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   HOCHBAUM DS, 1990, J ACM, V37, P843, DOI 10.1145/96559.96597
   Houdaille R., 2012, P 3 MULTIMEDIA SYSTE, P1
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Kleinrouweler J.W., 2017, Proceedings of the 27th Workshop on Network and Operating Systems Support for Digital Audio and Video, P73, DOI DOI 10.1145/3083165.3083167
   Kleinrouweler JW, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092838
   Kua J, 2017, IEEE COMMUN SURV TUT, V19, P1842, DOI 10.1109/COMST.2017.2685630
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   Mok R. K. P., 2011, 2011 IFIP/IEEE International Symposium on Integrated Network Management (IM 2011), P485, DOI 10.1109/INM.2011.5990550
   Mu M, 2016, IEEE J SEL AREA COMM, V34, P2168, DOI 10.1109/JSAC.2016.2577318
   Nguyen DM, 2015, PROCEEDINGS OF 2015 2ND NATIONAL FOUNDATION FOR SCIENCE AND TECHNOLOGY DEVELOPMENT CONFERENCE ON INFORMATION AND COMPUTER SCIENCE NICS 2015, P248, DOI 10.1109/NICS.2015.7302201
   Petrangeli S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818361
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Quinlan JJ, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P386, DOI 10.1145/2910017.2910625
   Seetharam A, 2015, IEEE T MOBILE COMPUT, V14, P619, DOI 10.1109/TMC.2014.2331963
   Thang TC, 2013, J COMMUN NETW-S KOR, V15, P635, DOI 10.1109/JCN.2013.000112
   Wei Pu, 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P65, DOI 10.1109/PV.2012.6229745
   Yao J, 2011, LECT NOTES COMPUT SC, V6640, P92, DOI 10.1007/978-3-642-20757-0_8
   Yousaf FZ, 2013, IEEE NETWORK, V27, P14, DOI 10.1109/MNET.2013.6485091
   Zahran AH, 2016, IEEE INT CONF MULTI
   Zahran AH, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P13, DOI 10.1145/3083187.3083199
NR 36
TC 0
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
SU S
AR 61
DI 10.1145/3219750
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG9GD
UT WOS:000455314500009
OA Bronze
DA 2024-07-18
ER

PT J
AU Kim, Y
   Provost, EM
AF Kim, Yelin
   Provost, Emily Mower
TI Emotion Recognition During Speech Using Dynamics of Multiple Regions of
   the Face
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Emotion; emotion recognition; facial movement;
   segmentation
ID EXPRESSION; MODELS
AB The need for human-centered, affective multimedia interfaces has motivated research in automatic emotion recognition. In this article, we focus on facial emotion recognition. Specifically, we target a domain in which speakers produce emotional facial expressions while speaking. The main challenge of this domain is the presence of modulations due to both emotion and speech. For example, an individual's mouth movement may be similar when he smiles and when he pronounces the phoneme /IY/, as in "cheese". The result of this confusion is a decrease in performance of facial emotion recognition systems. In our previous work, we investigated the joint effects of emotion and speech on facial movement. We found that it is critical to employ proper temporal segmentation and to leverage knowledge of spoken content to improve classification performance. In the current work, we investigate the temporal characteristics of specific regions of the face, such as the forehead, eyebrow, cheek, and mouth. We present methodology that uses the temporal patterns of specific regions of the face in the context of a facial emotion recognition system. We test our proposed approaches on two emotion datasets, the IEMOCAP and SAVEE datasets. Our results demonstrate that the combination of emotion recognition systems based on different facial regions improves overall accuracy compared to systems that do not leverage different characteristics of individual regions.
C1 [Kim, Yelin; Provost, Emily Mower] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
C3 University of Michigan System; University of Michigan
RP Kim, Y (corresponding author), Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
EM yelinkim@umich.edu
OI Provost, Emily/0000-0003-1870-6063
CR [Anonymous], P ACM INT C MULT ACM
   [Anonymous], P INT PITTSB PA US
   [Anonymous], 2011, ICDECOM
   [Anonymous], P INTERSPEECH
   [Anonymous], 2013, 2013 NAT C COMM NCC
   [Anonymous], 2013, P 3 ACM INT WORKSHOP
   [Anonymous], Environmental Psychology & Nonverbal Behavior
   [Anonymous], 2014, DEPRESSION
   [Anonymous], 2004, P INTERSPEECH
   [Anonymous], 2011, Speech and Audio Signal Processing: Processing and Perception of Speech and Music, DOI 10.1002/9781118142882
   [Anonymous], OXFORD HDB AFFECTIVE
   [Anonymous], IEEE T SPEECH AUDIO
   [Anonymous], 2004, P 10 AUSTR INT C SPE
   Arons B., 1994, ICSLP 94. 1994 International Conference on Spoken Language Processing, P1931
   Bates D., 2007, LME4 LINEAR MIXED EF
   Bevacqua E, 2004, COMPUT ANIMAT VIRT W, V15, P297, DOI 10.1002/cav.32
   Bhattacharya S., 2013, Proc. 21st ACM Internat. Conf. Multimedia (ACM, P361
   Bigot Benjamin, 2008, 2008 International Workshop on Content-based Multimedia Indexing - CBMI 2008, P162, DOI 10.1109/CBMI.2008.4564942
   Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277
   Boston MF, 2008, J EYE MOVEMENT RES, V2
   Busso C, 2007, IEEE T AUDIO SPEECH, V15, P2331, DOI 10.1109/TASL.2007.905145
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Cambria E., 2013, IEEE INTELLIGENT SYS, P1
   Chandrasekaran C, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000436
   Chen JY, 2003, PATTERN RECOGN, V36, P943, DOI 10.1016/S0031-3203(02)00128-0
   Chen T, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P367, DOI 10.1145/2647868.2654935
   Dhall Abhinav, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P878, DOI 10.1109/FG.2011.5771366
   Dietterich TG, 1998, NEURAL COMPUT, V10, P1895, DOI 10.1162/089976698300017197
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Gharavian D, 2012, NEURAL COMPUT APPL, V21, P2115, DOI 10.1007/s00521-011-0643-1
   Gunes Hatice, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P827, DOI 10.1109/FG.2011.5771357
   Haq S., 2010, Machine Audition: Principles, Algorithms and Systems, P398, DOI DOI 10.4018/978-1-61520-919-4
   Jou B, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P213, DOI 10.1145/2647868.2656408
   Keshet J., 2005, Phoneme alignment based on discriminative learning
   Kipp M., 2009, 2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops, P1, DOI DOI 10.1109/ACII.2009.5349544
   Kleinsmith A, 2013, IEEE T AFFECT COMPUT, V4, P15, DOI 10.1109/T-AFFC.2012.16
   Lee CC, 2011, SPEECH COMMUN, V53, P1162, DOI 10.1016/j.specom.2011.06.004
   Lee CC, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P328
   Lee J G, 2007, SIGMOD C, P593, DOI DOI 10.1145/1247480.1247546
   Mariooryad S, 2013, IEEE INT CONF AUTOMA
   Meng HY, 2011, LECT NOTES COMPUT SC, V6975, P378, DOI 10.1007/978-3-642-24571-8_49
   Metallinou A, 2013, IMAGE VISION COMPUT, V31, P137, DOI 10.1016/j.imavis.2012.08.018
   Metallinou A, 2012, IEEE T AFFECT COMPUT, V3, P184, DOI 10.1109/T-AFFC.2011.40
   Metallinou A, 2010, INT CONF ACOUST SPEE, P2474, DOI 10.1109/ICASSP.2010.5494893
   Mower E, 2011, INT CONF ACOUST SPEE, P2372
   Mower E, 2011, IEEE T AUDIO SPEECH, V19, P1057, DOI 10.1109/TASL.2010.2076804
   Mower E, 2009, IEEE T MULTIMEDIA, V11, P843, DOI 10.1109/TMM.2009.2021722
   Narayanan S, 2013, P IEEE, V101, P1203, DOI 10.1109/JPROC.2012.2236291
   Nicolle J, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P501
   Pantic M., 2007, FACE RECOGNITION, P377
   Provost EM, 2013, INT CONF ACOUST SPEE, P3682, DOI 10.1109/ICASSP.2013.6638345
   Qiao Y, 2008, INT CONF ACOUST SPEE, P3989
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Sandbach G., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P406, DOI 10.1109/FG.2011.5771434
   Savran A, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P485
   Schuller B, 2013, COMPUT SPEECH LANG, V27, P4, DOI 10.1016/j.csl.2012.02.005
   Schuller B, 2011, SPEECH COMMUN, V53, P1062, DOI 10.1016/j.specom.2011.01.011
   Shah M, 2013, INT CONF AFFECT, P49, DOI 10.1109/ACII.2013.15
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Toledano DT, 2003, IEEE T SPEECH AUDI P, V11, P617, DOI 10.1109/TSA.2003.813579
   Vlasenko B, 2014, COMPUT SPEECH LANG, V28, P483, DOI 10.1016/j.csl.2012.11.003
   Wan SH, 2014, PATTERN RECOGN, V47, P1859, DOI 10.1016/j.patcog.2013.11.025
   Wu SQ, 2011, SPEECH COMMUN, V53, P768, DOI 10.1016/j.specom.2010.08.013
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
NR 65
TC 19
Z9 20
U1 1
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 25
DI 10.1145/2808204
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100015
DA 2024-07-18
ER

PT J
AU Prasad, M
   Russell, M
   Hammond, TA
AF Prasad, Manoj
   Russell, Murat
   Hammond, Tracy A.
TI Designing Vibrotactile Codes to Communicate Verb Phrases
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Human Factors; Measurement; Standardization; Tactile
   interface; tactile code; user centric design; Vibrotactile pattern
   perception; graph model; perception model; communication
AB Soldiers, to guard themselves from enemy assault, have to maintain visual and auditory awareness of their environment. Their visual and auditory senses are thus saturated. This makes these channels less usable for communication. The tactile medium of communication with users is appropriate for displaying information in such situations. Research in interpersonal communication among soldiers shows that the most common form of communication between soldiers involves the use of verb phrases. In this article, we have developed a three-by-three tactile display and proposed a method for mapping the components of a verb phrase to two dimensions of tactile codes-shape and waveform. Perception of tactile codes by users depends on the ability of users to distinguish shape and waveform of the code. We have proposed a measure to rate the distinguish-ability of any two shapes and created a graph-based user-centric model using this measure to select distinguishable shapes from a set of all presentable shapes. We conducted two user studies to evaluate the ability of users to perceive tactile information. The results from our first study showed users' ability to perceive tactile shapes, tactile waveforms, and form verb phrases from tactile codes. The recognition accuracy and time taken to distinguish were better when the shapes were selected from the graph model than when shapes were chosen based on intuition. The second user study was conducted to test the performance of users while performing a primary visual task simultaneously with a secondary audio or haptic task. Users were more familiar with perceiving information from an auditory medium than from a haptic medium, which was reflected in their performance. Thus the performance of users in the primary visual task was better while using an audio medium of communication than while using a haptic medium of communication.
C1 [Prasad, Manoj; Russell, Murat; Hammond, Tracy A.] Texas A&M Univ, College Stn, TX 77843 USA.
C3 Texas A&M University System; Texas A&M University College Station
RP Prasad, M (corresponding author), 3112 TAMU, College Stn, TX 77843 USA.
EM manoj.prasad@neo.tamu.edu; mrussell@tamu.edu; hammond@cse.tamu.edu
OI Hammond, Tracy/0000-0001-7272-0507
CR BLISS JC, 1970, IEEE T MAN MACHINE, VMM11, P58, DOI 10.1109/TMMS.1970.299963
   Borst CW, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P599, DOI 10.1109/WHC.2009.4810862
   Borst CW, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P329
   Chomsky Noam., 1966, Syntactic structures
   Chomsky Noam, 1965, ASPECTS THEORY SYNTA, V11
   CRAIG JC, 1982, PERCEPT PSYCHOPHYS, V31, P523, DOI 10.3758/BF03204184
   Entin EE, 1999, HUM FACTORS, V41, P312, DOI 10.1518/001872099779591196
   Hu YF, 2012, CH CRC COMP SCI SER, P525
   Lindeman R.W., 2004, ACM Symposium on Virtual Reality Software and Technology, P146, DOI DOI 10.1145/1077534.1077562
   Lipari NG, 2010, LECT NOTES COMPUT SC, V6453, P729
   MacLean K., 2003, P EUROHAPTICS 2003, P351, DOI DOI 10.1109/HAPTICS.2006.171
   MacLean KE, 2008, IEEE T HAPTICS, V1, P84, DOI [10.1109/TOH.2008.20, 10.1109/ToH.2008.20]
   Neyem A, 2006, LECT NOTES COMPUT SC, V4154, P228
   Nicolau Hugo., 2013, P 15 INT ACM SIGACCE, V23, P1, DOI DOI 10.1145/2513383.2513437
   Norretranders T., 1991, The user illusion: Cutting consciousness down to size
   Prasad Manoj A., 2014, P HAPT S HAPTICS
   Rupert AH, 2000, IEEE ENG MED BIOL, V19, P71, DOI 10.1109/51.827409
   SALAS E, 1995, MIL PSYCHOL, V7, P55, DOI 10.1207/s15327876mp0702_2
   Self B. P., 2005, TACTILE DISPLAYS ORI
   SHERRICK CE, 1975, AM PSYCHOL, V30, P353
   Ternes David Richard, 2007, THESIS U BRIT COLUMB
   URBAN JM, 1995, MIL PSYCHOL, V7, P123, DOI 10.1207/s15327876mp0702_6
   van Erp J., 2003, P EUROHAPTICS, V2003, P111
   van Erp JanB. F., 2001, P EUROHAPTICS, P99
   Vibration Motor, 2012, VIBRATION MOTOR PROD
   Wickens C. D., 2002, Theor Issues Ergon Sci, V3, P159, DOI [10.1080/14639220210123806, DOI 10.1080/14639220210123806]
NR 26
TC 7
Z9 7
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2014
VL 11
IS 1
SU S
SI SI
AR 11
DI 10.1145/2637289
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AS0RG
UT WOS:000343984800003
DA 2024-07-18
ER

PT J
AU Liu, YJ
   Ma, CX
   Fu, QF
   Fu, XL
   Qin, SF
   Xie, LX
AF Liu, Yong-Jin
   Ma, Cui-Xia
   Fu, Qiufang
   Fu, Xiaolan
   Qin, Sheng-Feng
   Xie, Lexing
TI A Sketch-Based Approach for Interactive Organization of Video Clips
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; Performance; Sketching interface; video
   organization; sketch annotation; context-aware recommendation
ID SCALE; MODEL
AB With the rapid growth of video resources, techniques for efficient organization of video clips are becoming appealing in the multimedia domain. In this article, a sketch-based approach is proposed to intuitively organize video clips by: (1) enhancing their narrations using sketch annotations and (2) structurizing the organization process by gesture-based free-form sketching on touch devices. There are two main contributions of this work. The first is a sketch graph, a novel representation for the narrative structure of video clips to facilitate content organization. The second is a method to perform context-aware sketch recommendation scalable to large video collections, enabling common users to easily organize sketch annotations. A prototype system integrating the proposed approach was evaluated on the basis of five different aspects concerning its performance and usability. Two sketch searching experiments showed that the proposed context-aware sketch recommendation outperforms, in terms of accuracy and scalability, two state-of-the-art sketch searching methods. Moreover, a user study showed that the sketch graph is consistently preferred over traditional representations such as keywords and keyframes. The second user study showed that the proposed approach is applicable in those scenarios where the video annotator and organizer were the same person. The third user study showed that, for video content organization, using sketch graph users took on average 1/3 less time than using a mass-market tool Movie Maker and took on average 1/4 less time than using a state-of-the-art sketch alternative. These results demonstrated that the proposed sketch graph approach is a promising video organization tool.
C1 [Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, TNList, Beijing, Peoples R China.
   [Ma, Cui-Xia] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100864, Peoples R China.
   [Fu, Qiufang; Fu, Xiaolan] Chinese Acad Sci, Inst Psychol, State Key Lab Brain & Cognit Sci, Beijing 100864, Peoples R China.
   [Qin, Sheng-Feng] Northumbria Univ, Dept Design, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
   [Xie, Lexing] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia.
C3 Tsinghua University; Chinese Academy of Sciences; Institute of Software,
   CAS; Chinese Academy of Sciences; Institute of Psychology, CAS;
   Northumbria University; Australian National University
RP Liu, YJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, TNList, Beijing, Peoples R China.
EM liuyongjin@tsinghua.edu.cn
RI Liu, Yong/GWQ-6163-2022; , Grace/ABB-4349-2021
OI Xie, Lexing/0000-0001-8319-0118; Qin, Shengfeng/0000-0001-8538-8136
FU The Natural Science Foundation of China [61322206, 61173058]; 973
   program of China [2011CB302202]; 863 Program of China [2012AA011801];
   Tsinghua University Initiative Scientific Research Program [20131089252]
FX This work is supported by The Natural Science Foundation of China under
   grants 61322206, 61173058, the 973 program of China under grant
   2011CB302202, the 863 Program of China under grant 2012AA011801, and the
   Tsinghua University Initiative Scientific Research Program, under grant
   20131089252.
CR [Anonymous], 2011, ADV NEURAL INFORM PR
   [Anonymous], 2011, EUROGRAPHICS STARS
   Bailey BrianP., 2001, P 9 ACM INT C MULTIM, P241
   Best J.B., 1986, COGNITIVE PSYCHOL
   Bulterman DCA, 2005, ACM T MULTIM COMPUT, V1, P89, DOI 10.1145/1047936.1047943
   Cao Y, 2011, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2011.5995460
   Correa CD, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778825
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Fu Qiu-Fang, 2013, J VIS, V13, P9, DOI DOI 10.1167/13.9.1060
   Harada K., 1996, Proceedings ACM Multimedia 96, P341, DOI 10.1145/244130.244235
   Irani M, 1998, P IEEE, V86, P905, DOI 10.1109/5.664279
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Kang H, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P43
   Kanizsa G., 1979, Organization in Vision: Essays on Gestalt Perception
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Liu YJ, 2013, IEEE T AUTOM SCI ENG, V10, P783, DOI 10.1109/TASE.2012.2228481
   Liu YJ, 2013, SCI CHINA INFORM SCI, V56, DOI 10.1007/s11432-013-4994-3
   Ma CX, 2012, IEEE T MULTIMEDIA, V14, P1153, DOI 10.1109/TMM.2012.2190389
   Ma CX, 2011, IEEE T AUTOM SCI ENG, V8, P431, DOI 10.1109/TASE.2010.2086444
   Madirakshi Das, 1998, P IEEE INT C MULT CO, P372
   Mei T, 2008, MULTIMED TOOLS APPL, V40, P89, DOI 10.1007/s11042-007-0186-8
   Mei T, 2009, VISUAL COMPUT, V25, P39, DOI 10.1007/s00371-008-0282-4
   Moxley E, 2010, IEEE T MULTIMEDIA, V12, P184, DOI 10.1109/TMM.2010.2041101
   Rodgers P.A., 2000, Design Studies, V21, P451, DOI [DOI 10.1016/S0142-694X(00)00018-1, 10.1016/S0142-694X, DOI 10.1016/S0142-694X]
   RUBINE D, 1991, COMP GRAPH, V25, P329, DOI 10.1145/127719.122753
   Sun X., 2013, Proceedings of the 21st ACM international conference on Multimedia, P475
   Sun ZB, 2012, LECT NOTES COMPUT SC, V7572, P626, DOI 10.1007/978-3-642-33718-5_45
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Wang Jingdong, 2011, ACM T INTEL SYST TEC, V3
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Yu CC, 2014, GRAPH MODELS, V76, P507, DOI 10.1016/j.gmod.2014.03.015
   Zhang JK, 2013, J COMPUT SCI TECH-CH, V28, P810, DOI 10.1007/s11390-013-1379-4
   Zhang YJ, 2002, PATTERN RECOGN, V35, P2381, DOI 10.1016/S0031-3203(01)00189-3
   Zhu XQ, 2005, IEEE T MULTIMEDIA, V7, P648, DOI 10.1109/TMM.2005.850977
NR 35
TC 7
Z9 7
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2014
VL 11
IS 1
AR 2
DI 10.1145/2645643
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AP2XQ
UT WOS:000341939800002
DA 2024-07-18
ER

PT J
AU Li, HT
   Cheng, X
   Liu, JC
AF Li, Haitao
   Cheng, Xu
   Liu, Jiangchuan
TI Understanding Video Sharing Propagation in Social Networks: Measurement
   and Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurement; Performance; Social network; information propagation; video
   sharing; measurement
AB Modern online social networking has drastically changed the information distribution landscape. Recently, video has become one of the most important types of objects spreading among social networking service users. The sheer and ever-increasing data volume, the broader coverage, and the longer access durations of video objects, however, present significantly more challenges than other types of objects. This article takes an initial step toward understanding the unique characteristics of video sharing propagation in social networks. Based on realworld data traces from a large-scale online social network, we examine the user behavior from diverse aspects and identify different types of users involved in video propagation. We closely investigate the temporal distribution during propagation as well as the typical propagation structures, revealing more details beyond stationary coverage. We further extend the conventional epidemic models to accommodate diverse types of users and their probabilistic viewing and sharing behaviors. The model, effectively capturing the essentials of the propagation process, serves as a valuable basis for such applications as workload synthesis, traffic prediction, and resource provision of video servers.
C1 [Li, Haitao; Cheng, Xu; Liu, Jiangchuan] Simon Fraser Univ, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Li, HT (corresponding author), Simon Fraser Univ, 8888 Univ Dr, Burnaby, BC V5A 1S6, Canada.
EM haitaol@sfu.ca
RI Li, Haitao/ISB-0809-2023
OI Li, Haitao/0000-0003-0682-2868
FU Canadian NSERC Discovery Grant; Chinese NSFC Major Program of
   International Cooperation Grant [61120106008]
FX This research is supported by a Canadian NSERC Discovery Grant, a
   Strategic Project Grant, and a Chinese NSFC Major Program of
   International Cooperation Grant (61120106008).
CR ADAR E, 2005, P IEEE WIC ACM INT C
   Bakshy E., 2011, P ACM INT C WEB SEAR
   Borghol Y., 2012, ACM SIGKDD International Knowledge Discovery and Data Mining, P1186, DOI [DOI 10.1145/2339530.2339717, 10.1145/2339530.2339717]
   Borghol Y, 2011, PERFORM EVALUATION, V68, P1037, DOI 10.1016/j.peva.2011.07.008
   Budak C., 2011, P 20 INT C WORLD WID, P665, DOI DOI 10.1145/1963405.1963499
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Daley DJ., 2001, Epidemic Modelling: An Introduction
   Do YH, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.035202
   Dyagilev K., 2010, P WORKSH SOC MED AN
   Ganesh A, 2005, IEEE INFOCOM SER, P1455
   Goel S., 2012, P 13 ACM C EL COMM E
   Jiang J., 2010, P 10 ACM SIGCOMM INT
   Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497
   Li H., 2010, P 4 IEEE INT C CLOUD, P203
   Li H., 2012, P 20 IEEE INT WORKSH
   Li H., 2013, P IEEE INFOCOM MIN
   Li Haitao., 2012, P 22 INT WORKSHOP NE, P83
   Li H, 2013, TRANSGENIC RES, V22, P169, DOI 10.1007/s11248-012-9623-1
   Newman MEJ, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.016128
   Rosoff Matt., 2011, Twitter Just Had Its CNN Moment
   Siegler M., 2009, USE TWITTER ANONYMOU
   Steeg G. V., 2011, P 5 AAAI INT C WEBL
   Tang S., 2011, P 30 IEEE ANN INT C
   Wang D., 2011, the 20th International World Wide Web Conference (WWW), P735, DOI DOI 10.1145/1963405.1963508
   Wang Zhi., 2012, Proceedings of the 20th ACM international conference on Multimedia, MM '12, P29
NR 25
TC 8
Z9 9
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2014
VL 10
IS 4
AR 33
DI 10.1145/2594440
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO4FY
UT WOS:000341292900003
DA 2024-07-18
ER

PT J
AU She, J
   Crowcroft, J
   Fu, H
   Li, F
AF She, James
   Crowcroft, Jon
   Fu, Hao
   Li, Flora
TI Convergence of Interactive Displays with Smart Mobile Devices for
   Effective Advertising: A Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Human Factors; Interactive display; advertising; smart mobile
   device; human factors; marketing
ID PHONE
AB The trend of replacing public static signages with digital displays creates opportunities for interactive display systems, which can be used in collaborative workspaces, social gaming platforms and advertising. Based on marketing communication concepts and existing models for consumer behavior, three stages, namely attraction, interaction and conation, are defined in this article to analyze the effectiveness of interactive display advertising. By reviewing various methods and strategies employed by existing systems with attraction, interaction and conation stages, this article concludes that smart mobile devices should be integrated as a component to increase the effectiveness of interactive displays as advertising tools. Future research challenges related to this topic are also discussed.
C1 [She, James] Univ Cambridge, Comp Lab, Cambridge CB2 1TN, England.
   [She, James; Fu, Hao] Hong Kong Univ Sci & Technol, Kowloon, Hong Kong, Peoples R China.
   [Crowcroft, Jon] Univ Cambridge, Comp Lab, Cambridge CB2 1TN, England.
   [Li, Flora] Univ Cambridge, Elect Engn Div, Cambridge CB2 1TN, England.
C3 University of Cambridge; Hong Kong University of Science & Technology;
   University of Cambridge; University of Cambridge
RP She, J (corresponding author), Univ Cambridge, Comp Lab, Cambridge CB2 1TN, England.
EM james.she@cl.cam.ac.uk; jon.crowcroft@cl.cam.ac.uk; hfu@ust.hk;
   flora.li@eng.cam.ac.uk
OI Crowcroft, Jon/0000-0002-7013-0121
CR Agamanolis S, 2003, KIS CO SUP COOP WORK, V2, P309
   [Anonymous], P MOBILEHCI 10
   [Anonymous], 2010, P 18 ACM INT C MULTI
   Ballagas R, 2006, IEEE PERVAS COMPUT, V5, P70, DOI 10.1109/MPRV.2006.18
   Ballagas R., 2005, CHI 05 CHI 05 EXTEND, P1200
   Barkhuus L, 2003, LECT NOTES COMPUT SC, V2864, P149
   Brignull Harry, 2003, Proc. Interact, V53, P17
   Cardoso JCS, 2009, LECT NOTES COMPUT SC, V5872, P118, DOI 10.1007/978-3-642-05290-3_21
   Chehimi F., 2009, ADJ P 11 INT C UB CO
   CHEVERST K, 2005, P WORKSH PERV MOB IN, P43
   Cheverst Keith., 2005, P 7 INT C HUMAN COMP, P47
   Cuypers T., 2009, P IEEE INT WORKSH PR, P61
   Dachselt R., 2008, GI Jahrestagung, P272
   Davies N, 2009, MOBISYS'09: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P151
   Ding G, 2004, SECOND IEEE ANNUAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS WORKSHOPS, PROCEEDINGS, P104, DOI 10.1109/PERCOMW.2004.1276914
   Echtler F, 2009, PERS UBIQUIT COMPUT, V13, P609, DOI 10.1007/s00779-009-0246-3
   Erbad A, 2008, INT CONF PERVAS COMP, P509, DOI 10.1109/PERCOM.2008.109
   Feiner S., 2010, PUTTING SURFACE CONT
   Finke M., 2008, Proceedings of the 3rd International Conference on Digital Interactive Media in Entertainment and Arts, P26, DOI DOI 10.1145/1413634.1413644
   Hanqing Ruan, 2010, 2010 Proceedings of 7th International Conference on Ubiquitous Intelligence & Computing and 7th International Conference on Autonomic & Trusted Computing (UIC/ATC 2010), P262, DOI 10.1109/UIC-ATC.2010.36
   Hardy R, 2009, FIRST INTERNATIONAL WORKSHOP ON NEAR FIELD COMMUNICATION, PROCEEDINGS, P36, DOI 10.1109/NFC.2009.10
   Hogg M. K., 2004, BLACKWELL ENCY MANAG
   Hosio S., 2010, P 9 INT C MOB UB MUL, P1
   Huang EM, 2008, LECT NOTES COMPUT SC, V5013, P228, DOI 10.1007/978-3-540-79576-6_14
   Kaviani N., 2009, P 1 INT C INTERNET M, P129, DOI DOI 10.1145/1734605.1734637
   Kern D., 2008, CHI 2008, P3363
   Leikas J, 2006, PERCOM 2006: FOURTH ANNUAL IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS, PROCEEDINGS, P66
   Lorenz A., 2010, P 9 INT C MOB UB MUL, P15
   Majoe D., 2007, PERVASIVE COMPUTING, P699
   Michelis D, 2011, INT J HUM-COMPUT INT, V27, P562, DOI 10.1080/10447318.2011.555299
   Microsoft, 2011, MICR SURF PIXELSENSE
   MITCHELL K, 2006, P WORKSH PERV DISPL
   Müller J, 2009, LECT NOTES COMPUT SC, V5538, P17, DOI 10.1007/978-3-642-01516-8_3
   Muller J., 2008, Proceedings of the 5th Nordic Conference on Human-computer Interaction: Building Bridges, P308
   Muller J., 2007, ADJ P 9 INT C UB COM
   Nancel M., 2011, P SIGCHI C HUM FACT, DOI [DOI 10.1145/1978942.1978969, 10.1145/1978942.1978969]
   Nicholas Katzakis, 2011, P 74 HIS SIGVR WORKS
   Payne T, 2006, FRONT ARTIF INTEL AP, V141, P285
   Pears N, 2009, IEEE PERVAS COMPUT, V8, P14, DOI 10.1109/MPRV.2009.35
   Pering T, 2005, COMMUN ACM, V48, P53, DOI 10.1145/1081992.1082020
   Ray M.L., 1973, NEW MODELS MASS COMM, P146
   RPA, 2011, MOB MOUS IPHONE IPOD
   Scheible J., 2005, MULTIMEDIA 05, P199, DOI DOI 10.1145/1101149.1101178
   Scheible Jurgen., 2008, P 16 ACM INT C MULTI, P957, DOI DOI 10.1145/1459359.1459532
   Schmidt Dominik., 2010, Proceedings of the 23nd annual ACM symposium on User interface software and technology-UIST '10, P13, DOI [10.1145/1866029.1866034, DOI 10.1145/1866029.1866034]
   Seewoonauth K., 2009, P MOBILEHCI 09, P1
   Shirazi A. S., 2009, MOBILEHCI, P93
   Shirazi Alireza Sahami, 2009, P 11 INT C HUM COMP, P1
   Suomela R, 2004, LECT NOTES COMPUT SC, V3166, P308
   TERRENGHI L, 2009, PERS UBIQUIT COMPUT, V13, P583, DOI DOI 10.1007/s00779-009-0244-5
   Tesco, 2011, QR COD TESC STOR CON
   Tuulos VH, 2007, LECT NOTES COMPUT SC, V4480, P37
   Vajk T, 2008, INT J COMPUT GAMES T, V2008, DOI 10.1155/2008/539078
   Wilson Andrew D., 2007, Proceedings Graphics Interface 2007, P119, DOI 10.1145/1268517.1268539
NR 54
TC 23
Z9 26
U1 1
U2 39
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2014
VL 10
IS 2
AR 17
DI 10.1145/2557450
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB6DY
UT WOS:000331879000002
DA 2024-07-18
ER

PT J
AU Carbunar, B
   Potharaju, R
   Pearce, M
   Vasudevan, V
   Needham, M
AF Carbunar, Bogdan
   Potharaju, Rahul
   Pearce, Michael
   Vasudevan, Venugopal
   Needham, Michael
TI A Framework for Network Aware Caching for Video on Demand Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Measurement; Caching; video on demand; content distribution
   networks
ID PLACEMENT
AB Video on Demand (VoD) services allow users to select and locally consume remotely stored content. We investigate the use of caching to solve the scalability issues of several existing VoD providers. We propose metrics and goals that define the requirements of a caching framework for CDNs of VoD systems. Using data logs collected from Motorola equipment from Comcast VoD deployments we show that several classic caching solutions do not satisfy the proposed goals. We address this issue by developing novel techniques for predicting future values of several metrics of interest. We rely on computed predictions to define the penalty imposed on the system, both network and caching sites, when not storing individual items. We use item penalties to devise novel caching and static content placement strategies. We use the previously mentioned data logs to validate our solutions and show that they satisfy all the defined goals.
C1 [Carbunar, Bogdan] Florida Int Univ, Miami, FL 33199 USA.
   [Potharaju, Rahul] Purdue Univ, W Lafayette, IN 47907 USA.
C3 State University System of Florida; Florida International University;
   Purdue University System; Purdue University
RP Carbunar, B (corresponding author), Florida Int Univ, Miami, FL 33199 USA.
EM carbunar@cs.fiu.edu; rpothara@purdue.edu;
   Michael.Pearce@motorolasolutions.com; venu.vasudevan@motorola.com;
   michael.needham@motorola.com
CR AMBLE M., 2011, P ANN JOINT C IEEE C
   [Anonymous], 2002, WSNA, DOI DOI 10.1145/570738.570750
   [Anonymous], 2009, 5740 IETF RFC
   BORST S.C., 2010, P ANN JOINT C IEEE C
   Cao P., 1997, P USENIX S INT TECHN
   CARBUNAR B., 2012, P 13 INT S WORLD WIR
   Dabek F, 2004, ACM SIGCOMM COMP COM, V34, P15, DOI 10.1145/1030194.1015471
   DAHLIN M, 1994, P 1 USENIX S OP SYST
   Kangasharju J, 2002, COMPUT COMMUN, V25, P376, DOI 10.1016/S0140-3664(01)00409-1
   KARLSSON M, 2002, P 7 INT WEB CONT CAC
   Laoutaris N, 2007, IEEE T PARALL DISTR, V18, P1361, DOI 10.1109/TPDS.2007.1076
   LEFF A, 1993, IEEE T PARALL DISTR, V4, P1185, DOI 10.1109/71.250099
   Leong B, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, P71, DOI 10.1109/ICNP.2007.4375838
   MINER M., 2012, NEW CABLE DEAL CHICA
   MOTOROLA, 2012, B 1 VID SERV
   MOTOROLA, 2012, B 3 MOT EXP DEM PLAT
   PARK SH, 2001, P 15 INT PAR DISTR P
   Qiu LL, 2001, IEEE INFOCOM SER, P1587, DOI 10.1109/INFCOM.2001.916655
   Ramesh S, 2001, IEEE INFOCOM SER, P85, DOI 10.1109/INFCOM.2001.916690
   Ratnasamy S, 2001, ACM SIGCOMM COMP COM, V31, P161, DOI 10.1145/964723.383072
   Rowstron A., 2001, Proceedings of the Middleware 2001, P329, DOI DOI 10.1007/3-540-45518-3_18
   SEN S., 1999, P ANN JOINT C IEEE C
   SORENTO, SOL ARCH CABL VID ON
   Stoica I, 2003, IEEE ACM T NETWORK, V11, P17, DOI 10.1109/TNET.2002.808407
   Thatcher J., 2009, Solid State Storage Initiative
   Wang Bing, 2004, IEEE T MULTIMEDIA
   Wauters T, 2006, COMPUT COMMUN, V29, P3313, DOI 10.1016/j.comcom.2006.05.008
   WU K.-L., 2001, P INT WORLD WID WEB
   WUJUAN L., 2006, COMPUT COMMUN, V29, P18
   Zaman S, 2011, IEEE T PARALL DISTR, V22, P1455, DOI 10.1109/TPDS.2011.27
   Zhao B, 2001, UCBCSD011141
   Zhuo JC, 2008, IEEE T CONSUM ELECTR, V54, P1947, DOI 10.1109/TCE.2008.4711257
NR 32
TC 3
Z9 3
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2013
VL 9
IS 4
AR 30
DI 10.1145/2501643.2501652
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 206EW
UT WOS:000323501800008
DA 2024-07-18
ER

PT J
AU Khodabakhshi, N
   Hefeeda, M
AF Khodabakhshi, Naghmeh
   Hefeeda, Mohamed
TI Spider: A System for Finding 3D Video Copies
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Video copy detection; 3D video; video
   fingerprinting; depth features; visual features
AB This article presents a novel content-based copy detection system for 3D videos. The system creates compact and robust depth and visual signatures from the 3D videos. Then, signature of a query video is compared against an indexed database of reference videos' signatures. The system returns a score, using both spatial and temporal characteristics of videos, indicating whether the query video matches any video in the reference video database, and in case of matching, which portion of the reference video matches the query video. Analysis shows that the system is efficient, both computationally and storage-wise. The system can be used, for example, by video content owners, video hosting sites, and third-party companies to find illegally copied 3D videos. We implemented Spider, a complete realization of the proposed system, and conducted rigorous experiments on it. Our experimental results show that the proposed system can achieve high accuracy in terms of precision and recall even if the 3D videos are subjected to several transformations at the same time. For example, the proposed system yields 100% precision and recall when copied videos are parts of original videos, and more than 90% precision and recall when copied videos are subjected to different individual transformations.
C1 [Khodabakhshi, Naghmeh] Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
   [Hefeeda, Mohamed] Qatar Fdn, Qatar Comp Res Inst, Doha, Qatar.
C3 Simon Fraser University; Qatar Foundation (QF); Hamad Bin Khalifa
   University-Qatar; Qatar Computing Research Institute
RP Khodabakhshi, N (corresponding author), Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
EM nkhodaba@sfu.ca; mhefeeda@qf.org.qa
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada and in part by the British Columbia
   Innovation Council.
CR [Anonymous], 2004, P 20 ACM S COMP
   [Anonymous], KLUWER INT SERIES EN
   [Anonymous], P BRIT MACH VIS C BM
   [Anonymous], FLANN FAST LIB APPR
   [Anonymous], P ACM MULT SYST C MM
   [Anonymous], P ACM MULT SYST C MM
   [Anonymous], M15377 ISOIECJTC1SC2
   [Anonymous], P IEEE INT C INF AUT
   [Anonymous], 2011, WACV
   [Anonymous], TEXTS COMPUTER SCI
   [Anonymous], MSR 3D VIDEO
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Chen WY, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P1315
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Gong YH, 2000, PROC CVPR IEEE, P174, DOI 10.1109/CVPR.2000.854772
   Hampapur A, 2002, P SOC PHOTO-OPT INS, V4676, P194
   Kahng AB, 1998, 1998 DESIGN AUTOMATION CONFERENCE, PROCEEDINGS, P776, DOI 10.1109/DAC.1998.724576
   Kauff P, 2007, SIGNAL PROCESS-IMAGE, V22, P217, DOI 10.1016/j.image.2006.11.013
   Koz A, 2010, IEEE T IMAGE PROCESS, V19, P1785, DOI 10.1109/TIP.2010.2045024
   Li Zhe-yu, 2010, Microcomputer Information, P1
   Liu Z., 2010, Proc. of the international conference on Multimedia information retrieval (MIR'10), P119
   Liu Z, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1487
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Merkle P, 2010, IEEE T CONSUM ELECTR, V56, P946, DOI 10.1109/TCE.2010.5506024
   Ramachandra Vikas, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P81, DOI 10.1109/3DTV.2008.4547813
   Roth Gerhard, 2010, Proceedings of the 2010 Seventh Canadian Conference on Computer and Robot Vision (CRV 2010), P63, DOI 10.1109/CRV.2010.15
   Silpa-Anan C., 2008, P IEEE C COMPUTER VI, P1
   Smolic A, 2007, IEEE T CIRC SYST VID, V17, P1606, DOI 10.1109/TCSVT.2007.909972
   Tasdemir Kasim, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3134, DOI 10.1109/ICPR.2010.767
   Wang L, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P798
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 31
TC 3
Z9 3
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2013
VL 9
IS 1
AR 7
DI 10.1145/2422956.2422963
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 097EO
UT WOS:000315457000007
DA 2024-07-18
ER

PT J
AU Sachan, A
   Emmanuel, S
   Kankanhalli, MS
AF Sachan, Amit
   Emmanuel, Sabu
   Kankanhalli, Mohan S.
TI Aggregate Licenses Validation for Digital Rights Violation Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Security; Verification; Digital Rights Management (DRM);
   license organization; license validation
ID FREQUENT PATTERNS; MANAGEMENT
AB Digital Rights Management (DRM) is the term associated with the set of technologies to prevent illegal multimedia content distribution and consumption. DRM systems generally involve multiple parties such as owner, distributors, and consumers. The owner issues redistribution licenses to its distributors. The distributors in turn using their received redistribution licenses can generate and issue new redistribution licenses to other distributors and new usage licenses to consumers. As a part of rights violation detection, these newly generated licenses must be validated by a validation authority against the redistribution license used to generate them. The validation of these newly generated licenses becomes quite complex when there exist multiple redistribution licenses for a media with the distributors. In such cases, the validation process requires validation using an exponential number (to the number of redistribution licenses) of validation inequalities and each validation inequality may contain up to an exponential number of summation terms. This makes the validation process computationally intensive and necessitates to do the validation efficiently. To overcome this, we propose validation tree, a prefix-tree-based validation method to do the validation efficiently. Theoretical analysis and experimental results show that our proposed technique reduces the validation time significantly.
C1 [Sachan, Amit; Emmanuel, Sabu] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
   [Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
C3 Nanyang Technological University; National University of Singapore
RP Sachan, A (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
EM amit0009@ntu.edu.sg
RI Emmanuel, Sabu/A-3690-2011; Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
FU Agency for Science, Technology and Research (A-STAR), Singapore
   [0721010022]
FX Thanks to the Agency for Science, Technology and Research (A-STAR),
   Singapore for supporting this work under the project "Digital Rights
   Violation Detection for Digital Asset Management" (project no.
   0721010022).
CR [Anonymous], P 20 2NT C VER 6ARG
   [Anonymous], 1998, SORTING SEARCHING
   ARNAB A, 2005, ACM WORKS DIG RIGHTS, P00001
   Burdick D, 2005, IEEE T KNOWL DATA EN, V17, P1490, DOI 10.1109/TKDE.2005.183
   Chuang KT, 2008, VLDB J, V17, P1321, DOI 10.1007/s00778-007-0078-6
   Chui CK, 2007, LECT NOTES COMPUT SC, V4426, P47
   Eavis T, 2009, LECT NOTES COMPUT SC, V5463, P369, DOI 10.1007/978-3-642-00887-0_33
   García R, 2010, INFORM SYST, V35, P483, DOI 10.1016/j.is.2008.12.001
   Han JW, 2004, DATA MIN KNOWL DISC, V8, P53, DOI 10.1023/B:DAMI.0000005258.31418.83
   Hwang SO, 2004, J SYST SOFTWARE, V73, P533, DOI 10.1016/j.jss.2003.10.016
   Iannella R, 2008, STUD COMPUT INTELL, V120, P327
   Jamkhedkar PA, 2009, COMPUT ELECTR ENG, V35, P376, DOI 10.1016/j.compeleceng.2008.06.012
   Liu GM, 2004, DATA MIN KNOWL DISC, V9, P249, DOI 10.1023/B:DAMI.0000041128.59011.53
   O'Driscoll G., 2008, Next Generation IPTV Services and Technologies
   Rojas C, 2007, PROCEEDINGS OF THE IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, P221, DOI 10.1109/WI.2007.114
   Sachan A, 2010, LECT NOTES COMPUT SC, V5982, P313
   Sans T, 2007, INT FED INFO PROC, V232, P349
   van der Veen JAA, 2005, J OPER RES SOC, V56, P757, DOI 10.1057/palgrave.jors.2601879
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 24
TC 0
Z9 0
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 2
SU S
AR 37
DI 10.1145/2344436.2344443
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 011KE
UT WOS:000309162800007
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, X
   Ward, TE
   McLoone, S
AF Zhang, Xin
   Ward, Tomas E.
   McLoone, Seamus
TI An Information-Based Dynamic Extrapolation Model for Networked Virtual
   Environments
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurements; Performance; Theory; Consistency; collaborative virtual
   environments; distributed interactive applications; distributed
   interactive simulation; networked multi-player computer games; networked
   virtual environments; information management techniques
ID MUTUAL INFORMATION
AB Various Information Management techniques have been developed to help maintain a consistent shared virtual world in a Networked Virtual Environment. However, such techniques have to be carefully adapted to the application state dynamics and the underlying network. This work presents a novel framework that minimizes inconsistency by optimizing bandwidth usage to deliver useful information. This framework measures the state evolution using an information model and dynamically switches extrapolation models and the packet rate to make the most information-efficient usage of the available bandwidth. The results shown demonstrate that this approach can help optimize consistency under constrained and time-varying network conditions.
C1 [Zhang, Xin; Ward, Tomas E.; McLoone, Seamus] Natl Univ Ireland Maynooth, Dept Elect Engn, Maynooth, Kildare, Ireland.
C3 Maynooth University
RP Zhang, X (corresponding author), Natl Univ Ireland Maynooth, Dept Elect Engn, Maynooth, Kildare, Ireland.
EM xzhang@eeng.nuim.ie; tomas.ward@eeng.nuim.ie;
   seamus.mcloone@eeng.nuim.ie
RI Ward, Tomas/AEN-2410-2022; Ward, Tomas E/G-9221-2011
FU Irish Research Council for Science, Engineering, and Technology
   (IRCSET); National Development Plan
FX This work is supported by the Irish Research Council for Science,
   Engineering, and Technology (IRCSET) and funded by the National
   Development Plan.
CR [Anonymous], 2006, Elements of Information Theory
   [Anonymous], 15162000 IEEE
   [Anonymous], 1998, 12781A1998 IEEE
   Armitage G, 2003, ICON 2003: 11TH IEEE INTERNATIONAL CONFERENCE ON NETWORKS, P137
   CALVIN J, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P450, DOI 10.1109/VRAIS.1993.380745
   Capps M, 2000, IEEE COMPUT GRAPH, V20, P12, DOI 10.1109/38.865873
   Capps M, 1997, SIXTH IEEE WORKSHOPS ON ENABLING TECHNOLOGIES: INFRASTRUCTURE FOR COLLABORATIVE ENTERPRISES, PROCEEDINGS, P205, DOI 10.1109/ENABL.1997.630815
   Claypool M, 2005, COMPUT NETW, V49, P52, DOI 10.1016/j.comnet.2005.04.008
   DELANEY D, 2003, P 21 IASTED INT MULT
   Dellaney D, 2006, PRESENCE-VIRTUAL AUG, V15, P218, DOI 10.1162/pres.2006.15.2.218
   Farber J., 2002, Proceedings of the 1st workshop on Network and system support for games, P53, DOI [10.1145/566500.566508, DOI 10.1145/566500.566508]
   Feng WC, 2005, IEEE ACM T NETWORK, V13, P488, DOI 10.1109/TNET.2005.850221
   Ferretti S., 2005, P 2005 ACM SIGCHI IN, P405
   FRASER AM, 1986, PHYS REV A, V33, P1134, DOI 10.1103/PhysRevA.33.1134
   Frecon E., 1998, Distributed Systems Engineering, V5, P91, DOI 10.1088/0967-1846/5/3/002
   Hanawa D, 2006, 2006 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P107, DOI 10.1109/CW.2006.10
   Harcsik Szabolcs., 2007, NETGAMES 07, P129
   Kushner D, 2002, IEEE SPECTRUM, V39, P42, DOI 10.1109/MSPEC.2002.1021943
   LEE BS, 2000, INT J SIMULATION SYS, V1, P21
   MARSHALL D., 2008, ACM T MULTIM COMPUT, V6, P4
   Marshall D, 2006, CGAMES'2006: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON COMPUTER GAMES: ARTIFICIAL INTELLIGENCE AND MOBILE SYSTEMS, P88
   McCoy A, 2005, SIMULATION IN WIDER EUROPE, P727
   McCoy A, 2007, ACM T MODEL COMPUT S, V17, DOI 10.1145/1276927.1276929
   MCCOY AB, 2007, THESIS NATL U IRELAN
   MILLER DC, 1995, P IEEE, V83, P1114, DOI 10.1109/5.400452
   MOON YI, 1995, PHYS REV E, V52, P2318, DOI 10.1103/PhysRevE.52.2318
   Pantel Lothar., 2002, NETGAMES 02 P 1 WORK, P79
   Pantel Lothar., 2002, NOSSDAV 02 P 12 INT, P23
   Roehle B, 1997, IEEE SPECTRUM, V34, P32, DOI 10.1109/6.576006
   Roulston MS, 1999, PHYSICA D, V125, P285, DOI 10.1016/S0167-2789(98)00269-3
   SINGHAL S, 1996, THESIS STANFORD U ST
   Singhal S., 1999, Networked Virtual Environments
   Steuer R, 2002, BIOINFORMATICS, V18, pS231, DOI 10.1093/bioinformatics/18.suppl_2.S231
   Zhang XQ, 2008, PROC CVPR IEEE, P1317, DOI 10.1109/CVPR.2008.4587512
   Zhang X, 2009, IEEE ACM DIS SIM, P121, DOI 10.1109/DS-RT.2009.23
NR 35
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2012
VL 8
IS 3
AR 27
DI 10.1145/2240136.2240140
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 986AM
UT WOS:000307311700004
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, XB
   Yan, SC
   Chua, TS
   Jin, H
AF Liu, Xiaobai
   Yan, Shuicheng
   Chua, Tat-Seng
   Jin, Hai
TI Image Label Completion by Pursuing Contextual Decomposability
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Image label completion; label ranking; Multilabel
   classification; image annotation
ID SUPPORT; SCENE
AB This article investigates how to automatically complete the missing labels for the partially annotated images, without image segmentation. The label completion procedure is formulated as a nonnegative data factorization problem, to decompose the global image representations that are used for describing the entire images, for instance, various image feature descriptors, into their corresponding label representations, that are used for describing the local semantic regions within images. The solution provided in this work is motivated by following observations. First, label representations of the regions with the same label often share certain commonness, yet may be essentially different due to the large intraclass variations. Thus, each label or concept should be represented by using a subspace spanned by an ensemble of basis, instead of a single one, to characterize the intralabel diversities. Second, the subspaces for different labels are different from each other. Third, while two images are similar with each other, the corresponding label representations should be similar. We formulate this cross-image context as well as the given partial label annotations in the framework of nonnegative data factorization and then propose an efficient multiplicative nonnegative update rules to alternately optimize the subspaces and the reconstruction coefficients. We also provide the theoretic proof of algorithmic convergence and correctness. Extensive experiments over several challenging image datasets clearly demonstrate the effectiveness of our proposed solution in boosting the quality of image label completion and image annotation accuracy. Based on the same formulation, we further develop a label ranking algorithms, to refine the noised image labels without any manual supervision. We compare the proposed label ranking algorithm with the state-of-the-arts over the popular evaluation databases and achieve encouragingly improvements.
C1 [Liu, Xiaobai; Jin, Hai] Huazhong Univ Sci & Technol, SCTS, Wuhan 430074, Peoples R China.
   [Liu, Xiaobai; Jin, Hai] Huazhong Univ Sci & Technol, CGCL, Wuhan 430074, Peoples R China.
   [Yan, Shuicheng; Chua, Tat-Seng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; National University of Singapore
RP Liu, XB (corresponding author), Huazhong Univ Sci & Technol, SCTS, Wuhan 430074, Peoples R China.
EM xbliu.lhi@gmail.com
RI liu, xiaobai/J-4120-2014; Yan, Shuicheng/HCI-1431-2022
FU CSIDM [CSIDM-200803]; National Research Foundation (NRF); National High
   Technology Research and Development Program of China [2006AA01A115]
FX This work is supported by the CSIDM Project No. CSIDM-200803 partially
   funded by a grant from the National Research Foundation (NRF)
   administered by the Media Development Authority (MDA) of Singapore. This
   work is also partially supported by the National High Technology
   Research and Development Program of China under grant No. 2006AA01A115.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], 2007, ACM MULTIMEDIA, DOI DOI 10.1145/1291233.1291379
   [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], EUR C COMP VIS ECCV
   [Anonymous], ADV NEURAL INFORM PR
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   DALAL N., 2009, P IEEE C COMP VIS PA
   Fan RE, 2005, J MACH LEARN RES, V6, P1889
   Godbole S, 2004, LECT NOTES ARTIF INT, V3056, P22
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Hyvarinen A., 1999, Neural Computing Surveys, V2
   Kang F., 2006, CVPR, V2, P1719
   KUHN H., 1951, P 2 BERK S
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   LIU D., 2010, P INT WORLD WID WEB, P180
   Liu XB, 2009, IEEE DATA MINING, P307, DOI 10.1109/ICDM.2009.18
   Liu XB, 2010, IEEE T IMAGE PROCESS, V19, P1126, DOI 10.1109/TIP.2009.2039050
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   RATTENBURY T., 2007, P ACM SPEC INT GROUP
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Schölkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tu Z., 2005, INT J COMPUT VISION
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   WANG C., 2009, P IEEE C COMP VIS PA
   WANG Z., 2010, P ACM C IM VID RETR
   Xu X, 2004, LECT NOTES ARTIF INT, V3056, P272
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhou DY, 2004, ADV NEUR IN, V16, P169
   Zhou Z.-H., 2007, P ADV NEUR INF PROC, P1609, DOI DOI 10.1016/J.PATCOG.2006.12.019
NR 38
TC 7
Z9 8
U1 1
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2012
VL 8
IS 2
AR 21
DI 10.1145/2168996.2169001
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 945BX
UT WOS:000304248900005
DA 2024-07-18
ER

PT J
AU Zhu, JK
   Hoi, SCH
   Lyu, MR
   Yan, SC
AF Zhu, Jianke
   Hoi, Steven C. H.
   Lyu, Michael R.
   Yan, Shuicheng
TI Near-Duplicate Keyframe Retrieval by Semi-Supervised Learning and
   Nonrigid Image Matching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Performance; Experimentations; Near-duplicate keyframe;
   image copy detection; nonrigid image matching; semi-supervised learning
ID RECOGNITION; DISTANCE
AB ear-duplicate keyframe (NDK) retrieval techniques are critical to many real-world multimedia applications. Over the last few years, we have witnessed a surge of attention on studying near-duplicate image/keyframe retrieval in the multimedia community. To facilitate an effective approach to NDK retrieval on large-scale data, we suggest an effective Multi-Level Ranking (MLR) scheme that effectively retrieves NDKs in a coarse-to-fine manner. One key stage of the MLR ranking scheme is how to learn an effective ranking function with extremely small training examples in a near-duplicate detection task. To attack this challenge, we employ a semi-supervised learning method, semi-supervised support vector machines, which is able to significantly improve the retrieval performance by exploiting unlabeled data. Another key stage of the MLR scheme is to perform a fine matching among a subset of keyframe candidates retrieved from the previous coarse ranking stage. In contrast to previous approaches based on either simple heuristics or rigid matching models, we propose a novel Nonrigid Image Matching (NIM) approach to tackle near-duplicate keyframe retrieval from real-world video corpora in order to conduct an effective fine matching. Compared with the conventional methods, the proposed NIM approach can recover explicit mapping between two near-duplicate images with a few deformation parameters and find out the correct correspondences from noisy data simultaneously. To evaluate the effectiveness of our proposed approach, we performed extensive experiments on two benchmark testbeds extracted from the TRECVID2003 and TRECVID2004 corpora. The promising results indicate that our proposed method is more effective than other state-of-the-art approaches for near-duplicate keyframe retrieval.
C1 [Zhu, Jianke] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Hoi, Steven C. H.] Nanyang Technol Univ, Sch Comp Engn, Div Informat Syst, Singapore 639798, Singapore.
   [Lyu, Michael R.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, ECE Dept, Singapore 117576, Singapore.
C3 Zhejiang University; Nanyang Technological University; Chinese
   University of Hong Kong; National University of Singapore
RP Zhu, JK (corresponding author), Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
EM jianke.zhu@gmail.com
RI HOI, Steven C. H./A-3736-2011; Yan, Shuicheng/HCI-1431-2022
OI Hoi, Steven/0000-0002-4584-3453
FU Research Grants Council [CUHK4154/09E]; Singapore MOE [RG67/07]; NRF/IDM
   Program [NRF2008IDMIDM004-029]
FX The work was supported in part by three grants: the Research Grants
   Council General Research Fund (CUHK4154/09E), the Singapore MOE AcRF
   Tier-1 research grant (RG67/07), and NRF/IDM Program under research
   Grant NRF2008IDMIDM004-029.
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P 14 ACM INT C MULT
   [Anonymous], P ACM MULT
   [Anonymous], 1998, STAT LEARNING THEORY
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Boyd S., 2004, CONVEX OPTIMIZATION
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221
   CHUM O, 2007, P 6 ACM INT C IM VID, P549
   Everingham Mark, 2007, PASCAL VISUAL OBJECT
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   HOI CH, 2003, P INT C IM VID RETR, P373
   Hoi SCH, 2008, IEEE T MULTIMEDIA, V10, P607, DOI 10.1109/TMM.2008.921735
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Ke Y., 2004, ACM MULTIMEDIA 04, P869
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pilet J, 2008, INT J COMPUT VISION, V76, P109, DOI 10.1007/s11263-006-0017-9
   Qamra A, 2005, IEEE T PATTERN ANAL, V27, P379, DOI 10.1109/TPAMI.2005.54
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sindhwani V., 2005, ICML, V2005, P74
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Wu A. G., 2007, P ACM MM, P218
   Wu X., 2007, ACM MULTIMEDIA 07, P168
   Wu XB, 2007, PROCEEDINGS OF THE 2007 INTERNATIONAL CONFERENCE ON AGRICULTURE ENGINEERING, P162
   XU D, 2008, P C COMP VIS PATT RE
   Xu Z., 2007, Advances in Neural Information Processing Systems, P1641
   ZHANG D.-Q., 2004, PROC ACM INT C MULTI, P877, DOI DOI 10.1145/1027527.1027730
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhao WL, 2007, IEEE T MULTIMEDIA, V9, P1037, DOI 10.1109/TMM.2007.898928
   Zhao WL, 2006, LECT NOTES COMPUT SC, V4071, P72
   ZHU J, 2008, P EUR C COMP VIS, V3, P766
   Zhu J., 2007, 2007 IEEE C COMP VIS, P1
   Zhu JK, 2008, IEEE T MULTIMEDIA, V10, P86, DOI 10.1109/TMM.2007.911245
   Zhu J, 2009, IEEE T PATTERN ANAL, V31, P1210, DOI 10.1109/TPAMI.2008.151
   Zhu Jianke., 2008, P 16 ACM INT C MULTI, P41
   Zhu X., 2005, Time-sensitive Dirichlet process mixture models
NR 43
TC 5
Z9 5
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2011
VL 7
IS 1
AR 4
DI 10.1145/1870121.1870125
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 712GI
UT WOS:000286653800004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hefeeda, M
   Mokhtarian, K
AF Hefeeda, Mohamed
   Mokhtarian, Kianoosh
TI Authentication Schemes for Multimedia Streams: Quantitative Analysis and
   Comparison
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Multimedia authentication; authentication schemes; scalable
   coding; multimedia security; secure streaming; multimedia streaming
ID SECURITY
AB With the rapid increase in the demand for multimedia services, securing the delivery of multimedia content has become an important issue. Accordingly, the problem of multimedia stream authentication has received considerable attention by previous research and various solutions have been proposed. However, these solutions have not been rigorously analyzed and contrasted to each other, and thus their relative suitability for different streaming environments is not clear. This article presents comprehensive analysis and comparison among different schemes proposed in the literature to authenticate multimedia streams. Authentication schemes for nonscalable and scalable multimedia streams are analyzed. To conduct this analysis, we define five important performance metrics, which are computation cost, communication overhead, receiver buffer size, delay, and tolerance to packet losses. We derive analytic formulas for these metrics for all considered authentication schemes to numerically analyze their performance. In addition, we implement all schemes in a simulator to study and compare their performance in different environments. The parameters for the simulator are carefully chosen to mimic realistic settings. We draw several conclusions on the advantages and disadvantages of each scheme. We extend our analysis to authentication techniques for scalable streams. We pay careful attention to the flexibility of scalable streams and analyze its impacts on the authentication schemes. Our analysis and comparison reveal the merits and shortcomings of each scheme, provide guidelines on choosing the most appropriate scheme for a given multimedia streaming application, and could stimulate designing new authentication schemes or improving existing ones. For example, our detailed analysis has led us to design a new authentication scheme that combines the best features of two previous schemes.
C1 [Hefeeda, Mohamed; Mokhtarian, Kianoosh] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Hefeeda, M (corresponding author), Simon Fraser Univ, Sch Comp Sci, 8888 Univ Dr, Burnaby, BC V5A 1S6, Canada.
EM mhefeeda@cs.sfu.ca
CR Amon P, 2007, IEEE T CIRC SYST VID, V17, P1174, DOI 10.1109/TCSVT.2007.905521
   [Anonymous], 2005, 3984 RFC IETF
   [Anonymous], 2003, NDSS
   [Anonymous], RFC1321
   [Anonymous], 2006, P 8 WORKSHOP MULTIME, DOI DOI 10.1145/1161366.1161375
   Argyroudis PG, 2004, THIRD IEEE INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS, PROCEEDINGS, P169, DOI 10.1109/NCA.2004.1347774
   Atrey PK, 2007, MULTIMED TOOLS APPL, V34, P107, DOI 10.1007/s11042-006-0074-7
   Challal Y, 2004, IEEE COMMUN SURV TUT, V6, P34, DOI 10.1109/COMST.2004.5342292
   DENG RMAD, 2000, MULTIMEDIA SYST, V11, P60
   Gennaro R, 1997, LECT NOTES COMPUT SC, V1294, P180
   Gentry C, 2005, IEEE J SEL AREA COMM, V23, P464, DOI 10.1109/JSAC.2004.839391
   *GLOB IND AN INC, 2007, VID C GLOB STRAT BUS
   Golle P., 2001, Proceedings of the 8th Annual Network and Distributed Systems Security Symposium (NDSS), P13
   Grosbois R, 2001, PROC SPIE, V4472, P95, DOI 10.1117/12.449744
   Habib A, 2005, IEEE T KNOWL DATA EN, V17, P1010, DOI 10.1109/TKDE.2005.102
   Jung Min Park, 2003, ACM Transactions on Information and Systems Security, V6, P258, DOI 10.1145/762476.762480
   KACED R, 2006, P INT C INTERNET SUR
   Kaced R., 2006, P INT C DIG TEL ICDT
   KLIMA V, 2005, 2005075 CRYPT EPRINT
   Li TY, 2005, LECT NOTES COMPUT SC, V3783, P389
   Liang CY, 2007, 2007 THIRD INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P225
   Lin CY, 2000, PROC SPIE, V3971, P140, DOI 10.1117/12.384968
   Menezes A. J., 1996, HDB APPL CRYPTOGRAPH, V1st
   MERKLE RC, 1990, LECT NOTES COMPUT SC, V435, P218, DOI 10.1007/0-387-34805-0_21
   *NIST, 1993, FED INF PROC STAND
   Park Y, 2004, LECT NOTES COMPUT SC, V3046, P799
   Peng C., 2003, P ACM INT C MULTIMED, P433
   Perrig A, 2000, P IEEE S SECUR PRIV, P56, DOI 10.1109/SECPRI.2000.848446
   Perrig A., 2001, P NETW DISTR SYST SE, P35
   RABIN MO, 1989, J ACM, V36, P335, DOI 10.1145/62044.62050
   RIVEST RL, 1978, COMMUN ACM, V21, P120, DOI [10.1145/359340.359342, 10.1145/357980.358017]
   *RNCOS, 2006, GLOB IPTV MARK AN 20
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Sullivan GJ, 2005, P IEEE, V93, P18, DOI 10.1109/JPROC.2004.839617
   SUN A, 2003, P IEEE INT C MULT EX, V2, P209
   SUN Q, 2002, P IEEE INT S CIRCUIT, V2, P440
   SUZUKI T, 2004, LECT NOTES COMPUTER, V175, P237
   Tillich S, 2004, LECT NOTES COMPUT SC, V3280, P935
   Wong CK, 1999, IEEE ACM T NETWORK, V7, P502, DOI 10.1109/90.793005
   Wu YD, 2006, IEEE T MULTIMEDIA, V8, P152, DOI 10.1109/TMM.2005.861283
   Yajnik M, 1999, IEEE INFOCOM SER, P345, DOI 10.1109/INFCOM.1999.749301
   Zhang ZS, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P784
   ZHISHOU Z, 2007, P IEEE INT C IM PROC, V6, P121
   Zhu BB, 2004, PROC SPIE, V5601, P157, DOI 10.1117/12.571869
   2006, STREAMING MEDIA IPTV
NR 47
TC 22
Z9 29
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2010
VL 6
IS 1
AR 6
DI 10.1145/1671954.1671960
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 563VL
UT WOS:000275163200006
DA 2024-07-18
ER

PT J
AU Franke, IS
   Pannasch, S
   Helmert, JR
   Rieger, R
   Groh, R
   Velichkovsky, BM
AF Franke, Ingmar S.
   Pannasch, Sebastian
   Helmert, Jens R.
   Rieger, Robert
   Groh, Rainer
   Velichkovsky, Boris M.
TI Towards attention-centered interfaces: An aesthetic evaluation of
   perspective with eye tracking
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE design; eye tracking; subjective evaluation; perspective projection;
   scene perception
AB The established method of representing three-dimensional space on a two-dimensional surface involves camera based, point of regard systems, comparable in design to the early "camera obscura". However, geometrical limitations of such models lead to distortions of perspective when projected. This research investigated the influence of single-versus multi-perspectives on aesthetic choices within one image. A clear perceptual bias towards multi-perspective images was found, additionally supported by an eye tracking study. We propose that human users are more attracted by multi-perspective images, which emphasise the "semantic foci" of the scene, than by those being synthesized statically with only one geometrical prospect.
C1 [Franke, Ingmar S.; Rieger, Robert; Groh, Rainer] Tech Univ Dresden, Dept Comp Sci, Inst Software Multimedia Technol, D-01187 Dresden, Germany.
   [Pannasch, Sebastian; Helmert, Jens R.; Velichkovsky, Boris M.] Tech Univ Dresden, Inst Psychol 3, Appl Cognit Res Unit, D-01069 Dresden, Germany.
C3 Technische Universitat Dresden; Technische Universitat Dresden
RP Franke, IS (corresponding author), Tech Univ Dresden, Dept Comp Sci, Inst Software Multimedia Technol, D-01187 Dresden, Germany.
EM if4@mail.inf.tu-dresden.de; pannasch@psychomail.tu-dresden.de;
   helmert@psychomail.tu-dresden.de; robert.rieger@mail.inf.tu-dresden.de;
   rg5@mail.inf.tu-dresden.de; velich@psychomail.tu-dresden.de
OI Helmert, Jens R./0000-0003-1000-1915
CR Agrawala M, 2000, SPRING COMP SCI, P125
   Angel E., 1997, INTERACTIVE COMPUTER
   [Anonymous], SIGGRAPH 98
   [Anonymous], 2003, Active vision: The psychology of looking and seeing
   [Anonymous], P 15 EUR WORKSH REND
   Arnheim R., 1998, The power of the center: A study of composition in the visual arts
   Dodge R, 1900, PSYCHOL REV, V7, P454, DOI 10.1037/h0067215
   Edgerton SamuelY., 1975, RENAISSANCE REDISCOV
   FOLEY JD, 1999, COMPUTER GRAPHICS  P
   FRANKE IS, 2007, IPT EGVE S IPT EGVE, P117
   FRANKE IS, 2005, NEUE MEDIEN INFORMAT, V2, P309
   GROH R, 2000, REALISMUS BILDER INT
   GROH R, 2005, INTERAKTIONSBILD  TH
   GROH R, 2006, VIRTUAL
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jokela T., 2003, CLIHC '03: Proceedings of the Latin American Conference on Human-Computer Interaction, Rio de Janeiro, Brazil, P53
   Klotz Heinrich., 1990, FILIPPO BRUNELLESCHI
   KOENDERINK JJ, 2003, LOOKING PICTURES ANT
   Levoy M., 1990, Computer Graphics, V24, P217, DOI 10.1145/91394.91449
   Palmer S., 1999, VISION SCI PHOTONS P
   Puolamaki K., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P146, DOI 10.1145/1076034.1076062
   RAUBER T, 1993, ALGORITHMEN COMPUTER
   Román A, 2004, IEEE VISUALIZATION 2004, PROCEEEDINGS, P537, DOI 10.1109/VISUAL.2004.50
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   Velichkovsky B. M., 1996, Human Factors in Computing Systems. Common Ground. CHI 96 Conference Proceedings, P496, DOI 10.1145/238386.238619
   Velichkovsky BM, 2002, MEMORY, V10, P405, DOI 10.1080/09658210244000234
   Von Helmholtz H., 1867, Handbuch Der Physiologischen Optik
   Watt A., 1993, 3D COMPUTER GRAPHICS
   Zomet A, 2003, IEEE T PATTERN ANAL, V25, P741, DOI 10.1109/TPAMI.2003.1201823
   Zorin D., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P257, DOI 10.1145/218380.218449
NR 30
TC 8
Z9 8
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2008
VL 4
IS 3
AR 18
DI 10.1145/1386109.1386111
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 351OI
UT WOS:000259433300002
DA 2024-07-18
ER

PT J
AU Jie, L
   Clark, JJ
AF Jie, Li
   Clark, James J.
TI Video game design using an eye-movement-dependent model of visual
   attention
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE design; human factors; entertainment; eye movements; eye tracking; HCI;
   visual attention; video games
ID SEARCH; INFORMATION; FIXATIONS; FEATURES; SCENES
AB Eye movements can be used to infer the allocation of covert attention. In this article, we propose to model the allocation of attention in a task-dependent manner based on different eye movement conditions, specifically fixation and pursuit. We show that the image complexity at eye fixation points during fixation, and the pursuit direction during pursuit are significant factors in attention allocation. Results of the study are applied to the design of an interactive computer game. Real-time eye movement information is taken as one of inputs for the game. The utility of such eye information for controlling game difficulty is shown.
C1 [Jie, Li; Clark, James J.] McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2A7, Canada.
C3 McGill University
RP Jie, L (corresponding author), McGill Univ, Ctr Intelligent Machines, Montreal, PQ H3A 2A7, Canada.
EM jieli@cim.mcgill.ca; clark@cim.mcgill.ca
OI Clark, James/0000-0002-4512-6171
CR [Anonymous], 2006, P 2006 ACM SIGCHI IN, DOI DOI 10.1145/1178823.1178849
   [Anonymous], [No title captured], DOI DOI 10.1007/978-1-4612-2852-3_12
   DYE M, 2004, J VIS, V4
   ELNASR MS, 2004, INT C COMP GRAPH INT
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Green CS, 2007, PSYCHOL SCI, V18, P88, DOI 10.1111/j.1467-9280.2007.01853.x
   Guo K, 2006, EXP BRAIN RES, V171, P91, DOI 10.1007/s00221-005-0248-y
   Henderson J.M., 1998, EYE GUIDANCE READING, P269, DOI DOI 10.1016/B978-008043361-5/50013-4
   Henderson JM, 1999, J EXP PSYCHOL HUMAN, V25, P210, DOI 10.1037/0096-1523.25.1.210
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   JIE L, 2005, VIS SCI SOC ANN M VS, V5
   Kayser C, 2006, VISION RES, V46, P2535, DOI 10.1016/j.visres.2006.02.003
   Land MF, 2001, VISION RES, V41, P3559, DOI 10.1016/S0042-6989(01)00102-X
   LOFTUS GR, 1985, J EXP PSYCHOL GEN, V114, P342, DOI 10.1037/0096-3445.114.3.342
   Mannan SK, 1997, SPATIAL VISION, V11, P157, DOI 10.1163/156856897X00177
   Mannan SK, 1996, SPATIAL VISION, V10, P165, DOI 10.1163/156856896X00123
   Oliva A, 2003, IEEE IMAGE PROC, P253, DOI 10.1109/icip.2003.1246946
   Parkhurst DJ, 2003, SPATIAL VISION, V16, P125, DOI 10.1163/15685680360511645
   Pomplun M, 2006, VISION RES, V46, P1886, DOI 10.1016/j.visres.2005.12.003
   Raj R, 2005, J OPT SOC AM A, V22, P2039, DOI 10.1364/JOSAA.22.002039
   Rajashekar U, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P313
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Reichle ED, 1998, PSYCHOL REV, V105, P125, DOI 10.1037/0033-295X.105.1.125
   Rueda MR, 2005, P NATL ACAD SCI USA, V102, P14931, DOI 10.1073/pnas.0506897102
   Stiefelhagen R., 2002, CHI '02 Extended Abstracts on Human Factors in Computing Systems CHI EA '02, P858, DOI DOI 10.1145/506443.506634.
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   Turano KA, 2003, VISION RES, V43, P333, DOI 10.1016/S0042-6989(02)00498-4
   Van Donkelaar P, 2002, PROG BRAIN RES, V140, P267
   Vinje WE, 2000, SCIENCE, V287, P1273, DOI 10.1126/science.287.5456.1273
   WOLFE JM, 1989, J EXP PSYCHOL HUMAN, V15, P419, DOI 10.1037/0096-1523.15.3.419
   Yarbus A. L., 1967, Eye Movements and Vision
NR 31
TC 16
Z9 16
U1 1
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2008
VL 4
IS 3
AR 22
DI 10.1145/1386109.1386115
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 351OI
UT WOS:000259433300006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cheng, B
   Stein, L
   Jin, H
   Liao, XF
   Zhang, Z
AF Cheng, Bin
   Stein, Lex
   Jin, Hai
   Liao, Xiaofei
   Zhang, Zheng
TI GridCast: Improving Peer Sharing for P2P VoD
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Measurement; Performance; Video-on-demand; peer-to-peer;
   caching; replication
AB Video-on-Demand (VoD) is a compelling application, but costly. VoD is costly due to the load it places on video source servers. Many have proposed using peer-to-peer (P2P) techniques to shift load from servers to peers. Yet, nobody has implemented and deployed a system to openly and systematically evaluate how these techniques work.
   This article describes the design, implementation and evaluation of GridCast, a real deployed P2P VoD system. GridCast has been live on CERNET since May of 2006. It provides seek, pause, and play operations, and employs peer sharing to improve system scalability. In peak months, GridCast has served videos to 23,000 unique users. From the first deployment, we have gathered information to understand the system and evaluate how to further improve peer sharing through caching and replication.
   We first show that GridCast with single video caching (SVC) can decrease load on source servers by an average of 22% from a client-server architecture. We analyze the net effect on system resources and determine that peer upload is largely idle. This leads us to changing the caching algorithm to cache multiple videos (MVC). MVC decreases source load by an average of 51% over the client-server. The improvement is greater as user load increases. This bodes well for peer-assistance at larger scales.
   A detailed analysis of MVC shows that departure misses become a major issue in a P2P VoD system with caching optimization. Motivated by this observation, we examine how to use replication to eliminate departure misses and further reduce server load. A framework for lazy replication is presented and evaluated in this article. In this framework, two predictors are plugged in to create the working replication algorithm. With these two simple predictors, lazy replication can decrease server load by 15% from MVC with only a minor increase in network traffic.
C1 [Cheng, Bin; Jin, Hai; Liao, Xiaofei] Huazhong Univ Sci & Technol, Serv Comp Technol & Syst Lab, Wuhan 430074, Peoples R China.
   [Stein, Lex; Zhang, Zheng] Microsoft Res Asia, Beijing, Peoples R China.
C3 Huazhong University of Science & Technology; Microsoft Research Asia;
   Microsoft
RP Cheng, B (corresponding author), Huazhong Univ Sci & Technol, Serv Comp Technol & Syst Lab, Wuhan 430074, Peoples R China.
EM showersky@hust.edu.cn; castein@microsoft.com; hjin@hust.edu.cn;
   xfliao@hust.edu.cn; zzhang@microsoft.com
FU National Natural Science Foundation of China (NSFC) [60703050, 60433040,
   60731160630]; Research Fund for the Doctoral Program of Higher Education
   grant [20050487040]; Wuhan Chengguang Plan grant [200850731350];
   Microsoft Research Asia
FX This work is supported by National Natural Science Foundation of China
   (NSFC) grants 60703050, 60433040 and 60731160630, Research Fund for the
   Doctoral Program of Higher Education grant 20050487040, Wuhan Chengguang
   Plan grant 200850731350, as well as grants from Microsoft Research Asia.
CR Annapureddy S, 2007, P INT WORLD WID WEB
   [Anonymous], 2003, P WORKSH EC PEER PEE
   [Anonymous], 2007, JOOST MEASUREMENT ST
   *CERNET, 2006, CHIN ED RES NETW ANN
   CHENG B, 2007, P IEEE INT C COMM
   CHENG B, 2008, P EUR PROF SOC COMP
   CHU YH, 2000, P ACM SIGMETRICS JOI
   CUI L, 2004, IEEE J SELECT AREA C
   GUO L, 2005, P INT MARK C
   GUO Y, 2003, P INT WORLD WID WEB
   Handley M, 2006, 4566 RFC
   HEI X, 2007, IEEE T MULTIMED
   HUANG C, 2007, P ACM SIGCOMM DAT CO
   HUANG Y, 2008, P ACM SIGCOMM DAT CO
   LIAO X, 2006, P ANN JOINT C IEEE C
   MOL J, 2008, P MULT COMP NETW C
   QIU D, 2004, P ACM SIGCOMM DAT CO
   Schulzrinne H., 2003, RFC 3550, DOI 10.17487/RFC3550
   Schulzrinne H., 1998, 2326 RFC
   SRIPANIDKULCHAI K, 2004, P INT MARK C
   TAI D, 2004, P IEEE INT C COMM
   TIAN J, 2007, P 6 INT WORKSH PEER
   WANG D, 2006, P IEEE INT C MULT EU
   YU HL, 2006, P EUR PROF SOC COMP
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 25
TC 15
Z9 18
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 4
AR 26
DI 10.1145/1412196.1412199
PG 31
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 376AP
UT WOS:000261155300003
DA 2024-07-18
ER

PT J
AU Dong, J
   Ota, K
   Dong, MX
AF Dong, Jiong
   Ota, Kaoru
   Dong, Mianxiong
TI Video Frame Interpolation: A Comprehensive Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video Frame Interpolation; deep learning; convolutional neural network
ID QUALITY ASSESSMENT; MOTION ESTIMATION; ENHANCEMENT; NETWORK; IOT
AB Video Frame Interpolation (VFI) is a fascinating and challenging problem in the computer vision (CV) field, aiming to generate non-existing frames between two consecutive video frames. In recent years, many algorithms based on optical flow, kernel, or phase information have been proposed. In this article, we provide a comprehensive review of recent developments in the VFI technique. We first introduce the history of VFI algorithms' development, the evaluation metrics, and publicly available datasets. We then compare each algorithm in detail, point out their advantages and disadvantages, and compare their interpolation performance and speed on different remarkable datasets. VFI technology has drawn continuous attention in the CV community, some video processing applications based on VFI are also mentioned in this survey, such as slow-motion generation, video compression, video restoration. Finally, we outline the bottleneck faced by the current video frame interpolation technology and discuss future research work.
C1 [Dong, Jiong; Ota, Kaoru; Dong, Mianxiong] Muroran Inst Technol, 27-1 Mizumoto Cho, Muroran, Hokkaido, Japan.
C3 Muroran Institute of Technology
RP Dong, MX (corresponding author), Muroran Inst Technol, 27-1 Mizumoto Cho, Muroran, Hokkaido, Japan.
EM 20096511@mmm.muroran-it.ac.jp; ota@csse.muroran-it.ac.jp;
   mx.dong@csse.muroran-it.ac.jp
RI Dong, Mianxiong/O-7489-2019; Dong, Jiong/JCE-9575-2023
OI Dong, Mianxiong/0000-0002-2788-3451; Dong, Jiong/0000-0001-6178-3318;
   Ota, Kaoru/0000-0002-3382-1652
FU JSPS KAKENHI [JP19K20250, JP20H04174, JP22K11989]; Leading Initiative
   for Excellent Young Researchers (LEADER), MEXT, Japan; JST, PRESTO,
   Japan [JPMJPR21P3]; China Scholarship Council (CSC) [202108050244]
FX This work is partially supported by JSPS KAKENHI Grant Numbers
   JP19K20250, JP20H04174, JP22K11989, Leading Initiative for Excellent
   Young Researchers (LEADER), MEXT, Japan, and JST, PRESTO Grant Number
   JPMJPR21P3, Japan. This work was also supported by the China Scholarship
   Council (CSC) under Grant 202108050244.
CR Ahn HE, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11050619
   Argaw DM, 2021, AAAI CONF ARTIF INTE, V35, P901
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Bao WB, 2018, IEEE T IMAGE PROCESS, V27, P3813, DOI 10.1109/TIP.2018.2825100
   Bégaint J, 2019, IEEE DATA COMPR CONF, P556, DOI 10.1109/DCC.2019.00068
   Brooks T, 2019, PROC CVPR IEEE, P6833, DOI 10.1109/CVPR.2019.00700
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Castagno R, 1996, IEEE T CIRC SYST VID, V6, P436, DOI 10.1109/76.538926
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen W, 2016, ADV NEUR IN, V29
   Chen ZQ, 2021, IEEE OPEN J SIGNAL P, V2, P413, DOI 10.1109/OJSP.2021.3075879
   Cheng XH, 2020, AAAI CONF ARTIF INTE, V34, P10607
   Cheng XH, 2020, IEEE T CIRC SYST VID, V30, P3968, DOI 10.1109/TCSVT.2019.2939143
   Cheng Xianhang, 2021, IEEE T PATTERN ANAL
   Choi BD, 2007, IEEE T CIRC SYST VID, V17, P407, DOI 10.1109/TCSVT.2007.893835
   Choi H, 2020, IEEE T CIRC SYST VID, V30, P1843, DOI 10.1109/TCSVT.2019.2924657
   Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663
   Choi Myungsub, 2020, CVPR, P9444
   Choi Myungsub, 2021, IEEE T PATTERN ANAL, P1
   Choi Myungsub, 2021, International Conference on Computer Vision, P13839
   Choi W, 2021, IEEE ACCESS, V9, P150470, DOI 10.1109/ACCESS.2021.3126593
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng JJ, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P193, DOI 10.1109/MIPR.2019.00042
   Ding TY, 2021, PROC CVPR IEEE, P7997, DOI 10.1109/CVPR46437.2021.00791
   Ding Xiangling, 2021, IEEE J EMERGING SELE, P1
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Dutta S, 2021, IEEE COMPUT SOC CONF, P314, DOI 10.1109/CVPRW53098.2021.00041
   Dutta Saikat, 2021, P NEURIPS 2021 WORKS
   Feuvre J L., 2014, ACM Multimedia Systems Conference, P7
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Fortun Denis, 2015, Computer Vision and Image Understanding, V134, P1, DOI 10.1016/j.cviu.2015.02.008
   Fourure D, 2017, Arxiv, DOI arXiv:1707.07958
   Gong C, 2020, IEEE INTERNET THINGS, V7, P9372, DOI 10.1109/JIOT.2020.2986015
   Gu DH, 2019, IEEE INT CON MULTI, P1768, DOI 10.1109/ICME.2019.00304
   Gui S., 2020, P IEEECVF C COMPUTER, P14004
   Guo YY, 2020, PROC CVPR IEEE, P4725, DOI 10.1109/CVPR42600.2020.00478
   Ha T, 2004, IEEE T CONSUM ELECTR, V50, P752, DOI 10.1109/TCE.2004.1309458
   Haoxian Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P474, DOI 10.1007/978-3-030-58595-2_29
   Haris M, 2020, PROC CVPR IEEE, P2856, DOI 10.1109/CVPR42600.2020.00293
   Hu MS, 2020, INT CONF ACOUST SPEE, P4347, DOI [10.1109/ICASSP40776.2020.9053223, 10.1109/icassp40776.2020.9053223]
   Huang AM, 2008, IEEE T IMAGE PROCESS, V17, P694, DOI 10.1109/TIP.2008.919360
   Huang AM, 2009, IEEE T IMAGE PROCESS, V18, P740, DOI 10.1109/TIP.2008.2010206
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Ilg E, 2018, LECT NOTES COMPUT SC, V11216, P626, DOI 10.1007/978-3-030-01258-8_38
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jeon BW, 2003, IEEE T CONSUM ELECTR, V49, P499, DOI 10.1109/TCE.2003.1233761
   Jiang B, 2021, I S BIOMED IMAGING, P334, DOI 10.1109/ISBI48211.2021.9434049
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Jin MG, 2019, PROC CVPR IEEE, P8104, DOI 10.1109/CVPR.2019.00830
   Jin X, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13020296
   Jonschkowski Rico, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P557, DOI 10.1007/978-3-030-58536-5_33
   Junheum Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P109, DOI 10.1007/978-3-030-58568-6_7
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kalluri Tarun, 2020, arXiv
   Kang SJ, 2007, IEEE T CONSUM ELECTR, V53, P1759, DOI 10.1109/TCE.2007.4429281
   Kim SY, 2020, AAAI CONF ARTIF INTE, V34, P11278
   Koren Mark, 2017, Frame interpolation using generative adversarial networks
   Kwon YH, 2021, IEEE ACCESS, V9, P32457, DOI 10.1109/ACCESS.2021.3053695
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Li E, 2020, IEEE T WIREL COMMUN, V19, P447, DOI 10.1109/TWC.2019.2946140
   Li HP, 2020, INT CONF ACOUST SPEE, P2613, DOI [10.1109/icassp40776.2020.9053987, 10.1109/ICASSP40776.2020.9053987]
   Li HP, 2019, IEEE ACCESS, V7, P118287, DOI 10.1109/ACCESS.2019.2936549
   Li H, 2018, IEEE NETWORK, V32, P96, DOI 10.1109/MNET.2018.1700202
   Li H, 2020, IEEE T CLOUD COMPUT, V8, P1264, DOI 10.1109/TCC.2017.2672554
   Liang Dong-xue, 2021, CARDIOVASC INNOV APP
   Ling Y, 2008, IEEE T CONSUM ELECTR, V54, P863, DOI 10.1109/TCE.2008.4560172
   Liu YL, 2019, AAAI CONF ARTIF INTE, P8794
   Liu Z., 2020, arXiv
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Men H, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123096
   Meyer S, 2018, PROC CVPR IEEE, P498, DOI 10.1109/CVPR.2018.00059
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Moraes T, 2020, COMP M BIO BIO E-IV, V8, P294, DOI 10.1080/21681163.2019.1683469
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Niklaus S, 2021, IEEE WINT CONF APPL, P1098, DOI 10.1109/WACV48630.2021.00114
   Niklaus S, 2020, PROC CVPR IEEE, P5436, DOI 10.1109/CVPR42600.2020.00548
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Oh J, 2021, Arxiv, DOI arXiv:2111.09985
   Paikin G, 2021, IEEE COMPUT SOC CONF, P1291, DOI 10.1109/CVPRW53098.2021.00142
   Parihar AS, 2022, VISUAL COMPUT, V38, P295, DOI 10.1007/s00371-020-02016-y
   Park J., 2021, P IEEECVF INT C COMP, P14539
   Peleg T, 2019, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2019.00250
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Poetsch Gabriel, 2020, DAIN APP APPL VIDEO
   Rakêt LL, 2012, LECT NOTES COMPUT SC, V7431, P447, DOI 10.1007/978-3-642-33179-4_43
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Reda FA, 2019, IEEE I CONF COMP VIS, P892, DOI 10.1109/ICCV.2019.00098
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sanghyun Son, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P23, DOI 10.1007/978-3-030-66823-5_2
   Santurkar S, 2018, PICT COD SYMP, P258, DOI 10.1109/PCS.2018.8456298
   Savian S., 2020, Deep Biometrics, P257
   Schatz Kara Marie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P410, DOI 10.1007/978-3-030-58583-9_25
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Shen W, 2021, IEEE T IMAGE PROCESS, V30, P277, DOI 10.1109/TIP.2020.3033617
   Shen W, 2020, PROC CVPR IEEE, P5113, DOI 10.1109/CVPR42600.2020.00516
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi ZH, 2022, IEEE T MULTIMEDIA, V24, P426, DOI 10.1109/TMM.2021.3052419
   Sim Hyeonjun, 2021, P IEEECVF INT C COMP, P14489
   SimonMeister JunhwaHur, 2018, P AAAI C ARTIFICIAL, V32
   Siyao L., 2021, P IEEECVF C COMPUTER, P6587
   Song L, 2013, INT WORK QUAL MULTIM, P34, DOI 10.1109/QoMEX.2013.6603201
   Songnan Lin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P695, DOI 10.1007/978-3-030-58598-3_41
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun N, 2019, IEEE ACCESS, V7, P186470, DOI 10.1109/ACCESS.2019.2960828
   Tao M, 2012, COMPUT GRAPH FORUM, V31, P345, DOI 10.1111/j.1467-8659.2012.03013.x
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Tran Phong, 2021, ARXIV
   Tran QN, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186245
   Tu ZG, 2019, SIGNAL PROCESS-IMAGE, V72, P9, DOI 10.1016/j.image.2018.12.002
   Tulyakov S, 2021, PROC CVPR IEEE, P16150, DOI 10.1109/CVPR46437.2021.01589
   Usman M, 2016, IEEE T MULTIMEDIA, V18, P831, DOI 10.1109/TMM.2016.2537200
   van Amersfoort J, 2019, Arxiv, DOI arXiv:1711.06045
   Vaswani A, 2023, Arxiv, DOI [arXiv:1706.03762, DOI 10.48550/ARXIV.1706.03762, 10.48550/arXiv.1706.03762]
   Wang DM, 2010, IEEE T BROADCAST, V56, P142, DOI 10.1109/TBC.2010.2043895
   Wang HY, 2021, PROC CVPR IEEE, P5459, DOI 10.1109/CVPR46437.2021.00542
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZJ, 2021, Arxiv, DOI arXiv:2101.06771
   Wang ZJ, 2021, BIODATA MIN, V14, DOI 10.1186/s13040-021-00236-z
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Wen SP, 2019, IEEE T CIRC SYST VID, V29, P2337, DOI 10.1109/TCSVT.2018.2867934
   Werlberger Manuel, 2011, Energy Minimization Methods in Computer Vision and Pattern Recognition. Proceedings 8th International Conference, EMMCVPR 2011, P273, DOI 10.1007/978-3-642-23094-3_20
   Wu CY, 2018, LECT NOTES COMPUT SC, V11212, P425, DOI 10.1007/978-3-030-01237-3_26
   Wu JY, 2016, IEEE T WIREL COMMUN, V15, P2713, DOI 10.1109/TWC.2015.2509063
   Wu XY, 2021, IEEE ACCESS, V9, P113566, DOI 10.1109/ACCESS.2021.3104526
   Wu ZT, 2020, Arxiv, DOI [arXiv:2001.11698, 10.3233/FAIA200314, DOI 10.3233/FAIA200314]
   Xiang XY, 2020, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR42600.2020.00343
   Xing JB, 2021, COMPUT VIS MEDIA, V7, P393, DOI 10.1007/s41095-021-0208-x
   Xu XY, 2019, ADV NEUR IN, V32
   Xue F., 2021, P 2021 IEEE INT C MU, P1
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Xue W, 2020, NEUROCOMPUTING, V380, P95, DOI 10.1016/j.neucom.2019.11.015
   Yan B, 2021, IEEE T BROADCAST, V67, P174, DOI 10.1109/TBC.2020.3028323
   Yang KC, 2008, IEEE T BROADCAST, V54, P680, DOI 10.1109/TBC.2008.2001243
   Yihao Liu, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P41, DOI 10.1007/978-3-030-66823-5_3
   Yu S, 2019, IEEE INT CONF COMP V, P3503, DOI 10.1109/ICCVW.2019.00434
   Yu ZF, 2013, IEEE T CIRC SYST VID, V23, P1235, DOI 10.1109/TCSVT.2013.2242631
   Yuan LZ, 2019, PROC CVPR IEEE, P12175, DOI 10.1109/CVPR.2019.01246
   Zhang HX, 2019, IEEE ACCESS, V7, P130610, DOI 10.1109/ACCESS.2019.2940510
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y., 2020, Advances in Neural Information Processing Systems, V33, P13308
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao B, 2021, Arxiv, DOI arXiv:2105.07673
   Zhao L, 2018, IEEE IMAGE PROC, P206, DOI 10.1109/ICIP.2018.8451465
   Zheng MH, 2021, Arxiv, DOI [arXiv:2011.09315, 10.48550/arXiv.2011.09315]
   Zhixiang Chi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P107, DOI 10.1007/978-3-030-58583-9_7
   Zhou CC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5445, DOI 10.1145/3474085.3475672
   Zhou JY, 2019, IEEE WIREL COMMUN LE, V8, P825, DOI 10.1109/LWC.2019.2894703
   Zhu MC, 2017, Arxiv, DOI arXiv:1710.01878
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
   Zichen Zhu, 2021, DAMON'21: Proceedings of the 17th International Workshop on Data Management on New Hardware (DaMoN 2021), DOI 10.1145/3465998.3466002
NR 163
TC 4
Z9 4
U1 10
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 78
DI 10.1145/3556544
PG 31
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300003
DA 2024-07-18
ER

PT J
AU Xu, C
   Chen, ZJ
   Mai, JJ
   Xu, XM
   He, SF
AF Xu, Cheng
   Chen, Zejun
   Mai, Jiajie
   Xu, Xuemiao
   He, Shengfeng
TI Pose- and Attribute-consistent Person Image Synthesis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image synthesis; image editing; pose transfer; generative adversarial
   network
AB Person Image Synthesis aims at transferring the appearance of the source person image into a target pose. Existing methods cannot handle large pose variations and therefore suffer from two critical problems: (1) synthesis distortion due to the entanglement of pose and appearance information among different body components and (2) failure in preserving original semantics (e.g., the same outfit). In this article, we explicitly address these two problems by proposing a Pose- and Attribute-consistent Person Image Synthesis Network (PAC-GAN). To reduce pose and appearance matching ambiguity, we propose a component-wise transferring model consisting of two stages. The former stage focuses only on synthesizing target poses, while the latter renders target appearances by explicitly transferring the appearance information from the source image to the target image in a component-wise manner. In this way, source-target matching ambiguity is eliminated due to the component-wise disentanglement of pose and appearance synthesis. Second, to maintain attribute consistency, we represent the input image as an attribute vector and impose a high-level semantic constraint using this vector to regularize the target synthesis. Extensive experimental results on the DeepFashion dataset demonstrate the superiority of our method over the state of the art, especially for maintaining pose and attribute consistencies under large pose variations.
C1 [Xu, Cheng; Chen, Zejun; Mai, Jiajie; Xu, Xuemiao; He, Shengfeng] South China Univ Technol, Guangzhou, Guangdong, Peoples R China.
   [Mai, Jiajie; Xu, Xuemiao] Kings Coll London, London, England.
   [Mai, Jiajie; Xu, Xuemiao] State Key Lab Subtrop Bldg Sci, Guangzhou, Guangdong, Peoples R China.
   [Mai, Jiajie; Xu, Xuemiao] Minist Educ, Key Lab Big Data & Intelligent Robot, Guangzhou, Guangdong, Peoples R China.
   [Mai, Jiajie; Xu, Xuemiao] Guangdong Prov Key Lab Computat Intelligence & Cy, Guangzhou, Guangdong, Peoples R China.
C3 South China University of Technology; University of London; King's
   College London
RP Mai, JJ; Xu, XM; He, SF (corresponding author), South China Univ Technol, Guangzhou, Guangdong, Peoples R China.; Mai, JJ; Xu, XM (corresponding author), Kings Coll London, London, England.; Mai, JJ; Xu, XM (corresponding author), State Key Lab Subtrop Bldg Sci, Guangzhou, Guangdong, Peoples R China.; Mai, JJ; Xu, XM (corresponding author), Minist Educ, Key Lab Big Data & Intelligent Robot, Guangzhou, Guangdong, Peoples R China.; Mai, JJ; Xu, XM (corresponding author), Guangdong Prov Key Lab Computat Intelligence & Cy, Guangzhou, Guangdong, Peoples R China.
EM cschengxu@gmail.com; darkhorsezzz@163.com; k20035517@kcl.ac.uk;
   xuemx@scut.edu.cn; hesfe@scut.edu.cn
RI He, Shengfeng/E-5682-2016; Xu, Cheng/HZL-1279-2023
OI He, Shengfeng/0000-0002-3802-4644; MAI, JIAJIE/0000-0002-0172-5981; Xu,
   Cheng/0000-0002-4281-6214
FU Key-Area Research and Development Program of Guangdong Province, China
   [2020B010165004, 2020B010166003]; National Natural Science Foundation of
   China [61772206, 61972162]; Guangdong International Science and
   Technology Cooperation Project [2021A0505030009]; Guangdong Natural
   Science Foundation [2021A1515012625]; Guangzhou Basic and Applied
   Research Project [202102021074]; CCF-Tencent Open Research fund
   [RAGR20210114]
FX This work is supported by the Key-Area Research and Development Program
   of Guangdong Province, China (2020B010165004, 2020B010166003); National
   Natural Science Foundation of China (61772206, 61972162); Guangdong
   International Science and Technology Cooperation Project
   (2021A0505030009); Guangdong Natural Science Foundation
   (2021A1515012625); Guangzhou Basic and Applied Research Project
   (202102021074); and CCF-Tencent Open Research fund (RAGR20210114).
CR Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Dong HY, 2019, IEEE I CONF COMP VIS, P1161, DOI 10.1109/ICCV.2019.00125
   Dong Haoye, 2018, ADV NEURAL INFORM PR, V2
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Gao C, 2020, Arxiv, DOI arXiv:2006.01435
   Ge Pu, 2021, P IEEE CVFWINTER C A, P3370
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Huang SY, 2020, Arxiv, DOI arXiv:2007.09077
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Jia J, 2020, Arxiv, DOI arXiv:2005.11909
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2014, arXiv
   Lathuilière S, 2020, IEEE WINT CONF APPL, P428, DOI [10.1109/WACV45572.2020.9093602, 10.1109/wacv45572.2020.9093602]
   Li DW, 2016, Arxiv, DOI arXiv:1603.07054
   Li K, 2020, IEEE T IMAGE PROCESS, V29, P9584, DOI 10.1109/TIP.2020.3029455
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma L, 2017, ADV NEURAL INFORM PR
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Paszke A, 2019, ADV NEUR IN, V32
   Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T, 2016, ADV NEUR IN, V29
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Tang H, 2020, Arxiv, DOI arXiv:2008.04381
   Tang H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2052, DOI 10.1145/3343031.3350980
   Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang L, 2020, P IEEE INT C MULT EX, P1, DOI DOI 10.1142/S021800142059003X
   Yue HJ, 2017, IEEE T IMAGE PROCESS, V26, P3981, DOI 10.1109/TIP.2017.2703078
   Yurui Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7687, DOI 10.1109/CVPR42600.2020.00771
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou Q, 2021, IEEE T IMAGE PROCESS, V30, P1623, DOI 10.1109/TIP.2019.2914575
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 55
TC 3
Z9 3
U1 4
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 81
DI 10.1145/3554739
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300006
OA Green Published
DA 2024-07-18
ER

PT J
AU Xu, YZ
   Yang, ZJ
   Chen, TS
   Li, K
   Qing, CM
AF Xu, Yongzong
   Yang, Zhijing
   Chen, Tianshui
   Li, Kai
   Qing, Chunmei
TI Progressive Transformer Machine for Natural Character Reenactment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Character reenactment; full-head reenactment; neural rendering; video
   render; 3DMM; transformer
ID 3D FACE RECONSTRUCTION; NEURAL-NETWORKS
AB Character reenactment aims to control a target person's full-head movement by a driving monocular sequence that is made up of the driving character video. Current algorithms utilize convolution neural networks in generative adversarial networks, which extract historical and geometric information to iteratively generate video frames. However, convolution neural networks can merely capture local information with limited receptive fields and ignore global dependencies that play a crucial role in face synthesis, leading to generating unnatural video frames. In this work, we design a progressive transformer module that introduces multi-head self-attention with convolution refinement to simultaneously capture global-local dependencies. Specifically, we utilize the non-lapping window-based multi-head self-attentionmechanism with hierarchical architecture to obtain the larger receptive fields at low-resolution feature map and thus extract global information. To better model local dependencies, we introduce the convolution operation to further refine the attentional weight in the multi-head self-attention mechanism. Finally, we use several stacked progressive transformer modules with the down-sampling operation to encode information of appearance information of previously generated frames and parameterized 3D face information of the current frame. Similarly, we use several stacked progressive transformer modules with the up-sampling operation to iteratively generate video frames. In this way, it can capture global-local information to facilitate generating video frames that are globally natural while preserving sharp outlines and rich detail information. Extensive experiments on several standard benchmarks suggest that the proposed method outperforms current leading algorithms.
C1 [Xu, Yongzong; Yang, Zhijing; Chen, Tianshui] Guangdong Univ Technol, Guangzhou 510006, Guangdong, Peoples R China.
   [Li, Kai] ZEGO, Guangzhou, Guangdong, Peoples R China.
   [Qing, Chunmei] South China Univ Technol, Guangzhou 510640, Guangdong, Peoples R China.
C3 Guangdong University of Technology; South China University of Technology
RP Chen, TS (corresponding author), Guangdong Univ Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM yongzong506@gmail.com; yzhj@gdut.edu.cn; chentianshui@gdut.edu.cn;
   doctorimage@qq.com; qchm@scut.edu.cn
OI Chen, Tianshui/0000-0002-5848-5624; Qing, Chunmei/0000-0002-4733-306X;
   LI, KAI/0000-0003-1266-0084
FU National Natural Science Foundation of China [61972163]; Science and
   Technology Project of Guangdong Province [2021A1515011341]; Guangzhou
   Science and Technology Plan Project [202002030386]; Guangdong Provincial
   Key Laboratory of Human Digital Twin [2022B1212010004]
FX Thiswork is supported in part by the National Natural Science Foundation
   of China (Grant No. 61972163), Science and Technology Project of
   Guangdong Province (Grant No. 2021A1515011341), Guangzhou Science and
   Technology Plan Project (Grant No. 202002030386), and Guangdong
   Provincial Key Laboratory of Human Digital Twin (Grant No.
   2022B1212010004).
CR [Anonymous], 2023, ACM T MULTIM COMPUT, V19
   Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Booth J, 2018, IEEE T PATTERN ANAL, V40, P2638, DOI 10.1109/TPAMI.2018.2832138
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chen TS, 2024, Arxiv, DOI [arXiv:2205.11131, 10.48550/arXiv.2205.11131]
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P9887, DOI 10.1109/TPAMI.2021.3131222
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Dong YH, 2021, Arxiv, DOI arXiv:2103.03404
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Doukas Michail Christos, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P31, DOI 10.1109/TBIOM.2021.3049576
   Garrido P, 2014, PROC CVPR IEEE, P4217, DOI 10.1109/CVPR.2014.537
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Ilg E, 2016, Arxiv, DOI arXiv:1612.01925
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang Y, 2021, arXiv, DOI DOI 10.48550/ARXIV.2102.07074
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Koujan MR, 2020, IEEE INT CONF AUTOMA, P16, DOI 10.1109/FG47880.2020.00048
   Koujan MR, 2018, PROCEEDINGS CVMP 2018: THE 15TH ACM SIGGRAPH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION, DOI 10.1145/3278471.3278476
   Li HF, 2021, IEEE T NEUR NET LEAR, V32, P1460, DOI 10.1109/TNNLS.2020.2984770
   Li YW, 2021, Arxiv, DOI arXiv:2104.05707
   Liu SG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3464382
   Liu ZC, 2001, COMP GRAPH, P271
   Liu Z, 2021, Arxiv, DOI [arXiv:2103.14030, DOI 10.48550/ARXIV.2103.14030]
   Loshchilov I., 2018, arXiv
   Luo WJ, 2016, ADV NEUR IN, V29
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Messina N, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451390
   Ouyang DQ, 2021, NEUROCOMPUTING, V453, P590, DOI 10.1016/j.neucom.2020.09.019
   Ouyang DQ, 2020, NEURAL COMPUT APPL, V32, P6571, DOI 10.1007/s00521-019-04115-x
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Saragih Jason M, 2011, Proc Int Conf Autom Face Gesture Recognit, P117, DOI 10.1109/FG.2011.5771400
   Shaw P, 2018, Arxiv, DOI arXiv:1803.02155
   Siarohin A, 2019, ADV NEUR IN, V32
   Siarohin A, 2019, PROC CVPR IEEE, P2372, DOI 10.1109/CVPR.2019.00248
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sutherland DJ, 2021, Arxiv, DOI arXiv:1611.04488
   Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thies J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201350
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056
   Touvron H, 2021, Arxiv, DOI arXiv:2103.17239
   Unterthiner T., 2019, FVD: A new metric for video generation
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang TC, 2018, Arxiv, DOI arXiv:1808.06601
   Wang X., 2021, arXiv
   Wang XP, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355397
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Wu HP, 2021, Arxiv, DOI [arXiv:2103.15808, DOI 10.48550/ARXIV.2103.15808]
   Yuan K, 2021, Arxiv, DOI [arXiv:2103.11816, DOI 10.48550/ARXIV.2103.11816]
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang LR, 2020, IEEE T NEUR NET LEAR, V31, P5092, DOI 10.1109/TNNLS.2019.2963146
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou DQ, 2021, Arxiv, DOI [arXiv:2103.11886, 10.48550/arXiv.2103.11886, DOI 10.48550/ARXIV.2103.11886]
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P523, DOI 10.1111/cgf.13382
NR 61
TC 0
Z9 0
U1 4
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 92
DI 10.1145/3559107
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300017
DA 2024-07-18
ER

PT J
AU Zhang, DY
   Huang, P
   Ding, XL
   Li, F
   Zhu, WJ
   Song, Y
   Yang, GB
AF Zhang, Dengyong
   Huang, Pu
   Ding, Xiangling
   Li, Feng
   Zhu, Wenjie
   Song, Yun
   Yang, Gaobo
TI <i>L</i><SUP>2</SUP><i>BEC</i><SUP>2</SUP>: Local Lightweight
   Bidirectional Encoding and Channel Attention Cascade for Video Frame
   Interpolation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video frame interpolation; lightweight network; bidirectional encoding;
   channel attention cascade
AB Video frame interpolation (VFI) is of great importance for many video applications, yet it is still challenging even in the era of deep learning. Some existing VFI models directly exploit existing lightweight network frameworks, thus making synthesized in-between frames blurry and creating artifacts due to imprecise motion representation. The other existing VFI models typically depend on heavy model architectures with a large number of parameters, preventing them from being deployed on small terminals. To address these issues, we propose a local lightweight VFI network ((LBEC2)-B-2) that leverages bidirectional encoding structure with channel attention cascade. Specifically, we improve visual quality by introducing a forward and backward encoding structure with channel attention cascade to better characterize motion information. Furthermore, we introduce a local lightweight strategy into the state-of-the-art Adaptive Collaboration of Flows (AdaCoF) model to simplify its model parameters. Compared with the original AdaCoF model, the proposed (LBEC2)-B-2 obtains performance gain at the cost of only one-third of the number of parameters and performs favorably against the state-of-the-art works on public datasets. Our source code is available at https://github.com/Pumpkin123709/LBEC.git.
C1 [Zhang, Dengyong; Huang, Pu; Li, Feng; Zhu, Wenjie; Song, Yun] Changsha Univ Sci & Technol, Changsha, Peoples R China.
   [Ding, Xiangling] Hunan Univ Sci & Technol, Xiangtan, Peoples R China.
   [Ding, Xiangling] Chinese Acad Sci, State Key Lab Informat Secur, Inst Informat Engn, Beijing, Peoples R China.
   [Ding, Xiangling] Zhengzhou Xinda Inst Adv Technol, Zhengzhou, Peoples R China.
   [Yang, Gaobo] Hunan Univ, Changsha, Peoples R China.
C3 Changsha University of Science & Technology; Hunan University of Science
   & Technology; Chinese Academy of Sciences; Institute of Information
   Engineering, CAS; Hunan University
RP Ding, XL (corresponding author), Hunan Univ Sci & Technol, Xiangtan, Peoples R China.; Ding, XL (corresponding author), Chinese Acad Sci, State Key Lab Informat Secur, Inst Informat Engn, Beijing, Peoples R China.; Ding, XL (corresponding author), Zhengzhou Xinda Inst Adv Technol, Zhengzhou, Peoples R China.
EM zhdy@csust.edu.cn; pumpkin@stu.csust.edu.cn; xianglingding@163.com;
   lif@csust.edu.cn; wenjiezhu@stu.csust.edu.cn; sonie@126.com;
   yanggaobo@hnu.edu.cn
OI ding, xiangling/0000-0002-6581-4633; Zhang,
   Dengyong/0000-0002-2789-2980; Yang, Gaobo/0000-0003-2734-659X; Li,
   Feng/0000-0003-2718-9918
FU National Natural Science Foundation of China [62172059, 62072055]; Hunan
   Provincial Natural Science Foundations of China [2020JJ4626,
   2020JJ4029]; Scientific Research Fund of Hunan Provincial Education
   Department of China [19B004]; Postgraduate Scientific Research
   Innovation Project of Changsha University of Science and Technology
   [CX2021SS76]; Postgraduate Scientific Research Innovation Project of
   Hunan Province [CX20210811]; Opening Project of State Key Laboratory of
   Information Security [2021-ZD-07]; Open Foundation of Henan Key
   Laboratory of Cyberspace Situation Awareness [HNTS2022025]
FX This work is supported in part by the National Natural Science
   Foundation of China (62172059, 62072055), the Hunan Provincial Natural
   Science Foundations of China (2020JJ4626, 2020JJ4029), the Scientific
   Research Fund of Hunan Provincial Education Department of China
   (19B004), the Postgraduate Scientific Research Innovation Project of
   Changsha University of Science and Technology (CX2021SS76), the
   Postgraduate Scientific Research Innovation Project of Hunan Province
   (CX20210811), the Opening Project of State Key Laboratory of Information
   Security (2021-ZD-07) and the Open Foundation of Henan Key Laboratory of
   Cyberspace Situation Awareness (No. HNTS2022025).
CR Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cheng XH, 2022, IEEE T PATTERN ANAL, V44, P7029, DOI 10.1109/TPAMI.2021.3100714
   Cheng XH, 2020, AAAI CONF ARTIF INTE, V34, P10607
   Chi ZX, 2021, Arxiv, DOI arXiv:2007.11762
   Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Ding TY, 2021, PROC CVPR IEEE, P7997, DOI 10.1109/CVPR46437.2021.00791
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gupta A., 2020, ARXIV
   He JL, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3382506
   Huang Z., 2020, arXiv
   Ioannou Y, 2017, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2017.633
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Koren Mark, 2017, Frame interpolation using generative adversarial networks
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Li HP, 2020, INT CONF ACOUST SPEE, P2613, DOI [10.1109/icassp40776.2020.9053987, 10.1109/ICASSP40776.2020.9053987]
   Li HP, 2019, IEEE ACCESS, V7, P118287, DOI 10.1109/ACCESS.2019.2936549
   Liu Z., 2020, arXiv
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26
   Meyer S, 2018, PROC CVPR IEEE, P498, DOI 10.1109/CVPR.2018.00059
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Park M, 2021, IEEE T CIRC SYST VID, V31, P754, DOI 10.1109/TCSVT.2020.2981964
   Paszke A, 2019, ADV NEUR IN, V32
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tran QN, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186245
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao J, 2020, IEEE ACCESS, V8, P94842, DOI 10.1109/ACCESS.2020.2995705
   Xue TF, 2019, Arxiv, DOI arXiv:1711.09078
   Yu S, 2019, IEEE INT CONF COMP V, P3503, DOI 10.1109/ICCVW.2019.00434
   Yuan LZ, 2019, PROC CVPR IEEE, P12175, DOI 10.1109/CVPR.2019.01246
   Zagoruyko Sergey, 2016, ARXIV
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhuang JK, 2020, IEEE IMAGE PROC, P543, DOI 10.1109/ICIP40778.2020.9191039
NR 43
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 66
DI 10.1145/3547660
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000016
DA 2024-07-18
ER

PT J
AU Xiao, MY
   Li, XL
   Zhao, Y
   Ma, B
   Guo, GD
AF Xiao, Mengyao
   Li, Xiaolong
   Zhao, Yao
   Ma, Bin
   Guo, Guodong
TI A Novel Reversible Data Hiding Scheme Based on Pixel-Residual Histogram
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Reversible data hiding; prediction-error expansion; pixel-residual
   histogram; adaptive embedding
ID DIFFERENCE EXPANSION; WATERMARKING; PREDICTION
AB Prediction-error expansion (PEE) is the most popular reversible data hiding (RDH) technique due to its efficient capacity-distortion tradeoff. With the generated prediction-error histogram (PEH) and adaptively selected expansion bins, the image redundancy is well exploited by PEE. However, for the most widely used rhombus predictor, the rounding operation which groups different prediction-errors into one value is completely unnecessary. The embedding can be extended to a general case by removing the rounding operation, and more histogram bins can be derived for expansion with a new mapping mechanism. Therefore, in this article, instead of pixel prediction-error, we propose to compute the pixel residuals without the rounding operation, and a new embedding mechanism based on pixel-residual histogram (PRH) modification is devised. In PRH, four bins correspond to one bin in PEH. Then, different from the one-to-one mapping between the prediction-error and pixel modification, a four-to-one mapping between the pixel-residual and pixel modification is established, and the performance is optimized by adaptively selecting four expansion bin pairs for embedding. Since more modification selections are considered, better performance can be obtained. Moreover, the proposed scheme is extended to the two-dimensional (2D) histogram and multiple histograms based embedding, and the performance is further enhanced. The superiority of the proposed method is experimentally verified by comparing it with some state-of-the-art works.
C1 [Xiao, Mengyao; Li, Xiaolong; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Ma, Bin] Qilu Univ Technol, Sch Cyber Secur, Jinan 250353, Peoples R China.
   [Ma, Bin] Shandong Prov Key Lab Comp Networks, Jinan 250353, Peoples R China.
   [Guo, Guodong] Inst Deep Learning & Natl Engn Lab Deep Learning, Baidu Res, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University; Qilu University of Technology; Baidu
RP Xiao, MY (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
EM xiaomengyao@bjtu.edu.cn; lixl@bjtu.edu.cn; yzhao@bjtu.edu.cn;
   sddxmb@126.com; guoguodong01@baidu.com
RI Li, xiaolong/GRS-9148-2022; Guo, Guodong/M-5066-2015
OI Zhao, Yao/0000-0002-8581-9554; Guo, Guodong/0000-0001-9583-0055
FU National Key R&D Program of China [2021ZD0112100]; National Natural
   Science Foundation of China [61972031, U1936212, 62120106009]
FX This work was supported by the National Key R&D Program of China (No.
   2021ZD0112100), and the National Natural Science Foundation of China
   (Nos. 61972031, U1936212, 62120106009).
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   [Anonymous], 1977, USC SIPI IMAGE DATAB
   Bhowmik D, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3357333
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chen HS, 2017, IEEE SIGNAL PROC LET, V24, P574, DOI 10.1109/LSP.2017.2679043
   Coatrieux G, 2013, IEEE T INF FOREN SEC, V8, P111, DOI 10.1109/TIFS.2012.2224108
   Coatrieux G, 2009, IEEE T INF TECHNOL B, V13, P158, DOI 10.1109/TITB.2008.2007199
   Cox IJ., 2007, DIGITAL WATERMARKING
   Dragoi IC, 2014, IEEE T IMAGE PROCESS, V23, P1779, DOI 10.1109/TIP.2014.2307482
   Fridrich J, 2001, P SOC PHOTO-OPT INS, V4314, P197, DOI 10.1117/12.435400
   Fridrich J., 2009, Steganography in Digital Media: Principles, Algorithms, and Applications
   He WG, 2020, IEEE T INF FOREN SEC, V15, P3859, DOI 10.1109/TIFS.2020.3002377
   Hu YJ, 2009, IEEE T CIRC SYST VID, V19, P250, DOI 10.1109/TCSVT.2008.2009252
   Huang FJ, 2016, IEEE T CIRC SYST VID, V26, P1610, DOI 10.1109/TCSVT.2015.2473235
   Hwang HJ, 2010, KSII T INTERNET INF, V4, P655, DOI 10.3837/tiis.2010.08.0012
   Kim S, 2019, IEEE T CIRC SYST VID, V29, P3236, DOI 10.1109/TCSVT.2018.2878932
   Li XL, 2015, IEEE T INF FOREN SEC, V10, P2016, DOI 10.1109/TIFS.2015.2444354
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Li XL, 2013, SIGNAL PROCESS, V93, P198, DOI 10.1016/j.sigpro.2012.07.025
   Luo LX, 2010, IEEE T INF FOREN SEC, V5, P187, DOI 10.1109/TIFS.2009.2035975
   Ma B, 2020, SIGNAL PROCESS, V172, DOI 10.1016/j.sigpro.2020.107544
   Ma B, 2016, IEEE T INF FOREN SEC, V11, P1914, DOI 10.1109/TIFS.2016.2566261
   Ma Bin, 2018, P IWDW 2018, P195
   Naskar R, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487272
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ou B, 2020, IEEE T CIRC SYST VID, V30, P2329, DOI 10.1109/TCSVT.2019.2921812
   Ou B, 2019, IEEE T CIRC SYST VID, V29, P2176, DOI 10.1109/TCSVT.2018.2859792
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Peng F, 2012, SIGNAL PROCESS, V92, P54, DOI 10.1016/j.sigpro.2011.06.006
   Qi WF, 2020, IEEE T CIRC SYST VID, V30, P2300, DOI 10.1109/TCSVT.2019.2942489
   Qian ZX, 2019, IEEE T CIRC SYST VID, V29, P351, DOI 10.1109/TCSVT.2018.2797897
   Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wang JX, 2020, IEEE T CIRC SYST VID, V30, P2313, DOI 10.1109/TCSVT.2019.2915584
   Wang JX, 2017, IEEE T CYBERNETICS, V47, P315, DOI 10.1109/TCYB.2015.2514110
   Wang X, 2010, IEEE SIGNAL PROC LET, V17, P567, DOI 10.1109/LSP.2010.2046930
   Weng SW, 2008, IEEE SIGNAL PROC LET, V15, P721, DOI 10.1109/LSP.2008.2001984
   Xiang Shijun, 2021, IEEE T CIRC SYST VID
   Xiao MY, 2021, IEEE T CIRC SYST VID, V31, P2535, DOI 10.1109/TCSVT.2020.3027391
   Yang HY, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700299
   Yin ZX, 2020, IEEE T CIRC SYST VID, V30, P2343, DOI 10.1109/TCSVT.2020.2969463
   Zhang T, 2020, IEEE T INF FOREN SEC, V15, P2306, DOI 10.1109/TIFS.2019.2963766
   Zhang XP, 2016, IEEE T CIRC SYST VID, V26, P1622, DOI 10.1109/TCSVT.2015.2433194
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zhang Y, 2018, SIGNAL PROCESS, V146, P99, DOI 10.1016/j.sigpro.2018.01.011
NR 47
TC 1
Z9 1
U1 8
U2 43
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
AR 46
DI 10.1145/3534565
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800020
DA 2024-07-18
ER

PT J
AU Xu, LM
   Zeng, XH
   Li, WS
   Zheng, BC
AF Xu, Liming
   Zeng, Xianhua
   Li, Weisheng
   Zheng, Bochuan
TI MFGAN: Multi-modal Feature-fusion for CT Metal Artifact Reduction Using
   GANs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Feature fusion; generative adversarial nets; metal artifact reduction;
   second artifact; edge enhancement
ID INFORMATION; NETWORK; MODEL
AB Due to the existence of metallic implants in certain patients, the Computed Tomography (CT) images from these patients are often corrupted by undesirable metal artifacts, which causes severe problem of metal artifact. Although many methods have been proposed to reduce metal artifact, reduction is still challenging and inadequate. Some reduced results are suffering from symptom variance, second artifact, and poor subjective evaluation. To address these, we propose a novel method based on generative adversarial nets (GANs) to reduce metal artifacts. Specifically, we firstly encode interactive information (text) and imaging CT (image) to yield multi-modal feature-fusion representation, which overcomes representative ability limitation of single-modal CT images. The incorporation of interaction information constrains feature generation, which ensures symptom consistency between corrected and target CT. Then, we design an enhancement network to avoid second artifact and enhance edge as well as suppress noise. Besides, three radiology physicians are invited to evaluate the corrected CT image. Experiments show that our method gains significant improvement over other methods. Objectively, ours achieves an average increment of 7.44% PSNR and 6.12% SSIM on two medical image datasets. Subjectively, ours outperforms others in comparison in term of sharpness, resolution, invariance, and acceptability.
C1 [Xu, Liming; Zheng, Bochuan] China West Normal Univ, 1 Shida Rd, Nanchong 637009, Sichuan, Peoples R China.
   [Zeng, Xianhua; Li, Weisheng] Chongqing Univ Posts & Telecommun, 2 Chongwen Rd, Chongqing 400065, Peoples R China.
C3 China West Normal University; Chongqing University of Posts &
   Telecommunications
RP Xu, LM (corresponding author), China West Normal Univ, 1 Shida Rd, Nanchong 637009, Sichuan, Peoples R China.
EM xulm@cwnu.edu.cn; zengxh@cqupt.edu.cn; liws@cqupt.edu.cn;
   zhengbc@cwnu.edu.cn
OI zheng, bochuan/0000-0003-4495-8299; Xu, Liming/0000-0002-0671-8182;
   Zeng, Xianhua/0000-0001-5892-2372
FU National Natural Science Foundation of China [62076044, 62176217];
   Natural Science Foundation of Chongqing, China [cstc2019jcyj-zdxm0011];
   Doctoral Research Innovation Project [21E025]; Youth Science Foundation
   of Sichuan [2022NSFSC0866]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 62076044, 62176217), the Natural Science Foundation of
   Chongqing, China (Grant No. cstc2019jcyj-zdxm0011), the Doctoral
   Research Innovation Project (Grant No. 21E025) and the Youth Science
   Foundation of Sichuan (Grant No. 2022NSFSC0866).
CR Arjovsky M., 2017, ARXIV170107875
   Bal V, 2005, P SOC PHOTO-OPT INS, V5747, P2075, DOI 10.1117/12.593095
   Désidéri JA, 2012, CR MATH, V350, P313, DOI 10.1016/j.crma.2012.03.014
   Gjesteby Lars, 2017, PROC SPIE, P21
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Hoffer E, 2017, ADV NEUR IN, V30
   Jiang K, 2019, IEEE T GEOSCI REMOTE, V57, P5799, DOI 10.1109/TGRS.2019.2902431
   Kachelriess M, 2001, MED PHYS, V28, P475, DOI 10.1118/1.1358303
   KALENDER WA, 1987, RADIOLOGY, V164, P576, DOI 10.1148/radiology.164.2.3602406
   Karras T, 2018, P INT C LEARN REPR I
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lemmens C, 2009, IEEE T MED IMAGING, V28, P250, DOI 10.1109/TMI.2008.929103
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Liao HF, 2020, IEEE T MED IMAGING, V39, P634, DOI 10.1109/TMI.2019.2933425
   Lin WA, 2019, PROC CVPR IEEE, P10504, DOI 10.1109/CVPR.2019.01076
   Meyer E, 2010, MED PHYS, V37, P5482, DOI 10.1118/1.3484090
   Park HS, 2016, IEEE T MED IMAGING, V35, P480, DOI 10.1109/TMI.2015.2478905
   Prell D, 2009, PHYS MED BIOL, V54, P6575, DOI 10.1088/0031-9155/54/21/009
   Salimans T., 2017, ADV NEURAL INFORM PR, P2234
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Shmelkov K, 2018, LECT NOTES COMPUT SC, V11206, P218, DOI 10.1007/978-3-030-01216-8_14
   Wang JN, 2018, LECT NOTES COMPUT SC, V11070, P3, DOI 10.1007/978-3-030-00928-1_1
   Wang X, 2007, IEEE T PATTERN ANAL, V29, P886, DOI 10.1109/TPAMI.2007.1027
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Xie YT, 2018, INFORM FUSION, V42, P102, DOI 10.1016/j.inffus.2017.10.005
   Xu LM, 2020, NEURAL NETWORKS, V128, P82, DOI 10.1016/j.neunet.2020.05.001
   Xu SY, 2018, PROC SPIE, V10573, DOI 10.1117/12.2293945
   Zhang HM, 2018, SIAM J IMAGING SCI, V11, P707, DOI 10.1137/17M1140212
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang YB, 2018, IEEE T MED IMAGING, V37, P1370, DOI 10.1109/TMI.2018.2823083
NR 31
TC 0
Z9 0
U1 4
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
DI 10.1145/3528172
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800007
DA 2024-07-18
ER

PT J
AU Yu, H
   Cheang, C
   Fu, YW
   Xue, XY
AF Yu, Hang
   Cheang, Chilam
   Fu, Yanwei
   Xue, Xiangyang
TI Multi-view Shape Generation for a 3D Human-like Body
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D reconstruction; human body reconstruction; multi-view stereo
AB Three-dimensional (3D) human-like body reconstruction via a single RGB image has attracted significant research attention recently. Most of the existing methods rely on the Skinned Multi-Person Linear model and thus can only predict unified human bodies. Moreover, meshes reconstructed by current methods sometimes perform well from a canonical view but not from other views, as the reconstruction process is commonly supervised by only a single view. To address these limitations, this article proposes a multi-view shape generation network for a 3D human-like body. Particularly, we propose a coarse-to-fine learning model that gradually deforms a template body toward the ground truth body. Our model utilizes the information of multi-view renderings and corresponding 3D vertex transformation as supervision. Such supervision will help to generate 3D bodies well aligned to all views. To accurately operate mesh deformation, a graph convolutional network structure is introduced to support the shape generation from 3D vertex representation. Additionally, a graph up-pooling operation is designed over the intermediate representations of the graph convolutional network, and thus our model can generate 3D shapes with higher resolution. Novel loss functions are employed to help optimize the whole multi-view generation model, resulting in smoother surfaces. In addition, twomulti-view human body datasets are produced and contributed to the community. Extensive experiments conducted on the benchmark datasets demonstrate the efficacy of our model over the competitors.
C1 [Yu, Hang] Fudan Univ, Acad Engn & Technol, Shanghai, Peoples R China.
   [Cheang, Chilam; Xue, Xiangyang] Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China.
   [Fu, Yanwei] Fudan Univ, Sch Data Sci, Shanghai, Peoples R China.
   [Fu, Yanwei] Zhejiang Normal Univ, ISTBI ZJNU Algorithm Ctr Brain Inspired Intellige, Jinhua, Zhejiang, Peoples R China.
C3 Fudan University; Fudan University; Fudan University; Zhejiang Normal
   University
RP Fu, YW (corresponding author), Fudan Univ, Sch Data Sci, Shanghai, Peoples R China.; Fu, YW (corresponding author), Zhejiang Normal Univ, ISTBI ZJNU Algorithm Ctr Brain Inspired Intellige, Jinhua, Zhejiang, Peoples R China.
EM sir.hangyu@gmail.com; 19210240252@fudan.edu.cn; yanweifu@fudan.edu.cn;
   xyxue@fudan.edu.cn
RI Fu, Yanwei/JTT-7059-2023
OI Fu, Yanwei/0000-0002-6595-6893
FU Shanghai Municipal Science and Technology Major Project
   [2021SHZDZX0103]; STCSM Project [19ZR1471800]
FX Yanwei Fu this work was partially supported by Shanghai Municipal
   Science and Technology Major Project (2021SHZDZX0103) and STCSM Project
   (19ZR1471800).
CR Alldieck T, 2019, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR.2019.00127
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 2017, ADV NEURAL INFORM PR
   Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552
   Blender Online Community, 2018, Blender-A 3D modelling and rendering package
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Bruna J, 2014, Arxiv, DOI [arXiv:1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Choi Hongsuk, 2020, P EUROPEAN C COMPUTE
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Corona E, 2021, PROC CVPR IEEE, P11870, DOI 10.1109/CVPR46437.2021.01170
   Defferrard M., 2016, P 30 INT C NEURAL IN, V29, P3844
   Everingham M., 2010, BMVC, V2, P5
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500
   Lewiner T., 2003, Journal of Graphics Tools, V8, P1, DOI 10.1080/10867651.2003.10487582
   Liang JB, 2019, IEEE I CONF COMP VIS, P4351, DOI 10.1109/ICCV.2019.00445
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Niepert M, 2016, PR MACH LEARN RES, V48
   Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Peng SD, 2021, PROC CVPR IEEE, P9050, DOI 10.1109/CVPR46437.2021.00894
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Pumarola A, 2019, IEEE I CONF COMP VIS, P2242, DOI [10.1109/ICCV.2019.00233, 10.1109/ICCV.2019.2019.00233]
   Saito S, 2019, Arxiv, DOI arXiv:1905.05172
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   The SAE Foundation, CIV AM EUR SURF ANTH
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3408317
   Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zheng ZR, 2022, IEEE T PATTERN ANAL, V44, P3170, DOI 10.1109/TPAMI.2021.3050505
   Zhu H, 2019, PROC CVPR IEEE, P4486, DOI 10.1109/CVPR.2019.00462
NR 54
TC 9
Z9 9
U1 5
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 11
DI 10.1145/3514248
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400011
DA 2024-07-18
ER

PT J
AU Zhang, YZ
   Tiwari, P
   Rong, L
   Chen, R
   Alnajem, NA
   Hossain, MS
AF Zhang, Yazhou
   Tiwari, Prayag
   Rong, Lu
   Chen, Rui
   Alnajem, Nojoom A.
   Hossain, M. Shamim
TI Affective Interaction: Attentive Representation Learning for Multi-Modal
   Sentiment Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-modal sentiment analysis; emotion recognition; representation
   learning; deep learning; artificial intelligence
ID INTERACTION DYNAMICS; EDGE
AB The recent booming of artificial intelligence (AI) applications, e.g., affective robots, human-machine interfaces, autonomous vehicles, and so on, has produced a great number of multi-modal records of human communication. Such data often carry latent subjective users' attitudes and opinions, which provides a practical and feasible path to realize the connection between human emotion and intelligence services. Sentiment and emotion analysis of multi-modal records is of great value to improve the intelligence level of affective services. However, how to find an optimal manner to learn people's sentiments and emotional representations has been a difficult problem, since both of them involve subtle mind activity. To solve this problem, a lot of approaches have been published, but most of them are insufficient to mine sentiment and emotion, since they have treated sentiment analysis and emotion recognition as two separate tasks. The interaction between them has been neglected, which limits the efficiency of sentiment and emotion representation learning. In this work, emotion is seen as the external expression of sentiment, while sentiment is the essential nature of emotion. We thus argue that they are strongly related to each other where one's judgment helps the decision of the other. The key challenges are multi-modal fused representation and the interaction between sentiment and emotion. To solve such issues, we design an external knowledge enhanced multi-task representation learning network, termed KAMT. The major elements contain two attention mechanisms, which are inter-modal and inter-task attentions and an external knowledge augmentation layer. The external knowledge augmentation layer is used to extract the vector of the participant's gender, age, occupation, and of overall color or shape. The main use of inter-modal attention is to capture effective multi-modal fused features. Inter-task attention is designed to model the correlation between sentiment analysis and emotion classification. We perform experiments on three widely used datasets, and the experimental performance proves the effectiveness of the KAMT model.
C1 [Zhang, Yazhou; Chen, Rui] Zhengzhou Univ Light Ind, Software Engn Coll, 136 Sci Ave, Zhengzhou 450002, Peoples R China.
   [Tiwari, Prayag] Aalto Univ, Dept Comp Sci, Espoo 02150, Finland.
   [Rong, Lu] Univ Coll Sedaya Int, Fac Social Sci & Liberal Arts, Kuala Lumpur, Malaysia.
   [Alnajem, Nojoom A.; Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, POB 51178, Riyadh 11543, Saudi Arabia.
C3 Zhengzhou University of Light Industry; Aalto University; UCSI
   University; King Saud University
RP Zhang, YZ (corresponding author), Zhengzhou Univ Light Ind, Software Engn Coll, 136 Sci Ave, Zhengzhou 450002, Peoples R China.
EM yzzhang@zzuli.edu.cn; prayag.liwari@aalto.fi; lurong2013@outlook.com;
   2019020@zzuli.edu.cn; nojoomalnajem@ieee.org; mshossain@ksu.edu.sa
RI Hossain, M. Shamim/K-1362-2014; Tiwari, Prayag/N-6261-2017
OI Hossain, M. Shamim/0000-0001-5906-9422; Tiwari,
   Prayag/0000-0002-2851-4260
FU King Saud University, Riyadh, Saudi Arabia [RSP-2021/32]; National
   Science Foundation of China [62006212]; State Key Lab. for Novel
   Software Technology in Nanjing University [KFKT2021B41]; Industrial
   Science and Technology Research Project of Henan Province [222102210031,
   212102210418, 202102210387]
FX This work was supported by the Researchers Supporting Project number
   (RSP-2021/32), King Saud University, Riyadh, Saudi Arabia. This work is
   also supported by National Science Foundation of China under grant No.
   62006212, the fund of State Key Lab. for Novel Software Technology in
   Nanjing University under grant No. KFKT2021B41, the Industrial Science
   and Technology Research Project of Henan Province under Grants
   222102210031, 212102210418, 202102210387.
CR Aloufi S, 2018, IEEE ACCESS, V6, P78609, DOI 10.1109/ACCESS.2018.2885117
   Becigneul Gary, 2021, P 2 INT C COMPUTING, P1
   Chakravarthi Bharathi Raja., 2021, DravidianMultiModality: A Dataset for Multi-Modal Sentiment Analysis in Tamil and Malayalam
   Chauhan DS, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4351
   Chen J, 2021, NEURAL COMPUT APPL, V33, P8669, DOI 10.1007/s00521-020-05616-w
   Cimtay Y, 2020, IEEE ACCESS, V8, P168865, DOI 10.1109/ACCESS.2020.3023871
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Du JY, 2021, COGN COMPUT, V13, P1114, DOI 10.1007/s12559-021-09855-4
   Du YP, 2022, KNOWL-BASED SYST, V240, DOI 10.1016/j.knosys.2021.108107
   Ethayarajh K, 2019, Arxiv, DOI [arXiv:1909.00512, DOI 10.18653/V1/D19]
   He ZP, 2020, BRAIN SCI, V10, DOI 10.3390/brainsci10100687
   Hossain MS, 2019, INFORM SCIENCES, V504, P589, DOI 10.1016/j.ins.2019.07.040
   Hossain MS, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3241056
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Hossain MS, 2018, IEEE INTERNET THINGS, V5, P2399, DOI 10.1109/JIOT.2017.2772959
   Hossain MS, 2016, MOBILE NETW APPL, V21, P753, DOI 10.1007/s11036-016-0685-9
   Hossain MS, 2017, IEEE ACCESS, V5, P2281, DOI 10.1109/ACCESS.2017.2672829
   Hossain MS, 2016, J MULTIMODAL USER IN, V10, P325, DOI 10.1007/s12193-015-0207-2
   Hossain MS, 2015, IEEE T CIRC SYST VID, V25, P2105, DOI 10.1109/TCSVT.2015.2444731
   Huang FR, 2019, KNOWL-BASED SYST, V167, P26, DOI 10.1016/j.knosys.2019.01.019
   Huddar MG, 2021, MULTIMED TOOLS APPL, V80, P13059, DOI 10.1007/s11042-020-10285-x
   Keswani Vishal, 2020, P 14 WORKSH SEM EV, P1135
   Kumar A, 2020, IEEE ACCESS, V8, P6388, DOI 10.1109/ACCESS.2019.2963630
   Li D, 2016, 2016 FIRST IEEE INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATION AND THE INTERNET (ICCCI 2016), P471, DOI 10.1109/CCI.2016.7778967
   Li Q., 2019, P THE10TH ITALIAN IN, P1
   Lin H, 2021, IEEE INTERNET THINGS, V8, P15683, DOI 10.1109/JIOT.2020.3033129
   Liu F, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23101349
   Liu JX, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6339, DOI 10.1109/ICASSP39728.2021.9413608
   Liu YC, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2021, P871
   Mittal T, 2020, AAAI CONF ARTIF INTE, V34, P1359
   Morishita T., 2020, P 14 WORKSH SEM EV S, P1126
   Muhammad G, 2021, INFORM FUSION, V72, P80, DOI 10.1016/j.inffus.2021.02.013
   Muhammad G, 2021, IEEE J SEL AREA COMM, V39, P603, DOI 10.1109/JSAC.2020.3020654
   Nemati S, 2019, IEEE ACCESS, V7, P172948, DOI 10.1109/ACCESS.2019.2955637
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Potamias RA, 2020, NEURAL COMPUT APPL, V32, P17309, DOI 10.1007/s00521-020-05102-3
   Qiu Y, 2019, IEEE T MULTIMEDIA, V21, P1778, DOI 10.1109/TMM.2018.2883866
   Rahman MA, 2021, IEEE INTERNET THINGS, V8, P9603, DOI 10.1109/JIOT.2020.3013710
   Sharma C., 2020, P 14 INT WORKSHOP SE
   Shorfuzzaman M, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107700
   Singh T, 2016, PROCEDIA COMPUT SCI, V89, P549, DOI 10.1016/j.procs.2016.06.095
   Song LL, 2018, 2018 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND BIG DATA (ICAIBD), P117, DOI 10.1109/ICAIBD.2018.8396178
   Tan MX, 2019, PR MACH LEARN RES, V97
   Trausan-Matu S., 2020, P 14 WORKSH SEM EV, P1208
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Tu G, 2022, KNOWL-BASED SYST, V235, DOI 10.1016/j.knosys.2021.107598
   [王雨竹 Wang Yuzhu], 2021, [数据分析与知识发现, Data Analysis and Knowledge Discovery], V5, P49
   Wen HL, 2021, PATTERN RECOGN LETT, V146, P252, DOI 10.1016/j.patrec.2021.03.025
   Wu T, 2022, KNOWL-BASED SYST, V235, DOI 10.1016/j.knosys.2021.107676
   Xiang Li, 2021, 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P2392, DOI 10.1109/BIBM52615.2021.9669461
   Yang KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P521, DOI 10.1145/3394171.3413690
   Yang SQ, 2018, MOBILE NETW APPL, V23, P216, DOI 10.1007/s11036-017-0929-3
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yazhou Zhang, 2021, 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P3142, DOI 10.1109/BIBM52615.2021.9669546
   Yu WM, 2021, Arxiv, DOI [arXiv:2102.04830, 10.48550/arXiv.2102.04830]
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zhang DS, 2002, SIGNAL PROCESS-IMAGE, V17, P825, DOI 10.1016/S0923-5965(02)00084-X
   Zhang SX, 2018, FUTURE GENER COMP SY, V81, P395, DOI 10.1016/j.future.2017.09.048
   Zhang YZ, 2021, IEEE T FUZZY SYST, V29, P3696, DOI 10.1109/TFUZZ.2021.3072492
   Zhang YZ, 2021, NEURAL NETWORKS, V133, P40, DOI 10.1016/j.neunet.2020.10.001
   Zhang YZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5436
   Zhang YZ, 2020, INFORM FUSION, V62, P14, DOI 10.1016/j.inffus.2020.04.003
   Zhang YZ, 2018, THEOR COMPUT SCI, V752, P21, DOI 10.1016/j.tcs.2018.04.029
   Zhang YZ, 2018, LECT NOTES COMPUT SC, V10772, P316, DOI 10.1007/978-3-319-76941-7_24
   Zhang Y, 2021, IEEE NETWORK, V35, P228, DOI 10.1109/MNET.011.2000400
   Zhang Y, 2019, IEEE NETWORK, V33, P58, DOI 10.1109/MNET.2019.1800344
   Zhang Y, 2019, IEEE T MULTIMEDIA, V21, P617, DOI 10.1109/TMM.2018.2882744
   Zhou JH, 2019, IEEE ACCESS, V7, P38856, DOI 10.1109/ACCESS.2019.2905048
   Zhou P, 2019, INFORM FUSION, V52, P167, DOI 10.1016/j.inffus.2019.03.003
NR 69
TC 0
Z9 0
U1 9
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 136
DI 10.1145/3527175
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800006
DA 2024-07-18
ER

PT J
AU Liang, LQ
   Lang, CY
   Li, Z
   Zhao, J
   Wang, T
   Feng, SH
AF Liang, Liqian
   Lang, Congyan
   Li, Zun
   Zhao, Jian
   Wang, Tao
   Feng, Songhe
TI Seeing Crucial Parts: Vehicle Model Verification via a Discriminative
   Representation Model
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Vehicle model verification; vehicle re-identification; image
   representation learning; deep metric learning
ID DEEP; REIDENTIFICATION
AB Widely used surveillance cameras have promoted large amounts of street scene data, which contains one important but long-neglected object: the vehicle. Here we focus on the challenging problem of vehicle model verification. Most previous works usually employ global features (e.g., fully connected features) to further perform vehicle-level deep metric learning (e.g., triplet-based network). However, we argue that it is noteworthy to investigate the distinctiveness of local features and consider vehicle-part-level metric learning by reducing the intra-class variance as much as possible. In this article, we introduce a simple yet powerful deep model the enforced intra-class alignment network (EIA-Net) which can learn a more discriminative image representation by localizing key vehicle parts and jointly incorporating two distance metrics: vehicle-level embedding and vehicle-part-sensitive embedding. For learning features, we propose an effective feature extraction module that is composed of two components: the regional proposal network (RPN)-based network and part-based CNN. The RPN is used to define key vehicle regions and aggregate local features on these regions, whereas part-based CNN offers supplementary global features for the RPN-based network. The fusion features learned by feature extraction module are cast into the deep metric learning module. Especially, we derived an enforced intra-class alignment loss by re-utilizing key vehicle part information to enhance reducing intra-class variance. Furthermore, we modify the coupled cluster loss to model the vehicle-level embedding by enlarging the inter-class variance while shortening intra-class variance. Extensive experiments over benchmark datasets VehicleID and CompCars have shown that the proposed EIA-Net significantly outperforms the state-of-the-art approaches for vehicle model verification. Furthermore, we also conduct comprehensive experiments on vehicle re-identification datasets (i.e., VehicleID and VeRi776) to validate the generalization ability effectiveness of our proposed method.
C1 [Liang, Liqian; Lang, Congyan] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing Key Lab Traff Data Anal & Min, 3 Shangyuancun, Beijing 100044, Peoples R China.
   [Li, Zun; Wang, Tao; Feng, Songhe] Beijing Jiaotong Univ, 3 Shangyuancun, Beijing 100044, Peoples R China.
   [Zhao, Jian] Inst North Elect Equipment, 226 North Fourth Ring Rd, Beijing 100191, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University
RP Lang, CY (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing Key Lab Traff Data Anal & Min, 3 Shangyuancun, Beijing 100044, Peoples R China.
EM lqliang@bjtu.edu.cn; cylang@bjtu.edu.cn; 16112072@bjtu.edu.cn;
   zhaojian90@u.nus.edu; twang@bjtu.edu.cn; shfeng@bjtu.edu.cn
RI zhao, jian/HTM-3920-2023
OI Liang, Liqian/0000-0001-8701-4074
FU National Natural Science Foundation of China [62072027, 61872032,
   62076021]; Beijing Natural Science Foundation [4202057, 4202058,
   4202060]
FX This work was supported in part by the National Natural Science
   Foundation of China under grants 62072027, 61872032, and 62076021, and
   in part by the Beijing Natural Science Foundation under grants 4202057,
   4202058, and 4202060.
CR Alfasly S, 2019, IEEE ACCESS, V7, P162605, DOI 10.1109/ACCESS.2019.2948965
   [Anonymous], 2018, PR MACH LEARN RES
   [Anonymous], 2015, P 2015 IEEE C COMPUT
   [Anonymous], 2013, ARXIV PREPRINT ARXIV
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Cormier M, 2016, 2016 IEEE WINTER APPLICATIONS OF COMPUTER VISION WORKSHOPS (WACVW)
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Feris RS, 2012, IEEE T MULTIMEDIA, V14, P28, DOI 10.1109/TMM.2011.2170666
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Guo HY, 2018, AAAI CONF ARTIF INTE, P6853
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hossein A., 2014, ARXIV PREPRINT ARXIV
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin YL, 2014, LECT NOTES COMPUT SC, V8692, P466, DOI 10.1007/978-3-319-10593-2_31
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu X., 2020, P ACM MULT
   Liu XB, 2018, IEEE INT CON MULTI
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Lou YH, 2019, IEEE T IMAGE PROCESS, V28, P3794, DOI 10.1109/TIP.2019.2902112
   Ma XH, 2019, PROC CVPR IEEE, P8258, DOI 10.1109/CVPR.2019.00846
   Meng DC, 2020, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR42600.2020.00713
   Qian J, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4755
   Ramnath K, 2014, IEEE WINT CONF APPL, P285, DOI 10.1109/WACV.2014.6836087
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ross A. S., 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.74
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen YT, 2017, IEEE I CONF COMP VIS, P1918, DOI 10.1109/ICCV.2017.210
   Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22
   Simonyan K, 2015, IEEE INT C ICLR
   Simonyan K., 2012, VERY DEEP CONVOLUTIO
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tang Z, 2019, IEEE I CONF COMP VIS, P211, DOI 10.1109/ICCV.2019.00030
   Tolias G., 2016, Conference Track Proceedings,
   Wang JJ, 2014, IEEE CONF COMPU INTE
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Xiang Y, 2015, PROC CVPR IEEE, P1903, DOI 10.1109/CVPR.2015.7298800
   Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Yang X, 2018, IEEE T IMAGE PROCESS, V27, P791, DOI 10.1109/TIP.2017.2765836
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhang XF, 2016, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR.2016.126
   Zhang ZX, 2012, IEEE T IMAGE PROCESS, V21, P1, DOI 10.1109/TIP.2011.2160954
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zhou F, 2016, PROC CVPR IEEE, P1124, DOI 10.1109/CVPR.2016.127
   Zhou Y., 2017, 1 AS AUSTR C PREC PA, P1, DOI DOI 10.5244/C.31.186
   Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679
   Zhu J., 2016, P INT C LEARN REPR
NR 66
TC 0
Z9 0
U1 3
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 28
DI 10.1145/3474596
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300005
DA 2024-07-18
ER

PT J
AU Aloufi, S
   El Saddik, A
AF Aloufi, Samah
   El Saddik, Abdulmotaleb
TI MMSUM Digital Twins: A Multi-view Multi-modality Summarization Framework
   for Sporting Events
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Digital twins; sporting events; multimedia; summarization; subjective;
   multi-view; sentiment analysis; popularity prediction; social media;
   social events
AB Sporting events generate a massive amount of traffic on social media with live moment-to-moment accounts as any given situation unfolds. The generated data are intensified by fans feelings, reactions, and subjective opinions towards what happens during the event, all of which are based on their individual points of view. Analyzing and summarizing this data will generate a comprehensive overview of the event in terms of how the event evolves and how fans react and view the event based on their perspectives. Previously, most of the summarization works ignore fan reactions and subjective opinions, and focus primarily on generating an objective-view summary. We believe that an effective and useful summary should consider human reactions, sentiment, and point of view, as opposed to simply describing what happens during the event. Accordingly, in this work, we propose MMSUM Digital Twins: a summarization framework that is capable of generating a multi-view multi-modal summary for sporting events in real-time. The proposed digital twins-based framework consists of four main components: sub-event recognition which detects the event's key moments, tweet categorization. which determines which team the tweets' writers support and assigns tweets to their teams, sentiment analysis to track fans' state of mind, and image popularity prediction for selecting representative images. Furthermore, the MMSUM employs a visual-filtering model to address the issue of noisy images that inundate social media, compromising the summarization quality. We leverage the knowledge of sport fans to evaluate the generated multi-view summarization through an online user study. The experiment results confirm the effectiveness of our proposed approach for summarizing sporting events by considering multimedia data, sentiment, and subjective views of the event.
C1 [Aloufi, Samah; El Saddik, Abdulmotaleb] Univ Ottawa, Multimedia Commun Res Lab MCRLab, Ottawa, ON K1N 6N5, Canada.
   [Aloufi, Samah] Taibah Univ, Coll Comp Sci Arid Engn CCSE, Medina, Saudi Arabia.
C3 University of Ottawa; Taibah University
RP Aloufi, S (corresponding author), Univ Ottawa, Multimedia Commun Res Lab MCRLab, Ottawa, ON K1N 6N5, Canada.; Aloufi, S (corresponding author), Taibah Univ, Coll Comp Sci Arid Engn CCSE, Medina, Saudi Arabia.
EM salou102@uottawa.ca; elsaddik@uottawa.ca
CR Aloufi S, 2018, IEEE ACCESS, V6, P78609, DOI 10.1109/ACCESS.2018.2885117
   Aloufi S, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030631
   [Anonymous], 2013, P 7 INT AAAI C WEBL
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Bin Guo, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3130920
   Bischke Benjamin, 2019, BIG DATA ANAL LARGE, P157
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Chakrabarti D., 2011, ICWSM
   Corney David, 2014, ICMR 2014 1 INT WORK
   El Saddik A., 2019, IEEE COMSOC MMTC Commun. Front, V14, P39
   El Saddik A, 2018, IEEE MULTIMEDIA, V25, P87, DOI 10.1109/MMUL.2018.023121167
   Gratch J, 2015, INT CONF AFFECT, P898, DOI 10.1109/ACII.2015.7344681
   Inouye D., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P298, DOI 10.1109/PASSAT/SocialCom.2011.31
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kubo M, 2013, 2013 IEEE/WIC/ACM INTERNATIONAL JOINT CONFERENCES ON WEB INTELLIGENCE (WI) AND INTELLIGENT AGENT TECHNOLOGIES (IAT), VOL 1, P527, DOI 10.1109/WI-IAT.2013.74
   Lee Michael, 2017, THESIS QUEENSLAND U
   Liang-Chi Hsieh, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P949, DOI 10.1109/ICME.2012.135
   Lienhart R, 2002, J ELECTRON IMAGING, V11, P445, DOI 10.1117/1.1502259
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Marcelino G, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P10, DOI 10.1145/3206025.3206053
   Marcus A, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P227
   McParlane P.J., 2014, P 23 ACM INT C INF K, P1459
   Meladianos P., 2015, P INT AAAI C WEB SOC
   Meladianos P, 2018, LECT NOTES COMPUT SC, V10772, P481, DOI 10.1007/978-3-319-76941-7_36
   Nichols J., 2012, Summarizing sporting events using Twitter, P189, DOI DOI 10.1145/2166966.2166999
   Phuvipadawat S., 2010, Proceedings of the 2010 IEEE/ACM International Conference on Web Intelligence-Intelligent Agent Technology - Workshops (WI-IAT 2010), P120, DOI 10.1109/WI-IAT.2010.205
   Qian XM, 2019, KNOWL-BASED SYST, V164, P107, DOI 10.1016/j.knosys.2018.10.028
   Schinas M, 2016, INT J MULTIMED INF R, V5, P51, DOI 10.1007/s13735-015-0089-9
   Sharifi B., 2010, HUMAN LANGUAGE TECHN, P685
   Shih HC, 2018, IEEE T CIRC SYST VID, V28, P1212, DOI 10.1109/TCSVT.2017.2655624
   Tagawa Yuuki, 2018, SPORTS GAME SUMMARIZ, P65, DOI [10.1007/978-3-319-70636-8_5, DOI 10.1007/978-3-319-70636-8_5]
   Tang Anthony., 2012, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. CHI '12, P1569
   Van der Lee C., 2017, ACL Anthology, P95, DOI DOI 10.18653/V1/W17-3513
   Van Oorschot Guido, 2012, P DETECTION REPRESEN
   Wang F, 2006, LECT NOTES COMPUT SC, V4071, P473
   Zhang MY, 2014, 2014 INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATION AND SENSOR NETWORK (WCSN), P213, DOI 10.1109/WCSN.2014.50
   Zubiaga Arkaitz., 2012, Proceedings of the 23rd ACM Conference on Hypertext and Social Media, P319, DOI DOI 10.1145/2309996.2310053
NR 38
TC 7
Z9 7
U1 3
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 5
DI 10.1145/3462777
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900005
DA 2024-07-18
ER

PT J
AU Fu, YF
   Yu, HC
   Yeh, CK
   Lee, TY
   Zhang, JJ
AF Fu, Yunfei
   Yu, Hongchuan
   Yeh, Chih-Kuo
   Lee, Tong-Yee
   Zhang, Jian J.
TI Fast Accurate and Automatic Brushstroke Extraction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Brushstroke extraction; painting authentication; hard and soft
   segmentation; Pix2Pix network
AB Brushstrokes are viewed as the artist's "handwriting" in a painting. In many applications such as style learning and transfer, mimicking painting, and painting authentication, it is highly desired to quantitatively and accurately identify brushstroke characteristics from old masters' pieces using computer programs. However, due to the nature of hundreds or thousands of intermingling brushstrokes in the painting, it still remains challenging. This article proposes an efficient algorithm for brush Stroke extraction based on a Deep neural network, i.e., DStroke. Compared to the state-of-the-art research, the main merit of the proposed DStroke is to automatically and rapidly extract brushstrokes from a painting without manual annotation, while accurately approximating the real brushstrokes with high reliability. Herein, recovering the faithful soft transitions between brushstrokes is often ignored by the other methods. In fact, the details of brushstrokes in a master piece of painting (e.g., shapes, colors, texture, overlaps) are highly desired by artists since they hold promise to enhance and extend the artists' powers, just like microscopes extend biologists' powers. To demonstrate the high efficiency of the proposed DStroke, we perform it on a set of real scans of paintings and a set of synthetic paintings, respectively. Experiments show that the proposed DStroke is noticeably faster and more accurate at identifying and extracting brushstrokes, outperforming the other methods.
C1 [Fu, Yunfei] iArt Ai, Shenzhen, Peoples R China.
   [Yu, Hongchuan; Zhang, Jian J.] Bournemouth Univ, Natl Ctr Comp Animat, Poole, Dorset, England.
   [Yeh, Chih-Kuo] Zhaoqing Univ, Sch Comp Sci & Software, Zhaoqing 701, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
C3 Bournemouth University; Zhaoqing University; National Cheng Kung
   University
RP Fu, YF (corresponding author), iArt Ai, Shenzhen, Peoples R China.
EM fuyunfei1991@gmail.com; hyu@bournemouth.ac.uk; simpson.ycg@gmail.com;
   tonylee@ncku.edu.tw; jzhang@bournemouth.ac.uk
RI Yeh, Chih-Kuo/JBS-2228-2023
FU EU-H2020-MSCA project: AniAge [691215]; Ministry of Science and
   Technology, Taiwan [107-2221-E-006-196-MY3, 108-2221-E-006 -038 -MY3]
FX This research was supported by EU-H2020-MSCA project: AniAge (No.
   691215) and the grant of the Ministry of Science and Technology, Taiwan
   (Grant No. 107-2221-E-006-196-MY3 and Grant No. 108-2221-E-006 -038
   -MY3).
CR Aharoni-Mack E., 2017, Proceedings of the Symposium on Non-Photorealistic Animation and Rendering, NPAR'17, DOI DOI 10.1145/3092919.3092926
   Aksoy Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201275
   Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32
   Arbeláez P, 2009, PROC CVPR IEEE, P2294, DOI 10.1109/CVPRW.2009.5206707
   Berezhnoy IE, 2009, MACH VISION APPL, V20, P1, DOI 10.1007/s00138-007-0098-7
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Cook R. L., 1981, Computer Graphics, V15, P307, DOI 10.1145/965161.806819
   Fu YF, 2019, IEEE T VIS COMPUT GR, V25, P2763, DOI 10.1109/TVCG.2018.2860004
   Gooch B., 2002, 2 INT S NONPHOTOREAL, P83
   Han ZY, 2018, MED IMAGE ANAL, V50, P23, DOI 10.1016/j.media.2018.08.005
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hegde S, 2013, COMPUT ANIMAT VIRT W, V24, P43, DOI 10.1002/cav.1435
   Hertzmann A, 2003, IEEE COMPUT GRAPH, V23, P70, DOI 10.1109/MCG.2003.1210867
   Hertzmann A., 2002, NPAR, P91, DOI 10.1145/508530.508546
   Hurtut T., 2010, 2D ARTISTIC IMAGES A
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ivanova Krassimira., 2010, P MED C INF SYST 201
   Jing YC, 2018, LECT NOTES COMPUT SC, V11217, P244, DOI 10.1007/978-3-030-01261-8_15
   Koyama Y, 2018, COMPUT GRAPH FORUM, V37, P397, DOI 10.1111/cgf.13577
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lamberti F, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-53
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P1699, DOI 10.1109/TPAMI.2008.168
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li CC, 2009, IEEE J-STSP, V3, P236, DOI 10.1109/JSTSP.2009.2015077
   Li J, 2012, IEEE T PATTERN ANAL, V34, P1159, DOI 10.1109/TPAMI.2011.203
   Lu JW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461998
   Lutz S, 2018, P BRIT MACH VIS C 20
   McCann J, 2012, COMPUT GRAPH FORUM, V31, P469, DOI 10.1111/j.1467-8659.2012.03026.x
   McCann J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531390
   Nie Liqiang., 2019, MULTIMODAL LEARNING, DOI [10.2200/S00938ED1V01Y201907IVM020, DOI 10.2200/S00938ED1V01Y201907IVM020]
   Porter T., 1984, Computers & Graphics, V18, P253
   Richardt C, 2014, COMPUT GRAPH FORUM, V33, P11, DOI 10.1111/cgf.12408
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shamir L, 2015, INT J ARTS TECHNOL, V8, P1, DOI 10.1504/IJART.2015.067389
   Singaraju D, 2011, IEEE T PATTERN ANAL, V33, P1295, DOI 10.1109/TPAMI.2010.206
   Tan JC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275054
   Tan JC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2988229
   van der Maaten LJP, 2010, PROC SPIE, V7798, DOI 10.1117/12.863082
   Xie N, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2531
   Xie N, 2013, IEICE T INF SYST, VE96D, P1134, DOI 10.1587/transinf.E96.D.1134
   Xu SH, 2006, ACM T GRAPHIC, V25, P239, DOI 10.1145/1138450.1138454
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng Ningyuan., 2019, P INT C LEARN REPR 2
NR 46
TC 5
Z9 5
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 44
DI 10.1145/3429742
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000010
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, JH
   Song, XM
   Nie, LQ
   Gan, T
   Ma, J
AF Liu, Jinhuan
   Song, Xuemeng
   Nie, Liqiang
   Gan, Tian
   Ma, Jun
TI An End-to-End Attention-Based Neural Model for Complementary Clothing
   Matching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE End-to-end; feature-level attention; complementary clothing matching
AB In modern society, people tend to prefer fashionable and decent outfits that can meet more than basic physiological needs. In fact, a proper outfit usually relies on good matching among complementary fashion items (e.g., the top, bottom, and shoes) that compose it, which thus propels us to investigate the automatic complementary clothing matching scheme. However, this is non-trivial due to the following challenges. First, the main challenge lies in how to accurately model the compatibility between complementary fashion items (e.g., the top and bottom) that come from the heterogeneous spaces with multi-modalities (e.g., the visual modality and textual modality). Second, since different features (e.g., the color, style, and pattern) of fashion items may contribute differently to compatibility modeling, how to encode the confidence of different pairwise features presents a tough challenge. Third, how to jointly learn the latent representation of multi-modal data and the compatibility between complementary fashion items contributes to the last challenge. Toward this end, in this work, we present an end-to-end attention-based neural framework for the compatibility modeling, where we introduce a feature-level attention model to adaptively learn the confidence for different pairwise features. Extensive experiments on a public available real-world dataset show the superiority of our model over state-of-the-art methods.
C1 [Liu, Jinhuan; Song, Xuemeng; Nie, Liqiang; Gan, Tian; Ma, Jun] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
C3 Shandong University
RP Song, XM (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
EM liujinhuan.sdu@gmail.com; sxmustc@gmail.com; nieliqiang@gmail.com;
   gantian@sdu.edu.cn; majun@sdu.edu.cn
FU National Basic Research Program of China (973 Program) [2015CB352502];
   National Natural Science Foundation of China [61772310, 61702300,
   61702302, 61672322]; Young Scholars Program of Shandong University;
   Project of Thousand Youth Talents 2016
FX This work was supported by the National Basic Research Program of China
   (973 Program), no. 2015CB352502; the National Natural Science Foundation
   of China, nos. 61772310, 61702300, 61702302, and 61672322; Young
   Scholars Program of Shandong University; and the Project of Thousand
   Youth Talents 2016.
CR [Anonymous], ARXIV171201262
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2015, PROCIEEE CONFCOMPUT
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Dong X, 2017, AAAI CONF ARTIF INTE, P1309
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   He J, 2016, AAAI CONF ARTIF INTE, P137
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He XT, 2017, AAAI CONF ARTIF INTE, P4075
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Iwata Tomoharu, 2011, P 22 INT JOINT C ON, V3, P2262
   Jagadeesh V, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1925, DOI 10.1145/2623330.2623332
   Jiang SH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152114
   Jiang YG, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3184745
   Kim Y, 2014, ARXIV PREPRINT ARXIV, DOI 10.3115/v1/D14-1181
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li J, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1419, DOI 10.1145/3132847.3132926
   Li XF, 2017, EURASIP J BIOINFORM, DOI [10.1186/s13637-017-0061-5, 10.1186/s13637-017-0061]
   Li XL, 2018, IEEE T CYBERNETICS, V48, P1923, DOI 10.1109/TCYB.2017.2718579
   Li XL, 2016, MULTIMED TOOLS APPL, V75, P11961, DOI 10.1007/s11042-015-2735-x
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Lin SH, 2017, AAAI CONF ARTIF INTE, P1424
   Lin Yujie, 2018, ARXIV180608977
   Lin Zhouhan, 2017, A structured self-attentive sentence embedding
   Liu CX, 2017, AAAI CONF ARTIF INTE, P4176
   Liu T.Y., 2017, P ANN INT ACM SIGIR, V763, P772
   Liu Y, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1792
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Ma YH, 2017, AAAI CONF ARTIF INTE, P38
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Min XK, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996463
   Mnih V, 2014, ADV NEUR IN, V27
   Nakamura T., 2018, ARXIV180703133
   Pang L, 2016, AAAI CONF ARTIF INTE, P2793
   Rendle S., 2009, P 25 C UNC ART INT, P452, DOI DOI 10.5555/1795114.1795167
   Seo S, 2017, PROCEEDINGS OF THE ELEVENTH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'17), P297, DOI 10.1145/3109859.3109890
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Song X, 2018, NEURAL COMPATIBILITY
   Song XM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P753, DOI 10.1145/3123266.3123314
   Szegedy C., 2017, AAAI, V4, P12
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Wang AR, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115932
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
NR 50
TC 7
Z9 7
U1 0
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 114
DI 10.1145/3368071
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800014
DA 2024-07-18
ER

PT J
AU Liu, SG
   Huang, ZQ
AF Liu, Shiguang
   Huang, Ziqing
TI Efficient Image Hashing with Geometric Invariant Vector Distance for
   Copy Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image hashing; geometric invariant vector distance; robustness;
   discrimination; copy detection
ID RING PARTITION; ROBUST; SECURE; QUANTIZATION; WATERMARKING; RECOGNITION;
   ALGORITHMS; SCHEME
AB Hashing method is an efficient technique of multimedia security for content protection. It maps an image into a content-based compact code for denoting the image itself While most existing algorithms focus on improving the classification between robustness and discrimination, little attention has been paid to geometric invariance under normal digital operations, and therefore results in quite fragile to geometric distortion when applied in image copy detection. In this article, a novel effective image hashing method is proposed based on geometric invariant vector distance in both spatial domain and frequency domain. First, the image is preprocessed by some joint operations to extract robust features. Then, the preprocessed image is randomly divided into several overlapping blocks under a secret key, and two different feature matrices are separately obtained in the spatial domain and frequency domain through invariant moment and low frequency discrete cosine transform coefficients. Furthermore, the invariant distances between vectors in feature matrices are calculated and quantified to form a compact hash code. We conduct various experiments to demonstrate that the proposed hashing not only reaches good classification between robustness and discrimination, but also resists most geometric distortion in image copy detection. In addition, both receiver operating characteristics curve comparisons and mean average precision in copy detection clearly illustrate that the proposed hashing method outperforms state-of-the-art algorithms.
C1 [Liu, Shiguang; Huang, Ziqing] Tianjin Univ, Coll Intelligence & Comp, Sch Comp Sci & Technol, 135 Yaguan Rd,Haihe Educ Pk, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Sch Comp Sci & Technol, 135 Yaguan Rd,Haihe Educ Pk, Tianjin 300350, Peoples R China.
EM lsg@tju.edu.cn; skyhuangzq@163.com
FU Natural Science Foundation of China [61672375, 61170118]
FX This work was partly supported by the Natural Science Foundation of
   China under grant nos. 61672375 and 61170118.
CR Ahmed F, 2010, SIGNAL PROCESS, V90, P1456, DOI 10.1016/j.sigpro.2009.05.024
   Balado F, 2007, IEEE T INF FOREN SEC, V2, P254, DOI 10.1109/TIFS.2007.897258
   Davarzani R, 2016, MULTIMED TOOLS APPL, V75, P4639, DOI 10.1007/s11042-015-2496-6
   De Roover C, 2005, IEEE T SIGNAL PROCES, V53, P4020, DOI 10.1109/TSP.2005.855414
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Gordo A, 2014, IEEE T PATTERN ANAL, V36, P33, DOI 10.1109/TPAMI.2013.101
   Hsiao JH, 2007, IEEE T IMAGE PROCESS, V16, P2069, DOI 10.1109/TIP.2007.900099
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   Huang X, 2016, IEEE TRUST BIG, P14, DOI [10.1109/TrustCom.2016.39, 10.1109/TrustCom.2016.0040]
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Kang WC, 2016, AAAI CONF ARTIF INTE, P1230
   Khelifi F, 2010, IEEE T IMAGE PROCESS, V19, P981, DOI 10.1109/TIP.2009.2038637
   Kim C, 2003, SIGNAL PROCESS-IMAGE, V18, P169, DOI 10.1016/S0923-5965(02)00130-3
   Kozat SS, 2004, IEEE IMAGE PROC, P3443, DOI 10.1109/ICIP.2004.1421855
   Lefebvre F., 2002, Signal Processing Conference, 2002 11th European, P1
   Li KH, 2018, ELECTRON J DIFFER EQ
   Li R, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1352012.1352014
   Li YN, 2012, IEEE T IMAGE PROCESS, V21, P1963, DOI 10.1109/TIP.2011.2171698
   Lin CY, 2001, IEEE T CIRC SYST VID, V11, P153, DOI 10.1109/76.905982
   Ling HF, 2012, IEEE MULTIMEDIA, V19, P60, DOI 10.1109/MMUL.2011.75
   Liu XL, 2017, IEEE T IMAGE PROCESS, V26, P5324, DOI 10.1109/TIP.2017.2729896
   Liu YL, 2016, RADIOENGINEERING, V25, P556, DOI 10.13164/re.2016.0556
   Lv XD, 2012, IEEE T INF FOREN SEC, V7, P1081, DOI 10.1109/TIFS.2012.2190594
   Lv XD, 2009, EURASIP J INF SECUR, DOI 10.1155/2009/859859
   Manuel S, 2011, DESIGN CODE CRYPTOGR, V59, P247, DOI 10.1007/s10623-010-9458-9
   Menezes A., 1998, HDB APPL CRYPTOGRAPH, P683
   Monga V, 2007, IEEE T INF FOREN SEC, V2, P376, DOI 10.1109/TIFS.2007.902670
   Nguyen DQ, 2011, LECT NOTES COMPUT SC, V7025, P186, DOI 10.1007/978-3-642-24712-5_17
   Ou Y, 2009, 2009 INTERNATIONAL SYMPOSIUM ON INTELLIGENT SIGNAL PROCESSING AND COMMUNICATION SYSTEMS (ISPACS 2009), P595, DOI 10.1109/ISPACS.2009.5383770
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   Podilchuk CI, 2001, IEEE SIGNAL PROC MAG, V18, P33, DOI 10.1109/79.939835
   Qin C, 2018, INFORM SCIENCES, V423, P284, DOI 10.1016/j.ins.2017.09.060
   Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Shijun Xiang, 2010, Proceedings 2010 Second International Conference on Multimedia Information Networking and Security (MINES 2010), P653, DOI 10.1109/MINES.2010.142
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2018, PATTERN RECOGN, V75, P175, DOI 10.1016/j.patcog.2017.03.021
   Swaminathan A, 2006, IEEE T INF FOREN SEC, V1, P215, DOI 10.1109/TIFS.2006.873601
   Tang ZJ, 2019, IEEE T KNOWL DATA EN, V31, P549, DOI 10.1109/TKDE.2018.2837745
   Tang ZJ, 2018, NEUROCOMPUTING, V308, P147, DOI 10.1016/j.neucom.2018.04.057
   Tang ZJ, 2017, IETE TECH REV, V34, P440, DOI 10.1080/02564602.2016.1200500
   Tang ZJ, 2017, SIGNAL PROCESS, V137, P240, DOI 10.1016/j.sigpro.2017.02.008
   Tang ZJ, 2016, COMPUT SECUR, V62, P133, DOI 10.1016/j.cose.2016.07.006
   Tang ZJ, 2016, AEU-INT J ELECTRON C, V70, P833, DOI 10.1016/j.aeue.2016.03.010
   Tang ZJ, 2016, IEEE T INF FOREN SEC, V11, P200, DOI 10.1109/TIFS.2015.2485163
   Tang ZJ, 2015, DIGIT SIGNAL PROCESS, V43, P17, DOI 10.1016/j.dsp.2015.05.002
   Tang ZJ, 2014, OPTIK, V125, P5102, DOI 10.1016/j.ijleo.2014.05.015
   Tang ZJ, 2014, IET IMAGE PROCESS, V8, P142, DOI 10.1049/iet-ipr.2013.0332
   Tang ZJ, 2014, IEEE T KNOWL DATA EN, V26, P711, DOI 10.1109/TKDE.2013.45
   Turpin A., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P11, DOI 10.1145/1148170.1148176
   Venkatesan R, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P664, DOI 10.1109/ICIP.2000.899541
   Wang XM, 2015, NEURAL DEV, V10, DOI 10.1186/s13064-015-0035-9
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang XY, 2017, J PARENTER ENTERAL N, V24, P4
   Wolberg G., 2002, P INT C IM PROC, V1, P493
   Woods R. E., 2007, DIGITAL IMAGE PROCES, V3
   Yan CP, 2016, IEEE T INF FOREN SEC, V11, P2664, DOI 10.1109/TIFS.2016.2594136
   Yan CP, 2016, SIGNAL PROCESS, V121, P1, DOI 10.1016/j.sigpro.2015.10.027
   Yang LJ, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240141
   Ze Wang J., 1997, International Journal on Digital Libraries, V1, P311, DOI 10.1007/s007990050026
   Zhao Y., 2013, BIODISCOVERY, V4, P1, DOI DOI 10.7750/BI0DISC0VERY.2013.8.4
   Zhou ZL, 2017, IEEE T INF FOREN SEC, V12, P48, DOI 10.1109/TIFS.2016.2601065
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
NR 63
TC 27
Z9 28
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 106
DI 10.1145/3355394
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800006
DA 2024-07-18
ER

PT J
AU da Costa, RIT
   Luizelli, MC
   Petrangeli, S
   Vega, MT
   van der Hooft, J
   Wauters, T
   De Turck, F
   Gaspary, LP
AF Tavares da Costa Filho, Roberto Iraja
   Luizelli, Marcelo Caggiani
   Petrangeli, Stefano
   Vega, Maria Torres
   van der Hooft, Jeroen
   Wauters, Tim
   De Turck, Filip
   Gaspary, Luciano Paschoal
TI Dissecting the Performance of VR Video Streaming through the VR-EXP
   Experimentation Platform
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Virtual reality; adaptive streaming; quality of experience; quality of
   service
AB To cope with the massive bandwidth demands of Virtual Reality (VR) video streaming, both the scientific community and the industry have been proposing optimization techniques such as viewport-aware streaming and tile-based adaptive bitrate heuristics. As most of the VR video traffic is expected to be delivered through mobile networks, a major problem arises: both the network performance and VR video optimization techniques have the potential to influence the video playout performance and the Quality of Experience (QoE). However, the interplay between them is neither trivial nor has it been properly investigated. To bridge this gap, in this article, we introduce VR-EXP, an open-source platform for carrying out VR video streaming performance evaluation. Furthermore, we consolidate a set of relevant VR video streaming techniques and evaluate them under variable network conditions, contributing to an in-depth understanding of what to expect when different combinations are employed. To the best of our knowledge, this is the first work to propose a systematic approach, accompanied by a software toolkit, which allows one to compare different optimization techniques under the same circumstances. Extensive evaluations carried out using realistic datasets demonstrate that VR-EXP is instrumental in providing valuable insights regarding the interplay between network performance and VR video streaming optimization techniques.
C1 [Tavares da Costa Filho, Roberto Iraja; Gaspary, Luciano Paschoal] Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.
   [Luizelli, Marcelo Caggiani] Fed Univ Pampa, Bage, RS, Brazil.
   [Petrangeli, Stefano] Adobe Res, San Francisco, CA USA.
   [Vega, Maria Torres; van der Hooft, Jeroen; Wauters, Tim; De Turck, Filip] Ghent Univ imec, Dept Informat Technol EA05, Technol Pk Zwijnaarde 15, Ghent, Belgium.
   [Tavares da Costa Filho, Roberto Iraja; Gaspary, Luciano Paschoal] Av Bento Goncalves 9500,Campus Vale,Bloco 4, BR-91501970 Porto Alegre, RS, Brazil.
   [Luizelli, Marcelo Caggiani] Av Tiaraju 810, BR-97546550 Alegrete, RS, Brazil.
   [Petrangeli, Stefano] 345 Pk Ave, San Jose, CA 95110 USA.
C3 Universidade Federal do Rio Grande do Sul; Universidade Federal do
   Pampa; Adobe Systems Inc.; Ghent University; IMEC
RP da Costa, RIT (corresponding author), Av Bento Goncalves 9500,Campus Vale,Bloco 4, BR-91501970 Porto Alegre, RS, Brazil.
EM roberto.costa@inf.ufrgs.br; marceloluizelli@unipampa.edu.br;
   petrange@adobe.com; maria.torresvega@ugent.be;
   jeroen.vanderhooft@ugent.be; tim.wauters@ugent.be;
   filip.deturck@ugent.be; paschoal@inf.ufrgs.br
RI Vega, Maria Torres/AAL-1171-2020
OI Vega, Maria Torres/0000-0002-5656-6607; Tavares da Costa Filho, Roberto
   Iraja/0000-0003-4804-8954; De Turck, Filip/0000-0003-4824-1199
FU fund for Scientific Research-Flanders (FWO-V) [G025615N]; grant of the
   Research Foundation - Flanders (FWO); CAPES; CNPq; FAPERGS; IFSul
FX This research was performed partially within the project G025615N
   "Optimized source coding for multiple terminals in self-organizing
   networks" from the fund for Scientific Research-Flanders (FWO-V). Maria
   Torres Vega is funded by a grant of the Research Foundation - Flanders
   (FWO). This work was also partially funded by CAPES, CNPq, FAPERGS, and
   IFSul.
CR Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   Almquist M, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P258, DOI 10.1145/3204949.3204970
   [Anonymous], 2019, TECHNICAL REPORT
   [Anonymous], JVET COMMON TEST CON
   Chen ZZ, 2018, SIGNAL PROCESS, V146, P66, DOI 10.1016/j.sigpro.2018.01.004
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Dimopoulos G., 2016, P 2016 INT MEAS C SA, P513, DOI DOI 10.1145/2987443.2987459
   dos Santos GL, 2007, 2007 10TH IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2009), VOLS 1 AND 2, P246, DOI 10.1109/INM.2007.374789
   Eunyoung Jeong, 2018, 2018 Tenth International Conference on Ubiquitous and Future Networks (ICUFN), P679, DOI 10.1109/ICUFN.2018.8436981
   Fan C, 2017, SIXTEENTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, P67
   Graf M, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P261, DOI 10.1145/3083187.3084016
   He J, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P64, DOI 10.1145/3204949.3204957
   Hosseini M, 2016, IEEE INT SYM MULTIM, P107, DOI [10.1109/ISM.2016.45, 10.1109/ISM.2016.0028]
   Hou Xueshi, 2018, P 2018 MORNING WORKS, P20
   Hristova H., 2018, 2018 IEEE 20 INT WOR, P1
   Kohavi R., 1995, IJCAI-95. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, P1137
   Ma L, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Morton A., 2016, TECHNICAL REPORT
   Petrangeli S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P157, DOI 10.1109/AIVR.2018.00033
   Petrangeli S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P306, DOI 10.1145/3123266.3123453
   Petrangeli S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818361
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P123, DOI 10.1145/3204949.3204953
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Stangherlin K, 2011, 2011 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P1044, DOI 10.1109/WCNC.2011.5779279
   da Costa RIT, 2016, IEEE GLOB COMM CONF
   da Costa RIT, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P270, DOI 10.1145/3204949.3204966
   TELCO, 2018, MOB OP MARK SHAR BRA
   Nguyen TC, 2018, IEEE COMMUN LETT, V22, P1858, DOI 10.1109/LCOMM.2018.2848915
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Viitanen M, 2015, IEEE INT SYMP CIRC S, P1662, DOI 10.1109/ISCAS.2015.7168970
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Yin XQ, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P325, DOI 10.1145/2785956.2787486
   Zhou C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3209660
NR 36
TC 14
Z9 14
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 111
DI 10.1145/3360286
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800011
OA Green Published
DA 2024-07-18
ER

PT J
AU Siarohin, A
   Zen, G
   Majtanovic, C
   Alameda-Pineda, X
   Ricci, E
   Sebe, N
AF Siarohin, Aliaksandr
   Zen, Gloria
   Majtanovic, Cveta
   Alameda-Pineda, Xavier
   Ricci, Elisa
   Sebe, Nicu
TI Increasing Image Memorability with Neural Style Transfer
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; style transfer; memorability
ID MEMORY; COMPLEXITY; AMYGDALA; EMOTION
AB Recent works in computer vision and multimedia have shown that image memorability can be automatically inferred exploiting powerful deep-learning models. This article advances the state of the art in this area by addressing a novel and more challenging issue: "Given an arbitrary input image, can we make it more memorable?" To tackle this problem, we introduce an approach based on an editing-by-applying-filters paradigm: given an input image, we propose to automatically retrieve a set of "style seeds," i.e., a set of style images that, applied to the input image through a neural style transfer algorithm, provide the highest increase in memorability. We show the effectiveness of the proposed approach with experiments on the publicly available LaMem dataset, performing both a quantitative evaluation and a user study. To demonstrate the flexibility of the proposed framework, we also analyze the impact of different implementation choices, such as using different state-of-the-art neural style transfer methods. Finally, we show several qualitative results to provide additional insights on the link between image style and memorability.
C1 [Siarohin, Aliaksandr; Zen, Gloria; Majtanovic, Cveta; Ricci, Elisa; Sebe, Nicu] Univ Trento, Via Sommarive 9, I-38123 Trento, Italy.
   [Majtanovic, Cveta] Univ Novi Sad, Novi Sad, Serbia.
   [Alameda-Pineda, Xavier] INRIA, 655 Ave Europe, F-38330 Montbonnot St Martin, France.
   [Ricci, Elisa] FBK, Trento, Italy.
C3 University of Trento; University of Novi Sad; Inria; Fondazione Bruno
   Kessler
RP Siarohin, A (corresponding author), Univ Trento, Via Sommarive 9, I-38123 Trento, Italy.
EM siarohin@unitn.it; gloria.zen@unitn.it; cveta.majtanovic@unitn.it;
   xavier.alameda-pineda@inria.fr; e.ricci@unitn.it; niculae.sebe@unitn.it
RI Ricci, Elisa/IYS-6532-2023; Sebe, Niculae/KEC-2000-2024
OI Sebe, Niculae/0000-0002-6597-7248; Ricci, Elisa/0000-0002-0228-1147
FU Fondazione Caritro
FX We gratefully acknowledge Fondazione Caritro for supporting SMARTourism
   project and NVIDIA Corporation for the donation of the TitanX GPUs.
CR AITKEN PP, 1974, J EXP PSYCHOL, V103, P240, DOI 10.1037/h0036787
   [Anonymous], P IS T SPIE EL IM C
   [Anonymous], P C EXH COMP GRAPH I
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2017, BRIT MACH VIS C 2017
   [Anonymous], 2018, INT J COMPUT VISION, DOI DOI 10.1007/s11263-018-1089-z
   [Anonymous], P ACM MULT C
   [Anonymous], ARXIV160301768
   [Anonymous], 2018, CENTURY EXCELLENCE M
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], PREDICTING HUMAN BEH
   [Anonymous], ATTEN PERCEPT PSYCHO
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P INT C MULT RETR IC
   [Anonymous], P INT C MACH LEARN I
   [Anonymous], INT C MACH LEARN ICL
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2010, P ACM INT C MULT
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P EUR C COMP VIS WOR
   Berlyne D.E., 1960, CONFLICT AROUSAL CUR, P350, DOI DOI 10.1037/11164-000
   BERLYNE DE, 1963, CAN J PSYCHOLOGY, V17, P274, DOI 10.1037/h0092883
   Bylinskii Z, 2015, VISION RES, V116, P165, DOI 10.1016/j.visres.2015.03.005
   Cahill L, 1995, CONSCIOUS COGN, V4, P410, DOI 10.1006/ccog.1995.1048
   EISENMAN R, 1966, PERCEPT MOTOR SKILL, V23, P1167, DOI 10.2466/pms.1966.23.3f.1167
   Fei MJ, 2018, NEUROCOMPUTING, V275, P1911, DOI 10.1016/j.neucom.2017.10.030
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gygli M, 2013, IEEE I CONF COMP VIS, P1633, DOI 10.1109/ICCV.2013.205
   He L, 2015, SIGNAL IMAGE VIDEO P, V9, P1965, DOI 10.1007/s11760-014-0691-y
   Huo J, 2016, PROCEEDINGS OF THE 2016 SAI COMPUTING CONFERENCE (SAI), P264, DOI 10.1109/SAI.2016.7555993
   Isola P., 2011, ADV NEURAL INFORM PR
   Isola P, 2014, IEEE T PATTERN ANAL, V36, P1469, DOI 10.1109/TPAMI.2013.200
   Kim HR, 2016, COMPUT GRAPH FORUM, V35, P209, DOI 10.1111/cgf.13018
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Maren S, 1999, TRENDS NEUROSCI, V22, P561, DOI 10.1016/S0166-2236(99)01465-4
   Peng KC, 2015, P IEEE C COMP VIS PA
   Phelps EA, 2004, CURR OPIN NEUROBIOL, V14, P198, DOI 10.1016/j.conb.2004.03.015
   Sanocki T, 2011, PERCEPTION, V40, P635, DOI 10.1068/p6655
   Sartori A, 2015, ACM T INTERACT INTEL, V5, DOI 10.1145/2768209
   Shekhar Sumit, 2017, U. S. Patent, Patent No. [9,805,269, 9805269]
   Sheng Lu, 2018, P IEEE C COMP VIS PA
   STANDING L, 1970, PSYCHON SCI, V19, P73, DOI 10.3758/BF03337426
   STANDING L, 1973, Q J EXP PSYCHOL, V25, P207, DOI 10.1080/14640747308400340
   Ulyanov D., 2017, P IEEE C COMP VIS PA
NR 47
TC 6
Z9 6
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 42
DI 10.1145/3311781
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IM3ZV
UT WOS:000477935400013
OA Green Published
DA 2024-07-18
ER

PT J
AU Hossain, MS
   Amin, SU
   Alsulaiman, M
   Muhammad, G
AF Hossain, M. Shamim
   Amin, Syed Umar
   Alsulaiman, Mansour
   Muhammad, Ghulam
TI Applying Deep Learning for Epilepsy Seizure Detection and Brain Mapping
   Visualization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; epileptic seizure detection; electroencephalogram
ID NEURAL-NETWORKS; EEG; PREDICTION; POWER
AB Deep Convolutional Neural Network (CNN) has achieved remarkable results in computer vision tasks for end-to-end learning. We evaluate here the power of a deep CNN to learn robust features from raw Electroencephalogram (EEG) data to detect seizures. Seizures are hard to detect, as they vary both inter- and intra-patient. In this article, we use a deep CNN model for seizure detection task on an open-access EEG epilepsy dataset collected at the Boston Children's Hospital. Our deep learning model is able to extract spectral, temporal features from EEG epilepsy data and use them to learn the general structure of a seizure that is less sensitive to variations. For cross-patient EEG data, our method produced an overall sensitivity of 90.00%, specificity of 91.65%, and overall accuracy of 98.05% for the whole dataset of 23 patients. The system can detect seizures with an accuracy of 99.46%. Thus, it can be used as an excellent cross-patient seizure classifier. The results show that our model performs better than the previous state-of-the-art models for patient-specific and cross-patient seizure detection task. The method gave an overall accuracy of 99.65% for patient-specific data. The system can also visualize the special orientation of band power features. We use correlation maps to relate spectral amplitude features to the output in the form of images. By using the results from our deep learning model, this visualization method can be used as an effective multimedia tool for producing quick and relevant brain mapping images that can be used by medical experts for further investigation.
C1 [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, Res Chair Pervas & Mobile Comp, Riyadh 11543, Saudi Arabia.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 11543, Saudi Arabia.
   [Amin, Syed Umar; Alsulaiman, Mansour; Muhammad, Ghulam] King Saud Univ, Coll Comp & Informat Sci, Dept Comp Engn, Riyadh 11543, Saudi Arabia.
C3 King Saud University; King Saud University; King Saud University
RP Hossain, MS (corresponding author), King Saud Univ, Coll Comp & Informat Sci, Res Chair Pervas & Mobile Comp, Riyadh 11543, Saudi Arabia.; Hossain, MS (corresponding author), King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 11543, Saudi Arabia.
EM mshossain@ksu.edu.sa; samin@ksu.edu.sa; msuliman@ksu.edu.sa;
   ghulam@ksu.edu.sa
RI Hossain, M. Shamim/K-1362-2014; Guizani, Mohsen/AAX-4534-2021; Amin,
   Syed Umar/D-9632-2019; Huda, Prof Dr Mohammad Nurul Nurul/AAX-1111-2021;
   Muhammad, Ghulam/H-5884-2011
OI Hossain, M. Shamim/0000-0001-5906-9422; Guizani,
   Mohsen/0000-0002-8972-8094; Amin, Syed Umar/0000-0002-4718-0155;
   Muhammad, Ghulam/0000-0002-9781-3969
FU Deanship of Scientific Research at King Saud University, Riyadh, Saudi
   Arabia
FX The authors are grateful to the Deanship of Scientific Research at King
   Saud University, Riyadh, Saudi Arabia for funding this work through the
   Vice Deanship of Scientific Research Chairs.
CR Alhussein M, 2018, MOBILE NETW APPL, V23, P1624, DOI 10.1007/s11036-018-1113-0
   [Anonymous], 2016, 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)
   [Anonymous], ARXIV14126502
   [Anonymous], BERNST C 2016
   [Anonymous], 2017, EPILEPSY
   Bashivan P., 2016, Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks
   Canolty RT, 2006, SCIENCE, V313, P1626, DOI 10.1126/science.1128115
   Chávez M, 2003, J NEUROSCI METH, V124, P113, DOI 10.1016/S0165-0270(02)00367-9
   Clevert D-A, 2016, ARXIV151107289, V1511
   Duun-Henriksen J, 2012, CLIN NEUROPHYSIOL, V123, P84, DOI 10.1016/j.clinph.2011.06.001
   Echauz J, 2007, MED C CONTR AUTOMAT, P191
   Fatichah C, 2014, P INT CONF NAT COMPU, P186, DOI 10.1109/ICNC.2014.6975832
   Fergus P., 2016, Applied Computing and Informatics, V12, P70, DOI 10.1016/j.aci.2015.01.001
   Gogna A, 2017, IEEE T BIO-MED ENG, V64, P2196, DOI 10.1109/TBME.2016.2631620
   Greenfield L.J., 2012, Reading EEGs: a practical approach
   Hills M., 2014, TECHNICAL REPORT
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Johansen AR, 2016, INT CONF ACOUST SPEE, P754, DOI 10.1109/ICASSP.2016.7471776
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuhlmann L, 2009, ANN BIOMED ENG, V37, P2129, DOI 10.1007/s10439-009-9755-5
   LeCun Y., 1995, The handbook of brain theory and neural networks, V3361, DOI [10.5555/303568.303704, DOI 10.5555/303568.303704]
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li DH, 2016, IEEE VTS VEH TECHNOL, DOI 10.1109/VTCSpring.2016.7504310
   Litt B, 2002, LANCET NEUROL, V1, P22, DOI 10.1016/S1474-4422(02)00003-0
   Minasyan GR, 2010, J CLIN NEUROPHYSIOL, V27, P163, DOI 10.1097/WNP.0b013e3181e0a9b6
   Mormann F, 2000, PHYSICA D, V144, P358, DOI 10.1016/S0167-2789(00)00087-7
   Mormann F, 2005, CLIN NEUROPHYSIOL, V116, P569, DOI 10.1016/j.clinph.2004.08.025
   Mormann F, 2007, BRAIN, V130, P314, DOI 10.1093/brain/awl241
   Osorio I, 2009, EPILEPSY BEHAV, V16, P391, DOI 10.1016/j.yebeh.2009.08.024
   Panayiotopoulos C. P, 2010, CLIN GUIDE EPILEPTIC
   Park Y, 2011, EPILEPSIA, V52, P1761, DOI 10.1111/j.1528-1167.2011.03138.x
   Parvez MZ, 2015, IET SIGNAL PROCESS, V9, P467, DOI 10.1049/iet-spr.2013.0288
   Pierre T., 2016, MACH LEARN HEALTHC C, P178, DOI DOI 10.1162/NEC0.1997.9.8.1735
   Qi Y, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/703816
   Schirrmeister RT, 2017, HUM BRAIN MAPP, V38, P5391, DOI 10.1002/hbm.23730
   Shoeb  A., 2009, P ENG MED BIOL SOC A
   Shoeb A.H., 2009, Application of Machine Learning to Epileptic Seizure Onset Detection and Treatment, DOI DOI 10.1016/J.BSPC.2020.101856
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Supratak A, 2014, IEEE ENG MED BIO, P4184, DOI 10.1109/EMBC.2014.6944546
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tieng QM, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/2/026018
   Turner J., 2014, P AAAI SPRING S SER
   Tzallas AT., 2012, Epilepsyhistological, electroencephalographic and psychological aspects, P2027, DOI [10.5772/31597, DOI 10.5772/31597]
   Van Quyen ML, 2003, EPILEPSIA, V44, P30
   Wilson SB, 2004, CLIN NEUROPHYSIOL, V115, P2280, DOI 10.1016/j.clinph.2004.05.018
   Wu YX, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0169833
   Wulsin DF, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/3/036015
   Xun GX, 2015, IEEE INT C BIOINFORM, P325, DOI 10.1109/BIBM.2015.7359702
   Zabihi M, 2016, IEEE T NEUR SYS REH, V24, P386, DOI 10.1109/TNSRE.2015.2505238
   Zheng G, 2016, IEEE INT C BIOINF BI, P1, DOI 10.1109/BIBE.2016.11
NR 50
TC 181
Z9 189
U1 8
U2 101
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 10
DI 10.1145/3241056
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100010
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Liu, RY
   Zhao, Y
   Wei, SK
   Zheng, L
   Yang, Y
AF Liu, Ruoyu
   Zhao, Yao
   Wei, Shikui
   Zheng, Liang
   Yang, Yi
TI Modality-Invariant Image-Text Embedding for Image-Sentence Matching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image-text embedding; adversarial learning; retrieval
AB Performing direct matching among different modalities (like image and text) can benefit many tasks in computer vision, multimedia, information retrieval, and information fusion. Most of existing works focus on class-level image-text matching, called cross-modal retrieval, which attempts to propose a uniform model for matching images with all types of texts, for example, tags, sentences, and articles (long texts). Although cross-model retrieval alleviates the heterogeneous gap among visual and textual information, it can provide only a rough correspondence between two modalities. In this article, we propose a more precise image-text embedding method, image-sentence matching, which can provide heterogeneous matching in the instance level. The key issue for image-text embedding is how to make the distributions of the two modalities consistent in the embedding space. To address this problem, some previous works on the cross-model retrieval task have attempted to pull close their distributions by employing adversarial learning. However, the effectiveness of adversarial learning on image-sentence matching has not been proved and there is still not an effective method. Inspired by previous works, we propose to learn a modality-invariant image-text embedding for image-sentence matching by involving adversarial learning. On top of the triplet loss-based baseline, we design a modality classification network with an adversarial loss, which classifies an embedding into either the image or text modality. In addition, the multi-stage training procedure is carefully designed so that the proposed network not only imposes the image-text similarity constraints by ground-truth labels, but also enforces the image and text embedding distributions to be similar by adversarial learning. Experiments on two public datasets (Flickr30k and MSCOCO) demonstrate that our method yields stable accuracy improvement over the baseline model and that our results compare favorably to the state-of-the-art methods.
C1 [Liu, Ruoyu; Zhao, Yao; Wei, Shikui] Beijing Jiaotong Univ, 3 Shuangyuancun, Beijing 100044, Peoples R China.
   [Zheng, Liang] Australian Natl Univ, 115 North Rd, Acton, ACT 2601, Australia.
   [Yang, Yi] Univ Technol Sydney, 15 Broadway, Ultimo, NSW 2007, Australia.
C3 Beijing Jiaotong University; Australian National University; University
   of Technology Sydney
RP Zhao, Y (corresponding author), Beijing Jiaotong Univ, 3 Shuangyuancun, Beijing 100044, Peoples R China.
EM 12112062@bjtu.edu.cn; yzhao@bjtu.edu.cn; shkwei@bjtu.edu.cn;
   liangzheng06@gmail.com; yee.i.yang@gmail.com
RI yang, yang/HGT-7999-2022; yang, yang/GVT-5210-2022; Lang,
   Ming/HIK-0758-2022; yang, yang/GWB-9426-2022; Yang, Yi/B-9273-2017
OI Yang, Yi/0000-0002-0512-880X; Zheng, Liang/0000-0002-1464-9500
FU National Key Research and Development of China [2016YFB0800404]; Natural
   Science Foundation of China [61532005, 61332012, 61572065]; Ministry of
   Education of China [MCM20160102]; China Mobile [MCM20160102];
   Fundamental Research Funds for the Central Universities [2018JBZ001]
FX This work was supported in part by National Key Research and Development
   of China (grant no. 2016YFB0800404), in part by the Natural Science
   Foundation of China (grant nos. 61532005, 61332012, and 61572065), in
   part by the Joint Fund of Ministry of Education of China and China
   Mobile (No. MCM20160102), and in part by the Fundamental Research Funds
   for the Central Universities (grant no. 2018JBZ001). Part of this work
   was done when R. Liu was a visiting student at the Centre for Artificial
   Intelligence, Faculty of Engineering and Information Technology,
   University of Technology Sydney, Australia.
CR [Anonymous], ARXIV161105588
   [Anonymous], 2014, P SSST EMNLP 2014 8
   [Anonymous], ARXIV171202036
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2017, ARXIV170303567
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2017, ARXIV PREPRINT ARXIV
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], 2017, ARXIV170705612
   [Anonymous], PROC CVPR IEEE
   [Anonymous], J MACHINE LEARNING R
   [Anonymous], ICCV
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Chung Junyoung, 2014, ARXIV14123555
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Eisenschtat Aviv, 2016, LINKING IMAGE TEXT 2
   Feng FX, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808205
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He L, 2017, IEEE INT CON MULTI, P1153, DOI 10.1109/ICME.2017.8019549
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   King DB, 2015, ACS SYM SER, V1214, P1
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Kiros Ryan., 2015, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, P3294
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Lev G, 2016, LECT NOTES COMPUT SC, V9910, P833, DOI 10.1007/978-3-319-46466-4_50
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin X, 2016, LECT NOTES COMPUT SC, V9906, P261, DOI 10.1007/978-3-319-46475-6_17
   Liu Ren., 2015, Modeling and Simulation of Cyber-Physical Energy Systems (MSCPES), 2015 Workshop on, P1
   Liu RY, 2018, IEEE MULTIMEDIA, V25, P71, DOI 10.1109/MMUL.2018.112142537
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Mao Junhua., 2014, Explain images with multimodal recurrent neural networks
   Mikolov T, 2012, IEEE W SP LANG TECH, P234, DOI 10.1109/SLT.2012.6424228
   Nam Hyeonseob, 2016, ARXIV161100471
   Peng YX, 2017, FRONT INFORM TECH EL, V18, P44, DOI 10.1631/FITEE.1601787
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Simonyan K., 2014, 14091556 ARXIV
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Xu X, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P302, DOI 10.1145/2964284.2967231
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zhang Jian, 2017, ARXIV171200358
   Zheng Zhedong, 2017, ARXIV171105535
NR 61
TC 22
Z9 22
U1 5
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
AR 27
DI 10.1145/3300939
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9JB
UT WOS:000459798800011
DA 2024-07-18
ER

PT J
AU Kong, LC
   Dai, R
AF Kong, Lingchao
   Dai, Rui
TI Efficient Video Encoding for Automatic Video Analysis in Distributed
   Wireless Surveillance Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video encoding; video analysis; surveillance systems; wireless systems
ID SALIENCY DETECTION; COMPRESSION
AB In many distributed wireless surveillance applications, compressed videos are used for performing automatic video analysis tasks. The accuracy of object detection, which is essential for various video analysis tasks, can be reduced due to video quality degradation caused by lossy compression. This article introduces a video encoding framework with the objective of boosting the accuracy of object detection for wireless surveillance applications. The proposed video encoding framework is based on systematic investigation of the effects of lossy compression on object detection. It has been found that current standardized video encoding schemes cause temporal domain fluctuation for encoded blocks in stable background areas and spatial texture degradation for encoded blocks in dynamic foreground areas of a raw video, both of which degrade the accuracy of object detection. Two measures, the sum-of-absolute frame difference (SFD) and the degradation of texture in 2D transform domain (TXD), are introduced to depict the temporal domain fluctuation and the spatial texture degradation in an encoded video, respectively. The proposed encoding framework is designed to suppress unnecessary temporal fluctuation in stable background areas and preserve spatial texture in dynamic foreground areas based on the two measures, and it introduces new mode decision strategies for both intra- and interframes to improve the accuracy of object detection while maintaining an acceptable rate distortion performance. Experimental results show that, compared with traditional encoding schemes, the proposed scheme improves the performance of object detection and results in lower bit rates and significantly reduced complexity with comparable quality in terms of PSNR and SSIM.
C1 [Kong, Lingchao; Dai, Rui] Univ Cincinnati, Dept Elect Engn & Comp Sci, Cincinnati, OH 45221 USA.
C3 University System of Ohio; University of Cincinnati
RP Kong, LC (corresponding author), Univ Cincinnati, Dept Elect Engn & Comp Sci, Cincinnati, OH 45221 USA.
EM konglo@mail.uc.edu; rui.dai@uc.edu
OI Dai, Rui/0000-0001-6620-7862
FU National Institute of Standards and Technology [60NANB17D193]; National
   Science Foundation [CNS-1644946]
FX This work was supported by the National Institute of Standards and
   Technology under Grant 60NANB17D193 and the National Science Foundation
   under Grant CNS-1644946.
CR [Anonymous], IEEE C COMP VIS PATT
   Bagdanov A. D., 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P190, DOI 10.1109/ISM.2011.38
   Baumann A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/824726
   Brutzer S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1937, DOI 10.1109/CVPR.2011.5995508
   Chao JS, 2015, IEEE T CIRC SYST VID, V25, P958, DOI 10.1109/TCSVT.2014.2367354
   Chen X, 2015, IEEE INT WORKSH MULT, DOI 10.1109/MMSP.2015.7340838
   Chun SS, 2006, IEEE T CONSUM ELECTR, V52, P1303, DOI 10.1109/TCE.2006.273149
   Corke P, 2010, P IEEE, V98, P1903, DOI 10.1109/JPROC.2010.2068530
   Du W, 2016, IEEE ACM T NETWORK, V24, P2498, DOI 10.1109/TNET.2015.2476349
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   Hu HM, 2012, IEEE T CIRC SYST VID, V22, P1564, DOI 10.1109/TCSVT.2012.2199398
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   ITU-T, 2008, ITU T P910 SUBJECTIV
   Jiménez-Moreno A, 2014, IEEE T MULTIMEDIA, V16, P1863, DOI 10.1109/TMM.2014.2347257
   Joint Video Team (JVT), 2001, JVTB118
   Kafetzakis E, 2013, IEEE INT SYM MULTIM, P333, DOI 10.1109/ISM.2013.64
   Kong LC, 2017, IEEE MULTIMEDIA, V24, P76, DOI 10.1109/MMUL.2017.29
   Kong LC, 2016, IEEE INT SYM MULTIM, P126, DOI [10.1109/ISM.2016.134, 10.1109/ISM.2016.0032]
   Kong LC, 2016, IEEE IMAGE PROC, P3797, DOI 10.1109/ICIP.2016.7533070
   Korshunov P, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000488
   Kuo T., 2010, Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on, IEEE, P25
   Leszczuk M, 2014, MULTIMED TOOLS APPL, V68, P41, DOI 10.1007/s11042-012-1161-6
   Ma Z, 2012, IEEE T CIRC SYST VID, V22, P671, DOI 10.1109/TCSVT.2011.2177143
   MathWorks Inc, 2006, LOC RANG IM MATLAB R
   Ou YF, 2011, IEEE T CIRC SYST VID, V21, P286, DOI 10.1109/TCSVT.2010.2087833
   Rajakaruna R. M. T. P., 2011, 2011 IEEE 6th International Conference on Industrial and Information Systems (ICIIS 2011), P76, DOI 10.1109/ICIINFS.2011.6038044
   Rosa Medina D., 2017, P 16 ANN MED AD HOC, P1, DOI [10.1109/chilecon.2017.8229531, DOI 10.1109/CHILECON.2017.8229531]
   Snidaro L, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071403
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   Soyak E, 2011, IEEE T CIRC SYST VID, V21, P1378, DOI 10.1109/TCSVT.2011.2163448
   Tan Ee-Leng., 2015, PERCEPTUAL IMAGE COD, P21
   Tavli B, 2012, MULTIMED TOOLS APPL, V60, P689, DOI 10.1007/s11042-011-0840-z
   VideoLAN Organization, 2005, X264 BEST H 264 AVC
   Wang P, 2013, IEEE IMAGE PROC, P1986, DOI 10.1109/ICIP.2013.6738409
   Wei Zhuo, 2016, ACM T MULTIM COMPUT, V12, P64
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yang H, 2008, IEEE IMAGE PROC, P2868, DOI 10.1109/ICIP.2008.4712393
   Ye Y, 2013, IEEE ACCESS, V1, P646, DOI 10.1109/ACCESS.2013.2282613
   Zhang F, 2011, IEEE J-STSP, V5, P1378, DOI 10.1109/JSTSP.2011.2165201
   Zhang X, 2017, IEEE T IMAGE PROCESS, V26, P633, DOI 10.1109/TIP.2016.2629447
NR 42
TC 10
Z9 14
U1 3
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
AR 72
DI 10.1145/3226036
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GT7CI
UT WOS:000444677600005
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, JJ
   Amos, B
   Das, A
   Pillai, P
   Sadeh, N
   Satyanarayanan, M
AF Wang, Junjue
   Amos, Brandon
   Das, Anupam
   Pillai, Padmanabhan
   Sadeh, Norman
   Satyanarayanan, Mahadev
TI Enabling Live Video Analytics with a Scalable and Privacy-Aware
   Framework
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Privacy mediator; face recognition; cloudlet; edge computing; cloud
   computing
ID RECOGNITION; EIGENFACES
AB We show how to build the components of a privacy-aware, live video analytics ecosystem from the bottom up, starting with OpenFace, our new open-source face recognition system that approaches state-of-the-art accuracy. Integrating OpenFace with interframe tracking, we build RTFace, a mechanism for denaturing video streams that selectively blurs faces according to specified policies at full frame rates. This enables privacy management for live video analytics while providing a secure approach for handling retrospective policy exceptions. Finally, we present a scalable, privacy-aware architecture for large camera networks using RTFace and show how it can be an enabler for a vibrant ecosystem and marketplace of privacy-aware video streams and analytics services.
C1 [Wang, Junjue; Amos, Brandon; Das, Anupam; Sadeh, Norman; Satyanarayanan, Mahadev] Carnegie Mellon Univ, Sch Comp Sci, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
   [Pillai, Padmanabhan] Intel Labs Pittsburgh, 4720 Forbes Ave, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University; Intel Corporation
RP Wang, JJ (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
EM junjuew@cs.cmu.edu; bamos@cs.cmu.edu; anupamd@cs.cmu.edu;
   padmanabhan.s.pillai@intel.com; sadeh@cs.cmu.edu; satya@cs.cmu.edu
RI Amos, Brandon/C-6312-2015; Wang, Junjue/ADN-0339-2022
OI Wang, Junjue/0000-0001-9096-4126
FU National Science Foundation (NSF) [CNS-1518865, SBE-1513957]; DARPA
   [FA8750-15-2-0277]; Air Force Research Laboratory [FA8750-15-2-0277];
   Intel as part of the Intel STC for Visual Cloud Systems (ISTC-VCS);
   Google; Vodafone; Deutsche Telekom; Verizon; Crown Castle; NVIDIA; NTT;
   Conklin Kistler family fund; NSF Graduate Research Fellowship Program
   [DGE1252522]
FX This research was partly funded by the National Science Foundation (NSF)
   under grant number CNS-1518865 and SBE-1513957, by DARPA and the Air
   Force Research Laboratory under agreement number FA8750-15-2-0277, and
   by Intel as part of the Intel STC for Visual Cloud Systems (ISTC-VCS).
   Additional support was provided by Google, Vodafone, Deutsche Telekom,
   Verizon, Crown Castle, NVIDIA, NTT, and the Conklin Kistler family fund.
   Brandon Amos is supported by the NSF Graduate Research Fellowship
   Program under Grant No. DGE1252522. Any opinions, findings, conclusions,
   or recommendations expressed in this material are those of the authors
   and should not be attributed to their employers or funding sources.
CR Aditya Paarijaat, 2016, P ACM MOBISYS
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Amazon, 2017, EC2 INST TYP
   Amazon, 2017, S3 PRIC
   Amazon, 2017, EC2 SPOT INST PRIC
   Amazon, 2017, EC2 PRIC
   Amazon, 2017, SURV CAM PRIC
   Amazon, 2017, EC2 RES INST PRIC
   Amazon, 2017, SEAG 1TB IRONWOLF NA
   [Anonymous], 1995, THESIS
   [Anonymous], 1973, PICTURE PROCESSING S
   [Anonymous], 2017, Electric power monthly
   [Anonymous], P 8 ACM MULT SYST C
   [Anonymous], 2015, CMUCS15123
   [Anonymous], 2017, STEALTH COMMUNICATIO
   [Anonymous], DAILY TELEGRAPH
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bo Cheng, 2014, P ACM SENSYS
   Carnegie Mellon University Personalized Privacy Assistant Team, 2017, PERS PRIV ASS PROJ
   Chen TYH, 2015, SENSYS'15: PROCEEDINGS OF THE 13TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P155, DOI 10.1145/2809695.2809711
   Collobert R, 2011, BIGLEARN NIPS WORKSH, P1
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Das Anupam, 2017, P IEEE CVPR WORKSH
   Davies N., 2016, P ACM HOTMOBILE 2016
   Debate. org, 2017, AR VID SURV CAM PUBL
   Dell, 2017, POWEREDGE R430 SERV
   Gross Ralph, 2006, P IEEE CVPR WORKSH
   Intel, 2016, MIL LEV INT PROC INT
   Jafri R, 2009, J INF PROCESS SYST, V5, P41, DOI 10.3745/JIPS.2009.5.2.041
   Jana S, 2013, P IEEE S SECUR PRIV, P349, DOI 10.1109/SP.2013.31
   Kampf M, 2002, BRAIN COGNITION, V50, P35, DOI 10.1016/S0278-2626(02)00008-8
   Kim M., 2008, P IEEE CVPR, P1
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kozat SS, 2004, IEEE IMAGE PROC, P3443, DOI 10.1109/ICIP.2004.1421855
   Labeled Faces in the Wild, 2017, LFW RES
   Monga V, 2006, IEEE T IMAGE PROCESS, V15, P3452, DOI 10.1109/TIP.2006.881948
   Naeini P. E., 2017, P SOUPS
   Netflix, 2017, INT CONN SPEED REC
   Newton EM, 2005, IEEE T KNOWL DATA EN, V17, P232, DOI 10.1109/TKDE.2005.32
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Ramon M, 2011, PERCEPTION, V40, P437, DOI 10.1068/p6794
   Raval Nisarg, 2016, P ACM MOBISYS
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schiff Jeremy, 2007, P IEEE RSJ INT ROB S
   Schroff F., 2015, P IEEE CVPR
   Seagate Technology, 2014, VID SURV TRENDS REP
   Senior A, 2005, IEEE SECUR PRIV, V3, P50, DOI 10.1109/MSP.2005.65
   Simoens Pieter, 2013, P ACM MOBISYS
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Western Digital Corporation, 2017, MY CLOUD PRO SER
   Yaniv Taigman, 2014, P IEEE CVPR
   Yi Dong, 2014, ARXIV14117923
   YouTube and Tubefilter, 2017, HOURS VID UPL YOUT E
   YouTube Help, 2017, REC UPL ENC SETT
   Zhang N., 2007, Tech. Rep. 07-49, P7
NR 56
TC 23
Z9 26
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
SU S
AR 64
DI 10.1145/3209659
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG9GD
UT WOS:000455314500012
OA Bronze
DA 2024-07-18
ER

PT J
AU Luo, WQ
   Li, HD
   Yan, Q
   Yang, R
   Huang, JW
AF Luo, Weiqi
   Li, Haodong
   Yan, Qi
   Yang, Rui
   Huang, Jiwu
TI Improved Audio Steganalytic Feature and Its Applications in Audio
   Forensics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Audio steganalysis; audio forensics; markov transition
   probability
AB Digital multimedia steganalysis has attracted wide attention over the past decade. Currently, there are many algorithms for detecting image steganography. However, little research has been devoted to audio steganalysis. Since the statistical properties of image and audio files are quite different, features that are effective in image steganalysis may not be effective for audio. In this article, we design an improved audio steganalytic feature set derived from both the time and Mel-frequency domains for detecting some typical steganography in the time domain, including LSB matching, Hide4PGP, and Steghide. The experiment results, evaluated on different audio sources, including various music and speech clips of different complexity, have shown that the proposed features significantly outperform the existing ones. Moreover, we use the proposed features to detect and further identify some typical audio operations that would probably be used in audio tampering. The extensive experiment results have shown that the proposed features also outperform the related forensic methods, especially when the length of the audio clip is small, such as audio clips with 800 samples. This is very important in real forensic situations.
C1 [Luo, Weiqi; Yan, Qi; Yang, Rui] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Luo, Weiqi; Yan, Qi; Yang, Rui] Sun Yat Sen Univ, Guangdong Key Lab Informat Secur & Technol, Guangzhou 510006, Guangdong, Peoples R China.
   [Li, Haodong; Huang, Jiwu] Shenzhen Univ, Coll Informat Engn, Shenzhen 518052, Peoples R China.
   [Li, Haodong; Huang, Jiwu] Shenzhen Univ, Shenzhen Key Lab Media Secur, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518052, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Shenzhen University;
   Shenzhen University
RP Luo, WQ (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.; Luo, WQ (corresponding author), Sun Yat Sen Univ, Guangdong Key Lab Informat Secur & Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM luoweiqi@mail.sysu.edu.cn; lihaodong@szu.edu.cn;
   yanqi@mail2.sysu.edu.cn; yangr23@mail.sysu.edu.cn; jwhuang@szu.edu.cn
RI huang, jw/KVY-9917-2024; Li, Haodong/AAG-7592-2019
OI Li, Haodong/0000-0003-0532-9481
FU NSFC [61672551, U1636202]; Special plan of Guangdong Province
   [2015TQ01X365]; Fok Ying-Tong Education Foundation [142003]; Science and
   Technology Planning Project of Guangdong Province [201707010167];
   Shenzhen RD Program [JCYJ20160328144421330]; Alibaba Innovative Research
   (AIR) Program by Alibaba Group
FX This work is supported in part by the NSFC (61672551, U1636202), the
   Special plan of Guangdong Province (2015TQ01X365), the Fok Ying-Tong
   Education Foundation (142003), the Science and Technology Planning
   Project of Guangdong Province (201707010167), Shenzhen R&D Program
   (JCYJ20160328144421330), and Alibaba Innovative Research (AIR) Program
   by Alibaba Group.
CR [Anonymous], 2012, IEEE T INF FORENSICS, DOI DOI 10.1109/TIFS.2011.2175919
   Farid H., 2001, TECHNICAL REPORT
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Johnson MK, 2005, PROC SPIE, V5681, P664, DOI 10.1117/12.586941
   Kang XG, 2013, IEEE T INF FOREN SEC, V8, P1456, DOI 10.1109/TIFS.2013.2273394
   Knerr S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P41
   Kraetzer C, 2007, PROC SPIE, V6505, DOI 10.1117/12.704040
   Liu QZ, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000492
   Liu QZ, 2009, IEEE T INF FOREN SEC, V4, P359, DOI 10.1109/TIFS.2009.2024718
   Luo D, 2016, IEEE SIGNAL PROC LET, V23, P688, DOI 10.1109/LSP.2016.2549600
   Luo D, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2575978
   Qiao MY, 2013, INFORM SCIENCES, V231, P123, DOI 10.1016/j.ins.2012.10.013
   Qiu X., 2014, P 2 ACM WORKSH INF H, P165, DOI DOI 10.1145/2600918.2600941
   Ren YZ, 2015, IEEE T INF FOREN SEC, V10, P1801, DOI 10.1109/TIFS.2015.2421322
   Shi YQ, 2007, LECT NOTES COMPUT SC, V4437, P249
   Tang W, 2014, P 2 ACM WORKSH INF H, P91, DOI [10.1145/2600918.2600935, DOI 10.1145/2600918.2600935]
   Wu HJ, 2014, IEEE T INF FOREN SEC, V9, P489, DOI 10.1109/TIFS.2014.2301912
   Zou DK, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1365, DOI 10.1109/ICME.2006.262792
NR 18
TC 19
Z9 23
U1 1
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
AR 43
DI 10.1145/3190575
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GG9BS
UT WOS:000432996000001
DA 2024-07-18
ER

PT J
AU Wang, C
   Bhat, D
   Rizk, A
   Zink, M
AF Wang, Cong
   Bhat, Divyashri
   Rizk, Amr
   Zink, Michael
TI Design and Analysis of QoE-Aware Quality Adaptation for DASH: A
   Spectrum-Based Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adaptive bitrate streaming; quality of experience; DASH; TCP
ID BANDWIDTH; VIDEO
AB The dynamics of the application-layer-based control loop of dynamic adaptive streaming over HTTP (DASH) make video bitrate selection for DASH a difficult problem. In this work, we provide a DASH quality adaptation algorithm, named SQUAD, that is specifically tailored to provide a high quality of experience (QoE). We review and provide new insights into the challenges for DASH rate estimation. We found that in addition to the ON-OFF behavior of DASH clients, there exists a discrepancy in the timescales that form the basis of the rate estimates across (i) different video segments and (ii) the rate control loops of DASH and Transmission Control Protocol (TCP). With these observations in mind, we design SQUAD aiming to maximize the average quality bitrate while minimizing the quality variations. We test our implementation of SQUAD together with a number of different quality adaptation algorithms under various conditions in the Global Environment for Networking Innovation testbed, as well as, in a series of measurements over the public Internet. Through a measurement study, we show that by sacrificing little to nothing in average quality bitrate, SQUAD can provide significantly better QoE in terms of quality switching and magnitude. In addition, we show that retransmission of higher-quality segments that were originally received in low-quality is feasible and improves the QoE.
C1 [Wang, Cong; Bhat, Divyashri; Zink, Michael] Univ Massachusetts, Dept Elect & Comp Engn, 151 Holdsworth Way, Amherst, MA 01003 USA.
   [Rizk, Amr] Tech Univ Darmstadt, Rundeturmstr 10, D-64293 Darmstadt, Germany.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   Technical University of Darmstadt
RP Wang, C (corresponding author), Univ Massachusetts, Dept Elect & Comp Engn, 151 Holdsworth Way, Amherst, MA 01003 USA.
RI Wang, Cong/AAE-3356-2020
OI Wang, Cong/0000-0001-6429-8799; Zink, Michael/0000-0002-0309-9240; Bhat,
   Divyashri/0000-0001-5768-291X
CR [Anonymous], GLOB INT PHEN REP 20
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], P ACM SIGC
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], AD HTTP DYN STREM
   [Anonymous], MICR SMOOTH STREAM
   [Anonymous], PROC ACM CONF SIGC
   [Anonymous], P ACM MULT SYST C MM
   [Anonymous], OPT SEGM LENGTH AD S
   [Anonymous], ART COMPUTER SYSTEMS
   [Anonymous], P ACM MULT SYST C MM
   [Anonymous], APPL HTTP LIV STREAM
   Berman M, 2015, COMMUN ACM, V58, P78, DOI 10.1145/2699392
   Berman M, 2014, COMPUT NETW, V61, P5, DOI 10.1016/j.bjp.2013.12.037
   De Cicco L., 2011, P 2 ANN ACM C MULTIM, P145
   Fund F., 2013, PACK VID WORKSH PV 2, P1, DOI DOI 10.1109/PV.2013.6691455
   Giambene Giovanni., 2005, QUEUING THEORY TELEC
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Jain M, 2003, IEEE ACM T NETWORK, V11, P537, DOI 10.1109/TNET.2003.815304
   Juluri P, 2015, IEEE INT CONF COMM, P1765, DOI 10.1109/ICCW.2015.7247436
   Krishnan R. K., 2012, P INT MEAS C, P211
   Lederer S., 2012, P 3 MULT SYST C, P89
   Li Z., 2014, Proceedings of the 5th ACM Multimedia Systems Conference, MMSys '14, P248
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liebeherr J, 2010, IEEE ACM T NETWORK, V18, P1040, DOI 10.1109/TNET.2009.2035115
   Oyman O, 2012, IEEE COMMUN MAG, V50, P20, DOI 10.1109/MCOM.2012.6178830
   Rao A., 2011, Em: Proceedings of the Seventh COnference on emerging Networking EXperiments and Technologies, P1, DOI DOI 10.1145/2079296.2079321
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Tian G., 2012, P 8 INT C EM NETW EX, P109
   Villa BJ, 2013, INT CON ADV INFO NET, P830, DOI 10.1109/AINA.2013.9
   Vulimiri A, 2013, PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT '13), P283, DOI 10.1145/2535372.2535392
   White B, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P255, DOI 10.1145/1060289.1060313
   Whiteaker J, 2011, ACM SIGCOMM COMP COM, V41, P39, DOI 10.1145/1925861.1925867
   Xiang S., 2012, ACM MMSys '12, P167
   Zink M, 2005, IEEE T MULTIMEDIA, V7, P75, DOI 10.1109/TMM.2004.840595
NR 37
TC 7
Z9 8
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
SU S
AR 45
DI 10.1145/3092839
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FP1UO
UT WOS:000417400400013
DA 2024-07-18
ER

PT J
AU Chang, ZY
   Chan, SHG
AF Chang, Zhangyu
   Chan, S. -H. Gary
TI Video Management and Resource Allocation for a Large-Scale VoD Cloud
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Distributed video-on-demand; optimization; resource allocation; linear
   programming; spectral clustering
ID STORAGE-SYSTEM; DEMAND; POWER; LIVE
AB We consider providing large-scale Netflix-like video-on-demand (VoD) service on a cloud platform, where cloud proxy servers are placed close to user pools. Videos may have heterogeneous popularity at different geo-locations. A repository provides video backup for the network, and the proxy servers collaboratively store and stream videos. To deploy the VoD cloud, the content provider rents resources consisting of link capacities among servers, server storage, and server processing capacity to handle remote requests.
   We study how to minimize the deployment cost by jointly optimizing video management (in terms of video placement and retrieval at servers) and resource allocation (in terms of link, storage, and processing capacities), subject to a certain user delay requirement on video access. We first formulate the joint optimization problem and show that it is NP-hard. To address it, we propose Resource allocation And Video management Optimization (RAVO), a novel and efficient algorithm based on linear programming with proven optimality gap. For a large video pool, we propose a video clustering algorithm to substantially reduce the run-time computational complexity without compromising performance. Using extensive simulation and trace-driven real data, we show that RAVO achieves close-to-optimal performance, outperforming other advanced schemes significantly (often by multiple times).
C1 [Chang, Zhangyu; Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Chang, ZY (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
EM zchang@cse.ust.hk; gchan@cse.ust.hk
OI Chang, Zhangyu/0000-0002-1069-4048
FU Hong Kong Research Grant Council (RGC) General Research Fund [610713];
   National Natural Science Foundation of China [61472455]
FX This work was supported, in part, by the Hong Kong Research Grant
   Council (RGC) General Research Fund (610713) and the National Natural
   Science Foundation of China (61472455).
CR Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   Aggarwal V., 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P637, DOI 10.1109/INFCOMW.2011.5928890
   Alasaad A, 2012, IEEE GLOBE WORK, P753, DOI 10.1109/GLOCOMW.2012.6477669
   [Anonymous], 2015, INT J COMPUT APPL
   [Anonymous], CISC VIS NETW IND GL
   [Anonymous], INTEGER PROGRAMMING
   [Anonymous], GOOGL CLOUND PLATF
   [Anonymous], MANAGING TRAFFIC PER
   [Anonymous], 2007, P 16 INT C WORLD WID
   [Anonymous], UBIQUITOUS INFORM TE
   Applegate D., 2010, P 6 INT C, P4
   Bobroff N, 2007, 2007 10TH IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2009), VOLS 1 AND 2, P119, DOI 10.1109/INM.2007.374776
   Borst S, 2010, IEEE INFOCOM SER, DOI 10.1109/infcom.2010.5461964
   Chan SHG, 2013, IEEE T MULTIMEDIA, V15, P2125, DOI 10.1109/TMM.2013.2280989
   Chang ZY, 2015, IEEE T MULTIMEDIA, V17, P723, DOI 10.1109/TMM.2015.2416636
   Chu YM, 2014, IEEE SYST J, V8, P292, DOI 10.1109/JSYST.2013.2257338
   Cong X, 2014, PEER PEER NETW APPL, V7, P175, DOI 10.1007/s12083-012-0193-z
   De Melo RM, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/764010
   Fernando T, 2013, INT CONF ADV ICT, P160, DOI 10.1109/ICTer.2013.6761172
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Hu H, 2015, IEEE GLOB COMM CONF, DOI 10.1109/GLOCOM.2015.7417027
   Kusic D, 2009, CLUSTER COMPUT, V12, P1, DOI 10.1007/s10586-008-0070-y
   Laoutaris N, 2005, COMPUT NETW, V47, P409, DOI 10.1016/j.comnet.2004.07.020
   Li HY, 2010, J MICROBIOL, V48, P1, DOI 10.1007/s12275-009-0163-1
   Li H, 2015, COMPUT J, V58, P1373, DOI 10.1093/comjnl/bxu122
   Lin MH, 2011, IEEE INFOCOM SER, P1098, DOI 10.1109/INFCOM.2011.5934885
   Lin Y., 2015, PROC IEEE INT C PEER, P1
   Liu N, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2568194
   Magharei N, 2014, IEEE ACM T NETWORK, V22, P244, DOI 10.1109/TNET.2013.2257840
   Niu D, 2012, IEEE INFOCOM SER, P460, DOI 10.1109/INFCOM.2012.6195785
   Shuja J, 2014, CLUSTER COMPUT, V17, P1265, DOI 10.1007/s10586-014-0365-0
   Thouin F, 2006, NCA 2006: FIFTH IEEE INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS, PROCEEDINGS, P103
   Wang FG, 2015, IEEE ICC, P460, DOI 10.1109/ICC.2015.7248364
   Wang M, 2011, IEEE INFOCOM SER, P71, DOI 10.1109/INFCOM.2011.5935254
   Wu WJ, 2012, IEEE T PARALL DISTR, V23, P1492, DOI 10.1109/TPDS.2011.295
   Wu Y, 2011, INT CON DISTR COMP S, P268, DOI 10.1109/ICDCS.2011.50
   Yiu WPK, 2007, IEEE J SEL AREA COMM, V25, P1717, DOI 10.1109/JSAC.2007.071210
   Zhenhuan Gong, 2010, 6th International Conference on Network and Service Management (CNSM 2010), P9, DOI 10.1109/CNSM.2010.5691343
NR 38
TC 5
Z9 5
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2016
VL 12
IS 5
SU S
AR 72
DI 10.1145/2983638
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EJ0VT
UT WOS:000392929700003
DA 2024-07-18
ER

PT J
AU Feng, FX
   Wang, XJ
   Li, RF
   Ahmad, I
AF Feng, Fangxiang
   Wang, Xiaojie
   Li, Ruifan
   Ahmad, Ibrar
TI Correspondence Autoencoders for Cross-Modal Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Cross-modal; retrieval; image and
   text; deep learning; autoencoder
AB This article considers the problem of cross-modal retrieval, such as using a text query to search for images and vice-versa. Based on different autoencoders, several novel models are proposed here for solving this problem. These models are constructed by correlating hidden representations of a pair of autoencoders. A novel optimal objective, which minimizes a linear combination of the representation learning errors for each modality and the correlation learning error between hidden representations of two modalities, is used to train the model as a whole. Minimizing the correlation learning error forces the model to learn hidden representations with only common information in different modalities, while minimizing the representation learning error makes hidden representations good enough to reconstruct inputs of each modality. To balance the two kind of errors induced by representation learning and correlation learning, we set a specific parameter in our models. Furthermore, according to the modalities the models attempt to reconstruct they are divided into two groups. One group including three models is named multimodal reconstruction correspondence autoencoder since it reconstructs both modalities. The other group including two models is named unimodal reconstruction correspondence autoencoder since it reconstructs a single modality. The proposed models are evaluated on three publicly available datasets. And our experiments demonstrate that our proposed correspondence autoencoders perform significantly better than three canonical correlation analysis based models and two popular multimodal deep models on cross-modal retrieval tasks.
C1 [Feng, Fangxiang; Wang, Xiaojie; Li, Ruifan; Ahmad, Ibrar] Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing 100876, Peoples R China.
   [Ahmad, Ibrar] Univ Peshawar, Dept Comp Sci, Peshawar, Pakistan.
C3 Beijing University of Posts & Telecommunications; University of Peshawar
RP Wang, XJ (corresponding author), Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing 100876, Peoples R China.
EM f.fangxiang@gmail.com; xjwang@bupt.edu; rfli@bupt.edu;
   ibrar@upesh.edu.pk
RI LI, Ruifan/AFM-1702-2022
FU National Natural Science Foundation of China [61273365]; National High
   Technology Research and Development Program of China [2012AA011103];
   discipline building plan in 111 base [B08004]; Fundamental Research
   Funds for the Central Universities [2013RC0304]; Engineering Research
   Center of Information Networks, Ministry of Education
FX This work was partially supported by the National Natural Science
   Foundation of China (No. 61273365), the National High Technology
   Research and Development Program of China (No. 2012AA011103), discipline
   building plan in 111 base (No. B08004), the Fundamental Research Funds
   for the Central Universities (No. 2013RC0304) and the Engineering
   Research Center of Information Networks, Ministry of Education.
CR [Anonymous], 2011, P ICML
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], P INT C MACH LEARN R
   Bastan M, 2010, IEEE MULTIMEDIA, V17, P62, DOI 10.1109/MMUL.2010.5692184
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bird S., 2004, P ACL INTERACTIVE PO, P214
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2009, INT C NEURAL INF PRO, P1607
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Kim JW, 2012, INT J STEEL STRUCT, V12, P579, DOI 10.1007/s13296-012-4012-4
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Silberer C, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P721
   Smolensky P., 1986, Information processing in dynamical systems: Foundations of harmony theory
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935, DOI DOI 10.1007/978-3-319-46478-7
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3
   Zhuang Y., 2013, P 27 AAAI C ART INT, P1070
NR 31
TC 15
Z9 21
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 26
DI 10.1145/2808205
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100016
DA 2024-07-18
ER

PT J
AU You, SD
   Pu, YH
AF You, Shingchern D.
   Pu, Yi-Han
TI Using Paired Distances of Signal Peaks in Stereo Channels as
   Fingerprints for Copy Identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Music fingerprint; MPEG-7 audio signature descriptor; rough
   longest common subsequence; stereo music; envelope peak
ID LONGEST COMMON SUBSEQUENCE; AUDIO
AB This article proposes to use the relative distances between adjacent envelope peaks detected in stereo audio as fingerprints for copy identification. The matching algorithm used is the rough longest common subsequence (RLCS) algorithm. The experimental results show that the proposed approach has better identification accuracy than an MPEG-7 based scheme for distorted and noisy audio. When compared with other schemes, the proposed scheme uses fewer bits with comparable performance. The proposed fingerprints can also be used in conjunction with the MPEG-7 based scheme for lower computational burden.
C1 [You, Shingchern D.; Pu, Yi-Han] Natl Taipei Univ Technol, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
C3 National Taipei University of Technology
RP You, SD (corresponding author), Natl Taipei Univ Technol, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
EM you@csie.ntut.edu.tw
RI You, Shingchern/AAG-6401-2020
FU National Science Council of Taiwan [NSC-101-2221-E-027-127]
FX This work was supported by the National Science Council of Taiwan
   through the grant NSC-101-2221-E-027-127.
CR [Anonymous], 2012, 26404 3GPP TS
   [Anonymous], 138183 ISOIEC
   [Anonymous], P ACM WORKSH MULT
   [Anonymous], 1997, ACM SIGACT NEWS
   Baluja S, 2007, INT CONF ACOUST SPEE, P213
   Bellettini C, 2007, I S INTELL SIG PROC, P28
   Bellettini Carlo, 2010, Journal of Communications, V5, P409, DOI 10.4304/jcm.5.5.409-424
   Bello JP, 2005, IEEE T SPEECH AUDI P, V13, P1035, DOI 10.1109/TSA.2005.851998
   Burges CJC, 2003, IEEE T SPEECH AUDI P, V11, P165, DOI 10.1109/TSA.2003.811538
   Cano P, 2005, J VLSI SIG PROC SYST, V41, P271, DOI 10.1007/s11265-005-4151-3
   Chandrasekhar Vijay., 2011, ISMIR, V20, P801
   Crysandt Holger, 2003, P 115 AES CONV
   Doets P. J. O., 2006, P SPIE, V6072
   Ellis Dan., 2009, ROBUST LANDMARK BASE
   Gomes LDT, 2003, J NEW MUSIC RES, V32, P65, DOI 10.1076/jnmr.32.1.65.16803
   Haitsma J, 2003, INT CONF ACOUST SPEE, P728
   Haitsma J., 2002, P ISMIR 2002 3 INT C, P107
   Hellmuth Oliver, 2003, P 115 AES CONV
   HIRSCHBERG DS, 1977, J ACM, V24, P664, DOI 10.1145/322033.322044
   ISO/IEC, 1993, 111723 ISOIEC
   ISO/IEC, 2002, 159384 ISOIEC
   Ke Y., 2005, P IEEE C COMP VIS PA
   Kiwi M, 2005, ADV MATH, V197, P480, DOI 10.1016/j.aim.2004.10.012
   Lee JY, 2005, LECT NOTES COMPUT SC, V3768, P526, DOI 10.1007/11582267_46
   Lin HJ, 2011, J INF SCI ENG, V27, P95
   Ramona M, 2013, INT CONF ACOUST SPEE, P818, DOI 10.1109/ICASSP.2013.6637762
   Ramona M, 2012, APPL ARTIF INTELL, V26, P119, DOI 10.1080/08839514.2012.629840
   Ramona Mathieu, 2011, P INT C AC SPEECH SI, P477
   Wang A, 2003, ISMIR
   Wang A, 2006, COMMUN ACM, V49, P44, DOI 10.1145/1145287.1145312
   You SD, 2013, SCI WORLD J, DOI 10.1155/2013/752464
   You SD, 2012, APPL MATH INFORM SCI, V6, p397S
   You Shingchern D., 2013, MULTIMEDIA TOOLS APP
NR 33
TC 2
Z9 2
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2015
VL 12
IS 1
AR 1
DI 10.1145/2742059
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CP9LG
UT WOS:000360215200001
DA 2024-07-18
ER

PT J
AU Langroodi, MJ
   Peters, J
   Shirmohammadi, S
AF Langroodi, Mohsen Jamali
   Peters, Joseph
   Shirmohammadi, Shervin
TI Decoder-Complexity-Aware Encoding of Motion Compensation for Multiple
   Heterogeneous Receivers
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Measurement; Decoder complexity modeling;
   H.264/AVC decoding; H.264/SVC decoding; motion compensation
ID VIDEO ENCODER; ALGORITHM
AB For mobile multimedia systems, advances in battery technology have been much slower than those in memory, graphics, and processing power, making power consumption a major concern in mobile systems. The computational complexity of video codecs, which consists of CPU operations and memory accesses, is one of the main factors affecting power consumption. In this article, we propose a method that achieves near-optimal video quality while respecting user-defined bounds on the complexity needed to decode a video. We specifically focus on the motion compensation process, including motion vector prediction and interpolation, because it is the single largest component of computation-based power consumption. We start by formulating a scenario with a single receiver as a rate-distortion optimization problem and we develop an efficient decoder-complexity-aware video encoding method to solve it. Then we extend our approach to handle multiple heterogeneous receivers, each with a different complexity requirement. We test our method experimentally using the H.264 standard for the single receiver scenario and the H.264 SVC extension for the multiple receiver scenario. Our experimental results show that our method can achieve up to 97% of the optimal solution value in the single receiver scenario, and an average of 97% of the optimal solution value in the multiple receiver scenario. Furthermore, our tests with actual power measurements show a power saving of up to 23% at the decoder when the complexity threshold is halved in the encoder.
C1 [Langroodi, Mohsen Jamali; Peters, Joseph] Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
   [Shirmohammadi, Shervin] Univ Ottawa, Sch Elect Engn & Comp Sci, Ottawa, ON K1N 6N5, Canada.
C3 Simon Fraser University; University of Ottawa
RP Peters, J (corresponding author), Simon Fraser Univ, Sch Comp Sci, 250-13450 102nd Ave, Surrey, BC V3T 0A3, Canada.
EM peters@cs.sfu.ca
RI Shirmohammadi, Shervin/E-6945-2012
OI Shirmohammadi, Shervin/0000-0002-3973-4445
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council (BCIC)
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada and in part by the British Columbia
   Innovation Council (BCIC).
CR [Anonymous], 2013, PROC VISUAL COMMUNIC
   [Anonymous], TREPN PROF
   Bean J. C., 1987, MULTIPLE CHOICE KNAP
   BENMOUSSA Y, 2013, P 16 EUR C DIG SYST, P890, DOI DOI 10.1109/DSD.2013.100
   Berger Karl-Eduard, 2013, 2013 IEEE International Symposium on Parallel and Distributed Processing, Workshops and PhD Forum (IPDPSW), P1797, DOI 10.1109/IPDPSW.2013.208
   Bross B., 2013, JCTVCL10003
   Choi JA, 2008, LECT NOTES COMPUT SC, V5353, P138, DOI 10.1007/978-3-540-89796-5_15
   da Fonseca TA, 2013, INT CONF ACOUST SPEE, P1739, DOI 10.1109/ICASSP.2013.6637950
   Grois D, 2014, IEEE T CIRC SYST VID, V24, P1025, DOI 10.1109/TCSVT.2014.2302557
   HIRSCHBERG DS, 1975, COMMUN ACM, V18, P341, DOI 10.1145/360825.360861
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   Joint Video Team, 2009, H 264 AVC REF SOFTW
   Joint Video Team, 2010, 1449610 ISOIEC JOINT
   Joint Video Team, 2011, H 264 SVC REF SOFT J
   Langroodi M. Jamali, 2013, P NETW OP SYST SUPP
   Lee SW, 2011, J VIS COMMUN IMAGE R, V22, P61, DOI 10.1016/j.jvcir.2010.10.004
   Lee SW, 2010, IEEE T CIRC SYST VID, V20, P706, DOI 10.1109/TCSVT.2010.2045913
   Lee Y, 2012, IEEE T VLSI SYST, V20, P310, DOI 10.1109/TVLSI.2010.2102055
   Ma Z, 2011, IEEE T MULTIMEDIA, V13, P1240, DOI 10.1109/TMM.2011.2165056
   Nibbelink K, 2007, IEEE INT CONF ASAP, P160
   Richardson Iain E, 2004, H. 264 and MPEG-4 Video Compression: Video Coding for Next-Generation Multimedia
   Rodríguez A, 2006, PAR ELEC 2006: INTERNATIONAL SYMPOSIUM ON PARALLEL COMPUTING IN ELECTRICAL ENGINEERING, PROCEEDINGS, P363
   Ryoo K., 2013, FUTURE INFORM COMMUN, V235, P549
   Sankaraiah S, 2011, INT PROC COMPUT SCI, V7, P127
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Semsarzadeh M., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P925, DOI 10.1109/ICME.2012.91
   Viitanen M, 2012, IEEE INT SYMP CIRC S, P882, DOI 10.1109/ISCAS.2012.6272182
   Wang YF, 2013, IEEE T CONSUM ELECTR, V59, P666, DOI 10.1109/TCE.2013.6626254
   Wiegand T., 2007, JVTW201 ISOIEC MPEG
   Zrida HK, 2009, DES AUT TEST EUROPE, P940
NR 30
TC 4
Z9 4
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2015
VL 11
IS 2
SU S
SI SI
AR 46
DI 10.1145/2700300
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC7RV
UT WOS:000350567000006
OA Green Published
DA 2024-07-18
ER

PT J
AU Rainer, B
   Timmerer, C
AF Rainer, Benjamin
   Timmerer, Christian
TI A Generic Utility Model Representing the Quality of Sensory Experience
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Human Factors; Sensory experience; quality of experience;
   MPEG-V; subjective quality assessment
AB Current QoE research is mainly focusing on single modalities (audio, visual) or combinations thereof. In our research, we propose annotating traditional multimedia content with additional sensory effects, such as ambient light, vibration, wind, and olfaction, which could potentially stimulate all human senses. Investigating the influence of individual sensory effects and combinations thereof is important in order to understand how these individual sensory effects influence the Quality of Experience (QoE) as a whole. In this article, we describe the results of such a subjective quality assessment of audio-visual sequences which are annotated with additional sensory effects such as ambient light, wind, and vibration using the MPEG-V standard. The results of this assessment allow us to derive a utility model representing the Quality of Sensory Experience (QuaSE) complementary to existing QoE models described in terms of Quality of Service (QoS) parameters. For validating our proposed utility model, we provide an example instantiation and validate it against results of subjective quality assessments.
C1 [Rainer, Benjamin; Timmerer, Christian] Alpen Adria Univ Klagenfurt, Klagenfurt, Austria.
C3 University of Klagenfurt
RP Rainer, B (corresponding author), Univ Str 65-67, A-9020 Klagenfurt Am Worthersee, Austria.
EM benjamin.rainer@itec.aau.at; christian.timmerer@itec.aau.at
RI Rainer, Benjamin/AAG-7407-2019
OI Rainer, Benjamin/0000-0003-1954-019X; Timmerer,
   Christian/0000-0002-0031-5243
FU EC in the context of the ALICANTE [FP7-ICT-248652]; SocialSensor
   [FP7-ICT-287975]; QUALINET [COST IC 1003]
FX This work was supported in part by the EC in the context of the ALICANTE
   (FP7-ICT-248652), SocialSensor (FP7-ICT-287975), and QUALINET (COST IC
   1003) projects and partly performed in the Lakeside Labs research
   cluster at AAU.
CR [Anonymous], P 7 IEEE WORKSH MULT
   [Anonymous], 2011, 23005 ISOIEC
   [Anonymous], P EUR NETW QUAL EXP
   Fiedler M, 2010, IEEE NETWORK, V24, P36, DOI 10.1109/MNET.2010.5430142
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   Hands DS, 2004, IEEE T MULTIMEDIA, V6, P806, DOI 10.1109/TMM.2004.837233
   Hock David, 2008, P 18 ITC SPEC SEM QU
   Ishibashi Yutaka, 2012, P 11 ANN WORKSH NETW
   ISO, 2000, 138181 ISOIEC
   Keith T.Z., 2006, MULTIPLE REGRESSION
   Kofler Ingo, 2006, P AXMEDIS LEEDS UK N, P207
   Kyoungro Yoon, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P257, DOI 10.1109/MMSP.2010.5662029
   Murray Niall., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys'13, P162, DOI [https://doi.org/10.1145/2483977.2483999, DOI 10.1145/2483977.2483999]
   Rainer B, 2012, INT WORK QUAL MULTIM, P278, DOI 10.1109/QoMEX.2012.6263842
   Reichl P, 2010, IEEE ICC
   Rice JA., 2007, MATH STAT DATA ANAL
   Suk CB, 2009, 2009 FOURTH INTERNATIONAL CONFERENCE ON INTERNET AND WEB APPLICATIONS AND SERVICES, P649, DOI 10.1109/ICIW.2009.104
   Timmerer C, 2012, SIGNAL PROCESS-IMAGE, V27, P909, DOI 10.1016/j.image.2012.01.016
   Waltl Markus, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P124, DOI 10.1109/QOMEX.2010.5517704
   Waltl M., 2012, P 20 ACM INT C MULT, P1469
   Waltl M., 2010, P 11 INT WORKSH IM A, P1
   Waltl M, 2012, INT WORK QUAL MULTIM, P115, DOI 10.1109/QoMEX.2012.6263841
   Waltl Markus, 2012, MULTIMED TOOLS APPL, V70, P1
NR 23
TC 9
Z9 10
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2014
VL 11
IS 1
SU S
SI SI
AR 14
DI 10.1145/2648429
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS0RG
UT WOS:000343984800006
DA 2024-07-18
ER

PT J
AU Gaeta, R
   Grangetto, M
   Bovio, L
AF Gaeta, Rossano
   Grangetto, Marco
   Bovio, Lorenzo
TI DIP: Distributed Identification of Polluters in P2P Live Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Design; Experimentation; Peer-to-peer; pollution attack;
   malicious node identification; P2P streaming; belief propagation;
   statistical inference; PlanetLab
ID ALGORITHM
AB Peer-to-peer live streaming applications are vulnerable to malicious actions of peers that deliberately modify data to decrease or prevent the fruition of the media (pollution attack). In this article we propose DIP, a fully distributed, accurate, and robust algorithm for the identification of polluters. DIP relies on checks that are computed by peers upon completing reception of all blocks composing a data chunk. A check is a special message that contains the set of peer identifiers that provided blocks of the chunk as well as a bit to signal if the chunk has been corrupted. Checks are periodically transmitted by peers to their neighbors in the overlay network; peers receiving checks use them to maintain a factor graph. This graph is bipartite and an incremental belief propagation algorithm is run on it to compute the probability of a peer being a polluter. Using a prototype deployed over PlanetLab we show by extensive experimentation that DIP allows honest peers to identify polluters with very high accuracy and completeness, even when polluters collude to deceive them. Furthermore, we show that DIP is efficient, requiring low computational, communication, and storage overhead at each peer.
C1 [Gaeta, Rossano; Grangetto, Marco; Bovio, Lorenzo] Univ Turin, Dipartimento Informat, Corso Svizzera 185, I-10149 Turin, Italy.
C3 University of Turin
RP Gaeta, R (corresponding author), Univ Turin, Dipartimento Informat, Corso Svizzera 185, I-10149 Turin, Italy.
EM rossano@di.unito.it
RI Grangetto, Marco/AFM-8024-2022; GAETA, Rossano/C-6256-2011
OI Grangetto, Marco/0000-0002-2709-7864; GAETA, Rossano/0000-0002-6521-403X
CR [Anonymous], P IEEE S SEC PRIV
   [Anonymous], P IEEE INFOCOM
   Bioglio V, 2009, IEEE COMMUN LETT, V13, P953, DOI 10.1109/LCOMM.2009.12.091824
   Bioglio Valerio, 2009, P WORKSH PEER PEER S, P323
   Borges A, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P481, DOI 10.1109/ICME.2008.4607476
   Dinger Jochen, 2006, P 1 IEEE INT C AV RE
   Gaeta R, 2013, IEEE T PARALL DISTR, V24, P1994, DOI 10.1109/TPDS.2012.342
   Gkantsidis C, 2006, P 25 IEEE INT C COMP, P1
   Gupta Ruchir, 2013, AVOIDING WHITEWASHIN
   Ho T, 2008, IEEE T INFORM THEORY, V54, P2798, DOI 10.1109/TIT.2008.921894
   Huang Gale., 2007, P ACM SIGCOMM WORKSH
   Jaggi S, 2008, IEEE T INFORM THEORY, V54, P2596, DOI 10.1109/TIT.2008.921711
   Jin X, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1671962.1671965
   Kehdi Elias, 2009, P 28 IEEE INT C COMP
   Kötter R, 2008, IEEE T INFORM THEORY, V54, P3579, DOI 10.1109/TIT.2008.926449
   Levine Brian Neil, 2006, TECH REP
   Li YK, 2010, PERFORM EVALUATION, V67, P1273, DOI 10.1016/j.peva.2010.08.005
   Liang J, 2005, IEEE INFOCOM SER, P1174
   Lin E., 2010, P 18 INT WORKSH QUAL, P1
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Sereno M., 2010, P ACM WORKSH ADV VID, P7
   Silverston T, 2009, COMPUT NETW, V53, P470, DOI 10.1016/j.comnet.2008.09.024
   Vieira Alex Borges, 2012, INT J COMPUT TELCOMM, V57, P1019
   Wang Q, 2011, PSYCHOL MED, V41, P1690, DOI 10.1017/S0033291710002412
   Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585
   Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085
   Yu Zhen, 2009, P 28 IEEE INT C COMP
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
   Zhang X., 2005, INFOCOM 2005. 24th Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings IEEE, V3, P13
NR 29
TC 7
Z9 7
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2014
VL 10
IS 3
AR 24
DI 10.1145/2568223
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF3AM
UT WOS:000334583800002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Nguyen, TV
   Liu, S
   Ni, BB
   Tan, J
   Rui, Y
   Yan, SC
AF Nguyen, Tam V.
   Liu, Si
   Ni, Bingbing
   Tan, Jun
   Rui, Yong
   Yan, Shuicheng
TI Towards Decrypting Attractiveness via Multi-Modality Cues
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; {Face, dressing, voice}
   attractiveness; latent attributes
ID OBJECT RECOGNITION; SHAPE; MODELS
AB Decrypting the secret of beauty or attractiveness has been the pursuit of artists and philosophers for centuries. To date, the computational model for attractiveness estimation has been actively explored in computer vision and multimedia community, yet with the focus mainly on facial features. In this article, we conduct a comprehensive study on female attractiveness conveyed by single/multiple modalities of cues, that is, face, dressing and/or voice, and aim to discover how different modalities individually and collectively affect the human sense of beauty. To extensively investigate the problem, we collect the Multi-Modality Beauty ((MB)-B-2) dataset, which is annotated with attractiveness levels converted from manual k-wise ratings and semantic attributes of different modalities. Inspired by the common consensus that middle-level attribute prediction can assist higher-level computer vision tasks, we manually labeled many attributes for each modality. Next, a tri-layer Dual-supervised Feature-Attribute-Task (DFAT) network is proposed to jointly learn the attribute model and attractiveness model of single/multiple modalities. To remedy possible loss of information caused by incomplete manual attributes, we also propose a novel Latent Dual-supervised Feature-Attribute-Task (LDFAT) network, where latent attributes are combined with manual attributes to contribute to the final attractiveness estimation. The extensive experimental evaluations on the collected (MB)-B-2 dataset well demonstrate the effectiveness of the proposed DFAT and LDFAT networks for female attractiveness prediction.
C1 [Nguyen, Tam V.; Liu, Si; Yan, Shuicheng] Natl Univ Singapore, Singapore 117583, Singapore.
   [Ni, Bingbing] Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Tan, Jun] Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
   [Rui, Yong] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 National University of Singapore; National University of Defense
   Technology - China; Microsoft Research Asia; Microsoft
RP Nguyen, TV (corresponding author), Natl Univ Singapore, 4 Engn Dr 3, Singapore 117583, Singapore.
EM tamnguyen@nus.edu.sg; dcslius@nus.edu.sg; bingbing.ni@adsc.com.sg;
   tanjun.nudt@gmail.com; yongrui@microsoft.com; eleyans@nus.edu.sg
RI Nguyen, Tam/HSG-3007-2023; Nguyen, Tam/AAU-6504-2020; Yan,
   Shuicheng/HCI-1431-2022
OI Nguyen, Tam/0000-0003-0236-7992; 
FU Singapore National Research Foundation under its International Research
   Centre @Singapore Funding Initiative; Human Sixth Sense Programme at the
   Advanced Digital Sciences Center from Singapore's Agency for Science,
   Technology and Research (A*Star)
FX This research is supported by the Singapore National Research Foundation
   under its International Research Centre @Singapore Funding Initiative
   and administered by the IDM Programme Office. B. Ni is supported by a
   research grant from the Human Sixth Sense Programme at the Advanced
   Digital Sciences Center from Singapore's Agency for Science, Technology
   and Research (A*Star).
CR ALLEY TR, 1991, PSYCHOL SCI, V2, P123, DOI 10.1111/j.1467-9280.1991.tb00113.x
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2007, MIR MATLAB
   [Anonymous], C P IEEE INT C SYST
   Beauprè MG, 2006, PERS SOC PSYCHOL B, V32, P16, DOI 10.1177/0146167205277097
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413
   Brinton D.G., 1890, RACES PEOPLES LECT S
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160
   DION K., 1972, J APPL SOC PSYCH, V24, P90
   Eisenthal Y, 2006, NEURAL COMPUT, V18, P119, DOI 10.1162/089976606774841602
   Glassenberg AN, 2010, ARCH SEX BEHAV, V39, P1289, DOI 10.1007/s10508-009-9559-6
   Gray D, 2010, LECT NOTES COMPUT SC, V6316, P434, DOI 10.1007/978-3-642-15567-3_32
   GREEN CD, 1995, PERCEPTION, V24, P937, DOI 10.1068/p240937
   Guo D, 2009, PROC CVPR IEEE, P73, DOI 10.1109/CVPRW.2009.5206833
   Haykin S., 1999, NEURAL NETWORK COMPR
   Hughes SM, 2004, EVOL HUM BEHAV, V25, P295, DOI 10.1016/j.evolhumbehav.2004.06.001
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Kagian A., 2005, ADV NEURAL INFORM PR, P649
   Kumar N, 2008, LECT NOTES COMPUT SC, V5305, P340, DOI 10.1007/978-3-540-88693-8_25
   Lennon S.J., 1990, HOME EC RES J, V18, P303, DOI DOI 10.1177/1077727X9001800403
   Likert R., 1932, TECHNIQUE MEASUREMEN, DOI 1933-01885-001
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Mittal A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.75
   Nguyen Tam V., 2012, PROC ACM INT C MULTI, P239
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pallett PM, 2010, VISION RES, V50, P149, DOI 10.1016/j.visres.2009.11.003
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Song Z, 2011, IEEE I CONF COMP VIS, P1084, DOI 10.1109/ICCV.2011.6126355
   Tanaka JW, 2004, COGNITION, V93, pB1, DOI 10.1016/j.cognition.2003.09.011
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   ZUCKERMAN M, 1993, J NONVERBAL BEHAV, V17, P119, DOI 10.1007/BF01001960
NR 38
TC 14
Z9 15
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2013
VL 9
IS 4
AR 28
DI 10.1145/2501643.2501650
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 206EW
UT WOS:000323501800006
DA 2024-07-18
ER

PT J
AU Zhou, WG
   Li, HQ
   Lu, YJ
   Tian, Q
AF Zhou, Wengang
   Li, Houqiang
   Lu, Yijuan
   Tian, Qi
TI SIFT Match Verification by Geometric Coding for Large-Scale
   Partial-Duplicate Web Image Search
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Verification; Image retrieval; partial
   duplicate; large scale; rotation-invariant; geometric square coding;
   geometric fan coding
ID SHAPE
AB Most large-scale image retrieval systems are based on the bag-of-visual-words model. However, the traditional bag-of-visual-words model does not capture the geometric context among local features in images well, which plays an important role in image retrieval. In order to fully explore geometric context of all visual words in images, efficient global geometric verification methods have been attracting lots of attention. Unfortunately, current existing methods on global geometric verification are either computationally expensive to ensure real-time response, or cannot handle rotation well. To solve the preceding problems, in this article, we propose a novel geometric coding algorithm, to encode the spatial context among local features for large-scale partial-duplicate Web image retrieval. Our geometric coding consists of geometric square coding and geometric fan coding, which describe the spatial relationships of SIFT features into three geo-maps for global verification to remove geometrically inconsistent SIFT matches. Our approach is not only computationally efficient, but also effective in detecting partial-duplicate images with rotation, scale changes, partial-occlusion, and background clutter.
   Experiments in partial-duplicate Web image search, using two datasets with one million Web images as distractors, reveal that our approach outperforms the baseline bag-of-visual-words approach even following a RANSAC verification in mean average precision. Besides, our approach achieves comparable performance to other state-of-the-art global geometric verification methods, for example, spatial coding scheme, but is more computationally efficient.
C1 [Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept EEIS, Hefei 230027, Peoples R China.
   [Lu, Yijuan] SW Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Texas State University System; Texas State University San
   Marcos; University of Texas System; University of Texas at San Antonio
   (UTSA)
RP Tian, Q (corresponding author), Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
EM qitian@cs.utsa.edu
RI Li, Houqiang Li/B-6259-2013; LU, YIJUAN/GNM-8769-2022
OI LU, YIJUAN/0000-0002-9855-8365
FU Fundamental Research Funds for the Central Universities of China
   [WK2100230003]; Research Enhancement Program (REP); Texas State
   University; DoD HBCU/MI [W911NF-12-1-0057]; NSF [IIS-1052851]; ARO
   [W911BF-12-1-0057]; Faculty Research Awards by Google FXPAL; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [1052851] Funding Source: National Science Foundation
FX This work is supported in part by the Fundamental Research Funds for the
   Central Universities of China (WK2100230003) to H. Li, in part by
   Research Enhancement Program (REP), start-up funding from the Texas
   State University and DoD HBCU/MI grant W911NF-12-1-0057 to Y. Lu, and in
   part by NSF IIS-1052851, Faculty Research Awards by Google FXPAL, NEC
   Laboratories of America, and ARO grant W911BF-12-1-0057 to Q. Tian.
CR Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Cao Y, 2010, PROC CVPR IEEE, P3352, DOI 10.1109/CVPR.2010.5540021
   CHANG SK, 1987, IEEE T PATTERN ANAL, V9, P413, DOI 10.1109/TPAMI.1987.4767923
   Chum O., 2004, Proc. of the Asian Conference on Computer Vision ACCV, V2, P812
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hoàng NV, 2010, PATTERN RECOGN, V43, P3013, DOI 10.1016/j.patcog.2010.03.024
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jegou H, 2007, PROC CVPR IEEE, P9
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384
   Nister David, 2006, CVPR
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Philbin J., 2008, P CVPR, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   Tang J., 2009, P ACM INT C MULT
   Tang JH, 2010, IEEE T MULTIMEDIA, V12, P131, DOI 10.1109/TMM.2009.2037373
   Wang X., 2011, P INT C COMP VIS
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
   Zhou W., 2011, P ACM INT C MULT
NR 29
TC 48
Z9 54
U1 1
U2 35
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2013
VL 9
IS 1
AR 4
DI 10.1145/2422956.2422960
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 097EO
UT WOS:000315457000004
DA 2024-07-18
ER

PT J
AU Hamza, A
   Hefeeda, M
AF Hamza, Ahmed
   Hefeeda, Mohamed
TI Energy-Efficient Multicasting of Multiview 3D Videos to Mobile Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Multimedia networking; energy saving; wireless networks; 3D
   video; WiMAX; LTE
ID O(N) ALGORITHM; 3-D VIDEO; DEPTH
AB Multicasting multiple video streams over wireless broadband access networks enables the delivery of multimedia content to large-scale user communities in a cost-efficient manner. Three dimensional (3D) videos are the next natural step in the evolution of digital media technologies. In order to provide 3D perception, 3D video streams contain one or more views that greatly increase their bandwidth requirements. Due to the limited channel capacity and variable bit rate of the videos, multicasting multiple 3D videos over wireless broadband networks is a challenging problem. In this article, we consider a 4G wireless access network in which a number of 3D videos represented in two-view plus depth format and encoded using scalable video coders are multicast. We formulate the optimal 3D video multicasting problem to maximize the quality of rendered virtual views on the receivers' displays. We show that this problem is NP-complete and present a polynomial time approximation algorithm to solve it. We then extend the proposed algorithm to efficiently schedule the transmission of the chosen substreams from each video in order to maximize the power saving on the mobile receivers. Our simulation-based experimental results show that our algorithm provides solutions that are within 0.3 dB of the optimal solutions while satisfying real-time requirements of multicast systems. In addition, our algorithm results in an average power consumption reduction of 86%.
C1 [Hamza, Ahmed; Hefeeda, Mohamed] Simon Fraser Univ, 102nd Ave, Survey, BC V3T 0A3, Canada.
C3 Simon Fraser University
RP Hamza, A (corresponding author), Simon Fraser Univ, 102nd Ave, Survey, BC V3T 0A3, Canada.
EM aah10@cs.sfu.ca; mhefeeda@cs.sfu.ca
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council (BCIC)
FX This work was partially supported by the Natural Sciences and
   Engineering Research Council (NSERC) of Canada and in part by the
   British Columbia Innovation Council (BCIC).
CR ABI RESEARCH, 2010, 3D MOB DEV
   Akar GB, 2007, IEEE T CIRC SYST VID, V17, P1622, DOI 10.1109/TCSVT.2007.905365
   [Anonymous], 2011, P 3DTV C TRUE VIS CA
   [Anonymous], 2010, P 1 ANN ACM C MULT S
   [Anonymous], P SPIE
   Arican Z, 2009, PROC SPIE, V7443, DOI 10.1117/12.829381
   Chen Y, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/786015
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   CPLEX, 2011, IBM ILOG CPLEX OPT
   de Diego Balaguer E., 2005, 2005 IEEE 16th International Symposium on Personal, Indoor and Mobile Radio Communications (IEEE Cat. No. 05TH8889), P2221
   De Silva D. V. S. X., 2010, Proceedings of the 2010 5th International Conference on Information and Automation for Sustainability (ICIAfS), P298, DOI 10.1109/ICIAFS.2010.5715677
   Dodgson NA, 2005, COMPUTER, V38, P31, DOI 10.1109/MC.2005.252
   DYER ME, 1984, MATH PROGRAM, V29, P57, DOI 10.1007/BF02591729
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   GOTFRYD M., 2008, JTC1SC29WG11 ISOIEC
   HARRISON D, 2010, HOCKEY NIGHT CANADA
   HEFEEDA M, 2008, P IEEE INN INF TECHN, P430
   INFORMA TELECOMS AND MEDIA, 2010, 22 5 MILL HOM WILL T
   Järvinen K, 2010, COMPUT COMMUN, V33, P1916, DOI 10.1016/j.comcom.2010.04.019
   JSVM, 2011, JOINT SCAL VID MOD J
   Kauff P, 2007, SIGNAL PROCESS-IMAGE, V22, P217, DOI 10.1016/j.image.2006.11.013
   Kellerer H., 2004, Knapsack Problems. Springer Nature Book Archives Millennium, P317
   Kimata H, 2004, FOURTH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY, PROCEEDINGS, P52, DOI 10.1109/CIT.2004.1357174
   Kumar A., 2008, MOBILE BROADCASTING
   Liu YW, 2009, SIGNAL PROCESS-IMAGE, V24, P666, DOI 10.1016/j.image.2009.06.002
   MASTERIMAGE, 2012, MAST 3D AUT 3D LCD
   Mathew R, 2010, IEEE T CIRC SYST VID, V20, P1331, DOI 10.1109/TCSVT.2010.2077480
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   MULTIMEDIA SCALABLE 3D FOR EUROPE (MUSCADE) PROJECT, 2010, MISS SCEN SERV REQ
   Petrovic G, 2010, PROC SPIE, V7524, DOI 10.1117/12.840230
   PHILIPS ELECTRONICS, 2012, PHIL WOWVX AUT DISPL
   PILKINGTON E, 2010, ESPN VIEWERS CAN WAT
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   *SEQ COMM, 2007, DAT SQN1130 SYST ON
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Su GM, 2011, INT J COMMUN SYST, V24, P1261, DOI 10.1002/dac.1190
   Tanimoto M., 2008, JTC1SC29WG11 ISOIEC
   Uehara S, 2008, PROC SPIE, V6803, DOI 10.1117/12.766936
   Urey H, 2011, P IEEE, V99, P540, DOI 10.1109/JPROC.2010.2098351
   VETRO A, 2004, P 23 PICT COD S PCS, P319
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WEARDEN G, 2010, 3D TV DOMINATES IFA
   Yang XD, 2004, SYMPOTIC '04: JOINT IST WORKSHOP ON MOBILE FUTURE & SYMPOSIUM ON TRENDS IN COMMUNICATIONS, PROCEEDINGS, P183, DOI 10.1109/TIC.2004.1409529
   Yuan GX, 2010, IEEE COMMUN MAG, V48, P88, DOI 10.1109/MCOM.2010.5402669
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
   ZEMEL E, 1984, INFORM PROCESS LETT, V18, P123, DOI 10.1016/0020-0190(84)90014-0
NR 48
TC 17
Z9 17
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 3
SU S
AR 45
DI 10.1145/2348816.2348824
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021VG
UT WOS:000309912900008
DA 2024-07-18
ER

PT J
AU Liu, YW
   Ci, S
   Tang, H
   Ye, Y
   Liu, JX
AF Liu, Yanwei
   Ci, Song
   Tang, Hui
   Ye, Yun
   Liu, Jinxia
TI QoE-Oriented 3D Video Transcoding for Mobile Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Mobile 3D video; transcoding; QoE;
   3D video streaming
AB With advance in mobile 3D display, mobile 3D video is already enabled by the wireless multimedia networking, and it will be gradually popular since it can make people enjoy the natural 3D experience anywhere and anytime. In current stage, mobile 3D video is generally delivered over the heterogeneous network combined by wired and wireless channels. How to guarantee the optimal 3D visual quality of experience (QoE) for the mobile 3D video streaming is one of the important topics concerned by the service provider. In this article, we propose a QoE-oriented transcoding approach to enhance the quality of mobile 3D video service. By learning the pre-controlled QoE patterns of 3D contents, the proposed 3D visual QoE inferring model can be utilized to regulate the transcoding configurations in real-time according to the feedbacks of network and user-end device information. In the learning stage, we propose a piecewise linear mean opinion score (MOS) interpolation method to further reduce the cumbersome manual work of preparing QoE patterns. Experimental results show that the proposed transcoding approach can provide the adapted 3D stream to the heterogeneous network, and further provide superior QoE performance to the fixed quantization parameter (QP) transcoding and mean squared error (MSE) optimized transcoding for mobile 3D video streaming.
C1 [Liu, Yanwei; Ci, Song; Tang, Hui] Chinese Acad Sci, Inst Acoust, Beijing 100190, Peoples R China.
   [Ci, Song; Ye, Yun] Univ Nebraska Lincoln, Peter Kiewit Inst 200B, Omaha, NE 68182 USA.
   [Liu, Jinxia] Zhejiang Wanli Univ, Ningbo 315100, Zhejiang, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Acoustics, CAS; University of
   Nebraska System; University of Nebraska Lincoln; Zhejiang Wanli
   University
RP Liu, YW (corresponding author), Chinese Acad Sci, Inst Acoust, 21,N 4th Ring W Rd, Beijing 100190, Peoples R China.
EM liuyw@hpnl.ac.cn; cisong@ieee.org; tangh@hpnl.ac.cn;
   yye@huskers.unl.edu; liujinxia1969@126.com
RI Ci, Song/R-8324-2019; liu, yanwei/L-2453-2019; Liu, Jinxia/H-1794-2011
FU Important National Science & Technology Specific Project
   [2012ZX03003006-004, 2011ZX03004-005]; NSFC [60972083, 61102077];
   Chinese Academy of Sciences [Y129081621]
FX This work was supported in part by Important National Science &
   Technology Specific Project under contracts 2012ZX03003006-004 and
   2011ZX03004-005, NSFC under grant Nos. 60972083 and 61102077, and
   Initial Fund of President Award of Chinese Academy of Sciences under
   contract Y129081621.
CR Ameigeiras P, 2010, COMPUT COMMUN, V33, P571, DOI 10.1016/j.comcom.2009.10.016
   [Anonymous], 3D MOVIE MAKING STER
   [Anonymous], TRUE VIS CAPT TRANSM
   [Anonymous], 2002, METH SUBJ ASS QUAL T
   [Anonymous], P IEEE INT C AC SPEE
   [Anonymous], NEW APP 1 DEF QUAL E
   CHEN W., 2010, P VPQM
   Chen ZP, 2010, PROCEEDINGS OF THE ASME PRESSURE VESSELS AND PIPING CONFERENCE 2010, VOL 1: CODES AND STANDARDS, P19
   Domanski M, 2009, ISO/IEC JTC1/SC29/WG11 MPEG/M17050
   Girod Bernd, 1993, P207
   Ho Yo-Sung, 2008, JTC1SC29WG11 ISOIEC
   Jumisko-Pyykkö S, 2011, PROC SPIE, V7881, DOI 10.1117/12.879226
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Liang YJ, 2008, IEEE T CIRC SYST VID, V18, P861, DOI 10.1109/TCSVT.2008.923139
   Liu S., 2010, ACM MULTIMEDIA, P795
   Liu YW, 2012, 3D RES, V3, DOI 10.1007/3DRes.01(2012)5
   Liu YW, 2009, SIGNAL PROCESS-IMAGE, V24, P666, DOI 10.1016/j.image.2009.06.002
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   MERKLE P., 2009, P IEEE 3DTV C POTSD
   Mohamed S, 2002, IEEE T CIRC SYST VID, V12, P1071, DOI 10.1109/TCSVT.2002.806808
   PENG G., 2010, P 11 PAC RIM C MULT
   PERKINS MG, 1992, IEEE T COMMUN, V40, P684, DOI 10.1109/26.141424
   Piamrat K., 2009, P IEEE GLOBECOM
   Piamrat K., 2008, P IEEE VTC, P1
   RUCKERT J., 2012, QUALITY ADAPTATION P
   Seuntiens P. J. H., 2006, THESIS TU EINDHOVEN
   Smolic A, 2011, P IEEE, V99, P607, DOI 10.1109/JPROC.2010.2098350
   Thakolsri S., 2010, P 18 ACM INT C MULT, P783
   Vetro A, 2005, IEEE WIREL COMMUN, V12, P14, DOI 10.1109/MWC.2005.1497854
   VETRO A., 2011, IEEE T BROADCAST, V57, P348
   WORRALL S. T., 2010, 2010 NEW SUMM FUT ME
   Yin P, 2003, P SOC PHOTO-OPT INS, V5022, P479, DOI 10.1117/12.476355
   Zou William, 2009, Information Display, V25, P14
NR 33
TC 17
Z9 18
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 3
SU S
AR 42
DI 10.1145/2348816.2348821
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021VG
UT WOS:000309912900005
DA 2024-07-18
ER

PT J
AU Huang, JL
   Chiu, SC
   Shan, MK
AF Huang, Jiun-Long
   Chiu, Shih-Chuan
   Shan, Man-Kwan
TI Towards an Automatic Music Arrangement Framework Using Score Reduction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Theory; Score reduction; automatic music arrangement; piano reduction;
   phrase selection
AB Score reduction is a process that arranges music for a target instrument by reducing original music. In this study we present a music arrangement framework that uses score reduction to automatically arrange music for a target instrument. The original music is first analyzed to determine the type of arrangement element of each section, then the phrases are identified and each is assigned a utility according to its type of arrangement element. For a set of utility-assigned phrases, we transform the music arrangement into an optimization problem and propose a phrase selection algorithm. The music is arranged by selecting appropriate phrases satisfying the playability constraints of a target instrument. Using the proposed framework, we implement a music arrangement system for the piano. An approach similar to Turing test is used to evaluate the quality of the music arranged by our system. The experiment results show that our system is able to create viable music for the piano.
C1 [Huang, Jiun-Long; Chiu, Shih-Chuan] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
   [Shan, Man-Kwan] Natl Chengchi Univ, Dept Comp Sci, Taipei 116, Taiwan.
C3 National Yang Ming Chiao Tung University; National Chengchi University
RP Huang, JL (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
EM jlhuang@cs.nctu.edu.tw
CR [Anonymous], 1992, P 5 ANN WORKSHOP COM
   BERNDT A., 2006, P AUD MOSTL C
   BRUCKER P, 1994, SIAM J COMPUT, P52
   CAMBOUROPOULOS E, 2001, P INT COMP MUSIC C I
   CHUNG J. W, 2006, P C HUM FACT COMP SY
   Corozine Vince., 2002, Arranging Music for the Real World: Classical and Commercial Aspects
   DANIEL R. AND POTTER, 2006, P INT C EV COMP CEC
   Hastie T, 1998, ANN STAT, V26, P451
   Jones NC., 2004, An introduction to bioinformatics algorithms
   KASIMI A. A., 2005, P INT C MUS INF RETR
   Kasimi A. A., 2007, P INT C MUS INF RETR
   Lui S, 2006, IEEE MULTIMEDIA, V13, P52, DOI 10.1109/MMUL.2006.35
   Miranda Eduardo, 2001, Composing music with computers, DOI DOI 10.4324/9780080502403
   Nagashima T, 1997, INT J INTELL SYST, V12, P323, DOI 10.1002/(SICI)1098-111X(199704)12:4<323::AID-INT5>3.0.CO;2-Q
   Owsinski Bobby., 1999, MIXING ENG HDB
   PEARCE M, 2001, P S ART INT CREAT AR
   RIMSKY-KORSAKOV N. A, 1888, SHEHERAZADE OP
   Sorensen A., 2000, P AUSTR COMP MUS C
   Stein Leon., 1979, STRUCTURE STYLE STUD
   Tuohy D. R., 2006, P INT COMP MUS C ICM
   White Gary., 1992, Instrumental Arranging
   Witten I. H., 2005, DATA MINING PRACTICA
   Yonebayashi Y., 2007, P INT JOINT C ART IN
NR 23
TC 5
Z9 5
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2012
VL 8
IS 1
AR 8
DI 10.1145/2071396.2071404
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 894AV
UT WOS:000300400200008
DA 2024-07-18
ER

PT J
AU Biel, JI
   Gatica-Perez, D
AF Biel, Joan-Isaac
   Gatica-Perez, Daniel
TI VlogSense: Conversational Behavior and Social Attention in YouTube
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Measurement; vlogging; YouTube; social media; nonverbal
   behavior
ID DOMINANCE
AB We introduce the automatic analysis of conversational vlogs (VlogSense, for short) as a new research domain in social media. Conversational vlogs are inherently multimodal, depict natural behavior, and are suitable for large-scale analysis. Given their diversity in terms of content, VlogSense requires the integration of robust methods for multimodal analysis and for social media understanding. We present an original study on the automatic characterization of vloggers' audiovisual nonverbal behavior, grounded in work from social psychology and behavioral computing. Our study on 2,269 vlogs from YouTube shows that several nonverbal cues are significantly correlated with the social attention received by videos.
C1 [Biel, Joan-Isaac] Ctr Parc, Idiap Res Inst, CH-1920 Martigny, Switzerland.
   Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Biel, JI (corresponding author), Ctr Parc, Idiap Res Inst, Rue Marconi 19,CP 592, CH-1920 Martigny, Switzerland.
EM jibiel@idiap.ch
FU Swiss National Science Foundation under the National Center of
   Competence in Research (NCCR) on Interactive Multimodal Information
   Management (IM) 2
FX This work is supported by the Swiss National Science Foundation under
   the National Center of Competence in Research (NCCR) on Interactive
   Multimodal Information Management (IM) 2.
CR AMBADY N, 1992, PSYCHOL BULL, V111, P256, DOI 10.1037/0033-2909.111.2.256
   [Anonymous], P ACM SIGCOMM INT ME
   [Anonymous], 2008, HONEST SIGNALS, DOI DOI 10.7551/MITPRESS/8022.001.0001
   [Anonymous], P 3 ACM INT WORKSH H
   BIEL JI, 2010, P INT AAAI C WEBL SO
   BIEL JI, 2009, P 17 ACM INT C MULT
   Bradski G., 2008, LEARNING OPENCV
   Burgess J., 2009, YouTube: Online video and participatory culture
   Cheng X, 2008, P IEEE 16 INT WORKSH
   CURHAN JR, 2007, J APPL PSYCH, V92
   DEVASCONCELOS JE, 2009, P INT ACM C SUPP GRO
   DOVIDIO JF, 1982, SOC PSYCHOL QUART, V45, P106, DOI 10.2307/3033933
   EVANS DC, 2008, P INT AAAI C WEBL SO
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   GILL JA, 2009, P INT AAAI C WEBL SO
   Goswami S., 2009, P INT AAAI C WEBL SO
   Griffith M., 2007, P ANN M ASS ED JOURN
   HALVEY MartinJ., 2007, P 16 INT C WORLD WID
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   Harley D, 2009, UNIVERSAL ACCESS INF, V8, P5, DOI 10.1007/s10209-008-0127-y
   HUBERMAN BA, 2009, J INFORM SCI, V35
   HUNG H, 2008, P 10 INT C MULT INT
   JAYAGOPI DB, 2008, P 10 INT C MULT INT
   Jayagopi DB, 2009, IEEE T AUDIO SPEECH, V17, P501, DOI 10.1109/TASL.2008.2008238
   Knapp M.L., 2005, Nonverbal communication in human interaction, V6th
   Kramer A., 2008, P 26 ANN SIGCHI C HU
   Landry B.M., 2008, ART CIRCUS CHARACTER
   LANGE PG, 2007, J COMPUT MEDIAT COMM, V1
   Levelt W., 1989, Speaking-from intention to articulation
   LIN WH, 2008, P AAAI FALL S MULT I
   Mishne Gilad, 2005, P SIGIR WORKSH STYL
   MISLOVE A, 2007, P ACM SIGCOMM INT ME
   MOLYNEAUX H, 2008, AM COMM J, V10
   NGUYEN DT, 2009, P 27 ANN SIGCHI C HU
   Scherer K., 1979, SOCIAL MARKERS SPEEC, P147
   SELLEN AJ, 1995, HUM-COMPUT INTERACT, V10, P401, DOI 10.1207/s15327051hci1004_2
   Strangelove Michael., 2010, Watching YouTube: Extraordinary Videos by Ordinary People
   VIOLA P, 2002, INT J COMPUT VIS, V57
   Vonderau P., 2010, YOUTUBE READER
   Zancanaro M., 2006, P 8 INT C MULT INT
   ZHANG X, 2009, IEEE T MULTIMEDIA, V11
NR 41
TC 18
Z9 22
U1 0
U2 36
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 33
DI 10.1145/2037676.2037690
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 857NR
UT WOS:000297725800014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Adams, B
   Phung, D
   Venkatesh, S
AF Adams, Brett
   Phung, Dinh
   Venkatesh, Svetha
TI Sensing and Using Social Context
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Human Factors; Experimentation; Multimedia browsing; social
   context
AB We present online algorithms to extract social context: Social spheres are labeled locations of significance, represented as convex hulls extracted from GPS traces. Colocation is determined from Bluetooth and GPS to extract social rhythms, patterns in time, duration, place, and people corresponding to real-world activities. Social ties are formulated from proximity and shared spheres and rhythms. Quantitative evaluation is performed for 10+ million samples over 45 man-months. Applications are presented with assessment of perceived utility: Socio-Graph, a video and photo browser with filters for social metadata, and Jive, a blog browser that uses rhythms to discover similarity between entries automatically.
C1 [Adams, Brett] Curtin Univ Technol, Dept Comp, Perth, WA 6845, Australia.
C3 Curtin University
RP Adams, B (corresponding author), Curtin Univ Technol, Dept Comp, Perth, WA 6845, Australia.
EM badams@curtin.edu.au
RI Phung, Dinh Q/D-1328-2012
OI Venkatesh, Svetha/0000-0001-8675-6631; Phung, Dinh/0000-0002-9977-8247
CR ADAMS B, 2006, P ACM INT C MULT
   ADAMS B, 2006, P INT C MULT EXP
   ADAMS B, 2005, P ACM INT C MULT
   [Anonymous], P 13 ANN ACM INT C M
   [Anonymous], 2000, The Humane Interface: New directions for designing interactive systems
   [Anonymous], P ACM MULT
   APPAN P, 2004, P ACM INT C MULT
   Ashbrook D, 2003, PERS UBIQUIT COMPUT, V7, P275, DOI 10.1007/s00779-003-0240-0
   Carrasco JA, 2006, TRANSPORTATION, V33, P463, DOI 10.1007/s11116-006-8074-z
   CHOUDHURY T, 2004, P C NEUR INF PROC SY
   Clarkson B, 2002, THESIS MIT
   Cooper M, 2005, ACM T MULTIM COMPUT, V1
   COUNTS S, 2005, P C HUM FACT COMP SY, P1308
   Cox LP, 2006, SEVENTH IEEE WORKSHOP ON MOBILE COMPUTING SYSTEMS & APPLICATIONS, PROCEEDINGS, P55, DOI 10.1109/WMCSA.2006.21
   DAVIS M, 2005, P 13 ANN ACM INT C M, P267
   de Berg M., 2000, COMPUTATIONAL GEOMET
   DREYFUS Hubert., 2001, INTERNET
   EAGLE N, 2006, P ROY SOC A IN PRESS
   Eagle N. N., 2005, THESIS MIT
   Ester M., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P323
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Gehl J., 1987, Life Between Buildings: Using Public Space
   Graham A., 2002, Proceedings of the second ACM/IEEE-CS joint conference on Digital libraries, P326
   Hariharan R, 2004, LECT NOTES COMPUT SC, V3234, P106
   HOGAN B, 2005, MAKING CONNECTED LIV
   Jones Q., 2004, P 2004 ACM C COMPUTE, P202, DOI DOI 10.1145/1031607.1031640
   Kang JH, 2004, P 2 ACM INT WORKSH W, V9, P110, DOI [DOI 10.1145/1024733.1024748, 10.1145/1024733.1024748.766]
   Kern N, 2003, SEVENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P223, DOI 10.1109/ISWC.2003.1241415
   KRUMM J, 2004, P INT C MOB UB SYST
   LEE H, 2002, J DIG INFORM, V2
   LUDFORD P, 2007, P SIGCHI C HUM FACT
   MARSDEN PV, 1984, SOC FORCES, V63, P482, DOI 10.2307/2579058
   Mika P, 2004, P 1 WORKSH FRIEND FR
   Naaman M, 2005, ACM-IEEE J CONF DIG, P178, DOI 10.1145/1065385.1065430
   Naaman M., 2004, P 12 ANN ACM INT C M, P196
   NAIR R, 2005, ADJ P 7 INT C UB COM
   Nippert-Eng ChristenaE., 1995, HOME WORK
   Norman D., 1999, The Design of Everyday Things
   Nurmi P., 2006, P 3 ANN INT C MOB UB
   Ratti C., 2005, P 3 S LBS TELECARTOG
   Rice R., 2001, ACCESSING BROWSING I
   Rodden K., 2003, P SIGCHI C HUMAN FAC, P409, DOI DOI 10.1145/642611.642682
   Westermann U, 2007, IEEE MULTIMEDIA, V14, P19, DOI 10.1109/MMUL.2007.23
   ZHOU C, 2005, P 1 INT WORKSH DAT M
NR 44
TC 17
Z9 21
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2008
VL 5
IS 2
AR 11
DI 10.1145/1413862.1413864
PG 27
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 376AU
UT WOS:000261155900002
DA 2024-07-18
ER

PT J
AU Hefeeda, M
   Hsu, CH
AF Hefeeda, Mohamed
   Hsu, Cheng-Hsin
TI Rate-distortion optimized streaming of fine-grained scalable video
   sequences
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE performance; fine-grained scalable streaming; FGS; rate-distortion
   optimized streaming; video streaming; peer-to-peer streaming;
   distributed streaming; rate-distortion models
AB We present optimal schemes for allocating bits of fine-grained scalable video sequences among multiple senders streaming to a single receiver. This allocation problem is critical in optimizing the perceived quality in peer-to-peer and distributed multi-server streaming environments. Senders in such environments are heterogeneous in their outgoing bandwidth and they hold different portions of the video stream. We first formulate and optimally solve the problem for individual frames, then we generalize to the multiple frame case. Specifically, we formulate the allocation problem as an optimization problem, which is nonlinear in general. We use rate-distortion models in the formulation to achieve the minimum distortion in the rendered video, constrained by the outgoing bandwidth of senders, availability of video data at senders, and incoming bandwidth of receiver. We show how the adopted rate-distortion models transform the nonlinear problem to an integer linear programming (ILP) problem. We then design a simple rounding scheme that transforms the ILP problem to a linear programming (LP) one, which can be solved efficiently using common optimization techniques such as the Simplex method. We prove that our rounding scheme always produces a feasible solution, and the solution is within a negligible margin from the optimal solution. We also propose a new algorithm (FGSAssign) for the single-frame allocation problem that runs in O(n log n) steps, where n is the number of senders. We prove that FGSAssign is optimal. Furthermore, we propose a heuristic algorithm (mFGSAssign) that produces near-optimal solutions for the multiple-frame case, and runs an order of magnitude faster than the optimal one. Because of its short running time, mFGSAssign can be used in real time. Our experimental study validates our analytical analysis and shows the effectiveness of our allocation algorithms in improving the video quality.
C1 [Hefeeda, Mohamed; Hsu, Cheng-Hsin] Simon Fraser Univ, Surrey, BC V3T AO3, Canada.
C3 Simon Fraser University
RP Hefeeda, M (corresponding author), Simon Fraser Univ, 250-13450,102nd Ave, Surrey, BC V3T AO3, Canada.
EM mhefeeda@cs.sfu.ca; cha16@cs.sfu.ca
CR [Anonymous], 1982, COMBINATORIAL OPTIMI
   [Anonymous], 2004, 144962 ISOIEC
   BEGEN A, 2003, P IEEE INT C IM PROC
   CHAKARESKI J, 2003, P DAT COMP C DCC 03
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   CUI Y, 2003, P ACM INT WORKSH NET
   DAI M, 2004, P IEEE INT C IM PROC
   DAI M, 2003, P ACM INT WORKSH NET
   de Philippe C, 2005, INT J COMMUN SYST, V18, P449, DOI 10.1002/dac.711
   Goldfarb D., 1989, HDB OR MS, V1, P73
   Hefeeda M, 2005, MULTIMEDIA SYST, V11, P68, DOI 10.1007/s00530-005-0191-6
   HSU C, 2006, 200612 S FRAS U
   HSU C, 2006, 200620 S FRAS U
   *ISO IEC, 2004, 144965 ISOIEC
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   MAGHAREI N, 2006, ACM SPRINGER MULTIME, V11, P1
   Marler RT, 2004, STRUCT MULTIDISCIP O, V26, P369, DOI 10.1007/s00158-003-0368-6
   NGUYEN T, 2002, P MULT COMP NETW MMC
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   SCHWARZ H, 2006, P EUR S MOB MED DEL
   SERMADEVI Y, 2003, P DAT COMP C DCC 03
   SU X, 2006, ACM SPRINGER MULTIME, V11, P455
   SUN J, 2005, P SPIE INT C VIS COM
   Wang Y., 2002, VIDEO PROCESSING COM
   Zhang XM, 2003, IEEE T CIRC SYST VID, V13, P121, DOI 10.1109/TCSVT.2002.808437
   ZINK M, 2003, P IEEE INT WORKSH QU
NR 27
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 1
AR 2
DI 10.1145/1324287.1324289
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 264UK
UT WOS:000253315700002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hsu, CH
   Hefeeda, M
AF Hsu, Cheng-Hsin
   Hefeeda, Mohamed
TI On the accuracy and complexity of rate-distortion models for
   fine-grained scalable video sequences
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE performance; multimedia streaming; fine-grained scalable coding;
   rate-distortion models
AB Rate-distortion ( R-D) models are functions that describe the relationship between the bitrate and expected level of distortion in the reconstructed video stream. R-D models enable optimization of the received video quality in different network conditions. Several R-D models have been proposed for the increasingly popular fine-grained scalable video sequences. However, the models' relative performance has not been thoroughly analyzed. Moreover, the time complexity of each model is not known, nor is the range of bitrates in which the model produces valid results. This lack of quantitative performance analysis makes it difficult to select the model that best suits a target streaming system. In this article, we classify, analyze, and rigorously evaluate all R-D models proposed for FGS coders in the literature. We classify R-D models into three categories: analytic, empirical, and semi-analytic. We describe the characteristics of each category. We analyze the R-D models by following their mathematical derivations, scrutinizing the assumptions made, and explaining when the assumptions fail and why. In addition, we implement all R-D models, a total of eight, and evaluate them using a diverse set of video sequences. In our evaluation, we consider various source characteristics, diverse channel conditions, different encoding/decoding parameters, different frame types, and several performance metrics including accuracy, range of applicability, and time complexity of each model. We also present clear systematic ways ( pseudo codes) for constructing various R-D models from a given video sequence. Based on our experimental results, we present a justified list of recommendations on selecting the best R-D models for video-on-demand, video conferencing, real-time, and peer-to-peer streaming systems.
C1 [Hsu, Cheng-Hsin; Hefeeda, Mohamed] Simon Fraser Univ, Surrey, BC V3T AO3, Canada.
   [Hefeeda, Mohamed] Mansoura Univ, Mansoura, Egypt.
C3 Simon Fraser University; Egyptian Knowledge Bank (EKB); Mansoura
   University
RP Hsu, CH (corresponding author), Simon Fraser Univ, 250-13450 102nd Ave, Surrey, BC V3T AO3, Canada.
EM cha16@cs.sfu.ca; mhefeeda@cs.sfu.ca
CR Adjeroh DA, 2004, IEEE T MULTIMEDIA, V6, P58, DOI 10.1109/TMM.2003.819578
   [Anonymous], 2004, 144962 ISOIEC
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   DAI M, 2004, P IEEE INT C IM PROC
   DAI M, 2004, THESIS TEXAS A M U
   DAI M, 2003, P IEEE INT C IM PROC
   DAI M, 2003, P ACM INT WORKSH NET
   Dai M, 2006, IEEE T MULTIMEDIA, V8, P1135, DOI 10.1109/TMM.2006.884626
   Ding W, 1996, IEEE T CIRC SYST VID, V6, P12, DOI 10.1109/76.486416
   Hang HM, 1997, IEEE T CIRC SYST VID, V7, P287
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P1221, DOI 10.1109/76.974677
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P970, DOI 10.1109/TCSVT.2002.805511
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P928, DOI 10.1109/76.937431
   HSU C, 2006, SOURCE MODELS FINE G
   HSU C, 2006, 200612 TR SIMON FRAS
   *ISO IEC, 2004, 144965 ISOIEC
   JOSHI RL, 1995, IEEE SIGNAL PROC LET, V2, P81, DOI 10.1109/97.386283
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Lin LJ, 1998, IEEE T CIRC SYST VID, V8, P446, DOI 10.1109/76.709411
   Mallat S, 1998, IEEE T SIGNAL PROCES, V46, P1027, DOI 10.1109/78.668554
   Martinez WL., 2002, Computational Statistics Handbook with MATLAB
   MULLER F, 1993, ELECTRON LETT, V29, P1935, DOI 10.1049/el:19931288
   PARK HJ, 2004, P ADV NEUR INF PROC
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   SUN J, 2005, P SPIE INT C VIS COM
   VARANASI MK, 1989, J ACOUST SOC AM, V86, P1404, DOI 10.1121/1.398700
   Zhang XM, 2003, IEEE T CIRC SYST VID, V13, P121, DOI 10.1109/TCSVT.2002.808437
NR 28
TC 0
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 2
AR 15
DI 10.1145/1352012.1352019
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 306RG
UT WOS:000256264900007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Agrafiotis, D
   Davies, SJC
   Canagarajah, N
   Bull, DR
AF Agrafiotis, D.
   Davies, S. J. C.
   Canagarajah, N.
   Bull, D. R.
TI Towards efficient context-specific video coding based on gaze-tracking
   analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; human factors; eye tracking; context-based video coding;
   subjective video quality; transformation of eye movements into useful
   knowledge; multimedia perceptual quality; applications
ID VISUAL-ATTENTION; MODEL
AB This article discusses a framework for model-based, context-dependent video coding based on exploitation of characteristics of the human visual system. The system utilizes variable-quality coding based on priority maps which are created using mostly context-dependent rules. The technique is demonstrated through two case studies of specific video context, namely open signed content and football sequences. Eye-tracking analysis is employed for identifying the characteristics of each context, which are subsequently exploited for coding purposes, either directly or through a gaze prediction model. The framework is shown to achieve a considerable improvement in coding efficiency.
C1 [Agrafiotis, D.; Davies, S. J. C.; Canagarajah, N.; Bull, D. R.] Univ Bristol, Dept Elect & Elect Engn, Bristol BS8 1TH, Avon, England.
C3 University of Bristol
RP Agrafiotis, D (corresponding author), Univ Bristol, Dept Elect & Elect Engn, Senate House,Tyndall Ave, Bristol BS8 1TH, Avon, England.
EM d.agrafiotis@bristol.ac.uk
CR AGRAFIOTIS D, 2003, P INT C VIS COMM IM
   Agrafiotis D, 2006, SIGNAL PROCESS-IMAGE, V21, P531, DOI 10.1016/j.image.2006.02.003
   Appleby S, 2006, BT TECHNOL J, V24, P174, DOI 10.1007/s10550-006-0056-3
   Chen MJ, 2003, IEEE T CONSUM ELECTR, V49, P724, DOI 10.1109/TCE.2003.1233810
   Cheng WH, 2005, IEICE T INF SYST, VE88D, P1578, DOI 10.1093/ietisy/e88-d.7.1578
   CRABTREE B, 2006, P PICT COD S CHIN
   DALY S, 1998, P INT C IM PROC CHIC
   GEILSER WS, 1998, P SPIE C HUM VIS EL, P3299
   *ISO IEC, 2003, 1449610 ISO IEC 10
   *ISO IEC, 2000, 1449622000 ISO IEC
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   ITTI L, 2005, P INT C COMP VIS PAT
   ITTI L, 2005, ADV NEURAL INFORM PR, V19, P1
   *ITU, 1995, 500112002 R BT
   Lee S, 2003, IEEE T CIRC SYST VID, V13, P149, DOI 10.1109/TCSVT.2002.808441
   LIN CW, 2000, P INT C IM PROC VANC
   LIU Y, 2006, P INT C IM PROC ATL
   MUIR L, 2003, P PICT COD S SAINT M
   Nadenau MJ, 2002, SIGNAL PROCESS-IMAGE, V17, P807, DOI 10.1016/S0923-5965(02)00060-7
   PRIVITERA C, 1997, M9772 UCB ERL
   Tang CW, 2006, IEEE T MULTIMEDIA, V8, P11, DOI 10.1109/TMM.2005.861295
NR 22
TC 5
Z9 5
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 4
AR 22
DI 10.1145/1314303.1314307
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 250QU
UT WOS:000252315900004
DA 2024-07-18
ER

PT J
AU Wang, S
   Dash, M
   Chia, LT
   Xu, M
AF Wang, Surong
   Dash, Manoranjan
   Chia, Liang-Tien
   Xu, Min
TI Efficient sampling of training set in large and noisy multimedia data
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; performance; sampling; noise; histogram; image
   classification; audio event identification
AB As the amount of multimedia data is increasing day-by-day thanks to less expensive storage devices and increasing numbers of information sources, machine learning algorithms are faced with large-sized and noisy datasets. Fortunately, the use of a good sampling set for training influences the final results significantly. But using a simple random sample (SRS) may not obtain satisfactory results because such a sample may not adequately represent the large and noisy dataset due to its blind approach in selecting samples. The difficulty is particularly apparent for huge datasets where, due to memory constraints, only very small sample sizes are used. This is typically the case for multimedia applications, where data size is usually very large. In this article we propose a new and efficient method to sample of large and noisy multimedia data. The proposed method is based on a simple distance measure that compares the histograms of the sample set and the whole set in order to estimate the representativeness of the sample. The proposed method deals with noise in an elegant manner which SRS and other methods are not able to deal with. We experiment on image and audio datasets. Comparison with SRS and other methods shows that the proposed method is vastly superior in terms of sample representativeness, particularly for small sample sizes although time-wise it is comparable to SRS, the least expensive method in terms of time.
C1 Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Wang, S (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Blk N4-02a-32,Nanyang Ave, Singapore 639798, Singapore.
EM pg02759741@ntu.edu.sg; asmdash@ntu.edu.sg; asltchia@ntu.edu.sg;
   mxu@ntu.edu.sg
RI Chia, Liang-Tien/A-9874-2008
OI Xu, Min/0000-0001-9581-8849
CR Agrawal R., 1994, PROC 20 INT C VERY L
   Angluin D., 1988, Machine Learning, V2, P319, DOI 10.1007/BF00116828
   [Anonymous], 2000, Proceedings of the 17th International Conference on Machine Learning (ICML 2000)
   [Anonymous], 1995, 12 INT C MACH LEARN
   [Anonymous], 1993, P INT C MAN DAT
   [Anonymous], 1994, SIGIR
   ASTASHYN A, 2004, THESIS
   Atlas L.E., 1990, NIPS, V2, P566
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655
   Brodley CE, 1999, J ARTIF INTELL RES, V11, P131, DOI 10.1613/jair.606
   BRONNIMANN H, 2003, P 9 ACM SIGKDD INT C, P59
   Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646
   Chawla N, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P580, DOI 10.1109/ICDM.2001.989568
   CHEN B, 2002, P INT C KNOWL DISC D
   Cohn D. A., 1995, Advances in Neural Information Processing Systems 7, P705
   DUAN L, 2003, P ACM MULT C
   Gu B, 2000, SAMPLING ITS APPL DA
   HAN J, 2000, P INT C MAN DAT
   *ISO IEC IED, 2005, ISOIEC159388FDIS3 IE
   Iyengar V.S., 2000, Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, (Boston, Massachusetts, USA), P92
   JIN R, 2003, AAAI SPRING S INT MU
   Lewis D.D., 1994, MACH LEARN P 1994, P148
   Manjunath B.S., 2002, INTRO MPEG 7
   Meek C, 2002, J MACH LEARN RES, V2, P397, DOI 10.1162/153244302760200678
   NEPAL S, 2001, P ACM MULT C LOS ANG
   Ojala T, 2002, INT C PATT RECOG, P1021, DOI 10.1109/ICPR.2002.1048479
   Olken F., 1993, Random Sampling from Databases
   PLUTOWSKI M, 1993, IEEE T NEURAL NETWOR, V4, P305, DOI 10.1109/72.207618
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Saar-Tsechansky Maytal., 2001, Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence (IJCAI-2001), P911
   SARAWAGI S, 2002, P 8 ACM INT C KNOWL
   Scheffer T., 2001, P INT S INT DAT AN
   VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165
   WANG S, 2005, P INT C MULT EXP
   WANG S, 2005, P PAC AS C KNOWL DIS
   XU M, 2004, P PAC C MULT, V3, P566
   XU M, 2004, P ACM MULT C
   Young S., 2002, HTK BOOK HTK VERSION
   Zhu Xingquan., 2003, Proceedings of the 20th International Conference on Machine Learning, P920
   Zhu XQ, 2004, ARTIF INTELL REV, V22, P177, DOI 10.1007/s10462-004-0751-8
NR 40
TC 11
Z9 13
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 3
AR 14
DI 10.1145/1236471.1236473
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230JB
UT WOS:000250871700002
DA 2024-07-18
ER

PT J
AU Xu, CS
   Maddage, NC
   Shao, X
   Tian, Q
AF Xu, Changsheng
   Maddage, Namunu C.
   Shao, Xi
   Tian, Qi
CA Inst Infocomm Res
TI Content-adaptive digital music watermarking based on music structure
   analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; experimentation; performance; content-adaptive; digital
   watermarking; music structure; note-based segmentation; inaudibility;
   robustness
ID AUDIO; ROBUST; SCALE
AB A novel content-adaptive music watermarking technique is proposed in this article. To optimally balance inaudibility and robustness when embedding and extracting watermarks, the embedding scheme is highly related to the music structure and human auditory system (HAS). A note-based segmentation method is proposed and used for music vocal/instrumental boundary detection. A multiple bit hopping and hiding scheme with different embedding parameters is applied to vocal and instrumental frames of the music. The experimental results in inaudibility and robustness are provided to support all novel features in the proposed watermarking scheme.
C1 Inst Infocomm Res, Singapore 119613, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R)
RP Xu, CS (corresponding author), Inst Infocomm Res, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore.
EM xucs@i2r.a-star.edu.sg
RI xu, cj/HJZ-3488-2023; shao, xi/ABE-3263-2021; chen, yue/JXW-9556-2024
CR [Anonymous], 1997, P 5 EUR C SPEECH COM
   BASSIA P, 1998, P 9 EUR SIGN P C EUS, V1, P13
   BEERENDS JG, 1992, J AUDIO ENG SOC, V40, P963
   Bender W., 1996, IBM SYST J, V35, DOI DOI 10.1147/SJ.353.0313
   Collobert R, 2001, J MACH LEARN RES, V1, P143, DOI 10.1162/15324430152733142
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Delaigle JF, 1998, SIGNAL PROCESS, V66, P319, DOI 10.1016/S0165-1684(98)00013-9
   DUXBURG C, 2002, P INT C DAFX
   Gruhl D, 1996, PROCEEDING 1 INFOROM, P295
   John RDeller., 2000, DISCRETE TIME PROCES
   Kacker D, 2003, IEEE T SIGNAL PROCES, V51, P1054, DOI 10.1109/TSP.2003.809369
   Kirovski D, 2003, IEEE T SIGNAL PROCES, V51, P1020, DOI 10.1109/TSP.2003.809384
   Ko BS, 2002, INT CONF ACOUST SPEE, P2001
   Kuo S, 2002, INT CONF ACOUST SPEE, P1753
   Large EW, 2002, COGNITIVE SCI, V26, P1, DOI 10.1207/s15516709cog2601_1
   Lemma AN, 2003, IEEE T SIGNAL PROCES, V51, P1088, DOI 10.1109/TSP.2003.809372
   Lin CY, 2001, IEEE T IMAGE PROCESS, V10, P767, DOI 10.1109/83.918569
   Lu CS, 2000, IEEE T MULTIMEDIA, V2, P209, DOI 10.1109/6046.890056
   MADDAGE NC, 2004, P 12 ANN ACM INT C M, P112, DOI DOI 10.1145/1027527.1027549
   Mathai NJ, 2003, IEEE T SIGNAL PROCES, V51, P925, DOI 10.1109/TSP.2003.809382
   Navarro G, 2001, ACM COMPUT SURV, V33, P31, DOI 10.1145/375360.375365
   Rossing T.D., 2001, The Science of Sound, V3rd
   SPORER T, 1996, P AES 101 CONVENTION
   Swanson MD, 1998, SIGNAL PROCESS, V66, P337, DOI 10.1016/S0165-1684(98)00014-0
   Tang CW, 2003, IEEE T SIGNAL PROCES, V51, P950, DOI 10.1109/TSP.2003.809367
   Thiede T, 2000, J AUDIO ENG SOC, V48, P3
   Thiede T., 2000, J. Acoust. Soc. Am, V48, p1/2
   TURNER LF, 1989, Patent No. 8908915
   Xu CS, 2002, MULTIMEDIA SYST, V8, P353, DOI 10.1007/s005300200055
   2003, MUSIC TECH MAGAZINE, P62
NR 30
TC 8
Z9 9
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 1
AR 1
DI 10.1145/1198302.1198303
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IZ
UT WOS:000250871500001
DA 2024-07-18
ER

PT J
AU Cesar, P
   Vuorimaa, P
   Vierinen, J
AF Cesar, Pablo
   Vuorimaa, Petri
   Vierinen, Juha
TI A graphics architecture for high-end interactive television terminals
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
AB This article presents a graphics software architecture for next-generation digital television receivers. We propose that such receivers should include a standardised Java-based procedural environment capable of rendering 2D/3D graphics and video, and a declarative environment supporting W3C recommendations such as SMIL and XForms. We also introduce a graphics architecture model that meets such requirements. As a proof-of-concept, a prototype implementation of the model is presented. This implementation enhances television content by allowing the user to play 3D graphics games, to run Java applications, and to browse XML-based documents while meeting current hardware restrictions.
C1 Aalto Univ, FIN-02015 Espoo, Finland.
C3 Aalto University
RP Cesar, P (corresponding author), CWI, POB 94079, NL-1090 GB Amsterdam, Netherlands.
EM p.s.cesar@cwi.nl
RI Vierinen, Juha/M-9726-2015; Vuorimaa, Petri/G-6303-2011
OI Cesar, Pablo/0000-0003-1752-6837; Vuorimaa, Petri/0009-0007-6198-6650
CR *ARIB, 2003, DAT COD TRANSM SPECF
   *ARIB, 2003, APPL EX ENG PLATF DI
   *ATSC, 2004, ATSC CAND STAND ADV
   Bulterman D, 2005, SYNCHRONIZED MULTIME
   BULTERMAN DCA, 2004, SMIL 2 INT MULT WEB
   Cesar P, 2003, IEEE FIFTH INTERNATIOANL SYMPOSIUM ON MULTIMEDIA SOFTWARE ENGINEERING, PROCEEDINGS, P21
   Cha KA, 2005, MULTIMED TOOLS APPL, V25, P111, DOI 10.1023/B:MTAP.0000046384.83647.54
   CHEOK LT, 2002, SMIL MPEG4 BIFS
   CHORIANOPOULOS K, 2004, THESIS U ECONOMICS B
   DUBINKO M, 2003, XFORMS 1
   *DVB, 2004, DIG VIDE BROADC GLOB
   *DVB, 2005, DIG VIDE BROADC MULT
   Green M., 1991, COMPUT GRAPHICS-US, V25, P229, DOI DOI 10.1145/126640.126677
   Honkala M., 2004, Proceedings. IEEE Sixth International Symposium on Multimedia Software, P116
   ILLGNER K, 2001, P INT BROADC CONV IB
   *ITUT, 2001, WORLDW COMM COR APPL
   *ITUT, 2004, HARM DECL CONT FORM
   *ITUT, 2003, HARM PROC CONT FORM
   JALAVA T, 2005, TIMESHEETS XML TIMIN
   MAGNOR M, 2004, P WORKSH MULT IM COM, P1
   MALERCZYNK C, 2003, P 11 INT C CENTR EUR
   MARRIN C, 2001, P ACM WEB3D 2001, P7
   Myers B., 2000, ACM Transactions on Computer-Human Interaction, V7, P3, DOI 10.1145/344949.344959
   MYERS B, 2000, ACM T COMPUTHUM INTE, V1, P3
   MYERS B, 2004, CRC HDB COMPUTER SCI
   OLSEN DR, 1998, DEV USER INTERFACES
   ONURAL L, 2004, P EUR WORKSH INT KNO
   PEREIRA F, 2002, MPEG4 BOOK
   Pulles R., 2004, P 2 EUR UN S AMB INT, P31
   ROUSSEL N, 2003, P LAT AM C HUM COMP, P117
   Tran SM, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P429
NR 31
TC 5
Z9 6
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2006
VL 2
IS 4
BP 343
EP 357
DI 10.1145/1201730.1201735
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IY
UT WOS:000250871400005
DA 2024-07-18
ER

PT J
AU Becattini, F
   Bongini, P
   Bulla, L
   Marinucci, L
   del Bimbo, A
   Mongiovì, M
   Presutti, V
AF Becattini, Federico
   Bongini, Pietro
   Bulla, Luana
   Marinucci, Ludovica
   del Bimbo, Alberto
   Mongiovi, Misael
   Presutti, Valentina
TI VISCOUNTH: A Large-scale Multilingual Visual Question Answering Dataset
   for Cultural Heritage
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual question answering; cultural heritage
AB Visual question answering has recently been settled as a fundamental multi-modal reasoning task of artificial intelligence that allows users to get information about visual content by asking questions in natural language. In the cultural heritage domain, this task can contribute to assisting visitors in museums and cultural sites, thus increasing engagement. However, the development of visual question answering models for cultural heritage is prevented by the lack of suitable large-scale datasets. To meet this demand, we built a large-scale heterogeneous and multilingual (Italian and English) dataset for cultural heritage that comprises approximately 500K Italian cultural assets and 6.5M question-answer pairs. We propose a novel formulation of the task that requires reasoning over both the visual content and an associated natural language description, and present baselines for this task. Results show that the current state of the art is reasonably effective but still far from satisfactory; therefore, further research in this area is recommended. Nonetheless, we also present a holistic baseline to address visual and contextual questions and foster future research on the topic.
C1 [Becattini, Federico; Bongini, Pietro; del Bimbo, Alberto] Univ Florence, Media Integrat & Commun Ctr, Viale Giovanni Battista Morgagni 65, I-50134 Florence, Italy.
   [Bulla, Luana; Mongiovi, Misael] CNR, Inst Sci & Technol Cognit, Via Paolo Gaifami 18, I-95126 Catania, Italy.
   [Marinucci, Ludovica] CNR, Inst Sci & Technol Cognit, Via S Martino Battaglia 44, I-00185 Rome, Italy.
   Univ Bologna, Dept Modern Languages Literatures & Cultures, Via Cartoleria 5, I-40124 Bologna, Italy.
C3 University of Florence; Consiglio Nazionale delle Ricerche (CNR);
   Istituto di Scienze e Tecnologie della Cognizione (ISTC-CNR); Consiglio
   Nazionale delle Ricerche (CNR); Istituto di Scienze e Tecnologie della
   Cognizione (ISTC-CNR); University of Bologna
RP Mongiovì, M (corresponding author), CNR, Inst Sci & Technol Cognit, Via Paolo Gaifami 18, I-95126 Catania, Italy.
EM federico.becattini@unifi.it; p.bongini@unifi.it;
   luana.bulla@istc.cnr.it; ludovica.marinucci@istc.cnr.it;
   alberto.delbimbo@unifi.it; misael.mongiovi@istc.cnr.it;
   valentina.presutti@unibo.it
OI DEL BIMBO, ALBERTO/0000-0002-1052-8322; Bulla,
   Luana/0000-0003-1165-853X; Marinucci, Ludovica/0000-0002-1605-8819;
   Mongiovi, Misael/0000-0003-0528-5490
FU Italian PON project [ARS01_00421]; European Commission under European
   Horizon 2020 Programme [101004545-ReInHerit]
FX This work is supported by the Italian PON project ARS01_00421:
   "IDEHA-Innovazioni per l'elaborazione dei dati nel settore del
   Patrimonio Culturale." This work is partially supported by the European
   Commission under European Horizon 2020 Programme, Grant No.
   101004545-ReInHerit.
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Asprino L., 2021, P 7 INT C MACHINE LE, P193
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Bai Z., 2021, P IEEECVF INT C COMP, P5422
   Barra S, 2021, PATTERN RECOGN LETT, V151, P325, DOI 10.1016/j.patrec.2021.09.008
   Becattini F., 2016, Euro-Mediterranean Conference, P781
   Biten AF, 2019, IEEE I CONF COMP VIS, P4290, DOI 10.1109/ICCV.2019.00439
   Bongini Pietro, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13801), P268, DOI 10.1007/978-3-031-25056-9_18
   Bongini P, 2020, Arxiv, DOI arXiv:2003.09853
   Bugeja M., 2020, Rediscovering Heritage Through Technology, P69
   Bulla L, 2022, COMM COM INF SC, V1652, P529, DOI 10.1007/978-3-031-15743-1_48
   Carriero VA, 2019, LECT NOTES COMPUT SC, V11779, P36, DOI 10.1007/978-3-030-30796-7_3
   Castellano G, 2021, NEURAL COMPUT APPL, V33, P12263, DOI 10.1007/s00521-021-05893-z
   Cetinic E., 2021, PROC INT CONF PATT R, P502
   Cetinic E, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3475799
   Cetinic E, 2018, EXPERT SYST APPL, V114, P107, DOI 10.1016/j.eswa.2018.07.026
   Cucchiara R, 2014, IEEE MULTIMEDIA, V21, P74, DOI 10.1109/MMUL.2014.19
   Del Chiaro R, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P467, DOI 10.5220/0007392704670475
   Del Chiaro R, 2019, PATTERN RECOGN LETT, V128, P420, DOI 10.1016/j.patrec.2019.09.027
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Duguleana M, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12176958
   Gan Zhe, 2020, NEURIPS
   Gao H., 2015, INT C NEURAL INF PRO, P2296
   Garcia Noa, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12536), P92, DOI 10.1007/978-3-030-66096-3_8
   Ioannakis G, 2020, J CULT HERIT, V42, P171, DOI 10.1016/j.culher.2019.07.019
   Jiang XZ, 2020, AAAI CONF ARTIF INTE, V34, P11125
   Jin X, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7050528
   Kahou Samira Ebrahimi, 2018, P INT C LEARN REPR
   Kosmopoulos D, 2018, PERVASIVE MOB COMPUT, V47, P54, DOI 10.1016/j.pmcj.2018.05.002
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu YH, 2019, Arxiv, DOI arXiv:1907.11692
   Malinowski M, 2015, Arxiv, DOI arXiv:1410.0210
   Marino K, 2021, PROC CVPR IEEE, P14106, DOI 10.1109/CVPR46437.2021.01389
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   Mensink T, 2014, P INT C MULT RETR, P451, DOI DOI 10.1145/2578726.2578791
   Milani F, 2021, ACM J COMPUT CULT HE, V14, DOI 10.1145/3458885
   Presutti V., 2012, Ontology Engineering in a Networked World, P35
   Rajpurkar P., 2016, P 2016 C EMP METH NA, V2016, P2383
   Virto NR, 2019, ROBOTS, ARTIFICIAL INTELLIGENCE, AND SERVICE AUTOMATION IN TRAVEL, TOURISM AND HOSPITALITY, P239, DOI 10.1108/978-1-78756-687-320191018
   Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982
   Ren M., 2015, NEURIPS, P2953
   Sanh V, 2020, Arxiv, DOI arXiv:1910.01108
   Seidenari L, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092832
   Shah S, 2019, AAAI CONF ARTIF INTE, P8876
   Sheng S., 2016, P COLING WORKSHOP LA, P10
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Stefanini M, 2019, LECT NOTES COMPUT SC, V11752, P729, DOI 10.1007/978-3-030-30645-8_66
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tan WR, 2016, IEEE IMAGE PROC, P3703, DOI 10.1109/ICIP.2016.7533051
   Temmermans Frederik, 2011, P 12 INTERNATIONALWO
   Vallez N, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030779
   Vannoni Francesco, 2020, INT C PATTERN RECOGN
   Vrandecic D, 2012, P 21 INT C WORLD WID, P1063, DOI DOI 10.1145/2187980.2188242
   Wang P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1290
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Yan JH, 2022, PATTERN RECOGN LETT, V161, P24, DOI 10.1016/j.patrec.2022.07.009
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yi KX, 2018, ADV NEUR IN, V31
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu LC, 2015, Arxiv, DOI arXiv:1506.00278
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zheng WB, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2360, DOI 10.1145/3447548.3467285
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
   Zhu ZH, 2020, Arxiv, DOI arXiv:2006.09073
NR 69
TC 1
Z9 1
U1 5
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 193
DI 10.1145/3590773
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200016
OA hybrid, Green Accepted
DA 2024-07-18
ER

PT J
AU Gao, XY
   Xie, JY
   Chen, ZY
   Liu, AA
   Sun, ZN
   Lyu, L
AF Gao, Xingyu
   Xie, Jinyang
   Chen, Zhenyu
   Liu, An-An
   Sun, Zhenan
   Lyu, Lei
TI Dilated Convolution-based Feature Refinement Network for Crowd
   Localization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Dilated convolution; Feature Refinement; crowd localization; contextual
   information
ID MEAN SQUARED ERROR
AB As an emerging computer vision task, crowd localization has received increasing attention due to its ability to produce more accurate spatially predictions. However, continuous scale variations in complex crowd scenes lead to tiny individuals at the edges, so that existing methods cannot achieve precise crowd localization. Aiming at alleviating the above problems, we propose a novel Dilated Convolution-based Feature Refinement Network (DFRNet) to enhance the representation learning capability. Specifically, the DFRNet is built with three branches that can capture the information of each individual in crowd scenes more precisely. More specifically, we introduce a Feature Perception Module to model long-range contextual information at different scales by adopting multiple dilated convolutions, thus providing sufficient feature information to perceive tiny individuals at the edge of images. Afterwards, a Feature Refinement Module is deployed at multiple stages of the three branches to facilitate the mutual refinement of feature information at different scales, thus further improving the expression capability of multi-scale contextual information. By incorporating the above modules, DFRNet can locate individuals in complex scenes more precisely. Extensive experiments on multiple datasets demonstrate that the proposed method has more advanced performance compared to existing methods and can be more accurately adapted to complex crowd scenes.
C1 [Gao, Xingyu] Chinese Acad Sci, Inst Microelect, Beijing, Peoples R China.
   [Xie, Jinyang; Lyu, Lei] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan, Peoples R China.
   [Chen, Zhenyu] State Grid Corp China, Big Data Ctr, Beijing, Peoples R China.
   [Chen, Zhenyu] China Elect Power Res Inst, Beijing, Peoples R China.
   [Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
   [Sun, Zhenan] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
   [Sun, Zhenan] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Microelectronics, CAS;
   Shandong Normal University; State Grid Corporation of China; Tianjin
   University; Chinese Academy of Sciences; Institute of Automation, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Xie, JY; Lyu, L (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan, Peoples R China.
EM gxy9910@gmail.com; xiejinyangsdnu@163.com; czy9907@gmail.com;
   anan0422@gmail.com; znsun@nlpr.ia.ac.cn; lvlei@sdnu.edu.cn
RI Chen, Zhenyu/AAA-6776-2022; Gao, Xingyu/AAL-3288-2021
OI Gao, Xingyu/0000-0002-4660-8092
FU National Natural Science Foundation of China [61976127]; Science and
   Technology Innovation 2030-Major Project (Brain Science and Brain-Like
   Intelligence Technology) [2022ZD0208700]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61976127, and the Science and Technology
   Innovation 2030-Major Project (Brain Science and Brain-Like Intelligence
   Technology) under Grant 2022ZD0208700.
CR Abousamra S, 2021, AAAI CONF ARTIF INTE, V35, P872
   Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   Astrid M, 2021, IEEE INT CONF COMP V, P207, DOI 10.1109/ICCVW54120.2021.00028
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen G, 2021, Arxiv, DOI [arXiv:2101.04279, 10.48550/arXiv.2101.04279]
   Chen GY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9804, DOI 10.1109/ICCV48922.2021.00968
   Chen XY, 2019, IEEE WINT CONF APPL, P1941, DOI 10.1109/WACV.2019.00211
   Chen XQ, 2021, LECT NOTES COMPUT SC, V13019, P203, DOI 10.1007/978-3-030-88004-0_17
   Cheng J, 2021, IEEE T IMAGE PROCESS, V30, P2862, DOI 10.1109/TIP.2021.3055631
   Das K, 2004, ANN STAT, V32, P818
   Deng JK, 2019, Arxiv, DOI [arXiv:1905.00641, DOI 10.48550/ARXIV.1905.00641, 10.48550/ARXIV.1905.00641]
   Gao JY, 2021, Arxiv, DOI [arXiv:2108.00584, DOI 10.1016/J.NEUCOM.2022.09.113]
   Gao JY, 2021, Arxiv, DOI arXiv:2012.04164
   Gao JY, 2021, Arxiv, DOI arXiv:1912.03677
   Gu JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15283, DOI 10.1109/ICCV48922.2021.01502
   Guo D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1823, DOI 10.1145/3343031.3350881
   Hossain MA, 2019, IEEE WINT CONF APPL, P1280, DOI 10.1109/WACV.2019.00141
   Hou Xinyu, 2021, P IEEE INT C MULTIME, P1
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Jiang MY, 2021, NEUROCOMPUTING, V459, P35, DOI 10.1016/j.neucom.2021.06.055
   Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li XL, 2020, IEEE T IMAGE PROCESS, V29, P5571, DOI 10.1109/TIP.2020.2985284
   Li XL, 2017, AAAI CONF ARTIF INTE, P4147
   Li ZH, 2019, Arxiv, DOI arXiv:1904.00386
   Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192
   Liang DK, 2021, Arxiv, DOI arXiv:2102.07925
   Liang Dingkang, 2021, RECIPROCAL DIS UNPUB
   Liu CC, 2019, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2019.00131
   Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545
   Liu LB, 2021, PROC CVPR IEEE, P4821, DOI 10.1109/CVPR46437.2021.00479
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Mauro dosSantos de Arruda., 2021, COUNTING LOCATING HI
   QiWang Junyu Gao, 2020, IEEE Trans. Pattern Anal. Mach. Intell., V43, P2141
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Sam DB, 2020, IEEE WINT CONF APPL, P2853, DOI 10.1109/WACV45572.2020.9093386
   Sam Deepak Babu, IEEE T PATTERN ANAL, V2020
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Q., 2021, Rethinking counting and localization in crowds: A purely point-based framework, P3365
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   van Oosterhout T, 2011, VISAPP 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, P620
   Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201
   Wan Jia, 2020, ADV NEURAL INF PROCE, V33
   Wang HL, 2022, IEEE T CYBERNETICS, V52, P10750, DOI 10.1109/TCYB.2021.3064089
   Wang Y, 2021, IEEE T IMAGE PROCESS, V30, P2876, DOI 10.1109/TIP.2021.3055632
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Willmott CJ, 2005, CLIMATE RES, V30, P79, DOI 10.3354/cr030079
   Xu Chenfeng, 2019, AUTOSCALE LEAR UNPUB
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yu XH, 2020, IEEE WINT CONF APPL, P1246, DOI 10.1109/WACV45572.2020.9093394
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
NR 53
TC 2
Z9 3
U1 2
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 217
DI 10.1145/3571134
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200039
DA 2024-07-18
ER

PT J
AU Zhou, W
   Xia, ZW
   Dou, P
   Su, T
   Hu, HF
AF Zhou, Wei
   Xia, Zhiwu
   Dou, Peng
   Su, Tao
   Hu, Haifeng
TI Double Attention Based on Graph Attention Network for Image Multi-Label
   Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-label classification; label correlation; channel attention
   mechanism; graph attention network; visual analysis
ID EFFICIENT
AB The task of image multi-label classification is to accurately recognize multiple objects in an input image. Most of the recent works need to leverage the label co-occurrence matrix counted from training data to construct the graph structure, which are inflexible and may degrade model generalizability. In addition, these methods fail to capture the semantic correlation between the channel feature maps to further improve model performance. To address these issues, we propose DA-GAT (a Double Attention framework based on the Graph Attention neTwork) to effectively learn the correlation between labels from training data. First, we devise a new channel attention mechanism to enhance the semantic correlation between channel feature maps, so as to implicitly capture the correlation between labels. Second, we propose a new label attention mechanism to avoid the adverse impact of a manually constructed label co-occurrence matrix. It only needs to leverage the label embedding as the input of network, then automatically constructs the label relation matrix to explicitly establish the correlation between labels. Finally, we effectively fuse the output of these two attention mechanisms to further improve model performance. Extensive experiments are conducted on three public multi-label classification benchmarks. Our DA-GAT model achieves mean average precision of 87.1%, 96.6%, and 64.3% on MS-COCO 2014, PASCAL VOC 2007, and NUS-WIDE, respectively, and obviously outperforms other existing state-of-the-art methods. In addition, visual analysis experiments demonstrate that each attention mechanism can capture the correlation between labels well and significantly promote the model performance.
C1 [Zhou, Wei; Xia, Zhiwu; Dou, Peng; Su, Tao; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou, Peoples R China.
EM zhouw75@mail2.sysu.edu.cn; xiazhw@mail2.sysu.edu.cn;
   doup@mail2.sysu.edu.cn; sutao@mail.sysu.edu.cn; huhaif@mail.sysu.edu.cn
RI Peng/AAP-6413-2021
OI Zhou, Wei/0000-0002-9237-7205
FU National Natural Science Foundation of China [62076262, 61673402,
   61273270, 60802069]; Natural Science Foundation of Guangdong Province
   [2017A030311029]; Science and Technology Program of Guangdong Province
   [2021B1101270007, 2019B010140002]
FX This work was supported in part by the National Natural Science
   Foundation of China (62076262, 61673402, 61273270, 60802069), in part by
   the Natural Science Foundation of Guangdong Province (2017A030311029),
   and in part by the Science and Technology Program of Guangdong Province
   (2021B1101270007, 2019B010140002).
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2023, PEER PEER NETW APPL, V19
   Ben-Baruch E, 2021, Arxiv, DOI [arXiv:2009.14119, 10.48550/arXiv.2009.14119, DOI 10.48550/ARXIV.2009.14119]
   Cevikalp H, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107164
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Chen ZM, 2019, IEEE INT CON MULTI, P622, DOI 10.1109/ICME.2019.00113
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Durand T, 2019, IEEE T PATTERN ANAL, V41, P337, DOI 10.1109/TPAMI.2017.2788435
   Durand T, 2017, PROC CVPR IEEE, P5957, DOI 10.1109/CVPR.2017.631
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao B., 2020, arXiv
   Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082
   Gupta S, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3436494
   Hassanin M, 2021, Arxiv, DOI arXiv:2107.11159
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jin JR, 2016, INT C PATT RECOG, P2452, DOI 10.1109/ICPR.2016.7900004
   Jin Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P649, DOI 10.1007/978-3-030-58589-1_39
   Li JB, 2020, LECT NOTES COMPUT SC, V12396, P736, DOI 10.1007/978-3-030-61609-0_58
   Li L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3359753
   Li Q, 2019, Arxiv, DOI arXiv:1909.13005
   Li XT, 2021, IEEE T IMAGE PROCESS, V30, P7050, DOI 10.1109/TIP.2021.3099369
   Li ZJ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3362988
   Li ZX, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3426974
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu LQ, 2017, IEEE T PATTERN ANAL, V39, P2335, DOI 10.1109/TPAMI.2017.2651061
   Liu SL, 2021, Arxiv, DOI arXiv:2107.10834
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Meng Quanling, 2019, P ACM MULTIMEDIA ASI
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Nguyen HD, 2021, AAAI CONF ARTIF INTE, V35, P9092
   Paszke A., 2017, AUTOMATIC DIFFERENTI
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Qian SS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451215
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Srivastava G, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3386249
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Vu XS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2299, DOI 10.1145/3394171.3414047
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y., 2020, P INT C NEUR INF PRO, V33, P1
   Wang YT, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1575, DOI 10.1145/3340531.3411880
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P1316, DOI 10.1109/TMM.2020.2995290
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Wen KY, 2021, IEEE T CIRC SYST VID, V31, P2866, DOI 10.1109/TCSVT.2020.3030656
   Wen SP, 2021, IEEE T SYST MAN CY-S, V51, P7250, DOI 10.1109/TSMC.2020.2967071
   Wu XP, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P284, DOI 10.1145/3394171.3414046
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan Z, 2019, IEEE ACCESS, V7, P98005, DOI 10.1109/ACCESS.2019.2929512
   You RC, 2020, AAAI CONF ARTIF INTE, V34, P12709
   Yu WJ, 2019, PATTERN RECOGN, V91, P322, DOI 10.1016/j.patcog.2019.03.006
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang K., 2020, P IEEE CVF C COMP VI, P9050
   Zhao HY, 2020, IEEE ACCESS, V8, P225539, DOI 10.1109/ACCESS.2020.3044446
   Zhou FT, 2021, Arxiv, DOI arXiv:2012.12509
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
NR 63
TC 11
Z9 11
U1 9
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 18
DI 10.1145/3519030
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400018
DA 2024-07-18
ER

PT J
AU Liu, ZQ
   Zhu, GP
   Ding, F
   Luo, XY
   Kwong, S
   Li, P
AF Liu, Zuquan
   Zhu, Guopu
   Ding, Feng
   Luo, Xiangyang
   Kwong, Sam
   Li, Peng
TI Contrast-Enhanced Color Visual Cryptography for (k, n) Threshold Schemes
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual cryptography; threshold scheme; contrast enhancement; random
   grid; pixel expansion
ID RANDOM GRIDS
AB In traditional visual cryptography schemes (VCSs), pixel expansion remains to be an unsolved challenge. To alleviate the impact of pixel expansion, several colored-black-and-white VCSs, called CBW-VCSs, were proposed in recent years. Although these methods could ease the effect of pixel expansion, the reconstructed image obtained by these methods may also suffer from low contrasts. To address this issue, we propose a contrast-enhanced (k, n) CBW-VCS based on random grids, named (k, n) RG-CBW-VCS, in this article. By applying color random grids, a binary secret image is encrypted into n color shares that have no pixel expansion. When any k(1) (k(1) >= k) color shares are collected together, the stacked results of them can be identified as the secret image; whereas the superposition of any k(2) (k(2) < k) color shares shows nothing. Through theoretical analysis and experimental results, we justify the effectiveness of the proposed (k, n) RG-CBW-VCS. Compared with related methods in feature, contrast, and pixel expansion, the results indicate that the proposed method generally achieves better performance.
C1 [Liu, Zuquan; Zhu, Guopu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Liu, Zuquan] Univ Chinese Acad Sci, Shenzhen Coll Adv Technol, Shenzhen 518055, Peoples R China.
   [Zhu, Guopu] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Ding, Feng] Nanchang Univ, Sch Software, Nanchang 330047, Jiangxi, Peoples R China.
   [Luo, Xiangyang] State Key Lab Math Engn & Adv Comput Ing, Zhengzhou 450000, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Li, Peng] North China Elect Power Univ, Dept Math & Phys, Baoding 071003, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Harbin Institute of Technology; Nanchang University; City
   University of Hong Kong; North China Electric Power University
RP Zhu, GP (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.; Zhu, GP (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM zq.liu2@siat.ac.cn; guopu.zhu@hit.edu.cn; fd26@njit.edu;
   luoxy_ieu@sina.com; cssamk@cityu.edu.hk; lphit@163.com
RI Li, Peng/D-7073-2012; Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261; Liu, Zuquan/0000-0002-5819-1298
FU National Natural Science Foundation of China [62172402, 61872350,
   U1804263, 62172435]; Zhongyuan Science and Technology Innovation Leading
   Talent Project of China [214200510019]; Tip-top Scientific and Technical
   Innovative Youth Talents of Guangdong Special Support Program
   [2019TQ05X696]; Basic Research Program of Shenzhen
   [JCYJ20170818163403748]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62172402, Grant 61872350, Grant
   U1804263, and Grant 62172435, in part by the Zhongyuan Science and
   Technology Innovation Leading Talent Project of China under Grant
   214200510019, in part by the Tip-top Scientific and Technical Innovative
   Youth Talents of Guangdong Special Support Program under Grant
   2019TQ05X696, and in part by the Basic Research Program of Shenzhen
   under Grant JCYJ20170818163403748.
CR Blakley G. R., 1979, P NAT COMP C AM FED, P313, DOI [10.1109/MARK.1979.8817296, DOI 10.1109/AFIPS.1979.98, DOI 10.1109/MARK.1979.8817296]
   Blundo C, 1999, J CRYPTOL, V12, P261, DOI 10.1007/s001459900057
   Blundo C, 2006, THEOR COMPUT SCI, V369, P169, DOI 10.1016/j.tcs.2006.08.008
   Chen TH, 2011, IEEE T CIRC SYST VID, V21, P1693, DOI 10.1109/TCSVT.2011.2133470
   Chen TH, 2011, SIGNAL PROCESS, V91, P90, DOI 10.1016/j.sigpro.2010.06.012
   Chen TH, 2009, PATTERN RECOGN, V42, P2203, DOI 10.1016/j.patcog.2008.11.015
   Chen YC, 2012, IEEE T IMAGE PROCESS, V21, P3319, DOI 10.1109/TIP.2012.2190082
   Cimato S, 2006, COMPUT J, V49, P97, DOI 10.1093/comjnl/bxh152
   De Prisco R, 2014, IEEE T INF FOREN SEC, V9, P653, DOI 10.1109/TIFS.2014.2305574
   De Prisco R, 2013, THEOR COMPUT SCI, V510, P62, DOI 10.1016/j.tcs.2013.09.005
   Hou YC, 2003, PATTERN RECOGN, V36, P1619, DOI 10.1016/S0031-3203(02)00258-3
   Hou YC, 2014, IEEE T CIRC SYST VID, V24, P733, DOI 10.1109/TCSVT.2013.2280097
   Hou YC, 2013, INFORM SCIENCES, V233, P290, DOI 10.1016/j.ins.2013.01.006
   Hu CM, 2007, IEEE T IMAGE PROCESS, V16, P36, DOI 10.1109/TIP.2006.884916
   Jia XX, 2018, IEEE T CIRC SYST VID, V28, P1056, DOI 10.1109/TCSVT.2016.2631404
   Kang I, 2011, IEEE T IMAGE PROCESS, V20, P132, DOI 10.1109/TIP.2010.2056376
   Liu ZQ, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3418212
   Naor M., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P1, DOI 10.1007/BFb0053419
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Shen G, 2017, DESIGN CODE CRYPTOGR, V85, P15, DOI 10.1007/s10623-016-0285-5
   Shivani S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2935618
   Shyu SH, 2006, PATTERN RECOGN, V39, P866, DOI 10.1016/j.patcog.2005.06.010
   Shyu SJ, 2007, PATTERN RECOGN, V40, P3633, DOI 10.1016/j.patcog.2007.03.012
   Shyu SJ, 2015, IEEE T CIRC SYST VID, V25, P1557, DOI 10.1109/TCSVT.2015.2389372
   Shyu SJ, 2013, IEEE T CIRC SYST VID, V23, P414, DOI 10.1109/TCSVT.2012.2204940
   Shyu SJ, 2011, IEEE T INF FOREN SEC, V6, P960, DOI 10.1109/TIFS.2011.2158096
   Wang DS, 2013, IEEE T INF FOREN SEC, V8, P2059, DOI 10.1109/TIFS.2013.2281108
   Wang DS, 2011, INFORM SCIENCES, V181, P2189, DOI 10.1016/j.ins.2011.01.019
   Wu XT, 2020, J VIS COMMUN IMAGE R, V70, DOI 10.1016/j.jvcir.2020.102793
   Wu XT, 2019, SIGNAL PROCESS-IMAGE, V75, P100, DOI 10.1016/j.image.2019.03.017
   Wu XT, 2019, J VIS COMMUN IMAGE R, V61, P74, DOI 10.1016/j.jvcir.2019.03.020
   Wu XT, 2013, IEEE T INF FOREN SEC, V8, P1541, DOI 10.1109/TIFS.2013.2274955
   Wu XT, 2013, SIGNAL PROCESS, V93, P977, DOI 10.1016/j.sigpro.2012.11.014
   Yan XH, 2014, SIGNAL PROCESS, V105, P389, DOI 10.1016/j.sigpro.2014.06.011
   Yang CN, 2008, PATTERN RECOGN, V41, P3114, DOI 10.1016/j.patcog.2008.03.031
   Yang CN, 2019, IEEE T CIRC SYST VID, V29, P252, DOI 10.1109/TCSVT.2017.2771255
   Yang CN, 2016, THEOR COMPUT SCI, V609, P143, DOI 10.1016/j.tcs.2015.09.016
   Yang CN, 2014, INFORM SCIENCES, V278, P141, DOI 10.1016/j.ins.2014.03.033
   Yang CN, 2014, INFORM SCIENCES, V271, P246, DOI 10.1016/j.ins.2014.02.099
   Yang CN, 2000, DESIGN CODE CRYPTOGR, V20, P325, DOI 10.1023/A:1008382327051
   Yang CN, 2004, PATTERN RECOGN LETT, V25, P481, DOI 10.1016/j.patrec.2003.12.011
NR 41
TC 7
Z9 9
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 148
DI 10.1145/3508394
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800018
DA 2024-07-18
ER

PT J
AU Xia, ZH
   Ji, QJ
   Gu, Q
   Yuan, CS
   Xiao, FJ
AF Xia, Zhihua
   Ji, Qiuju
   Gu, Qi
   Yuan, Chengsheng
   Xiao, Fengjun
TI A Format-compatible Searchable Encryption Scheme for JPEG Images Using
   Bag-of-words
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Searchable encryption; privacy-preserving image retrieval; outsourced
   computing
ID RETRIEVAL
AB The development of cloud computing attracts enterprises and individuals to outsource their data, such as images, to the cloud server. However, direct outsourcing causes the extensive concern of privacy leakage, as images often contain rich sensitive information. A straightforward way to protect privacy is to encrypt the images using the standard cryptographic tools before outsourcing. However, in such a way the possible usage of the outsourced images would be strongly limited together with the services provided to users, like the Content-Based Image Retrieval (CBIR). In this article, we propose a secure outsourced CBIR scheme, in which an encryption scheme is designed for the widely used JPEG-format images, and the secure features can be directly extracted from such encrypted images. Specifically, the JPEG images are encrypted by the block permutation, intro- block permutation, polyalphabetic cipher, and stream cipher. Then secure local histograms are extracted from the encrypted DCT blocks and the Bag-Of-Words (BOW) model is further used to organize the encrypted local features to represent the image. The proposed image encryption gets all of the image data protected and the experimental results show that the proposed scheme achieves improved accuracy with a small file size expansion.
C1 [Xia, Zhihua] Jinan Univ, Coll Cyber Secur, Jinan, Peoples R China.
   [Xia, Zhihua] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing, Peoples R China.
   [Ji, Qiuju; Gu, Qi; Yuan, Chengsheng] Nanjing Univ Informat Sci & Technol, Coll Comp, Nanjing, Peoples R China.
   [Xiao, Fengjun] Hangzhou Dianzi Univ, Hangzhou, Peoples R China.
C3 University of Jinan; Nanjing University of Information Science &
   Technology; Nanjing University of Information Science & Technology;
   Hangzhou Dianzi University
RP Xia, ZH (corresponding author), Jinan Univ, Coll Cyber Secur, Jinan, Peoples R China.; Xia, ZH (corresponding author), Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing, Peoples R China.
EM xia_zhihua@163.com; ji_qiuju@163.com; guqi@nuist.edu.cn;
   ycs_nuist@163.com; bhxfj@126.com
RI Wang, Shiyao/JLL-7826-2023; yang, kun/JGM-4169-2023; Li,
   Kun/JLL-6505-2023; Xia, Zhihua/C-8581-2011; Wang, Xintong/JJE-1189-2023
OI Li, Kun/0000-0002-3638-2974; Xia, Zhihua/0000-0001-6860-647X; South,
   Set/0000-0002-3881-3168
FU Jiangsu Basic Research Programs-Natural Science Foundation [BK20181407];
   National Natural Science Foundation of China [62122032, 62102189,
   U1936118, 61672294]; Six Peak Talent project of Jiangsu Province
   [R2016L13]; Qinglan Project of Jiangsu Province; "333" project of
   Jiangsu Province; Priority Academic Program Development of Jiangsu
   Higher Education Institutions (PAPD) fund; Collaborative Innovation
   Center of Atmospheric Environment and Equipment Technology (CICAEET)
   fund, China; BK21+ program from the Ministry of Education of Korea
FX This work is supported in part by the Jiangsu Basic Research
   Programs-Natural Science Foundation under grant numbers BK20181407, in
   part by the National Natural Science Foundation of China under grant
   numbers 62122032, 62102189, U1936118, and 61672294, in part by Six Peak
   Talent project of Jiangsu Province (R2016L13), Qinglan Project of
   Jiangsu Province, and "333" project of Jiangsu Province, in part by the
   Priority Academic Program Development of Jiangsu Higher Education
   Institutions (PAPD) fund, in part by the Collaborative Innovation Center
   of Atmospheric Environment and Equipment Technology (CICAEET) fund,
   China. Zhihua Xia is supported by BK21+ program from the Ministry of
   Education of Korea.
CR Awasthi P, 2019, ADV INTELL SYST, V707, P509, DOI 10.1007/978-981-10-8639-7_53
   Bellafqira R, 2015, IEEE ENG MED BIO, P2944, DOI 10.1109/EMBC.2015.7319009
   Benhamouda F, 2019, IBM J RES DEV, V63, DOI 10.1147/JRD.2019.2913621
   Cheng H, 2016, EURASIP J INF SECUR, DOI 10.1186/s13635-015-0028-6
   Cheng H, 2016, J VIS COMMUN IMAGE R, V40, P111, DOI 10.1016/j.jvcir.2016.06.016
   Cheng SL, 2021, MULTIMED TOOLS APPL, V80, P22733, DOI 10.1007/s11042-019-07753-4
   Chum O., 2008, BMVC, P812, DOI DOI 10.5244/C.22.50
   Ferreira B, 2019, IEEE T CLOUD COMPUT, V7, P784, DOI 10.1109/TCC.2017.2669999
   Guo C, 2020, COMPUT SECUR, V99, DOI 10.1016/j.cose.2020.102021
   Hsu CY, 2012, IEEE T IMAGE PROCESS, V21, P4593, DOI 10.1109/TIP.2012.2204272
   Hu LS, 2020, SIGNAL PROCESS-IMAGE, V84, DOI 10.1016/j.image.2020.115837
   Hu SS, 2016, IEEE T IMAGE PROCESS, V25, P3411, DOI 10.1109/TIP.2016.2568460
   Iakovidou C, 2019, IEEE T IMAGE PROCESS, V28, P3115, DOI 10.1109/TIP.2019.2894281
   Iida K, 2021, EUR SIGNAL PR CONF, P730, DOI 10.23919/Eusipco47968.2020.9287671
   Iida K, 2020, IEEE ACCESS, V8, P200038, DOI 10.1109/ACCESS.2020.3035563
   Janani T, 2022, IEEE T MULTIMEDIA, V24, P3794, DOI 10.1109/TMM.2021.3107681
   Li Bin, 2021, J PHYS C SERIES, V1856
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Li YY, 2022, IEEE T CLOUD COMPUT, V10, P2936, DOI 10.1109/TCC.2020.3034232
   Liang HH, 2019, J VIS COMMUN IMAGE R, V61, P149, DOI 10.1016/j.jvcir.2019.03.021
   Liu F, 2019, IEEE ACCESS, V7, P119209, DOI 10.1109/ACCESS.2019.2935222
   Lu W, 2010, PROC SPIE, V7541, DOI 10.1117/12.838745
   Lu WJ, 2009, INT CONF ACOUST SPEE, P1533, DOI 10.1109/ICASSP.2009.4959888
   Shen M, 2020, FUTURE GENER COMP SY, V109, P621, DOI 10.1016/j.future.2018.04.089
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Wang Zhangdong, 2021, MULTIMEDIA SYST, V27, P1
   Weng L, 2016, IEEE T KNOWL DATA EN, V28, P2738, DOI 10.1109/TKDE.2016.2587258
   Weng L, 2015, IEEE T INF FOREN SEC, V10, P152, DOI 10.1109/TIFS.2014.2365998
   Xia Z, 2020, ARXIV PREPRINT ARXIV
   Xia ZH, 2022, IEEE T SERV COMPUT, V15, P202, DOI 10.1109/TSC.2019.2927215
   Xia ZH, 2016, IEEE T INF FOREN SEC, V11, P2594, DOI 10.1109/TIFS.2016.2590944
   Xia ZH, 2018, IEEE T CLOUD COMPUT, V6, P276, DOI 10.1109/TCC.2015.2491933
   Xu YY, 2017, J VIS COMMUN IMAGE R, V43, P164, DOI 10.1016/j.jvcir.2017.01.006
   Yuan JW, 2015, IEEE INFOCOM SER, DOI 10.1109/INFOCOM.2015.7218593
   Zhang CY, 2020, NEUROCOMPUTING, V406, P386, DOI 10.1016/j.neucom.2019.11.119
   Zhang L, 2017, IEEE T PARALL DISTR, V28, P3258, DOI 10.1109/TPDS.2017.2712148
NR 37
TC 12
Z9 12
U1 4
U2 39
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 85
DI 10.1145/3492705
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600019
DA 2024-07-18
ER

PT J
AU Ge, SM
   Lin, FZ
   Li, CY
   Zhang, DC
   Wang, WP
   Zeng, D
AF Ge, Shiming
   Lin, Fanzhao
   Li, Chenyu
   Zhang, Daichi
   Wang, Weiping
   Zeng, Dan
TI Deepfake Video Detection via Predictive Representation Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deepfake video detection; representation learning; deep learning; video
   understanding
AB Increasingly advanced deepfake approaches have made the detection of deepfake videos very challenging. We observe that the general deepfake videos often exhibit appearance-level temporal inconsistencies in some facial components between frames, resulting in discriminative spatiotemporal latent patterns among semantic-level feature maps. Inspired by this finding, we propose a predictive representative learning approach termed Latent Pattern Sensing to capture these semantic change characteristics for deepfake video detection. The approach cascades a Convolution Neural Network-based encoder, a ConvGRU-based aggregator, and a single-layer binary classifier. The encoder and aggregator are pretrained in a self-supervised manner to form the representative spatiotemporal context features. Then, the classifier is trained to classify the context features, distinguishing fake videos from real ones. Finally, we propose a selective self-distillation fine-tuning method to further improve the robustness and performance of the detector. In this manner, the extracted features can simultaneously describe the latent patterns of videos across frames spatially and temporally in a unified way, leading to an effective and robust deepfake video detector. Extensive experiments and comprehensive analysis prove the effectiveness of our approach, e.g., achieving a very highest Area Under Curve (AUC) score of 99.94% on FaceForensics++ benchmark and surpassing 12 states of the art at least 7.90%@AUC and 8.69%@AUC on challenging DFDC and Celeb-DF(v2) benchmarks, respectively.
C1 [Ge, Shiming; Lin, Fanzhao; Li, Chenyu; Zhang, Daichi; Wang, Weiping] Chinese Acad Sci, Inst Informat Engn, Beijing 100095, Peoples R China.
   [Ge, Shiming; Lin, Fanzhao; Li, Chenyu; Zhang, Daichi; Wang, Weiping] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100095, Peoples R China.
   [Zeng, Dan] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Shanghai University
RP Ge, SM (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing 100095, Peoples R China.; Ge, SM (corresponding author), Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100095, Peoples R China.; Zeng, D (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM geshiming@iie.ac.cn; linfanzhao@iie.ac.cn; zhangdaichi@iie.ac.cn;
   wangweiping@iie.ac.cn; dzeng@shu.edu.cn
OI Zhang, DaiChi/0000-0002-5377-964X; Ge, Shiming/0000-0001-5293-310X; Lin,
   Fanzhao/0000-0003-0339-9400
FU National Key Research and Development Plan [2020AAA0140001]; Beijing
   Natural Science Foundation [19L2040]; National Natural Science
   Foundation of China [61772513]; Youth Innovation Promotion Association,
   Chinese Academy of Sciences
FX This work was partially supported by grants from the National Key
   Research and Development Plan (2020AAA0140001), Beijing Natural Science
   Foundation (19L2040), and National Natural Science Foundation of China
   (61772513). Shiming Ge is also supported by the Youth Innovation
   Promotion Association, Chinese Academy of Sciences.
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Agarwal S, 2020, IEEE COMPUT SOC CONF, P2814, DOI 10.1109/CVPRW50498.2020.00338
   [Anonymous], 2015, P INT C NEUR INF PRO
   Arandjelovic R, 2016, PROC CVPR IEEE, P5297, DOI 10.1109/CVPR.2016.572
   Ballas Nicolas, 2016, INT C LEARN REPR
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Bregler C., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P353, DOI 10.1145/258734.258880
   BUCKLAND M, 1994, J AM SOC INFORM SCI, V45, P12, DOI 10.1002/(SICI)1097-4571(199401)45:1<12::AID-ASI2>3.0.CO;2-L
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Dale K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024164
   Das S, 2021, IEEE INT CONF COMP V, P3769, DOI 10.1109/ICCVW54120.2021.00421
   Denton Emily, 2017, P ADV NEUR INF PROC, P4417
   Dolhansky B, 2019, Arxiv, DOI arXiv:1910.08854
   Franceschi J. -Y., 2020, INT C MACH LEARN, P3233
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Ge Shiming, 2021, ACM MULT AS
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Güera D, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P127
   Haliassos A, 2021, PROC CVPR IEEE, P5037, DOI 10.1109/CVPR46437.2021.00500
   Han TD, 2019, IEEE INT CONF COMP V, P1483, DOI 10.1109/ICCVW.2019.00186
   He Kaiming, 2022, P IEEE CVF C COMPUTE
   Hernandez-Ortega Javier, 2021, P AAAI C ARTIFICIAL, P1
   Khan SA, 2021, Arxiv, DOI [arXiv:2102.05950, 10.48550/arXiv.2102.05950, DOI 10.48550/ARXIV.2102.05950]
   Kim M, 2021, IEEE COMPUT SOC CONF, P1001, DOI 10.1109/CVPRW53098.2021.00111
   Kingma D, 2014, ICLR P, V2014, P1
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   Li YZ, 2020, Arxiv, DOI arXiv:1909.12962
   Liang Ruofan, 2020, P INT C LEARNING REP
   Liu HG, 2021, PROC CVPR IEEE, P772, DOI 10.1109/CVPR46437.2021.00083
   Liu W, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3524497
   Luo YC, 2021, PROC CVPR IEEE, P16312, DOI 10.1109/CVPR46437.2021.01605
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   Mirsky Y, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3425780
   Mittal T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2823, DOI 10.1145/3394171.3413570
   Nguyen HH, 2019, INT CONF BIOMETR THE, DOI 10.1109/btas46853.2019.9185974
   Nirkin Y, 2022, IEEE T PATTERN ANAL, V44, P6111, DOI 10.1109/TPAMI.2021.3093446
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Sabir Ekraam, 2019, CVPRW, V3
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sun C, 2019, Arxiv, DOI arXiv:1906.05743
   Sun ZK, 2021, PROC CVPR IEEE, P3608, DOI 10.1109/CVPR46437.2021.00361
   Tengda Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P312, DOI 10.1007/978-3-030-58580-8_19
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056
   Tolosana R, 2020, INFORM FUSION, V64, P131, DOI 10.1016/j.inffus.2020.06.014
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang J, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3779, DOI 10.1145/3474085.3478324
   Wang YH, 2020, IEEE INT CONF AUTOMA, P515, DOI 10.1109/FG47880.2020.00089
   Wolchover Natalie, 2017, QUANTA MAGAZINE, V2017, P3
   Yeh CY, 2020, IEEE WINT CONF APPL, P53, DOI [10.1109/WACVW50321.2020.9096939, 10.1109/wacvw50321.2020.9096939]
   Zhang Daichi, 2021, P INT JOINT C ARTIFI, P565
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zheng YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15024, DOI 10.1109/ICCV48922.2021.01477
   Zhu XY, 2021, PROC CVPR IEEE, P2928, DOI 10.1109/CVPR46437.2021.00295
   Zi BJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2382, DOI 10.1145/3394171.3413769
NR 58
TC 13
Z9 13
U1 4
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 115
DI 10.1145/3536426
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000002
OA Bronze
DA 2024-07-18
ER

PT J
AU Lv, ZH
   Chen, DL
   Lv, HB
AF Lv, Zhihan
   Chen, Dongliang
   Lv, Haibin
TI Smart City Construction and Management by Digital Twins and BIM Big Data
   in COVID-19 Scenario
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Smart City; digital twins; BIM; multimedia big data; Bayesian Network
   Structural Learning Algorithm
ID OPTIMIZATION; INTEGRATION; PERFORMANCE; ALGORITHM; FRAMEWORK; MODEL; GIS
AB With the rapid development of information technology and the spread of Corona Virus Disease 2019 (COVID-19), the government and urban managers are looking for ways to use technology to make the city smarter and safer. Intelligent transportation can play a very important role in the joint prevention. This work expects to explore the building information modeling (BIM) big data (BD) processing method of digital twins (DTs) of Smart City, thus speeding up the construction of Smart City and improve the accuracy of data processing. During construction, DTs build the same digital copy of the smart city. On this basis, BIM designs the building's keel and structure, optimizing various resources and configurations of the building. Regarding the fast data growth in smart cities, a complex data fusion and efficient learning algorithm, namely Multi-Graphics Processing Unit (GPU), is proposed to process the multi-dimensional and complex BD based on the compositive rough set model. The Bayesian network solves the multi-label classification. Each label is regarded as a Bayesian network node. Then, the structural learning approach is adopted to learn the label Bayesian network's structure from data. On the P53-old and the P53-new datasets, the running time of Multi-GPU decreases as the number of GPUs increases, approaching the ideal linear speedup ratio. With the continuous increase of K value, the deterministic information input into the tag BN will be reduced, thus reducing the classification accuracy. When K = 3, MLBN can provide the best data analysis performance. On genbase dataset, the accuracy of MLBN is 0.982 +/- 0.013. Through experiments, the BIM BD processing algorithm based on Bayesian Network Structural Learning (BNSL) helps decision-makers use complex data in smart cities efficiently.
C1 [Lv, Zhihan] Uppsala Univ, Fac Arts, Dept Game Design, Uppsala, Sweden.
   [Chen, Dongliang] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Peoples R China.
   [Lv, Haibin] Minist Nat Resources North Sea Bur, North China Sea Offshore Engn Survey Inst, Qingdao 266061, Peoples R China.
C3 Uppsala University; Qingdao University
RP Lv, ZH (corresponding author), Uppsala Univ, Fac Arts, Dept Game Design, Uppsala, Sweden.
EM lvzhihan@gmail.com; cdlord@qq.com; lvhaibinsoa@gmail.com
FU National Natural Science Foundation of China (NSFC) [61902203]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant Nos. 61902203.
CR Alastal A. I., 2019, CURRENT URBAN STUDIE, V7, P143
   Arai K, 2019, IFAC PAPERSONLINE, V52, P153, DOI 10.1016/j.ifacol.2019.08.171
   Arcuri N, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12187546
   Ates EC, 2020, J PENAL LAW CRIMINOL, V8, P293, DOI 10.26650/JPLC2020-813328
   Austin M, 2020, J MANAGE ENG, V36, DOI 10.1061/(ASCE)ME.1943-5479.0000774
   Chen HS, 2021, AUTOMAT CONSTR, V125, DOI 10.1016/j.autcon.2021.103631
   Demirdögen G, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12177061
   El-Hallaq M.A., 2019, J GEOGR INF SYST, V11, P321, DOI [DOI 10.4236/JGIS.2019.113019, 10.4236/jgis.2019.113019]
   Fan C, 2020, J MANAGE ENG, V36, DOI 10.1061/(ASCE)ME.1943-5479.0000745
   Fan C, 2021, INT J INFORM MANAGE, V56, DOI 10.1016/j.ijinfomgt.2019.102049
   Francisco A, 2020, J MANAGE ENG, V36, DOI 10.1061/(ASCE)ME.1943-5479.0000741
   Graham Katie, 2019, Technology Architecture + Design, V3, P234, DOI 10.1080/24751448.2019.1640541
   Greene M.C., 2020, INT J DIGITAL INNOVA, V9, P1, DOI 10.4018/IJDIBE.2020070101
   Ham Y, 2020, J MANAGE ENG, V36, DOI 10.1061/(ASCE)ME.1943-5479.0000748
   Huang H, 2019, INFORM SCIENCES, V495, P100, DOI 10.1016/j.ins.2019.04.058
   Jegadeesan S, 2019, SUSTAIN CITIES SOC, V49, DOI 10.1016/j.scs.2019.101522
   Jing SY, 2018, SOFT COMPUT, V22, P7553, DOI 10.1007/s00500-018-3050-z
   Li DR, 2021, COMPUT URBAN SCI, V1, DOI 10.1007/s43762-021-00005-y
   Li J, 2020, FUTURE GENER COMP SY, V107, P247, DOI 10.1016/j.future.2019.12.040
   Liu MN, 2021, J MANUF SYST, V58, P346, DOI 10.1016/j.jmsy.2020.06.017
   Liu Z, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su13042090
   Luo FF, 2017, NEUROCOMPUTING, V260, P313, DOI 10.1016/j.neucom.2017.04.052
   Ma YP, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11052101
   Maftei S. S., 2020, J CULTURAL MANAGEMEN, V6, P143
   Min QF, 2019, INT J INFORM MANAGE, V49, P502, DOI 10.1016/j.ijinfomgt.2019.05.020
   Nikitas A, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12072789
   Sahin M., 2020, Egitsel Veri Madenciligi ve Ogrenme Analitikleri: Dunu, Bugunu ve Gelecegi, V9, P121, DOI DOI 10.14686/BUEFAD.606077
   Sarkar M, 2017, INT J AMBIENT COMPUT, V8, P1, DOI 10.4018/IJACI.2017070101
   Sato M, 2019, ALGORITHMS, V12, DOI 10.3390/a12010015
   Sepasgozar SME, 2021, BUILDINGS-BASEL, V11, DOI 10.3390/buildings11040151
   Sinha S, 2018, J GLOB INF MANAG, V26, P37, DOI 10.4018/JGIM.2018070104
   Syafrudin M, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18092946
   Tang S, 2019, AUTOMAT CONSTR, V101, P127, DOI 10.1016/j.autcon.2019.01.020
   Wang H, 2018, ENERG CONVERS MANAGE, V156, P113, DOI 10.1016/j.enconman.2017.10.078
   Yamamura S, 2017, PROCEDIA ENGINEER, V180, P1462, DOI 10.1016/j.proeng.2017.04.309
   Yamazaki I, 2015, SIAM J SCI COMPUT, V37, pC307, DOI 10.1137/14M0973773
   Yan R, 2021, BMC MED INFORM DECIS, V21, DOI 10.1186/s12911-020-01340-6
   Zhang J, 2019, APPL SOFT COMPUT, V76, P425, DOI 10.1016/j.asoc.2018.12.016
   Zheng SQ, 2013, P NATL ACAD SCI USA, V110, pE1248, DOI 10.1073/pnas.1209247110
   Zhu JX, 2021, ANN GIS, V27, P99, DOI 10.1080/19475683.2020.1743355
NR 40
TC 80
Z9 81
U1 42
U2 139
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 117
DI 10.1145/3529395
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000004
OA Bronze
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Tiwari, A
   Pant, M
AF Tiwari, Arti
   Pant, Millie
TI Optimized Deep-Neural Network for Content-based Medical Image Retrieval
   in a Brownfield IoMT Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Content-based Image Retrieval; IoMT; Optimized Neural Network;
   Differential Evolution
ID QUERY
AB In this paper, a brownfield Internet of Medical Things network is introduced for imaging data that can be easily scaled out depending on the objectives, functional requirements, and the number of facilities and devices connected to it. This is further used to develop a novel Content-based Medical Image Retrieval framework. The developed framework uses DenseNet-201 architecture for generating the image descriptors. Then for classification, the optimized Deep Neural Network model has been configured through a population-based metaheuristic Differential Evolution. Differential Evolution iteratively performs the joint optimization of hyperparameters and architecture of Deep Neural Networks. The competence of the proposed model is validated on three publicly available datasets: Brain Tumor MRI dataset, Covid-19 Radiography database, and Breast Cancer MRI dataset, and by comparing it with selected models over different aspects of performance evaluation. Results show that the convergence rate of the proposed framework is very fast, and it achieves at least 97.28% accuracy across all the models.
C1 [Tiwari, Arti; Pant, Millie] Indian Inst Technol Roorkee, Dept Appl Math & Sci Comp, Roorkee 247667, Uttarakhand, India.
   [Pant, Millie] Indian Inst Technol Roorkee, Mehta Family Sch Data Sci & Artificial Intelligen, Roorkee 247667, Uttarakhand, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Roorkee
RP Pant, M (corresponding author), Indian Inst Technol Roorkee, Dept Appl Math & Sci Comp, Roorkee 247667, Uttarakhand, India.; Pant, M (corresponding author), Indian Inst Technol Roorkee, Mehta Family Sch Data Sci & Artificial Intelligen, Roorkee 247667, Uttarakhand, India.
EM atiwari1@as.iitr.ac.in; pant.milli@as.iitr.ac.in
RI TIWARI, ARTI/JBS-3494-2023
OI TIWARI, ARTI/0009-0005-3234-2472; Pant, Millie/0000-0002-7668-7887;
   Tiwari, Arti/0000-0003-2310-4251
CR Agrawal U, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3350532
   [Anonymous], MED IM RETR US DEEP
   [Anonymous], COVID 19 RAD DAT
   [Anonymous], BREAST CANC PAT MRIS
   Baioletti M, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8010069
   Bhuvaji S, 2020, BRAIN TUMOR CLASSIFI, DOI [10.34740/KAGGLE/DSV/1183165, DOI 10.34740/KAGGLE/DSV/1183165, 10.34740/kaggle/dsv/1183165]
   Cai W., 2020, Biomedical information technology, P321
   Chowdhury MEH, 2020, IEEE ACCESS, V8, P132665, DOI 10.1109/ACCESS.2020.3010287
   elsevier, HYBR MPSO CNN MULT P
   Ferreira B, 2015, SYM REL DIST SYST, P11, DOI 10.1109/SRDS.2015.27
   Feurer M, 2019, SPRING SER CHALLENGE, P3, DOI 10.1007/978-3-030-05318-5_1
   FLICKNER M, 1995, COMPUTER, V28, P23, DOI 10.1109/2.410146
   Ghazal Taher M., 2022, ACM Transactions on Asian and Low-Resource Language Information Processing. Just Accepted, DOI [10.1145/3523283, DOI 10.1145/3523283]
   Gkelios S, 2021, EXPERT SYST APPL, V177, DOI 10.1016/j.eswa.2021.114940
   Han F, 2017, NEUROCOMPUTING, V228, P133, DOI 10.1016/j.neucom.2016.09.092
   Hirose Y., 2021, P MACHINE LEARNING R, V157, P2021
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kim TY, 2021, NEUROCOMPUTING, V456, P666, DOI 10.1016/j.neucom.2020.07.154
   Kumar A, 2013, J DIGIT IMAGING, V26, P1025, DOI 10.1007/s10278-013-9619-2
   Li XQ, 2021, NEUROCOMPUTING, V452, P675, DOI 10.1016/j.neucom.2020.07.139
   Liu LJ, 2018, J BIOMED INFORM, V77, P21, DOI 10.1016/j.jbi.2017.11.013
   Liu Y, 2018, PROCEEDINGS OF 2018 THE 3RD INTERNATIONAL CONFERENCE ON MULTIMEDIA AND IMAGE PROCESSING (ICMIP 2018), P68, DOI 10.1145/3195588.3195605
   Ma L, 2017, J BIOMED INFORM, V66, P148, DOI 10.1016/j.jbi.2017.01.002
   Mahdaddi A, 2021, EXPERT SYST APPL, V185, DOI 10.1016/j.eswa.2021.115525
   Maji Subhadip, 2021, ACM/IMS Transactions on Data Science, V2, DOI 10.1145/3470568
   Manjula G, 2019, PROCEEDINGS OF 2019 THE 3RD INTERNATIONAL CONFERENCE ON CRYPTOGRAPHY, SECURITY AND PRIVACY (ICCSP 2019) WITH WORKSHOP 2019 THE 4TH INTERNATIONAL CONFERENCE ON MULTIMEDIA AND IMAGE PROCESSING (ICMIP 2019), P93, DOI 10.1145/3309074.3309113
   Mason K, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE COMPANION (GECCO'17 COMPANION), P213, DOI 10.1145/3067695.3075967
   Muhammad G, 2021, INFORM FUSION, V76, P355, DOI 10.1016/j.inffus.2021.06.007
   Noor J, 2021, J NETW COMPUT APPL, V192, DOI 10.1016/j.jnca.2021.103167
   Rahman T, 2021, COMPUT BIOL MED, V132, DOI 10.1016/j.compbiomed.2021.104319
   Rajalaxmi R.R., 2022, Transactions on Asian and Low-Resource Language Information Processing, DOI [10.1145/3511897, DOI 10.1145/3511897]
   Ravi C, 2022, ACM T INTERNET TECHN, V22, DOI 10.1145/3412353
   Shakarami A, 2020, OPTIK, V214, DOI 10.1016/j.ijleo.2020.164833
   Shamna P, 2019, J BIOMED INFORM, V91, DOI 10.1016/j.jbi.2019.103112
   Sisodia A, 2021, ENG APPL ARTIF INTEL, V104, DOI 10.1016/j.engappai.2021.104377
   Sonthi VK, 2023, ACM T ASIAN LOW-RESO, V22, DOI 10.1145/3520439
   Suganuma M, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P497, DOI 10.1145/3071178.3071229
   Do TT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3314051
   Tortorella GL, 2021, TECHNOL FORECAST SOC, V166, DOI 10.1016/j.techfore.2021.120666
   Vaccaro Federico, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P311, DOI 10.1145/3372278.3390732
   Wazid M, 2023, ACM T INTERNET TECHN, V23, DOI 10.1145/3511898
   Waziri Usman., 2014, P 11 ACM C COMPUTING, P1, DOI [10.1145/2597917.2597956, DOI 10.1145/2597917.2597956]
   Xu YY, 2017, J VIS COMMUN IMAGE R, V43, P164, DOI 10.1016/j.jvcir.2017.01.006
   Yan HY, 2019, INFORM SCIENCES, V479, P153, DOI 10.1016/j.ins.2018.11.046
   Yoo Y, 2019, KNOWL-BASED SYST, V178, P74, DOI 10.1016/j.knosys.2019.04.019
   Zhang W, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3279952
   Zhuang Y, 2014, INFORM SCIENCES, V263, P60, DOI 10.1016/j.ins.2013.10.013
NR 47
TC 2
Z9 2
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 125
DI 10.1145/3546194
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000012
DA 2024-07-18
ER

PT J
AU Baiju, PS
   George, SN
AF Baiju, P. S.
   George, Sudhish N.
TI TTV Regularized LRTA Technique for the Estimation of Haze Model
   Parameters in Video Dehazing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video dehazing; Low Rank Tensor Approximation (LRTA); tensor total
   variation and video deweathering
ID IMAGE QUALITY ASSESSMENT; FRAMEWORK; COMPLETION; ALGORITHM; TENSORS;
   SPARSE
AB Nowadays, intelligent transport systems have a major role in providing a safe and secure traffic society for passengers, pedestrians, and vehicles. However, some bad weather conditions such as haze or fog may affect the visual clarity of video footage captured by the camera. This will cause a malfunction in further video processing algorithms performed by such automated systems. This article proposes an efficient technique for estimating the atmospheric light and the transmission map in the haze model entirely in tensor domain for video dehazing. In this work, the atmospheric light is appraised using the Mie scattering principle of visible light and the temporal coherency among the frames is achieved by means of tensor algebra. Furthermore, the transmission map is computed using Low Rank Tensor Approximation (LRTA) based on Weighted Tensor Nuclear Norm (WTNN) minimization and Tensor Total Variation (TTV) regularization. WTNN minimization is used to smooth the coarse transmission map, and TTV regularization is employed to maintain spatio-temporal continuity by preserving the details of salient structures and edges. The novelty of the proposed model is confined in the efficient formulation of a unified optimization model for the estimation of transmission map and atmospheric light in the tensor domain with fine-tuned regularization terms, which is not reported till now in the direction of video dehazing. Extensive experiments show that the proposed method outperforms state-of-the-art methods in video dehazing.
C1 [Baiju, P. S.; George, Sudhish N.] Natl Inst Technol Calicut, Calicut 673601, Kerala, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Calicut
RP Baiju, PS (corresponding author), Natl Inst Technol Calicut, Calicut 673601, Kerala, India.
EM baijupstvm@gmail.com; sudhish@nitc.ac.in
CR Alajarmeh A, 2018, MULTIMED TOOLS APPL, V77, P26315, DOI 10.1007/s11042-018-5861-4
   [Anonymous], 2014, 18 IEEE INT S CONSUM
   Baburaj M, 2019, MULTIMED TOOLS APPL, V78, P1805, DOI 10.1007/s11042-018-6251-7
   Baiju PS, 2018, NATL CONF COMMUN
   Bektas S, 2010, INT J PHYS SCI, V5, P1721
   Blanchet G., 2012, Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2012), P1065, DOI 10.1109/ICASSP.2012.6288070
   Bolun Cai, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9917, P315, DOI 10.1007/978-3-319-48896-7_31
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cao WF, 2016, IEEE T IMAGE PROCESS, V25, P4075, DOI 10.1109/TIP.2016.2579262
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1014, DOI 10.1109/TCYB.2015.2419737
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Chen BH, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2726947
   Chen C, 2016, LECT NOTES COMPUT SC, V9906, P576, DOI 10.1007/978-3-319-46475-6_36
   Comon P, 2014, IEEE SIGNAL PROC MAG, V31, P44, DOI 10.1109/MSP.2014.2298533
   Das A, 2020, ADV INTELL SYST COMP, V1024, P127, DOI 10.1007/978-981-32-9291-8_11
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   Dong TY, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071593
   Economopoulos TL, 2010, IMAGE VISION COMPUT, V28, P45, DOI 10.1016/j.imavis.2009.04.011
   Emberton S, 2018, COMPUT VIS IMAGE UND, V168, P145, DOI 10.1016/j.cviu.2017.08.003
   Gibson KB, 2012, IEEE T IMAGE PROCESS, V21, P662, DOI 10.1109/TIP.2011.2166968
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jiang F, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1363, DOI 10.1109/ICASSP.2018.8461973
   Kernfeld E, 2015, LINEAR ALGEBRA APPL, V485, P545, DOI 10.1016/j.laa.2015.07.021
   Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Li BY, 2018, AAAI CONF ARTIF INTE, P7016
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li MD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3341728
   Li ZW, 2015, PROC CVPR IEEE, P4988, DOI 10.1109/CVPR.2015.7299133
   Lin XD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3241058
   Luan Z, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/9241629
   Madathil B, 2018, NEUROCOMPUTING, V318, P120, DOI 10.1016/j.neucom.2018.08.038
   Madathil B, 2018, INFORM SCIENCES, V423, P376, DOI 10.1016/j.ins.2017.09.058
   Martin CD, 2013, SIAM J SCI COMPUT, V35, pA474, DOI 10.1137/110841229
   McCartney E.J., 1976, OPTICS ATMOSPHERE
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Park Y, 2018, IEEE ACCESS, V6, P10003, DOI 10.1109/ACCESS.2018.2806378
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178
   Tan HL, 2017, MULTIMED TOOLS APPL, V76, P23413, DOI 10.1007/s11042-016-4036-4
   Tom AJ, 2020, IEEE T IMAGE PROCESS, V29, P7590, DOI 10.1109/TIP.2020.3004696
   Tom AJ, 2021, IEEE T CYBERNETICS, V51, P1004, DOI 10.1109/TCYB.2019.2921827
   Tom AJ, 2018, INT CO SIG PROC COMM, P327, DOI 10.1109/SPCOM.2018.8724459
   Tsai CC, 2019, OPT EXPRESS, V27, P11877, DOI 10.1364/OE.27.011877
   Wang MH, 2018, MULTIMED TOOLS APPL, V77, P11259, DOI 10.1007/s11042-017-5518-8
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Yang S, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P641
   Ye JP, 2005, MACH LEARN, V61, P167, DOI 10.1007/s10994-005-3561-6
   [禹晶 Yu Jing], 2011, [中国图象图形学报, Journal of Image and Graphics], V16, P1561
   Zhang JW, 2011, VISUAL COMPUT, V27, P749, DOI 10.1007/s00371-011-0569-8
   Zhang ZM, 2014, PROC CVPR IEEE, P3842, DOI 10.1109/CVPR.2014.485
   Zhao XT, 2018, IET IMAGE PROCESS, V12, P88, DOI 10.1049/iet-ipr.2017.0060
   Zhu L, 2016, COMPUT GRAPH FORUM, V35, P217, DOI 10.1111/cgf.13019
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 59
TC 1
Z9 1
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 4
DI 10.1145/3465454
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900004
DA 2024-07-18
ER

PT J
AU Silva, EP
   Vieira, N
   Amorim, G
   Mousinho, R
   Guedes, G
   Ghinea, G
   Dos Santos, JAF
AF Silva, Ellen P.
   Vieira, Natalia
   Amorim, Glauco
   Mousinho, Renata
   Guedes, Gustavo
   Ghinea, Gheorghita
   Dos Santos, Joel A. F.
TI Using Multisensory Content to Impact the Quality of Experience of
   Reading Digital Books
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multisensorial reading; mulsemedia; reading interest; e-book
ID MULSEMEDIA
AB Multisensorial books enrich a story with either traditional multimedia content or sensorial effects. The main idea is to increase children's interest in reading by enhancing their QoE while reading. Studies on enriched and/or augmented e-books also propose synchronizing additional content with text. However, they usually focus on audio, video, images, or haptic feedback. In this work, we present MBook, a tool for presenting multisensorial books. It decouples the book's textual content from the additional content, as well as its synchronization, and rendering. Thus, a change in the additional content or its synchronization does not require changes to the book's content. To enable fine-grained synchronization, MBook captures the reading position using an eye-tracker. Experimental results with students within the 13- to 19-year-old age group point to MBook being able to provide good usability.
C1 [Silva, Ellen P.; Vieira, Natalia; Amorim, Glauco; Guedes, Gustavo; Dos Santos, Joel A. F.] CEFET RJ, Av Maracana 229,Bloco E,5 Andar Maracana, BR-20271110 Rio De Janeiro, Brazil.
   [Mousinho, Renata] Univ Fed Rio de Janeiro, Av Venceslau Bras 95 Botafogo, BR-22290140 Rio De Janeiro, Brazil.
   [Ghinea, Gheorghita] Brunel Univ, Wilfred Brown Bldg 215, London, England.
C3 Centro Federal de Educacao Tecnologica Celso Suckow da Fonseca
   (CEFET-RJ); Universidade Federal do Rio de Janeiro; Brunel University
RP Silva, EP (corresponding author), CEFET RJ, Av Maracana 229,Bloco E,5 Andar Maracana, BR-20271110 Rio De Janeiro, Brazil.
EM ellen.silva@eic.cefet-rj.br; natalia.vieira@eic.cefet-rj.br;
   glauco.amorim@cefet-rj.br; renatamousinho.ufrj@gmail.com;
   gustavo.guedes@cefetrj.br; george.ghinca@brunel.ac.uk;
   jsantos@eic.cefet-rj.br
RI Vieira, Natalia/HMV-2652-2023; Guedes, Gustavo/H-3227-2018; Ghinea,
   Gheorghita/AAG-6770-2020; dos Santos, Joel/O-6246-2016
OI Guedes, Gustavo/0000-0001-8593-1506; Ghinea,
   Gheorghita/0000-0003-2578-5580; dos Santos, Joel/0000-0001-7234-613X
FU CAPES; CNPq; FAPERJ
FX This work was supported in part by CAPES, CNPq, and FAPERJ funding.
CR Ademoye OA, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487270
   Alam KM, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2501643.2501649
   [Anonymous], 2015, ADJ P 2015 ACM INT J
   Borgstrom L., 2011, Mousaion, V29, P193
   Bottos S., 2019, 22nd International Conference on Information Fusion, P1
   Brooke J., 1996, USABILITY EVALUATION, P189, DOI DOI 10.1201/9781498710411-35
   Cecotti H, 2016, IEEE T HUM-MACH SYST, V46, P601, DOI 10.1109/THMS.2016.2537749
   Fu FL, 2009, COMPUT EDUC, V52, P101, DOI 10.1016/j.compedu.2008.07.004
   Ghinea G, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2617994
   Gibson A, 2018, IEEE T HUM-MACH SYST, V48, P604, DOI 10.1109/THMS.2018.2849018
   Gokbulut Burak, 2017, QUAL QUANT, V52, P235, DOI [10.1007/s11135-017-0608-2, DOI 10.1007/S11135-017-0608-2]
   Guedes GP, 2018, LAT AM C LEARN OBJ T
   Heikkilä J, 2015, EXP PSYCHOL, V62, P123, DOI 10.1027/1618-3169/a000279
   Kovács PT, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON INTERACTIVE MOBILE COMMUNICATION TECHNOLOGIES AND LEARNING (IMCL), P283, DOI 10.1109/IMCTL.2015.7359604
   Lin JM, 2016, ADV INTELL SYST, V388, P243, DOI 10.1007/978-3-319-23207-2_24
   Murray N, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3108243
   Ribeiro P, 2018, PROCEEDINGS OF THE TWELFTH INTERNATIONAL CONFERENCE ON TANGIBLE, EMBEDDED, AND EMBODIED INTERACTION (TEI'18), P88, DOI 10.1145/3173225.3173274
   Ribeiro P, 2017, INTERACT DES ARCHIT, P84
   Sanchez S., 2016, CHI EXTENDED ABSTRAC, P1459
   Shams L, 2008, TRENDS COGN SCI, V12, P411, DOI 10.1016/j.tics.2008.07.006
   Spence C, 2020, MULTISENS RES, V33, P902, DOI 10.1163/22134808-bja10015
   S┬u├nchez-Azqueta C., 2016, 2 INT C HIGH ED ADV, P84
   Tal I, 2019, IEEE COMMUN MAG, V57, P60, DOI 10.1109/MCOM.001.1900241
   Tung T, 2014, IEEE T HUM-MACH SYST, V44, P625, DOI 10.1109/THMS.2014.2326873
   Vieira N, 2018, WEBMEDIA'18: PROCEEDINGS OF THE 24TH BRAZILIAN SYMPOSIUM ON MULTIMEDIA AND THE WEB, P133, DOI 10.1145/3243082.3267446
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
   Zou LH, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P315, DOI 10.1145/3083187.3084014
NR 27
TC 2
Z9 2
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 124
DI 10.1145/3458676
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800008
DA 2024-07-18
ER

PT J
AU Lu, SY
   Wu, D
   Zhang, Z
   Wang, SH
AF Lu, Siyuan
   Wu, Di
   Zhang, Zheng
   Wang, Shui-Hua
TI An Explainable Framework for Diagnosis of COVID-19 Pneumonia via
   Transfer Learning and Discriminant Correlation Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE COVID-19; ResNet; randomized neural network; extreme learning machine;
   random vector functional-link net; Schmidt neural network; computed
   tomography
ID DATA AUGMENTATION; CLASSIFICATION; NETWORK; IDENTIFICATION; FUSION
AB The new coronavirus COVID-19 has been spreading all over the world in the last six months, and the death toll is still rising. The accurate diagnosis of COVID-19 is an emergent task as to stop the spreading of the virus. In this paper, we proposed to leverage image feature fusion for the diagnosis of COVID-19 in lung window computed tomography (CT). Initially, ResNet-18 and ResNet-50 were selected as the backbone deep networks to generate corresponding image representations from the CT images. Second, the representative information extracted from the two networks was fused by discriminant correlation analysis to obtain refined image features. Third, three randomized neural networks (RNNs): extreme learning machine, Schmidt neural network and random vector functional-link net, were trained using the refined features, and the predictions of the three RNNs were ensembled to get a more robust classification performance. Experiment results based on five-fold cross validation suggested that our method outperformed state-of-the-art algorithms in the diagnosis of COVID-19.
C1 [Lu, Siyuan; Wang, Shui-Hua] Univ Leicester, Sch Comp & Math Sci, Univ Rd, Leicester LE1 7RH, Leics, England.
   [Wu, Di] Univ Melbourne, Grattan St, Parkville, Vic 3010, Australia.
   [Zhang, Zheng] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Zhang, Zheng] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Wang, Shui-Hua] King Abdulaziz Univ, Fac Comp & Informat Technol, Dept Informat Syst, Jeddah 21589, Saudi Arabia.
C3 University of Leicester; University of Melbourne; Harbin Institute of
   Technology; University of Macau; King Abdulaziz University
RP Wang, SH (corresponding author), Univ Leicester, Sch Comp & Math Sci, Univ Rd, Leicester LE1 7RH, Leics, England.; Zhang, Z (corresponding author), Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.; Zhang, Z (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.; Wang, SH (corresponding author), King Abdulaziz Univ, Fac Comp & Informat Technol, Dept Informat Syst, Jeddah 21589, Saudi Arabia.
EM siyuan_lu@foxmail.com; wudi_wendy@outlook.com; darrenzz219@gmail.com;
   shuihuawang@ieee.org
RI Zhang, Zhang/JAX-2097-2023; wu, di/IYS-9217-2023; Lu,
   Siyuan/ABE-7949-2020; Zhang, Zheng/M-6325-2014; Wang,
   Shuihua/G-7326-2016
OI Lu, Siyuan/0000-0001-6720-1323; Zhang, Zheng/0000-0003-1470-6998; 
FU Royal Society International Exchanges Cost Share Award, UK [RP202G0230];
   Medical Research Council Confidence in Concept Award, UK [MC_PC_17171];
   Hope Foundation for Cancer Research, UK [RM60G0680]; Guangxi Key
   Laboratory of Trusted Software [kx201901]; Fundamental Research Funds
   for the Central Universities [CDLS-2020-03]; Key Laboratory of Child
   Development and Learning Science (Southeast University), Ministry of
   Education; University of Leicester
FX Siyuan Lu holds a CSC scholarship with the University of Leicester. This
   paper is partially supported by Royal Society International Exchanges
   Cost Share Award, UK (RP202G0230); Medical Research Council Confidence
   in Concept Award, UK (MC_PC_17171); Hope Foundation for Cancer Research,
   UK (RM60G0680); Guangxi Key Laboratory of Trusted Software (kx201901);
   Fundamental Research Funds for the Central Universities (CDLS-2020-03);
   Key Laboratory of Child Development and Learning Science (Southeast
   University), Ministry of Education.
CR Abbas Asmaa, 2021, Applied Intelligence. The International Journal of Research on Intelligent Systems for Real Life Complex Problems, V51, P854, DOI 10.1007/s10489-020-01829-7
   Afshar P, 2020, PATTERN RECOGN LETT, V138, P638, DOI 10.1016/j.patrec.2020.09.010
   [Anonymous], 2016, ARXIV160207360
   Apostolopoulos ID, 2020, PHYS ENG SCI MED, V43, P635, DOI 10.1007/s13246-020-00865-4
   Apostolopoulos ID, 2020, J MED BIOL ENG, V40, P462, DOI 10.1007/s40846-020-00529-4
   Barstugan M, 2020, ARXIV200309424
   Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502
   Haghighat M, 2016, IEEE T INF FOREN SEC, V11, P1984, DOI 10.1109/TIFS.2016.2569061
   Hassanien A.E., 2020, MEDRXIV, DOI https://doi.org/10.1101/2020.03.30.20047787
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li L, 2020, RADIOLOGY, V296, pE65, DOI 10.1148/radiol.2020200905
   Li X, 2020, ARXIV200403042V2
   Mangal A, 2020, 200409803 ARXIV
   Ozturk T, 2020, COMPUT BIOL MED, V121, DOI 10.1016/j.compbiomed.2020.103792
   PAO YH, 1994, NEUROCOMPUTING, V6, P163, DOI 10.1016/0925-2312(94)90053-1
   Ruder S., ARXIV160904747
   Salman F.M., 2020, Covid-19 detection using artificial intelligence, V4, P18
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   SCHMIDT WF, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL II, P1, DOI 10.1109/ICPR.1992.201708
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Ucar F, 2020, MED HYPOTHESES, V140, DOI 10.1016/j.mehy.2020.109761
   Waheed A, 2020, IEEE ACCESS, V8, P91916, DOI 10.1109/ACCESS.2020.2994762
   Wang S, 2021, EUR RADIOL, V31, P6096, DOI [10.1080/1064119X.2021.1966557, 10.1079/9781789246070.0001, 10.1007/s00330-021-07715-1]
   Wang SH, 2021, INFORM FUSION, V67, P208, DOI 10.1016/j.inffus.2020.10.004
   Wang YG, 2011, NEUROCOMPUTING, V74, P2483, DOI 10.1016/j.neucom.2010.11.030
   Wu YH, 2021, IEEE T IMAGE PROCESS, V30, P3113, DOI 10.1109/TIP.2021.3058783
   Yu ZK, 2020, BIOMED ENG ONLINE, V19, DOI 10.1186/s12938-020-00807-x
   Zhang YD, 2020, MACH VISION APPL, V32, DOI 10.1007/s00138-020-01128-8
   Zhang YD, 2019, J MED IMAG HEALTH IN, V9, P2012, DOI 10.1166/jmihi.2019.2692
   Zhang YD, 2018, J COMPUT SCI-NETH, V28, P1, DOI 10.1016/j.jocs.2018.07.003
   Zhang YD, 2018, J COMPUT SCI-NETH, V27, P57, DOI 10.1016/j.jocs.2018.05.005
   Zhang YD, 2008, SENSORS-BASEL, V8, P7518, DOI 10.3390/s8117518
   Zhang YD, 2009, SCI CHINA SER F, V52, P914, DOI 10.1007/s11432-009-0019-7
   Zhao J., 2020, ARXIV PREPRINT ARXIV
   Zheng CM, 2021, IEEE T MULTIMEDIA, V23, P2520, DOI 10.1109/TMM.2020.3013398
NR 39
TC 8
Z9 8
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 103
DI 10.1145/3449785
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600005
DA 2024-07-18
ER

PT J
AU Lv, ZH
   Qiao, L
   Song, HB
AF Lv, Zhihan
   Qiao, Liang
   Song, Houbing
TI Analysis of the Security of Internet of Multimedia Things
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Internet of things; multimedia VANET; an undo list; privacy information;
   security
ID PRESERVING AUTHENTICATION SCHEME; PRIVACY
AB To study the security performance of the Internet of multimedia things on the privacy protection of user identity, behavior trajectory, and preference under the new information technology industry wave, in this study, aiming at the problems of the sharing of Internet of things perception data and the exposure of users' privacy information, the Anonymous Batch Authentication Scheme (ABAH) for privacy protection is designed. Hash-based Message Authentication Code is used to cancel the list-checking process and analyze its security performance. Compared with the methods of elliptic curve digital signature algorithm, Bayes least-square method, identity-based bulk verification, anonymous batch authentication and key protocol, conditional privacy authentication scheme, and expert message authentication protocol, the transmission delay, packet loss rate, and computation cost are studied without considering the undo list and during the undo check. The results show that with the increase of information size, the transmission delay and packet loss rate also increase, and the transmission delay of ABAH increases by about I5%, while the correlation between speed and transmission delay is small. In the case of the same amount of validation information, ABAH has the highest validation efficiency, and it still has an efficient validation effect in the case of invalid information. The message packet loss rate for ABAH is always 0 when the undo check validation overhead is considered. It can be found that ABAH can avoid the communication overhead and privacy leakage caused by the revocation list, ensure the integrity of batch verification information, meet the security performance of the vehicular ad hoc network under the Internet of Things, and protect the privacy of users from being disclosed.
C1 [Lv, Zhihan; Qiao, Liang] Qingdao Univ, Sch Data Sci & Software Engn, Qingdao 266071, Peoples R China.
   [Lv, Zhihan] Warsaw Univ Technol, Inst Comp Sci, Artificial Intelligence Dept, Warsaw, Poland.
   [Song, Houbing] Embry Riddle Aeronaut Univ, Dept Elect Engn & Comp Sci, Daytona Beach, FL 32114 USA.
C3 Qingdao University; Warsaw University of Technology; Embry-Riddle
   Aeronautical University
RP Lv, ZH (corresponding author), Qingdao Univ, Sch Data Sci & Software Engn, Qingdao 266071, Peoples R China.; Lv, ZH (corresponding author), Warsaw Univ Technol, Inst Comp Sci, Artificial Intelligence Dept, Warsaw, Poland.
EM lvzhihan@gmail.com; leonqiaoove@gmail.com; h.song@ieee.org
RI Lyu, Zhihan/I-3187-2014; Lv, Zhihan/GLR-6000-2022; Qiao,
   Liang/AAJ-8750-2021; song, hu/JVO-3838-2024; Song, Houbing
   Herbert/E-3628-2010
OI Lyu, Zhihan/0000-0003-2525-3074; Lv, Zhihan/0000-0003-2525-3074; Song,
   Houbing Herbert/0000-0003-2631-9223; Qiao, Liang/0000-0002-8188-886X
FU National Natural Science Foundation of China [61902203]; Key Research
   and Development Plan-Major Scientific and Technological Innovation
   Projects of ShanDong Province [2019JZZY020101]
FX This work was supported by National Natural Science Foundation of China
   (Grant No. 61902203) and Key Research and Development Plan-Major
   Scientific and Technological Innovation Projects of ShanDong Province
   (Grant No. 2019JZZY020101).
CR Aghili SF, 2019, IEEE CONF COMPUT, P348, DOI [10.1109/infcomw.2019.8845220, 10.1109/INFCOMW.2019.8845220]
   Aghili SF, 2019, FUTURE GENER COMP SY, V96, P410, DOI 10.1016/j.future.2019.02.020
   Barenghi A, 2016, ACM J EMERG TECH COM, V13, DOI 10.1145/2767132
   Bi MH, 2017, IEEE PHOTONIC TECH L, V29, P2147, DOI 10.1109/LPT.2017.2764476
   Chen C, 2019, IEEE T INTELL TRANSP, V20, P1604, DOI 10.1109/TITS.2018.2828025
   Chen CM, 2019, IEEE ACCESS, V7, P12047, DOI 10.1109/ACCESS.2019.2891105
   Cui J, 2019, IEEE T VEH TECHNOL, V68, P2972, DOI 10.1109/TVT.2019.2896018
   Cui J, 2017, IEEE T VEH TECHNOL, V66, P10283, DOI 10.1109/TVT.2017.2718101
   Dabironezare SO, 2018, IEEE T THZ SCI TECHN, V8, P746, DOI 10.1109/TTHZ.2018.2873973
   Desai N, 2018, IEEE J SOLID-ST CIRC, V53, P236, DOI 10.1109/JSSC.2017.2737562
   Fu S, 2017, IEEE MULTIMEDIA, V24, P38, DOI 10.1109/MMUL.2017.3051514
   Hakeem SAA, 2019, IEEE ACCESS, V7, P119689, DOI 10.1109/ACCESS.2019.2937182
   Huang KX, 2017, IEEE T SYST MAN CY-S, V47, P2704, DOI 10.1109/TSMC.2017.2698457
   Huguenin K, 2018, IEEE T MOBILE COMPUT, V17, P760, DOI 10.1109/TMC.2017.2741958
   Hussain T., 2019, IEEE T IND INFOR AUG
   Jiang B, 2019, IEEE T IND INFORM, V15, P6472, DOI 10.1109/TII.2019.2917693
   Jiang B, 2019, IEEE INTERNET THINGS, V6, P1375, DOI 10.1109/JIOT.2018.2842229
   Jiang B, 2019, IEEE INTERNET THINGS, V6, P3525, DOI 10.1109/JIOT.2018.2886964
   Kang JW, 2018, IEEE T INTELL TRANSP, V19, P2627, DOI 10.1109/TITS.2017.2764095
   Khan AA, 2018, IEEE T VEH TECHNOL, V67, P4501, DOI 10.1109/TVT.2018.2790391
   Liu ZC, 2018, IEEE ACCESS, V6, P26307, DOI 10.1109/ACCESS.2018.2834224
   Lv Z., 2019, IEEE T IND INFOR MAY
   Lv Z., 2019, IEEE T IND INFOR APR
   Lv Z, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2645860
   Mehmood I, 2019, IEEE INTERNET THINGS, V6, P9246, DOI 10.1109/JIOT.2019.2896151
   Mozaffari M, 2017, IEEE T WIREL COMMUN, V16, P7574, DOI 10.1109/TWC.2017.2751045
   Neshenko N, 2019, IEEE COMMUN SURV TUT, V21, P2702, DOI 10.1109/COMST.2019.2910750
   Park J, 2018, IEEE T VEH TECHNOL, V67, P2594, DOI 10.1109/TVT.2017.2769704
   Qiu T, 2018, IEEE T MOBILE COMPUT, V17, P72, DOI 10.1109/TMC.2017.2702670
   Sattar NY, 2019, IEEE ASME INT C ADV, P120, DOI [10.1109/aim.2019.8868796, 10.1109/AIM.2019.8868796]
   Sun C, 2017, IEEE ACCESS, V5, P24012, DOI 10.1109/ACCESS.2017.2768499
   Tanveer, 2019, J ARTIF INTELL SYST, V1, P110, DOI [DOI 10.33969/AIS.2019.11007, 10.33969/AIS.2019.11007]
   Wang J.Y., 2017, ACTA ECOL SIN, V37, P1
   Wu H, 2018, IEEE T INF FOREN SEC, V13, P1432, DOI 10.1109/TIFS.2018.2790382
   You I, 2018, IEEE ACCESS, V6, P39398, DOI 10.1109/ACCESS.2018.2855258
   Yu Y., 2019, IEEE T IND INFORM, V16, P3290
   Zhang XH, 2019, IEEE ACCESS, V7, P58241, DOI 10.1109/ACCESS.2018.2890736
   Zhang Y, 2017, IEEE COMMUN MAG, V55, P94, DOI 10.1109/MCOM.2017.1601185
   Zhang YQ, 2018, IEEE J SOLID-ST CIRC, V53, P995, DOI 10.1109/JSSC.2017.2776302
   Zhu HL, 2019, IEEE ACCESS, V7, P90036, DOI 10.1109/ACCESS.2019.2924486
NR 40
TC 32
Z9 32
U1 1
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 3
SU S
AR 97
DI 10.1145/3398201
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2RB
UT WOS:000612586300005
DA 2024-07-18
ER

PT J
AU Ruan, WJ
   Liang, C
   Yu, Y
   Wang, Z
   Liu, W
   Chen, J
   Ma, JY
AF Ruan, Weijian
   Liang, Chao
   Yu, Yi
   Wang, Zheng
   Liu, Wu
   Chen, Jun
   Ma, Jiayi
TI Correlation Discrepancy Insight Network for Video Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video re-identification; correlation insight; discrepancy description
   network; cross-domain variation
ID PERSON REIDENTIFICATION; TRACKING
AB Video-based person re-identification (ReID) aims at re-identifying a specified person sequence from videos that were captured by disjoint cameras. Most existing works on this task ignore the quality discrepancy across frames by using all video frames to develop a ReID method. Additionally, they adopt only the person self-characteristic as the representation, which cannot adapt to cross-camera variation effectively. To that end, we propose a novel correlation discrepancy insight network for video-based person ReID, which consists of an unsupervised correlation insight model (CIM) for video purification and a discrepancy description network (DDN) for person representation. Concretely, CIM is constructed by using kernelized correlation filters to encode person half-parts, which evaluates the frame quality by the cross correlation across frames for selecting discriminative video fragments. Furthermore, DDN exploits the selected video fragments to generate a discrepancy descriptor using a compression network, which aims at employing the discrepancies with other persons' to facilitate the representation of the target person rather than only using the self-characteristic. Due to the advantage in handling cross-domain variation, the discrepancy descriptor is expected to provide a new pattern for the object representation in cross-camera tasks. Experimental results on three public benchmarks demonstrate that the proposed method outperforms several state-of-the-art methods.
C1 [Ruan, Weijian; Liang, Chao; Chen, Jun] Wuhan Univ, Sch Comp, NERCMS, Wuhan, Peoples R China.
   [Yu, Yi; Wang, Zheng] Natl Inst Informat, Tokyo, Japan.
   [Liu, Wu] JD AI Res, Tokyo, Japan.
   [Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan, Peoples R China.
C3 Wuhan University; Research Organization of Information & Systems (ROIS);
   National Institute of Informatics (NII) - Japan; Wuhan University
RP Liang, C; Chen, J (corresponding author), Wuhan Univ, Sch Comp, NERCMS, Wuhan, Peoples R China.
EM rweij@whu.edu.cn; cliang@whu.edu.cn; yiyu@nii.ac.jp; wangz@nii.ac.jp;
   liuwu@live.cn; chenj@whu.edu.cn; jyma2010@gmail.com
RI Ma, Jiayi/Y-2470-2019; Wang, Zheng/ABC-6029-2020
OI Ma, Jiayi/0000-0003-3264-3265; Wang, Zheng/0000-0003-3846-9157
FU National Key R&D Program of China [2017YFC0803700]; National Nature
   Science Foundation of China [U1903214, 61876135]; Natural Science
   Fundation ofHubei Province [2019CFB472, 2018AAA062, 2018CFA024]
FX Thiswork is supported by National Key R&D Program of China (No.
   2017YFC0803700), National Nature Science Foundation of China (U1903214,
   61876135), and Natural Science Fundation ofHubei Province (2019CFB472,
   2018AAA062, 2018CFA024).
CR Auguste R, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P91, DOI 10.1145/2671188.2749332
   Bak S, 2012, LECT NOTES COMPUT SC, V7574, P806, DOI 10.1007/978-3-642-33712-3_58
   Bedagkar-Gala A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1721, DOI 10.1109/ICCVW.2011.6130457
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Chen WH, 2017, AAAI CONF ARTIF INTE, P3988
   Chen Y., 2018, ARXIV180807301
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DEHGHAN A, 2015, PROC CVPR IEEE, P4091, DOI DOI 10.1109/CVPR.2015
   Gheissari N., 2006, 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06), V2, P1528
   Gong SG, 2011, VISUAL ANALYSIS OF BEHAVIOUR: FROM PIXELS TO SEMANTICS, P1, DOI 10.1007/978-0-85729-670-2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Huang WJ, 2018, AAAI CONF ARTIF INTE, P2273
   Jewell EL, 2016, SOFT MATTER, V12, P2501, DOI 10.1039/c5sm03066h
   Karanam Srikrishna, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P33, DOI 10.1109/CVPRW.2015.7301392
   Karanam S, 2015, IEEE COMPUT SOC CONF
   Karanam S, 2015, IEEE I CONF COMP VIS, P4516, DOI 10.1109/ICCV.2015.513
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058
   Li SQ, 2016, INT CONF CLOUD COMPU, P480, DOI 10.1109/CCIS.2016.7790306
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu JW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231741
   Liu Kan, 2015, P INT C COMP VIS
   Liu W, 2018, NEUROINFORMATICS, V16, P457, DOI 10.1007/s12021-018-9362-4
   Liu W, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1618, DOI 10.1145/3123266.3123422
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   McLaughlin N., 2016, P C COMP VIS PATT RE
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   MOHMMED N, 2018, IEEE T CIRC SYST VID, V29, P4424, DOI DOI 10.1002/LDR.3180
   Ouyang DQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1562, DOI 10.1145/3240508.3240622
   Ouyang DQ, 2019, PATTERN RECOGN LETT, V117, P153, DOI 10.1016/j.patrec.2018.05.009
   Ruan WJ, 2020, NEUROCOMPUTING, V384, P200, DOI 10.1016/j.neucom.2019.11.102
   Ruan WJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P284, DOI 10.1145/3343031.3350984
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Ruan WJ, 2017, IEEE INT CON MULTI, P1231, DOI 10.1109/ICME.2017.8019504
   Ruan Weijian, 2016, ICME, P1
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Sun Y, 2019, IEEE I CONF COMP VIS, P5348, DOI 10.1109/ICCV.2019.00545
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tao DP, 2013, IEEE T CIRC SYST VID, V23, P1675, DOI 10.1109/TCSVT.2013.2255413
   Wang HW, 2009, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE AND ENGINEERING MANAGEMENT, P124
   Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45
   Wang X, 2020, IEEE T CIRC SYST VID, V30, P3332, DOI 10.1109/TCSVT.2019.2913114
   Wang Z., 2016, ADV ELECTRON MATER, V2
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu YJ, 2019, PROC SINGAP HEALTHC, V28, P6, DOI 10.1177/2010105818779607
   Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507
   Ye M, 2019, IEEE T IMAGE PROCESS, V28, P2976, DOI 10.1109/TIP.2019.2893066
   Ye M, 2018, LECT NOTES COMPUT SC, V11211, P176, DOI 10.1007/978-3-030-01234-2_11
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Ye Mang, 2017, P INT C COMP VIS
   You JJ, 2016, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2016.150
   You Jinjie, 2016, P ANN C COMPUTER VIS
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang W, 2018, IEEE T CIRC SYST VID, V28, P2768, DOI 10.1109/TCSVT.2017.2718188
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2016, arXiv preprint arXiv
   Zheng Liang, 2016, P EUR C COMP VIS
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717
   Zhu Xiaoke, 2017, P 31 AAAI C ART INT
   Zhu Xiaoke, 2016, P INT JOINT C ART IN
NR 70
TC 14
Z9 14
U1 3
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 120
DI 10.1145/3402666
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800004
DA 2024-07-18
ER

PT J
AU Sahoo, KS
   Puthal, D
AF Sahoo, Kshira Sagar
   Puthal, Deepak
TI SDN-Assisted DDoS Defense Framework for the Internet of Multimedia
   Things
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Control plane; entropy; IoMT; machine learning; SDN; security
ID SOFTWARE-DEFINED NETWORKS; CHALLENGES; SECURITY; ATTACK
AB The Internet of Things is visualized as a fundamental networking model that bridges the gap between the cyber and real-world entity. Uniting the real-world object with virtualization technology is opening further opportunities for innovation in nearly every individual's life. Moreover, the usage of smart heterogeneous multimedia devices is growing extensively. These multimedia devices that communicate among each other through the Internet form a unique paradigm called the Internet of Multimedia Things (IoMT). As the volume of the collected data in multimedia application increases, the security, reliability of communications, and overall quality of service need to be maintained. Primarily, distributed denial of service attacks unveil the pervasiveness of vulnerabilities in IoMT systems. However, the Software Defined Network (SDN) is a new network architecture that has the central visibility of the entire network, which helps to detect any attack effectively. In this regard, the combination of SDN and IoMT, termed Sll-IoMT has the immense ability to improve the network management and security capabilities of the IoT system. This article proposes an SDN-assisted two-phase detection framework, namely SD-IoMT-Protector, in which the first phase utilizes the entropy technique as the detection metric to verify and alert about the malicious traffic. The second phase has trained with an optimized machine learning technique for classifying different attacks:Me outcomes of the experimental results signify the usefulness and effectiveness of the proposed framework for addressing distributed denial of service issues of the SD-IoMT system.
C1 [Sahoo, Kshira Sagar] VNR VJIET, Hyderabad, India.
   [Puthal, Deepak] Newcastle Univ, Newcastle Upon Tyne, Tyne & Wear, England.
C3 Vallurupalli Nageswara Rao Vignana Jyothi Institute of Engineering
   &Technology (VNR VJIET); Newcastle University - UK
RP Sahoo, KS (corresponding author), VNR VJIET, Hyderabad, India.
EM kshirasagar12@gmail.com; dputhal88@gmail.com
RI Puthal, Deepak/V-6529-2019
OI Sahoo, Kshira/0000-0002-6435-5738
CR Alkasassbeh Mouhammd, 2016, INT J ADV COMPUTER S, V7
   Alsaeedi M, 2019, IEEE ACCESS, V7, P107346, DOI 10.1109/ACCESS.2019.2932422
   Alsmadr I, 2015, COMPUT SECUR, V53, P79, DOI 10.1016/j.cose.2015.05.006
   Alvi SA, 2015, AD HOC NETW, V33, P87, DOI 10.1016/j.adhoc.2015.04.006
   Bera S, 2017, IEEE INTERNET THINGS, V4, P1994, DOI 10.1109/JIOT.2017.2746186
   Bhoi S. K., 2019, International Journal of Information Technology, V11, P853, DOI 10.1007/s41870-018-0148-6
   Braga R, 2010, C LOCAL COMPUT NETW, P408, DOI 10.1109/LCN.2010.5735752
   Chen Z, 2018, INT CONF BIG DATA, P251, DOI 10.1109/BigComp.2018.00044
   Chu YuHunag, 2010, 2010 12th IEEE International Conference on Communication Technology (ICCT 2010), P385, DOI 10.1109/ICCT.2010.5689156
   Daneshgadeh S, 2019, CONF INNOV CLOUD, P222, DOI 10.1109/ICIN.2019.8685891
   David J, 2015, PROCEDIA COMPUT SCI, V50, P30, DOI 10.1016/j.procs.2015.04.007
   Dayal N, 2018, INT CONF COMMUN SYST, P17, DOI 10.1109/COMSNETS.2018.8328175
   Flauzac O, 2015, 2015 IEEE 29TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS WORKSHOPS WAINA 2015, P688, DOI 10.1109/WAINA.2015.110
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Isa MN, 2016, INT CONF ELECTRON D, P304, DOI 10.1109/ICED.2016.7804657
   Jagadeesan NA, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2655690
   Kokila RT, 2014, INT CONF ADV COMPU, P205, DOI 10.1109/ICoAC.2014.7229711
   Kreutz D., 2013, P 2 ACM SIGCOMM WORK, P55
   Ma XL, 2014, IEEE COMMUN LETT, V18, P114, DOI 10.1109/LCOMM.2013.112613.132275
   Mehdi SA, 2011, LECT NOTES COMPUT SC, V6961, P161, DOI 10.1007/978-3-642-23644-0_9
   Mishra P, 2019, IEEE WIREL COMMUN, V26, P64, DOI 10.1109/MWC.001.1900083
   Oshima S, 2010, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPLEX, INTELLIGENT AND SOFTWARE INTENSIVE SYSTEMS (CISIS 2010), P168, DOI 10.1109/CISIS.2010.53
   Puthal D, 2019, IEEE CONSUM ELECTR M, V8, P92, DOI 10.1109/MCE.2019.2893674
   Puthal D, 2016, IEEE CLOUD COMPUT, V3, P64, DOI 10.1109/MCC.2016.63
   Raczko E, 2017, EUR J REMOTE SENS, V50, P144, DOI 10.1080/22797254.2017.1299557
   Raja SP, 2018, J CIRCUIT SYST COMP, V27, DOI 10.1142/S0218126618300076
   Rani S, 2017, IEEE INTERNET THINGS, V4, P832, DOI 10.1109/JIOT.2017.2671460
   Sadeghi AR, 2015, DES AUT CON, DOI 10.1145/2744769.2747942
   Sahoo KS, 2020, IEEE INTERNET THINGS, V7, P5852, DOI 10.1109/JIOT.2019.2952527
   Sahoo KS, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P795, DOI 10.1145/3241539.3267764
   Sahoo KS, 2019, J SUPERCOMPUT, V75, P4829, DOI 10.1007/s11227-019-02767-z
   Sahoo KS, 2018, FUTURE GENER COMP SY, V89, P685, DOI 10.1016/j.future.2018.07.017
   Sood K, 2016, IEEE INTERNET THINGS, V3, P453, DOI 10.1109/JIOT.2015.2480421
   Su YX, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P45, DOI 10.1145/3077136.3080828
   Wan JF, 2019, WIREL COMMUN MOB COM, DOI 10.1155/2019/7947638
   Wang R, 2015, 2015 IEEE TRUSTCOM/BIGDATASE/ISPA, VOL 1, P310, DOI [10.1109/Trustcom-2015.389, 10.1109/Trustcom.2015.389]
   Yan Q, 2018, IEEE COMMUN MAG, V56, P30, DOI 10.1109/MCOM.2018.1700621
NR 37
TC 16
Z9 16
U1 1
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 3
SU S
AR 98
DI 10.1145/3394956
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2RB
UT WOS:000612586300006
DA 2024-07-18
ER

PT J
AU Liu, FY
   Lebret, R
   Orel, D
   Sordet, P
   Aberer, K
AF Liu, Fangyu
   Lebret, Remi
   Orel, Didier
   Sordet, Philippe
   Aberer, Karl
TI Upgrading the Newsroom: An Automated Image Selection System for News
   Articles
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodal retrieval; multimodal machine learning; neural networks; deep
   learning; natural language processing; news image article analysis; news
   media
AB We propose an automated image selection system to assist photo editors in selecting suitable images for news articles. The system fuses multiple textual sources extracted from news articles and accepts multilingual inputs. It is equipped with char-level word embeddings to help both modeling morphologically rich languages, e.g., German, and transferring knowledge across nearby languages. The text encoder adopts a hierarchical self-attentionmechanism to attend more to both keywordswithin a piece of text and informative components of a news article. We extensively experiment our system on a large-scale text-image database containing multimodal multilingual news articles collected from Swiss local news media websites. The system is compared with multiple baselines with ablation studies and is shown to beat existing text-image retrieval methods in a weakly supervised learning setting. Besides, we also offer insights on the advantage of using multiple textual sources and multilingual data.
C1 [Liu, Fangyu] Univ Cambridge, Language Technol Lab LTL, Sect Theoret & Appl Linguist, Fac Modern & Medieval Languages, 9 West Rd, Cambridge CB3 9DB, England.
   [Lebret, Remi; Aberer, Karl] Ecole Polytech Fed Lausanne, Distributed Informat Syst Lab LSIR, Lausanne, Switzerland.
   [Orel, Didier; Sordet, Philippe] Tamedia, Zurich, Switzerland.
   [Lebret, Remi; Aberer, Karl] EPFL IC IIF LSIR, BC 168,Batiment BC,Stn 14, Lausanne, Switzerland.
   [Orel, Didier; Sordet, Philippe] Ave Gare 33, CH-1003 Lausanne, Switzerland.
C3 University of Cambridge; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne
RP Liu, FY (corresponding author), Univ Cambridge, Language Technol Lab LTL, Sect Theoret & Appl Linguist, Fac Modern & Medieval Languages, 9 West Rd, Cambridge CB3 9DB, England.
EM fl399@cam.ac.uk; remi.lebret@epfl.ch; didier.orel@tamedia.ch;
   philippe.sordet@tamedia.ch; karl.aberer@epfl.ch
RI Liu, Fangyu/AHA-5291-2022; Liu, Fangyu/IWM-0424-2023; Aberer,
   Karl/G-6336-2011
OI Liu, Fangyu/0000-0001-7038-3623; Lebret, Remi/0000-0001-5439-7574
FU NVIDIA Corporation
FX We gratefully acknowledge the support of NVIDIA Corporation with the
   donation of the Titan Xp GPU used for this research.
CR [Anonymous], 2006, PROC IEEE COMPUT SOC
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bojanowski P., 2017, Transactions of the Association for Computational Linguistics, V5, P135, DOI DOI 10.1162/TACLA00051
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Chatfield K, 2015, INT J MULTIMED INF R, V4, P75, DOI 10.1007/s13735-015-0077-0
   Chen SZ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2236, DOI 10.1145/3343031.3350571
   Chen SZ, 2019, AAAI CONF ARTIF INTE, P8207
   Cho K., 2014, ARXIV14061078
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   DeVries T, 2017, PREPRINT
   Engilberge M, 2018, PROC CVPR IEEE, P3984, DOI 10.1109/CVPR.2018.00419
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Feng Y., 2010, Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010), P831
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Feng YS, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P1239
   Feng YS, 2013, IEEE T PATTERN ANAL, V35, P797, DOI 10.1109/TPAMI.2012.118
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gehring J., 2017, P MACHINE LEARNING R, P1243
   Gella Spandana, 2017, P 2017 C EMPIRICAL M, P2839
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Grave E, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P3483
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Honnibal M., 2017, IN PRESS, DOI DOI 10.3233/978-1-60750-588-4-1080
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang Po-Sen, 2013, P ACM INT C FORM KNO
   Johnson M., 2017, T ASSOC COMPUT LING, V5, P339, DOI [10.1162/tacla00065, 10.1162/tacl_a_00065, DOI 10.1162/TACL_A_00065]
   Joulin A, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2979
   Kalchbrenner Nal, 2016, ARXIV161010099
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Khalid Y. I. A., 2011, 2011 International Conference on Semantic Technology and Information Retrieval (STAIR 2011), P144, DOI 10.1109/STAIR.2011.5995779
   Kingma D. P., 2014, arXiv
   Kiros R., 2015, T ASS COMPUT LING
   Lample Guillaume, 2018, P 6 INT C LEARN REPR
   Lebret R, 2015, PR MACH LEARN RES, V37, P2085
   Lee K, 2018, CONSERV GENET RESOUR, V10, P201, DOI 10.1007/s12686-017-0798-x
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Lin AY, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P873, DOI 10.1145/3178876.3186135
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu FY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019:): STUDENT RESEARCH WORKSHOP, P169
   Liu Fangyu, 2020, P 34 AAAI C ART INT
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Pecher D., 2005, GROUNDING COGNITION
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Rajendran J., 2016, 2016 C N AM CHAPT AS, P171, DOI [DOI 10.18653/V1/N16-1021, 10.18653/v1/N16-1021]
   Ramisa A, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5136
   Ramisa A, 2018, IEEE T PATTERN ANAL, V40, P1072, DOI 10.1109/TPAMI.2017.2721945
   Ravi H, 2018, PROC CVPR IEEE, P7613, DOI 10.1109/CVPR.2018.00794
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rotman G, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P910
   Rush AM, 2018, NLP OPEN SOURCE SOFTWARE (NLP-OSS), P52
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su Weijie, 2020, INT C LEARN REPR
   Tsai YHH, 2017, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2017.386
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Vendrov Ivan, 2016, P INT C LEARN REPR I, P1
   Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535
   Wehrmann J ^onatas, 2018, P IEEE C COMP VIS PA, P7718
   Wu C, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY (CIT), P1, DOI [10.1109/PESGM.2017.8273771, 10.1109/CIT.2017.11]
   Wu H, 2019, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2019.00677
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Yamada Yoshihiro, 2018, P INT C LEARN REPR W
   Zahavy T., 2018, P 30 AAAI C INN APPL
   Zhang M, 2018, FOOT ANKLE SURG, V24, P394, DOI 10.1016/j.fas.2017.04.013
   Zhong Zhun, 2017, arXiv preprint arXiv:1708.04896
NR 71
TC 4
Z9 4
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 81
DI 10.1145/3396520
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200007
DA 2024-07-18
ER

PT J
AU Zhang, JF
   Hu, HF
   Shen, GB
AF Zhang, Junfeng
   Hu, Haifeng
   Shen, Guobin
TI Joint Stacked Hourglass Network and Salient Region Attention Refinement
   for Robust Face Alignment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-scale learning; hourglass network; salient region attention;
   inception-ResNet; maximum coordinate loss
AB Facial landmark detection aims to locate keypoints for facial images, which typically suffer from variations caused by arbitrary pose, diverse facial expressions, and partial occlusion. In this article, we propose a coarseto-fine framework that joins a stacked hourglass network and salient region attention refinement for robust face alignment. To achieve this goal, we first present a multi-scale region learning module to analyze the structure information at a different facial region and extract a strong discriminative deep feature. Then we employ a stacked hourglass network for heatmap regression and initial facial landmarks prediction. Specifically, the stacked hourglass network introduces an improved Inception-ResNet unit as a basic building block, which can effectively improve the receptive field and learn contextual feature representations. Meanwhile, a novel loss function takes into account global weights and local weights to make the heatmap regression more accurate. Different from existing heatmap regression models, we present a salient region attention refinement module to extract a precise feature based on the heatmap regression, and utilize the filtered feature for landmarks refinement to achieve accurate prediction. Extensive experimental results of several challenging datasets (including 300 Faces in the Wild, Caltech Occluded Faces in the Wild, and Annotated Facial Landmarks Faces in the Wild) confirm that our approach can achieve more competitive performance than the most advanced algorithms.
C1 [Zhang, Junfeng; Hu, Haifeng; Shen, Guobin] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM zhangjf26@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn;
   shengb@mail2.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; Natural Science Foundation of Guangdong [2017A030311029];
   National Key RAMP;D Program of China [2018YFB1601101, 2018YFB1601100];
   Science and Technology Program of Guangzhou [201704020180]; Fundamental
   Research Funds for the Central Universities of China [19lgjc03]
FX This work was supported in part by the National Natural Science
   Foundation of China under grants 61673402, 61273270, and 60802069; in
   part by the Natural Science Foundation of Guangdong under grant
   2017A030311029; in part by the National Key R&D Program of China under
   Grant No. 2018YFB1601101 of No. 2018YFB1601100; in part by the Science
   and Technology Program of Guangzhou under grant 201704020180; and in
   part by the Fundamental Research Funds for the Central Universities of
   China under grant 19lgjc03.
CR [Anonymous], 2014, INT J DISTRIB SENS N
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2018, P 2018 EUR C COMP VI
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-46484-8_29
   [Anonymous], 1995, COMPUTER VISION IMAG
   [Anonymous], 2017, ARXIV170507426
   Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Bulat Adrian, 2016, P 2016 BRIT MACH VIS
   Bulat Adrian, 2017, P 2017 INT C COMP VI
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Chu Xiao, 2017, ARXIV170207432
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Feng ZH, 2017, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2017.392
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Jourabloo A, 2015, IEEE I CONF COMP VIS, P3694, DOI 10.1109/ICCV.2015.421
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Koestinger Martin, 2011, P 2011 1 IEEE INT WO
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu QS, 2017, IEEE T IMAGE PROCESS, V26, P797, DOI 10.1109/TIP.2016.2633939
   Liu YJ, 2017, IEEE INT CONF COMP V, P1619, DOI 10.1109/ICCVW.2017.190
   Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393
   Ren Shaoqing, 2016, IEEE Trans Image Process, V25, P1233, DOI 10.1109/TIP.2016.2518867
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tompson J, 2014, ADV NEUR IN, V27
   Trigeorgis George, 2016, P 2016 C COMP VIS PA
   Tzimiropoulos G, 2015, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2015.7298989
   Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79
   Valle R., 2018, P EUR C COMP VIS ECC, P585
   Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227
   Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P52, DOI 10.1007/978-3-319-46454-1_4
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 42
TC 5
Z9 6
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 10
DI 10.1145/3374760
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100009
DA 2024-07-18
ER

PT J
AU Gupta, A
   Singhal, D
AF Gupta, Abhinav
   Singhal, Divya
TI A Simplistic Global Median Filtering Forensics Based on Frequency Domain
   Analysis of Image Residuals
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Median filter; image forensics; global DCT coefficients; spatial
   frequency; median filtered residual
ID DISCRETE COSINE TRANSFORM; ANTI-FORENSICS; ENHANCEMENT; RECOGNITION;
   NETWORKS
AB Sophisticated image forgeries introduce digital image forensics as an active area of research. In this area, many researchers have addressed the problem of median filtering forensics. Existing median filtering detectors are adequate to classify median filtered images in uncompressed mode and in compressed mode at high-quality factors. Despite that, the field is lacking a robust method to detect median filtering in low-resolution images compressed with low-quality factors. In this article, a novel feature set (four feature dimensions), based on first-order statistics of frequency contents of median filtered residuals (MFRs) of original and median filtered images, has been proposed. The proposed feature set outperforms handcrafted features-based state-of-the-art detectors in terms of feature set dimensions and detection results obtained for low-resolution images at all quality factors. Also, results reveal the efficacy of proposed method over deep-learning-based median filtering detector. Comprehensive results expose the efficacy of the proposed detector to detect median filtering against other similar manipulations. Additionally, generalization ability test on cross-database images support the cross-validation results on four different databases. Thus, our proposed detector meets the current challenges in the field, to a great extent.
C1 [Gupta, Abhinav; Singhal, Divya] Jaypee Inst Informat Technol, Elect & Commun Engn Dept, A-10,Sect 62, Noida 201309, Uttar Pradesh, India.
C3 Jaypee Institute of Information Technology (JIIT)
RP Gupta, A (corresponding author), Jaypee Inst Informat Technol, Elect & Commun Engn Dept, A-10,Sect 62, Noida 201309, Uttar Pradesh, India.
EM abhinav.gupta@jiit.ac.in; singhal_dia@yahoo.com
RI Singhal, Divya/ADN-8363-2022
OI GUPTA, ABHINAV/0000-0002-1939-5407
CR ALT FL, 1962, J ACM, V9, P240, DOI 10.1145/321119.321122
   [Anonymous], P SPIE
   [Anonymous], 2011, P 13 INF HID C PRAG
   Bas P., 2007, Bows-2
   Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953
   BOVIK AC, 1987, IEEE T ACOUST SPEECH, V35, P493, DOI 10.1109/TASSP.1987.1165153
   Brosch T, 2015, NEURAL COMPUT, V27, P211, DOI 10.1162/NECO_a_00682
   Cao G, 2014, IEEE T INF FOREN SEC, V9, P515, DOI 10.1109/TIFS.2014.2300937
   Chen CL, 2013, IEEE T IMAGE PROCESS, V22, P4699, DOI 10.1109/TIP.2013.2277814
   Chen JS, 2015, IEEE SIGNAL PROC LET, V22, P1849, DOI 10.1109/LSP.2015.2438008
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Dang-Nguyen D.T., 2015, P ACM MULT SYST C, P219
   Er MJ, 2005, IEEE T NEURAL NETWOR, V16, P679, DOI 10.1109/TNN.2005.844909
   Ergen B, 2016, TURK J ELECTR ENG CO, V24, P1768, DOI 10.3906/elk-1309-65
   Fan W, 2015, IEEE T INF FOREN SEC, V10, P1076, DOI 10.1109/TIFS.2015.2398362
   Farmer ME, 2004, INT C PATT RECOG, P106, DOI 10.1109/ICPR.2004.1334053
   Gupta A, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176650
   Harding PRG, 2004, INT C PATT RECOG, P286, DOI 10.1109/ICPR.2004.1334523
   Kang XG, 2013, IEEE T INF FOREN SEC, V8, P1456, DOI 10.1109/TIFS.2013.2273394
   Kirchner M, 2008, IEEE T INF FOREN SEC, V3, P582, DOI 10.1109/TIFS.2008.2008214
   Li HD, 2018, IEEE T CIRC SYST VID, V28, P31, DOI 10.1109/TCSVT.2016.2599849
   Liao SX, 1996, IEEE T PATTERN ANAL, V18, P254, DOI 10.1109/34.485554
   Liu AN, 2017, MULTIMED TOOLS APPL, V76, P22119, DOI 10.1007/s11042-017-4845-0
   Liu ZM, 2010, PATTERN RECOGN, V43, P2882, DOI 10.1016/j.patcog.2010.03.003
   Niu YK, 2017, SIGNAL PROCESS-IMAGE, V53, P65, DOI 10.1016/j.image.2017.01.008
   Papakostas G. A., 2009, IMAGE PROCESSING, DOI [10.5772/7043, DOI 10.5772/7043]
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Stamm MC, 2011, IEEE T INF FOREN SEC, V6, P1050, DOI 10.1109/TIFS.2011.2119314
   Stamm MC, 2010, IEEE T INF FOREN SEC, V5, P492, DOI 10.1109/TIFS.2010.2053202
   Stuchi J.A., 2017, 2017 IEEE 27 INT WOR, P1
   Tang HS, 2018, J VIS COMMUN IMAGE R, V51, P162, DOI 10.1016/j.jvcir.2018.01.011
   Thai TH, 2017, IEEE T INF FOREN SEC, V12, P123, DOI 10.1109/TIFS.2016.2604208
   USDA NRCS, 2014, NAT RES CONS SERV PH
   Wang DP, 2018, MULTIMED TOOLS APPL, V77, P23411, DOI 10.1007/s11042-018-5651-z
   Wu ZH, 2013, INT CONF ACOUST SPEE, P3043, DOI 10.1109/ICASSP.2013.6638217
   Yuan HD, 2011, IEEE T INF FOREN SEC, V6, P1335, DOI 10.1109/TIFS.2011.2161761
NR 36
TC 6
Z9 6
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2019
VL 15
IS 3
AR 71
DI 10.1145/3321508
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ6YR
UT WOS:000494301200003
DA 2024-07-18
ER

PT J
AU Yuan, B
   Gao, XB
   Niu, ZX
   Tian, Q
AF Yuan, Bo
   Gao, Xinbo
   Niu, Zhenxing
   Tian, Qi
TI Discovering Latent Topics by Gaussian Latent Dirichlet Allocation and
   Spectral Clustering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Latent Dirichlet allocation; Gaussian; spectral clustering; image
   retrieval; diversity
AB Today, diversifying the retrieval results of a certain query will improve customers' search efficiency. Showing the multiple aspects of information provides users an overview of the object, which helps them fast target their demands. To discover aspects, research focuses on generating image clusters from initially retrieved results. As an effective approach, latent Dirichlet allocation (LDA) has been proved to have good performance on discovering high-level topics. However, traditional LDA is designed to process textual words, and it needs the input as discrete data. When we apply this algorithm to process continuous visual images, a common solution is to quantize the continuous features into discrete form by a bag-of-visual-words algorithm. During this process, quantization error will lead to information that inevitably is lost. To construct a topic model with complete visual information, this work applies Gaussian latent Dirichlet allocation (GLDA) on the diversity issue of image retrieval. In this model, traditional multinomial distribution is substituted with Gaussian distribution to model continuous visual features. In addition, we propose a two-phase spectral clustering strategy, called dual spectral clustering, to generate clusters from region level to image level. The experiments on the challenging landmarks of the DIV400 database show that our proposal improves relevance and diversity by about 10% compared to traditional topic models.
C1 [Yuan, Bo; Gao, Xinbo] Xidian Univ, 2 Taibai South Rd, Xian 710071, Shaanxi, Peoples R China.
   [Niu, Zhenxing] Alibaba Grp, 969 Wenyi West Rd, Hangzhou 311121, Zhejiang, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, San Antonio, TX 78249 USA.
C3 Xidian University; Alibaba Group; University of Texas System; University
   of Texas at San Antonio (UTSA)
RP Yuan, B (corresponding author), Xidian Univ, 2 Taibai South Rd, Xian 710071, Shaanxi, Peoples R China.
EM nosuper1@163.com; xbgao@mail.xidian.edu.cn; zhenxingniu@gmail.com;
   qitian@cs.utsa.edu
RI WANG, YANAN/KCL-4840-2024; Gao, Xinbo/Q-8622-2016
OI Gao, Xinbo/0000-0003-1443-0776
FU National High-Level Talents Special Support Program (Leading Talent of
   Technological Innovation of Ten-Thousands Talents Program)
   [CS31117200001]; National Key Research and Development Program of China
   [2016QY01W0200]; National Natural Science Foundation of China [61432014]
FX The work was supported by the National High-Level Talents Special
   Support Program (Leading Talent of Technological Innovation of
   Ten-Thousands Talents Program (no. CS31117200001)), the National Key
   Research and Development Program of China (no. 2016QY01W0200), the
   National Natural Science Foundation of China (grant no. U1605252), and
   the National Natural Science Foundation of China (grant no. 61432014).
CR Ahern S, 2007, ACM-IEEE J CONF DIG, P1, DOI 10.1145/1255175.1255177
   [Anonymous], 2014, MULT SYST C 2014
   [Anonymous], 2004, Proceedings of the 12th ACM International Conference on Multimedia, DOI DOI 10.1145/1027527.1027747
   [Anonymous], ACM MULTIMEDIA
   Bianco S, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2801126
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Clarke C. L., 2008, P 31 ANN INT ACM SIG, P659, DOI [DOI 10.1145/1390334.1390446, 10.1145/1390334.1390446]
   Das R, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P795
   Deselaers Thomas., 2009, Proceedings of the ACM international conference on image and video retrieval, P1
   Dang-Nguyen DT, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3103613
   Hu PF, 2014, PATTERN RECOGN, V47, P1138, DOI 10.1016/j.patcog.2013.06.010
   Irie G, 2013, PROC CVPR IEEE, P329, DOI 10.1109/CVPR.2013.49
   Jiang ZB, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P545, DOI 10.1145/3077136.3080805
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Li Y., 2008, 4 INT C WIRELESS COM, P1
   Liang SS, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P751, DOI 10.1145/2623330.2623650
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Paramita ML, 2010, LECT NOTES COMPUT SC, V6242, P45, DOI 10.1007/978-3-642-15751-6_6
   Russell B.C., 2006, Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, V2, P1605, DOI DOI 10.1109/CVPR.2006.326
   Santos R. L. T., 2010, P 19 INT C WORLD WID, P881, DOI [10.1145/1772690.1772780, DOI 10.1145/1772690.1772780]
   Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407
   Spyromitros-Xioufis E, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P323, DOI 10.1145/2671188.2749334
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   Vieira MR, 2011, PROC INT CONF DATA, P1163, DOI 10.1109/ICDE.2011.5767846
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang D, 2016, IEEE T IMAGE PROCESS, V25, P4540, DOI 10.1109/TIP.2016.2592800
   Wang M, 2017, IEEE T KNOWL DATA EN, V29, P1101, DOI 10.1109/TKDE.2017.2654445
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wood F, 2008, J NEUROSCI METH, V173, P1, DOI 10.1016/j.jneumeth.2008.04.030
   Yang LJ, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240141
   Zahálka J, 2015, IEEE T MULTIMEDIA, V17, P2235, DOI 10.1109/TMM.2015.2480007
   Zhang Y, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2659520
   Zhou WG, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422960
NR 36
TC 4
Z9 4
U1 0
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
AR 25
DI 10.1145/3290047
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9JB
UT WOS:000459798800009
DA 2024-07-18
ER

PT J
AU Yang, L
   Wang, YX
   Gu, JH
   Cao, XC
   Wang, X
   Jin, D
   Ding, GG
   Han, JG
   Zhang, WX
AF Yang, Liang
   Wang, Yuexue
   Gu, Junhua
   Cao, Xiaochun
   Wang, Xiao
   Jin, Di
   Ding, Guiguang
   Han, Jungong
   Zhang, Weixiong
TI Autonomous Semantic Community Detection via Adaptively Weighted Low-rank
   Approximation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Community detection; nonnegative matrix factorization; low-rank
   approximation; rank determination
ID COMPLEX NETWORKS; SYNCHRONIZATION; ALGORITHMS; MODEL
AB Identification of semantic community structures is important for understanding the interactions and sentiments of different groups of people and predicting the social emotion. A robust community detection method needs to autonomously determine the number of communities and community structure for a given network. Nonnegative matrix factorization (NMF), a component decomposition approach for latent sentiment discovery, has been extensively used for community detection. However, the existing NMF-based methods require the number of communities to be determined a priori, limiting their applicability in practice of affective computing. Here, we develop a novel NMF-based method to autonomously determine the number of semantic communities and community structure simultaneously. In our method, we use an initial number of semantic communities, larger than the actual number, in the NMF formulation, and then suppress some of the communities by introducing an adaptively weighted group-sparse low-rank regularization to derive the target number of communities and at the same time the corresponding community structure. Our method not only maintains the efficiency without increasing the complexity compared to the original NMF method but also can be straightforwardly extended to handle the non-network data. We thoroughly examine the new method, showing its superior performance over several competing methods on synthetic and large real-world social networks.
C1 [Yang, Liang; Wang, Yuexue; Gu, Junhua] Hebei Univ Technol, Hebei Prov Key Lab Big Data Calculat, Sch Artificial Intelligence, 5340 Xiping Rd, Tianjin 300130, Peoples R China.
   [Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, 89A Minzhuang Rd, Beijing 100093, Peoples R China.
   [Wang, Xiao] Beijing Univ Posts & Telecommun, Sch Comp Sci, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
   [Jin, Di] Tianjin Univ, Coll Intelligence & Comp, 135 Yaguan Rd, Tianjin 300350, Peoples R China.
   [Ding, Guiguang] Tsinghua Univ, Sch Software, Beijing, Peoples R China.
   [Han, Jungong] Univ Warwick, Coventry, W Midlands, England.
   [Zhang, Weixiong] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
C3 Hebei University of Technology; Chinese Academy of Sciences; Institute
   of Information Engineering, CAS; Beijing University of Posts &
   Telecommunications; Tianjin University; Tsinghua University; University
   of Warwick; Washington University (WUSTL)
RP Gu, JH (corresponding author), Hebei Univ Technol, Hebei Prov Key Lab Big Data Calculat, Sch Artificial Intelligence, 5340 Xiping Rd, Tianjin 300130, Peoples R China.; Wang, X (corresponding author), Beijing Univ Posts & Telecommun, Sch Comp Sci, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
EM yangliang@vip.qq.com; yuexuewang@yeah.net; jhgu@hebut.com;
   caoxiaochun@iie.ac.cn; xiaowang@bupt.edu.cn; jindi@tju.edu.cn;
   dinggg@tsinghua.edu.cn; jungong.han@warwick.ac.uk;
   weixiong.zhang@wustl.edu
RI Jin, Di/AAC-3716-2019; Ding, Guiguang/KIL-3528-2024; Han,
   Jungong/ABE-6812-2020
OI Zhang, Weixiong/0000-0002-4998-9791; Yang, Liang/0000-0002-3358-4052
FU National Key R&D Program of China [2017YFC0820106, 2016YFB0800403];
   National Natural Science Foundation of China [61972442, U1636214,
   61772361]; Beijing Natural Science Foundation [4172068]
FX This work was supported in part by the National Key R&D Program of China
   (Grants No. 2017YFC0820106 and No. 2016YFB0800403), National Natural
   Science Foundation of China (Grants No. 61972442, No. U1636214, and No.
   61772361) and Beijing Natural Science Foundation (Grant No. 4172068).
CR Adamic N., 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277
   Ahn YY, 2010, NATURE, V466, P761, DOI 10.1038/nature09182
   AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   [Anonymous], 2008, P 31 ANN INT ACM SIG, DOI DOI 10.1145/1390334.1390387
   Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cemgil Ali Taylan, 2009, Comput Intell Neurosci, P785152, DOI 10.1155/2009/785152
   Demmel J., 1993, ACTA NUMER, V2, P111
   Ding GG, 2018, IEEE T INTELL TRANSP, V19, P140, DOI 10.1109/TITS.2017.2774778
   Duch J, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.027104
   Fortunato S, 2016, PHYS REP, V659, P1, DOI 10.1016/j.physrep.2016.09.002
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Henderson K., 2012, P ACM SIGKDD INT C K, P1231
   Hoffman M.D., 2010, ICML, P439
   Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107
   Lancichinetti A, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.046110
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Liu C., 2010, P 19 INT C WORLD WID, P681, DOI [10.1145/1772690.1772760, DOI 10.1145/1772690.1772760]
   Lu RQ, 2014, IEEE T NEUR NET LEAR, V25, P2110, DOI 10.1109/TNNLS.2014.2305443
   Lusseau D, 2004, P ROY SOC B-BIOL SCI, V271, pS477, DOI 10.1098/rsbl.2004.0225
   Malliaros FD, 2013, PHYS REP, V533, P95, DOI 10.1016/j.physrep.2013.08.002
   Mazumder R, 2010, J MACH LEARN RES, V11, P2287
   Morup Morten, 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P1923
   Morup M, 2009, J CHEMOMETR, V23, P352, DOI 10.1002/cem.1223
   Nelson DL, 2004, BEHAV RES METH INS C, V36, P402, DOI 10.3758/BF03195588
   Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Newman MEJ, 2013, PHYS REV E, V88, DOI 10.1103/PhysRevE.88.042822
   Psorakis I, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.066114
   Qin JH, 2015, IEEE T NEUR NET LEAR, V26, P510, DOI 10.1109/TNNLS.2014.2316245
   Rosli Nurlaila, 2016, Advances in Machine Learning and Signal Processing, MALSIP 2015. Proceedings: LNEE 387, P175, DOI 10.1007/978-3-319-32213-1_16
   Rosvall M, 2008, P NATL ACAD SCI USA, V105, P1118, DOI 10.1073/pnas.0706851105
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Shen B, 2013, IEEE T NEUR NET LEAR, V24, P2027, DOI 10.1109/TNNLS.2013.2271357
   Silva TC, 2012, IEEE T NEUR NET LEAR, V23, P385, DOI 10.1109/TNNLS.2011.2181866
   Smaragdis P, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P177, DOI 10.1109/ASPAA.2003.1285860
   Song P, 2016, INT CONF ACOUST SPEE, P5180, DOI 10.1109/ICASSP.2016.7472665
   Strehl A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P93, DOI 10.1162/153244303321897735
   Tan VYF, 2013, IEEE T PATTERN ANAL, V35, P1592, DOI 10.1109/TPAMI.2012.240
   Traud AL, 2012, PHYSICA A, V391, P4165, DOI 10.1016/j.physa.2011.12.021
   Wang F, 2011, DATA MIN KNOWL DISC, V22, P493, DOI 10.1007/s10618-010-0181-y
   Wang Y.-X., 2013, IEEE T KNOWL DATA EN, V25, P6, DOI DOI 10.1109/TKDE.2012.51
   Xie JR, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2501654.2501657
   Yang JY, 2017, SCI REP-UK, V7, DOI [10.1038/s41598-017-09018-2, 10.1038/srep39903]
   Yang L., 2016, P 25 INT JOINT C ART, V16, P2252, DOI DOI 10.5555/3060832.3060936
   Yang L.Y., 2015, SCI REP, V5, P1
   Yang L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0178029
   Yang L, 2015, IEEE T CYBERNETICS, V45, P2585, DOI 10.1109/TCYB.2014.2377154
   Yang TB, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P927
   ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752
   Zhao SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1319, DOI 10.1145/3240508.3240591
   Zhao SC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3233184
   Zhao SC, 2018, IEEE T CYBERNETICS, V48, P3218, DOI 10.1109/TCYB.2017.2762344
   Zhao SC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4669
   Zhao SC, 2020, IEEE T AFFECT COMPUT, V11, P574, DOI [10.1109/TAFFC.2018.2818685, 10.1109/TAFFC.2016.2628787]
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhong Mingjun, 2009, J MACH LEARN RES, P663
NR 59
TC 7
Z9 7
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2019
VL 15
IS 3
SU S
SI SI
AR 98
DI 10.1145/3355393
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LR5EU
UT WOS:000535718800014
DA 2024-07-18
ER

PT J
AU Barbon, S
   Campos, GFC
   Tavares, GM
   Igawa, RA
   Proenca, ML
   Guido, RC
AF Barbon, Sylvio, Jr.
   Campos, Gabriel F. C.
   Tavares, Gabriel M.
   Igawa, Rodrigo A.
   Proenca, Mario L., Jr.
   Guido, Rodrigo Capobianco
TI Detection of Human, Legitimate Bot, and Malicious Bot in Online Social
   Networks Based on Wavelets
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE OSN frauds; text mining; writing style; wavelets; bots
ID ACCESS-CONTROL
AB Social interactions take place in environments that influence people's behaviours and perceptions. Nowadays, the users of Online Social Network (OSN) generate a massive amount of content based on social interactions. However, OSNs wide popularity and ease of access created a perfect scenario to practice malicious activities, compromising their reliability. To detect automatic information broadcast in OSN, we developed a wavelet-based model that classifies users as being human, legitimate robot, or malicious robot, as a result of spectral patterns obtained from users' textual content. We create the feature vector from the Discrete Wavelet Transform along with a weighting scheme called Lexicon-based Coefficient Attenuation. In particular, we induce a classification model using the Random Forest algorithm over two real Twitter datasets. The corresponding results show the developed model achieved an average accuracy of 94.47% considering two different scenarios: single theme and miscellaneous one.
C1 [Barbon, Sylvio, Jr.; Campos, Gabriel F. C.; Tavares, Gabriel M.; Igawa, Rodrigo A.; Proenca, Mario L., Jr.] Londrina State Univ UEL, Dept Comp, Rod Celso Garcia Cid km 380, BR-86057970 Londrina, PR, Brazil.
   [Guido, Rodrigo Capobianco] Sao Paulo State Univ UNESP, Inst Biociencias Letras & Ciencias Exatas, Rua Cristovao Colombo 2265, BR-15054000 Sao Jose Do Rio Preto, SP, Brazil.
C3 Universidade Estadual Paulista
RP Barbon, S (corresponding author), Londrina State Univ UEL, Dept Comp, Rod Celso Garcia Cid km 380, BR-86057970 Londrina, PR, Brazil.
EM barbon@uel.br; camposg@uel.br; gtavares@uel.br; igawa@uel.br;
   proenca@uel.br; guido@ieee.org
RI Barbon Junior, Sylvio/L-6137-2013; Proença, Mario Lemes/B-8340-2016;
   GUIDO, RODRIGO CAPOBIANCO/L-6236-2018
OI Barbon Junior, Sylvio/0000-0002-4988-0702; Proença, Mario
   Lemes/0000-0002-0492-322X; GUIDO, RODRIGO
   CAPOBIANCO/0000-0002-0924-8024; Marques Tavares,
   Gabriel/0000-0002-2601-8108
CR Aggarwal CC, 2011, SOCIAL NETWORK DATA ANALYTICS, P1
   [Anonymous], 2017, ANAIS PRINCIPAIS 13
   [Anonymous], 2006, TEXT MINING HDB ADV
   [Anonymous], P ANN C BRAZ S INF S
   [Anonymous], 2016, ISYS REV BRASILEIRA
   [Anonymous], CH CRC MACH LEARN PA
   [Anonymous], ARXIV14062746
   [Anonymous], 2012, ONLINE SPAM FILTERIN
   [Anonymous], JURNAL ILMIAH TEKNOL
   [Anonymous], P 2006 IEEE WIC ACM
   [Anonymous], P 2 INT C INN COMP I
   [Anonymous], P DOCT S INF ENG DSI
   [Anonymous], 2010, P 17 ACM C COMP COMM
   [Anonymous], THESIS
   [Anonymous], P NETW DISTR SYST S
   Arru G, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P941
   Bahrainian SA, 2013, IEEE INT C COMPUT, P227, DOI 10.1109/CSE.2013.44
   Barbon S, 2007, IEEE INT SYM MULTIM, P256, DOI 10.1109/ISM.Workshops.2007.51
   Barbon S, 2017, MULTIMED TOOLS APPL, V76, P3213, DOI 10.1007/s11042-016-3899-8
   Bhat SajidYousuf., 2013, ADV SOCIAL NETWORKS, P100
   Boshmaf Y, 2011, 27TH ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE (ACSAC 2011), P93
   Breiman L., 2001, Mach. Learn., V45, P5
   Cao Q., 2012, P 9 USENIX C NETW SY, P15
   Chen L, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700290
   Chu Z, 2010, 26TH ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE (ACSAC 2010), P21
   Chu Z, 2012, IEEE T DEPEND SECURE, V9, P811, DOI 10.1109/TDSC.2012.75
   Cingiz MÖ, 2015, EXPERT SYST APPL, V42, P5256, DOI 10.1016/j.eswa.2015.02.025
   del Río S, 2014, INFORM SCIENCES, V285, P112, DOI 10.1016/j.ins.2014.03.043
   Fong S, 2012, 2012 INTERNATIONAL CONFERENCE ON FUTURE GENERATION COMMUNICATION TECHNOLOGY (FGCT), P58, DOI 10.1109/FGCT.2012.6476584
   Grier C, 2010, PROCEEDINGS OF THE 17TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'10), P27, DOI 10.1145/1866307.1866311
   Hall M.A., 1999, P 17 INT C MACHINE L, P359
   Hassan A, 2013, 2013 ASE/IEEE INTERNATIONAL CONFERENCE ON SOCIAL COMPUTING (SOCIALCOM), P357, DOI 10.1109/SocialCom.2013.56
   Igawa RA, 2016, INFORM SCIENCES, V332, P72, DOI 10.1016/j.ins.2015.10.039
   Jiang M, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P305, DOI 10.1145/2567948.2577306
   Jin L, 2013, COMPUT SECUR, V37, P15, DOI 10.1016/j.cose.2013.04.003
   Jin L, 2013, IEEE COMMUN MAG, V51, P144, DOI 10.1109/MCOM.2013.6588663
   Keretna S, 2013, IEEE SYS MAN CYBERN, P3079, DOI 10.1109/SMC.2013.525
   Li Y, 2015, COMPUT SECUR, V49, P239, DOI 10.1016/j.cose.2014.10.012
   Liang-Chi Hsieh, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P949, DOI 10.1109/ICME.2012.135
   Ma SX, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808206
   Miller NE, 1998, VISUALIZATION '98, PROCEEDINGS, P189, DOI 10.1109/VISUAL.1998.745302
   Miller Z, 2014, INFORM SCIENCES, V260, P64, DOI 10.1016/j.ins.2013.11.016
   Mostafa MM, 2013, EXPERT SYST APPL, V40, P4241, DOI 10.1016/j.eswa.2013.01.019
   Pang J, 2015, COMPUT SECUR, V54, P44, DOI 10.1016/j.cose.2015.04.013
   Park LAF, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P346, DOI 10.1109/ICDM.2002.1183922
   Potha N, 2014, LECT NOTES ARTIF INT, V8445, P313, DOI 10.1007/978-3-319-07064-3_25
   Shehab M, 2012, COMPUT SECUR, V31, P897, DOI 10.1016/j.cose.2012.07.008
   Singh K, 2014, INFORM SCIENCES, V278, P488, DOI 10.1016/j.ins.2014.03.066
   Smailovic J, 2014, INFORM SCIENCES, V285, P181, DOI 10.1016/j.ins.2014.04.034
   Thaicharoen S., 2008, AUSDM VOLUME 87 CRPI, V87, P209
   Wald Randall, 2013, 2013 IEEE 14th International Conference on Information Reuse & Integration (IRI), P6, DOI 10.1109/IRI.2013.6642447
   Wald R, 2013, PROC INT C TOOLS ART, P135, DOI 10.1109/ICTAI.2013.30
   Wang Z, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2523001.2523005
   Weng J., 2021, Proc. Int. AAAI Conf. Web Soc. Media, V5, P401, DOI [10.1609/icwsm.v5i1.14102, DOI 10.1609/ICWSM.V5I1.14102]
   Wong PC, 2003, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2003, PROCEEDINGS, P97
   Xexeo Geraldo, 2008, Wl 2008. 2008 IEEE/WIC/ACM International Conference on Web Intelligence. IAT 2008. 2008 IEEE/WIC/ACM International Conference on Intelligent Agent Technology. Wl-IAT Workshop 2008 2008 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology Workshops, P272, DOI 10.1109/WIIAT.2008.221
   Ye CH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978571
   Zappavigna M, 2011, NEW MEDIA SOC, V13, P788, DOI 10.1177/1461444810385097
NR 58
TC 24
Z9 24
U1 1
U2 38
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2018
VL 14
IS 1
SU S
AR 26
DI 10.1145/3183506
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZK
UT WOS:000433517100012
OA Green Published
DA 2024-07-18
ER

PT J
AU Fränti, P
   Mariescu-Istodor, R
   Sengupta, L
AF Franti, Pasi
   Mariescu-Istodor, Radu
   Sengupta, Lahari
TI O-Mopsi: Mobile Orienteering Game for Sightseeing, Exercising, and
   Education
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE GPS; location; orienteering; target finding; competition
AB Location-based games have been around already since 2000 but only recently when PokemonGo came to markets it became clear that they can reach wide popularity. In this article, we perform a literature-based analytical study of what kind of issues location-based game design faces, and how they can be solved. We study how to use and verify the location, the role of the games as exergames, use in education, and study technical and safety issues. As a case study, we present O-Mopsi game that combines physical activity with problem solving. It includes three challenges: (1) navigating to the next target, (2) deciding the order of targets, (3) physical movement. All of them are unavoidable and relevant. For guiding the players, we use three types of multimedia: images (targets and maps), sound (user guidance), and GPS (for positioning). We discuss motivational aspects, analysis of the playing, and content creation. The quality of experiences is reported based on playing in SciFest Science festivals during 2011-2016.
C1 [Franti, Pasi; Mariescu-Istodor, Radu; Sengupta, Lahari] Univ Eastern Finland, Sch Comp, Box 111, Joensuu 80101, Finland.
C3 University of Eastern Finland
RP Fränti, P (corresponding author), Univ Eastern Finland, Sch Comp, Box 111, Joensuu 80101, Finland.
EM franti@cs.uef.fi; radum@cs.uef.fi; lahari@cs.uef.fi
RI Sengupta, Lahari/ABC-7073-2020; Sengupta, Lahari/AAL-1034-2020
OI Sengupta, Lahari/0000-0002-5265-6728; Franti, Pasi/0000-0002-9554-2827
CR [Anonymous], 2011, SBGAMES 11 P 2011 BR
   [Anonymous], 2012, LOCATION BASED MOBIL
   Baranowski T, 2015, GAMES HEALTH J, V4, P421, DOI 10.1089/g4h.2015.0070
   Benford S., 2006, ACM Transactions on Computer-Human Interaction, V13, P100, DOI 10.1145/1143518.1143522
   Best JR, 2010, DEV REV, V30, P331, DOI 10.1016/j.dr.2010.08.001
   Blanchette DM, 2005, CREATIVITY RES J, V17, P257, DOI 10.1207/s15326934crj1702&3_10
   Boulos MNK, 2013, INT J HEALTH GEOGR, V12, DOI 10.1186/1476-072X-12-18
   Brilhante IR, 2015, INFORM PROCESS MANAG, V51, P1, DOI 10.1016/j.ipm.2014.10.003
   Chang Kuo Ping, 2014, P 1 ACM SIGCHI ANN S, P323, DOI [10., DOI 10.1145/2658537.2662975]
   Cheok AD, 2004, PERS UBIQUIT COMPUT, V8, P71, DOI 10.1007/s00779-004-0267-x
   Chin TJ, 2009, VISUAL COMPUT, V25, P25, DOI 10.1007/s00371-008-0283-3
   Chittaro Luca, 2012, Persuasive Technology. Design for Health and Safety. Proceedings 7th International Conference, PERSUASIVE 2012, P43, DOI 10.1007/978-3-642-31037-9_4
   Ercsey-Ravasz M, 2012, SCI REP-UK, V2, DOI 10.1038/srep00725
   Facer K, 2004, J COMPUT ASSIST LEAR, V20, P399, DOI 10.1111/j.1365-2729.2004.00105.x
   Falk Jennica., 2001, CHI 01, P119, DOI [10.1145/634067.634140, DOI 10.1145/634067.634140]
   Gao Z, 2014, J PHYS ACT HEALTH, V11, P992, DOI 10.1123/jpah.2012-0228
   Graves L, 2008, BRIT J SPORT MED, V42, P592, DOI 10.1136/bmj.39415.632951.80
   Ha O, 2016, J PROF ISS ENG ED PR, V142, DOI 10.1061/(ASCE)EI.1943-5541.0000266
   Hamilton MT, 2008, CURR CARDIOVASC RISK, V2, P292, DOI 10.1007/s12170-008-0054-8
   Hartman N.W., 2006, ACM SIGGRAPH 2006 ED, P46, DOI DOI 10.1145/1179295.1179342
   Hjorth L., 2014, Gaming in social, locative and mobile media
   Jormanainen I., 2010, P 10 KOL CALL INT C
   La Guardia D., 2012, INT C FUTURE ED, V2nd, P508, DOI [10.1007/s10055-010-0177-3, DOI 10.1007/S10055-010-0177-3]
   Lankoski Petri, 2004, P 3 NORD C HUM COMP, P413, DOI DOI 10.1145/1028014.1028083
   Li M, 2008, WEBIST 2008: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS AND TECHNOLOGIES, VOL 1, P220
   Lynch Maria, 2012, J INT COMPUTER SCI, V18, P15
   Matai Rajesh, 2010, Traveling Salesman Problem, Theory and Applications, P1
   Matyas S., 2008, ACE 08 P 2008 INT C, P244, DOI 10.1145/1501750.1501806
   Misund G., 2009, ACE'09, Proceedings of the International Conference on Advances in Computer Entertainment Technology, P73
   Moore A., 2009, P WORLD C MOB CONT L
   Moreno A, 2013, INT IEEE CONSUM ELEC, P175, DOI 10.1109/IGIC.2013.6659161
   Nasar JL, 2013, ACCIDENT ANAL PREV, V57, P91, DOI 10.1016/j.aap.2013.03.021
   Jacob JTPN, 2011, INT J COMPUT GAMES T, V2011, DOI 10.1155/2011/495437
   Neustaedter C., 2012, ACM2012 C COMPUTER S, P235, DOI [10.1145/2141512.2141586, DOI 10.1145/2141512.2141586]
   Papadimitriou C. H., 1977, Theoretical Computer Science, V4, P237, DOI 10.1016/0304-3975(77)90012-3
   Piekarski W, 2002, COMMUN ACM, V45, P36, DOI 10.1145/502269.502291
   Puja J-C, 2011, 2011 11th IEEE International Conference on Advanced Learning Technologies (ICALT 2011), P42, DOI 10.1109/ICALT.2011.20
   González CR, 2013, PROCEDIA COMPUT SCI, V25, P428, DOI 10.1016/j.procs.2013.11.054
   Sailer M, 2013, INTERACT DES ARCHIT, P28
   Schlieder C, 2005, LECT NOTES COMPUT SC, V3814, P164, DOI 10.1007/11590323_17
   Scott A, 2011, MATH INTELL, V33, P5, DOI 10.1007/s00283-011-9256-x
   Sedano CI, 2012, EDUC TECHNOL SOC, V15, P257
   Sharad M., 2012, Neural Networks (IJCNN), The 2012 International Joint Conference on, P1
   Sharker MH, 2014, INT J GEOGR INF SCI, V28, P343, DOI 10.1080/13658816.2013.841317
   Sintoris C, 2013, INTERACT DES ARCHIT, P47
   Spikol D, 2008, FIFTH IEEE INTERNATIONAL CONFERENCE ON WIRELESS, MOBILE AND UBIQUITOUS TECHNOLOGIES IN EDUCATION, PROCEEDINGS, P31, DOI 10.1109/WMUTE.2008.37
   Suomela R, 2006, LECT NOTES COMPUT SC, V4161, P250
   Tabarcea Andrei, 2013, WEBIST 2013. 9th International Conference on Web Information Systems and Technologies. Proceedings, P300
   Thompson LL, 2013, INJURY PREV, V19, P232, DOI 10.1136/injuryprev-2012-040601
   Toth P, 2014, MOS-SIAM SER OPTIMIZ, P1
   Tsypchenko Anton, 2015, THESIS
   Tüzün H, 2009, COMPUT EDUC, V52, P68, DOI 10.1016/j.compedu.2008.06.008
   Vansteenwegen P, 2011, EUR J OPER RES, V209, P1, DOI 10.1016/j.ejor.2010.03.045
   Wan ZQ, 2014, THESIS
   Wetzel R., 2012, Proceedings of the International Conference on the Foundations of Digital Games, FDG'12, (New York, NY, USA), P238
   Yim Jeffrey., 2007, Proceedings of the 2007 conference on Future Play - Future Play '07, ACM Press, P166, DOI https://doi.org/10.1145/1328202.1328232
NR 56
TC 17
Z9 18
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 56
DI 10.1145/3115935
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA FL6JZ
UT WOS:000414352300010
DA 2024-07-18
ER

PT J
AU Guo, JT
   Zheng, PJ
   Huang, JW
AF Guo, Jianting
   Zheng, Peijia
   Huang, Jiwu
TI An Efficient Motion Detection and Tracking Scheme for Encrypted
   Surveillance Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Motion detection and tracking; video processing on the encrypted videos;
   H.264/AVC; partial encryption; surveillance video; encrypted bitstream;
   cloud
ID SELECTIVE ENCRYPTION; FAMILY-INTERACTION; H.264/AVC
AB Performing detection on surveillance videos contributes significantly to the goals of safety and security. However, performing detection on unprotected surveillance video may reveal the privacy of innocent people in the video. Therefore, striking a proper balance between maintaining personal privacy while enhancing the feasibility of detection is an important issue. One promising solution to this problem is to encrypt the surveillance videos and perform detection on the encrypted videos. Most existing encrypted signal processing methods focus on still images or small data volumes; however, because videos are typically much larger, investigating how to process encrypted videos is a significant challenge. In this article, we propose an efficient motion detection and tracking scheme for encrypted H.264/AVC video bitstreams, which does not require the previous decryption on the encrypted video. The main idea is to first estimate motion information from the bitstream structure and codeword length and, then, propose a region update (RU) algorithm to deal with the loss and error drifting of motion caused by the video encryption. The RU algorithm is designed based on the prior knowledge that the object motion in the video is continuous in space and time. Compared to the existing scheme, which is based on video encryption that occurs at the pixel level, the proposed scheme has the advantages of requiring only a small storage of the encrypted video and has a low computational cost for both encryption and detection. Experimental results show that our scheme performs better regarding detection accuracy and execution speed. Moreover, the proposed scheme can work with more than one format-compliant video encryption method, provided that the positions of the macroblocks can be extracted from the encrypted video bitstream. Due to the coupling of video stream encryption and detection algorithms, our scheme can be directly connected to the video stream output (e.g., surveillance cameras) without requiring any camera modifications.
C1 [Guo, Jianting] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou Higher Educ Mega Ctr, Guangzhou 510006, Guangdong, Peoples R China.
   [Zheng, Peijia] Sun Yat Sen Univ, Guangzhou Higher Educ Mega Ctr, Sch Data Sci & Comp, Guangzhou 510006, Guangdong, Peoples R China.
   [Zheng, Peijia] Sun Yat Sen Univ, Guangzhou Higher Educ Mega Ctr, Guangdong Key Lab Informat Secur & Technol, Guangzhou 510006, Guangdong, Peoples R China.
   [Huang, Jiwu] Shenzhen Univ, Coll Informat Engn, Nanhai Ave 3688, Shenzhen 518060, GD, Peoples R China.
   [Huang, Jiwu] Shenzhen Univ, Shenzhen Key Lab Media Secur, Nanhai Ave 3688, Shenzhen 518060, GD, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Sun Yat Sen University;
   Shenzhen University; Shenzhen University
RP Zheng, PJ (corresponding author), Sun Yat Sen Univ, Guangzhou Higher Educ Mega Ctr, Sch Data Sci & Comp, Guangzhou 510006, Guangdong, Peoples R China.; Zheng, PJ (corresponding author), Sun Yat Sen Univ, Guangzhou Higher Educ Mega Ctr, Guangdong Key Lab Informat Secur & Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM guojting@mail2.sysu.edu.cn; zhpj@mail.sysu.edu.cn; jwhuang@szu.edu.cn
RI huang, jw/KVY-9917-2024
FU NSFC [U1636202, 61332012, 61502547, 61379155]; Shenzhen RD Program
   [JCYJ20160328144421330]; Guangdong Natural Science Foundation
   [2015A030310319]
FX This work is supported by the NSFC (Grant no. U1636202, 61332012,
   61502547, 61379155), the Shenzhen R&D Program (Grant no.
   JCYJ20160328144421330), and the Guangdong Natural Science Foundation
   (Grant no. 2015A030310319).
CR Alattar A. M., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P256, DOI 10.1109/ICIP.1999.819590
   [Anonymous], 2003, H.264 and MPEG-4 video compression: video coding for next generation multimedia
   [Anonymous], 2000, P EUR C COMP VIS, DOI DOI 10.1007/3-540-45053-X_48
   [Anonymous], 2001, CAVIAR DAT
   [Anonymous], 1978, FDN SEC COMPUT
   Bianchi T, 2013, IEEE SIGNAL PROC MAG, V30, P87, DOI 10.1109/MSP.2012.2228342
   Celano M, 2008, FAM PROCESS, V47, P7, DOI 10.1111/j.1545-5300.2008.00236.x
   Chu K.-Y., 2013, Proceedings of the 21st ACM international conference on Multimedia, P597
   Daemen Joan, 1999, AES Proposal
   Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010
   DUNCAN JH, 1992, IEEE T PATTERN ANAL, V14, P346, DOI 10.1109/34.120329
   Erkin Z, 2009, LECT NOTES COMPUT SC, V5672, P235, DOI 10.1007/978-3-642-03168-7_14
   Ferguson N., 2000, FAST SOFTWARE ENCRYP, P213
   Gustafsson PA, 2002, PEDIAT ALLERG IMM-UK, V13, P51, DOI 10.1034/j.1399-3038.2002.00086.x
   Lathey A, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2656205
   Laumer M., 2013, P C COMP VIS IM APPL
   LAUMER M, 2015, PROCEEDINGS OF THE P, P282
   Li Y, 2005, Proceedings of 2005 International Conference on Innovation & Management, P1120
   Lian SG, 2008, MULTIMED TOOLS APPL, V38, P75, DOI 10.1007/s11042-007-0150-7
   Lian SG, 2006, IEEE T CONSUM ELECTR, V52, P621, DOI 10.1109/TCE.2006.1649688
   Lin Chih-Yang, 2016, MULTIMED TOOLS APPL, P1
   Manzanera A, 2007, PATTERN RECOGN LETT, V28, P320, DOI 10.1016/j.patrec.2006.04.007
   Paillier P, 1999, LECT NOTES COMPUT SC, V1592, P223
   Poppe C, 2009, J VIS COMMUN IMAGE R, V20, P428, DOI 10.1016/j.jvcir.2009.05.001
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917
   Shahid Z, 2011, IEEE T CIRC SYST VID, V21, P565, DOI 10.1109/TCSVT.2011.2129090
   Spanos G. A., 1995, Proceedings Fourth International Conference on Computer Communications and Networks (ICCCN'95) (Cat. No.95TB8110), P2, DOI 10.1109/ICCCN.1995.540095
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Stinson D. R., 2005, CRYPTOGRAPHY THEORY
   Stütz T, 2012, IEEE T CIRC SYST VID, V22, P325, DOI 10.1109/TCSVT.2011.2162290
   Szczerba K, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P478, DOI 10.1109/AVSS.2009.78
   Thomas Nithin M., 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P85
   Thompson C, 2016, PROCEEDINGS OF THE 6TH WORKSHOP ON SECURITY AND PRIVACY IN SMARTPHONES AND MOBILE DEVICES (SPSM'16), P53, DOI 10.1145/2994459.2994461
   Tom M., 2013, 2013 4 NATL C COMPUT, P1
   Tsai DM, 2009, IEEE T IMAGE PROCESS, V18, P158, DOI 10.1109/TIP.2008.2007558
   Upmanyu M, 2009, IEEE I CONF COMP VIS, P1639, DOI 10.1109/ICCV.2009.5459370
   VACAVANT A, 2011, P INT C COMP VIS THE, P51
   Wang S, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700292
   Xu DW, 2014, IEEE T INF FOREN SEC, V9, P596, DOI 10.1109/TIFS.2014.2302899
   Yang XS, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2962719
   YOKOYAMA T, 2009, PROCEEDINGS OF THE 7, P201
NR 42
TC 16
Z9 16
U1 3
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 61
DI 10.1145/3131342
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300015
DA 2024-07-18
ER

PT J
AU Zhang, HB
   Zhong, BN
   Lei, Q
   Du, JX
   Peng, JL
   Chen, DS
   Ke, X
AF Zhang, Hong-Bo
   Zhong, Bineng
   Lei, Qing
   Du, Ji-Xiang
   Peng, Jialin
   Chen, Duansheng
   Ke, Xiao
TI Sparse Representation-Based Semi-Supervised Regression for People
   Counting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Counting people; sparse representation; semi-supervised regression;
   sparse reconstruction; reconstruction error
ID SCENE
AB Label imbalance and the insufficiency of labeled training samples are major obstacles in most methods for counting people in images or videos. In this work, a sparse representation-based semi-supervised regression method is proposed to count people in images with limited data. The basic idea is to predict the unlabeled training data, select reliable samples to expand the labeled training set, and retrain the regression model. In the algorithm, the initial regression model, which is learned from the labeled training data, is used to predict the number of people in the unlabeled training dataset. Then, the unlabeled training samples are regarded as an over-complete dictionary. Each feature of the labeled training data can be expressed as a sparse linear approximation of the unlabeled data. In turn, the labels of the labeled training data can be estimated based on a sparse reconstruction in feature space. The label confidence in labeling an unlabeled sample is estimated by calculating the reconstruction error. The training set is updated by selecting unlabeled samples with minimal reconstruction errors, and the regression model is retrained on the new training set. A co-training style method is applied during the training process. The experimental results demonstrate that the proposed method has a low mean square error and mean absolute error compared with those of state-of-the-art people-counting benchmarks.
C1 [Zhang, Hong-Bo] Huaqiao Univ, Xiamen Key Lab Comp Vis & Pattern Recognit, Xiamen, Peoples R China.
   [Zhong, Bineng; Lei, Qing; Du, Ji-Xiang; Peng, Jialin; Chen, Duansheng] Huaqiao Univ, Xiamen, Peoples R China.
   [Ke, Xiao] Fuzhou Univ, Fujian Key Lab Network Comp & Intelligent Informa, Fuzhou, Fujian, Peoples R China.
   [Zhang, Hong-Bo; Zhong, Bineng; Lei, Qing; Du, Ji-Xiang; Peng, Jialin; Chen, Duansheng] Huaqiao Univ, Dept Comp Sci & Technol, Xiamen 361021, Peoples R China.
   [Ke, Xiao] Fuzhou Univ, Coll Math & Comp Sci, Fuzhou 350116, Fujian, Peoples R China.
C3 Huaqiao University; Huaqiao University; Fuzhou University; Huaqiao
   University; Fuzhou University
RP Zhang, HB (corresponding author), Huaqiao Univ, Xiamen Key Lab Comp Vis & Pattern Recognit, Xiamen, Peoples R China.; Du, JX (corresponding author), Huaqiao Univ, Xiamen, Peoples R China.; Zhang, HB; Du, JX (corresponding author), Huaqiao Univ, Dept Comp Sci & Technol, Xiamen 361021, Peoples R China.
EM zhanghongbo@hqu.edu.cn; bnzhong@hqu.edu.cn; leiqing@hqu.edu.cn;
   jxdu@hqu.edu.cn; jialinpeng@hqu.edu.cn; dschen@hqu.edu.cn;
   kex@fzu.edu.cn
OI Peng, Jialin/0000-0002-1797-0762; Berretti, Stefano/0000-0003-1219-4386
FU National Science Foundation of China [61502182, 11401231, 61502105,
   61572205, 61673186]; Natural Science Foundation of Fujian Province of
   China [2015J01253, 2015J01257, 2017J01110]; Education and scientific
   research projects of young and middle-aged teachers in Fujian Province
   [JA15075]; Pilot Project of Fujian Province of China [2015H0025];
   Huaqiao University [ZQN-YX108]
FX This work is supported by the National Science Foundation of China (No.
   61502182, 11401231, 61502105, 61572205, and 61673186), the Natural
   Science Foundation of Fujian Province of China (No. 2015J01253,
   2015J01257, and 2017J01110), Education and scientific research projects
   of young and middle-aged teachers in Fujian Province (JA15075), the
   Pilot Project of Fujian Province of China (No. 2015H0025), and the
   Promotion Program for Young and Middle-aged Teachers in Science and
   Technology Research of Huaqiao University (No. ZQN-YX108).
CR [Anonymous], IEEE T INTELLIGENT T
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI [10.1145/1899412.1899418, DOI 10.1145/1899412.1899418]
   [Anonymous], MULTIMEDIA TOOLS APP
   [Anonymous], INT J ADV COMPUTER S
   Bai S, 2016, IEEE T IMAGE PROCESS, V25, P1056, DOI 10.1109/TIP.2016.2514498
   Chan AB, 2012, IEEE T IMAGE PROCESS, V21, P2160, DOI 10.1109/TIP.2011.2172800
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Chen WJ, 2014, INT J MACH LEARN CYB, V5, P459, DOI 10.1007/s13042-013-0183-3
   Chengbin Zeng, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2069, DOI 10.1109/ICPR.2010.509
   Cong Y, 2009, PROC CVPR IEEE, P1093, DOI 10.1109/CVPRW.2009.5206648
   Foroughi H, 2015, PATTERN RECOGN, V48, P3038, DOI 10.1016/j.patcog.2015.02.009
   Hou YL, 2011, IEEE T SYST MAN CY A, V41, P24, DOI 10.1109/TSMCA.2010.2064299
   Huang CL, 2011, 2011 FIRST ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P505, DOI 10.1109/ACPR.2011.6166629
   Karaman S, 2014, PATTERN RECOGN, V47, P3767, DOI 10.1016/j.patcog.2014.06.003
   Loy CC, 2013, IEEE I CONF COMP VIS, P2256, DOI 10.1109/ICCV.2013.270
   Maddalena L, 2014, PATTERN RECOGN LETT, V36, P125, DOI 10.1016/j.patrec.2013.10.006
   Merad Djamel, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P233, DOI 10.1109/AVSS.2010.77
   Mukherjee S, 2015, VISUAL COMPUT, V31, P1405, DOI 10.1007/s00371-014-1022-6
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Raghavachari C, 2015, PROCEDIA COMPUT SCI, V58, P461, DOI 10.1016/j.procs.2015.08.064
   Raina R., 2007, P 24 INT C MACH LEAR, P759
   Ryan D, 2015, COMPUT VIS IMAGE UND, V130, P1, DOI 10.1016/j.cviu.2014.07.008
   Tan B, 2011, PATTERN RECOGN, V44, P2297, DOI 10.1016/j.patcog.2010.10.002
   Wang JQ, 2014, IEICE T INF SYST, VE97D, P1673, DOI 10.1587/transinf.E97.D.1673
   Yang AY, 2010, IEEE IMAGE PROC, P1849, DOI 10.1109/ICIP.2010.5651522
   Yu GX, 2015, KNOWL INF SYST, V43, P81, DOI 10.1007/s10115-013-0702-2
   Zhou ZH, 2007, IEEE T KNOWL DATA EN, V19, P1479, DOI [10.1109/TKDE.2007.190644, 10.1109/TKDE.2007.190644.]
NR 27
TC 2
Z9 2
U1 2
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 47
DI 10.1145/3106156
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300001
DA 2024-07-18
ER

PT J
AU Ota, K
   Dao, MS
   Mezaris, V
   De Natale, FGB
AF Ota, Kaoru
   Minh Son Dao
   Mezaris, Vasileios
   De Natale, Francesco G. B.
TI Deep Learning for Mobile Multimedia: A Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; deep neural networks; mobile multimedia computing
AB Deep Learning (DL) has become a crucial technology for multimedia computing. It offers a powerful instrument to automatically produce high-level abstractions of complex multimedia data, which can be exploited in a number of applications, including object detection and recognition, speech-to-text, media retrieval, multimodal data analysis, and so on. The availability of affordable large-scale parallel processing architectures, and the sharing of effective open-source codes implementing the basic learning algorithms, caused a rapid diffusion of DL methodologies, bringing a number of new technologies and applications that outperform, in most cases, traditional machine learning technologies. In recent years, the possibility of implementing DL technologies on mobile devices has attracted significant attention. Thanks to this technology, portable devices may become smart objects capable of learning and acting. The path toward these exciting future scenarios, however, entangles a number of important research challenges. DL architectures and algorithms are hardly adapted to the storage and computation resources of a mobile device. Therefore, there is a need for new generations of mobile processors and chipsets, small footprint learning and inference algorithms, new models of collaborative and distributed processing, and a number of other fundamental building blocks. This survey reports the state of the art in this exciting research area, looking back to the evolution of neural networks, and arriving to the most recent results in terms of methodologies, technologies, and applications for mobile environments.
C1 [Ota, Kaoru] Muroran Inst Technol, Muroran, Hokkaido, Japan.
   [Minh Son Dao] Univ Teknol Brunei, Mukim Gadong A, Brunei.
   [Mezaris, Vasileios] ITI Ctr Res & Technol Hellas, Nicosia, Cyprus.
   [De Natale, Francesco G. B.] Univ Trento, DISI, Trento, Italy.
C3 Muroran Institute of Technology; University of Technology Brunei;
   University of Trento
RP Ota, K (corresponding author), Muroran Inst Technol, Muroran, Hokkaido, Japan.
EM ota@csse.muroran-it.ac.jp; minh.son@utb.edu.bn; bmezaris@iti.gr;
   denatale@ing.unitn.it
RI Dao, Minh-Son/S-5984-2019; Ota, Kaoru/HDR-5303-2022
OI Ota, Kaoru/0000-0002-3382-1652
FU Grants-in-Aid for Scientific Research [15K15976] Funding Source: KAKEN
CR Alvarez J., 2016, arXiv preprint arXiv:1606.05426
   [Anonymous], ABS160600094 CORR
   [Anonymous], CHIP COULD BRING DEE
   [Anonymous], 1958, PSYCHOL REV
   [Anonymous], 2014, ABS14126115 CORR
   [Anonymous], NEXT GEN SMARTPH TAB
   [Anonymous], PROGR GUID
   [Anonymous], ARXIV E PRINTS
   [Anonymous], 2016, ABS160908144 CORR
   [Anonymous], P 6 NAT C ART INT
   [Anonymous], COMPRESSION DEEP NEU
   [Anonymous], 2016, ABS160304467 CORR
   [Anonymous], 1962, PRINCIPLES NEURODYNA
   [Anonymous], DEEPLEARNINGKIT OPEN
   [Anonymous], EMB SYST DEV KITS MO
   [Anonymous], 2016, P 24 ACM INT C MULT
   [Anonymous], QUALC HELPS MAK YOUR
   [Anonymous], GOOGLE MOVIDIUS BRIN
   [Anonymous], 2015, ABS151000149 CORR
   [Anonymous], 2001, GRADIENT FLOW RECURR, DOI DOI 10.1109/9780470544037.CH14
   [Anonymous], P INT C COMP ARCH SY
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2016, P IEEE INT C GLOB CO, DOI DOI 10.1007/978-3-319-24990-21
   [Anonymous], EMB NEUR NETW COMP F
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI [DOI 10.1109/IJCNN.2015.7280815, 10.1109/IJCNN.2015.7280815]
   [Anonymous], P INT C AC SPEECH SI
   [Anonymous], 2014, P 22 ACM INT C MULTI
   [Anonymous], 2015, P 2015 INT WORKSHOP
   [Anonymous], 2016, P 15 ACM IEEE INT C
   [Anonymous], 2013, Queue, DOI [DOI 10.1145/2436696.2443836, 10.1145/2436696.2443836]
   [Anonymous], T NEUR NETW
   [Anonymous], 2015, INT C NEUR INF PROC
   [Anonymous], DEEP SEMANTIC MOBILE
   [Anonymous], APPL ADV NONLINEAR S
   [Anonymous], INTR BRAIN INSP COMP
   [Anonymous], 2016, TSINGHUA SCI TECHNOL
   [Anonymous], 1990, Adv Neural Inform Process Syst.
   [Anonymous], ABS14121442 CORR
   [Anonymous], 2011, 22 INT JT C ART INT, DOI 10.5555/2283516.2283603
   [Anonymous], 1991, UNTERSUCHUNGEN DYNAM
   [Anonymous], 2015, P 24 ACM INT C INF K
   [Anonymous], 2016, ABS160502688 CORR
   Anwar S, 2017, ACM J EMERG TECH COM, V13, DOI 10.1145/3005348
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Biggio B, 2014, IEEE T KNOWL DATA EN, V26, P984, DOI 10.1109/TKDE.2013.57
   Castellano G, 1997, IEEE T NEURAL NETWOR, V8, P519, DOI 10.1109/72.572092
   Catanzaro B., 2014, ARXIV NEURAL EVOLUTI
   Chellapilla K., 2006, HAL
   Chen TS, 2014, ACM SIGPLAN NOTICES, V49, P269, DOI 10.1145/2541940.2541967
   Chen WL, 2015, PR MACH LEARN RES, V37, P2285
   Chen Y., 2014, J DAIRY SCI, V10, P2014, DOI DOI 10.1109/MICR0.2014.58
   Chen YH, 2016, ISSCC DIG TECH PAP I, V59, P262, DOI 10.1109/ISSCC.2016.7418007
   Chilimbi Trishul M, 2014, P USENIX OSDI, V14, P571
   Coates A, 2013, PROC INT C MACH LEAR, P1337
   Cui HG, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901323
   DREYFUS SE, 1973, IEEE T AUTOMAT CONTR, VAC18, P383, DOI 10.1109/TAC.1973.1100330
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Gao Y, 2016, 2016 IEEE FIRST INTERNATIONAL CONFERENCE ON CONNECTED HEALTH: APPLICATIONS, SYSTEMS AND ENGINEERING TECHNOLOGIES (CHASE), P163, DOI 10.1109/CHASE.2016.14
   Gokhale V, 2014, IEEE COMPUT SOC CONF, P696, DOI 10.1109/CVPRW.2014.106
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hou SF, 2016, 2016 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE WORKSHOPS (WIW 2016), P104, DOI [10.1109/WIW.2016.040, 10.1109/WIW.2016.15]
   Huynh LN, 2016, WEARSYS'16: PROCEEDINGS OF THE 2016 WORKSHOP ON WEARABLE SYSTEMS AND APPLICATIONS, P25, DOI 10.1145/2935643.2935650
   Jindal V, 2016, 2016 IEEE/ACM INTERNATIONAL CONFERENCE ON MOBILE SOFTWARE ENGINEERING AND SYSTEMS (MOBILESOFT 2016), P36, DOI [10.1145/2897073.2897132, 10.1109/MobileSoft.2016.027]
   Kuhad P., 2015, P 2015 IEEE INT C CO, P1, DOI [10.1109/CIVEMSA.2015.7158594, DOI 10.1109/CIVEMSA.2015.7158594]
   Lane ND, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P283, DOI 10.1145/2750858.2804262
   Lane ND, 2015, 16TH INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS (HOTMOBILE' 15), P117, DOI 10.1145/2699343.2699349
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lee J, 2016, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI 2016), P948, DOI 10.1145/2957265.2962662
   Liu SC, 2016, MOBISYS'16: COMPANION COMPANION PUBLICATION OF THE 14TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P50, DOI 10.1145/2938559.2948831
   McCulloch WS, 2016, EMBODIMENTS OF MIND, P19
   Merler M, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P31, DOI 10.1145/2986035.2986036
   Mittal G, 2016, UBICOMP'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P940, DOI 10.1145/2971648.2971731
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   NARENDRA KS, 1974, IEEE T SYST MAN CYB, VSMC4, P323, DOI 10.1109/TSMC.1974.5408453
   Peddi VB, 2017, FUTURE GENER COMP SY, V66, P71, DOI 10.1016/j.future.2016.03.019
   Raina R., 2009, ICML, P1
   Rumelhart D., 1986, PARALLEL DISTRIBUTED, P318
   Sanders J, 2010, CUDA EXAMPLE INTRO G
   Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392
   Stepniewski SW, 1997, NEURAL COMPUT APPL, V5, P76, DOI 10.1007/BF01501173
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy C., 2014, P INT C LEARN REPR, P1
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tanno R, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P89, DOI 10.1145/2986035.2986044
   Tees RC, 2003, CAN PSYCHOL, V44, P74, DOI 10.1037/h0088061
   Vapnik V., 1999, NATURE STAT LEARNING
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang SW, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P851, DOI 10.1145/2984511.2984565
   Wen Wei, 2016, ABS160803665 CORR
   Weng J., 1992, IJCNN International Joint Conference on Neural Networks (Cat. No.92CH3114-6), P576, DOI 10.1109/IJCNN.1992.287150
   Widrow Bernard., 1962, Associative Storage and Retrieval of Digital Information in Networks of Adaptive "Neurons", P160
   Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521
   Yuan ZL, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P371, DOI [10.1145/2619239.2631434, 10.1145/2740070.2631434]
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang Sixin, 2015, Advances in Neural Information Processing Systems, P685
   Zhu JD, 2015, 2015 17TH INTERNATIONAL CONFERENCE ON E-HEALTH NETWORKING, APPLICATION & SERVICES (HEALTHCOM), P501, DOI 10.1109/HealthCom.2015.7454554
NR 99
TC 103
Z9 109
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
SU S
AR 34
DI 10.1145/3092831
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FP1UO
UT WOS:000417400400002
OA Green Published
DA 2024-07-18
ER

PT J
AU Wisniewski, P
   Batalla, JM
   Beben, A
   Krawiec, P
   Chydzinski, A
AF Wisniewski, Piotr
   Batalla, Jordi Mongay
   Beben, Andrzej
   Krawiec, Piotr
   Chydzinski, Andrzej
TI On Optimizing Adaptive Algorithms Based on Rebuffering Probability
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adaptive streaming; adaptation algorithms; ERA
AB Traditionally, video adaptive algorithms aim to select the representation that better fits to the current download rate. In recent years, a number of new approaches appeared that take into account the buffer occupancy and the probability of video rebuffering as important indicators of the representation to be selected. We propose an optimization of the existing algorithm based on rebuffering probability and argue that the algorithm should avoid the situations when the client buffer is full and the download is stopped, since these situations decrease the efficiency of the algorithm. Reducing full buffer states does not increase the rebuffering probability thanks to a clever management of the client buffer, which analyses the buffer occupancy and downloads higher bitrate representations only in the case of high buffer occupancy.
C1 [Wisniewski, Piotr; Batalla, Jordi Mongay; Krawiec, Piotr] Natl Inst Telecommun, Internet Technol & Applicat Dept, Szachowa Str 1, PL-04894 Warsaw, Poland.
   [Batalla, Jordi Mongay; Beben, Andrzej] Warsaw Univ Technol, Inst Telecommun, Nowowiejska Str 15-19, PL-00665 Warsaw, Poland.
   [Chydzinski, Andrzej] Silesian Tech Univ, Inst Informat, PL-44100 Gliwice, Poland.
C3 National Institute of Telecommunications - Poland; Warsaw University of
   Technology; Silesian University of Technology
RP Batalla, JM (corresponding author), Natl Inst Telecommun, Internet Technol & Applicat Dept, Szachowa Str 1, PL-04894 Warsaw, Poland.; Batalla, JM (corresponding author), Warsaw Univ Technol, Inst Telecommun, Nowowiejska Str 15-19, PL-00665 Warsaw, Poland.
EM P.Wisniewski@itl.waw.pl; J.mongay@itl.waw.pl; Abeben@tele.pw.edu.pl;
   P.Krawiec@itl.waw.pl; Andrzej.Chydzinski@polsl.pl
RI Beben, Andrzej/B-7241-2014; Batalla, Jordi Mongay/J-4084-2019; Batalla,
   Jordi/AAL-9056-2021; Krawiec, Piotr/M-9095-2013
OI Batalla, Jordi Mongay/0000-0002-1489-5138; Batalla,
   Jordi/0000-0002-1489-5138; Wisniewski, Piotr/0000-0001-8497-0896;
   Chydzinski, Andrzej/0000-0002-0168-6919; Krawiec,
   Piotr/0000-0002-2395-5155; Beben, Andrzej/0000-0003-3751-6230
FU Celtic-Plus Programme [C2015/4-5]; European Commission; Narodowe Centrum
   Badan i Rozwoju in Poland; Celtic-Plus Programme [C2015/4-5]; European
   Commission; Narodowe Centrum Badan i Rozwoju in Poland
FX This work was supported by Celtic-Plus Programme, under the project
   C2015/4-5, MONALIS: monitoring and control of QoE in large-scale media
   distribution architectures, and co-funded by the European Commission and
   the Narodowe Centrum Badan i Rozwoju in Poland.
CR Akhsabi S., 2011, P ACM MULTIMEDIA SYS
   Alberti C., 2013, P 5 INT WORKSH QUAL
   Andrzej Beben, 2016, P ACM MULTIMEDIA SYS
   Asmussen S, 2016, METHODOL COMPUT APPL, V18, P441, DOI 10.1007/s11009-014-9430-7
   Balakrishnan Hari., 1997, P ACM SIGMETRICS
   Batalla JM, 2016, IEEE J SEL AREA COMM, V34, P2154, DOI 10.1109/JSAC.2016.2577360
   Batalla JM, 2016, J REAL-TIME IMAGE PR, V12, P443, DOI 10.1007/s11554-015-0496-4
   Bezerra D., 2016, P IEEE S COMP COMM I
   Chiariotti F., 2016, P ACM MULTIMEDIA SYS
   Christopher Muller Christopher, 2012, P 4 WORKSH MOB VID
   Cicco L. D., 2011, P ACM MULTIMEDIA SYS
   Cicco L. D., 2013, P IEEE PACK VID WORK
   DASH-IF, 2015, GUID IMPL DASH AVC 2
   FELLER W., 1971, An Introduction to Probability Theory and Its Applications, V2
   Haakon Riiser, 2013, P ACM MULTIMEDIA SYS
   Huang T., 2013, P ACM FHMN WORKSH SI
   Huang T.-Y., 2014, P ACM SIGCOMM
   Hung Le T, 2013, P INT C ADV TECHN CO
   Jiang J., 2012, P ACM CONEXT
   Kim K., 2009, J SYST CYBERNET INF, V7, P54
   Liu C., 2011, P ACM MULTIMEDIA SYS
   Memos VA, 2016, J REAL-TIME IMAGE PR, V12, P473, DOI 10.1007/s11554-015-0509-3
   Miller K., 2012, P PACK VID WORKSH
   Mok R., 2012, P ACM MULTIMEDIA SYS
   Oyman O, 2012, IEEE COMMUN MAG, V50, P20, DOI 10.1109/MCOM.2012.6178830
   Piotr Wisniewski, 2015, P IEEE INT C COMM IC
   Psannis KE, 2016, J REAL-TIME IMAGE PR, V12, P509, DOI 10.1007/s11554-015-0514-6
   Salah K., 2011, COMMUNICATIONS IET, V5, P14
   Sanchez Y, 2012, SIGNAL PROCESS-IMAGE, V27, P329, DOI 10.1016/j.image.2011.10.002
   Seufert M., 2015, IEEE COMMUN SURVEYS
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Tian G., 2012, P 8 INT C EMERGING N
   Thang TC, 2012, IEEE T CONSUM ELECTR, V58, P78, DOI 10.1109/TCE.2012.6170058
   Yin X., 2014, ALG HTTP P 13 ACM WO
   Zhang Y., 2002, P ACM SIGCOMM
NR 35
TC 3
Z9 4
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
SU S
AR 43
DI 10.1145/3092837
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FP1UO
UT WOS:000417400400011
DA 2024-07-18
ER

PT J
AU Min, XK
   Zhai, GT
   Gu, K
   Yang, XK
AF Min, Xiongkuo J
   Zhai, Guangtao
   Gu, Ke
   Yang, Xiaokang
TI Fixation Prediction through Multimodal Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Audiovisual attention; multimodal analysis; saliency; eye fixation
   prediction; attention fusion
ID SALIENCY DETECTION; INFLUENCE GAZE; MODEL; ATTENTION; LOCALIZATION;
   FRAMEWORK; FUSION
AB In this article, we propose to predict human eye fixation through incorporating both audio and visual cues. Traditional visual attention models generally make the utmost of stimuli's visual features, yet they bypass all audio information. In the real world, however, we not only direct our gaze according to visual saliency; but also are attracted by salient audio cues. Psychological experiments show that audio has an influence on visual attention, and subjects tend to be attracted by the sound sources. Therefbre, we propose fusing both audio and visual information to predict eye fixation. In our proposed framework, we first localize the moving-sound-generating objects through multimodal analysis and generate an audio attention map. Then, we calculate the spatial and temporal attention maps using the visual modality. Finally, the audio, spatial, and temporal attention maps are fused to generate the final audiovisual saliency map. The proposed method is applicable to scenes containing moving sound-generating objects. We gather a set of video sequences and collect eye tracking data under an audiovisual test condition. Experiment results show that we can achieve better eye fixation prediction performance when taking both audio and visual cues into consideration, especially in some typical scenes in which object motion and audio are highly correlated.
C1 [Min, Xiongkuo J; Zhai, Guangtao; Gu, Ke; Yang, Xiaokang] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Min, XK; Zhai, GT; Gu, K; Yang, XK (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200240, Peoples R China.
EM minxiongkuo@gmail.com; zhaiguangtao@sjtu.edu.cn; guke.doctor@gmail.com;
   xkyang@sjtu.edu.cn
RI Zhai, Guangtao/X-5949-2019; Yang, Xiaokang/C-6137-2009; Gu,
   Ke/AAJ-9684-2021; Min, Xiongkuo/A-7097-2019
OI Zhai, Guangtao/0000-0001-8165-9322; Yang, Xiaokang/0000-0003-4029-3322;
   Min, Xiongkuo/0000-0001-5693-0416
FU National Natural Science Foundation of China [61422112, 61371146,
   61521062, 61527804]; National High-Tech R&D Program of China
   [2015AA015905]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61422112, 61371146, 61521062, and
   61527804; and the National High-Tech R&D Program of China under Grant
   2015AA015905.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2008, Advances in neural information processing systems
   [Anonymous], 2007, Information retrieval for music and motion
   [Anonymous], IEEE INT C IM PROC
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   [Anonymous], 2014, SALIENT OBJECT DETEC
   [Anonymous], 2009, ADV NEURAL INFORM PR
   Bao Xuan., 2010, PROC MOBISYS 2010, P357, DOI DOI 10.1145/1814433.1814468
   Barzelay Z., 2007, 2007 IEEE C COMP VIS, P1, DOI DOI 10.1109/CVPR.2007.383344
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bylinskii Zoya, 2012, MIT saliency benchmark
   Cerf M, 2009, J VISION, V9, DOI 10.1167/9.12.10
   Chamaret C, 2010, IEEE IMAGE PROC, P1077, DOI 10.1109/ICIP.2010.5651381
   Chen YX, 2014, IEEE T CIRC SYST VID, V24, P1992, DOI 10.1109/TCSVT.2014.2329380
   Cheng G., 2008, TRANSPORTATION RES B, P1
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Coutrot A., 2013, 2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS), P1
   Coutrot A, 2014, J VISION, V14, DOI 10.1167/14.8.5
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Gu K, 2015, IEEE T CIRC SYST VID, V25, P1480, DOI 10.1109/TCSVT.2014.2372392
   Gu K, 2015, IEEE SIGNAL PROC LET, V22, P1552, DOI 10.1109/LSP.2015.2413944
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V41, P35, DOI 10.1037/h0061495
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kidron E, 2005, PROC CVPR IEEE, P88
   Kidron E, 2007, IEEE T SIGNAL PROCES, V55, P1390, DOI 10.1109/TSP.2006.888095
   Kim H, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425544
   Lee JS, 2011, IEEE J-STSP, V5, P1322, DOI 10.1109/JSTSP.2011.2165199
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Li K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P147, DOI 10.1145/2647868.2654936
   Liu Ce, 2009, THESIS
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Minotto Vicente P., 2014, IEEE Transactions on Multimedia, V16, P1032, DOI 10.1109/TMM.2014.2305632
   PERROTT DR, 1990, PERCEPT PSYCHOPHYS, V48, P214, DOI 10.3758/BF03211521
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Snecdecor G.W., 1989, Statistical Methods, V8th
   Song GH, 2013, J EYE MOVEMENT RES, V6
   Vroomen J, 2000, J EXP PSYCHOL HUMAN, V26, P1583, DOI 10.1037/0096-1523.26.5.1583
   Xiongkuo Min, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P153, DOI 10.1109/QoMEX.2014.6982312
   Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45
   YANTIS S, 1990, J EXP PSYCHOL HUMAN, V16, P121, DOI 10.1037/0096-1523.16.1.121
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
NR 53
TC 61
Z9 62
U1 2
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2017
VL 13
IS 1
AR 6
DI 10.1145/2996463
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM7WJ
UT WOS:000395522700006
DA 2024-07-18
ER

PT J
AU Kuang, LW
   Yang, LT
   Rho, S
   Yan, Z
   Qiu, K
AF Kuang, Liwei
   Yang, Laurence T.
   Rho, Seungmin (Charlie)
   Yan, Zheng
   Qiu, Kai
TI A Tensor-Based Framework for Software-Defined Cloud Data Center
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Big data; tensor; software defined networks; data center
AB Multimedia has been exponentially increasing as the biggest big data, which consist of video clips, images, and audio files. Processing and analyzing them on a cloud data center have become a preferred solution that can utilize the large pool of cloud resources to address the problems caused by the tremendous amount of unstructured multimedia data. However, there exist many challenges in processing multimedia big data on a cloud data center, such as multimedia data representation approach, an efficient networking model, and an estimation method for traffic patterns. The primary purpose of this article is to develop a novel tensor-based software-defined networking model on a cloud data center for multimedia big-data computation and communication. First, an overview of the proposed framework is provided, in which the functions of the representative modules are briefly illustrated. Then, three models,-forwarding tensor, control tensor, and transition tensor-are proposed for management of networking devices and prediction of network traffic patterns. Finally, two algorithms about single-mode and multimode tensor eigen-decomposition are developed, and the incremental method is employed for efficiently updating the generated eigen-vector and eigen-tensor. Experimental results reveal that the proposed framework is feasible and efficient to handle multimedia big data on a cloud data center.
C1 [Kuang, Liwei; Yang, Laurence T.; Qiu, Kai] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Room 1548,Unit 1,Donghu Sq,Luoyu Rd 1077, Wuhan, Hubei, Peoples R China.
   [Yang, Laurence T.] St Francis Xavier Univ, Dept Comp Sci, Antigonish, NS, Canada.
   [Rho, Seungmin (Charlie)] Sungkyul Univ, Dept Multimedia, 102 Sungkyul Gwan, Anyang Si, South Korea.
   [Yan, Zheng] Aalto Univ, Dept Commun & Networking, Otakaari 5, Espoo 02150, Finland.
C3 Huazhong University of Science & Technology; Saint Francis Xavier
   University - Canada; Sungkyul University; Aalto University
RP Kuang, LW (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Room 1548,Unit 1,Donghu Sq,Luoyu Rd 1077, Wuhan, Hubei, Peoples R China.
EM liweikuang@gmail.com; ltyang@gmail.com; smrho@sungkyul.edu;
   zheng.yan@aalto.fi; qk.elvis18@gmail.com
RI zheng, yan/GQY-6668-2022; yang, zheng/HGC-7753-2022; Laurence T. Yang,
   FCAE/AAA-1898-2019; Yan, Zheng/AEV-7247-2022
OI Laurence T. Yang, FCAE/0000-0002-7986-4244; Yan,
   Zheng/0000-0002-9697-2108
CR [Anonymous], 2012, MATRIX COMPUTATIONS
   [Anonymous], 2006, KDD
   Aversa R., 2014, INT J BIG DATA INTEL, V1, P3, DOI DOI 10.1504/IJBDI.2014.063840
   Benson T., 2010, P 10 ACM SIGCOMM C I, P267, DOI DOI 10.1145/1879141.1879175
   Bickenbach F, 2003, INT REGIONAL SCI REV, V26, P363, DOI 10.1177/0160017603253789
   Booth TE, 2006, NUCL SCI ENG, V154, P48, DOI 10.13182/NSE05-05
   Brand Matthew, EUR C COMP VIS ECCV, P707
   Brazell M, 2013, SIAM J MATRIX ANAL A, V34, P542, DOI 10.1137/100804577
   Ching Wai-Ki., 2013, Markov Chains, P141, DOI DOI 10.1007/978-1-4614-6312-2_6
   Chunsheng Zhu, 2013, 2013 IEEE International Conference on Green Computing and Communications (GreenCom) and IEEE Internet of Things (iThings) and IEEE Cyber, Physical and Social Computing (CPSCom), P769, DOI 10.1109/GreenCom-iThings-CPSCom.2013.138
   Cichocki A, 2014, ARXIV14032048V4
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995
   Demmel JW, 1997, APPL NUMERICAL LINEA
   Desikan Prasanna., 2005, Special interest tracks and 114 posters of the 14th international conference on World Wide Web, P1094, DOI DOI 10.1145/1062745.1062885
   Dong MX, 2015, IEEE NETWORK, V29, P40, DOI 10.1109/MNET.2015.7166189
   Ethier Stewart N., 1986, BIOMETRICS, V43
   Gleich D. F., 2014, ARXIV14091465
   Gude N, 2008, ACM SIGCOMM COMP COM, V38, P105, DOI 10.1145/1384609.1384625
   Kim M., 2014, THESIS ARIZONA STATE
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kuang LW, 2014, IEEE T EMERG TOP COM, V2, P280, DOI 10.1109/TETC.2014.2330516
   Lantz B., 2010, Proceedings of the 9th ACM SIGCOMM Workshop on Hot Topics in Networks, P19, DOI 10.1145/1868447.1868466
   Li H, 2016, IEEE T EMERG TOP COM, V4, P266, DOI 10.1109/TETC.2016.2517930
   Li H, 2015, IEEE CLOUD COMPUT, V2, P42, DOI 10.1109/MCC.2015.114
   Li W, 2014, LINEAR MULTILINEAR A, V62, P362, DOI 10.1080/03081087.2013.777436
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Ning HZ, 2010, PATTERN RECOGN, V43, P113, DOI 10.1016/j.patcog.2009.06.001
   Parlett B.N., 1980, The symmetric eigenvalue problem, V7
   Savas B, 2007, PATTERN RECOGN, V40, P993, DOI 10.1016/j.patcog.2006.08.004
   Wetzker R., 2010, WSDM 10, P71, DOI 10.1145/1718487.1718497.
   Xia WF, 2015, IEEE COMMUN SURV TUT, V17, P27, DOI 10.1109/COMST.2014.2330903
   Yan S., 2005, IEEE INT C IM PROC I, V1, P1
   Zhang Q., 2015, EURASIP J WIREL COMM, V2015, P1, DOI DOI 10.1016/J.GL0PLACHA.2015.03.001
   Zhang Q, 2010, J INTERNET SERV APPL, V1, P7, DOI 10.1007/s13174-010-0007-6
   Zhang QC, 2016, IEEE T COMPUT, V65, P1351, DOI 10.1109/TC.2015.2470255
NR 36
TC 8
Z9 8
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2016
VL 12
IS 5
SU S
AR 74
DI 10.1145/2983640
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EJ0VT
UT WOS:000392929700005
DA 2024-07-18
ER

PT J
AU Hu, XJ
   Zhang, WM
   Li, K
   Hu, HG
   Yu, NH
AF Hu, Xianjun
   Zhang, Weiming
   Li, Ke
   Hu, Honggang
   Yu, Nenghai
TI Secure Nonlocal Denoising in Outsourced Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Image denoising; Johnson-Lindenstrauss transform; nonlocal
   means; Paillier homomorphic encryption
AB Signal processing in the encrypted domain becomes a desired technique to protect privacy of outsourced data in cloud. In this article, we propose a double-cipher scheme to implement nonlocal means (NLM) denoising in encrypted images. In this scheme, one ciphertext is generated by the Paillier scheme, which enables the mean filter, and the other is obtained by a privacy-preserving transform, which enables the nonlocal search. By the privacy-preserving transform, the cloud server can search the similar pixel blocks in the ciphertexts with the same speed as in the plaintexts; thus, the proposed method can be executed fast. To enhance the security, we randomly permutate both ciphertexts. To reduce the denoising complexity caused by random permutation, a random NLM method is exploited in the encrypted domain. The experimental results show that the quality of denoised images in the encrypted domain is comparable to that obtained in the plain domain.
C1 [Hu, Xianjun; Zhang, Weiming; Li, Ke; Hu, Honggang; Yu, Nenghai] Univ Sci & Technol China, Hefei, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Hu, XJ (corresponding author), Univ Sci & Technol China, Hefei, Peoples R China.
EM hxj2012@mail.ustc.edu.cn; zhangwm@ustc.edu.cn; lee0525@mail.ustc.edu.cn;
   hghu2005@ustc.edu.cn; ynh@ustc.edu.cn
OI Hu, Xianjun/0000-0002-2784-2588
FU National Natural Science Foundation of China [61572452, 61170234,
   61271271, 61522210]; Strategic Priority Research Program through Chinese
   Academy of Sciences [XDA06030601]; 100 Talents Program of Chinese
   Academy of Sciences; Fundamental Research Funds for the Central
   Universities in China [WK2101020005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61572452 and Grant 61170234, in part by
   the Strategic Priority Research Program through the Chinese Academy of
   Sciences under Grant XDA06030601, in part by the National Natural
   Science Foundation of China (61271271, 61522210), 100 Talents Program of
   Chinese Academy of Sciences, and the Fundamental Research Funds for the
   Central Universities in China (WK2101020005).
CR Aguilar-Melchor C, 2013, IEEE SIGNAL PROC MAG, V30, P108, DOI 10.1109/MSP.2012.2230219
   [Anonymous], 2009, FULLY HOMOMORPHIC EN, DOI 10.1145/1536414.1536440
   [Anonymous], 2009, NIST WORKIN IN PRESS
   [Anonymous], 2012, ARXIV12042606
   [Anonymous], 2014, ABSTR APPL AN
   [Anonymous], 2007, Introduction to Modern Cryptography: Principlesand Protocols
   [Anonymous], 1978, FDN SEC COMPUT
   [Anonymous], 2011, P 19 ACM INT C MULTI
   BENALOH JC, 1987, LECT NOTES COMPUT SC, V263, P251
   Bianchi T., 2009, EURASIP J. Inf. Security, DOI DOI 10.1155/2009/716357
   Bianchi T, 2010, IEEE T INF FOREN SEC, V5, P180, DOI 10.1109/TIFS.2009.2036230
   Bianchi T, 2009, IEEE T INF FOREN SEC, V4, P86, DOI 10.1109/TIFS.2008.2011087
   Blakley G. R., 1979, 1979 International Workshop on Managing Requirements Knowledge (MARK), P313, DOI 10.1109/MARK.1979.8817296
   Brakerski Zvika, 2014, ACM Transactions on Computation Theory, V6, DOI 10.1145/2633600
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Buades Antoni, 2004, 5 CMLA
   Chan SH, 2014, IEEE T IMAGE PROCESS, V23, P3711, DOI 10.1109/TIP.2014.2327813
   Cramer Ronald, 2001, MULTIPARTY COMPUTATI
   Damgård I, 2001, LECT NOTES COMPUT SC, V1992, P119
   Deng RH, 2014, MULTIMEDIA SYST, V20, P165, DOI 10.1007/s00530-013-0326-0
   Ding C., 1996, Chinese Remainder Theorem: Applications in Computing, Coding, Cryptography, V2nd ed.
   ELGAMAL T, 1985, IEEE T INFORM THEORY, V31, P469, DOI 10.1109/TIT.1985.1057074
   GOLDWASSER S, 1984, J COMPUT SYST SCI, V28, P270, DOI 10.1016/0022-0000(84)90070-9
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Johnson W.B., 1984, C MODERN ANAL PROBAB, V26
   Jost Christine, 2015, ENCRYPTION PERFORMAN
   Lagendijk RL, 2013, IEEE SIGNAL PROC MAG, V30, P82, DOI 10.1109/MSP.2012.2219653
   Lathey A, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2656205
   Lathey A, 2013, IEEE INT C SEMANT CO, P310, DOI 10.1109/ICSC.2013.60
   Micali S., 1987, ACM S THEOR COMP, P218, DOI [10.1145/28395.28420, DOI 10.1145/28395.28420]
   Orlandi C, 2011, INT CONF ACOUST SPEE, P5848
   Paillier P, 1999, LECT NOTES COMPUT SC, V1592, P223
   Peijia Zheng, 2013, Information Hiding. 14th International Conference, IH 2012 Revised Selected Papers, P240, DOI 10.1007/978-3-642-36373-3_16
   Ren K, 2012, IEEE INTERNET COMPUT, V16, P69, DOI 10.1109/MIC.2012.14
   SaghaianNejadEsfahani SM, 2012, IEEE IMAGE PROC, P253, DOI 10.1109/ICIP.2012.6466843
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Xianjun Hu, 2014, Internet of Vehicles - Technologies and Services. First International Conference (IOV). Proceedings: LNCS 8662, P386, DOI 10.1007/978-3-319-11167-4_38
   Yao A. C., 1982, 23rd Annual Symposium on Foundations of Computer Science, P160, DOI 10.1109/SFCS.1982.38
   Zheng PJ, 2013, IEEE T IMAGE PROCESS, V22, P2455, DOI 10.1109/TIP.2013.2253474
NR 39
TC 17
Z9 19
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2016
VL 12
IS 3
AR 40
DI 10.1145/2886777
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YO
UT WOS:000379425400006
DA 2024-07-18
ER

PT J
AU Merani, ML
   Natali, L
AF Merani, Maria Luisa
   Natali, Laura
TI Adaptive Streaming in P2P Live Video Systems: A Distributed Rate Control
   Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE DASH; peer-to-peer video streaming; flash-crowd; integer linear
   programming
AB Dynamic Adaptive Streaming over HTTP (DASH) is a recently proposed standard that offers different versions of the same media content to adapt the delivery process over the Internet to dynamic bandwidth fluctuations and different user device capabilities. The peer-to-peer (P2P) paradigm for video streaming allows us to leverage the cooperation among peers, guaranteeing the service of video requests with increased scalability and reduced cost. We propose to combine these two approaches in a P2P-DASH architecture, exploiting the potentiality of both. The new platform is made of several swarms and a different DASH representation is streamed within each of them; unlike client-server DASH architectures, where each client autonomously selects which version to download according to current network conditions and to its device resources, we put forth a new rate control strategy implemented at peer site to maintain a good viewing quality to the local user and to simultaneously guarantee the successful operation of the P2P swarms. The effectiveness of the solution is demonstrated through simulation and it indicates that the P2P-DASH platform is able to provide its users with very good performance, much more satisfying than in a conventional P2P environment where DASH is not employed. Through a comparison with a reference DASH system modeled via the Integer Linear Programming (ILP) approach, the new system is shown to outperform such reference architecture. To further validate the proposal, in terms of both robustness and scalability, system behavior is investigated in the critical condition of a flash crowd, showing that the strong upsurge of new users can be successfully revealed and gradually accommodated.
C1 [Merani, Maria Luisa; Natali, Laura] Univ Modena & Reggio Emilia, Dipartimento Ingn Enzo Ferrari, Modena, Italy.
C3 Universita di Modena e Reggio Emilia
RP Merani, ML (corresponding author), Univ Modena & Reggio Emilia, Dipartimento Ingn Enzo Ferrari, Modena, Italy.
EM marialuisa.merani@unimore.it; laura.natali@unimore.it
CR Akamai, 2014, STAT INT Q2 2014
   Babaoglu O, 2014, IEEE SPECTRUM, V51, P50, DOI 10.1109/MSPEC.2014.6905491
   Cisco WP, 2015, CISCO WP WHITE PAPER
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   European Commission, 2012, DIG AG EUR SCOR 2012
   Klusch M., 2014, INT C MOBILE UBIQUIT, P277
   Lederer S., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P161, DOI 10.1109/PV.2012.6229730
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liu YT, 2013, IEEE INT CONF COMM, P682, DOI 10.1109/ICCW.2013.6649320
   Microsoft, 2015, SMOOTH STREAM
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   Net T., 2014, T NET CONN VOC
   Orange, 2014, OR OFFR INT
   Shen JG, 2013, IEEE INT SYMP CIRC S, P1, DOI 10.1109/ISCAS.2013.6571767
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   SUH D, 2014, P 2014 INT C INF NET, P497
   Swisscom, 2014, SWISSC INT OFF
   Wu D, 2009, IEEE INFOCOM SER, P2726, DOI 10.1109/INFCOM.2009.5062220
   Zambelli Alex, 2009, TECHNICAL REPORT
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
   Zhang X., 2014, IEEE T CIRC SYST VID, V24, P4
   Zink M, 2003, LECT NOTES COMPUT SC, V2707, P137
NR 22
TC 7
Z9 8
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2016
VL 12
IS 3
AR 46
DI 10.1145/2912123
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YO
UT WOS:000379425400012
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Miao, D
   Fu, JJ
   Lu, Y
   Li, SP
   Chen, CW
AF Miao, Dan
   Fu, Jingjing
   Lu, Yan
   Li, Shipeng
   Chen, Chang Wen
TI A High-Fidelity and Low-Interaction-Delay Screen Sharing System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Interaction delay; video quality; bandwidth consumption; layered
   structure; screen coding; screen rendering
ID MOTION DETECTION; SCHEME
AB The pervasive computing environment and wide network bandwidth provide users more opportunities to share screen content among multiple devices. In this article, we introduce a remote display system to enable screen sharing among multiple devices with high fidelity and responsive interaction. In the developed system, the frame-level screen content is compressed and transmitted to the client side for screen sharing, and the instant control inputs are simultaneously transmitted to the server side for interaction. Even if the screen responds immediately to the control messages and updates at a high frame rate on the server side, it is difficult to update the screen content with low delay and high frame rate in the client side due to non-negligible time consumption on the whole screen frame compression, transmission, and display buffer updating. To address this critical problem, we propose a layered structure for screen coding and rendering to deliver diverse screen content to the client side with an adaptive frame rate. More specifically, the interaction content with small region screen update is compressed by a blockwise screen codec and rendered at a high frame rate to achieve smooth interaction, while the natural video screen content is compressed by standard video codec and rendered at a regular frame rate for a smooth video display. Experimental results with real applications demonstrate that the proposed system can successfully reduce transmission bandwidth cost and interaction delay during screen sharing. Especially for user interaction in small regions, the proposed system can achieve a higher frame rate than most previous counterparts.
C1 [Miao, Dan] Univ Sci & Technol China, Dept Elect Engn, Hefei, Anhui, Peoples R China.
   [Fu, Jingjing; Lu, Yan; Li, Shipeng] Microsoft Res Asia, Media Comp Grp, Beijing, Peoples R China.
   [Chen, Chang Wen] SUNY Buffalo, Dept Comp Sci, Buffalo, NY USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft; State University of New
   York (SUNY) System; State University of New York (SUNY) Buffalo
RP Miao, D (corresponding author), Univ Sci & Technol China, Dept Elect Engn, Hefei, Anhui, Peoples R China.
EM miaodan@mail.ustc.edu.cn; jifu@microsoft.com; yanlu@microsoft.com;
   spli@microsoft.com; chencw@buffalo.edu
RI Li, Shipeng/AAA-3374-2020; fu, jingjing/HLW-1028-2023
OI Li, Shipeng/0000-0001-5368-4256; Chen, Chang Wen/0000-0002-6720-234X
CR [Anonymous], 2012, P 11 ANN WORKSH NETW
   [Anonymous], TECHNICAL REPORT
   AnyDesk, 2016, REM DESKT SOFTW BENC
   Baratto RicardoA., 2005, Proceedings of the 20th ACM Symposium on Operating Systems Principles, SOSP '05, P277, DOI DOI 10.1145/1095810.1095837
   Bottou L, 1998, J ELECTRON IMAGING, V7, P410, DOI 10.1117/1.482609
   Chandra S., 2012, P 20 ACM INT C MULT, P389
   Chang Yu-Chun., 2011, IEEE INT WORKSHOP TE, P1, DOI [10.1109/CQR.2011.5996092, DOI 10.1109/ICME.2011.6012177]
   Chen K.-T., 2011, P 19 ACM INT C MULT, P1269
   Chen KT, 2014, IEEE T MULTIMEDIA, V16, P480, DOI 10.1109/TMM.2013.2291532
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   Christiansen BO, 2002, IEEE DATA COMPR CONF, P332, DOI 10.1109/DCC.2002.999971
   Deboosere Lien., 2007, International Conference on Networking and Services (ICNS'07), P38
   GamingAnywhere, 2016, GAMINGANYWHERE OP CL
   Hsu Chih-Fan, 2015, P ACM SIGMM C MULT S, P177
   Huang Chun-Ying, 2013, P 4 ACM MULT SYST C, P36, DOI DOI 10.1145/2483977.2483981
   Jarschel M., 2011, Proceedings of the 2011 Fifth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), P330, DOI 10.1109/IMIS.2011.92
   Lu Y, 2011, IEEE MULTIMEDIA, V18, P4, DOI 10.1109/MMUL.2011.33
   Miao D., 2013, P IEEE INT C MULT EX, P1, DOI DOI 10.1109/EVER.2013.6521550
   Miao D, 2014, IEEE INT SYMP CIRC S, P2157, DOI 10.1109/ISCAS.2014.6865595
   Microsoft, 2016, REM DESKT PROT RDP
   Microsoft, 2016, US PERF MON
   Pan ZT, 2013, IEEE T CIRC SYST VID, V23, P949, DOI 10.1109/TCSVT.2013.2243056
   Richardson T, 1998, IEEE INTERNET COMPUT, V2, P33, DOI 10.1109/4236.656066
   SCHEIFLER RW, 1986, ACM T GRAPHIC, V5, P79, DOI 10.1145/22949.24053
   Simoens P, 2008, ATNAC: 2008 AUSTRALASIAN TELECOMMUNICATION NETWOKS AND APPLICATIONS CONFERENCE, P391, DOI 10.1109/ATNAC.2008.4783356
   Tan KJ, 2010, IEEE INT CON MULTI, P992, DOI 10.1109/ICME.2010.5582993
   Wang SQ, 2012, IEEE INT SYMP CIRC S, P145
   Wenpeng Ding, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P337
   Wu JY, 2015, IEEE T CIRC SYST VID, V25, P1988, DOI 10.1109/TCSVT.2015.2441412
   Wu JY, 2015, IEEE T MOBILE COMPUT, V14, P688, DOI 10.1109/TMC.2014.2334592
   Zhang T, 2013, IEEE IMAGE PROC, P1943, DOI 10.1109/ICIP.2013.6738400
   Zhao L, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND INTELLIGENT SYSTEMS, PROCEEDINGS, VOL 3, P43, DOI 10.1109/ICICISYS.2009.5358230
NR 32
TC 7
Z9 9
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2016
VL 12
IS 3
AR 44
DI 10.1145/2897395
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YO
UT WOS:000379425400010
DA 2024-07-18
ER

PT J
AU El Essaili, A
   Wang, ZB
   Steinbach, E
   Zhou, L
AF El Essaili, Ali
   Wang, Zibin
   Steinbach, Eckehard
   Zhou, Liang
TI QoE-Based Cross-Layer Optimization for Uplink Video Transmission
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; Quality of Experience; live and VoD; LTE uplink;
   scalable video transmission; resource allocation
ID SCALABLE VIDEO; RESOURCE-ALLOCATION; CONGESTION CONTROL; WIRELESS;
   NETWORKS; DESIGN; DELAY
AB We study the problem of resource-efficient uplink distribution of user-generated video content over fourth-generation mobile networks. This is challenged by (1) the capacity-limited and time-variant uplink channel, (2) the resource-hungry upstreamed videos and their dynamically changing complexity, and (3) the different playout times of the video consumers. To address these issues, we propose a systematic approach for quality-of-experience (QoE)-based resource optimization and uplink transmission of multiuser generated video content. More specifically, we present an analytical model for distributed scalable video transmission at the mobile producers which considers these constraints. This is complemented by a multiuser cross-layer optimizer in the mobile network which determines the transmission capacity for each mobile terminal under current cell load and radio conditions. Both optimal and low-complexity solutions are presented. Simulation results for LTE uplink transmission show that significant gains in perceived video quality can be achieved by our cross-layer resource optimization scheme. In addition, the distributed optimization at the mobile producers can further improve the user experience across the different types of video consumers.
C1 [El Essaili, Ali; Wang, Zibin; Steinbach, Eckehard] Tech Univ Munich, Inst Media Technol, D-80290 Munich, Germany.
   [Zhou, Liang] Nanjing Univ Posts & Telecommun, Minist Educ, Nanjing, Jiangsu, Peoples R China.
C3 Technical University of Munich; Nanjing University of Posts &
   Telecommunications
RP El Essaili, A (corresponding author), Tech Univ Munich, Inst Media Technol, D-80290 Munich, Germany.
EM aessaili@gmail.com
CR Akamai Technologies, 2015, AK INTR PRED VID CEL
   [Anonymous], 2014, Cisco Visual Networking Index. Global Mobile Data Traffic Forecast Update
   [Anonymous], 2010, document TR 36
   [Anonymous], 29214 3GPP TS
   [Anonymous], 23705 3GPP TR
   [Anonymous], 29213 3GPP TS
   [Anonymous], 2008, 36942 3GPP TR
   Barry RA, 2004, IEEE SIGNAL PROC MAG, V21, P59, DOI 10.1109/MSP.2004.1328089
   Begen AC, 2011, IEEE INTERNET COMPUT, V15, P59, DOI 10.1109/MIC.2010.156
   Bertsekas D. P., 1999, Nonlinear Program, V2nd
   Boccardi F, 2014, IEEE COMMUN MAG, V52, P74, DOI 10.1109/MCOM.2014.6736746
   Boyd S., 2004, CONVEX OPTIMIZATION
   Capozzi F, 2013, IEEE COMMUN SURV TUT, V15, P678, DOI 10.1109/SURV.2012.060912.00100
   Chakareski J, 2006, IEEE T MULTIMEDIA, V8, P207, DOI 10.1109/TMM.2005.864284
   Chiang M, 2005, IEEE J SEL AREA COMM, V23, P104, DOI 10.1109/JSAC.2004.837347
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Cicalò S, 2014, IEEE T MULTIMEDIA, V16, P848, DOI 10.1109/TMM.2014.2300442
   Dimitrova DC, 2011, LECT NOTES COMPUT SC, V6886, P181, DOI 10.1007/978-3-642-23795-9_16
   Dosselmann R, 2011, SIGNAL IMAGE VIDEO P, V5, P81, DOI 10.1007/s11760-009-0144-1
   Dua A, 2010, IEEE T WIREL COMMUN, V9, P1001, DOI 10.1109/TWC.2010.03.070120
   El Essaili A., 2011, P IEEE INT C IM PROC
   El Essaili A, 2011, IEEE INT WORKSH MULT
   Ellenbeck J., 2012, IMTAPHY SOURCE CODE
   Gabin F, 2010, IEEE SIGNAL PROC MAG, V27, P134, DOI 10.1109/MSP.2010.938087
   Georgiadis L, 1997, IEEE T INFORM THEORY, V43, P1518, DOI 10.1109/18.623149
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Hamzaoui R, 2005, IEEE SIGNAL PROC MAG, V22, P91, DOI 10.1109/MSP.2005.1550192
   He JY, 2007, IEEE J SEL AREA COMM, V25, P868, DOI 10.1109/JSAC.2007.070602
   Ji X, 2009, IEEE T CIRC SYST VID, V19, P1549, DOI 10.1109/TCSVT.2009.2026812
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P1227, DOI 10.1109/TMM.2007.902852
   Khalek AA, 2012, IEEE J SEL AREA COMM, V30, P1157, DOI 10.1109/JSAC.2012.120802
   Khan S, 2006, IEEE COMMUN MAG, V44, P122, DOI 10.1109/MCOM.2006.1580942
   Khan S, 2007, ADV MULTIMED, V2007, DOI 10.1155/2007/94918
   Lin HL, 2012, IEEE COMMUN LETT, V16, P1349, DOI 10.1109/LCOMM.2012.070512.120760
   Oyman O, 2010, IEEE COMMUN MAG, V48, P68, DOI 10.1109/MCOM.2010.5534589
   Piro G, 2011, IEEE T MULTIMEDIA, V13, P1052, DOI 10.1109/TMM.2011.2152381
   SAUL A, 2007, P IEEE INT C COMM IC
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shakkottai S, 2002, WIREL NETW, V8, P13, DOI 10.1023/A:1012763307361
   SINGER D, 2008, 5285 RFC
   Thakolsri Srisakul, 2009, Journal of Communications, V4, P669, DOI 10.4304/jcm.4.9.669-680
   Thakolsri S, 2011, IEEE ICC
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   van der Schaar M, 2007, IEEE T MULTIMEDIA, V9, P185, DOI 10.1109/TMM.2006.886384
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Zhang HH, 2010, IEEE J SEL AREA COMM, V28, P344, DOI 10.1109/JSAC.2010.100406
NR 46
TC 3
Z9 3
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2015
VL 12
IS 1
AR 2
DI 10.1145/2801124
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CP9LG
UT WOS:000360215200002
DA 2024-07-18
ER

PT J
AU Song, M
   Lee, Y
   Park, J
AF Song, Minseok
   Lee, Yeongju
   Park, Jinhan
TI Scheduling a Video Transcoding Server to Save Energy
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Multimedia systems; low-power
   systems; dynamic voltage and frequency scaling
ID PROXY; STORAGE; AWARE
AB Recent popular streaming services such as TV Everywhere, N-Screen, and dynamic adaptive streaming over HTTP (DASH) need to deliver content to the wide range of devices, requiring video content to be transcoded into different versions. Transcoding tasks require a lot of computation, and each task typically has its own real-time constraint. These make it difficult to manage transcoding, but the more efficient use of energy in servers is an imperative. We characterize transcoding workloads in terms of deadlines and computation times, and propose a new dynamic voltage and frequency scaling (DVFS) scheme that allocates a frequency and a workload to each CPU with the aim of minimizing power consumption while meeting all transcoding deadlines. This scheme has been simulated, and also implemented in a Linux transcoding server, in which a frontend node distributes transcoding requests to heterogeneous backend nodes. This required a new protocol for communication between nodes, a DVFS management scheme to reduce power consumption and thread management and scheduling schemes which ensure that transcoding deadlines are met. Power measurements show that this approach can reduce system-wide energy consumption by 17% to 31%, compared with the Linux Ondemand governor.
C1 [Song, Minseok; Lee, Yeongju; Park, Jinhan] Inha Univ, Sch Comp Sci & Informat Engn, Inchon, South Korea.
C3 Inha University
RP Song, M (corresponding author), Inha Univ, Sch Comp Sci & Informat Engn, Inchon, South Korea.
EM mssong@inha.ac.kr
FU Development of Power Efficient High-Performance Multimedia Contents
   Service Technology using Context-Adapting Distributed Transcoding
   [10041971]; Inha University [200352423]; Ministry of Knowledge Economy
   (MKE, Korea), ICT R&D program of MSIP/IITP
FX This work was supported in part by the industrial strategic technology
   development program (10041971, Development of Power Efficient
   High-Performance Multimedia Contents Service Technology using
   Context-Adapting Distributed Transcoding) funded by the Ministry of
   Knowledge Economy (MKE, Korea), in part by the ICT R&D program of
   MSIP/IITP. [200352423, Component based Design Theory and Control Kernel
   for CPS (Cyber-Physical System)], and in part by Inha University.
CR [Anonymous], 2014, ONLINE13 POWER CALCU
   [Anonymous], 2013, CISCO VISUAL NETWORK
   [Anonymous], 2003, P IEEE PDPS APR
   Ashraf A, 2013, IEEE ACM INT SYMP, P482, DOI 10.1109/CCGrid.2013.21
   Bertini L, 2010, J SYST SOFTWARE, V83, P585, DOI 10.1016/j.jss.2009.10.040
   Bovet D., 2005, UNDERSTANDING THE LI
   Chen JJ, 2007, 13TH IEEE INTERNATIONAL CONFERENCE ON EMBEDDED AND REAL-TIME COMPUTING SYSTEMS AND APPLICATIONS, PROCEEDINGS, P28, DOI 10.1109/RTCSA.2007.37
   Chen Jian-Jia., 2005, Proceedings of the 2nd International Workshop on Power-Aware Real-Time Computing (PARC'05), P30
   Dan A, 1996, MULTIMEDIA SYST, V4, P112, DOI 10.1007/s005300050016
   Digalwar Mayuri, 2013, 2013 International Conference on Advanced Electronic Systems (ICAES), P325, DOI 10.1109/ICAES.2013.6659421
   Garcia A., 2010, Proc. ACM Multimedia Workshop on Mobile Cloud Media Computing (MCMC), P13, DOI DOI 10.1145/1877953.1877959
   Horvath T, 2007, IEEE T COMPUT, V56, P444, DOI 10.1109/tc.2007.1003
   Horvath T, 2008, PACT'08: PROCEEDINGS OF THE SEVENTEENTH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P270, DOI 10.1145/1454115.1454153
   Hsiao JL, 2008, IEEE T MULTIMEDIA, V10, P646, DOI 10.1109/TMM.2008.921852
   Huang JL, 2007, IEEE T MOBILE COMPUT, V6, P971, DOI 10.1109/TMC.2007.1038
   Hung HP, 2009, MULTIMEDIA SYST, V15, P49, DOI 10.1007/s00530-008-0143-z
   Jokhio Fareed, 2013, 2013 39th Euromicro Conference on Software Engineering and Advanced Applications (SEAA), P365, DOI 10.1109/SEAA.2013.17
   Kim M, 2012, IEEE T CIRC SYST VID, V22, P567, DOI 10.1109/TCSVT.2011.2170112
   Ko S., 2013, P 28 ANN ACM S APPL, P1610, DOI DOI 10.1145/2480362.2480663
   Kolpe T., 2011, DESIGN AUTOMATION TE, P1, DOI DOI 10.1109/DATE.2011.5763052
   Li Z., 2012, Proc. of ACM Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV), P33, DOI DOI 10.1145/2229087.2229097
   Lim G., 2012, P LIN S, P25
   Liu D., 2012, ACM TRANS MULTIMED C, V5, P333
   Liu D., 2006, PROCEEDINGS OF THE A
   Ma H., 2014, PROCEEDINGS OF THE A, P227
   Pallipadi V., 2006, Proceedings of the Linux Symposium, V2, P215
   Pillai P., 2001, Operating Systems Review, V35, P89, DOI 10.1145/502059.502044
   Pisinger D., 1995, THESIS
   Qu WY, 2007, COMPUT COMMUN, V30, P1802, DOI 10.1016/j.comcom.2007.02.012
   Rusu C, 2006, PROCEEDINGS OF THE 12TH IEEE REAL-TIME AND EMBEDDED TECHNOLOGY AND APPLICATIONS SYMPOSIUM, P418
   Santana C, 2011, CLUSTER COMPUT, V14, P471, DOI 10.1007/s10586-011-0187-2
   Seiden SS, 2002, J ACM, V49, P640, DOI 10.1145/585265.585269
   Sharma V, 2003, RTSS 2003: 24TH IEEE INTERNATIONAL REAL-TIME SYSTEMS SYMPOSIUM, PROCEEDINGS, P63, DOI 10.1109/REAL.2003.1253254
   Shen B, 2004, IEEE T MULTIMEDIA, V6, P375, DOI 10.1109/TMM.2003.822791
   Shin I, 2004, IEEE T CONSUM ELECTR, V50, P732, DOI 10.1109/TCE.2004.1309455
   Song M., 2013, P ACM WORKSH NETW OP, P1
   Song M., 2014, PROCEEDINGS OF THE A, P91
   Song M, 2009, ETRI J, V31, P333, DOI 10.4218/etrij.09.0208.0405
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Tang XY, 2002, PROC INT CONF PARAL, P287, DOI 10.1109/ICPP.2002.1040884
   Tseng P.-H., 2014, P IEEE ACM DAC, P1, DOI DOI 10.1109/VTCSPRING.2014.7023085
   Xian C., 2007, PROCEEDINGS OF THE A, P264
   Yibei Ling, 2000, Operating Systems Review, V34, P42, DOI 10.1145/346152.346320
   Zhang WW, 2014, IEEE T VEH TECHNOL, V63, P2002, DOI 10.1109/TVT.2014.2310394
   Zhu Q, 2005, P 20 ACM S OP SYST P, P177, DOI DOI 10.1145/1095809.1095828
   Zhu QB, 2005, IEEE T COMPUT, V54, P587
NR 46
TC 16
Z9 16
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2015
VL 11
IS 2
SU S
SI SI
AR 45
DI 10.1145/2700282
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC7RV
UT WOS:000350567000005
DA 2024-07-18
ER

PT J
AU Zhang, HW
   Zha, ZJ
   Yang, Y
   Yan, SC
   Gao, Y
   Chua, TS
AF Zhang, Hanwang
   Zha, Zheng-Jun
   Yang, Yang
   Yan, Shuicheng
   Gao, Yue
   Chua, Tat-Seng
TI Attribute-Augmented Semantic Hierarchy: Towards a Unified Framework for
   Content-Based Image Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Image retrieval; attribute;
   semantic hierarchy
ID RELEVANCE FEEDBACK
AB This article presents a novel attribute-augmented semantic hierarchy (A(2)SH) and demonstrates its effectiveness in bridging both the semantic and intention gaps in content-based image retrieval (CBIR). A(2)SH organizes semantic concepts into multiple semantic levels and augments each concept with a set of related attributes. The attributes are used to describe the multiple facets of the concept and act as the intermediate bridge connecting the concept and low-level visual content. An hierarchical semantic similarity function is learned to characterize the semantic similarities among images for retrieval. To better capture user search intent, a hybrid feedback mechanism is developed, which collects hybrid feedback on attributes and images. This feedback is then used to refine the search results based on A(2)SH. We use A(2)SH as a basis to develop a unified content-based image retrieval system. We conduct extensive experiments on a large-scale dataset of over one million Web images. Experimental results show that the proposed A(2)SH can characterize the semantic affinities among images accurately and can shape user search intent quickly, leading to more accurate search results as compared to state-of-the-art CBIR solutions.
C1 [Zhang, Hanwang; Yang, Yang; Yan, Shuicheng; Gao, Yue; Chua, Tat-Seng] Natl Univ Singapore, Singapore 117548, Singapore.
   [Zha, Zheng-Jun] Chinese Acad Sci, Inst Intelligent Machines, Hefei, Peoples R China.
C3 National University of Singapore; Chinese Academy of Sciences; Hefei
   Institutes of Physical Science, CAS
RP Zha, ZJ (corresponding author), Chinese Acad Sci, Inst Intelligent Machines, Hefei, Peoples R China.
EM junzzustc@gmail.com
RI Zha, Zheng-Jun/AAE-8408-2020; Yan, Shuicheng/HCI-1431-2022; Zha,
   Zheng-Jun/AAF-8667-2020
OI Zha, Zheng-Jun/0000-0003-2510-8993; Zhang, Hanwang/0000-0001-7374-8739
FU NUS-Tsinghua Extreme Search (NExT) project [R-252-300-001-490]
FX This work is supported by the NUS-Tsinghua Extreme Search (NExT) project
   under grant #R-252-300-001-490.
CR [Anonymous], P EUR C COMP VIS
   [Anonymous], 2013, P IEEE C COMP VIS PA
   [Anonymous], 2012, P IEEE C COMP VIS PA
   [Anonymous], 2012, P IEEE C COMP VIS PA
   [Anonymous], P ACM INT C MULT
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2009, P IEEE COMP VIS PATT
   [Anonymous], 2010, P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   Baker C. F., 1998, COLING ACL 98, P1, DOI DOI 10.3115/980451.980860
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Binder A, 2012, INT J COMPUT VISION, V99, P281, DOI 10.1007/s11263-010-0417-8
   Boureau Y, 2011, P INT C COMP VIS
   Crucianu M., 2004, DELOS2
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Deng J., 2011, P IEEE C COMP VIS PA
   Deselaers T., 2011, P IEEE COMP VIS PATT
   Douze M., 2011, P IEEE C COMP VIS PA
   Fellbaum C, 2010, THEORY AND APPLICATIONS OF ONTOLOGY: COMPUTER APPLICATIONS, P231, DOI 10.1007/978-90-481-8847-5_10
   GRIFFIN G, 2008, P IEEE C COMP VIS PA
   Jaimes A., 2000, P SPIE, V3964
   Kumar N, 2011, IEEE T PATTERN ANAL, V33, P1962, DOI 10.1109/TPAMI.2011.48
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Ma Z., 2012, P IEEE C COMP VIS PA
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Over P., 2012, P TRECVID C
   Parikh D., 2011, P IEEE C COMP VIS PA
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Russakovsky O., 2010, LECT NOTES COMPUTER, V6553
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Song Y., 2010, P IEEE C COMP VIS PA
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Tong S., 2001, P MULTIMEDIA 01 P 9, DOI [10.1145/500141.500159, DOI 10.1145/500141.500159]
   Verma N., 2012, P IEEE C COMP VIS PA
   Weinberger K. Q., 2006, P 20 ANN C NEUR INF
   YANG C., 2005, P ACM INT C MULT
   Zha Z.-J., 2009, PROCEEDINGS OF THE A
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zha ZJ, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823747
   ZHA ZJ, 2008, P IEEE C COMP VIS PA
   Zhang H., 2012, P ACM INT C MULT
   Zhang Hanwang, 2013, P ACM INT C MULT
   Zhang K, 2009, IEEE T NEURAL NETWOR, V20, P583, DOI 10.1109/TNN.2008.2010620
NR 45
TC 18
Z9 20
U1 2
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2014
VL 11
IS 1
SU S
SI SI
AR 21
DI 10.1145/2637291
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS0RG
UT WOS:000343984800013
DA 2024-07-18
ER

PT J
AU Sang, JT
   Xu, CS
AF Sang, Jitao
   Xu, Changsheng
TI Social Influence Analysis and Application on Multimedia Sharing Websites
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Social relation analysis;
   influence analysis; social media; topic model
AB Social media is becoming popular these days, where users necessarily interact with each other to form social networks. Influence network, as one special case of social network, has been recognized as significantly impacting social activities and user decisions. We emphasize in this article that the inter-user influence is essentially topic-sensitive, as for different tasks users tend to trust different influencers and be influenced most by them. While existing research focuses on global influence modeling and applies to text-based networks, this work investigates the problem of topic-sensitive influence modeling in the multimedia domain.
   According to temporal data justification, we propose a multimodal probabilistic model, considering both users' textual annotation and uploaded visual images. This model is capable of simultaneously extracting user topic distributions and topic-sensitive influence strengths. By identifying the topic-sensitive influencer, we are able to conduct applications, like collective search and collaborative recommendation. A risk minimization-based general framework for personalized image search is further presented, where the image search task is transferred to measure the distance of image and personalized query language models. The framework considers the noisy tag issue and enables easy incorporation of social influence. We have conducted experiments on a large-scale Flickr dataset. Qualitative as well as quantitative evaluation results have validated the effectiveness of the topic-sensitive influencer mining model, and demonstrated the advantage of incorporating topic-sensitive influence in personalized image search and topic-based image recommendation.
C1 [Sang, Jitao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   China Singapore Inst Digital Media, Singapore, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009]; Beijing Natural Science
   Foundation [4131004]; Singapore National Research Foundation under its
   International Research Centre @ Singapore Funding Initiative
FX This work is supported in part by the National Basic Research Program of
   China (No. 2012CB316304), the National Natural Science Foundation of
   China (No. 61225009), and the Beijing Natural Science Foundation (No.
   4131004). This work is also supported by the Singapore National Research
   Foundation under its International Research Centre @ Singapore Funding
   Initiative and administered by the IDM Programme Office.
CR Anagnostopoulos A., 2008, P 14 ACM SIGKDD INT, P7, DOI 10.1145/1401890.1401897
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], 1992, MANY ITERATIONS GIBB
   [Anonymous], 2011, PROC WWW
   [Anonymous], 2005, P 2005 ACM CIKM INT, DOI DOI 10.1145/1099554
   [Anonymous], 2012, P 20 ACM INT C MULTI
   ARTHUR D., 2009, ABS09023485 CORR
   Bender Matthias, 2008, 2008 IEEE 24th International Conference on Data Engineering Workshop (ICDE Workshop), P501, DOI 10.1109/ICDEW.2008.4498369
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Carmel D., 2009, P CIKM, P1227
   Cha M., 2010, P INT C WEBL SOC MED
   Chen W., 2010, ACM SIGKDD INT C KNO, P1029, DOI DOI 10.1145/1835804.1835934
   Chirita P.-A., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P178, DOI 10.1145/1076034.1076067
   Chua Tat-Seng, 2009, CIVR
   Cialdini RB, 2009, Influence: The psychology of persuasion
   Crandall DavidJ., 2008, KDD, P160, DOI DOI 10.1145/1401890.1401914
   Dunbar R., 2010, How many friends does one person need? Dunbars number and other evolutionary quirks
   Easley D., 2010, Networks, Crowds, and Markets: Reasoning about a highly connected world, V8
   GARDNER L., 2006, HDB SOCIAL PSYCHOL
   Gomez-Rodriguez M, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086741
   Gou Liang., 2010, P INT C MULTIMEDIA I, P367
   Kempe D, 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Konstas I, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P195, DOI 10.1145/1571941.1571977
   Lafferty J., 2001, SIGIR, P111, DOI DOI 10.1145/383952.383970
   Lane ND, 2010, UBICOMP 2010: PROCEEDINGS OF THE 2010 ACM CONFERENCE ON UBIQUITOUS COMPUTING, P109
   Lerman Kristina, 2009, HDB RES WEB 2 0 3 0
   Liu Lu., 2010, CIKM, DOI DOI 10.1145/1871437.1871467
   Lu DY, 2012, DECIS SUPPORT SYST, V53, P44, DOI 10.1016/j.dss.2011.12.003
   Matsumura N, 2007, INT J KNOWL-BASED IN, V11, P291, DOI 10.3233/KES-2007-11505
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   PANG L., 2011, P C ACM MULT, P1485
   Ponte JM, 1998, P 21 ANN INT ACM SIG, P275, DOI DOI 10.1145/290941.291008
   Qiu F., 2006, Proceedings of the 15th International Conference on World Wide Web, P727, DOI DOI 10.1145/1135777.1135883
   Robert Christian P., 1999, Monte Carlo Statistical Methods, V2
   Rogati M., 2010, Proceedings of the 19th international conference on World wide web, P981, DOI DOI 10.1145/1772690.1772790
   Ronald N., 2010, P MULT LOG LANG ORG
   Sang J., 2012, IEEE T MULTIMEDIA, V14
   Tang J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P807
   Weng J., 2010, P 3 ACM INT C WEB SE, P261, DOI [10.1145/1718487.1718520, DOI 10.1145/1718487.1718520]
NR 39
TC 6
Z9 6
U1 0
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 53
DI 10.1145/2502436
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700023
DA 2024-07-18
ER

PT J
AU Alam, KM
   Rahman, AMM
   El Saddik, A
AF Alam, Kazi Masudul
   Rahman, Abu Saleh Md Mahfujur
   El Saddik, Abdulmotaleb
TI Mobile Haptic E-Book System to Support 3D Immersive Reading in
   Ubiquitous Environments
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Haptic modality; multimodal interface; edutainment
   system; mobile learning
ID REALITY; DESIGN; STILL
AB In order to leverage the use of various modalities such as audio-visual materials in instilling effective learning behavior we present an intuitive approach of annotation based hapto-audio-visual interaction with the traditional digital learning materials such as e-books. By integrating the home entertainment system in the user's reading experience combined with haptic interfaces we want to examine whether such augmentation of modalities influence the user's learning patterns. The proposed Haptic E-Book (HE-Book) system leverages the haptic jacket, haptic arm band as well as haptic sofa interfaces to receive haptic emotive signals wirelessly in the form of patterned vibrations of the actuators and expresses the learning material by incorporating image, video, 3D environment based augmented display in order to pave ways for intimate reading experience in the popular mobile e-book platform.
C1 [Alam, Kazi Masudul; Rahman, Abu Saleh Md Mahfujur; El Saddik, Abdulmotaleb] Univ Ottawa, Multimedia Commun Lab, Ottawa, ON, Canada.
   NYU Abu Dhabi, Abu Dhabi, U Arab Emirates.
C3 University of Ottawa; New York University Abu Dhabi
RP Alam, KM (corresponding author), Univ Ottawa, Multimedia Commun Lab, 800 King Edward Ave, Ottawa, ON, Canada.
EM ffmalam@discover.uottawa.ca; fkafi@mcrlabg.uottawa.ca;
   abed@mcrlabg.uottawa.ca
RI /D-4159-2009
OI /0000-0002-7690-8547; ALAM, KAZI MASUDUL/0000-0003-4731-9847; Rahman, A
   S M Mahfujur/0000-0002-8243-1191
CR Alam K. M., 2011, 2011 IEEE World Haptics Conference (WHC 2011), P367, DOI 10.1109/WHC.2011.5945514
   ALAM K. M., 2011, P IEEE INSTR MEAS TE, P1, DOI DOI 10.1109/IMTC.2011.5944225
   Alam Kazi Masudul, 2011, PROC VRIC2011
   Arafsha F., 2012, 2012 International Conference on Innovations in Information Technology (IIT), P350, DOI 10.1109/INNOVATIONS.2012.6207766
   Barghout A, 2009, 2009 IEEE INTERNATIONAL WORKSHOP ON HAPTIC AUDIO VISUAL ENVIRONMENT AND GAMES, P19, DOI 10.1109/HAVE.2009.5356122
   Billinghurst M, 2001, IEEE COMPUT GRAPH, V21, P6, DOI 10.1109/38.920621
   Brewster S, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P159
   Brünken R, 2003, EDUC PSYCHOL-US, V38, P53, DOI 10.1207/S15326985EP3801_7
   Card StuartK., 2004, CHI EA 04 EXTENDEDAB, P1095
   Carney RN, 2002, EDUC PSYCHOL REV, V14, P5, DOI 10.1023/A:1013176309260
   CARVER RP, 1992, J READING, V36, P84
   Chen N, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1779
   Dale E., 1969, Audiovisual Methods in Teaching, VThird
   Grasset R, 2008, INT SYM MIX AUGMENT, P99, DOI 10.1109/ISMAR.2008.4637333
   Gupta Shilpi, 2006, 2006 IEEE/ACM International Symposium on Mixed and Augmented Reality, P177, DOI 10.1109/ISMAR.2006.297811
   Haans Antal, 2006, Virtual Reality, P149
   Harrison BL, 2000, IEEE COMPUT GRAPH, V20, P32, DOI 10.1109/38.844370
   Hoggan E, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1573
   HONG L., 2005, C HUM FACT COMP SYST, P463
   Kameyama S, 2006, PROC SPIE, V6391, DOI 10.1117/12.685686
   Lemmens P, 2009, WORLD HAPTICS 2009: THIRD JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P7, DOI 10.1109/WHC.2009.4810832
   Likert R., 1932, TECHNIQUE MEASUREMEN, DOI 1933-01885-001
   Mangen A, 2008, J RES READ, V31, P404, DOI 10.1111/j.1467-9817.2008.00380.x
   Park J, 2009, LECT NOTES COMPUT SC, V5709, P234, DOI 10.1007/978-3-642-04052-8_26
   Park T, 2009, LECT NOTES COMPUT SC, V5611, P496, DOI 10.1007/978-3-642-02577-8_54
   Rahal L, 2009, 2009 IEEE INTERNATIONAL WORKSHOP ON ROBOTIC AND SENSORS ENVIRONMENTS (ROSE 2009), P86, DOI 10.1109/ROSE.2009.5355986
   RAHMAN A. S. M. M., 2010, P IEEE INT S MULT
   Rahman AMM, 2011, LECT NOTES ARTIF INT, V6752, P321, DOI 10.1007/978-3-642-21538-4_32
   Schilit BN, 1999, COMPUTER, V32, P65, DOI 10.1109/2.738306
   Taketa N, 2007, LECT NOTES COMPUT SC, V4558, P475
   Toutanova K, 2000, PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA, P63, DOI 10.3115/1117794.1117802
   Welch G, 2005, IEEE MULTIMEDIA, V12, P22, DOI 10.1109/MMUL.2005.48
   Yi SG, 2010, IEEE INT C BIOINFORM, P373, DOI 10.1109/BIBM.2010.5706594
NR 33
TC 13
Z9 14
U1 2
U2 48
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2013
VL 9
IS 4
AR 27
DI 10.1145/2501643.2501649
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 206EW
UT WOS:000323501800005
DA 2024-07-18
ER

PT J
AU Li, GD
   Wang, M
   Lu, Z
   Hong, RC
   Chua, TS
AF Li, Guangda
   Wang, Meng
   Lu, Zheng
   Hong, Richang
   Chua, Tat-Seng
TI In-Video Product Annotation with Web Information Mining
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Product Annotation; video search;
   Web mining
ID SEARCH
AB Product annotation in videos is of great importance for video browsing, search, and advertisement. However, most of the existing automatic video annotation research focuses on the annotation of high-level concepts, such as events, scenes, and object categories. This article presents a novel solution to the annotation of specific products in videos by mining information from the Web. It collects a set of high-quality training data for each product by simultaneously leveraging Amazon and Google image search engine. A visual signature for each product is then built based on the bag-of-visual-words representation of the training images. A correlative sparsification approach is employed to remove noisy bins in the visual signatures. These signatures are used to annotate video frames. We conduct experiments on more than 1,000 videos and the results demonstrate the feasibility and effectiveness of our approach.
C1 [Wang, Meng; Hong, Richang] Hefei Univ Technol, Sch Comp & Informat, Hefei, Peoples R China.
   [Li, Guangda; Lu, Zheng; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 Hefei University of Technology; National University of Singapore
RP Wang, M (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei, Peoples R China.
EM eric.mengwang@gmail.com
RI Wang, Meng/ITR-8699-2023; Lu, Zheng/B-1537-2009
OI LU, Zheng/0000-0002-7799-1776; Lu, Zheng/0000-0003-4098-2486
CR [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P ACM INT C INF KNOW
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P EUR C COMP VIS
   [Anonymous], P INT C COMP VIS
   [Anonymous], 2003, P IEEE INT C COMP VI
   [Anonymous], P ACM INT WORKSH MUL
   Burghouts GJ, 2009, COMPUT VIS IMAGE UND, V113, P48, DOI 10.1016/j.cviu.2008.07.003
   Gao K, 2009, IEEE INT CON MULTI, P322, DOI 10.1109/ICME.2009.5202500
   Gao SH, 2010, PROC CVPR IEEE, P3555, DOI 10.1109/CVPR.2010.5539943
   Geng B., 2008, P 1 ACM INT C MULT I, P443
   Guo JL, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P628, DOI 10.1145/1571941.1572049
   Jing Y., 2008, P INT WORLD WID WEB
   KENNEDY L, 2006, REVISION LSCOM EVENT
   Kleban J., 2008, P IEEE INT C MULT EX
   LIANG F., 2009, P IEEE 12 INT C COMP
   LOWE D. G., 2004, INT J COMPUT VISI, V60
   LUSTIG M., 2007, IEEE J SEL TOP QUANT, V1, P4
   Mei T, 2010, P IEEE, V98, P1416, DOI 10.1109/JPROC.2009.2039841
   MILLER G, 1995, COMM ACM, V38
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   NATSEV A. P., 2007, P ACM INT C MULT
   Romberg S., 2011, ACM INT C MULT RETR
   Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133
   Setz AT, 2009, IEEE INT CON MULTI, P1460, DOI 10.1109/ICME.2009.5202778
   Snoek C., 2009, FOUND TRENDS INF RET, V4
   Tang S., 2008, TRECVID WORKSH
   Ulges A, 2010, COMPUT VIS IMAGE UND, V114, P429, DOI 10.1016/j.cviu.2009.08.002
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   WANG G., 2007, OPTIMOL AUTOMATIC ON
   WANG M., 2008, MSRTR200930
   Wang M, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2333112.2333120
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Xie X, 2008, P IEEE, V96, P589, DOI 10.1109/JPROC.2008.916351
   Zobel J, 1998, ACM T DATABASE SYST, V23, P453, DOI 10.1145/296854.277632
NR 38
TC 15
Z9 17
U1 0
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2012
VL 8
IS 4
AR 55
DI 10.1145/2379790.2379797
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 052QA
UT WOS:000312211900007
DA 2024-07-18
ER

PT J
AU Guan, W
   You, SY
   Newmann, U
AF Guan, Wei
   You, Suya
   Newmann, Ulrich
TI Efficient Matchings and Mobile Augmented Reality
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Theory; Efficient matching; image retrieval; mixed reality; mobile image
   matching
AB With the fast-growing popularity of smart phones in recent years, augmented reality (AR) on mobile devices is gaining more attention and becomes more demanding than ever before. However, the limited processors in mobile devices are not quite promising for AR applications that require real-time processing speed. The challenge exists due to the fact that, while fast features are usually not robust enough in matchings, robust features like SIFT or SURF are not computationally efficient. There is always a tradeoff between robustness and efficiency and it seems that we have to sacrifice one for the other. While this is true for most existing features, researchers have been working on designing new features with both robustness and efficiency. In this article, we are not trying to present a completely new feature. Instead, we propose an efficient matching method for robust features. An adaptive scoring scheme and a more distinctive descriptor are also proposed for performance improvements. Besides, we have developed an outdoor augmented reality system that is based on our proposed methods. The system demonstrates that not only it can achieve robust matchings efficiently, it is also capable to handle large occlusions such as passengers and moving vehicles, which is another challenge for many AR applications.
C1 [Guan, Wei; You, Suya; Newmann, Ulrich] Univ So Calif, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Guan, W (corresponding author), Univ So Calif, PHE 108,3737 Watt Way, Los Angeles, CA 90089 USA.
EM wguan@usc.edu; suyay@graphics.usc.edu; uneumann@graphics.usc.edu
FU Nokia Research Center
FX The authors would like to acknowledge Nokia Research Center for their
   support and test datasets. The authors also thank the anonymous
   reviewers for their valuable comments.
CR [Anonymous], 2009, P INT C COMP VIS ICC
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2008, P INT S MIX AUGM REA
   AZAD P., 2009, P EEE RSJ INT C INT
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Boykov Y., 2000, P IEEE C COMP VIS PA
   CHAM T, 1999, P IEEE C COMP VIS PA
   CHELLAPPA R., 2000, P IEEE C COMP VIS PA
   CHEN Y, 2001, P IEEE C COMP VIS PA
   Henze N., 2009, P WORKSH MOB INT REA
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   LIPTON A, 1998, P IEEE WORKSH APPL C
   Lowe D G., 1999, P 7 INT C COMP VIS I
   ROSALES R, 1999, P IEEE C COMP VIS PA
   Se S, 2001, IEEE INT CONF ROBOT, P2051, DOI 10.1109/ROBOT.2001.932909
   Takacs G., 2008, MIR 08, P427, DOI DOI 10.1145/1460096.1460165
   Tamimi H., 2005, P EUR C MOB ROB ECMR
   WANG L, 2009, P IEEE C COMP VIS PA
   WOLF J, 2002, P IEEE INT C ROB AUT
   ZHOU Q., 2009, P IEEE C COMP VIS PA
NR 20
TC 6
Z9 7
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 3
SU S
AR 47
DI 10.1145/2348816.2348826
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021VG
UT WOS:000309912900010
DA 2024-07-18
ER

PT J
AU Hoi, SCH
   Liu, W
   Chang, SF
AF Hoi, Steven C. H.
   Liu, Wei
   Chang, Shih-Fu
TI Semi-Supervised Distance Metric Learning for Collaborative Image
   Retrieval and Clustering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Distance metric learning; content-based
   image retrieval; multimedia data clustering
ID RELEVANCE FEEDBACK
AB Learning a good distance metric plays a vital role in many multimedia retrieval and data mining tasks. For example, a typical content-based image retrieval (CBIR) system often relies on an effective distance metric to measure similarity between any two images. Conventional CBIR systems simply adopting Euclidean distance metric often fail to return satisfactory results mainly due to the well-known semantic gap challenge. In this article, we present a novel framework of Semi-Supervised Distance Metric Learning for learning effective distance metrics by exploring the historical relevance feedback log data of a CBIR system and utilizing unlabeled data when log data are limited and noisy. We formally formulate the learning problem into a convex optimization task and then present a new technique, named as "Laplacian Regularized Metric Learning" (LRML). Two efficient algorithms are then proposed to solve the LRML task. Further, we apply the proposed technique to two applications. One direct application is for Collaborative Image Retrieval (CIR), which aims to explore the CBIR log data for improving the retrieval performance of CBIR systems. The other application is for Collaborative Image Clustering (CIC), which aims to explore the CBIR log data for enhancing the clustering performance of image pattern clustering tasks. We conduct extensive evaluation to compare the proposed LRML method with a number of competing methods, including 2 standard metrics, 3 unsupervised metrics, and 4 supervised metrics with side information. Encouraging results validate the effectiveness of the proposed technique.
C1 [Hoi, Steven C. H.] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
   [Liu, Wei; Chang, Shih-Fu] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
C3 Nanyang Technological University; Columbia University
RP Hoi, SCH (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
EM chhoi@ntu.edu.sg; wliu@columbia.edu; sfchang@columbia.edu
RI HOI, Steven C. H./A-3736-2011; Liu, Wei/L-1951-2019
OI Hoi, Steven/0000-0002-4584-3453; Liu, Wei/0000-0002-3865-8145
FU Singapore MOE [RG67/07]
FX The work was fully supported by Singapore MOE Academic Tier-1 Research
   Grant (RG67/07).
CR [Anonymous], 2003, CONVEX OPTIMIZATION
   [Anonymous], 1994, Multidimensional Scaling
   [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P C ADV NEUR INF PRO
   [Anonymous], 2000, WORKSHOP ARTIFICIAL
   Bar-Hillel AB, 2005, J MACH LEARN RES, V6, P937
   Beygelzimer A., 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857
   Dom B. E., 2001, 10219 RJ IBM
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   GIROSI F, 1995, NEURAL COMPUT, V7, P219, DOI 10.1162/neco.1995.7.2.219
   GLOBERSON A, 2005, P C ADV NEUR INF PRO
   Goldberger J., 2005, ADV NEURAL INFORM PR, V17
   He Xiaofei., 2004, ACM MULTIMEDIA, P17
   Hoi CH, 2004, INT C PATT RECOG, P874, DOI 10.1109/ICPR.2004.1334667
   HOI CH, 2004, P ACM INT C MULT
   HOI SC, 2005, P IEEE ICDE WORKSH M
   Hoi SCH, 2006, IEEE T KNOWL DATA EN, V18, P509, DOI 10.1109/TKDE.2006.1599389
   HOI SCH, 2006, P IEEE C COMP VIS PA
   Hoi SCH, 2009, ACM T INFORM SYST, V27, DOI 10.1145/1508850.1508854
   Jain AK, 1998, PATTERN RECOGN, V31, P1369, DOI 10.1016/S0031-3203(97)00131-3
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   King I, 2003, PATTERN RECOGN, V36, P2177, DOI 10.1016/S0031-3203(03)00043-8
   KUHN HW, 1982, SIGMAP B, V31, P6
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Liu F, 2007, PROTEOMICS, V7, P450, DOI 10.1002/pmic.200600465
   MANJUNATH B, 2001, SIGN PROCESS IMAGE C
   MULLER H, 2001, INT J COMPUT VISION, V56, P65
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Rui Y, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P815, DOI 10.1109/ICIP.1997.638621
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Si L, 2006, MULTIMEDIA SYST, V12, P34, DOI 10.1007/s00530-006-0033-1
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766
   TAO D, 2004, P IEEE INT C COMP VI
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Weinberger K.Q., 2006, Advances in neural information processing systems, P1473
   Yan R, 2006, IEEE T PATTERN ANAL, V28, P578, DOI 10.1109/TPAMI.2006.65
   YANG L, 2006, P WORKSH ART INT WEB
NR 41
TC 105
Z9 113
U1 0
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2010
VL 6
IS 3
SI SI
AR 18
DI 10.1145/1823746.1823752
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 649UL
UT WOS:000281799300007
OA Green Published
DA 2024-07-18
ER

PT J
AU Kotharu, PS
   Prabhakaran, B
AF Kotharu, Phani S.
   Prabhakaran, B.
TI Partial query resolution for animation authoring
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; theory; human factors; fuzzy query; top-k query; animation
   toolkit; multimedia authoring; aggregation function; partial ordering
AB Animations are a part of multimedia and techniques such as motion mapping and inverse kinematics aid in reusing models and motion sequences to create new animations. This reuse approach is facilitated by the use of content-based retrieval techniques that often require fuzzy query resolution. Most fuzzy query resolution approaches work on all the attributes of the query to minimize the database access cost thus resulting in an unsatisfactory result set. It turns out that the query resolution can be carried out in a partial manner to achieve user satisfactory results and aid in easy authoring. In this article, we present two partial fuzzy query resolution approaches, one that results in high-quality animations and the other that produces results with decreasing number of satisfied conditions in the query.
C1 [Kotharu, Phani S.; Prabhakaran, B.] Univ Texas Richardson, Erik Jonsson Sch Engn & Comp Sci, Richardson, TX 75083 USA.
C3 University of Texas System; University of Texas Dallas
RP Kotharu, PS (corresponding author), Univ Texas Richardson, Erik Jonsson Sch Engn & Comp Sci, EC32,POB 830688, Richardson, TX 75083 USA.
EM phanik@utdallas.edu; praba@utdallas.edu
CR Akanksha, 2007, MULTIMED TOOLS APPL, V32, P293, DOI 10.1007/s11042-006-0054-y
   Akanksha, 2003, INT J SOFTW ENG KNOW, V13, P1, DOI 10.1142/S0218194003001214
   AKANKSHA H, 2003, P GRAPHITE 2003 MELB, P29
   AKANKSHA H, 2001, P EUROGRAPHICS MM 20
   [Anonymous], 1998, Proc. SIGGRAPH, DOI 10.1145/280814.280820
   BEHM K, 2001, P INT C VER LARG DAT
   Boutilier C, 2004, J ARTIF INTELL RES, V21, P135, DOI 10.1613/jair.1234
   Chaudhuri S., 2004, IEEE Trans. Knowl. Data Eng, V16
   CHAUDHURI S, 1996, P ACM SIGMOD C, P91
   Ciaccia P, 2001, DESIGN AND MANAGEMENT OF MULTIMEDIA INFORMATION SYSTEMS: OPPORTUNITIES AND CHALLENGES, P201
   Ciaccia P, 2000, LECT NOTES COMPUT SC, V1762, P50
   DUBOIS D, 1999, KNOWLEDGE MANAGEMENT, P105
   Dubois D, 1980, Fuzzy sets and systems
   Fagin R, 2003, J COMPUT SYST SCI, V66, P614, DOI 10.1016/S0022-0000(03)00026-6
   Fagin R, 1999, J COMPUT SYST SCI, V58, P83, DOI 10.1006/jcss.1998.1600
   Fagin R, 2000, THEOR COMPUT SCI, V239, P309, DOI 10.1016/S0304-3975(99)00224-8
   GRAVANO L, 1997, P 23 VLDB C ATH GREE
   KIESSLING W, 2002, P 28 C VER LARG DAT
   LEE WM, 2000, IEEE COMPUT GRAPH, P11
   Marian A, 2004, ACM T DATABASE SYST, V29, P319, DOI 10.1145/1005566.1005569
   Nepal S, 1999, PROC INT CONF DATA, P22, DOI 10.1109/ICDE.1999.754894
   NGUYEN H, 2003, PHYSICA
   NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173
   NUTAN C, 2003, P MULT MOD C MMM 200, P343
   RABITTI F, 1990, P EDBT 90 VEN IT MAR
   SCHMITT I, 2004, P 3 INT S FDN INF KN, P252
   Tolani D, 2000, GRAPH MODELS, V62, P353, DOI 10.1006/gmod.2000.0528
   Zadeh LA, 2002, ADV SOFT COMP, P27
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   ZIMMERMANN H., 1996, FUZZY SET THEORY AND, V3rd
NR 30
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 1
AR 4
DI 10.1145/1324287.1324291
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 264UK
UT WOS:000253315700004
DA 2024-07-18
ER

PT J
AU Eide, VSW
   Granmo, OC
   Eliassen, F
   Michaelsen, JA
AF Eide, Viktor S. Wold
   Granmo, Ole-Christoffer
   Eliassen, Frank
   Michaelsen, Jorgen Andreas
TI Real-time video content analysis: QoS-Aware application composition and
   parallel processing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; design; measurement; performance; real-Time video content
   analysis; parallel processing; task graph scheduling; event-based
   communication; publish/subscribe; QoS and resource management
ID TRACKING; DESIGN
AB in real-time and with an acceptable error rate. Statements such as this express quality of service (QoS) requirements. In general, control of the QoS provided can be achieved by sacrificing application quality in one QoS dimension for better quality in another, or by controlling the allocation of processing resources to the application. However, controlling QoS in video content analysis is particularly difficult, not only because main QoS dimensions like accuracy are nonadditive, but also because both the communication- and the processing-resource requirements are challenging.
   This article presents techniques for QoS-aware composition of applications for real-time video content analysis, based on dynamic Bayesian networks. The aim of QoS-aware composition is to determine application deployment configurations which satisfy a given set of QoS requirements. Our approach consists of. (1) an algorithm for QoS-aware selection of configurations of feature extractor and classification algorithms which balances requirements for timeliness and accuracy against available processing resources, (2) a distributed content-based publish/subscribe system which provides application scalability at multiple logical levels of distribution, and (3) scalable solutions for video streaming, filtering/transformation, feature extraction, and classification.
   We evaluate our approach based on experiments with an implementation of a real-time motion vector based object-tracking application. The evaluation shows that the application largely behaves as expected when resource availability and selections of configurations of feature extractor and classification algorithms vary. The evaluation also shows that increasing QoS requirements can be met by allocating additional CPUs for parallel processing, with only minor overhead.
C1 Simula Res Lab, N-1325 Lysaker, Norway.
   Agder Univ Coll, N-4876 Grimstad, Norway.
   Univ Oslo, N-0314 Oslo, Norway.
C3 University of Agder; University of Oslo
RP Eide, VSW (corresponding author), Simula Res Lab, PO Box 134, N-1325 Lysaker, Norway.
EM viktore@simula.no; ole.granmo@hia.no; frank@simula.no;
   jorgenam@ifi.uio.no
RI Granmo, Ole-Christoffer/IAP-9212-2023
OI Granmo, Ole-Christoffer/0000-0002-7287-030X; Michaelsen, Jorgen
   Andreas/0000-0003-2556-688X
CR Amini L, 2000, PROC SPIE, V3969, P14
   [Anonymous], 1997, COMPUTER SCI SERIES
   Beymer D, 1997, PROC CVPR IEEE, P495, DOI 10.1109/CVPR.1997.609371
   Black AP, 2002, MULTIMEDIA SYST, V8, P406, DOI 10.1007/s005300200062
   Bors AG, 2000, IEEE T IMAGE PROCESS, V9, P1441, DOI 10.1109/83.855440
   Carzaniga A, 2001, ACM T COMPUT SYST, V19, P332, DOI 10.1145/380749.380767
   Castro M, 2003, IEEE INFOCOM SER, P1510
   CHANG SF, 2000, P IEEE INT C MULT EX, V2, P687
   Chen C, 2001, IEEE CONTR SYST MAG, V21, P26, DOI 10.1109/37.969132
   Chu YH, 2002, IEEE J SEL AREA COMM, V20, P1456, DOI 10.1109/JSAC.2002.803066
   Crowcroft J, 2002, LECT NOTES COMPUT SC, V2497, P1
   Dash M., 1997, Intelligent Data Analysis, V1
   DEERING SE, 1990, ACM T COMPUT SYST, V8, P85, DOI 10.1145/78952.78953
   *DMJ, 1999, DISTR MED JOURN PROJ
   Eide V.S.W., 2003, P 11 ACM INT C MULT, P21
   Eide VSW, 2005, PROC SPIE, V5680, P155, DOI 10.1117/12.592236
   EIDE VSW, 2004, P 12 ACM INT C MULT, P164
   EIDE VSW, 2003, 200303 SIM RES LAB
   Eugster PT, 2003, ACM COMPUT SURV, V35, P114, DOI 10.1145/857076.857078
   Garg A., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P384, DOI 10.1109/AFGR.2000.840663
   Grace P, 2004, LECT NOTES COMPUT SC, V3291, P1463
   Granmo OC, 2003, LECT NOTES COMPUT SC, V2749, P983
   GU X, 2005, IN PRESS IEEE T MULT
   JACOBSEN HA, 2003, P 2 INT WORKSH DISTR
   Jensen F. V., 2007, Bayesian networks and decision graphs
   Kwok YK, 1999, J PARALLEL DISTR COM, V59, P381, DOI 10.1006/jpdc.1999.1578
   Leszczuk M, 2002, LECT NOTES COMPUT SC, V2515, P176
   Li BX, 2002, IEEE T IMAGE PROCESS, V11, P530, DOI 10.1109/TIP.2002.1006400
   Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847
   Marcenaro L, 2001, P IEEE, V89, P1419, DOI 10.1109/5.959339
   Mayer-Patel K., 1998, Proceedings ACM Multimedia 98, P161, DOI 10.1145/290747.290768
   MAYERPATEL K, 1999, P MULT COMP NETW MMC, V3654, P252
   MAYERPATEL K, 1999, P 7 ACM INT C MULT, V1, P409
   McCanne S, 1997, IEEE J SEL AREA COMM, V15, P983, DOI 10.1109/49.611154
   Naphade MR, 2002, IEEE T NEURAL NETWOR, V13, P793, DOI 10.1109/TNN.2002.1021881
   OKADA R, 1996, P IEEE SICE RSJ INT, P565
   OOI WT, 2001, P ACM MULT SEPT 30 S, P159
   OPYRCHAL L, 2000, P IFIP ACM INT C DIS, P185
   OZER B, 2001, P C INT MULT NETW MA, V4519, P84
   PIETZUCH P, 2002, P 1 INT WORKSH DISTR
   PIETZUCH PR, 2003, PEER TO PEER OVERLAY
   PRATT W.K., 1991, DIGITAL IMAGE PROCES, V2
   Rafaelsen HO, 2002, LECT NOTES COMPUT SC, V2519, P675
   SEGALL B, 2000, P AUUG2K CANB AUSTR
   Smits PC, 2000, INT C PATT RECOG, P386, DOI 10.1109/ICPR.2000.906093
   *SUN MICROSYSTEMS, 1999, JAV MED FRAM API GUI
   TERPSTRA WW, 2003, PEER TO PEER APPROAC
   Yang JH, 1998, IEEE INTELL SYST APP, V13, P44, DOI 10.1109/5254.671091
   Yang MT, 2003, IEEE T PARALL DISTR, V14, P119, DOI 10.1109/TPDS.2003.1178876
   ZHOU W, 1998, P 32 AS C SIGN SYST, V1, P882
NR 50
TC 3
Z9 3
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2006
VL 2
IS 2
BP 149
EP 172
DI 10.1145/1142020.1142024
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IX
UT WOS:000250871300004
DA 2024-07-18
ER

PT J
AU Tai, YC
   Shi, HL
   Zeng, D
   Du, H
   Hu, YB
   Zhang, ZC
   Zhang, ZJ
   Mei, T
AF Tai, Yichun
   Shi, Hailin
   Zeng, Dan
   Du, Hang
   Hu, Yibo
   Zhang, Zicheng
   Zhang, Zhijiang
   Mei, Tao
TI Multi-Agent Semi-Siamese Training for Long-Tail and Shallow Face
   Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face recognition; shallow face learning; long-tail face learning
ID SOFTMAX
AB With the recent development of deep convolutional neural networks and large-scale datasets, deep face recognition has made remarkable progress and been widely used in various applications. However, unlike the existing public face datasets, in many real-world scenarios of face recognition, the depth of the training dataset is shallow, which means that only two face images are available for each ID. With the non-uniform increase of samples, such issue is converted to a more general case, known as long-tail face learning, which suffers from data imbalance and intra-class diversity dearth simultaneously. These adverse conditions damage the training and result in the decline of model performance. Based on Semi-Siamese Training, we introduce an advanced solution, named Multi-Agent Semi-Siamese Training (MASST), to address these problems. MASST includes a probe network and multiple gallery agents-the former aims to encode the probe features, and the latter constitutes a stack of networks that encode the prototypes (gallery features). For each training iteration, the gallery network, which is sequentially rotated from the stack, and the probe network form a pair of Semi-Siamese networks. We give the theoretical and empirical analysis that, given the long-tail (or shallow) data and training loss, MASST smooths the loss landscape and satisfies the Lipschitz continuity with the help of multiple agents and the updating gallery queue. The proposedmethod is out of extra-dependency, and thus can be easily integrated with the existing loss functions and network architectures. It is worth noting that although multiple gallery agents are employed for training, only the probe network is needed for inference, without increasing the inference cost. Extensive experiments and comparisons demonstrate the advantages of MASST for long-tail and shallow face learning.
C1 [Tai, Yichun; Zeng, Dan; Du, Hang; Zhang, Zicheng; Zhang, Zhijiang] Shanghai Univ, 99 Shangda Rd, Shanghai 200444, Peoples R China.
   [Shi, Hailin; Hu, Yibo; Mei, Tao] AI Res, 76 Zichun Rd, Beijing 100020, Peoples R China.
   [Zhang, Zicheng] Univ Chinese Acad Sci, 19 Yuquan Rd, Beijing 100040, Peoples R China.
C3 Shanghai University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Zeng, D; Zhang, ZC (corresponding author), Shanghai Univ, 99 Shangda Rd, Shanghai 200444, Peoples R China.; Shi, HL (corresponding author), AI Res, 76 Zichun Rd, Beijing 100020, Peoples R China.
EM taiyc@shu.edu.cn; shihailin@jd.com; dzeng@shu.edu.cn; duhang@shu.edu.cn;
   huyibo871079699@gmail.com; zjzhang@shu.edu.cn;
   zhangzicheng19@mails.ucas.ac.cn; tmei@live.com
OI Tai, Yichun/0000-0003-3656-2593
CR Alshammari S, 2022, PROC CVPR IEEE, P6887, DOI 10.1109/CVPR52688.2022.00677
   [Anonymous], 2000, INT C MACH LEARN
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Cheng Y, 2017, IEEE INT CONF COMP V, P1924, DOI 10.1109/ICCVW.2017.227
   Cheng ZY, 2018, Arxiv, DOI arXiv:1804.09691
   Cui J., 2021, P IEEECVF INT C COMP, P715
   DeepGlint, 2018, CHALL 3 FAC FEAT TES
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Ding ZM, 2019, Arxiv, DOI arXiv:1910.04860
   Dixit M, 2017, PROC CVPR IEEE, P3328, DOI 10.1109/CVPR.2017.355
   Du H., 2020, ECCV, P36
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   Guo YD, 2018, Arxiv, DOI arXiv:1707.05574
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   He J, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3511917
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YX, 2008, IEEE T PARALL DISTR, V19, P1263, DOI 10.1109/TPDS.2008.39
   Holte R. C., 2003, Workshop on learning from imbalanced datasets II, V11, P1
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang C, 2020, IEEE T PATTERN ANAL, V42, P2781, DOI 10.1109/TPAMI.2019.2914680
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Huang G B, 2008, P WORKSHOP FACESREAL
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527
   Khan SH, 2018, IEEE T NEUR NET LEAR, V29, P3573, DOI 10.1109/TNNLS.2017.2732482
   Li B, 2021, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR46437.2021.00376
   Li Y, 2019, PROC CVPR IEEE, P9564, DOI 10.1109/CVPR.2019.00980
   Liao SC, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014)
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu BY, 2019, IEEE I CONF COMP VIS, P10051, DOI 10.1109/ICCV.2019.01015
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853
   Ma YH, 2020, PATTERN RECOGN LETT, V133, P48, DOI 10.1016/j.patrec.2020.02.007
   Ma YH, 2018, IEEE IMAGE PROC, P2401, DOI 10.1109/ICIP.2018.8451561
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Nesterov, 2018, LECT CONVEX OPTIMIZA, V137
   Pan SY, 2022, MULTIMED TOOLS APPL, V81, P13593, DOI 10.1007/s11042-022-12323-2
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Schwartz E, 2018, ADV NEUR IN, V31
   Sengupta S, 2016, IEEE WINT CONF APPL
   Simon C, 2020, PROC CVPR IEEE, P4135, DOI 10.1109/CVPR42600.2020.00419
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohn K, 2016, ADV NEUR IN, V29
   Sun Y, 2014, ADV NEUR IN, V27
   Tan ZC, 2022, IEEE T IMAGE PROCESS, V31, P3224, DOI 10.1109/TIP.2021.3137005
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang LX, 2018, IEEE IMAGE PROC, P2386, DOI 10.1109/ICIP.2018.8451464
   Wang XB, 2020, AAAI CONF ARTIF INTE, V34, P12241
   Wang XB, 2019, IEEE I CONF COMP VIS, P9357, DOI 10.1109/ICCV.2019.00945
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87
   Wu JL, 2022, IEEE T MULTIMEDIA, V24, P3693, DOI 10.1109/TMM.2021.3106096
   Wu Y, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P408, DOI 10.1145/3126636.3126693
   Wu Y, 2017, IEEE INT CONF COMP V, P1933, DOI 10.1109/ICCVW.2017.228
   Xia ZQ, 2018, IET BIOMETRICS, V7, P56, DOI 10.1049/iet-bmt.2017.0193
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yao ZW, 2020, Arxiv, DOI arXiv:1810.01021
   Yin X, 2019, Arxiv, DOI arXiv:1803.09014
   Yin X, 2019, PROC CVPR IEEE, P5697, DOI 10.1109/CVPR.2019.00585
   Yoo D, 2015, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2015.305
   Li SZ, 2020, Arxiv, DOI arXiv:2006.08256
   Zhang B, 2022, IEEE T IMAGE PROCESS, V31, P2309, DOI 10.1109/TIP.2022.3154938
   Zhang SF, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P1, DOI 10.1109/BTAS.2017.8272675
   Zhang X, 2019, PROC CVPR IEEE, P10815, DOI 10.1109/CVPR.2019.01108
   Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578
   Zhang YB, 2020, IEEE COMPUT SOC CONF, P3594, DOI 10.1109/CVPRW50498.2020.00420
   Zheng T., 2018, Tech. Rep., V5
   Zheng TY, 2017, Arxiv, DOI arXiv:1708.08197
   Zheng YT, 2018, PROC CVPR IEEE, P5089, DOI 10.1109/CVPR.2018.00534
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhong YY, 2019, PROC CVPR IEEE, P7804, DOI 10.1109/CVPR.2019.00800
   Zhu XY, 2019, INT J COMPUT VISION, V127, P684, DOI 10.1007/s11263-019-01162-8
   Zhu Z, 2021, PROC CVPR IEEE, P10487, DOI 10.1109/CVPR46437.2021.01035
NR 79
TC 1
Z9 1
U1 5
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 196
DI 10.1145/3594669
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ahmed, F
   Rehman, MU
   Ahmad, J
   Khan, MS
   Boulila, W
   Srivastava, G
   Lin, JCW
   Buchanan, WJ
AF Ahmed, Fawad
   Rehman, Muneeb Ur
   Ahmad, Jawad
   Khan, Muhammad Shahbaz
   Boulila, Wadii
   Srivastava, Gautam
   Lin, Jerry Chun-Wei
   Buchanan, William J.
TI A DNA Based Colour Image Encryption Scheme Using A Convolutional
   Autoencoder
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Autoencoder; chaos; DNA coding; deep learning; colour image encryption;
   dimensionality reduction
ID HIDING MESSAGES; ALGORITHM; CHAOS; SEQUENCE
AB With the advancement in technology, digital images can easily be transmitted and stored over the Internet. Encryption is used to avoid illegal interception of digital images. Encrypting large-sized colour images in their original dimension generally results in low encryption/decryption speed along with exerting a burden on the limited bandwidth of the transmission channel. To address the aforementioned issues, a new encryption scheme for colour images employing convolutional autoencoder, DNA and chaos is presented in this paper. The proposed scheme has two main modules, the dimensionality conversion module using the proposed convolutional autoencoder, and the encryption/decryption module using DNA and chaos. The dimension of the input colour image is first reduced from NxMx3 to PxQ gray-scale image using the encoder. Encryption and decryption are then performed in the reduced dimension space. The decrypted gray-scale image is upsampled to obtain the original colour image having dimension N xM x 3. The training and validation accuracy of the proposed autoencoder is 97% and 95%, respectively. Once the autoencoder is trained, it can be used to reduce and subsequently increase the dimension of any arbitrary input colour image. The efficacy of the designed autoencoder has been demonstrated by the successful reconstruction of the compressed image into the original colour image with negligible perceptual distortion. The second major contribution presented in this paper is an image encryption scheme using DNA along with multiple chaotic sequences and substitution boxes. The security of the proposed image encryption algorithm has been gauged using several evaluation parameters, such as histogram of the cipher image, entropy, NPCR, UACI, key sensitivity, contrast, and so on. The experimental results of the proposed scheme demonstrate its effectiveness to perform colour image encryption.
C1 [Ahmed, Fawad] NUST, Pakistan Navy Engn Coll, Dept Cyber Secur, HABIB IR Rd, Karachi 75350, Sindh, Pakistan.
   [Rehman, Muneeb Ur; Khan, Muhammad Shahbaz] HITEC Univ, Dept Elect Engn, Museum Rd, Taxila 47080, Punjab, Pakistan.
   [Ahmad, Jawad; Buchanan, William J.] Edinburgh Napier Univ, Sch Comp, Merchiston Campus,10 Colinton Rd, Edinburgh EH10 5DT, Scotland.
   [Boulila, Wadii] Prince Sultan Univ, Robot & Internet of Things Lab, Rafha St, Riyadh 11586, Saudi Arabia.
   [Boulila, Wadii] Univ Manouba, Natl Sch Comp Sci, RIADI Lab, Manouba Campus, Manouba 2010, Tunisia.
   [Srivastava, Gautam] Brandon Univ, Dept Math & Comp Sci, 270 18th St, Brandon, MB R7A 6A9, Canada.
   [Srivastava, Gautam] China Med Univ, Res Ctr Interneural Comp, 91 Xueshi Rd, Taichung 40402, Taiwan.
   [Srivastava, Gautam] Lebanese Amer Univ, Dept Comp Sci & Math, POB 13-5053, Chouran Beirut 11022, Lebanon.
   [Lin, Jerry Chun-Wei] Western Norway Univ Appl Sci, Dept Comp Sci Elect Engn & Math Sci, Inndalsveien 28, N-5063 Bergen, Norway.
C3 National University of Sciences & Technology - Pakistan; NITEC
   University; Edinburgh Napier University; Prince Sultan University;
   Universite de la Manouba; Brandon University; China Medical University
   Taiwan; Lebanese American University; Western Norway University of
   Applied Sciences
RP Srivastava, G (corresponding author), Brandon Univ, Dept Math & Comp Sci, 270 18th St, Brandon, MB R7A 6A9, Canada.; Srivastava, G (corresponding author), China Med Univ, Res Ctr Interneural Comp, 91 Xueshi Rd, Taichung 40402, Taiwan.; Srivastava, G (corresponding author), Lebanese Amer Univ, Dept Comp Sci & Math, POB 13-5053, Chouran Beirut 11022, Lebanon.
EM fawad@pnec.nust.edu.pk; muneeb95rehman@gmail.com;
   shahbaz.khan@hitecuni.edu.pk; wboulila@psu.edu.sa;
   srivastavag@brandonu.ca; jerrylin@ieee.org; Buchanan@napier.ac.uk
RI Srivastava, Gautam/N-5668-2019; Boulila, Wadii/AGY-5718-2022; Khan,
   Muhammad Shahbaz/AGV-8767-2022; Ahmad, Jawad/AAC-3119-2020
OI Srivastava, Gautam/0000-0001-9851-4103; Boulila,
   Wadii/0000-0003-2133-0757; Khan, Muhammad Shahbaz/0000-0002-7166-6681;
   Ahmad, Jawad/0000-0001-6289-8248; Lin, Jerry
   Chun-Wei/0000-0001-8768-9709
CR Abdoun N, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22091012
   Abu Taha M, 2020, CRYPTOGRAPHY-BASEL, V4, DOI 10.3390/cryptography4020018
   Ahmad J, 2018, COMPUT SCI ELECTR, P208, DOI 10.1109/CEEC.2018.8674208
   Ahmad J, 2016, MULTIMED TOOLS APPL, V75, P13951, DOI 10.1007/s11042-015-2973-y
   Akkasaligar PT, 2020, INF SECUR J, V29, P91, DOI 10.1080/19393555.2020.1718248
   Ball‚ J, 2017, Arxiv, DOI [arXiv:1611.01704, 10.48550/arXiv.1611.01704]
   Battikh D, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21080748
   Belazi A, 2019, IEEE ACCESS, V7, P36667, DOI 10.1109/ACCESS.2019.2906292
   Chai XL, 2017, SIGNAL PROCESS-IMAGE, V52, P6, DOI 10.1016/j.image.2016.12.007
   Chai XL, 2017, OPT LASER ENG, V88, P197, DOI 10.1016/j.optlaseng.2016.08.009
   Cheng ZX, 2018, PICT COD SYMP, P253, DOI 10.1109/PCS.2018.8456308
   Chirakkarottu S, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-019-1685-8
   Clelland CT, 1999, NATURE, V399, P533, DOI 10.1038/21092
   Dimitrov MM, 2020, IEEE ACCESS, V8, P117173, DOI 10.1109/ACCESS.2020.3004526
   Durafe Asha, 2022, 2022 INT C ADVANCEME, P1
   Elkandoz MT, 2022, MULTIMED TOOLS APPL, V81, P25497, DOI 10.1007/s11042-022-12595-8
   Enayatifar R, 2014, OPT LASER ENG, V56, P83, DOI 10.1016/j.optlaseng.2013.12.003
   Garg Disha, 2022, 2022 2nd International Conference on Innovative Practices in Technology and Management (ICIPTM), P362, DOI 10.1109/ICIPTM54933.2022.9754031
   Gupta C, 2020, Arxiv, DOI arXiv:2001.03017
   Hu F, 2016, Arxiv, DOI arXiv:1608.05001
   Hu F, 2017, MATH PROBL ENG, V2017, DOI 10.1155/2017/3675459
   Kumar R, 2021, J PARALLEL DISTR COM, V152, P128, DOI 10.1016/j.jpdc.2021.02.022
   Lai Q, 2022, CHAOS SOLITON FRACT, V158, DOI 10.1016/j.chaos.2022.112017
   Latif S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21227518
   Liao Qi-nan, 2011, Proceedings of the 2011 International Conference on Network Computing and Information Security (NCIS), P402, DOI 10.1109/NCIS.2011.88
   Liu LL, 2012, COMPUT ELECTR ENG, V38, P1240, DOI 10.1016/j.compeleceng.2012.02.007
   Manikandan C, 2022, MULTIMED TOOLS APPL, V81, P10337, DOI 10.1007/s11042-022-11930-3
   Masood F, 2022, MULTIMED TOOLS APPL, V81, P30931, DOI 10.1007/s11042-022-12844-w
   Masood F, 2022, SOFT COMPUT, V26, P7461, DOI 10.1007/s00500-021-06459-w
   Mukherjee P, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22197332
   Namasudra S, 2022, IEEE T SERV COMPUT, V15, P2289, DOI 10.1109/TSC.2020.3046471
   Namasudra S, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3392665
   Namasudra S, 2020, COMPUT COMMUN, V151, P539, DOI 10.1016/j.comcom.2019.12.041
   Pak C, 2017, SIGNAL PROCESS, V138, P129, DOI 10.1016/j.sigpro.2017.03.011
   Pavithran P, 2022, COMPUT COMMUN, V188, P1, DOI 10.1016/j.comcom.2022.02.008
   Qayyum A, 2020, IEEE ACCESS, V8, P140876, DOI 10.1109/ACCESS.2020.3012912
   Shafique A, 2021, IEEE ACCESS, V9, P9383, DOI 10.1109/ACCESS.2020.3046528
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Sohal M, 2022, J KING SAUD UNIV-COM, V34, P1417, DOI 10.1016/j.jksuci.2018.09.024
   Sun CY, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23030291
   Tanaka K, 2005, BIOSYSTEMS, V81, P25, DOI 10.1016/j.biosystems.2005.01.004
   Telem ANK, 2021, MULTIMED TOOLS APPL, V80, P19011, DOI 10.1007/s11042-021-10549-0
   Hoang TM, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22050548
   Wang B, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/7276084
   Wang T, 2020, OPT LASER TECHNOL, V132, DOI 10.1016/j.optlastec.2020.106355
   Wang XY, 2021, OPT LASER ENG, V137, DOI 10.1016/j.optlaseng.2020.106393
   Wang YF, 2019, IEEE T NANOTECHNOL, V18, P299, DOI 10.1109/TNANO.2019.2904842
   Xue XL, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0241184
   Xue XL, 2010, NEURAL NETW WORLD, V20, P285
   Zhang SJ, 2021, MATH COMPUT SIMULAT, V190, P723, DOI 10.1016/j.matcom.2021.06.012
   Zhang Y, 2018, MULTIMED TOOLS APPL, V77, P21589, DOI 10.1007/s11042-017-5585-x
   Zhen P, 2016, MULTIMED TOOLS APPL, V75, P6303, DOI 10.1007/s11042-015-2573-x
NR 52
TC 13
Z9 13
U1 20
U2 53
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 128
DI 10.1145/3570165
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700003
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, QQ
   Li, J
   Zhao, TJ
   Wang, YD
AF Gao, Qiqi
   Li, Jie
   Zhao, Tiejun
   Wang, Yadong
TI Real-time Image Enhancement with Attention Aggregation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image/video enhancement; relighting; attention aggregation; deep
   learning
AB Image enhancement has stimulated significant research works over the past years for its great application potential in video conferencing scenarios. Nevertheless, most existing image enhancement approaches are still struggling to find a good tradeoff that reduces the computational cost as much as possible while maintaining plausible result quality. Recently, curve-based mapping methods are proposed and have shown great potential for real-time and high-quality image enhancement of arbitrary resolutions. In this article, we take advantage of the curve-based mapping representation and focus on further improving the enhancement quality and robustness, while minimizing additional computational costs. Specifically, we (1) carefully re-formulate the curve function to improve learning stability, and (2) aggregate different semantic attention into the curve regression process, which can overcome the major problems of curve-based methods that generate moderate results with low contrast. The semantic attention is jointly learned with the supervision from class activation mapping of pre-trained feature extractors, thus reducing themanual annotation cost of semantic labels. Experiments have shown that our proposed method significantly improves curve-based methods both qualitatively and quantitatively, achieving visually plausible results compared with other deep neural network-based enhancement methods, and maintains a very low computational cost, i.e., taking 18.7 ms for a 360p image on a single P40 GPU. Extensive experiments demonstrate that our method is also capable of video enhancement tasks.
C1 [Gao, Qiqi; Li, Jie; Zhao, Tiejun; Wang, Yadong] Harbin Inst Technol, 92 Xidazhi St, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Institute of Technology
RP Gao, QQ (corresponding author), Harbin Inst Technol, 92 Xidazhi St, Harbin 150001, Heilongjiang, Peoples R China.
EM 18b903026@stu.hit.edu.cn; jieli@hit.edu.cn; tjzhao@hit.edu.cn;
   ydwang@hit.edu.cn
OI Gao, Qiqi/0000-0002-4743-9657
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629645
   Barash D, 2002, IEEE T PATTERN ANAL, V24, P844, DOI 10.1109/TPAMI.2002.1008390
   Bonneel N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818107
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   HaCohen Y, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461997
   Han-Ul Kim, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P374, DOI 10.1007/978-3-030-58577-8_23
   Harold Ahlberg J., 2016, THEORY SPLINES THEIR
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181974
   Huang Y, 2015, ADV NEUR IN, V28
   Huang YK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P365, DOI 10.1145/3343031.3350994
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kalchbrenner Nal, 2017, INT C MACHINE LEARNI, P1771
   Kingma D. P., 2014, arXiv
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Liu Shiguang, 2021, ACM T MULTIM COMPUT, V17, P1
   Liu SF, 2016, LECT NOTES COMPUT SC, V9908, P560, DOI 10.1007/978-3-319-46493-0_34
   Moran Sean, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12823, DOI 10.1109/CVPR42600.2020.01284
   Iandola FN, 2016, Arxiv, DOI arXiv:1602.07360
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   Pickup LC, 2014, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR.2014.262
   Rebol Manuel, 2020, P JOINT AUSTRIAN COM
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Wang BY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964959
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang W, 2019, IEEE I CONF COMP VIS, P4110, DOI 10.1109/ICCV.2019.00421
   Wikipedia contributors, 2021, CIELAB COL SPAC WIK
   Xide Xia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P327, DOI 10.1007/978-3-030-58598-3_20
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
   Zhang MH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4088, DOI 10.1145/3394171.3413951
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 46
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 98
DI 10.1145/3564607
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300023
DA 2024-07-18
ER

PT J
AU Jonna, S
   Medhi, M
   Sahay, RR
AF Jonna, Sankaraganesh
   Medhi, Moushumi
   Sahay, Rajiv Ranjan
TI Distill-DBDGAN: Knowledge Distillation and Adversarial Learning
   Framework for Defocus Blur Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Defocus blur detection; knowledge distillation; adversarial learning
ID NETWORK
AB Defocus blur detection (DBD) aims to segment the blurred regions from a given image affected by defocus blur. It is a crucial pre-processing step for various computer vision tasks. With the increasing popularity of small mobile devices, there is a need for a computationally efficient method to detect defocus blur accurately. We propose an efficient defocus blur detection method that estimates the probability of each pixel being focused or blurred in resource-constraint devices. Despite remarkable advances made by the recent deep learning-based methods, they still suffer from several challenges such as background clutter, scale sensitivity, indistinguishable low-contrast focused regions from out-of-focus blur, and especially high computational cost and memory requirement. To address the first three challenges, we develop a novel deep network that efficiently detects blur map from the input blurred image. Specifically, we integrate multi-scale features in the deep network to resolve the scale ambiguities and simultaneously modeled the non-local structural correlations in the high-level blur features. To handle the last two issues, we eventually frame our DBD algorithm to perform knowledge distillation by transferring information from the larger teacher network to a compact student network. All the networks are adversarially trained in an end-to-end manner to enforce higher order consistencies between the output and the target distributions. Experimental results demonstrate the state-ofthe-art performance of the larger teacher network, while our proposed lightweight DBD model imitates the output of the teacher network without significant loss in accuracy. The codes, pre-trained model weights, and the results will be made publicly available.
C1 [Jonna, Sankaraganesh; Medhi, Moushumi; Sahay, Rajiv Ranjan] Indian Inst Technol Kharagpur, Kharagpur 721302, W Bengal, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kharagpur
RP Medhi, M (corresponding author), Indian Inst Technol Kharagpur, Kharagpur 721302, W Bengal, India.
EM medhi.moushumi@iitkgp.ac.in
RI Medhi, Moushumi/ADP-6453-2022
OI Medhi, Moushumi/0000-0001-9069-8833; Sahay, Rajiv
   Ranjan/0000-0003-0820-0616
CR [Anonymous], 2023, ACM T MULTIM COMPUT, V19
   Bac S, 2007, COMPUT GRAPH FORUM, V26, P571, DOI 10.1111/j.1467-8659.2007.01080.x
   Carvalho M, 2019, LECT NOTES COMPUT SC, V11129, P307, DOI 10.1007/978-3-030-11009-3_18
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Golestaneh SA, 2017, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2017.71
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo WL, 2022, IEEE SIGNAL PROC LET, V29, P140, DOI 10.1109/LSP.2021.3128375
   Hensel M, 2017, ADV NEUR IN, V30
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang R, 2018, NEUROCOMPUTING, V285, P154, DOI 10.1016/j.neucom.2018.01.041
   Jolicoeur-Martineau Alexia, 2018, The relativistic discriminator: A key element missing from standard GAN
   Kingma D. P., 2014, arXiv
   Li BY, 2019, ADV NEUR IN, V32
   Li JX, 2021, IEEE T IMAGE PROCESS, V30, P3748, DOI 10.1109/TIP.2021.3065171
   Lin Zinan, 2020, ADV NEURAL INFORM PR, V34, P9652
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P5155, DOI 10.1109/TIP.2018.2847421
   Mao XD, 2019, IEEE T PATTERN ANAL, V41, P2947, DOI 10.1109/TPAMI.2018.2872043
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Miyato T., 2018, Proceedings of the 6th International Conference on Learning Representations, P1
   Mokhtarian F, 1998, IEEE T PATTERN ANAL, V20, P1376, DOI 10.1109/34.735812
   Ning Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P617, DOI 10.1007/978-3-030-58607-2_36
   Pang YW, 2016, IEEE T CYBERNETICS, V46, P2220, DOI 10.1109/TCYB.2015.2472478
   Park J, 2017, PROC CVPR IEEE, P2760, DOI 10.1109/CVPR.2017.295
   Radford A., 2015, ARXIV
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi JP, 2015, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2015.7298665
   Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379
   Sun XL, 2020, NEUROCOMPUTING, V414, P278, DOI 10.1016/j.neucom.2020.06.068
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang C, 2020, AAAI CONF ARTIF INTE, V34, P12063
   Tang C, 2021, IEEE T MULTIMEDIA, V23, P624, DOI 10.1109/TMM.2020.2985541
   Tang C, 2022, IEEE T PATTERN ANAL, V44, P955, DOI 10.1109/TPAMI.2020.3014629
   Tang C, 2019, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2019.00281
   Tang C, 2016, IEEE SIGNAL PROC LET, V23, P1652, DOI 10.1109/LSP.2016.2611608
   Xiaodong Cun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P747, DOI 10.1007/978-3-030-58601-0_44
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yi X, 2016, IEEE T IMAGE PROCESS, V25, P1626, DOI 10.1109/TIP.2016.2528042
   Zeng K, 2019, IEEE T IMAGE PROCESS, V28, P2107, DOI 10.1109/TIP.2018.2881830
   Zhai YP, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.107996
   Zhang JY, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND SIGNAL PROCESSING, P32, DOI 10.1109/ISISE.2009.113
   Zhao WD, 2021, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR46437.2021.00686
   Zhao WD, 2021, IEEE T IMAGE PROCESS, V30, P5426, DOI 10.1109/TIP.2021.3084101
   Zhao WD, 2019, PROC CVPR IEEE, P8897, DOI 10.1109/CVPR.2019.00911
   Zhao WD, 2020, IEEE T PATTERN ANAL, V42, P1884, DOI 10.1109/TPAMI.2019.2906588
   Zhao WD, 2018, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2018.00325
NR 48
TC 1
Z9 1
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 87
DI 10.1145/3557897
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300012
DA 2024-07-18
ER

PT J
AU Li, Y
   Zhang, L
   Zhang, K
AF Li, Yue
   Zhang, Li
   Zhang, Kai
TI iDAM: Iteratively Trained Deep In-loop Filter with Adaptive Model
   Selection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adaptive model selection; convolutional neural networks; conditional
   in-loop filter; deep in-loop filtering; iterative training; Versatile
   Video Coding
ID INTRA-PREDICTION; IMAGE
AB As a rapid development of neural-network-based machine learning algorithms, deep learning methods are being tentatively used in a much wider range than well-known artificial intelligence applications such as face recognition or auto-driving. Recently, deep learning models are investigated intensively to improve the compression efficiency for video coding, especially at the in-loop filtering stage. Although deep learningbased in-loop filtering methods in prior arts have already shown a remarkable potential capability in video coding, content propagation issue is still not well recognized and addressed yet. Content propagation is the fact that contents of reference frames are propagated to frames referring to them, which typically leads to over-filtering issues. In this article, we develop an iteratively trained deep in-loop filter with adaptive model selection (iDAM) to address the content propagation issue. First, we propose an iterative training scheme, which enables the network to gradually take into account the impacts of content propagation. Second, we propose a filter selection mechanism, i.e., allowing a block to select from a set of candidate filterswith different filtering strengths. Besides, we propose a novel approach to design a conditional in-loop filtering method that can deal with multiple quality levels with a single model and serve the functionality of filter selection by modifying the input parameters. Extensive experiments on top of the latest video coding standard (Versatile Video Coding, VVC) have been conducted to evaluate the proposed techniques. Compared with VTM-11.0, our scheme achieves a new state-of-the-art, leading to {7.91%, 20.25%, 20.44%}, {11.64%, 26.40%, 26.50%}, and {10.97%, 26.63%, 26.77%} BD-rate reductions on average for {Y, Cb, Cr} under all-intra, random-access, and low-delay configurations, respectively. As far as we know, our proposed iDAM scheme provides the highest coding performance compared to all existing solutions. In addition, the syntax elements of the proposed scheme were adopted at the 76th meeting of Audio Video coding Standard (AVS) held this year.
C1 [Li, Yue; Zhang, Li; Zhang, Kai] Bytedance Inc, 8910 Univ Ctr Lane, San Diego, CA 92122 USA.
RP Li, Y (corresponding author), Bytedance Inc, 8910 Univ Ctr Lane, San Diego, CA 92122 USA.
EM yue.li@bytedance.com; lizhang.idm@bytedance.com;
   zhangkai.video@bytedance.com
RI xin, liang/JFS-5770-2023; cheng, cheng/JBR-8359-2023; wang,
   yingying/JSK-6741-2023; Cheng, Yuan/JKJ-0794-2023
CR Bjotegaard G., 2001, VCEGM33
   Bordes Philippe, 2021, JVET V0092
   Bossen Frank, 2018, JVET K1010
   Bross B., 2020, JVET-S2001
   Chen Wei, 2021, IEEE T DEPEND SECURE
   Chiu Yi-Jen, 2008, CELL RES
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Kingma D. P., 2014, arXiv
   Li JH, 2018, IEEE T IMAGE PROCESS, V27, P3236, DOI 10.1109/TIP.2018.2817044
   Li Junru, 2021, HORTIC RES-ENGLAND
   Li Y, 2018, IEEE T CIRC SYST VID, V28, P2316, DOI 10.1109/TCSVT.2017.2727682
   Li Yue, 2021, J BIONIC ENG
   Li Yue, 2021, JVET U0068
   Lin JP, 2019, IEEE T CIRC SYST VID, V29, P3701, DOI 10.1109/TCSVT.2018.2884203
   List P, 2003, IEEE T CIRC SYST VID, V13, P614, DOI 10.1109/TCSVT.2003.815175
   Liu D, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3368405
   Liu JY, 2020, IEEE T IMAGE PROCESS, V29, P7845, DOI 10.1109/TIP.2020.3007828
   Ma D, 2020, Arxiv, DOI arXiv:2003.13552
   Ma Di, 2020, IEEE J-STSP, V2020
   Ma HC, 2020, IEEE I C VI COM I PR, P403, DOI 10.1109/vcip49819.2020.9301805
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Meng XD, 2018, IEEE DATA COMPR CONF, P187, DOI 10.1109/DCC.2018.00027
   Misra K, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954547
   Nasiri F, 2021, IEEE OPEN J SIGNAL P, V2, P466, DOI 10.1109/OJSP.2021.3092598
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Park Park W.-S. W.-S., 2016, P IM VID MULT SIGN P, P1
   Paszke A, 2019, ADV NEUR IN, V32
   Pennebaker W. B., 1992, JPEG STILL IMAGE DAT
   Rassool R, 2017, IEEE INT SYM BROADB, P351
   Schwarz H, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1929, DOI 10.1109/ICME.2006.262934
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tan MX, 2019, PR MACH LEARN RES, V97
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Tung F, 2020, IEEE T PATTERN ANAL, V42, P568, DOI 10.1109/TPAMI.2018.2886192
   Wang DZ, 2021, IEEE T IMAGE PROCESS, V30, P4198, DOI 10.1109/TIP.2021.3068638
   Wang MZ, 2019, IEEE ACCESS, V7, P145214, DOI 10.1109/ACCESS.2019.2944473
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2021, Arxiv, DOI arXiv:2104.12865
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521
   Yan N, 2019, IEEE T CIRC SYST VID, V29, P840, DOI 10.1109/TCSVT.2018.2816932
   Yang R, 2019, IEEE T CIRC SYST VID, V29, P2039, DOI 10.1109/TCSVT.2018.2867568
   Zhang F, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102912
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P3983, DOI 10.1109/TIP.2018.2830640
   Zhang Linfeng, 2021, IEEE T PATTERN ANAL, V2021
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3150, DOI 10.1109/TIP.2018.2812081
   Zhao L, 2019, IEEE T IMAGE PROCESS, V28, P4832, DOI 10.1109/TIP.2019.2913545
NR 53
TC 5
Z9 5
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
DI 10.1145/3529107
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800009
DA 2024-07-18
ER

PT J
AU Shen, FH
   Liu, J
AF Shen, Feihong
   Liu, Jun
TI Quantum Fourier Convolutional Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Quantum machine learning; convolutional neural network; hybrid
   quantum-classical circuit
ID TRANSFORM
AB The neural network and quantum computing are both significant and appealing fields, with their interactive disciplines promising for large-scale computing tasks that are untackled by conventional computers. However, both developments are restricted by the scope of the hardware development. Nevertheless, many neural network algorithms had been proposed before GPUs became powerful enough for running very deep models. Similarly, quantum algorithms can also be proposed as knowledge reserve before real quantum computers are easily accessible. Specifically, taking advantage of both the neural networks and quantum computation and designing quantum deep neural networks (QDNNs) for acceleration on the Noisy Intermediate-Scale Quantum (NISQ) processors is also an important research problem. As one of the most widely used neural network architectures, convolutional neural network (CNN) remains to be accelerated by quantum mechanisms, with only a few attempts having been demonstrated. In this article, we propose a new hybrid quantum-classical circuit, namely, Quantum Fourier Convolutional Network (QFCN). Our model achieves exponential speedup compared with classical CNN theoretically and improves over the existing best result of quantum CNN. We demonstrate the potential of this architecture by applying it on different deep learning tasks, including traffic prediction and image classification.
C1 [Shen, Feihong] Jilin Univ, Qianjing St, Changchun, Jilin, Peoples R China.
   [Liu, Jun] Singapore Univ Technol & Design, 8 Somapah Rd, Singapore, Singapore.
C3 Jilin University; Singapore University of Technology & Design
RP Shen, FH (corresponding author), Jilin Univ, Qianjing St, Changchun, Jilin, Peoples R China.
EM gregoryfeihong@gmail.com; junliu@sutd.edu.sg
OI Feihong, Shen/0000-0002-5706-8335; Liu, Jun/0000-0002-4365-4165
CR [Anonymous], 2015, Adv Neural Inf Proces Syst
   Biamonte J, 2017, NATURE, V549, P195, DOI 10.1038/nature23474
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Casares Pablo Antonio Moreno, 2020, ARXIV
   Chen HX, 2018, Arxiv, DOI arXiv:1805.08654
   Chitsaz K, 2020, Arxiv, DOI arXiv:2003.12621
   Chuang I., 2000, Quantum Information and Quantum Computation
   Cong I, 2019, NAT PHYS, V15, P1273, DOI 10.1038/s41567-019-0648-8
   Dai HJ, 2018, Arxiv, DOI arXiv:1806.02371
   Dallaire-Demers PL, 2018, PHYS REV A, V98, DOI 10.1103/PhysRevA.98.012324
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dunjko V, 2018, REP PROG PHYS, V81, DOI 10.1088/1361-6633/aab406
   Farhi E, 2014, Arxiv, DOI arXiv:1411.4028
   Ghosh P, 2020, IEEE WINT CONF APPL, P565, DOI 10.1109/WACV45572.2020.9093361
   Hadfield S, 2019, ALGORITHMS, V12, DOI 10.3390/a12020034
   Hales L, 2000, ANN IEEE SYMP FOUND, P515, DOI 10.1109/SFCS.2000.892139
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hibat-Allah M, 2020, PHYS REV RES, V2, DOI 10.1103/PhysRevResearch.2.023358
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jaffe A, 2020, P NATL ACAD SCI USA, V117, P10715, DOI 10.1073/pnas.2002813117
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kerenidis I, 2019, ADV NEUR IN, V32
   Kerenidis I, 2016, Arxiv, DOI [arXiv:1603.08675, DOI 10.48550/ARXIV.1603.08675]
   Kerenidis I, 2020, PHYS REV A, V101, DOI 10.1103/PhysRevA.101.022316
   Kerenidis Iordanis, 2019, INT C LEARNING REPRE
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Li S., 2020, P IEEE CVF C COMP VI, P8705
   Li YG, 2018, Arxiv, DOI [arXiv:1707.01926, DOI 10.48550/ARXIV.1707.01926]
   Lloyd S, 2013, Arxiv, DOI arXiv:1307.0411
   Lloyd S, 2018, PHYS REV LETT, V121, DOI 10.1103/PhysRevLett.121.040502
   Lloyd S, 2014, NAT PHYS, V10, P631, DOI [10.1038/nphys3029, 10.1038/NPHYS3029]
   Lomont C, 2003, Arxiv, DOI arXiv:quant-ph/0309070
   Ma GS, 2019, QUANTUM INF COMPUT, V19, P237
   McClean JR, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07090-4
   McClean JR, 2016, NEW J PHYS, V18, DOI 10.1088/1367-2630/18/2/023023
   Melnikov AA, 2019, NEW J PHYS, V21, DOI 10.1088/1367-2630/ab5c5e
   Moore C, 2006, ACM T ALGORITHMS, V2, DOI 10.1145/1198513.1198525
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Nussbaumer H.J., 1981, FAST FOURIER TRANSFO, P80, DOI DOI 10.1007/978-3-662-00551-4_4
   Park DK, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-40439-3
   Peruzzo A, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5213
   Pratt H, 2017, LECT NOTES ARTIF INT, V10534, P786, DOI 10.1007/978-3-319-71249-9_47
   Preskill J, 2018, QUANTUM-AUSTRIA, V2, DOI 10.22331/q-2018-08-06-79
   Rebentrost P, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.130503
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Reed MA, 2001, APPL PHYS LETT, V78, P3735, DOI 10.1063/1.1377042
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Tannu SS, 2018, Arxiv, DOI arXiv:1805.10224
   Skolik A, 2020, Arxiv, DOI arXiv:2006.14904
   Verdon G, 2019, Arxiv, DOI arXiv:1909.12264
   Wiebe N, 2014, Arxiv, DOI [arXiv:1401.2142, DOI 10.48550/ARXIV.1401.2142]
   WINOGRAD S, 1978, MATH COMPUT, V32, P175, DOI 10.1090/S0025-5718-1978-0468306-4
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Yang CHH, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6523, DOI 10.1109/ICASSP39728.2021.9413453
   Yu B, 2018, Arxiv, DOI arXiv:1709.04875
   Zhao C, 2020, Arxiv, DOI arXiv:1912.12660
   Zhao Pengpeng, 2020, IEEE T KNOWL DATA EN, V34, P5
NR 57
TC 1
Z9 1
U1 14
U2 41
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 13
DI 10.1145/3514249
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400013
DA 2024-07-18
ER

PT J
AU Guo, Y
   Gao, W
   Ma, SW
   Li, G
AF Guo, Yang
   Gao, Wei
   Ma, Siwei
   Li, Ge
TI Accelerating Transform Algorithm Implementation for Efficient Intra
   Coding of 8K UHD Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image and video coding; transform hardware architecture; FPGA
   implementation; ultra-high-definition (UHD) video; 8K dataset
ID INVERSE TRANSFORMS; DCT ARCHITECTURES; GAME-THEORY; APPROXIMATE;
   H.264/AVC; AVS
AB Real-time ultra-high-definition (UHD) video applications have attracted much attention, where the encoder side urgently demands the high-throughput two-dimensional (2D) transform hardware implementation for the latest video coding standards. This article proposes an effective acceleration method for transform algorithm in UHD intra coding based on the third generation of audio video coding standard (AVS3). First, by conducting detailed statistical analysis, we devise an efficient hardware-friendly transform algorithm that can reduce running cycles and resource consumption remarkably. Second, to implement multiplierless computation for saving resources and power, a series of shift-and-add unit (SAU) hardwares are investigated to have much less adoptions of shifters and adders than the existing methods. Third, different types of hardware acceleration methods, including calculation pipelining, logical-loop unrolling, and module-level parallelism, are designed to efficaciously support the data-intensive high frame-rate 8K UHD video coding. Finally, due to the scarcity of 8K video sources, we also provide a new dataset for the performance verification. Experimental results demonstrate that our proposed method can effectively fulfill the real-time 8K intra encoding at beyond 60 fps, with very negligible loss on rate-distortion (R-D) performance, which is averagely 0.98% Bjontegaard-Delta Bit-Rate (BD-BR).
C1 [Guo, Yang; Gao, Wei; Li, Ge] Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Guo, Yang; Gao, Wei] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Ma, Siwei] Peking Univ, Inst Digital Media, Beijing, Peoples R China.
C3 Peking University; Peng Cheng Laboratory; Peking University
RP Gao, W (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.; Gao, W (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.
EM guoyang@stu.pku.edu.cn; gaowei262@pku.edu.cn; swma@pku.edu.cn;
   geli@pku.edu.cn
OI Guo, Yang/0000-0001-5240-657X
FU Ministry of Science and Technology of China - Science and Technology
   Innovations 2030 [2020AAA0103501]; Natural Science Foundation of China
   [61801303, 62031013]; Guangdong Basic and Applied Basic Research
   Foundation [2020A1515012031]; Shenzhen Science and Technology Plan Basic
   Research Project [JCYJ20190808161805519]; Shenzhen Fundamental Research
   Program [GXWD20201231165807007-20200806163656003]
FX This work was supported by Ministry of Science and Technology of China -
   Science and Technology Innovations 2030 (2020AAA0103501), Natural
   Science Foundation of China (61801303 and 62031013), Guangdong Basic and
   Applied Basic Research Foundation (2020A1515012031), Shenzhen Science
   and Technology Plan Basic Research Project (JCYJ20190808161805519),
   Shenzhen Fundamental Research Program (No.
   GXWD20201231165807007-20200806163656003).
CR Abdallah M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3212804
   AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   [Anonymous], 1977, DISCRETE TIME SIGNAL
   Atapattu S, 2016, IEEE INT CONF ASAP, P191, DOI 10.1109/ASAP.2016.7760792
   Audio Video Coding Standard Workgroup, 2021, REF SOFTW AVS3 HIGH
   Audio Video Coding Standard Workgroup, 2021, AVS3 2
   Audio Video Coding Standard Workgroup, 2019, AVS PROP M4772 IMPL
   Bjotegaard G., 2001, VCEGM33
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Cai ZY, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401121
   Chatterjee S, 2018, IEEE T CIRCUITS-II, V65, P2052, DOI 10.1109/TCSII.2018.2815532
   Darji AD, 2015, 2015 19TH INTERNATIONAL SYMPOSIUM ON VLSI DESIGN AND TEST (VDAT)
   Dong XC, 2022, IEEE T MULTIMEDIA, V24, P400, DOI 10.1109/TMM.2021.3052348
   Dutta T, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3002178
   Fan CP, 2014, IEEE T CIRC SYST VID, V24, P714, DOI 10.1109/TCSVT.2013.2277580
   Fan CP, 2011, IEEE T CIRCUITS-II, V58, P517, DOI 10.1109/TCSII.2011.2158749
   Fan K, 2020, IEEE INT CONF MULTI
   Fan YB, 2020, IEEE T CIRC SYST VID, V30, P3289, DOI 10.1109/TCSVT.2019.2934752
   Gao W, 2017, IEEE T IMAGE PROCESS, V26, P6074, DOI 10.1109/TIP.2017.2745099
   Gao W, 2016, IEEE T MULTIMEDIA, V18, P988, DOI 10.1109/TMM.2016.2535254
   Gao W, 2016, IEEE T CIRC SYST VID, V26, P139, DOI 10.1109/TCSVT.2015.2444671
   GUPTA A, 1990, IEEE T ACOUST SPEECH, V38, P553, DOI 10.1109/29.106875
   Imen Werda, IEEE INT C DES TEST, P1, DOI [10.1109/DTS52014.2021.9498196, DOI 10.1109/DTS52014.2021.9498196]
   Ivanov YV, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1671954.1671959
   Jridi M, 2017, IEEE T CIRC SYST VID, V27, P1815, DOI 10.1109/TCSVT.2016.2556578
   Kahu Samruddhi, IEEE INT C IM PROC I, P2039, DOI [10.1109/ ICIP42928.2021.9506126, DOI 10.1109/ICIP42928.2021.9506126]
   Kammoun A, 2020, IEEE T CIRC SYST VID, V30, P4340, DOI 10.1109/TCSVT.2019.2954749
   Kammoun A, 2019, EUR SIGNAL PR CONF, DOI 10.23919/eusipco.2019.8902594
   Kong LC, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3226036
   Lengwehasatit K, 2004, IEEE T CIRC SYST VID, V14, P1236, DOI 10.1109/TCSVT.2004.835151
   Li Lingyu, 2015, 4 INT C MULT TECHN
   Liu Y, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2996461
   Ma SW, 2015, IEEE SIGNAL PROC MAG, V32, P172, DOI 10.1109/MSP.2014.2371951
   Masera M, 2015, CONF DESIGN ARCHIT, P167
   Meher PK, 2014, IEEE T CIRC SYST VID, V24, P168, DOI 10.1109/TCSVT.2013.2276862
   Merhav N, 1997, IEEE T CIRC SYST VID, V7, P468, DOI 10.1109/76.585926
   Pan ZQ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3380827
   Pan ZQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159170
   Panchani Nikuni, 2018, 3 IEEE INT C REC TRE, P724, DOI [10.1109/RTEICT42901.2018.9012109, DOI 10.1109/RTEICT42901.2018.9012109]
   Pao IM, 1999, IEEE T CIRC SYST VID, V9, P608, DOI 10.1109/76.767126
   Park J, 2010, IEEE T VLSI SYST, V18, P787, DOI 10.1109/TVLSI.2009.2016839
   Sharp, 2021, 8C B60A 8K PROF CAMC
   Shen LQ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3313185
   Shengyuan Wu, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P481, DOI 10.1007/978-3-030-67832-6_39
   Su GA, 2008, IEEE T CIRCUITS-II, V55, P1249, DOI 10.1109/TCSII.2008.2008058
   Sullivan G.J., 2014, High Efficiency Video Coding (HEVC)"
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Xilinx, 2021, VIRT ULTR FPGA
   Xilinx, 2021, VIV SIM
   Xilinx, 2021, ULTR FPGA PROD SEL G
   Xilinx, 2021, Ultrascale architecture DSP slice user guide
   Yang H, 2020, IEEE T CIRC SYST VID, V30, P1668, DOI 10.1109/TCSVT.2019.2904198
   Zhang J, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954503
   Zhang Y, 2015, IEEE T IND INFORM, V11, P1492, DOI 10.1109/TII.2015.2491646
   Zhou ML, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107616
   Zhou ML, 2017, J VIS COMMUN IMAGE R, V42, P46, DOI 10.1016/j.jvcir.2016.11.013
   Zong-Yi Chen, 2017, 2017 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW), P215, DOI 10.1109/ICCE-China.2017.7991072
NR 57
TC 2
Z9 2
U1 1
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 113
DI 10.1145/3507970
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600024
DA 2024-07-18
ER

PT J
AU Hui, C
   Liu, SH
   Shi, WZ
   Jiang, F
   Zhao, DB
AF Hui, Chen
   Liu, Shaohui
   Shi, Wuzhen
   Jiang, Feng
   Zhao, Debin
TI Spatio-Temporal Context Based Adaptive Camcorder Recording Watermarking
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video watermarking; camcorder recording; spatio-temporal context;
   geometric distortion
ID DIGITAL IMAGE; ROBUST; HISTOGRAM; COLOR
AB Video watermarking technology has attracted increasing attention in the past few years, and a great deal of traditional and deep learning-based methods have been proposed. However, these existing methods usually suffer from the following two challenges: First, most algorithms cannot resist camcorder recording attack, which limits their practical application. Second, watermark embedding may cause substantial degradation of video quality. Through analyzing the unique distortions presented in the camcorder recording process, including geometric distortion, temporal sampling distortion, sensor distortion and processing distortion, this paper proposes a novel spatio-temporal context based adaptive camcorder recording watermarking scheme STACR. In STACR, considering the geometric distortion and video visual quality, we embed the watermark by constructing a spatio-temporal histogram and incorporate a content features based adaptive locating algorithm to select embedding blocks and embedding strengths. As for the temporal sampling attack, we put forward a watermark correlation-based synchronization algorithm and combine it with cross-validation. Moreover, to resist the sensor distortion, we design a local matching-based algorithm to improve the extraction accuracy. In addition, grouped and repeated embedding strategies are combined to cope with the processing distortion. Experimental results compared with the state-of-the-art show that the proposed scheme achieves high video quality and is robust to geometric attacks, compression, scaling, transcoding, recoding, frame rate changes and especially for camcorder recording.
C1 [Hui, Chen; Liu, Shaohui; Jiang, Feng; Zhao, Debin] Harbin Inst Technol, Sch Comp Sci & Technol, 92 Xidazhi St, Harbin, Peoples R China.
   [Shi, Wuzhen] Shenzhen Univ, Coll Elect & Informat Engn, 3688 Nanhai Ave, Shenzhen, Peoples R China.
C3 Harbin Institute of Technology; Shenzhen University
RP Hui, C (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, 92 Xidazhi St, Harbin, Peoples R China.
EM 20B903013@stu.hit.edu.cn; shliu@hit.edu.cn; wzhshi@szu.edu.cn;
   fjiang@hit.edu.cn; dbzhao@hit.edu.cn
RI Liu, Shaohui/AAC-3092-2019; Zhao, Debin/JEP-0204-2023; JIANG,
   Feng/HTP-2862-2023; hui, chen/JDD-0526-2023
OI Liu, Shaohui/0000-0002-1810-5412; 
FU National Key Research and Development Program of China [2020YFB1406902,
   A12003]; National Science Foundation of China [62101346]; Guangdong
   Basic and Applied Basic Research Foundation [2021A1515011702]; Stable
   Support Plan for Shenzhen Higher Education Institutions
   [20200812104316001]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1406902 and A12003, in
   part by the National Science Foundation of China under Grant 62101346,
   in part by the Guangdong Basic and Applied Basic Research Foundation
   under Grant 2021A1515011702 and in part by the Stable Support Plan for
   Shenzhen Higher Education Institutions under Grant 20200812104316001.
CR Asikuzzaman Md, 2014, IEEE Transactions on Information Forensics and Security, V9, P1502, DOI 10.1109/TIFS.2014.2338274
   Asikuzzaman M, 2018, IEEE T CIRC SYST VID, V28, P2131, DOI 10.1109/TCSVT.2017.2712162
   Asikuzzaman M, 2016, IEEE T MULTIMEDIA, V18, P1733, DOI 10.1109/TMM.2016.2589208
   Baluja S, 2020, IEEE T PATTERN ANAL, V42, P1685, DOI 10.1109/TPAMI.2019.2901877
   Chen CS, 2018, IEEE T CIRC SYST VID, V28, P3300, DOI 10.1109/TCSVT.2017.2741472
   Chen CS, 2016, IEEE T IMAGE PROCESS, V25, P3444, DOI 10.1109/TIP.2016.2573592
   Choi D, 2010, SIGNAL PROCESS, V90, P1327, DOI 10.1016/j.sigpro.2009.10.009
   Do H, 2008, IEEE INT SYMP SIGNAL, P330, DOI 10.1109/ISSPIT.2008.4775680
   Doérr G, 2004, IEEE T SIGNAL PROCES, V52, P2955, DOI 10.1109/TSP.2004.833867
   Dubey N., 2021, Eng Sci, V15, P116, DOI [10.30919/es8d491, DOI 10.30919/ES8D491]
   Fang H, 2019, IEEE T INF FOREN SEC, V14, P1403, DOI 10.1109/TIFS.2018.2878541
   Fridrich J, 2009, IEEE SIGNAL PROC MAG, V26, P26, DOI 10.1109/MSP.2008.931078
   Gunturk BK, 2002, IEEE T IMAGE PROCESS, V11, P997, DOI 10.1109/TIP.2002.801121
   Hadjidemetriou E, 2001, INT J COMPUT VISION, V45, P5, DOI 10.1023/A:1012356022268
   Hartung F, 1999, P SOC PHOTO-OPT INS, V3657, P147, DOI 10.1117/12.344665
   Holliman M, 2000, P SOC PHOTO-OPT INS, V3971, P186, DOI 10.1117/12.384972
   Hui Cao, 2021, 2021 10th Mediterranean Conference on Embedded Computing (MECO), DOI 10.1109/MECO52532.2021.9460222
   Iwata M, 2017, IEICE T INF SYST, VE100D, P24, DOI 10.1587/transinf.2016MUP0008
   Kim W, 2006, LECT NOTES COMPUT SC, V4261, P106
   Kingsbury N, 1999, PHILOS T R SOC A, V357, P2543, DOI 10.1098/rsta.1999.0447
   Kingsbury Nick, 2005, HUMAN VISION
   Langelaar GC, 2000, IEEE SIGNAL PROC MAG, V17, P20, DOI 10.1109/79.879337
   Li L, 2015, J VIS COMMUN IMAGE R, V26, P1, DOI 10.1016/j.jvcir.2014.08.009
   Li Tianxing., 2015, Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services, P197, DOI 10.1145/2742647.2742667
   Liu JC, 2011, OPT ENG, V50, DOI 10.1117/1.3529430
   Lu Jianfeng, 2011, Proceedings of the 2011 Workshop on Digital Media and Digital Content Management (DMDCM 2011), P194, DOI 10.1109/DMDCM.2011.9
   Mareen H, 2021, IEEE T CIRC SYST VID, V31, P3403, DOI 10.1109/TCSVT.2020.3042882
   Miller ML, 2000, LECT NOTES COMPUT SC, V1768, P146
   Parraga CA, 1998, J OPT SOC AM A, V15, P563, DOI 10.1364/JOSAA.15.000563
   Prabakaran G., 2013, Proceedings of the 2013 International Conference on Pattern Recognition, Informatics and Mobile Engineering (PRIME), P251, DOI 10.1109/ICPRIME.2013.6496482
   RECOMMENDATION ITU-R BT, 2002, Methodology for the subjective assessment of the quality of television pictures
   Schaber P, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700295
   Su PC, 2013, IEEE T INF FOREN SEC, V8, P1897, DOI 10.1109/TIFS.2013.2282121
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tancik M, 2020, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR42600.2020.00219
   Nguyen V, 2016, IEEE INFOCOM SER
   Voloshynovskiy S, 2000, PROC SPIE, V3971, P358, DOI 10.1117/12.384990
   Wang YL, 2009, PROCEEDINGS 2009 IEEE INTERNATIONAL WORKSHOP ON OPEN-SOURCE SOFTWARE FOR SCIENTIFIC COMPUTATION, P169, DOI 10.1109/OSSC.2009.5416913
   Watson A. B., 1993, DCC '93. Data Compression Conference (Cat. No.93TH0536-3), P178, DOI 10.1109/DCC.1993.253132
   Xiang SJ, 2008, IEEE T CIRC SYST VID, V18, P777, DOI 10.1109/TCSVT.2008.918843
   Yule Yuan, 2010, Proceedings of the 2010 6th International Conference on Digital Content, Multimedia Technology and its Applications (IDC 2010), P7
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
   Zong TR, 2015, IEEE T CIRC SYST VID, V25, P717, DOI 10.1109/TCSVT.2014.2363743
NR 43
TC 1
Z9 1
U1 3
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 144
DI 10.1145/3503160
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800014
DA 2024-07-18
ER

PT J
AU Zeng, LH
   Tian, XM
AF Zeng, Linghua
   Tian, Xinmei
TI CRAR: Accelerating Stereo Matching with Cascaded Residual Regression and
   Adaptive Refinement
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Stereo matching; depth estimation; real-time algorithm
ID FILTER
AB Dense stereo matching estimates the depth for each pixel of the referenced images. Recently, deep learning algorithms have dramatically promoted the development of stereo matching. The state-of-the-art result is achieved by models adopting deep convolutional neural networks. However, a considerable computational burden is also introduced, which slows the inference. To solve this problem, previous works down-sampled the input images to decrease the spatial size. However, down-sampling increases the error rate and its lower bound. In this article, we accelerate stereo matching algorithms through the improvement of network structure. Inspired by network compression, we conduct decomposition and sparsification to squeeze the computationally expensive cost optimization network. It is sparsified and then decomposed into smaller networks, which are designed and trained in a cascaded manner to reach the nearest possible performance of the larger network. Previous methods have utilized numerous refinement methods to adjust the coarse disparity. We integrate refinement methods to create an unified algorithm to utilize parallelism for running devices to further accelerate the inference. The extensive experiments on Kitti2015, Kitti2012, and Middlebury datasets demonstrate the efficiency of our method.
C1 [Zeng, Linghua; Tian, Xinmei] Univ Sci & Technol China, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Tian, XM (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.
EM zenglh@mail.ustc.edu.cn; xinmei@ustc.edu.cn
FU NSFC [61872329]; National Key R&D Program of China [2017YFB1002203];
   Fundamental Research Funds for the Central Universities [WK3490000005]
FX This work was supported in part by NSFC number 61872329, the National
   Key R&D Program of China under contract number 2017YFB1002203, and the
   Fundamental Research Funds for the Central Universities under contract
   WK3490000005.
CR [Anonymous], 2015, CoRR
   Batsos K, 2018, PROC CVPR IEEE, P2060, DOI 10.1109/CVPR.2018.00220
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Duchowski AT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314309
   Elad M, 2002, IEEE T IMAGE PROCESS, V11, P1141, DOI 10.1109/TIP.2002.801126
   Figurnov M., 2016, ADV NEURAL INFORM PR, V29, P947
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gong YY, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321513
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Jie ZQ, 2018, PROC CVPR IEEE, P3838, DOI 10.1109/CVPR.2018.00404
   KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690
   Khamis S, 2018, LECT NOTES COMPUT SC, V11219, P596, DOI 10.1007/978-3-030-01267-0_35
   Knöbelreiter P, 2017, PROC CVPR IEEE, P1456, DOI 10.1109/CVPR.2017.159
   Kong Dan, 2006, P BRIT MACH VIS C BM, V1
   Kong Dan, 2004, P BRIT MACH VIS C BM, V1
   Kuzmin A, 2017, IEEE INT WORKS MACH
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li LC, 2018, IEEE T CIRC SYST VID, V28, P679, DOI 10.1109/TCSVT.2016.2628782
   Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297
   MATTHIES L, 1989, INT J COMPUT VISION, V3, P209, DOI 10.1007/BF00133032
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Mei X, 2011, PROC CVPR IEEE, P1257
   Nie GY, 2019, PROC CVPR IEEE, P3278, DOI 10.1109/CVPR.2019.00340
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Peris M, 2012, INT C PATT RECOG, P1038
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Schönberger JL, 2018, LECT NOTES COMPUT SC, V11217, P758, DOI 10.1007/978-3-030-01261-8_45
   Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703
   Taniai T, 2018, IEEE T PATTERN ANAL, V40, P2725, DOI 10.1109/TPAMI.2017.2766072
   Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028
   Tulyakov S, 2018, ADV NEUR IN, V31
   Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zeng LH, 2020, IEEE T CYBERNETICS, V50, P452, DOI 10.1109/TCYB.2018.2873762
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
   Zhang FH, 2018, IEEE T IMAGE PROCESS, V27, P822, DOI 10.1109/TIP.2017.2752370
   Zhang K, 2009, IEEE T CIRC SYST VID, V19, P1073, DOI 10.1109/TCSVT.2009.2020478
   Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579
   Zhou WG, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422960
NR 44
TC 2
Z9 2
U1 2
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 74
DI 10.1145/3488719
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600008
DA 2024-07-18
ER

PT J
AU Zhu, XG
   Zhu, Y
   Wang, HY
   Wen, HL
   Yan, Y
   Liu, PL
AF Zhu, Xiaoguang
   Zhu, Ye
   Wang, Haoyu
   Wen, Honglin
   Yan, Yan
   Liu, Peilin
TI Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion
   Network for Action Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Action recognition; neural networks; attention; multi-modality; feature
   fusion
AB Action recognition has been a heated topic in computer vision for its wide application in vision systems. Previous approaches achieve improvement by fusing the modalities of the skeleton sequence and RGB video. However, such methods pose a dilemma between the accuracy and efficiency for the high complexity of the RGB video network. To solve the problem, we propose a multi-modality feature fusion network to combine the modalities of the skeleton sequence and RGB frame instead of the RGB video, as the key information contained by the combination of the skeleton sequence and RGB frame is close to that of the skeleton sequence and RGB video. In this way, complementary information is retained while the complexity is reduced by a large margin. To better explore the correspondence of the two modalities, a two-stage fusion framework is introduced in the network. In the early fusion stage, we introduce a skeleton attention module that projects the skeleton sequence on the single RGB frame to help the RGB frame focus on the limb movement regions. In the late fusion stage, we propose a cross-attention module to fuse the skeleton feature and the RGB feature by exploiting the correlation. Experiments on two benchmarks, NTU RGB+D and SYSU, show that the proposed model achieves competitive performance compared with the state-of-the-art methods while reducing the complexity of the network.
C1 [Zhu, Xiaoguang; Wang, Haoyu; Wen, Honglin; Liu, Peilin] Shanghai Jiao Tong Univ, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Zhu, Ye; Yan, Yan] IIT, 10 West 31st St, Chicago, IL 60616 USA.
C3 Shanghai Jiao Tong University; Illinois Institute of Technology
RP Zhu, XG (corresponding author), Shanghai Jiao Tong Univ, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM zhuxiaoguang178@sjtu.edu.cn; yzhu96@hawk.iit.edu; gogowhy@sjtu.edu.cn;
   linlin00@sjtu.edu.cn; yyan34@iit.edu; liupeilin@sjtu.edu.cn
OI Zhu, Xiaoguang/0000-0001-9554-2133; Wen, Honglin/0000-0001-8263-1399
CR [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2011, AAAI
   Baradel F, 2018, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2018.00056
   Becattini F, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3402447
   Caetano C, 2019, SIBGRAPI, P16, DOI 10.1109/SIBGRAPI.2019.00011
   Cai JM, 2021, IEEE WINT CONF APPL, P2734, DOI 10.1109/WACV48630.2021.00278
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen YP, 2018, LECT NOTES COMPUT SC, V11205, P364, DOI 10.1007/978-3-030-01246-5_22
   Chen YX, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107321
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Ding ZH, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY AND SECURITY COMPANION (QRS-C), P610, DOI 10.1109/QRS-C.2017.134
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Hu JF, 2018, LECT NOTES COMPUT SC, V11211, P346, DOI 10.1007/978-3-030-01234-2_21
   Hu JF, 2017, IEEE T PATTERN ANAL, V39, P2186, DOI 10.1109/TPAMI.2016.2640292
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Huang JQ, 2020, IEEE WINT CONF APPL, P634, DOI [10.1109/wacv45572.2020.9093598, 10.1109/WACV45572.2020.9093598]
   Ji YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P574, DOI 10.1145/3343031.3350959
   Ke QH, 2020, IEEE T IMAGE PROCESS, V29, P959, DOI 10.1109/TIP.2019.2937757
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kipf T. N., 2017, ARXIV
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li JN, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107356
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572
   Li YS, 2019, IEEE INT CON MULTI, P1066, DOI 10.1109/ICME.2019.00187
   Liu JY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3365212
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Pérez-Rúa JM, 2019, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2019.00713
   Rahmani H, 2017, IEEE I CONF COMP VIS, P5833, DOI 10.1109/ICCV.2017.621
   Schindler K, 2008, PROC CVPR IEEE, P3025
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Song SJ, 2018, IEEE T IMAGE PROCESS, V27, P3459, DOI 10.1109/TIP.2018.2818328
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Trabelsi R, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3300937
   Vaezi Joze Hamid Reza, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13286, DOI 10.1109/CVPR42600.2020.01330
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Weng JW, 2018, LECT NOTES COMPUT SC, V11211, P142, DOI 10.1007/978-3-030-01234-2_9
   Wu D, 2014, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2014.98
   Wu QY, 2021, SIGNAL PROCESS-IMAGE, V91, DOI 10.1016/j.image.2020.116098
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xie CY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1639
   Xikun Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14321, DOI 10.1109/CVPR42600.2020.01434
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang JX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321511
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang PF, 2020, IEEE T IMAGE PROCESS, V29, P1061, DOI 10.1109/TIP.2019.2937724
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhao R, 2017, IEEE INT C INT ROBOT, P4260, DOI 10.1109/IROS.2017.8206288
   Zheng Yunpeng, 2020, ACM T MULTIM COMPUT, V15
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
   Zhu XQ, 2021, INNOV SMART GRID TEC, DOI 10.1109/ISGT49243.2021.9372266
   Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316
NR 69
TC 11
Z9 12
U1 3
U2 51
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 80
DI 10.1145/3491228
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Farhat, F
   Kamani, MM
   Wang, JZ
AF Farhat, Farshid
   Kamani, Mohammad Mahdi
   Wang, James Z.
TI CAPTAIN: Comprehensive Composition Assistance for Photo Taking
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image aesthetics; deep learning; image retrieval; recommender system
ID IMAGE; AESTHETICS
AB Many people are interested in taking astonishing photos and sharing them with others. Emerging high-tech hardware and software facilitate the ubiquitousness and functionality of digital photography. Because composition matters in photography, researchers have leveraged some common composition techniques, such as the rule of thirds and the perspective-related techniques, in providing photo-taking assistance. However, composition techniques developed by professionals are far more diverse than well-documented techniques can cover. We present a new approach to leverage the underexplored photography ideas, which are virtually unlimited, diverse, and correlated. We propose a comprehensive fork-join framework, named CAPTAIN (Composition Assistance for Photo Taking,), to guide a photographer with a variety of photography ideas. The framework consists of a few components: integrated object detection, photo genre classification, artistic pose clustering, and personalized aesthetics-aware image retrieval. CAPTAIN is backed by a large managed dataset crawled from a Website with ideas from photography enthusiasts and professionals. The work proposes steps to decompose a given amateurish shot into composition ingredients and compose them to bring the photographer a list of useful and related ideas. The work addresses personal preferences for composition by presenting a user-specified preference list of photography ideas. We have conducted many experiments on the newly proposed components and reported findings. A user study demonstrates that the work is useful to those taking photos.
C1 [Farhat, Farshid] Penn Slate Univ, Sch Elect Engn & Comp Sci, 207 Elect Engn West, University Pk, PA 16802 USA.
   [Kamani, Mohammad Mahdi; Wang, James Z.] Penn Slate Univ, Coll Informat Sci & Technol, E397 Westgate Bldg, University Pk, PA 16802 USA.
RP Farhat, F (corresponding author), Penn Slate Univ, Sch Elect Engn & Comp Sci, 207 Elect Engn West, University Pk, PA 16802 USA.
EM mqk5591@psu.edu; fuf111@psu.edu; jwang@ist.psu.edu
RI Wang, James/JAD-0675-2023
FU National Science Foundation [ACI-1548562]
FX This work used the Extreme Science and Engineering Discovery Environment
   (XSEDE) supported by National Science Foundation grant number
   ACI-1548562.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bhattacharya S., 2010, P 18 ACM INT C MULTI, P271
   Bhattacharya S, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037678
   Cao Y, 2017, PROC CVPR IEEE, P916, DOI 10.1109/CVPR.2017.104
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chang HT, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P927, DOI 10.1145/2733373.2806366
   Chang YY, 2009, IEEE I CONF COMP VIS, P2225, DOI 10.1109/ICCV.2009.5459470
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen WQ, 2018, CHINESE J CANCER RES, V30, P1, DOI 10.21147/j.issn.1000-9604.2018.01.01
   Cho TS, 2008, PROC CVPR IEEE, P2339
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dunn J. C., 1973, Journal of Cybernetics, P32, DOI 10.1080/01969727308546046
   Farhat F, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P17, DOI 10.1145/3126686.3126710
   Guo YW, 2012, COMPUT GRAPH FORUM, V31, P2193, DOI 10.1111/j.1467-8659.2012.03212.x
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He SQ, 2018, IEEE T MULTIMEDIA, V20, P496, DOI 10.1109/TMM.2017.2740026
   Iscen A, 2018, PROC CVPR IEEE, P7632, DOI 10.1109/CVPR.2018.00796
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kamani MM, 2018, APPL SOFT COMPUT, V70, P1154, DOI 10.1016/j.asoc.2017.05.037
   Kamani MM, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P901, DOI 10.1109/BigData.2016.7840685
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Ketchen DJ, 1996, STRATEGIC MANAGE J, V17, P441, DOI 10.1002/(SICI)1097-0266(199606)17:6<441::AID-SMJ819>3.0.CO;2-G
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Krages Bert, 2012, ART COMPOSITION
   Lauer D.A., 2011, Design Basics, V8th
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Li J., 2015, MOBILE CLOUD VISUAL, P113
   Li K, 2015, SIGNAL PROCESS-IMAGE, V39, P509, DOI 10.1016/j.image.2015.07.005
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu L., 2010, Electroactive Polymer Actuators and Devices (EAPAD) 2010, 8 March 2010, P1
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Mai L, 2017, PROC CVPR IEEE, P1121, DOI 10.1109/CVPR.2017.125
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mitro J., 2016, ARXIV160803811
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Ni BB, 2013, IEEE T MULTIMEDIA, V15, P1138, DOI 10.1109/TMM.2013.2241042
   Park J, 2012, IEEE IMAGE PROC, P2741, DOI 10.1109/ICIP.2012.6467466
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Rawat YS, 2020, IEEE T CIRC SYST VID, V30, P2276, DOI 10.1109/TCSVT.2019.2915103
   Rawat YS, 2018, IEEE T MULTIMEDIA, V20, P754, DOI 10.1109/TMM.2017.2750420
   Rawat YS, 2017, IEEE T CIRC SYST VID, V27, P149, DOI 10.1109/TCSVT.2016.2555658
   Rawat YS, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P641, DOI 10.1145/2733373.2807992
   Rawat YS, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P217, DOI 10.1145/2647868.2656409
   Rawat YS, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808199
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Samii A, 2015, COMPUT GRAPH FORUM, V34, P141, DOI 10.1111/cgf.12465
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Stentiford F., 2007, P WORKSH COMP ATT AP
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Valenzuela R., 2012, Picture Perfect Practice: A Self-training Guide to Mastering the Challenges of Taking World-class Photographs
   Wang Patricia P., 2008, 2008 IEEE 10th Workshop on Multimedia Signal Processing (MMSP), P6, DOI 10.1109/MMSP.2008.4665040
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   Wong LK, 2009, IEEE IMAGE PROC, P997, DOI 10.1109/ICIP.2009.5413825
   Xie JY, 2016, PR MACH LEARN RES, V48
   Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130
   Yeh CH, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2584105
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Zhang M., 2005, P IEEE C MULT EXP
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou ZH, 2017, IEEE T MULTIMEDIA, V19, P2651, DOI 10.1109/TMM.2017.2703954
NR 78
TC 0
Z9 0
U1 3
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 14
DI 10.1145/3462762
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Mao, AH
   Liang, Y
   Jiao, JB
   Liu, YT
   He, SF
AF Mao, Aihua
   Liang, Yuan
   Jiao, Jianbo
   Liu, Yongtuo
   He, Shengfeng
TI Mask-Guided Deformation Adaptive Network for Human Parsing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human parsing; multi-task learning; deformable convolution
ID PERSON REIDENTIFICATION; SINGLE; MODELS
AB Due to the challenges of densely compacted body parts, nonrigid clothing items, and severe overlap in crowd scenes, human parsing needs to focus more on multilevel feature representations compared to general scene parsing tasks. Based on this observation, we propose to introduce the auxiliary task of human mask and edge detection to facilitate human parsing. Different from human parsing, which exploits the discriminative features of each category, human mask and edge detection emphasizes the boundaries of semantic parsing regions and the difference between foreground humans and background clutter, which benefits the parsing predictions of crowd scenes and small human parts. Specifically, we extract human mask and edge labels from the human parsing annotations and train a shared encoder with three independent decoders for the three mutually beneficial tasks. Furthermore, the decoder feature maps of the human mask prediction branch are further exploited as attention maps, indicating human regions to facilitate the decoding process of human parsing and human edge detection. In addition to these auxiliary tasks, we further alleviate the problem of deformed clothing items under various human poses by tracking the deformation patterns with the deformable convolution. Extensive experiments show that the proposed method can achieve superior performance against state-of-the-art methods on both single and multiple human parsing datasets. Codes and trained models are available https://github.com/ViktorLiang/MGDAN.
C1 [Mao, Aihua; Liang, Yuan; Liu, Yongtuo; He, Shengfeng] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Jiao, Jianbo] Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.
C3 South China University of Technology; University of Oxford
RP He, SF (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM ahmao@scut.edu.cn; yuanliang07@gmail.com; jianbo@robots.ox.ac.uk;
   csmanlyt@mail.scut.edu.cn; hesfe@scut.edu.cn
RI He, Shengfeng/E-5682-2016
OI He, Shengfeng/0000-0002-3802-4644; Jiao, Jianbo/0000-0003-0833-5115
FU National Natural Science Foundation of China [61972162]; Guangdong
   International Science and Technology Cooperation Project
   [2021A0505030009]; Guangdong Natural Science Foundation
   [2019A1515010833, 2021A1515012625]; Guangzhou Basic and Applied Research
   Project [202102021074]; Fundamental Research Funds for the Central
   Universities [2020ZYGXZR089]; Social Science Research Base of Guangdong
   Province-Research Center of Network Civilization in New Era of SCUT;
   CCF-Tencent Open Research fund [CCF-Tencent RAGR20190112]
FX This project is supported by the National Natural Science Foundation of
   China under Grant No.:61972162; Guangdong International Science and
   Technology Cooperation Project (No. 2021A0505030009); Guangdong Natural
   Science Foundation (No. 2019A1515010833, 2021A1515012625); Guangzhou
   Basic and Applied Research Project (No. 202102021074); the Fundamental
   Research Funds for the Central Universities (No. 2020ZYGXZR089); the
   Social Science Research Base of Guangdong Province-Research Center of
   Network Civilization in New Era of SCUT; and the CCF-Tencent Open
   Research fund under Grant No.: CCF-Tencent RAGR20190112.
CR Bilinski P, 2018, PROC CVPR IEEE, P6596, DOI 10.1109/CVPR.2018.00690
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2016, PROC CVPR IEEE, P4545, DOI 10.1109/CVPR.2016.492
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Fang HS, 2018, PROC CVPR IEEE, P70, DOI 10.1109/CVPR.2018.00015
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Gong K, 2019, PROC CVPR IEEE, P7442, DOI [10.1109/cvpr.2019.00763, 10.1109/CVPR.2019.00763]
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   He Haoyu, 2020, P AAAI C ART INT AAA
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Li YW, 2019, PROC CVPR IEEE, P7019, DOI 10.1109/CVPR.2019.00719
   Liang XD, 2018, IEEE T PATTERN ANAL, V40, P2978, DOI 10.1109/TPAMI.2017.2775623
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Lin GS, 2020, IEEE T PATTERN ANAL, V42, P1228, DOI 10.1109/TPAMI.2019.2893630
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu S, 2015, PROC CVPR IEEE, P1419, DOI 10.1109/CVPR.2015.7298748
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Luo P, 2013, IEEE I CONF COMP VIS, P2648, DOI 10.1109/ICCV.2013.329
   Luo YW, 2018, LECT NOTES COMPUT SC, V11213, P424, DOI 10.1007/978-3-030-01240-3_26
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Quispe R, 2019, IMAGE VISION COMPUT, V92, DOI 10.1016/j.imavis.2019.07.009
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Ruyi Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P205, DOI 10.1007/978-3-030-58601-0_13
   Shahroudy A, 2016, IEEE T PATTERN ANAL, V38, P2123, DOI 10.1109/TPAMI.2015.2505295
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Tao Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9260, DOI 10.1109/CVPR42600.2020.00928
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3508, DOI 10.1109/TPAMI.2021.3055780
   Wang WG, 2019, IEEE I CONF COMP VIS, P5702, DOI 10.1109/ICCV.2019.00580
   Wang Y, 2012, J MACH LEARN RES, V13, P3075
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Xia FT, 2016, AAAI CONF ARTIF INTE, P3632
   Xiaomei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8968, DOI 10.1109/CVPR42600.2020.00899
   Xie SN, 2017, INT J COMPUT VISION, V125, P3, DOI 10.1007/s11263-017-1004-z
   Xiong YW, 2019, PROC CVPR IEEE, P8810, DOI 10.1109/CVPR.2019.00902
   Yamaguchi K, 2015, IEEE T PATTERN ANAL, V37, P1028, DOI 10.1109/TPAMI.2014.2353624
   Yu ZD, 2017, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2017.191
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao J, 2017, IEEE COMPUT SOC CONF, P1595, DOI 10.1109/CVPRW.2017.204
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhu Bingke, 2018, P AAAI C ART INT AAA
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 54
TC 1
Z9 1
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 11
DI 10.1145/3467889
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900011
OA Green Published
DA 2024-07-18
ER

PT J
AU Liu, SG
   Wang, HX
   Zhang, XL
AF Liu, Shiguang
   Wang, Huixin
   Zhang, Xiaoli
TI Video Decolorization Based on the CNN and LSTM Neural Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video decolorization; convolution neural network; RNN; LSTM; temporal
   consistency
ID IMAGE DECOLORIZATION; COLOR; CONVERSION
AB Video decolorization is the process of transferring three-channel color videos into single-channel grayscale videos, which is essentially the decolorization operation of video frames. Most existing video decolorization algorithms directly apply image decolorization methods to decolorize video frames. However, if we only take the single-frame decolorization result into account, it will inevitably cause temporal inconsistency and flicker phenomenon meaning that the same local content between continuous video frames may display different gray values. In addition, there are often similar local content features between video frames, which indicates redundant information. To solve the preceding problems, this article proposes a novel video decolorization algorithm based on the convolutional neural network and the long short-term memory neural network. First, we design a local semantic content encoder to learn and extract the same local content of continuous video frames, which can better preserve the contrast of video frames. Second, a temporal feature controller based on the bi-directional recurrent neural networks with Long short-term memory units is employed to refine the local semantic features, which can greatly maintain temporal consistency of the video sequence to eliminate the flicker phenomenon. Finally, we take advantages of deconvolution to decode the features to produce the grayscale video sequence. Experiments have indicated that our method can better preserve the local contrast of video frames and the temporal consistency over the state of the-art.
C1 [Liu, Shiguang; Wang, Huixin; Zhang, Xiaoli] Tianjin Univ, Sch Comp Sci & Technol, Div Intelligence & Comp, 135 Yaguan Rd,Haihe Educ Pk, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Div Intelligence & Comp, 135 Yaguan Rd,Haihe Educ Pk, Tianjin 300350, Peoples R China.
EM lsg@tju.edu.cn; wanghuixin_hx@foxmail.com; zxl218063@tju.edu.cn
FU Natural Science Foundation of China [62072328, 61672375]
FX This work was partly supported by the Natural Science Foundation of
   China under grants 62072328 and 61672375.
CR Ancuti CO, 2011, LECT NOTES COMPUT SC, V6492, P79, DOI 10.1007/978-3-642-19315-6_7
   Ancuti CO, 2011, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2011.5995414
   [Anonymous], 2007, PROC 3 EUR C COMPUTA
   [Anonymous], 2010, P AS C COMP VIS
   [Anonymous], 2018, ACMMM, DOI DOI 10.1145/3240508.3240708
   Bala R, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P82
   Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126
   Chen K, 2016, IEEE-ACM T AUDIO SPE, V24, P1185, DOI 10.1109/TASLP.2016.2539499
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gooch AA, 2005, ACM T GRAPHIC, V24, P634, DOI 10.1145/1073204.1073241
   Grundland M, 2007, PATTERN RECOGN, V40, P2891, DOI 10.1016/j.patcog.2006.11.003
   Hochreiter S, 1997, ADV NEUR IN, V9, P473
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Ji ZP, 2016, VISUAL COMPUT, V32, P1621, DOI 10.1007/s00371-015-1145-4
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kim Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618507
   Kuk JG, 2011, LECT NOTES COMPUT SC, V6495, P513, DOI 10.1007/978-3-642-19282-1_41
   Liu QG, 2019, INFORM FUSION, V46, P114, DOI 10.1016/j.inffus.2018.05.007
   Liu QG, 2017, IEEE T IMAGE PROCESS, V26, P5772, DOI 10.1109/TIP.2017.2745104
   Liu QG, 2017, IEEE T CIRC SYST VID, V27, P1856, DOI 10.1109/TCSVT.2016.2555779
   Liu QG, 2015, IEEE T IMAGE PROCESS, V24, P2889, DOI 10.1109/TIP.2015.2423615
   Liu SG, 2019, IEEE T MULTIMEDIA, V21, P2461, DOI 10.1109/TMM.2019.2903413
   Lu CW, 2014, INT J COMPUT VISION, V110, P222, DOI 10.1007/s11263-014-0732-6
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Smith K, 2008, COMPUT GRAPH FORUM, V27, P193, DOI 10.1111/j.1467-8659.2008.01116.x
   Song ML, 2010, IEEE T PATTERN ANAL, V32, P1537, DOI 10.1109/TPAMI.2009.74
   Song Y., 2013, Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on, P1, DOI DOI 10.1109/APSIPA.2013.6694333
   Song YB, 2014, IEEE WINT CONF APPL, P159, DOI 10.1109/WACV.2014.6836106
   Sutskever Ilya, 2014, P 27 INT C NEURAL IN, P3104
   Tao YZ, 2018, IEEE T CYBERNETICS, V48, P1406, DOI 10.1109/TCYB.2017.2695655
   Tao Yizhang, 2016, P 33 INT COMP GRAPH, P41
   Wu J., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Zhang HK, 2019, IEEE I CONF COMP VIS, P1725, DOI 10.1109/ICCV.2019.00181
   Zhang HC, 2017, LECT NOTES COMPUT SC, V10667, P560, DOI 10.1007/978-3-319-71589-6_49
   Zhang XL, 2018, VISUAL COMPUT, V34, P1099, DOI 10.1007/s00371-018-1524-8
   Zhao HL, 2018, COMPUT GRAPH-UK, V70, P251, DOI 10.1016/j.cag.2017.07.009
NR 39
TC 2
Z9 2
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 88
DI 10.1145/3446619
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400012
DA 2024-07-18
ER

PT J
AU Zhang, DL
   Wu, XJ
   Yu, J
AF Zhang, Donglin
   Wu, Xiao-Jun
   Yu, Jun
TI Label Consistent Flexible Matrix Factorization Hashing for Efficient
   Cross-modal Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Hashing; cross-modal retrieval; flexible matrix factorization
AB Hashing methods have sparked a great revolution on large-scale cross-media search due to its effectiveness and efficiency. Most existing approaches learn unified hash representation in a common Hamming space to represent all multimodal data. However, the unified hash codes may not characterize the cross-modal data discriminatively, because the data may vary greatly due to its different dimensionalities, physical properties, and statistical information. In addition, most existing supervised cross-modal algorithms preserve the similarity relationship by constructing an n x n pairwise similarity matrix, which requires a large amount of calculation and loses the category information. To mitigate these issues, a novel cross-media hashing approach is proposed in this article, dubbed label flexible matrix factorization hashing (LFMH). Specifically, LFMH jointly learns the modality-specific latent subspace with similar semantic by the flexible matrix factorization. In addition, LFMH guides the hash learning by utilizing the semantic labels directly instead of the large n x n pairwise similarity matrix. LFMH transforms the heterogeneous data into modality-specific latent semantic representation. Therefore, we can obtain the hash codes by quantifying the representations, and the learned hash codes are consistent with the supervised labels of multimodal data. Then, we can obtain the similar binary codes of the corresponding modality, and the binary codes can characterize such samples flexibly. Accordingly, the derived hash codes have more discriminative power for single-modal and cross-modal retrieval tasks. Extensive experiments on eight different databases demonstrate that our model outperforms some competitive approaches.
C1 [Zhang, Donglin; Wu, Xiao-Jun; Yu, Jun] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, 1800 Lihu Ave, Wuxi 214122, Jiangsu, Peoples R China.
C3 Jiangnan University
RP Wu, XJ (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, 1800 Lihu Ave, Wuxi 214122, Jiangsu, Peoples R China.
EM dlinzzhang@gmail.com; wu_xiaojun@jiangnan.edu.cn; yujunjason@aliyun.com
OI Zhang, Donglin/0000-0002-0716-0918
FU National Natural Science Foundation of China [61672265, U1836218]; 111
   Project of Chinese Ministry of Education [B12018]
FX This research was supported by the National Natural Science Foundation
   of China (Grant nos. 61672265, U1836218) and the 111 Project of Chinese
   Ministry of Education under Grant B12018.
CR [Anonymous], 2013, Book Linear cross-modal hashing for efficient multimedia search, DOI DOI 10.1145/2502081.2502107
   Baeza-Yates R., 1999, MODERN INFORM RETRIE, V463
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   FEIWANG PC, 2012, INFORM RETRIEVAL, V15, P179
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lin ZJ, 2017, IEEE T CYBERNETICS, V47, P4342, DOI 10.1109/TCYB.2016.2608906
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Liu LC, 2018, LECT NOTES COMPUT SC, V10828, P606, DOI 10.1007/978-3-319-91458-9_37
   Liu SW, 2014, COMPUT VIS IMAGE UND, V118, P30, DOI 10.1016/j.cviu.2013.06.011
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu X, 2018, MULTIMED TOOLS APPL, V77, P28665, DOI 10.1007/s11042-018-6006-5
   Mandal D, 2017, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR.2017.282
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rastegari Mohammad, 2013, Book Predictable Dual-View Hashing
   Rupnik J., 2010, P C DATA MINING DATA, P1
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Seewald AK, 2005, DIGITS DATASET HANDW
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Shu X, 2011, IMAGE VISION COMPUT, V29, P286, DOI 10.1016/j.imavis.2010.11.001
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Tang J, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2564638
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang D, 2018, IEEE T CIRC SYST VID, V28, P2703, DOI 10.1109/TCSVT.2017.2723302
   Wang D, 2019, IEEE T PATTERN ANAL, V41, P2466, DOI 10.1109/TPAMI.2018.2861000
   Wang D, 2016, IEEE T IMAGE PROCESS, V25, P4540, DOI 10.1109/TIP.2016.2592800
   Wang J., 2014, CoRR
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Wu Xiao-Jun, 2013, P CONST INT WORKSH S
   Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yang YL, 2014, PROCEEDINGS OF THE ASME 4TH INTERNATIONAL CONFERENCE ON MICRO/NANOSCALE HEAT AND MASS TRANSFER - 2013
   Yu Z, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P395, DOI 10.1145/2600428.2609563
   Zhong FM, 2018, PATTERN RECOGN, V83, P64, DOI 10.1016/j.patcog.2018.05.018
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
NR 45
TC 28
Z9 29
U1 1
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 90
DI 10.1145/3446774
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400014
DA 2024-07-18
ER

PT J
AU Yang, X
   Ma, ZL
   Yu, LT
   Cao, Y
   Yin, BC
   Wei, XP
   Zhang, Q
   Lau, RWH
AF Yang, Xin
   Ma, Zongliang
   Yu, Letian
   Cao, Ying
   Yin, Baocai
   Wei, Xiaopeng
   Zhang, Qiang
   Lau, Rynson W. H.
TI Automatic Comic Generation with Stylistic Multi-page Layouts and
   Emotion-driven Text Balloon Generation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Automatic; comic books; keyframes; stylizing; multi-page; layout
AB In this article, we propose a fully automatic system for generating comic books from videos without any human intervention. Given an input video along with its subtitles, our approach first extracts informative keyframes by analyzing the subtitles and stylizes keyframes into comic-style images. Then, we propose a novel automatic multi-page layout framework that can allocate the images across multiple pages and synthesize visually interesting layouts based on the rich semantics of the images (e.g., importance and inter-image relation). Finally, as opposed to using the same type of balloon as in previous works, we propose an emotionaware balloon generation method to create different types of word balloons by analyzing the emotion of subtitles and audio. Our method is able to vary balloon shapes and word sizes in balloons in response to different emotions, leading to more enriched reading experience. Once the balloons are generated, they are placed adjacent to their corresponding speakers via speaker detection. Our results show that our method, without requiring any user inputs, can generate high-quality comic pages with visually rich layouts and balloons. Our user studies also demonstrate that users prefer our generated results over those by state-of-the-art comic generation systems.
C1 [Yang, Xin; Ma, Zongliang; Yu, Letian; Yin, Baocai; Wei, Xiaopeng; Zhang, Qiang] Dalian Univ Technol, Dept Comp Sci, 2 Linggong Rd, Dalian 116024, Liaoning, Peoples R China.
   [Cao, Ying; Lau, Rynson W. H.] City Univ Hong Kong, Hong Kong, Peoples R China.
C3 Dalian University of Technology; City University of Hong Kong
RP Cao, Y (corresponding author), City Univ Hong Kong, Hong Kong, Peoples R China.
EM xinyang@dlut.edu.cn; liangongma1997@mail.dlut.edu.cn;
   yuley3012@mail.dlut.edu.cn; caoying59@gmail.com; ybc@dlut.edu.cn;
   weixp@dlut.edu.cn; zhangq@dlut.edu.cn; rynson.lau@cityu.edu.hk
RI zhang, qiang/HZJ-9551-2023; zhang, lin/IZQ-4870-2023; wei,
   xiao/ISB-6027-2023; jiang, lei/IWE-1124-2023; Zhang,
   Qiang/GXF-3105-2022; Zhang, Qiang/IWU-5000-2023; Jiang,
   Tao/IWM-7503-2023
OI Zhang, Qiang/0000-0003-3776-9799; , Xin/0000-0002-8046-722X
FU National Natural Science Foundation of China [91748104, 61972067,
   61632006, U1811463, U1908214, 61751203]; National Key Research and
   Development Program of China [2018AAA0102003, 2018YFC0910506];
   Innovation Technology Funding of Dalian [2020JJ26GX036]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 91748104, Grant 61972067, Grant
   61632006, Grant U1811463, Grant U1908214, Grant 61751203, in part by the
   National Key Research and Development Program of China under Grant
   2018AAA0102003, Grant 2018YFC0910506, in part by the Innovation
   Technology Funding of Dalian (Project No. 2020JJ26GX036).
CR [Anonymous], 2010, P 18 ACM INT C MULTI, DOI [DOI 10.1145/1873951.1874033, 10.1145/1873951.1874033]
   Buitelaar P, 2018, IEEE T MULTIMEDIA, V20, P2454, DOI 10.1109/TMM.2018.2798287
   Cao Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366160
   Chen SZ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2236, DOI 10.1145/3343031.3350571
   Chu WT, 2015, IEEE T MULTIMEDIA, V17, P201, DOI 10.1109/TMM.2014.2383616
   Filippova Katja, 2013, P C INT C COMP LING, P322
   Forceville C., 2010, BALLOONICS VISUALS B, P232
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gu SY, 2018, PROC CVPR IEEE, P8222, DOI 10.1109/CVPR.2018.00858
   Harik GR, 1999, IEEE T EVOLUT COMPUT, V3, P287, DOI 10.1109/4235.797971
   Jing GM, 2015, IEEE T MULTIMEDIA, V17, P2122, DOI 10.1109/TMM.2015.2474263
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kurlander D., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P225, DOI 10.1145/237170.237260
   Li YT, 2019, PROC CVPR IEEE, P6322, DOI 10.1109/CVPR.2019.00649
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Preu Jacqueline, 2007, P ACM ASS COMP MACH, P99
   Qu Zhong, 2013, J SOFTW, V8, P7
   Ravi H, 2018, PROC CVPR IEEE, P7613, DOI 10.1109/CVPR.2018.00794
   Ryu DS, 2008, 8TH IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY WORKSHOPS: CIT WORKSHOPS 2008, PROCEEDINGS, P336, DOI 10.1109/CIT.2008.Workshops.42
   Sawada T., 2013, INT C MULT MOD, P467
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Toyoura M, 2012, LECT NOTES COMPUT SC, V7131, P406
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P858, DOI 10.1109/TMM.2012.2187181
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Winnemoller H., 2011, P ACM SIGGRAPH EUR S, P147
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 28
TC 3
Z9 4
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 55
DI 10.1145/3440053
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jiang, SQ
   Min, WQ
   Lyu, YQ
   Liu, LH
AF Jiang, Shuqiang
   Min, Weiqing
   Lyu, Yongqiang
   Liu, Linhu
TI Few-shot Food Recognition via Multi-view Representation Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Food recognition; few-shot learning; visual recognition; deep learning
AB This article considers the problem of few-shot learning for food recognition. Automatic food recognition can support various applications, e.g., dietary assessment and food journaling. Most existing works focus on food recognition with large numbers of labelled samples, and fail to recognize food categories with few samples. To address this problem, we propose a Multi-View Few-Shot Learning (MVFSL) framework to explore additional ingredient information for few-shot food recognition. Besides category-oriented deep visual features, we introduce ingredient-supervised deep network to extract ingredient-oriented features. As general and intermediate attributes of food, ingredient-oriented features are informative and complementary to category-oriented features, and thus they play an important role in improving food recognition. Particularly in few-shot food recognition, ingredient information can bridge the gap between disjoint training categories and test categories. To take advantage of ingredient information, we fuse these two kinds of features by first combining their feature maps from their respective deep networks and then convolving combined feature maps. Such convolution is further incorporated into a multi-view relation network, which is capable of comparing pairwise images to enable fine-grained feature learning. MVFSL is trained in an end-to-end fashion for joint optimization on two types of feature learning subnetworks and relation subnetworks. Extensive experiments on different food datasets have consistently demonstrated the advantage of MVFSL in multi-view feature fusion. Furthermore, we extend another two types of networks, namely, Siamese Network and Matching Network, by introducing ingredient information for few-shot food recognition. Experimental results have also shown that introducing ingredient information into these two networks can improve the performance of few-shot food recognition.
C1 [Jiang, Shuqiang; Min, Weiqing; Liu, Linhu] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.
   [Lyu, Yongqiang] Qingdao KingAgroot Precis Agr Technol Co Ltd, Qingdao, Peoples R China.
   [Lyu, Yongqiang] Shandong Reebow Automat Equipment Co LTD, Qingdao Branch, Room 1901,Bldg 5, Qingdao, Shandong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.
EM sqjiang@ict.a.c.cn; weiqingmin@ict.ac.cn; lvyq@reebow.cn;
   linhu.liu@vipl.ict.ac.cn
FU National Natural Science Foundation of China [61532018, 61972378,
   U19B2040]; Beijing Natural Science Foundation [L182054]; National
   Program for Special Support of Eminent Professionals; National Program
   for Support of Top-notch Young Professionals
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants No. 61532018, No. 61972378, and No.
   U19B2040, in part by Beijing Natural Science Foundation under Grant No.
   L182054, in part by National Program for Special Support of Eminent
   Professionals and National Program for Support of Top-notch Young
   Professionals
CR Aizawa K, 2013, IEEE T MULTIMEDIA, V15, P2176, DOI 10.1109/TMM.2013.2271474
   Amato G, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1333, DOI 10.1145/3077136.3084142
   Andrychowicz M, 2016, ADV NEUR IN, V29
   [Anonymous], DRUG DEV IND PHARM
   [Anonymous], 2016, CORR
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], 2017, Meta networks
   Ao S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOP (ICDMW), P1196, DOI 10.1109/ICDMW.2015.203
   Beijbom O, 2015, IEEE WINT CONF APPL, P844, DOI 10.1109/WACV.2015.117
   Bertinetto L., 2016, Advances in neural information processing systems, P523
   Bettadapura V, 2015, IEEE WINT CONF APPL, P580, DOI 10.1109/WACV.2015.83
   Bolaños M, 2017, LECT NOTES COMPUT SC, V10590, P394, DOI 10.1007/978-3-319-70742-6_37
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen X., 2017, arXiv preprint arXiv:1705.02743
   Cheng G, 2016, IEEE T GEOSCI REMOTE, V54, P7405, DOI 10.1109/TGRS.2016.2601622
   Dehais J, 2017, IEEE T MULTIMEDIA, V19, P1090, DOI 10.1109/TMM.2016.2642792
   Deng LX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P112, DOI 10.1145/3343031.3351147
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Han JW, 2015, IEEE T GEOSCI REMOTE, V53, P3325, DOI 10.1109/TGRS.2014.2374218
   Han ZZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P766
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   Han Zhizhong., 2019, IEEE T IMAGE PROCESS, V28, P1941
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Kagaya H, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1085, DOI 10.1145/2647868.2654970
   Kawano Yoshiyuki, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8326, P369, DOI 10.1007/978-3-319-04117-9_38
   Kawano Y, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P589, DOI 10.1145/2638728.2641339
   Kingma D. P., 2014, arXiv
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Lake B., 2011, P ANN M COGNITIVE SC, P1
   Lake B. M., 2013, Adv. Neural Inf. Proces. Syst., P2526, DOI DOI 10.5555/2999792.2999894
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Liu YH, 2019, INT J PSYCHIAT CLIN, V23, P164, DOI 10.1080/13651501.2019.1569238
   Lu YZ, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7020189
   Marin J, 2021, IEEE T PATTERN ANAL, V43, P187, DOI 10.1109/TPAMI.2019.2927476
   Martinel N, 2018, IEEE WINT CONF APPL, P567, DOI 10.1109/WACV.2018.00068
   Martinel N, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P484, DOI 10.1109/ICCVW.2015.70
   Mesas AE, 2012, OBES REV, V13, P106, DOI 10.1111/j.1467-789X.2011.00936.x
   Min W., 2019, P ACM INT C MULT, P99
   Min WQ, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329168
   Min WQ, 2018, IEEE T MULTIMEDIA, V20, P950, DOI 10.1109/TMM.2017.2759499
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Ota K, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092831
   Pouladzadeh Parisa, 2017, ACM T MULTIM COMPUT, V13, p3s
   Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J., 2017, ADV NEURAL INFORM PR, V30, P4077
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tanno R, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P89, DOI 10.1145/2986035.2986044
   Thrun S, 1998, LEARNING TO LEARN, P181
   Vinyals O., 2016, ADV NEURAL INFORM PR, P3630, DOI DOI 10.48550/ARXIV.1606.04080
   Xu RH, 2015, IEEE T MULTIMEDIA, V17, P1187, DOI 10.1109/TMM.2015.2438717
   Yang S, 2010, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2010.5539907
   Zeiler MatthewD., 2013, CORR
   Zhang D, 2017, J MAT CHEM A, V5, P39
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zheng JN, 2017, SUSTAINABILITY-BASEL, V9, DOI 10.3390/su9050856
NR 66
TC 24
Z9 25
U1 1
U2 45
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 87
DI 10.1145/3391624
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200013
DA 2024-07-18
ER

PT J
AU Zheng, ZD
   Zheng, L
   Garrett, M
   Yang, Y
   Xu, ML
   Shen, YD
AF Zheng, Zhedong
   Zheng, Liang
   Garrett, Michael
   Yang, Yi
   Xu, Mingliang
   Shen, Yi-Dong
TI Dual-path Convolutional Image-Text Embeddings with Instance Loss
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image-sentence retrieval; cross-modal retrieval; language-based person
   search; convolutional neural networks
ID PERSON REIDENTIFICATION
AB Matching images and sentences demands a fine understanding of both modalities. In this article, we propose a new system to discriminatively embed the image and text to a shared visual-textual space. In this field, most existing works apply the ranking loss to pull the positive image/text pairs close and push the negative pairs apart from each other. However, directly deploying the ranking loss on heterogeneous features (i.e., text and image features) is less effective, because it is hard to find appropriate triplets at the beginning. So the naive way of using the ranking loss may compromise the network from learning inter-modal relationship. To address this problem, we propose the instance loss, which explicitly considers the intra-modal data distribution. It is based on an unsupervised assumption that each image/text group can be viewed as a class. So the network can learn the fine granularity from every image/text group. The experiment shows that the instance loss offers better weight initialization for the ranking loss, so that more discriminative embeddings can be learned. Besides, existing works usually apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So in a minor contribution, this article constructs an end-to-end dual-path convolutional network to learn the image and text representations. End-to-end learning allows the system to directly learn from the data and fully utilize the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO), experiments demonstrate that our method yields competitive accuracy compared to state-of-the-art methods. Moreover, in language-based person retrieval, we improve the state of the art by a large margin. The code has been made publicly available.
C1 [Zheng, Zhedong; Yang, Yi] Univ Technol Sydney, Ctr Artificial Intelligence, 15 Broadway, Ultimo, NSW 2007, Australia.
   [Zheng, Liang] Australian Natl Univ, Room N214,ANU Campus, Canberra, ACT 2601, Australia.
   [Garrett, Michael] CingleVue Int Australia, 270 Joondalup Dr, Joondalup, WA 6027, Australia.
   [Garrett, Michael] Edith Cowan Univ, 270 Joondalup Dr, Joondalup, WA 6027, Australia.
   [Xu, Mingliang] Zhengzhou Univ, 100 Kexue Ave, Zhengzhou, Henan, Peoples R China.
   [Shen, Yi-Dong] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, 4th Zhongguancun South Fourth St, Beijing, Peoples R China.
C3 University of Technology Sydney; Australian National University; Edith
   Cowan University; Zhengzhou University; Chinese Academy of Sciences;
   Institute of Software, CAS
RP Zheng, ZD (corresponding author), Univ Technol Sydney, Ctr Artificial Intelligence, 15 Broadway, Ultimo, NSW 2007, Australia.
EM Zhedong.Zheng@student.uts.edu.au; liang.zheng@anu.edu.au;
   michael.garrett@cinglevue.com; Yi.Yang@uts.edu.au;
   iexumingliang@zzu.edu.cn; ydshen@ios.ac.cn
RI Shen, Yi/GRS-3602-2022; Lang, Ming/HIK-0758-2022; yang,
   yang/HGT-7999-2022; Zheng, Zhedong/R-5314-2019; yang,
   yang/GWB-9426-2022; Yang, Yi/B-9273-2017; yang, yang/GVT-5210-2022;
   wang, Xiaoming/KBB-8854-2024
OI Zheng, Zhedong/0000-0002-2434-9050; Yang, Yi/0000-0002-0512-880X;
   Garrett, Michael/0000-0003-0352-5008; Zheng, Liang/0000-0002-1464-9500
CR [Anonymous], 2015, P ICLR
   [Anonymous], 2017, ARXIV170403470
   [Anonymous], 2016, ARXIV
   [Anonymous], 2014, P EUR C COMP VIS
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, P CVPR
   [Anonymous], 2016, ARXIV 1606 00915 CS
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90
   [Anonymous], 2014, P ECCV
   [Anonymous], 2015, P CVPR
   [Anonymous], 2015, P CVPR
   [Anonymous], 2016, P ECCV
   Castrejon Lluis, 2016, P CVPR
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Eisenschtat Aviv, 2017, P CVPR
   Faghri Fartash, 2018, P BMVC
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Feng FX, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808205
   Frome Andrea, 2013, ADV NEURAL INF PROCE
   Gehring Jonas, 2017, P ICML
   Glorot Xavier, 2010, P AISTAT
   Gray Douglas, 2007, P PETS
   Gu Jiuxiang, 2018, P CVPR
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He R, 2015, IEEE T IMAGE PROCESS, V24, P5543, DOI 10.1109/TIP.2015.2466106
   He XW, 2019, PATTERN RECOGN LETT, V119, P229, DOI 10.1016/j.patrec.2017.10.018
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hu Baotian, 2014, P NIPS
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Huang Eric H., 2012, P ACL
   Huang Yan, 2017, P CVPR
   Huang Yan, 2018, P CVPR
   Kaiye Wang Ran, 2013, P ICCV
   Karpathy Andrej, 2014, P NIPS
   Kim Yoon, 2014, P EMNLP
   Klein Benjamin, 2015, P CVPR
   Lee Kuang-Huei, 2018, P ECCV
   Li K, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152126
   Li Shuang, 2017, P CVPR
   Li Shuang, 2017, P ICCV
   Li Wei, 2014, P CVPR
   Lin Xiao, 2016, P ECCV
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu Kang, 2015, P ACL
   Liu RY, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3300939
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P4514, DOI 10.1109/TIP.2016.2593344
   Liu Yu, 2017, P ICCV
   Ma Lin, 2015, P ICCV
   Mikolov Tomas, 2013, Preprints
   Mikolov Tomas, 2010, INTERSPEECH
   Nam Hyeonseob, 2017, P CVPR
   Niu Zhenxing, 2017, P ICCV
   Qian Xuelin, 2017, P CVPR
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Rasiwasia Nikhil, 2010, P ACM MM
   Reed Scott, 2016, P CVPR
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sharma Abhishek, 2012, P CVPR
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vedaldi A., 2015, P ACM MM
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wang D, 2016, IEEE T IMAGE PROCESS, V25, P4540, DOI 10.1109/TIP.2016.2592800
   Wang Liwei, 2016, P CVPR
   Wang Wei, 2014, P VLDB ENDOWMENT, V7, P8
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wei YC, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2775109
   Wu Fei, 2013, P ACM MM
   Wu Yonghui, 2016, P C ASS MACH TRANSL
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Yan Fei, 2015, P CVPR
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang E, 2018, IEEE T NEUR NET LEAR, V29, P5292, DOI 10.1109/TNNLS.2018.2793863
   Yang Y, 2011, J NANOMATER, V2011, DOI 10.1155/2011/180896
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zhang CQ, 2017, IEEE T IMAGE PROCESS, V26, P648, DOI 10.1109/TIP.2016.2627806
   Zhang YC, 2018, ICMLC 2020: 2020 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING, P145, DOI 10.1145/3383972.3383975
   Zhang Yuting, 2017, P CVPR
   Zhao Junbo, 2015, P NIPS
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7
NR 85
TC 232
Z9 254
U1 5
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 51
DI 10.1145/3383184
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600012
OA Green Submitted, Green Accepted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Rout, RK
   Hassan, SKS
   Sindhwani, S
   Pandey, HM
   Umer, S
AF Rout, Ranjeet Kumar
   Hassan, S. K. Sarif
   Sindhwani, Sanchit
   Pandey, Hari Mohan
   Umer, Saiyed
TI Intelligent Classification and Analysis of Essential Genes Using
   Quantitative Methods
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Essential genes; fractal dimension; purines; pyrimidines; Shannon
   entropy; Hurst exponent
ID PREDICTING ESSENTIAL GENES; IDENTIFICATION; DISPENSABILITY; CELL; SET
AB Essential genes are considered to be the genes required to sustain life of different organisms. These genes encode proteins that maintain central metabolism, DNA replications, translation of genes, and basic cellular structure, and mediate the transport process within and out of the cell. The identification of essential genes is one of the essential problems in computational genomics. In this present study, to discriminate essential genes from other genes from a non-biologists perspective, the purine and pyrimidine distribution over the essential genes of four exemplary species, namely Homo sapiens, Arabidopsis thaliana, Drosophila melanogaster, and Danlo redo are thoroughly experimented using some quantitative methods. Moreover, the Indigent classification method has also been deployed for classification on the essential genes of the said species. Based on Shannon entropy, fractal dimension, Hurst exponent, and purine and pyrimidine bases distribution, 10 different clusters have been generated for the essential genes of the four species. Some proximity results are also reported herewith for the clusters of the essential genes.
C1 [Rout, Ranjeet Kumar] Natl Inst Technol Srinagar, Hazratbal 190006, J&K, India.
   [Hassan, S. K. Sarif] Vidyasagar Univ Maligram, Dept Math, Pingla Thana Mahavidyalaya, Paschim Medinipur 721140, W Bengal, India.
   [Sindhwani, Sanchit] Natl Inst Technol, Jalndhar 144011, Punjab, India.
   [Pandey, Hari Mohan] Edge Hill Univ, Dept Comp Sci, Ormskirk, England.
   [Umer, Saiyed] Aliah Univ, Dept Comp Sci & Engn, 11-A127,Act Area 2, Kolkata 700156, W Bengal, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Srinagar; Vidyasagar University; Edge Hill University; Aliah
   University
RP Rout, RK (corresponding author), Natl Inst Technol Srinagar, Hazratbal 190006, J&K, India.
EM ranjeethkumar-rout@nitsri.net; sarif.hassan@icts.res.in;
   sanchit.sncht@gmail.com; profharimohanpandey@gmail.com;
   saiyedumer@gmail.com
RI umer, saiyed/ABD-1070-2021; Rout, Ranjeet Kumar/JCE-3978-2023; Pandey,
   Hari Mohan/M-9658-2015; Rout, Ranjeet kumar/AAT-3763-2020
OI umer, saiyed/0000-0002-1476-041X; Pandey, Hari
   Mohan/0000-0002-9128-068X; Rout, Ranjeet Kumar/0000-0002-1546-1702
CR Acencio ML, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-290
   Becker SA, 2005, BMC MICROBIOL, V5, DOI 10.1186/1471-2180-5-8
   BERTHELSEN CL, 1992, PHYS REV A, V45, P8902, DOI 10.1103/PhysRevA.45.8902
   Blomen VA, 2015, SCIENCE, V350, P1092, DOI 10.1126/science.aac7557
   Cattani Carlo, 2010, MATH PROBL ENG, V2010
   Chen Y, 2005, BIOINFORMATICS, V21, P575, DOI 10.1093/bioinformatics/bti058
   Commichau FM, 2013, MOL BIOSYST, V9, P1068, DOI 10.1039/c3mb25595f
   Das JK, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-28289-x
   Gallagher LA, 2007, P NATL ACAD SCI USA, V104, P1009, DOI 10.1073/pnas.0606713104
   Giaever G, 2002, NATURE, V418, P387, DOI 10.1038/nature00935
   Harborth J, 2001, J CELL SCI, V114, P4557
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Hu WQ, 2007, PLOS PATHOG, V3, DOI 10.1371/journal.ppat.0030024
   ITAYA M, 1995, FEBS LETT, V362, P257, DOI 10.1016/0014-5793(95)00233-Y
   Jeong H, 2001, NATURE, V411, P41, DOI 10.1038/35075138
   Ji YD, 2001, SCIENCE, V293, P2266, DOI 10.1126/science.1063566
   Kim DU, 2010, NAT BIOTECHNOL, V28, P617, DOI 10.1038/nbt.1628
   Koonin EV, 2000, ANNU REV GENOM HUM G, V1, P99, DOI 10.1146/annurev.genom.1.1.99
   Koonin EV, 2003, NAT REV MICROBIOL, V1, P127, DOI 10.1038/nrmicro751
   Lamichhane G, 2003, P NATL ACAD SCI USA, V100, P7213, DOI 10.1073/pnas.1231432100
   Langridge GC, 2009, GENOME RES, V19, P2308, DOI 10.1101/gr.097097.109
   Liao BY, 2007, TRENDS GENET, V23, P378, DOI 10.1016/j.tig.2007.05.006
   Liu X., 2017, PLOS ONE, V12, P3
   Lu Y, 2014, CURR BIOINFORM, V9, P89, DOI 10.2174/1574893608999140109113434
   Lu Y, 2014, COMPUT BIOL CHEM, V50, P29, DOI 10.1016/j.compbiolchem.2014.01.011
   Makarychev K., 2002, Communications in Information and Systems, V2, P147, DOI 10.4310/CIS.2002.v2.n2.a3
   Maslov S, 2002, FEBS LETT, V530, P255, DOI 10.1016/S0014-5793(02)03428-2
   Meinke D, 2008, TRENDS PLANT SCI, V13, P483, DOI 10.1016/j.tplants.2008.06.003
   Mushegian AR, 1996, P NATL ACAD SCI USA, V93, P10268, DOI 10.1073/pnas.93.19.10268
   Ning K., 2010, BMC BIOINFORMATICS, V11, P1
   Ning LW, 2014, GENET MOL RES, V13, P4564, DOI 10.4238/2014.June.17.8
   O'Neill RS, 2013, GENOME, V56, P753, DOI 10.1139/gen-2013-0210
   Papp B, 2004, NATURE, V429, P661, DOI 10.1038/nature02636
   Plaimas K, 2010, BMC SYST BIOL, V4, DOI 10.1186/1752-0509-4-56
   Roach TNF, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19050192
   Roemer T, 2003, MOL MICROBIOL, V50, P167, DOI 10.1046/j.1365-2958.2003.03697.x
   Rout RK, 2018, COMP M BIO BIO E-IV, V6, P192, DOI 10.1080/21681163.2016.1214850
   Sarmiento F, 2013, P NATL ACAD SCI USA, V110, P4726, DOI 10.1073/pnas.1220225110
   Steinmetz LM, 2002, NAT GENET, V31, P400, DOI 10.1038/ng929
   Wang T, 2015, SCIENCE, V350, P1096, DOI 10.1126/science.aac7041
   Wu YP, 2015, AM J PHYSIOL-REG I, V309, pR747, DOI 10.1152/ajpregu.00148.2015
   Xu P, 2011, SCI REP-UK, V1, DOI 10.1038/srep00125
   Ye YN, 2013, BMC GENOMICS, V14, DOI 10.1186/1471-2164-14-769
   Yu HY, 2004, TRENDS GENET, V20, P227, DOI 10.1016/j.tig.2004.04.008
   Yu YM, 2017, MOL BIOSYST, V13, P577, DOI 10.1039/c6mb00806b
   Zhang R, 2009, NUCLEIC ACIDS RES, V37, pD455, DOI 10.1093/nar/gkn858
   Zhang XH, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0113801
   ZUREK WH, 1989, PHYS REV A, V40, P4731, DOI 10.1103/PhysRevA.40.4731
NR 48
TC 10
Z9 10
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 38
DI 10.1145/3343856
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300020
DA 2024-07-18
ER

PT J
AU Shamsolmoali, P
   Zareapoor, M
   Zhou, HY
   Yang, J
AF Shamsolmoali, Pourya
   Zareapoor, Masoumeh
   Zhou, Huiyu
   Yang, Jie
TI AMIL: Adversarial Multi-instance Learning for Human Pose Estimation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Pose estimations; adversarial network; multiple-instance learning;
   neural networks
AB Human pose estimation has an important impact on a wide range of applications, from human-computer interface to surveillance and content-based video retrieval. For human pose estimation, joint obstructions and overlapping upon human bodies result in departed pose estimation. To address these problems, by integrating priors of the structure of human bodies, we present a novel structure-aware network to discreetly consider such priors during the training of the network. Typically, learning such constraints is a challenging task. Instead, we propose generative adversarial networks as our learning model in which we design two residual Multiple-Instance Learning (MIL) models with identical architecture one is used as the generator, and the other one is used as the discriminator. The discriminator task is to distinguish the actual poses from the fake ones. If the pose generator generates results that the discriminator is not able to distinguish from the real ones, then the model has successfully learned the priors. In the proposed model, the discriminator differentiates the ground-truth heatmaps from the generated ones, and later the adversarial loss back-propagates to the generator. Such procedure assists the generator to learn reasonable body configurations and is proved to be advantageous to improve the pose estimation accuracy. Meanwhile, we propose a novel function for MIL. It is an adjustable structure for both instance selection and modeling to appropriately pass the information between instances in a single bag. In the proposed residual MIL neural network, the pooling action adequately updates the instance contribution to its bag. The proposed adversarial residual multi-instance neural network that is based on pooling has been validated on two datasets for the human pose estimation task and successfully outperforms the other state-of-the-art models. The code will be made available on https://github.com/pshams55/AMIL.
C1 [Shamsolmoali, Pourya; Zareapoor, Masoumeh; Yang, Jie] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai, Peoples R China.
   [Zhou, Huiyu] Univ Leicester, Dept Informat, Leicester LE1 7RH, Leics, England.
C3 Shanghai Jiao Tong University; University of Leicester
RP Yang, J (corresponding author), Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai, Peoples R China.
EM pshams@sjtu.edu.cn; mzarea@sjtu.edu.cn; hz143@leicester.ac.uk;
   jieyang@situ.edu.cn
RI Zareapoor, Dr. Masoumeh/AAE-6067-2019; Zhou, Huiyu/O-2692-2014
OI Zareapoor, Dr. Masoumeh/0000-0002-3991-0584; Zhou,
   Huiyu/0000-0003-1634-9840; Zareapoor, Masoumeh/0000-0002-7569-9018
FU NSFC, China [61876107, U1803261]; 973 Plan, China [2015CB856004]; UK
   EPSRC [EP/N011074/1]; Royal Society-Newton Advanced Fellowship
   [NA160342]; European Union [720325]
FX This research is partly supported by NSFC, China (No: 61876107,
   U1803261) and 973 Plan, China (No. 2015CB856004). H. Zhou was supported
   by UK EPSRC under Grant EP/N011074/1, Royal Society-Newton Advanced
   Fellowship under Grant NA160342, and European Union'sHorizon 2020
   research and innovation program under the Marie-Sklodowska-Curie grant
   agreement No 720325.
CR Ali S, 2010, IEEE T PATTERN ANAL, V32, P288, DOI 10.1109/TPAMI.2008.284
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2017, BEGAN BOUNDARY EQUIL
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-46484-8_29
   [Anonymous], 2018, ELECT J QUAL THEORY, DOI DOI 10.1155/2018/8345893
   [Anonymous], 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00742
   [Anonymous], 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00980
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   [Anonymous], 2014, P ADV NEUR INF PROC
   [Anonymous], 2017, ARXIV170400028CSLG
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Belagiannis V, 2017, IEEE INT CONF AUTOMA, P468, DOI 10.1109/FG.2017.64
   Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Chen Y, 2017, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2017.137
   Chen Yun-Chun., 2019, IEEE Transactions on Pattern Analysis and Machine Intelligence in press
   Chou C. J., 2017, P IEEE C COMP VIS PA
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng J., 2012, Imagenet large scale visual recognition competition 2012 (ILSVRC2012)
   Denton E.L., 2015, CoRR, P1486
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Goodfellow IJ, 2014, ADV NEUR IN, P2672, DOI DOI 10.1145/3422622
   Graving JM, 2019, ELIFE, V8, DOI 10.7554/eLife.47994
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hoang Q., 2018, P INT C LEARN REPR I
   Hoffman J., 2016, J MACHINE LEARNING R, V17, P1
   Hu YX, 2009, IEEE I CONF COMP VIS, P128, DOI 10.1109/ICCV.2009.5459153
   Ilse M., 2018, P INT C MACH LEARN P
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Johnson S, 2011, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR.2011.5995318
   Kingma D. P., 2014, arXiv
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu DL, 2017, 2017 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P406, DOI 10.1109/ICSGEA.2017.74
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Luc P., 2016, ABS161108408 CORR
   Nason RW, 2015, HEAD NECK CANC CLIN, P15, DOI 10.1007/978-81-322-2434-1_2
   Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17
   Radford A., 2015, ARXIV151106434
   Ravanbakhsh M., 2017, ARXIV PREPRINT ARXIV
   Ronchi MR, 2017, IEEE I CONF COMP VIS, P369, DOI 10.1109/ICCV.2017.48
   Sabour S, 2017, ADV NEUR IN, V30
   Salimans T, 2016, ADV NEUR IN, V29
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Sun M, 2016, INT C PATT RECOG, P3270, DOI 10.1109/ICPR.2016.7900139
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Nguyen T, 2017, INT CONF SYST SCI EN, P670, DOI 10.1109/ICSSE.2017.8030960
   Ukita N, 2018, COMPUT VIS IMAGE UND, V170, P67, DOI 10.1016/j.cviu.2018.02.003
   Wang XG, 2018, PATTERN RECOGN, V74, P15, DOI 10.1016/j.patcog.2017.08.026
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113
   Xu YY, 2016, NEUROCOMPUTING, V171, P826, DOI 10.1016/j.neucom.2015.07.024
   Yan Y., 2018, PMLR, P662
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   Yu T., 2018, ACM T MULTIMEDIA COM, V4, P1
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zeng T, 2015, IEEE DATA MINING, P579, DOI 10.1109/ICDM.2015.92
   Zhang D, 2017, J MAT CHEM A, V5, P39
   Zhang FF, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176646
   Zhang SP, 2017, IEEE T NEUR NET LEAR, V28, P2357, DOI 10.1109/TNNLS.2016.2586194
   Zhang SP, 2017, IEEE T CIRC SYST VID, V27, P421, DOI 10.1109/TCSVT.2016.2539860
   Zhang SP, 2015, IEEE T CIRC SYST VID, V25, P1749, DOI 10.1109/TCSVT.2015.2406194
   Zhou YZ, 2017, IEEE INT CONF COMP V, P318, DOI 10.1109/ICCVW.2017.46
   Zhu AC, 2019, AIP ADV, V9, DOI 10.1063/1.5080207
NR 68
TC 14
Z9 14
U1 1
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 23
DI 10.1145/3355612
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300005
DA 2024-07-18
ER

PT J
AU Huang, X
   Peng, YX
   Wen, Z
AF Huang, Xin
   Peng, Yuxin
   Wen, Zhang
TI RCE-HIL: Recognizing Cross-media Entailment with Heterogeneous
   Interactive Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-media reasoning; cross-media hybrid embedding; heterogeneous joint
   inference; recognizing cross-media entailment
AB Entailment recognition is an important paradigm of reasoning that judges if a hypothesis can be inferred from given premises. However, previous efforts mainly concentrate on text-based reasoning as recognizing textual entailment (RTE), where the hypotheses and premises are both textual. In fact, humans' reasoning process has the characteristic of cross-media reasoning. It is naturally based on the joint inference with different sensory organs, which represent complementary reasoning cues from unique perspectives as language, vision, and audition. How to realize cross-media reasoning has been a significant challenge to achieve the breakthrough for width and depth of entailment recognition. Therefore, this article extends RTE to a novel reasoning paradigm: recognizing cross-media entailment (RCE), and proposes heterogeneous interactive learning (HIL) approach. Specifically, HIL recognizes entailment relationships via cross-media joint inference, from image-text premises to text hypotheses. It is an end-to-end architecture with two parts: (1) Cross-media hybrid embedding is proposed to perform cross embedding of premises and hypotheses for generating their fine-grained representations. It aims to achieve the alignment of cross-media inference cues via image-text and text-text interactive attention. (2) Heterogeneous joint inference is proposed to construct a heterogeneous interaction tensor space and extract semantic features for entailment recognition. It aims to simultaneously capture the interaction between cross-media premises and hypotheses and distinguish their entailment relationships. Experimental results on widely used Stanford natural language inference (SNLI) dataset with image premises from Flickr30K dataset verify the effectiveness of HIL and the intrinsic intermedia complementarity in reasoning.
C1 [Huang, Xin; Peng, Yuxin; Wen, Zhang] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Peng, YX (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
FU National Natural Science Foundation of China [61925201, 61771025]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61925201 and Grant 61771025.
CR Andrew Galen, 2010, INT C MACH LEARN, P3408
   [Anonymous], 2016, P C N AM CHAPT ASS C, DOI DOI 10.18653/V1/N16-1170
   [Anonymous], 2011, P ICML
   [Anonymous], 2016, P COLING
   Bowman S. R., 2015, Proceedings of the 2015 conference on empirical methods in natural language processing, DOI [DOI 10.18653/V1/D15-1075, 10.18653/v1/d15-1075, 10.18653/v1/D15-1075]
   Bowman SR, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1466
   Chen HY, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1236471.1236472
   Chen JZ, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P551, DOI [10.1109/CIS.2016.133, 10.1109/CIS.2016.0134]
   Chen Q, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1657, DOI 10.18653/v1/P17-1152
   Chen Q, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2406
   Dagan Ido, 2004, P LEARN METH TEXT UN
   Feng FX, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808205
   Gong Yichen, 2017, ARXIVABS170904348
   Han D., 2017, PROC C EMPIRICAL MET, P2853
   Harabagiu S, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P905
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang R, 2017, IEEE INT VEH SYM, P1893, DOI 10.1109/IVS.2017.7995981
   Huang X, 2018, PROC CVPR IEEE, P8837, DOI 10.1109/CVPR.2018.00921
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li K., 2018, ELECT J DIFFERENTIAL, V2018, P1, DOI DOI 10.1155/2018/1915828
   Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289
   MacCartney B, 2009, THESIS
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Mirkin Shachar, 2009, P 47 ANN M ASS COMP, P791
   Mou Lili, 2016, P M ASS COMP LING AC, P1466
   Munkhdalai T, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P11
   Paria Biswajit, 2016, ARXIVABS161104741
   Parikh AP., 2016, EMNLP
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Plummer BA, 2017, INT J COMPUT VISION, V123, P74, DOI 10.1007/s11263-016-0965-7
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Shen Tao, 2018, P AAAI C ART INT AAA
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tay Y, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1565
   Wang P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1290
   Wang Z, 2017, IEEE IJCNN, P1411, DOI 10.1109/IJCNN.2017.7966018
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P5656, DOI 10.1109/TIP.2017.2746267
   Yin W., 2016, Transactions of the Association for computational linguistics, V4, P259, DOI [DOI 10.1162/TACL_A_00097, DOI 10.1162/TACLA00244, 10.1162/tacla00097, DOI 10.1162/TACLA00097]
NR 44
TC 10
Z9 12
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 5
DI 10.1145/3365003
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100004
DA 2024-07-18
ER

PT J
AU Tang, JH
   Wang, J
   Li, ZC
   Fu, JL
   Mei, T
AF Tang, Jinhui
   Wang, Jing
   Li, Zechao
   Fu, Jianlong
   Mei, Tao
TI Show, Reward, and Tell: Adversarial Visual Story Generation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual storytelling; reinforcement learning; policy gradient;
   adversarial training
AB Despite the promising progress made in visual captioning and paragraphing, visual storytelling is still largely unexplored. This task is more challenging due to the difficulty in modeling an ordered photo sequence and in generating a relevant paragraph with expressive language style for storytelling. To deal with these challenges, we propose an Attribute-based Hierarchical Generative model with Reinforcement Learning and adversarial training (AHGRL). First, to model the ordered photo sequence and the complex story structure, we propose an attribute-based hierarchical generator. The generator incorporates semantic attributes to create more accurate and relevant descriptions. The hierarchical framework enables the generator to learn from the complex paragraph structure. Second, to generate story-style paragraphs, we design a language-style discriminator, which provides word-level rewards to optimize the generator by policy gradient. Third, we further consider the story generator and the reward critic as adversaries. The generator aims to create indistinguishable paragraphs to human-level stories, whereas the critic aims at distinguishing them and further improving the generator. Extensive experiments on the widely used dataset well demonstrate the advantages of the proposed method over state-of-the-art methods.
C1 [Tang, Jinhui; Wang, Jing; Li, Zechao] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, 200 Xiaolingwei Rd, Nanjing 210094, Jiangsu, Peoples R China.
   [Fu, Jianlong] Microsoft Res, 5 Danling St, Beijing 100080, Peoples R China.
   [Mei, Tao] AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
C3 Nanjing University of Science & Technology; Microsoft
RP Li, ZC (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, 200 Xiaolingwei Rd, Nanjing 210094, Jiangsu, Peoples R China.
EM jinhuitang@njust.edu.cn; jwang@njust.edu.cn; zechao.li@njust.edu.cn;
   jianf@microsoft.com; tmei@jd.com
RI Wang, Jing/GXN-0709-2022; Mei, Tao/GQZ-0596-2022; Tang,
   Jinhui/KBR-0891-2024
OI Mei, Tao/0000-0002-5990-7307; Tang, Jinhui/0000-0001-9008-222X
CR [Anonymous], TOMCCAP
   [Anonymous], 2011, P 24 CVPR
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2017, P AAAI C ART INT
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2010, P EUR C COMP VIS
   Bengio S., 2015, NIPS, DOI DOI 10.5555/2969239.2969370
   Chen Liang-Chieh, 2017, P IEEE C COMP VIS PA
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Hihn J, 2016, AEROSP CONF PROC
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hong RC, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2043612.2043613
   Huang Ting-Hao, 2016, P C N AM CHAPT ASS C
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jingna Mao, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348279
   Karpathy Andrej, 2014, Advances in neural information processing systems, P1889
   Krasin I., 2017, Openimages: A public dataset for large-scale multi-label and multi-class image classification
   Krause Jonathan, 2017, P IEEE C COMP VIS PA
   Li Zhizhong, 2018, IEEE transactions on pattern analysis and machine intelligence
   Liang XD, 2017, PROC CVPR IEEE, P2175, DOI 10.1109/CVPR.2017.234
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Parikh, 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7299087
   Park C. C., 2015, ADV NEURAL INFORM PR
   Ranzato M, 2016, 4 INT C LEARN REPR I
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang JH, 2017, IEEE T PATTERN ANAL, V39, P1662, DOI 10.1109/TPAMI.2016.2608882
   Venugopalan S, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301326
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang Jing, 2018, P AAAI C ART INT
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang Zhilin, 2016, Advances in Neural Information Processing Systems
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Yu L, 2017, PROCEEDINGS OF THE ASME 36TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2017, VOL 3A
   Zaremba W., 2015, CoRR abs/1505.00521
NR 46
TC 8
Z9 8
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 54
DI 10.1145/3291925
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900007
DA 2024-07-18
ER

PT J
AU Liu, XL
   Wang, M
   Zha, ZJ
   Hong, RC
AF Liu, Xueliang
   Wang, Meng
   Zha, Zheng-Jun
   Hong, Richang
TI Cross-Modality Feature Learning via Convolutional Autoencoder
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross modality; feature learning; convolutional autoencoder
ID IMAGE RETRIEVAL
AB Learning robust and representative features across multiple modalities has been a fundamental problem in machine learning and multimedia fields. In this article, we propose a novel MUltimodal Convolutional AutoEncoder (MUCAE) approach to learn representative features from visual and textual modalities. For each modality, we integrate the convolutional operation into an autoencoder framework to learn a joint representation from the original image and text content. We optimize the convolutional autoencoders of different modalities jointly by exploiting the correlation between the hidden representations from the convolutional autoencoders, in particular by minimizing both the reconstructing error of each modality and the correlation divergence between the hidden feature of different modalities. Compared to the conventional solutions relying on hand-crafted features, the proposed MUCAE approach encodes features from image pixels and text characters directly and produces more representative and robust features. We evaluate MUCAE on cross-media retrieval as well as unimodal classification tasks over real-world large-scale multimedia databases. Experimental results have shown that MUCAE performs better than the state-of-the-arts methods.
C1 [Liu, Xueliang; Wang, Meng; Hong, Richang] Hefei Univ Technol, 193 Tuixi Rd, Hefei 230009, Anhui, Peoples R China.
   [Zha, Zheng-Jun] Univ Sci & Technol China, 96 Jinzhai Rd, Hefei 230000, Anhui, Peoples R China.
C3 Hefei University of Technology; Chinese Academy of Sciences; University
   of Science & Technology of China, CAS
RP Liu, XL (corresponding author), Hefei Univ Technol, 193 Tuixi Rd, Hefei 230009, Anhui, Peoples R China.
EM liuxueliang@hfut.edu.cn; eric.mengwang@gmail.com; zhazj@ustc.edu.cn;
   hongrc.hfut@gmail.com
RI Zha, Zheng-Jun/AAF-8667-2020; ARSLAN, Okan/AAA-3232-2020; Wang,
   Meng/ITR-8699-2023; Zha, Zheng-Jun/AAE-8408-2020
OI Zha, Zheng-Jun/0000-0003-2510-8993
FU National 973 Program of China [2014CB347600]; National Natural Science
   Foundation of China (NSFC) [61432019, 61732008, 61725203, 61632007,
   61502139, 61622211, 61472392, 61620106009]; Natural Science Foundation
   of Anhui Province [1608085MF128]; Fundamental Research Funds for the
   Central Universities [WK2100100030]
FX This work was supported in part by the National 973 Program of China
   under grant 2014CB347600; in part by the National Natural Science
   Foundation of China (NSFC) under grants 61432019, 61732008, 61725203,
   61632007, 61502139, 61622211, 61472392, and 61620106009; in part by the
   Natural Science Foundation of Anhui Province under grant 1608085MF128;
   and in part by the Fundamental Research Funds for the Central
   Universities under grant WK2100100030.
CR Alain G, 2014, J MACH LEARN RES, V15, P3563
   [Anonymous], 1976, THESIS
   [Anonymous], 2011, Advances in Neural Information Processing Systems
   [Anonymous], ARXIV160202255
   [Anonymous], 2009, P ADV NEUR INF PROC
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen MM, 2014, PR MACH LEARN RES, V32, P1476
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Donahue J., 2013, Decaf: A deep convolutional activation feature for generic visual recognition
   Dos Santos C., 2014, Coling, P69
   Dumoulin V., 2016, A guide to convolution arithmetic for deep learning[J
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Feng FX, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808205
   GUILLAUMIN M, 2010, PROC CVPR IEEE, P902, DOI DOI 10.1109/CVPR.2010.5540120
   Han JW, 2016, IEEE T CYBERNETICS, V46, P487, DOI 10.1109/TCYB.2015.2404432
   Hao Xue, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P26, DOI 10.1007/978-3-319-14442-9_3
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hong RC, 2014, IEEE T CYBERNETICS, V44, P669, DOI 10.1109/TCYB.2013.2265601
   Huiskes Mark J., 2008, P ACM C MULT INF RET
   Imran M, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2771588
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kim J., 2012, PROC COLING, P579
   Kim Y, 2014, ARXIV PREPRINT ARXIV, DOI 10.3115/v1/D14-1181
   Kim Y, 2016, AAAI CONF ARTIF INTE, P2741
   Krizhevsky A., 2011, P ESANN, VVolume 1, P2
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lev G, 2016, LECT NOTES COMPUT SC, V9910, P833, DOI 10.1007/978-3-319-46466-4_50
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li D., 2003, P 11 ACM INT C MULTI, P604
   Li H, 2013, IEEE T CYBERNETICS, V43, P412, DOI 10.1109/TSMCB.2012.2208743
   Li YX, 2012, IEEE T MULTIMEDIA, V14, P1618, DOI 10.1109/TMM.2012.2199292
   Liu XL, 2015, IEEE T CYBERNETICS, V45, P1811, DOI 10.1109/TCYB.2014.2360856
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Qian XM, 2014, IEEE T CYBERNETICS, V44, P2493, DOI 10.1109/TCYB.2014.2309593
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shenghua Gao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2809, DOI 10.1109/CVPR.2011.5995454
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Tan CH., 2008, THESIS U MALAYSIA SA
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang M, 2015, IEEE T KNOWL DATA EN, V27, P2564, DOI 10.1109/TKDE.2015.2415497
   Wang M, 2015, IEEE T CYBERNETICS, V45, P1561, DOI 10.1109/TCYB.2014.2356136
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wang W, 2016, VLDB J, V25, P79, DOI 10.1007/s00778-015-0391-4
   Wu F, 2015, IEEE T IMAGE PROCESS, V24, P1497, DOI 10.1109/TIP.2015.2403240
   Xie L, 2016, MULTIMED TOOLS APPL, V75, P9185, DOI 10.1007/s11042-016-3432-0
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang YS, 2009, PROCEEDINGS OF 2009 CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE & SYSTEM DYNAMICS, VOL 8, P175, DOI 10.1145/1631272.1631298
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang HW, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2978656
   Zhang LM, 2014, IEEE T CYBERNETICS, V44, P1408, DOI 10.1109/TCYB.2013.2285219
   Zhang  Xiang, 2015, ABS150201710 CORR
   Zhao  Fang, 2016, INT J COMPUT VISION, V3, P1
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zheng L, 2014, IEEE T IMAGE PROCESS, V23, P3368, DOI 10.1109/TIP.2014.2330763
   Zhong L, 2015, IEEE T CYBERNETICS, V45, P1499, DOI 10.1109/TCYB.2014.2354351
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 68
TC 11
Z9 14
U1 3
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 7
DI 10.1145/3231740
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100007
DA 2024-07-18
ER

PT J
AU Hong, XP
   Peng, W
   Harandi, M
   Zhou, ZH
   Pietikäinen, M
   Zhao, GY
AF Hong, Xiaopeng
   Peng, Wei
   Harandi, Mehrtash
   Zhou, Ziheng
   Pietikainen, Matti
   Zhao, Guoying
TI Characterizing Subtle Facial Movements via Riemannian Manifold
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Micro-expression recognition; visual speech recognition; motion
   magnification; motion description; video representation
ID REPRESENTATION; RECOGNITION; PATTERNS
AB Characterizing subtle facial movements from videos is one of the most intensive topics in computer vision research. It is, however, challenging, since (1) the intensity of subtle facial muscle movement is usually low, (2) the duration may be transient, and (3) datasets containing spontaneous subtle movements with reliable annotations are painful to obtain and often of small sizes.
   This article is targeted at addressing these problems for characterizing subtle facial movements from both the aspects of motion elucidation and description. First, we propose an efficient method for elucidating hidden and repressed movements to make them easier to get noticed. We explore the feasibility of linearizing motion magnification and temporal interpolation, which is obscured by the architecture of existing methods. On this basis, we propose a consolidated framework, termed MOTEL, to expand temporal duration and amplify subtle facial movements simultaneously. Second, we make our contribution to dynamic description. One major challenge is to capture the intrinsic temporal variations caused by movements and omit extrinsic ones caused by different individuals and various environments. To diminish the influences of such extrinsic diversity, we propose the tangent delta descriptor to characterize the dynamics of short-term movements using the differences between points on the tangent spaces to the manifolds, rather than the points themselves. We then relax the trajectory-smooth assumption of the conventional manifold-based trajectory modeling methods and incorporate the tangent delta descriptor with the sequential inference approaches to cover the period of facial movements. The proposed motion modeling approach is validated by a series of experiments on publicly available datasets in the tasks of micro-expression recognition and visual speech recognition.
C1 [Hong, Xiaopeng] Xi An Jiao Tong Univ, Xian, Peoples R China.
   [Hong, Xiaopeng; Peng, Wei; Pietikainen, Matti; Zhao, Guoying] Univ Oulu, Oulu, Finland.
   [Harandi, Mehrtash] Monash Univ, Clayton, Vic, Australia.
   [Harandi, Mehrtash] CSIRO, Data61, Canberra, ACT, Australia.
   [Zhou, Ziheng] VeChain Fdn, Singapore, Singapore.
C3 Xi'an Jiaotong University; University of Oulu; Monash University;
   Commonwealth Scientific & Industrial Research Organisation (CSIRO)
RP Zhao, GY (corresponding author), Univ Oulu, Oulu, Finland.
EM hongxiaopeng@mail.xjtu.edu.cn; Wei.Peng@oulu.fi;
   mehrtash.harandi@monash.edu; peter.zhou@vechain.com;
   Matti.Pietikainen@oulu.fi; guoying.zhao@oulu.fi
RI Zhao, Guoying/ABE-7716-2020; Peng, Wei/IQW-7233-2023; HONG,
   Xiaopeng/V-6078-2019; Harandi, Mehrtash/D-6586-2018
OI Zhao, Guoying/0000-0003-3694-206X; HONG, Xiaopeng/0000-0002-0611-0636;
   Peng, Wei/0000-0002-2892-5764; Harandi, Mehrtash/0000-0002-6937-6300
FU National Basic Research Program of China [2015CB351705]; National Major
   Project [2017YFC0803905]; Natural Science Foundation of China [61772419,
   61572205, 61601362]; Academy of Finland ICT 2023 [313600]; Tekes Fidipro
   Program [1849/31/2015]; Tekes project [3116/31/2017]; Infotech; Academy
   of Finland (AKA) [313600] Funding Source: Academy of Finland (AKA)
FX We express deep gratitude to National Basic Research Program of China
   (Grant No. 2015CB351705), National Major Project (Grant No.
   2017YFC0803905), and Natural Science Foundation of China under Contracts
   No. 61772419, No. 61572205, and No. 61601362, the Academy of Finland ICT
   2023 (Project No. 313600), Infotech, Tekes Fidipro Program (Grant No.
   1849/31/2015), Tekes project (Grant No. 3116/31/2017).
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   [Anonymous], 2015, ARXIV150306699
   [Anonymous], 2019, IEEE T IND ELECT
   [Anonymous], 2016, ASIAN C COMPUTER VIS
   [Anonymous], 2015, ARXIV151100423
   [Anonymous], 2012, ACM Transactions on Graphics (TOG), DOI DOI 10.1145/2185520.2185561
   Bloehdorn S, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P331, DOI 10.1109/ICDM.2004.10077
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chikuse Y, 2006, J MULTIVARIATE ANAL, V97, P1284, DOI 10.1016/j.jmva.2006.03.002
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Dalal N., CVPR, P886, DOI [DOI 10.1109/CVPR.2005.177, 10.1109/CVPR.2005.177]
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Davison AK, 2018, IEEE T AFFECT COMPUT, V9, P116, DOI 10.1109/TAFFC.2016.2573832
   Dollar P., 2005, P INT C COMP COMM NE
   Ekman Paul, 1997, What the Face Reveals: Basic and Applied Studies of Spontaneous Expression using the Facial Action Coding System (FACS)
   Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138
   Franke M., 2009, ELECT TECHNOLOGY ISS, P1
   Golub G.H., 1989, MATRIX COMPUTATIONS
   Habiboglu YH, 2012, MACH VISION APPL, V23, P1103, DOI 10.1007/s00138-011-0369-1
   Happy SL, 2019, IEEE T AFFECT COMPUT, V10, P394, DOI 10.1109/TAFFC.2017.2723386
   Harandi M, 2015, INT J COMPUT VISION, V114, P113, DOI 10.1007/s11263-015-0833-x
   Harandi MT, 2016, IEEE T NEUR NET LEAR, V27, P1294, DOI 10.1109/TNNLS.2014.2387383
   Hong XP, 2016, NEUROCOMPUTING, V184, P99, DOI 10.1016/j.neucom.2015.07.134
   Hong XP, 2014, IEEE T IMAGE PROCESS, V23, P2557, DOI 10.1109/TIP.2014.2316640
   Hong XP, 2009, PROC CVPR IEEE, P1802, DOI 10.1109/CVPRW.2009.5206742
   Huang XH, 2016, NEUROCOMPUTING, V175, P564, DOI 10.1016/j.neucom.2015.10.096
   Huang Xiaohua, 2017, P INT C FRONT ADV DA
   Huang XL, 2019, INT J PARASITOL-PAR, V10, P1, DOI 10.1016/j.ijppaw.2019.06.012
   Ke Yan, 2005, P INT C COMP VIS ICC
   Khor Huai-Qian, 2018, P INT C AUT FAC GEST
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Lan XY, 2018, IEEE T IMAGE PROCESS, V27, P2022, DOI 10.1109/TIP.2017.2777183
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2007, IEEE I CONF COMP VIS, P2165
   LEGGETTER CJ, 1995, COMPUT SPEECH LANG, V9, P171, DOI 10.1006/csla.1995.0010
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li XB, 2018, IEEE T AFFECT COMPUT, V9, P563, DOI 10.1109/TAFFC.2017.2667642
   Li XB, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553717
   Li YT, 2018, IEEE IMAGE PROC, P3094, DOI 10.1109/ICIP.2018.8451376
   LIANG M, 2015, PROC CVPR IEEE, P3367, DOI [10.1109/CVPR.2015.7298958, DOI 10.1109/CVPR.2015.7298958]
   Liong ST, 2018, SIGNAL PROCESS-IMAGE, V62, P82, DOI 10.1016/j.image.2017.11.006
   Liong Sze-Teng, 2014, P INT S INT SIGN PRO
   Liong Sze-Teng, 2014, P AS C COMP VIS
   Liu C, 2005, ACM T GRAPHIC, V24, P519, DOI 10.1145/1073204.1073223
   Liu M., 2014, P INT C MULT INT
   Liu MY, 2016, IEEE T IMAGE PROCESS, V25, P5920, DOI 10.1109/TIP.2016.2615424
   Liu YL, 2016, INT J POLYM SCI, V2016, DOI 10.1155/2016/6269302
   Lui YM, 2012, IMAGE VISION COMPUT, V30, P380, DOI 10.1016/j.imavis.2011.08.002
   Mase K., 1989, AUTOMATIC LIPREADING
   Matthews I, 2002, IEEE T PATTERN ANAL, V24, P198, DOI 10.1109/34.982900
   Mayya Veena, 2016, P INT C ADV COMP COM
   Nefian AV, 2002, EURASIP J APPL SIG P, V2002, P1274, DOI 10.1155/S1110865702206083
   Newman JL, 2012, IEEE T AUDIO SPEECH, V20, P1936, DOI 10.1109/TASL.2012.2191956
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Ong Eng-Jon, 2011, COMP VIS WORKSH ICCV, P958
   Park SY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P911, DOI 10.1145/2733373.2806362
   Park S, 2009, PATTERN RECOGN LETT, V30, P708, DOI 10.1016/j.patrec.2009.02.005
   Patel D, 2016, INT C PATT RECOG, P2258, DOI 10.1109/ICPR.2016.7899972
   Pei YR, 2013, IEEE I CONF COMP VIS, P129, DOI 10.1109/ICCV.2013.23
   Peng M, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01745
   Peng Wei, 2019, P IEEE INT C AUT FAC
   Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z
   Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4
   Pfister T, 2011, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2011.6126401
   POLIKOVSKY S., 2009, 3 INT C IM CRIM DET, P1, DOI [10.1049/ic.2009.0244, DOI 10.1049/IC.2009.0244]
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   Potamianos Gerasimos, 1998, P IEEE INT C IM PROC
   Saenko K, 2009, IEEE T PATTERN ANAL, V31, P1700, DOI 10.1109/TPAMI.2008.303
   Su JY, 2014, PROC CVPR IEEE, P620, DOI 10.1109/CVPR.2014.86
   Takalkar MA, 2017, 2017 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING - TECHNIQUES AND APPLICATIONS (DICTA), P688
   Tuzel O, 2008, PROC CVPR IEEE, P1389
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Wang H., 2009, BMVC
   Wang HJ, 2016, ACM T ACCESS COMPUT, V8, DOI 10.1145/2897735
   Wang SL, 2008, IEEE T CIRC SYST VID, V18, P1760, DOI 10.1109/TCSVT.2008.2004924
   Wang SJ, 2015, IEEE T IMAGE PROCESS, V24, P6034, DOI 10.1109/TIP.2015.2496314
   Wei XF, 2018, IEEE INT CONF AUTOMA, P31, DOI 10.1109/FG.2018.00015
   Wei X, 2018, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2018.00200
   Xia Zhaoqiang, 2018, P INT C IM PROC THEO
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Zhang YC, 2017, PROC CVPR IEEE, P502, DOI 10.1109/CVPR.2017.61
   Zhang Yue, 2018, P EUR C COMP VIS
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
   Zhao SC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5534
   Zhao SC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4669
   Zhao SC, 2020, IEEE T AFFECT COMPUT, V11, P574, DOI [10.1109/TAFFC.2018.2818685, 10.1109/TAFFC.2016.2628787]
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhou ZH, 2011, PROC CVPR IEEE, P137, DOI 10.1109/CVPR.2011.5995345
   Zhou ZH, 2014, IMAGE VISION COMPUT, V32, P590, DOI 10.1016/j.imavis.2014.06.004
   Zhou ZH, 2014, IEEE T PATTERN ANAL, V36, P181, DOI 10.1109/TPAMI.2013.173
   Zhou Ziheng, 2011, P IEEE C COMP VIS PA
   Zong Y, 2018, IEEE T MULTIMEDIA, V20, P3160, DOI 10.1109/TMM.2018.2820321
NR 96
TC 1
Z9 2
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2019
VL 15
IS 3
SU S
SI SI
AR 94
DI 10.1145/3342227
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LR5EU
UT WOS:000535718800010
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Lin, XD
   Kang, XG
AF Lin, Xiaodan
   Kang, Xiangui
TI Robust Electric Network Frequency Estimation with Rank Reduction and
   Linear Prediction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Electric network frequency; low rank; weighted linear prediction;
   Cramer-Rao lower bound
ID ENF CRITERION; AUDIO; SIGNAL; AUTHENTICITY; RECORDINGS; TIMESTAMP
AB This article deals with the problem of Electric Network Frequency (ENF) estimation where Signal to Noise Ratio (SNR) is an essential challenge. By exploiting the low-rank structure of the ENF signal from the audio spectrogram, we propose an approach based on robust principle component analysis to get rid of the interference from speech contents and some of the background noise, which in our case can be regarded as sparse in nature. Weighted linear prediction is enforced on the low-rank signal subspace to gain accurate ENF estimation. The performance of the proposed scheme is analyzed and evaluated as a function of SNR, and the Cramer-Rao Lower Bound (CRLB) is approached at an SNR level above -10 dB. Experiments on real datasets have demonstrated the advantages of the proposed method over state-of-the-art work in terms of estimation accuracy. Specifically, the proposed scheme can effectively capture the ENF fluctuations along the time axis using small numbers of signal observations while preserving sufficient frequency precision.
C1 [Lin, Xiaodan; Kang, Xiangui] Sun Yat Sen Univ, Guangdong Key Lab Informat Secur, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Lin, Xiaodan] Huaqiao Univ, Sch Informat Sci & Engn, Xiamen 361021, Peoples R China.
C3 Sun Yat Sen University; Huaqiao University
RP Kang, XG (corresponding author), Sun Yat Sen Univ, Guangdong Key Lab Informat Secur, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
EM xd_lin@hqu.edu.cn; isskxg@mail.sysu.edu.cn
RI Kang, Xiangui/AAO-5527-2020
FU NSFC [U1536204, 61772571]; Special funding for basic scientific research
   of Sun Yat-sen University [6177060230]; Fujian Educational Committee
   [JAT160036]
FX This work was supported by NSFC (Grant nos. U1536204, 61772571) and
   Special funding for basic scientific research of Sun Yat-sen University
   (6177060230), and Fujian Educational Committee (Grant No. JAT160036).
CR Esquef PAA, 2014, IEEE T INF FOREN SEC, V9, P2314, DOI 10.1109/TIFS.2014.2363524
   Bykhovsky D, 2013, IEEE T INF FOREN SEC, V8, P744, DOI 10.1109/TIFS.2013.2253462
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Fu L, 2013, IEEE T INF FOREN SEC, V8, P1173, DOI 10.1109/TIFS.2013.2265088
   Garg R, 2013, IEEE T INF FOREN SEC, V8, P1417, DOI 10.1109/TIFS.2013.2272217
   Garg R, 2012, IEEE INT WORKS INFOR, P67, DOI 10.1109/WIFS.2012.6412627
   Reis PMGI, 2017, IEEE T INF FOREN SEC, V12, P853, DOI 10.1109/TIFS.2016.2636095
   Grigoras C, 2007, FORENSIC SCI INT, V167, P136, DOI 10.1016/j.forsciint.2006.06.033
   Grigoras C, 2005, INT J SPEECH LANG LA, V12, P63, DOI 10.1558/sll.2005.12.1.63
   Hajj-Ahmad A, 2016, IEEE SIGNAL PROC LET, V23, P713, DOI 10.1109/LSP.2016.2537201
   Hajj-Ahmad A, 2015, IEEE T INF FOREN SEC, V10, P1125, DOI 10.1109/TIFS.2015.2398367
   Hajj-Ahmad A, 2013, IEEE SIGNAL PROC LET, V20, P883, DOI 10.1109/LSP.2013.2272523
   Handel P., 2011, IEEE T CIRCUITS SYST, V45, P230
   Hui Su, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4613, DOI 10.1109/ICASSP.2014.6854476
   Huijbregtse M, 2009, LECT NOTES COMPUT SC, V5718, P116, DOI 10.1007/978-3-642-03521-0_11
   Lin Z.C., 2009, AUGMENTED LAGRANGE M, P9
   Maher RC, 2009, IEEE SIGNAL PROC MAG, V26, P84, DOI 10.1109/MSP.2008.931080
   Rodríguez DPN, 2010, IEEE T INF FOREN SEC, V5, P534, DOI 10.1109/TIFS.2010.2051270
   Ojowu O, 2012, IEEE T INF FOREN SEC, V7, P1330, DOI 10.1109/TIFS.2012.2197391
   Rife D. C., 2003, IEEE T INFORM THEORY, V20, P591
   Roy R., 2002, IEEE T ACOUSTICS SPE, V37, P984
   Sanders Richard W., 2008, P 33 INT C AUD FOR T
   SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830
   Vatansever Saffet, 2017, IEEE SIGNAL PROCESSI, P1
NR 24
TC 8
Z9 8
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2018
VL 14
IS 4
AR 84
DI 10.1145/3241058
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JB
UT WOS:000457139200006
DA 2024-07-18
ER

PT J
AU Jiang, YG
   Li, MJ
   Wang, X
   Liu, W
   Hua, XS
AF Jiang, Yu-Gang
   Li, Minjun
   Wang, Xi
   Liu, Wei
   Hua, Xian-Sheng
TI DeepProduct: Mobile Product Search With Portable Deep Features
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Mobile product search; deep learning; efficiency; contrastive loss
ID RETRIEVAL
AB Features extracted by deep networks have been popular in many visual search tasks. This article studies deep network structures and training schemes for mobile visual search. The goal is to learn an effective yet portable feature representation that is suitable for bridging the domain gap between mobile user photos and (mostly) professionally taken product images while keeping the computational cost acceptable for mobile-based applications. The technical contributions are twofold. First, we propose an alternative of the contrastive loss popularly used for training deep Siamese networks, namely robust contrastive loss, where we relax the penalty on some positive and negative pairs to alleviate overfitting. Second, a simple multitask fine-tuning scheme is leveraged to train the network which not only utilizes knowledge from the provided training photo pairs but also harnesses additional information from the large ImageNet dataset to regularize the fine-tuning process, extensive experiments on challenging real-world datasets demonstrate that both the robust contrastive loss and the multitask fine-tuning scheme are effective, leading to very promising results with a time cost suitable for mobile product search scenarios.
C1 [Jiang, Yu-Gang; Li, Minjun; Wang, Xi] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, 825 Zhangheng Rd, Shanghai, Peoples R China.
   [Liu, Wei] Columbia Univ, New York, NY USA.
   [Hua, Xian-Sheng] Alibaba Grp, Hangzhou, Zhejiang, Peoples R China.
C3 Fudan University; Columbia University; Alibaba Group
RP Jiang, YG (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, 825 Zhangheng Rd, Shanghai, Peoples R China.
EM ygj@fudan.edu.cn; minjunli13@fudan.edu.cn; xwang10@fudan.edu.cn;
   wliu@ee.columbia.edu; huaxiansheng@gmail.com
RI Liu, Wei/L-1951-2019
OI Liu, Wei/0000-0002-3865-8145
FU NSF China [61622204, 61572134]
FX This work was supported by two grants from NSF China (61622204 and
   61572134).
CR [Anonymous], 2013, ICMR
   [Anonymous], P INT C COMP VIS
   Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J., 2013, Decaf: A deep convolutional activation feature for generic visual recognition
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo YH, 2012, IEEE T MULTIMEDIA, V14, P1079, DOI 10.1109/TMM.2012.2190386
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Lim D., 2013, ICML, P615
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu W, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0167-4
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simo-Serra E, 2016, PROC CVPR IEEE, P298, DOI 10.1109/CVPR.2016.39
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang X, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P7, DOI 10.1145/2911996.2912002
   Wu P., 2013, P 21 ACM INT C MULT, P153, DOI DOI 10.1145/2502081.2502112
   Wu Z., 2015, P 23 ACM INT C MULT, P461, DOI DOI 10.1145/2733373.2806222
   Yu-Gang Jiang, 2016, IEEE Transactions on Big Data, V2, P32, DOI 10.1109/TBDATA.2016.2530714
NR 33
TC 17
Z9 17
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
AR 50
DI 10.1145/3184745
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GG9BS
UT WOS:000432996000007
DA 2024-07-18
ER

PT J
AU Messaoudi, F
   Ksentini, A
   Simon, G
   Bertin, P
AF Messaoudi, Farouk
   Ksentini, Adlen
   Simon, Gwendal
   Bertin, Philippe
TI Performance Analysis of Game Engines on Mobile and Fixed Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Computation offloading; cloud gaming; games; unity 3D; rendering
ID CLOUD; VIRTUALIZATION
AB Mobile gaming is an emerging concept wherein gamers are using mobile devices, like smartphones and tablets, to play best-seller games. Compared to dedicated gaming boxes or PCs, these devices still fall short of executing newly complex 3D video games with a rich immersion. Three novel solutions, relying on cloud computing infrastructure, namely, computation offloading, cloud gaming, and client-server architecture, will represent the next generation of game engine architecture aiming at improving the gaming experience. The basis of these aforementioned solutions is the distribution of the game code over different devices (including set-top boxes, PCs, and servers). In order to know how the game code should be distributed, advanced knowledge of game engines is required. By consequence, dissecting and analyzing game engine performances will surely help to better understand how to move in these new directions (i.e., distribute game code), which is so far missing in the literature. Aiming at filling this gap, we propose in this article to analyze and evaluate one of the famous engines in the market, that is, "Unity 3D." We begin by detailing the architecture and the game logic of game engines. Then, we propose a test-bed to evaluate the CPU and GPU consumption per frame and per module for nine representative games on three platforms, namely, a stand-alone computer, embedded systems, and web players. Based on the obtained results and observations, we build a valued graph of each module, composing the Unity 3D architecture, which reflects the internal flow and CPU consumption. Finally, we made a comparison in terms of CPU consumption between these architectures.
C1 [Messaoudi, Farouk] IRT B Com, Paris, France.
   [Ksentini, Adlen] Eurecom, Biot, France.
   [Simon, Gwendal] Telecom Bretagne, Brest, France.
   [Bertin, Philippe] IRT B Com, Orange Labs, Paris, France.
C3 IMT - Institut Mines-Telecom; EURECOM; IMT - Institut Mines-Telecom; IMT
   Atlantique; Orange SA
RP Messaoudi, F (corresponding author), IRT B Com, Paris, France.
EM farouk.messaoudi@b-com.com; adlen.ksentini@eurecom.fr;
   gwendal.simon@imt-atlantique.fr; philippe.bertin@b-com.com
RI Simon, Gwendal/Y-6950-2019
OI Simon, Gwendal/0000-0002-7282-918X
CR Abolfazli S, 2014, IEEE COMMUN SURV TUT, V16, P337, DOI 10.1109/SURV.2013.070813.00285
   Anderson Eike Falk, 2008, P 2008 C FUT PLAY
   [Anonymous], ASSASSINS CREED MULT
   [Anonymous], INT J COMPUTER GAMES
   Armitage G., 2012, TOMCCAP, V8, P2
   Beskow Paul B., 2010, International Journal of Advanced Media and Communication, V4, P343, DOI 10.1504/IJAMC.2010.036835
   Brooker Will., 2012, Hunting The Dark Knight: Twenty-First Century Batman
   Buyukkaya E, 2015, PEER PEER NETW APPL, V8, P276, DOI 10.1007/s12083-013-0231-5
   Choy S, 2014, MULTIMEDIA SYST, V20, P503, DOI 10.1007/s00530-014-0367-z
   Choy Sharon, 2012, P 11 ACM IEEE NETG W
   Chuah SP, 2014, IEEE WIREL COMMUN, V21, P78, DOI 10.1109/MWC.2014.6882299
   Claypool M., 2010, Proceedings of the 4th International Conference on Foundations of Digital Games (ICFDG), P42, DOI [DOI 10.1145/1536513.1536530, 10.1145/1536513.1536530]
   Claypool M, 2006, COMMUN ACM, V49, P40, DOI 10.1145/1167838.1167860
   Claypool Mark, 2010, P 1 ANN MMSYS C
   Cowan Brent, 2014, P 14 IEEE INT C ADV
   Deering M., 1988, Computer Graphics, V22, P21, DOI 10.1145/378456.378468
   Feng Wuchang, 2003, P 2 ACM NETG WORKSH
   Hong HJ, 2015, IEEE T CLOUD COMPUT, V3, P42, DOI 10.1109/TCC.2014.2338295
   Huang CY, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2537855
   Humphreys G, 2002, ACM T GRAPHIC, V21, P693, DOI 10.1145/566570.566639
   Jarschel M., 2011, P 5 C INN MOB INT SE
   Kamarainen Teemu, 2014, P 13 ACM NETG WORKSH
   Khan AUR, 2014, IEEE COMMUN SURV TUT, V16, P393, DOI 10.1109/SURV.2013.062613.00160
   Kot Blazej J., 2005, P 6 ACM C COMP HUM I
   Lee Kyungmin, 2014, P 12 ACM MOBISYS C
   Lee Y., 2012, P 11 ACM NETG WORKSH
   Lewis M, 2002, COMMUN ACM, V45, P27
   Li YS, 2015, IEEE T CIRC SYST VID, V25, P2052, DOI 10.1109/TCSVT.2015.2450152
   Liu Y, 2015, IEEE T CIRC SYST VID, V25, P1960, DOI 10.1109/TCSVT.2015.2450175
   Luo Meng, 2015, P IEEE GEN C
   Marks Stefan, 2007, P 5 ACM INT C COMP G
   Messaoudi Farouk., 2015, 2015 international workshop on network and systems support for games (NetGames), P1
   Nae V, 2011, IEEE T PARALL DISTR, V22, P380, DOI 10.1109/TPDS.2010.82
   Nasiri Rasoul M., 2015, P 17 IEEE INT WORKSH
   Ng B., 2002, VRST 02, P163
   Quax P, 2013, INT IEEE CONSUM ELEC, P216, DOI 10.1109/IGIC.2013.6659141
   Raaen K., 2015, P 6 ACM MULT SYST C
   Raaen Kjetil, 2014, P 13 ACM IEEE NETG W
   Sackl Andreas, 2016, P IEEE INT C COMM WO
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Satyanarayanan Mahadev, 2014, P MOBICASE
   Schneegans Simon, 2014, P 7 IEEE WORKSH SOFT
   Semsarzadeh M, 2015, IEEE T CIRC SYST VID, V25, P1975, DOI 10.1109/TCSVT.2015.2452778
   Shea R, 2015, IEEE T CIRC SYST VID, V25, P2026, DOI 10.1109/TCSVT.2015.2450172
   Shea Ryan, 2013, P 12 IEEE ACM NETG W
   Smed J, 2002, ELECTRON LIBR, V20, P87, DOI 10.1108/02640470210424392
   Tulip James, 2006, P INT ENT C IE 06
   Veron Maxime, 2014, P 13 ACM IEEE NETG W
   Webb Steven Daniel, 2007, P 6 ACM NETG WORKSH
   Wolf Lars C., 2002, P 1 WORKSH NETW SYST
   Yahyavi A, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2522968.2522977
   Yahyavi Amir, 2013, P 33 IEEE INT C DIST
   Zhang YH, 2016, IEEE T PARALL DISTR, V27, P1239, DOI 10.1109/TPDS.2015.2433916
NR 53
TC 10
Z9 11
U1 1
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 57
DI 10.1145/3115934
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300011
DA 2024-07-18
ER

PT J
AU Yan, M
   Sang, JT
   Xu, CS
   Hossain, MS
AF Yan, Ming
   Sang, Jitao
   Xu, Changsheng
   Hossain, M. Shamim
TI A Unified Video Recommendation by Cross-Network User Modeling
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Personalized video
   recommendation; cross-network collaboration; user modeling
AB Online video sharing sites are increasingly encouraging their users to connect to the social network venues such as Facebook and Twitter, with goals to boost user interaction and better disseminate the high-quality video content. This in turn provides huge possibilities to conduct cross-network collaboration for personalized video recommendation. However, very few efforts have been devoted to leveraging users' social media profiles in the auxiliary network to capture and personalize their video preferences, so as to recommend videos of interest. In this article, we propose a unified YouTube video recommendation solution by transferring and integrating users' rich social and content information in Twitter network. While general recommender systems often suffer from typical problems like cold-start and data sparsity, our proposed recommendation solution is able to effectively learn from users' abundant auxiliary information on Twitter for enhanced user modeling and well address the typical problems in a unified framework. In this framework, two stages are mainly involved: (1) auxiliary-network data transfer, where user preferences are transferred from an auxiliary network by learning cross-network knowledge associations; and (2) cross-network data integration, where transferred user preferences are integrated with the observed behaviors on a target network in an adaptive fashion. Experimental results show that the proposed cross-network collaborative solution achieves superior performance not only in terms of accuracy, but also in improving the diversity and novelty of the recommended videos.
C1 [Yan, Ming; Sang, Jitao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yan, Ming; Xu, Changsheng] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Sang, Jitao] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 11543, Saudi Arabia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Nanjing University; King Saud University
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM ming.yan@nlpr.ia.ac.cn; jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn;
   mshossain@ksu.edu.sa
RI xu, cj/HJZ-3488-2023; Guizani, Mohsen/AAX-4534-2021; Hossain, M.
   Shamim/K-1362-2014
OI Guizani, Mohsen/0000-0002-8972-8094; Hossain, M.
   Shamim/0000-0001-5906-9422
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61432019, 61225009, 61303176,
   61272256, 61373122, 61332016]; Deanship of Scientific Research at King
   Saud University, Riyadh, Saudi Arabia [RGP-228]
FX This work is supported in part by National Basic Research Program of
   China (No. 2012CB316304), National Natural Science Foundation of China
   (No. 61432019, No. 61225009, No. 61303176, No. 61272256, No. 61373122,
   No. 61332016). The authors extend their appreciation to the Deanship of
   Scientific Research at King Saud University, Riyadh, Saudi Arabia for
   funding this work through the research group project no. RGP-228.
CR Abel F, 2011, LECT NOTES COMPUT SC, V6757, P28, DOI 10.1007/978-3-642-22233-7_3
   [Anonymous], 2014, P INT AAAI C WEB SOC
   [Anonymous], 2012, ACM MM'12'
   [Anonymous], 2007, P 6 ACM INT C IM VID
   [Anonymous], 2012, SPAN C INF RETR
   Balabanovic M, 1997, COMMUN ACM, V40, P66, DOI 10.1145/245108.245124
   BALABANOVIC M, 1997, COMMUNICATIONS ACM, V40
   Basilico J., 2004, P 21 INT C MACH LEAR, P9, DOI DOI 10.1145/1015330.1015394
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Burke R, 2002, USER MODEL USER-ADAP, V12, P331, DOI 10.1023/A:1021240730564
   CAMILLI G, 1978, PSYCHOL BULL, V85, P163, DOI 10.1037/0033-2909.85.1.163
   Cantador I, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P401, DOI 10.1145/2645710.2645777
   Cha YC, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P565, DOI 10.1145/2348283.2348360
   Chen Terence., 2012, P 2012 ACM WORKSHOP, P67
   Deng Zhengyu., 2013, IEEE INT C MULTIMEDI, P1
   Duggan M., 2013, Pew Internet American Life Project
   Gao Huiji, 2015, P 29 AAAI C ART INT
   Gao SH, 2010, PROC CVPR IEEE, P3555, DOI 10.1109/CVPR.2010.5539943
   Herlocker JL, 2004, ACM T INFORM SYST, V22, P5, DOI 10.1145/963770.963772
   Hong RC, 2014, IEEE T CYBERNETICS, V44, P669, DOI 10.1109/TCYB.2013.2265601
   Hong RC, 2012, NEUROCOMPUTING, V95, P1, DOI 10.1016/j.neucom.2012.02.025
   Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22
   Karypis G., 2001, Proceedings of the 2001 ACM CIKM. Tenth International Conference on Information and Knowledge Management, P247, DOI 10.1145/502585.502627
   Khoshgoftaar T.M., 2009, ADV ARTIFICIAL INTEL, DOI [10.1155/2009/421425, DOI 10.1155/2009/421425]
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Li B., 2009, P 26 ANN INT C MACH, P617, DOI DOI 10.1145/1553374.1553454
   Li B, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P2052
   Loni Babak, 2014, Advances in Information Retrieval. 36th European Conference on IR Research, ECIR 2014. Proceedings: LNCS 8416, P656, DOI 10.1007/978-3-319-06028-6_72
   Mayer-Schonberger V., 2014, BIG DATA REVOLUTION
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Park J, 2011, IEEE MULTIMEDIA, V18, P78, DOI 10.1109/MMUL.2010.6
   Pazzani M. J., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P325
   Ricci F, 2011, RECOMMENDER SYSTEMS HANDBOOK, P1, DOI 10.1007/978-0-387-85820-3_1
   Richang Hong, 2015, IEEE Transactions on Big Data, V1, P152, DOI 10.1109/TBDATA.2016.2515640
   Sang JT, 2015, IEEE T MULTIMEDIA, V17, P2259, DOI 10.1109/TMM.2015.2486524
   Shani G, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P35
   Shi Y, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2556270
   Singh A. P., 2008, P 14 ACM SIGKDD INT, P650
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Yan M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/2671188.2749344
   Zhao HY, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P200, DOI 10.1109/GSIS.2013.6714773
   Ziegler Cai-Nicolas, 2005, P 14 INT C WORLD WID, P22, DOI DOI 10.1145/1060745.1060754
NR 44
TC 24
Z9 26
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2016
VL 12
IS 4
AR 53
DI 10.1145/2957755
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EH
UT WOS:000382877500007
DA 2024-07-18
ER

PT J
AU Ma, SX
   Yan, Z
AF Ma, Sixuan
   Yan, Zheng
TI PSNController: An Unwanted Content Control System in Pervasive Social
   Networking Based on Trust Management
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Security; Algorithms; Performance; Unwanted content control;
   trust management; social networking; performance evaluation
ID PRACTICAL REPUTATION SYSTEM; AD HOC NETWORKS; TRAFFIC CONTROL;
   ACCEPTANCE
AB Pervasive social networking (PSN) supports online and instant social activities and communications in a universal and pervasive manner on the basis of heterogeneous networks. However, at the same time, when mobile users expect useful and valuable contents via PSN, they may also receive unwanted, unexpected, or even malicious contents. These contents may intrude user devices, occupy device memories, and irritate mobile users. Unwanted content control in PSN has become a crucial issue that impacts the success of PSN usage. Nowadays, the literature still lacks a robust and generic unwanted content control system that can be practically applied. In this article, we present the design and implementation of PSNController, an unwanted content control system in PSN based on trust management. We evaluate the system performance under a variety of intrusions and attacks. The result shows the system is effective with regard to accuracy, efficiency, and robustness. It can control unwanted contents in PSN according to trust evaluation. We further study user acceptance on PSNController prototype system based on a small-scale user study. We receive sound user feedback on PSNController with regard to perceived ease of use, perceived usefulness, interface design, playfulness, and acceptance attitude.
C1 [Ma, Sixuan] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Yan, Zheng] Xidian Univ, Sch Cyber Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Yan, Zheng] Aalto Univ, Dept Commun & Networking, Espoo 02150, Finland.
C3 Xidian University; Xidian University; Aalto University
RP Yan, Z (corresponding author), Xidian Univ, Sch Cyber Engn, State Key Lab Integrated Serv Networks, POB 119, Xian 710071, Peoples R China.
EM zyan@xidian.edu.cn
RI yang, zheng/HGC-7753-2022; zheng, yan/GQY-6668-2022; Yan,
   Zheng/AEV-7247-2022
OI Yan, Zheng/0000-0002-9697-2108
FU 111 project [B08038]; Ph.D grant of Chinese Educational Ministry
   [JY0300130104]; initial grant of Chinese Educational Ministry for
   researchers from abroad [JY0600132901]; Shaanxi Province for excellent
   researchers from abroad [680F1303]; Aalto University
FX This work is sponsored by the 111 project (Grant. No. B08038) the Ph.D
   grant (JY0300130104) of Chinese Educational Ministry, the initial grant
   of Chinese Educational Ministry for researchers from abroad
   (JY0600132901), the grant of Shaanxi Province for excellent researchers
   from abroad (680F1303) and Aalto University.
CR [Anonymous], 2004, P VLDB2004
   [Anonymous], P 11 INT WORKSH WEB
   [Anonymous], 2008, Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR'08
   Bi J, 2008, IEEE INFOCOM SER, P371
   Chen L, 2015, FUTURE GENER COMP SY, V49, P77, DOI 10.1016/j.future.2014.06.010
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   Deng WW, 2006, PROCEEDINGS OF 2006 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P1233
   Govindan K, 2012, IEEE COMMUN SURV TUT, V14, P279, DOI 10.1109/SURV.2011.042711.00083
   Grandison T, 2000, IEEE Communications Surveys Tutorials, V3, P2, DOI [DOI 10.1109/COMST.2000.5340804, 10.1109/COMST.2000.5340804]
   Hong Zhang, 2009, 2009 Symposia and Workshops on Ubiquitous, Autonomic and Trusted Computing in conjunction with the UIC 2009 and ATC 2009 Conferences, P513, DOI 10.1109/UIC-ATC.2009.15
   Janecek AGK, 2008, ARES 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON AVAILABILITY, SECURITY AND RELIABILITY, P10, DOI 10.1109/ARES.2008.187
   Kolan P, 2007, ACM T AUTON ADAP SYS, V2, DOI 10.1145/1216895.1216897
   Liu W., 2009, SAC 09 MARCH, P975, DOI DOI 10.1145/1529282.1529470
   Luo HY, 2004, IEEE ACM T NETWORK, V12, P1049, DOI 10.1109/TNET.2004.838598
   McGibney J, 2007, ARES 2007: SECOND INTERNATIONAL CONFERENCE ON AVAILABILITY, RELIABILITY AND SECURITY, PROCEEDINGS, P749
   Ott J, 2011, PERVASIVE MOB COMPUT, V7, P671, DOI 10.1016/j.pmcj.2011.09.001
   Page L., 1999, PAGERANK CITATION RA
   Sarigöl E, 2009, PROC VLDB ENDOW, V2, P1634, DOI 10.14778/1687553.1687611
   Shen Y, 2014, COMPUT SECUR, V47, P3, DOI 10.1016/j.cose.2014.03.010
   Shi G., 2014, THESIS XIDIAN U XIAN
   Tang YC, 2008, GLOB TELECOMM CONF, DOI 10.1109/GLOCOM.2008.ECP.419
   Taufiq M., 2010, P INT C COMP EL ENG, V1, P462
   Venkatesh V, 2008, DECISION SCI, V39, P273, DOI 10.1111/j.1540-5915.2008.00192.x
   Wu B., 2006, WWW 06, P63, DOI DOI 10.1145/1135777.1135792
   Yan Z, 2014, J SUPERCOMPUT, V70, P1051, DOI 10.1007/s11227-014-1116-y
   Yan Z, 2013, IEEE INT CONF TRUST, P202, DOI 10.1109/TrustCom.2013.29
   Yan Z, 2014, NEW REV HYPERMEDIA M, V20, P25, DOI 10.1080/13614568.2013.832807
   Yan Z, 2013, J COMPUT SYST SCI, V79, P556, DOI 10.1016/j.jcss.2012.11.003
   Yan Z, 2011, IEEE INT CONF TRUST, P647, DOI 10.1109/TrustCom.2011.83
   Yan Z, 2011, IEEE T DEPEND SECURE, V8, P810, DOI 10.1109/TDSC.2010.47
   Yang H, 2004, IEEE WIREL COMMUN, V11, P38, DOI 10.1109/MWC.2004.1269716
   Zhang Jianzhong, 2010, Proceedings of the 2010 International Conference on Communications and Mobile Computing (CMC 2010), P218, DOI 10.1109/CMC.2010.133
   Zheleva E, 2009, ACM T INFORM SYST, V27, DOI 10.1145/1416950.1416953
   Zheng Yan, 2012, 2012 IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom), P666, DOI 10.1109/TrustCom.2012.291
NR 34
TC 8
Z9 8
U1 0
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 17
DI 10.1145/2808206
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100007
DA 2024-07-18
ER

PT J
AU Wang, S
   Jiang, SQ
AF Wang, Shuang
   Jiang, Shuqiang
TI INSTRE: A New Benchmark for Instance-Level Object Retrieval and
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Verification; Instance-level; object retrieval; object recognition;
   dataset; annotation; evaluation; multiple object
AB Over the last several decades, researches on visual object retrieval and recognition have achieved fast and remarkable success. However, while the category-level tasks prevail in the community, the instance-level tasks (especially recognition) have not yet received adequate focuses. Applications such as content-based search engine and robot vision systems have alerted the awareness to bring instance-level tasks into a more realistic and challenging scenario. Motivated by the limited scope of existing instance-level datasets, in this article we propose a new benchmark for INSTance-level visual object REtrieval and REcognition (INSTRE). Compared with existing datasets, INSTRE has the following major properties: (1) balanced data scale, (2) more diverse intraclass instance variations, (3) cluttered and less contextual backgrounds, (4) object localization annotation for each image, (5) well-manipulated double-labelled images for measuring multiple object (within one image) case. We will quantify and visualize the merits of INSTRE data, and extensively compare them against existing datasets. Then on INSTRE, we comprehensively evaluate several popular algorithms to large-scale object retrieval problem with multiple evaluation metrics. Experimental results show that all the methods suffer a performance drop on INSTRE, proving that this field still remains a challenging problem. Finally we integrate these algorithms into a simple yet efficient scheme for recognition and compare it with classification-based methods. Importantly, we introduce the realistic multiobjects recognition problem. All experiments are conducted in both single object case and multiple objects case.
C1 [Wang, Shuang; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Wang, S (corresponding author), 6 Kexueyuan South Rd Zhongguancun, Beijing, Peoples R China.
EM shuang.wang@vipl.ict.ac.cn; sqjiang@ict.ac.cn
FU National Basic Research Program of China (973 Program) [2012CB316400];
   National Natural Science Foundation of China [61322212, 61035001]; Key
   Technologies R&D Program of China [2012BAH18B02]; National Hi-Tech
   Development Program (863 Program) of China [2014AA015202]; Lenovo
   Outstanding Young Scientists Program (LOYS)
FX This work was supported in part by National Basic Research Program of
   China (973 Program): 2012CB316400, in part by National Natural Science
   Foundation of China: 61322212, and 61035001, in part by the Key
   Technologies R&D Program of China under Grant no. 2012BAH18B02, and in
   part by National Hi-Tech Development Program (863 Program) of China:
   2014AA015202. This work is also funded by Lenovo Outstanding Young
   Scientists Program (LOYS).
CR Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13
   [Anonymous], 2013, ICCV
   [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], NEURAL INFORM PROCES
   [Anonymous], 2007, CVPR
   [Anonymous], 2011, ACM INT C MULTIMEDIA
   [Anonymous], 2009, P 17 ACM INT C MULTI, DOI DOI 10.1145/1631272.1631361
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   Arandjelovic R., 2011, P IEEE INT C COMP VI
   Avrithis Y., 2010, P ACM MULT C ACM
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bo L., 2009, NEURAL INFORM PROCES, P1730
   Chu L., 2013, IEEE T MULTIMEDIA
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao TS, 2011, IEEE I CONF COMP VIS, P2072, DOI 10.1109/ICCV.2011.6126481
   Geusebroek JM, 2005, INT J COMPUT VISION, V61, P103, DOI 10.1023/B:VISI.0000042993.50813.60
   Jain M., 2011, Proceedings of the 19th ACM international conference on Multimedia, P1441
   Jawahar C. V., 2012, P IEEE C COMP VIS PA
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Jiang YN, 2012, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR.2012.6248042
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nene S. A., 1996, COLUMBIA OBJECT IMAG
   Nister David, 2006, CVPR
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027
   Ponce J, 2006, LECT NOTES COMPUT SC, V4170, P29
   Romberg S., 2011, P 1 ACM INT C MULT R, P1
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Shen XH, 2012, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2012.6248031
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang S., 2013, P ACM INT C MULT RET, P317
   Yao BP, 2011, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR.2011.5995368
   Zhipeng Wu, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P842, DOI 10.1109/ICPR.2010.212
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
   Zhou WG, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422960
NR 42
TC 45
Z9 46
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2015
VL 11
IS 3
AR 37
DI 10.1145/2700292
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CB8BF
UT WOS:000349852500005
DA 2024-07-18
ER

PT J
AU Kroupi, E
   Yazdani, A
   Vesin, JM
   Ebrahimi, T
AF Kroupi, Eleni
   Yazdani, Ashkan
   Vesin, Jean-Marc
   Ebrahimi, Touradj
TI EEG Correlates of Pleasant and Unpleasant Odor Perception
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Human Factors; Olfaction; brain; odors;
   electroencephalogram; classification
ID ASYMMETRY; RESPONSES; ASSOCIATIONS; STIMULATION; JUDGMENT; VALENCE;
   CORTEX
AB Olfaction-enhanced multimedia experience is becoming vital for strengthening the sensation of reality and the quality of user experience. One approach to investigate olfactory perception is to analyze the alterations in brain activity during stimulation with different odors. In this article, the changes in the electroencephalogram (EEG) when perceiving hedonically-different odors are studied. Results of within and across-subject analysis are presented. We show that EEG-based odor classification using brain activity is possible and can be used to automatically recognize odor pleasantness when a subject-specific classifier is trained. However, it is a challenging problem to design a generic classifier.
C1 [Kroupi, Eleni; Yazdani, Ashkan; Ebrahimi, Touradj] Ecole Polytech Fed Lausanne, Multimedia Signal Proc Grp, CH-1015 Lausanne, Switzerland.
   [Vesin, Jean-Marc] Ecole Polytech Fed Lausanne, Appl Signal Proc Grp, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne
RP Kroupi, E (corresponding author), Ecole Polytech Fed Lausanne, Multimedia Signal Proc Grp, Stn 11, CH-1015 Lausanne, Switzerland.
EM eleni.kroupi@epfl.ch; ashkan.yazdani@epfl.ch; jean-marc.vesin@epfl.ch;
   touradj.ebrahimi@epfl.ch
OI Ebrahimi, Touradj/0000-0002-9900-3687
CR [Anonymous], 1998, PATTERN RECOGNITION
   [Anonymous], 1996, J ECON LIT
   [Anonymous], 1969, Text book of medical physiology
   Bensafi M, 2002, NEUROSCI LETT, V328, P309, DOI 10.1016/S0304-3940(02)00548-7
   Berka C, 2004, INT J HUM-COMPUT INT, V17, P151, DOI 10.1207/s15327590ijhc1702_3
   BRAUCHLI P, 1995, CHEM SENSES, V20, P505, DOI 10.1093/chemse/20.5.505
   BUCK L, 1991, CELL, V65, P175, DOI 10.1016/0092-8674(91)90418-X
   Chen D, 2005, CHEM SENSES, V30, P345, DOI 10.1093/chemse/bji029
   Cherninskii AA, 2009, NEUROPHYSIOLOGY+, V41, P63, DOI 10.1007/s11062-009-9078-z
   Djordjevic J, 2005, NEUROIMAGE, V24, P791, DOI 10.1016/j.neuroimage.2004.09.035
   Ghinea G, 2011, MULTIMED TOOLS APPL, V55, P601, DOI 10.1007/s11042-010-0581-4
   Gotlib IH, 1998, COGNITION EMOTION, V12, P449, DOI 10.1080/026999398379673
   Gretton A., 2008, P 21 ANN C NEUR INF
   GULAS CS, 1995, J BUS PSYCHOL, V10, P87, DOI 10.1007/BF02249272
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Howard JD, 2009, NAT NEUROSCI, V12, P932, DOI 10.1038/nn.2324
   Kline JP, 2000, BIOL PSYCHOL, V52, P241, DOI 10.1016/S0301-0511(99)00046-0
   Laudien JH, 2008, NEUROIMAGE, V41, P1426, DOI 10.1016/j.neuroimage.2008.03.046
   LAWLESS H, 1977, J EXP PSYCHOL-HUM L, V3, P52, DOI 10.1037/0278-7393.3.1.52
   Lorig TS, 2000, INT J PSYCHOPHYSIOL, V36, P91, DOI 10.1016/S0167-8760(99)00104-X
   Martin GN, 1998, INT J PSYCHOPHYSIOL, V30, P287, DOI 10.1016/S0167-8760(98)00025-7
   Nakamoto T., 2011, MULTIPLE SENSORIAL M
   Nakamoto T, 2008, IEEE COMPUT GRAPH, V28, P75, DOI 10.1109/MCG.2008.3
   RAY WJ, 1985, SCIENCE, V228, P750, DOI 10.1126/science.3992243
   Royet JP, 2003, NEUROIMAGE, V20, P713, DOI 10.1016/S1053-8119(03)00388-4
   SCHLEIDT M, 1988, CHEM SENSES, V13, P279, DOI 10.1093/chemse/13.2.279
   Sutton SK, 1997, PSYCHOL SCI, V8, P204, DOI 10.1111/j.1467-9280.1997.tb00413.x
   TOMARKEN AJ, 1992, PSYCHOPHYSIOLOGY, V29, P576, DOI 10.1111/j.1469-8986.1992.tb02034.x
   Turin L, 1996, CHEM SENSES, V21, P773, DOI 10.1093/chemse/21.6.773
   Wang LW, 2002, CLIN NEUROPHYSIOL, V113, P542, DOI 10.1016/S1388-2457(02)00029-9
   Yazdani A, 2012, INT WORK QUAL MULTIM, P272, DOI 10.1109/QoMEX.2012.6263860
   Zelano C, 2007, J NEUROPHYSIOL, V97, P1969, DOI 10.1152/jn.01122.2006
NR 32
TC 39
Z9 45
U1 5
U2 48
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2014
VL 11
IS 1
SU S
SI SI
AR 13
DI 10.1145/2637287
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS0RG
UT WOS:000343984800005
DA 2024-07-18
ER

PT J
AU Arefin, A
   Rivas, R
   Nahrstedt, K
AF Arefin, Ahsan
   Rivas, Raoul
   Nahrstedt, Klara
TI OSM: Prioritized Evolutionary QoS Optimization for Interactive 3D
   Teleimmersion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; Experimentation; 3D Tele-immersion; distributed
   application; session management; content distribution; optimization
AB Different 3D tele-immersive (3DTI) activities pose different prioritized requirements for application and network-level quality of service (QoS) to ensure a strong quality of experience (QoE) for participants. Some applications put heavy weight on audio quality, some consider higher quality for upper body video streams, and some seek very low end-to-end interactivity delay. In addition, a variation in streaming content may arise in the 3DTI space due to the participants' change in interests (e.g., view change). Therefore, there is a need for an adaptive multistream, multisite 3DTI session management strategy that is not only unobtrusive, but also optimizes prioritized QoS parameters in 3DTI content distribution based on user activity and content variation. To address this next generation session management problem, we revisit the design space of multistream and multisite 3DTI session layer. We present an evolutionary 3DTI session optimization approach using Open Session Management (OSM) architecture that uses a global view of participants and overlays network conditions to optimize QoS parameters. Experimental results with PlanetLab traces show that our optimization process is unobtrusive, and the optimized TI sessions provide higher satisfaction to the participants (in some cases up to 50% higher) compared to the current solutions in the 3DTI space.
C1 [Arefin, Ahsan; Rivas, Raoul; Nahrstedt, Klara] Univ Illinois, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Arefin, A (corresponding author), Univ Illinois, Urbana, IL 61801 USA.
EM ahsan.arefin@gmail.com
FU National Science Foundation [NetSE 1012194, NetTS 0964081, CSR 0834480];
   Direct For Computer & Info Scie & Enginr; Division Of Computer and
   Network Systems [1012194] Funding Source: National Science Foundation
FX This work is supported by National Science Foundation grants NetSE
   1012194, NetTS 0964081 and CSR 0834480. Any opinions expressed in this
   paper are those of the author(s) and do not necessarily reflect the
   views of the National Science Foundation.
CR [Anonymous], 2012, METH SUBJ ASS QUAL T
   [Anonymous], 2001, PERCEPTUAL EVALUATIO
   [Anonymous], IEEE J SEL AREAS COM
   Arefin A., 2011, P 19 ACM INT C MULT, P783
   Arefin A., 2013, IEEE Multimedia
   Arefin A., 2013, P 4 ACM MULT SYST C, P226
   Baker H., 2005, ACM T MULTIMEDIA COM
   Banerjee S., 2003, P IEEE INFOCOM
   Bauer F., 1995, P IEEE INFOCOM
   Cha J., 2009, ACM T MULTIMEDIA COM
   Chen F., 2012, P ACM NOSSDAV
   Chen X., 2011, P ACM MULT
   Cisco, 2007, TEL ON STAG HOL VID
   Deb K., 2002, T EVOLUTINARY COMPUT
   Deb K., 2010, MULTIOBJECTIVE OPTIM
   Garey M.R., 1979, COMPUTERS INTRACTABI
   Hosseini M., 2003, P ACM MULT
   Huang Z., 2012, P INFOCOM
   Kuipers F., 2002, IEEE COMMUN MAG
   Kuipers F., 2002, J COMPUT COMMUN
   Kumar D., 2011, P IEEE HPCC
   KyoungSoo Park, 2006, Operating Systems Review, V40, P65, DOI 10.1145/1113361.1113374
   Liu Xi., 2012, P ACM SIGCOMM
   Ott D. E., 2004, P ACM MULT
   Ravi R, 2001, ALGORITHMICA, V31, P58, DOI 10.1007/s00453-001-0038-2
   Upadhyaya S., 2010, J COMPUT SCI ENG
   Wang B., 2000, IEEE NETW
   Wu W., 2008, P IEEE ICDCS
   Wu W., 2011, P ACM MULT
   Xiang F., 1999, J COMPUT COMMUN, V22, P1392
   Yang Z., 2010, ACM T MULTIMEDIA COM
NR 31
TC 2
Z9 3
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2014
VL 10
IS 1
SU S
SI SI
AR 12
DI 10.1145/2543899
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA2EF
UT WOS:000330907200004
DA 2024-07-18
ER

PT J
AU Andre, E
AF Andre, Elisabeth
TI Exploiting Unconscious User Signals in Multimodal Human-Computer
   Interaction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Emotion recognition; social signal processing; multimodal
   interfaces
AB This article presents the idea of empathic stimulation that relies on the power and potential of unconsciously conveyed attentive and emotional information to facilitate human-machine interaction. Starting from a historical review of related work presented at past ACM Multimedia conferences, we discuss challenges that arise when exploiting unconscious human signals for empathic stimulation, such as the real-time analysis of psychological user states and the smooth adaptation of the human-machine interface based on this analysis. A classical application field that might benefit from the idea of unconscious human-computer interaction is the exploration of massive datasets.
C1 Univ Augsburg, Augsburg, Germany.
C3 University of Augsburg
RP Andre, E (corresponding author), Univ Augsburg, Augsburg, Germany.
EM andre@informatik.uni-augsburg.de
RI Andre, Elisabeth/AAW-4960-2021
OI Andre, Elisabeth/0000-0002-2367-162X
FU EU [FP7-ICT-2009-5]
FX This work is funded by EU under research grant CEEDS (FP7-ICT-2009-5).
   The author is solely responsible for the content of this publication. It
   does not represent the opinion of the EC, and the EC is not responsible
   for any use that might be made of data appearing therein.
CR Alejandro Jaimes, 2006, P 14 ANN ACM INT C M, P855, DOI [10.1145/1180639.1180829, DOI 10.1145/1180639.1180829]
   André E, 2011, IEEE PERVAS COMPUT, V10, P54, DOI 10.1109/MPRV.2011.50
   [Anonymous], 2005, P 13 ANN ACM INT C M
   BUSCHER G., 2012, 6 ACM INT C MULT F, V1
   Cohn J., 1998, Proceedings of the sixth ACM international conference on Multimedia: Face/gesture recognition and their applications, P41
   Gilroy SW., 2008, P 16 ACM INT C MULTI, P945
   Kapoor A., P 13 ANN ACM INT C M, P677
   Kowalska M., 2017, HDB COGNITION EMOTIO, P1, DOI [https://doi.org/10.1007/978-3-319-28099-8495-1, DOI 10.1002/0470013494.CH3, 10.1002/0470013494.ch3]
   Lingenfelser J., 2013, P 4 AUGM HUM INT C, P96, DOI [DOI 10.1145/2459236.2459253, 10.1145/2459236]
   Lisetti C.L., 2002, Proceedings of the 10th International Conference on Multimedia (MULTIMEDIA), P161
   Mower E, 2011, IEEE T AUDIO SPEECH, V19, P1057, DOI 10.1109/TASL.2010.2076804
   Nakatsu R, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P2, DOI 10.1109/AFGR.1998.670917
   Pentland A. S., 2005, 13th Annual ACM International Conference on Multimedia, P690, DOI 10.1145/1101149.1101302
   Stiefelhagen R, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P3, DOI 10.1145/319463.319464
   Vinciarelli A., 2008, P 16 ACM INT C MULT, P1061, DOI DOI 10.1145/1459359.1459573
   Vogt T, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P474, DOI 10.1109/ICME.2005.1521463
NR 16
TC 6
Z9 6
U1 2
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 48
DI 10.1145/2502433
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rowe, LA
AF Rowe, Lawrence A.
TI Looking Forward 10 Years to Multimedia Successes
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Management; Algorithms; Human Factors; Performance; Multimedia research
   directions
AB A panel at ACM Multimedia 2012 addressed research successes in the past 20 years. While the panel focused on the past, this article discusses successes since the ACM SIGMM 2003 Retreat and suggests research directions in the next ten years. While significant progress has been made, more research is required to allow multimedia to impact our everyday computing environment. The importance of hardware changes on future research directions is discussed. We believe ubiquitous computing-meaning abundant computation and network bandwidth-should be applied in novel ways to solve multimedia grand challenges and continue the IT revolution of the past century.
C1 FXPAL, Palo Alto, CA 94304 USA.
RP Rowe, LA (corresponding author), FXPAL, 3174 Porter Dr, Palo Alto, CA 94304 USA.
EM rowe@fxpal.com
CR Adcock John., 2010, Proceedings of the international conference on Multimedia, MM '10, P241, DOI DOI 10.1145/1873951.1873986
   [Anonymous], 2013, ACM T MULTIMEDIA COM, V9
   BRANHAM S. M., 2012, P WORKSH SOC MOB VID
   Brooks John., 1975, TELEPHONE 1 100 YEAR
   CORNING INC, 2012, DAY MAD GLASS 2
   Fox A., 2013, ENG SOFTWARE SERVICE
   HOWCAST, 2013, WATCH HOW
   Mukhopadhyay S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P477, DOI 10.1145/319463.319690
   MUVEE TECHNOLOGIES PTE. LTD, 1999, MUV LIF EXPR
   Rowe LA, 2005, ACM T MULTIM COMPUT, V1, P3, DOI 10.1145/1047936.1047938
   Shipman F., 2003, P 11 ACM INT C MULT, P392, DOI DOI 10.1145/957013.957096
   Walsh T, 2011, UNLOCKING THE GATES: HOW AND WHY LEADING UNIVERSITIES ARE OPENING UP ACCESS TO THEIR COURSES, P1
   WIKIPEDIA, 2013, DYN
   Yang ZY, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1671962.1671963
NR 14
TC 4
Z9 4
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 37
DI 10.1145/2490825
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700007
DA 2024-07-18
ER

PT J
AU Villanueva, A
   Ponz, V
   Sesma-Sanchez, L
   Ariz, M
   Porta, S
   Cabeza, R
AF Villanueva, Arantxa
   Ponz, Victoria
   Sesma-Sanchez, Laura
   Ariz, Mikel
   Porta, Sonia
   Cabeza, Rafael
TI Hybrid Method Based on Topography for Robust Detection of Iris Center
   and Eye Corners
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Iris detection; eye tracking; topography; corner detection
ID FACE DETECTION; TRACKING; RECOGNITION; LOCALIZATION
AB A multistage procedure to detect eye features is presented. Multiresolution and topographic classification are used to detect the iris center. The eye corner is calculated combining valley detection and eyelid curve extraction. The algorithm is tested in the BioID database and in a proprietary database containing more than 1200 images. The results show that the suggested algorithm is robust and accurate. Regarding the iris center our method obtains the best average behavior for the BioID database compared to other available algorithms. Additional contributions are that our algorithm functions in real time and does not require complex post processing stages.
C1 [Villanueva, Arantxa; Ponz, Victoria; Sesma-Sanchez, Laura; Ariz, Mikel; Porta, Sonia; Cabeza, Rafael] Univ Publ Navarra, Navarre, MN USA.
C3 Universidad Publica de Navarra
RP Villanueva, A (corresponding author), Univ Publ Navarra, Navarre, MN USA.
EM avilla@unavarra.es
RI Villanueva, Arantxa/M-1641-2014; Cabeza, Rafael/D-8236-2012;
   Sesma-Sanchez, Laura/K-6236-2013; Ariz, Mikel/IAO-0429-2023
OI Villanueva, Arantxa/0000-0001-9822-2530; Cabeza,
   Rafael/0000-0001-7999-1182; Ariz, Mikel/0000-0002-7328-1582;
   Sesma-Sanchez, Laura/0000-0002-6748-528X
FU Spanish Ministry of Science and Technology [TIN2009-12247]
FX The Spanish Ministry of Science and Technology has supported this work
   under Contract TIN2009-12247.
CR [Anonymous], P 7 ANN C US PROF AS
   [Anonymous], IEEE T IMAGE PROCESS
   [Anonymous], 1999, P SIGCHI C HUMAN FAC, DOI 10.1145/302979.303065
   [Anonymous], P 24 IEEE C COMP VIS
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2013, ACM T MULTIMEDIA COM, V9
   [Anonymous], 2008 19 INT C PATT R
   Asadifard M, 2010, LECT NOTES ENG COMP, P130
   ASTERIADIS S., 2006, P 2 INT C COMM SIGN
   Bailenson JN, 2008, INT J HUM-COMPUT ST, V66, P303, DOI 10.1016/j.ijhcs.2007.10.011
   Behnke S, 2002, LECT NOTES COMPUT SC, V2415, P1319
   Bertolino P, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL I, P257, DOI 10.1109/ICIP.1996.559482
   Bicego M., 2006, COMPUTER VISION PATT
   Bolt RA., 1982, PROC ACM CHI 82, P360
   CAMPADELLI P., 2006, P BRIT MACH VIS C
   Chen D, 2006, LECT NOTES COMPUT SC, V3972, P20
   Cristinacce D., 2004, BMVC, P231
   DYER C. R., 1987, PARALLEL COMPUTER VI, P171
   FERDOWSI S., 2008, P 16 EUR SIGN PROC C
   FUKUDA T., 2010, P INT WORKSH EYE GAZ
   Hamouz M, 2005, IEEE T PATTERN ANAL, V27, P1490, DOI 10.1109/TPAMI.2005.179
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   Ince IF, 2011, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2011-40
   Ince IF, 2009, LECT NOTES COMPUT SC, V5754, P526, DOI 10.1007/978-3-642-04070-2_58
   Jesorsky O, 2001, LECT NOTES COMPUT SC, V2091, P90
   Kroon B., 2008, P 2008 INT C CONT BA, P379
   Lindeberg T., J APPL STAT, V21, P224, DOI DOI 10.1080/757582976
   Majaranta, 2011, GAZE INTERACTION APP
   Meer P., 1992, Journal of Visual Communication and Image Representation, V3, P58, DOI 10.1016/1047-3203(92)90030-W
   MERCHANT J, 1974, IEEE T BIO-MED ENG, VBM21, P309, DOI 10.1109/TBME.1974.324318
   MIN R., 2011, P 9 IEEE C AUT FAC G
   Mitra S, 2007, IEEE T SYST MAN CY C, V37, P311, DOI 10.1109/TSMCC.2007.893280
   Monty R.A., 1976, Eye Movements and Psychological Processes
   Morimoto CH, 2005, COMPUT VIS IMAGE UND, V98, P4, DOI 10.1016/j.cviu.2004.07.010
   Moriyama T, 2006, IEEE T PATTERN ANAL, V28, P738, DOI 10.1109/TPAMI.2006.98
   Niu ZH, 2006, INT C PATT RECOG, P1216
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   PONG TC, 1985, PATTERN RECOGN, V18, P333, DOI 10.1016/0031-3203(85)90024-X
   PONZ V., 2011, P IR MACH VIS IM PRO, P1
   Poole A., 2005, ENCY HUMAN COMPUTER
   RESEARCH B. T., 2001, BIOID FACE DATABASE
   Sewell W., 2010, CHI 10 EXTENDED ABST, P3739, DOI DOI 10.1145/1753846.1754048
   Shih FY, 2008, INT J PATTERN RECOGN, V22, P445, DOI 10.1142/S0218001408006284
   Sigut J, 2011, IEEE T BIO-MED ENG, V58, P411, DOI 10.1109/TBME.2010.2087330
   Starker I., 1990, SIGCHI Bulletin, P3
   TAHERI S., 2011, P IEEE INT C AUT FAC
   Tian YL, 2000, LECT NOTES COMPUT SC, V1948, P143
   Timm F, 2011, VISAPP 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, P125
   Turkan Mehmet, 2007, VISAPP 2007. Second International Conference on Computer Vision Theory and Applications, P410
   Valenti R., 2008, P IEEE C COMP VIS PA
   Valenti R, 2009, LECT NOTES COMPUT SC, V5716, P662, DOI 10.1007/978-3-642-04146-4_71
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Xia H., 2009, 2 INT C IM SIGN PROC, P1, DOI [10.1109/CISP.2009.5304434, DOI 10.1109/CISP.2009.5304434]
   ZHOU R., 2011, APPL MECH MAT VOLUME, VII
   Zhou ZH, 2004, PATTERN RECOGN, V37, P1049, DOI 10.1016/j.patcog.2003.09.006
   Zhu J, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P131, DOI 10.1109/AFGR.2002.1004144
NR 56
TC 66
Z9 68
U1 3
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2013
VL 9
IS 4
AR 25
DI 10.1145/2501643.2501647
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 206EW
UT WOS:000323501800003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Mei, T
   Tang, LX
   Tang, JH
   Hua, XS
AF Mei, Tao
   Tang, Lin-Xie
   Tang, Jinhui
   Hua, Xian-Sheng
TI Near-Lossless Semantic Video Summarization and Its Applications to Video
   Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; Video summarization; video
   storage; video content analysis; video applications
ID ABSTRACTION; RETRIEVAL
AB The ever increasing volume of video content on the Web has created profound challenges for developing efficient indexing and search techniques to manage video data. Conventional techniques such as video compression and summarization strive for the two commonly conflicting goals of low storage and high visual and semantic fidelity. With the goal of balancing both video compression and summarization, this article presents a novel approach, called Near-Lossless Semantic Summarization (NLSS), to summarize a video stream with the least high-level semantic information loss by using an extremely small piece of metadata. The summary consists of compressed image and audio streams, as well as the metadata for temporal structure and motion information. Although at a very low compression rate (around 1/40 of H. 264 baseline, where traditional compression techniques can hardly preserve an acceptable visual fidelity), the proposed NLSS still can be applied to many video-oriented tasks, such as visualization, indexing and browsing, duplicate detection, concept detection, and so on. We evaluate the NLSS on TRECVID and other video collections, and demonstrate that it is a powerful tool for significantly reducing storage consumption, while keeping high-level semantic fidelity.
C1 [Mei, Tao] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Tang, Lin-Xie] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci, Nanjing, Jiangsu, Peoples R China.
C3 Microsoft Research Asia; Microsoft; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS; Nanjing University of
   Science & Technology
RP Mei, T (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM tmei@microsoft.com
RI Tang, Jinhui/KBR-0891-2024; Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307; Tang, Jinhui/0000-0001-9008-222X
CR [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 138182 ISOIEC
   [Anonymous], 2013, ACM T MULTIMEDIA COM, V9
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Boreczky J., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P185, DOI 10.1145/332040.332428
   Bouthemy P, 1999, IEEE T CIRC SYST VID, V9, P1030, DOI 10.1109/76.795057
   COVELL M, 1998, P IEEE INT C AC SPEE
   Fernando WAC, 1999, ISCAS '99: PROCEEDINGS OF THE 1999 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 4, P255, DOI 10.1109/ISCAS.1999.779990
   H263, 2000, ITU T REC H 263
   H264, 2003, ITU T REC H 264 ISO
   Hampapur A, 2002, P SOC PHOTO-OPT INS, V4676, P194
   Hauptmann A.G., 2007, Proceedings of the international workshop on TRECVID video summarization, P20
   Hsu WH, 2007, IEEE MULTIMEDIA, V14, P14, DOI 10.1109/MMUL.2007.61
   Irani M, 1998, P IEEE, V86, P905, DOI 10.1109/5.664279
   *ISO IEC, 1991, 109181 ISOIEC JTC1
   JIANG W., 2010, ACM T MULTIM COMPUT, V6, P3
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Kim C, 2002, IEEE T CIRC SYST VID, V12, P1128, DOI 10.1109/TCSVT.2002.806813
   Kim JG, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1171, DOI 10.1109/ICME.2000.871569
   KONRAD J., 1998, JTC1SC29WG11 ISOIEC
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li YJ, 2005, I S INTELL SIG PROC, P317
   Liu YA, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P500, DOI 10.1145/1571941.1572027
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu L, 2003, MULTIMEDIA SYST, V8, P482, DOI 10.1007/s00530-002-0065-0
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   Mei T., 2007, TREC VID RETR EV ONL
   Mei T, 2008, MULTIMED TOOLS APPL, V40, P89, DOI 10.1007/s11042-007-0186-8
   Mei T, 2007, IEEE T CIRC SYST VID, V17, P699, DOI 10.1109/TCSVT.2007.896640
   Mei T, 2009, IEEE T CIRC SYST VID, V19, P1866, DOI 10.1109/TCSVT.2009.2026949
   Mei T, 2009, VISUAL COMPUT, V25, P39, DOI 10.1007/s00371-008-0282-4
   Moxley E, 2010, IEEE T MULTIMEDIA, V12, P184, DOI 10.1109/TMM.2010.2041101
   *MPEG 4, JTC1SC29WG11 ISOIEC
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Nitta N, 2009, MULTIMED TOOLS APPL, V41, P1, DOI 10.1007/s11042-008-0217-0
   Over P., 2008, Proc. of the 2nd ACM TRECVid Video Summarization Workshop, P1, DOI [DOI 10.1145/1463563.1463564, 10.1145/1463563.1463564]
   PAISITKRIANGKRAI S., 2010, P ACM INT C IM VID R
   Shao X, 2006, ACM T MULTIM COMPUT, V2, P127, DOI 10.1145/1142020.1142023
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tang Lin-Xie., 2009, Proceedings of the 17th ACM international conference on Multimedia, P351
   Tjondronegoro D., 2003, Proceedings of the Fifth ACM SIGMM International Workshop on Multimedia Information Retrieval, P201, DOI [10.1145/973264.973296, DOI 10.1145/973264.973296]
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   TSE T., 1998, P WORKING C ADV VISU, P185, DOI DOI 10.1145/948496.948522
   WANG Y., 2011, P INT MULT MOD C
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu X, 2009, IEEE T MULTIMEDIA, V11, P196, DOI 10.1109/TMM.2008.2009673
   Zhao WL, 2007, IEEE T MULTIMEDIA, V9, P1037, DOI 10.1109/TMM.2007.898928
   Zhao WL, 2009, IEEE T IMAGE PROCESS, V18, P412, DOI 10.1109/TIP.2008.2008900
NR 48
TC 22
Z9 24
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2013
VL 9
IS 3
AR 16
DI 10.1145/2487268.2487269
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 175GL
UT WOS:000321218800001
DA 2024-07-18
ER

PT J
AU Zhang, TY
   Cheng, XQ
   Lv, JM
   Li, ZH
   Shi, WS
AF Zhang, Tieying
   Cheng, Xueqi
   Lv, Jianming
   Li, Zhenhua
   Shi, Weisong
TI Providing Hierarchical Lookup Service for P2P-VoD Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Performance; Peer-to-peer; video-on-demand; distributed lookup;
   hierarchical overlay
AB Supporting random jump in P2P-VoD systems requires efficient lookup for the "best" suppliers, where "best" means the suppliers should meet two requirements: content match and network quality match. Most studies use a DHT-based method to provide content lookup; however, these methods are neither able to meet the network quality requirements nor suitable for VoD streaming due to the large overhead. In this paper, we propose Mediacoop, a novel hierarchical lookup scheme combining both content and quality match to provide random jumps for P2P-VoD systems. It exploits the play position to efficiently locate the candidate suppliers with required data (content match), and performs refined lookup within the candidates to meet quality match. Theoretical analysis and simulation results show that Mediacoop is able to achieve lower jump latency and control overhead than the typical DHT-based method. Moreover, we implement Mediacoop in a BitTorrent-like P2P-VoD system called CoolFish and make optimizations for such "total cache" applications. The implementation and evaluation in CoolFish show that Mediacoop is able to improve user experiences, especially the jump latency, which verifies the practicability of our design.
C1 [Zhang, Tieying; Cheng, Xueqi] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
   [Lv, Jianming] S China Univ Technol, Guangzhou, Guangdong, Peoples R China.
   [Li, Zhenhua] Peking Univ, Beijing 100871, Peoples R China.
   [Shi, Weisong] Wayne State Univ, Detroit, MI 48202 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   South China University of Technology; Peking University; Wayne State
   University
RP Zhang, TY (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
EM zhangtiey@software.ict.ac.cn; jml@scut.edu.cn; lzh@net.pku.edu.cn;
   weisong@wayne.edu
RI Shi, Weisong/D-2233-2016; Li, Zhenhua/AAS-8358-2020; Cheng,
   Xueqi/F-1706-2010
OI Shi, Weisong/0000-0001-5864-4675; 
FU National Basic Research Program of China [2011CB302305]; National
   High-tech R&D Program of China [2010AA012500]; National Natural Science
   Foundation of China [60933005, 60873245, 61073015]
FX This research is supported by the National Basic Research Program of
   China (Grant No. 2011CB302305), the National High-tech R&D Program of
   China (Grant No. 2010AA012500) and the National Natural Science
   Foundation of China (Grant No. 60933005, No. 60873245 and No. 61073015).
CR [Anonymous], P INT C DEP SYST NET
   [Anonymous], 2004, P ANN C USENIX ANN T
   ATALLA F., 2008, P ACM S APPL COMP SA
   CHENG B., 2008, ACM T MULTIM COMPUT, V4, P1
   Cheng B, 2007, IEEE ICC, P1698, DOI 10.1109/ICC.2007.284
   COHEN E., 2002, P 16 INT C SUP
   Do TT, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1467, DOI 10.1109/ICC.2004.1312755
   GUMMADI K. P., 2002, P 2 ACM SIGCOMM WORK
   Guo L, 2004, INT CON DISTR COMP S, P778, DOI 10.1109/ICDCS.2004.1281646
   Guo L. H., 2005, International Electron Devices Meeting 2005 (IEEE Cat. No.05CH37703C)
   Guo Y., 2003, Proceedings of the 12th International Conference on World Wide Web, P301, DOI DOI 10.1145/775152.775195
   Hefeeda M., 2003, Proceedings of the eleventh ACM international conference on Multimedia, ser. MULTIMEDIA '03, P45, DOI [DOI 10.1145/957013.957022, 10.1145/957013.957022]
   Hodes TD, 2002, WIREL NETW, V8, P213, DOI 10.1023/A:1013772027164
   Huang Y, 2008, ACM SIGCOMM COMP COM, V38, P375, DOI 10.1145/1402946.1403001
   LEONARD D., P ACM SIGMETRICS INT
   LI J, 2005, P ANN JOINT C IEEE C
   LIAO C.-S., 2006, P 12 INT C PAR DISTR, P235
   Liu Y., 2007, MULTIMEDIA '07: Proceedings of the 15th international conference on Multimedia, P127
   Lv JM, 2007, EIGHTH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED COMPUTING, APPLICATIONS AND TECHNOLOGIES, PROCEEDINGS, P501, DOI 10.1109/.27
   MAVLANKAR A., 2008, P INT C MULT EXP
   Plaxton C. G., 1997, SPAA '97. 9th Annual ACM Symposium on Parallel Algorithms and Architectures, P311, DOI 10.1145/258492.258523
   PUCHA H., 2007, P ACM USENIX S NETW
   Ren Shansi., 2006, P INT C DISTRIBUTED, P70, DOI [10.1109/ ICDCS.2006.18, DOI 10.1109/ICDCS.2006.18]
   Stoica I, 2001, ACM SIGCOMM COMP COM, V31, P149, DOI 10.1145/964723.383071
   Stutzbach R., 2006, ACM SIGCOMM C INT ME, P189, DOI [10.1145/1177080.1177105, DOI 10.1145/1177080.1177105]
   WANG J., 2004, IEEE ACM T NETW, V12
   Xu Z., 2007, PROC IEEE PES GEN M, P1, DOI DOI 10.1109/PES.2007.386014
   Yiu WPK, 2007, IEEE J SEL AREA COMM, V25, P1717, DOI 10.1109/JSAC.2007.071210
   Zegura EW, 1996, IEEE INFOCOM SER, P594, DOI 10.1109/INFCOM.1996.493353
   Zhao BY, 2004, IEEE J SEL AREA COMM, V22, P41, DOI 10.1109/JSAC.2003.818784
   ZHAO S., 2010, P ANN JOINT C IEEE C
   Zhou XB, 2008, COMPUT COMMUN, V31, P4072, DOI 10.1016/j.comcom.2008.08.007
NR 32
TC 2
Z9 2
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2012
VL 8
IS 1
SI SI
AR 15
DI 10.1145/2089085.2089092
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 898YN
UT WOS:000300778400007
DA 2024-07-18
ER

PT J
AU Yang, ZY
   Wu, WM
   Nahrstedt, K
   Kurillo, G
   Bajcsy, R
AF Yang, Zhenyu
   Wu, Wanmin
   Nahrstedt, Klara
   Kurillo, Gregorij
   Bajcsy, Ruzena
TI Enabling Multiparty 3D Tele-Immersive Environments with ViewCast
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Performance; Experimentation; 3D tele-immersion; networking
   protocol; distributed multimedia system; multi-stream coordination; QoS
   adaptation; application level multicast
AB Three-dimensional tele-immersive (3DTI) environments have great potential to promote collaborative work among geographically distributed users. However, most existing 3DTI systems work with only two sites due to the huge demand of resources and the lack of a simple yet powerful networking model to handle connectivity, scalability, and quality-of-service (QoS) guarantees. In this article, we explore the design space from the angle of multistream management to enable multi-party 3DTI communication. Multiple correlated 3D video streams are employed to provide a comprehensive representation of the physical scene in each 3DTI environment, and rendered together to establish a common cyberspace among all participating 3DTI environments. The existence of multistream correlation provides the unique opportunity for new approaches in QoS provisioning. Previous work mostly concentrated on compression and adaptation techniques on the per-stream basis while ignoring the application layer semantics and the coordination required among streams. We propose an innovative and generalized ViewCast model to coordinate the multistream content dissemination over an overlay network. ViewCast leverages view semantics in 3D free-viewpoint video systems to fill the gap between the high-level user interest and the low-level stream management. In ViewCast, only the view information is specified by the user/application, while the underlying control dynamically performs stream differentiation, selection, coordination and dissemination. We present the details of ViewCast and evaluate it through both simulation and 3DTI sessions among tele-immersive environments residing in different institutes across the Internet2. Our experimental results demonstrate the implementation feasibility and performance enhancement of ViewCast in supporting the multiparty 3DTI collaboration.
C1 [Yang, Zhenyu; Wu, Wanmin; Nahrstedt, Klara] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
   [Kurillo, Gregorij; Bajcsy, Ruzena] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   University of California System; University of California Berkeley
RP Yang, ZY (corresponding author), Univ Illinois, Dept Comp Sci, 201 N Goodwin, Urbana, IL 61801 USA.
EM zyang2@uiuc.edu; wwu23@uiuc.edu; klara@uiuc.edu;
   gregorij@eecs.berkeley.edu; bajcsy@eecs.berkeley.edu
FU National Science Foundation (NSF) [NSF SCI 05-49242, NSF CNS 05-20182]
FX This work was supported by the National Science Foundation (NSF SCI
   05-49242 and NSF CNS 05-20182). The presented views are those of the
   authors and do not represent those of the NSF.
CR [Anonymous], 397 ETH I SCI COMP
   [Anonymous], P IEEE INT S CLUST C
   [Anonymous], P 8 INT C DISTR MULT
   [Anonymous], P 9 ACM INT C MULT M
   [Anonymous], P SIGCHI C HUM FACT
   [Anonymous], P 15 INT C MULT ACM
   [Anonymous], JTC1SC29WG11N5878
   [Anonymous], ACM T MULTIM COMPUT
   [Anonymous], 1995, ACM Transactions on Computer-Human Interaction (TOCHI), DOI DOI 10.1145/210079.210088
   [Anonymous], P 4 INT C COLL VIRT
   [Anonymous], P 7 IEEE INT S MULT
   [Anonymous], TEEV PROJ
   [Anonymous], SPIE MULT COMP NETW
   [Anonymous], P ACM NOSSDAV 2003 M
   Banerjee S, 2002, ACM SIGCOMM COMP COM, V32, P205, DOI 10.1145/964725.633045
   Bauer F, 1997, IEEE J SEL AREA COMM, V15, P382, DOI 10.1109/49.564136
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   Chu YH, 2000, PERF E R SI, V28, P1, DOI 10.1145/345063.339337
   Daniilidis K., 2000, CONFLUENCE COMPUTER, P253
   Gross M, 2003, ACM T GRAPHIC, V22, P819, DOI 10.1145/882262.882350
   Hosseini M., 2003, Proceedings of the eleventh ACM international conference on Multimedia, MULTIMEDIA '03, P480
   Jannotti J, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P197
   Mulligan J, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P10, DOI 10.1109/SMBV.2001.988758
   Ott DavidE., 2004, MULTIMEDIA '04: Proceedings of the 12th annual ACM inter- national conference on Multimedia, P596
   Raskar R., 1998, Computer Graphics, P179, DOI 10.1145/280814.280861
   Schreer O., 2001, Proc. Intl. Conf. eWork and eBusiness, P184
   Sripanidkulchai K., 2004, Proc. ACM Internet Measurement Conference, P41
   Sriram R, 1999, IEEE INFOCOM SER, P1073, DOI 10.1109/INFCOM.1999.751662
   Yang Z., 2006, P INT WORKSHOP NETWO
NR 29
TC 0
Z9 0
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2010
VL 6
IS 4
AR 29
DI 10.1145/1865106.1865113
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 688VA
UT WOS:000284880900007
DA 2024-07-18
ER

PT J
AU Gomes, JVP
   Inácio, PRM
   Lakic, B
   Freire, MM
   Da Silva, HJA
   Monteiro, PP
AF Gomes, Joao V. P.
   Inacio, Pedro R. M.
   Lakic, Branka
   Freire, Mario M.
   Da Silva, Henrique J. A.
   Monteiro, Paulo P.
TI Source Traffic Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Measurement; Performance; Modeling; Self-similarity;
   simulation; source traffic analysis
ID STATISTICAL-ANALYSIS; VIDEO; MODELS
AB Traffic modeling and simulation plays an important role in the area of Network Monitoring and Analysis, for it provides practitioners with efficient tools to evaluate the performance of networks and of their elements. This article focus on the traffic generated by a single source, providing an overview of what was done in the field and studying the statistical properties of the traffic produced by a personal computer, including analysis of the autocorrelation structure. Different distributions were fitted to the interarrival times, packet sizes, and byte count processes with the goal of singling out the ones most suitable for traffic generation.
C1 [Gomes, Joao V. P.; Inacio, Pedro R. M.; Monteiro, Paulo P.] Nokia Siemens Networks Portugal SA, P-2720093 Amadora, Portugal.
   [Lakic, Branka] Univ Aveiro, Inst Telecommun, P-3810193 Aveiro, Portugal.
   [Freire, Mario M.] Univ Beira Interior, Dept Comp Sci, IT Networks & Multimedia Grp, P-6201001 Covilha, Portugal.
   [Da Silva, Henrique J. A.] Univ Coimbra, Dept Elect & Comp Engn, P-3030290 Coimbra, Portugal.
C3 Nokia Corporation; Siemens AG; Nokia Siemens Networks; Instituto de
   Telecomunicacoes; Universidade de Aveiro; Universidade da Beira
   Interior; Universidade de Coimbra
RP Gomes, JVP (corresponding author), Nokia Siemens Networks Portugal SA, Rua Irmaos Siemens 1, P-2720093 Amadora, Portugal.
EM joao.1.gomes@nsn.com; pedro.inacio@nsn.com; branka@av.it.pt;
   mario@ubi.pt; hjas@ci.uc.pt; paulo.1.monteiro@nsn.com
RI Monteiro, Paulo/B-1823-2012; Silva, Henrique/A-9551-2011; Inácio, Pedro
   R. M./I-1778-2019; Monteiro, Paulo/AAE-5046-2019; Freire,
   Mario/C-2923-2008
OI Monteiro, Paulo/0000-0003-4664-9238; Silva,
   Henrique/0000-0003-2866-2746; Inácio, Pedro R. M./0000-0001-8221-0666;
   Monteiro, Paulo/0000-0003-4664-9238; Freire, Mario/0000-0002-9017-5001
FU Fundacao para a Ciencia e a Tecnologia, Portugal [SFRH/BDE/15592/2006,
   SFRH/BDE/15643/2006]; Nokia Siemens Networks Portugal S.A., Portugal;
   Institute of Telecommunications, Portugal; University of Beira Interior,
   Portugal;  [PTDC/EIA/73072/2006]; Fundação para a Ciência e a Tecnologia
   [SFRH/BDE/15643/2006, PTDC/EIA/73072/2006, SFRH/BDE/15592/2006] Funding
   Source: FCT
FX This research was supported by the Fundacao para a Ciencia e a
   Tecnologia, Portugal, through the grant contracts SFRH/BDE/15592/2006,
   SFRH/BDE/15643/2006 and by the project PTDC/EIA/73072/2006 TRAMANET:
   Traffic and Trust Management in Peer-to-Peer Networks. It was also
   funded by Nokia Siemens Networks Portugal S.A., Portugal, by the
   Institute of Telecommunications, Portugal, and by University of Beira
   Interior, Portugal.
CR Ansari N, 2002, IEEE T BROADCAST, V48, P337, DOI 10.1109/TBC.2002.806794
   Barford P., 1998, Performance Evaluation Review, V26, P151, DOI 10.1145/277858.277897
   BERAN J, 1995, IEEE T COMMUN, V43, P1566, DOI 10.1109/26.380206
   BOLOTIN VA, 1994, IEEE J SEL AREA COMM, V12, P433, DOI 10.1109/49.285304
   Cao J., 2002, NONLINEAR ESTIMATION, P83
   Casilari E, 2004, LECT NOTES COMPUT SC, V3079, P84
   Casilari E., 2002, Proceedings of Third International Symposium on Communication Systems Networks and Digital Signal Processing, P411
   *CISCO DOC SERV, 2002, TRAFF AN VOIP TECH R
   Cox D., 1984, STATISITCS, P55
   Dawood AM, 1999, IEEE T MULTIMEDIA, V1, P77, DOI 10.1109/6046.748173
   Feldmann Anja., 1998, ACM SIGCOMM Computer Communication Review, V28, P42
   Freeman, 2004, TELECOMMUNICATION SY
   GARRETT MW, ACM SIGCOMM COMPUT C, V24, P269
   Heyman DP, 1992, IEEE T CIRC SYST VID, V2, P49, DOI 10.1109/76.134371
   Heyman DP, 1997, IEEE ACM T NETWORK, V5, P554, DOI 10.1109/90.649513
   HUANG C, 1995, COMPUT COMMUN REV, V25, P114
   Hughes CJ, 1993, IEEE T IMAGE PROCESS, V2, P212, DOI 10.1109/83.217224
   ISHAC J., 2001, FTP traffic generator
   Jiang WY, 2000, IEEE IC COMP COM NET, P82, DOI 10.1109/ICCCN.2000.885474
   Krunz MM, 1998, IEEE J SEL AREA COMM, V16, P733, DOI 10.1109/49.700909
   LELAND WE, 1994, IEEE ACM T NETWORK, V2, P1, DOI 10.1109/90.282603
   LIU H, 1999, P GLOB TEL C GLOBECO, V2, P1184
   LIU X.-G., 2007, Simulation results for explicit PCN marking and flow termination (Preemption)
   MAGLARIS B, 1988, IEEE T COMMUN, V36, P834, DOI 10.1109/26.2812
   MELAMED B, 1994, IEEE T COMMUN, V42, P2773, DOI 10.1109/26.328944
   PAXSON V, 1995, IEEE ACM T NETWORK, V3, P226, DOI 10.1109/90.392383
   RAMAMURTHY G, 1992, P IEEE INFOCOM 92, V2, P817
   RAYCHAUDHURI D., 1994, P 14 INT TEL C ANT J, V1a, P295
   Rose O., 1995, Proceedings. 20th Conference on Local Computer Networks (Cat. No.95TB100005), P397, DOI 10.1109/LCN.1995.527368
   ROSE O., 1993, A comparison of models for VBR video traffic sources in B-ISDN
   ROSE O., 1995, Simple and efficient models for variable bit rate MPEG video traffic
   SEGER J, 2003, P 1 INT WORKSH INT P
   SEN P, 1989, IEEE J SEL AREA COMM, V7, P865, DOI 10.1109/49.32350
   *TCPDUMP, 2008, TCPDUMP PUBL REP
   Willinger W, 1998, PRACTICAL GUIDE TO HEAVY TAILS, P27
   Willinger W, 1997, IEEE ACM T NETWORK, V5, P71, DOI 10.1109/90.554723
NR 36
TC 5
Z9 6
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2010
VL 6
IS 3
SI SI
AR 21
DI 10.1145/1823746.1823755
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 649UL
UT WOS:000281799300010
DA 2024-07-18
ER

PT J
AU Babich, F
   D'Orlando, M
   Vatta, F
AF Babich, Fulvio
   D'Orlando, Marco
   Vatta, Francesca
TI Video quality estimation in wireless IP networks: Algorithms and
   applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; experimentation; measurement; performance; distortion
   estimation; error-concealment; error-resilience; H.264; packet loss
   rate; real time video; wireless networks
AB This article proposes three methods to estimate the distortion deriving from packet losses in wireless video communication. The proposed methods take into account the short-term properties of the encoded video sequences. A suitable set of functions is adopted to model the distortion envelope resulting from multiple losses. The estimated performance is compared with the actual distortion, evaluated by decoding the received sequence with a properly designed decoder. Numerical results confirm the accuracy of the proposed models in approximating the actual Mean Square Error (MSE) for a wide range of loss rates. Some applications of the proposed algorithms are presented.
C1 [Babich, Fulvio; D'Orlando, Marco; Vatta, Francesca] Univ Trieste, Dipartimento Elettrotecn Elettron & Informat, I-34127 Trieste, Italy.
C3 University of Trieste
RP Babich, F (corresponding author), Univ Trieste, Dipartimento Elettrotecn Elettron & Informat, Via A Valerio 10, I-34127 Trieste, Italy.
EM babich@units.it; mdorlando@units.it; vatta@units.it
RI Vatta, Francesca/O-2708-2014; BABICH, FULVIO/S-5006-2016
OI Vatta, Francesca/0000-0003-4105-8084; BABICH, FULVIO/0000-0003-0121-5524
CR [Anonymous], NS MANUAL
   Babich F, 2000, ACM SIGCOMM COMP COM, V30, P37, DOI 10.1145/505688.505693
   Chakareski J, 2005, IEEE T CIRC SYST VID, V15, P1257, DOI 10.1109/TCSVT.2005.854227
   CHOI L, 2005, P IEEE INT C IM PROC, P189
   Choi LU, 2005, IEEE IMAGE PROC, P465
   CHOU PA, 2001, MICROSOFT RES, P35
   Ding W, 1996, IEEE T CIRC SYST VID, V6, P12, DOI 10.1109/76.486416
   Hayashi T, 2006, IEICE T COMMUN, VE89B, P297, DOI 10.1093/ietcom/e89-b.2.297
   He ZH, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P853, DOI 10.1109/ICME.2002.1035916
   Liang YJ, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P684
   *OPN ARCH, 2007, OPNET US MAN
   Reibman A., 2005, Proc. ACM WWW'05, P1168
   Seferoglu H, 2005, IEEE ICC, P1190
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   Webb JLH, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P13, DOI 10.1109/ICIP.1997.638661
NR 16
TC 6
Z9 7
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 1
AR 3
DI 10.1145/1324287.1324290
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 264UK
UT WOS:000253315700003
DA 2024-07-18
ER

PT J
AU Kahol, K
   Tripathi, P
   McDaniel, T
   Bratton, L
   Panchanathan, S
AF Kahol, Kanav
   Tripathi, Priyamvada
   McDaniel, Troy
   Bratton, Laura
   Panchanathan, Sethuraman
TI Modeling context in haptic perception, rendering, and visualization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 11th International Workshop on Multimedia Information Systems
CY SEP 19-21, 2005
CL Sorrento, ITALY
DE human factors; design; experimentation; haptics; haptic user interfaces;
   haptic cueing systems; context modeling
AB Haptic perception refers to the ability of human beings to perceive spatial properties through touch-based sensations. In haptics, contextual clues about material, shape, size, texture, and weight configurations of an object are perceived by individuals leading to recognition of the object and its spatial features. In this paper, we present strategies and algorithms to model context in haptic applications that allow users to haptically explore objects in virtual reality/augmented reality environments. Initial results show significant improvement in accuracy and efficiency of haptic perception in augmented reality environments when compared to conventional approaches that do not model context in haptic rendering.
C1 Arizona State Univ, Ctr Cognit Comp, Tempe, AZ 85287 USA.
C3 Arizona State University; Arizona State University-Tempe
RP Kahol, K (corresponding author), Arizona State Univ, Ctr Cognit Comp, Tempe, AZ 85287 USA.
EM kanav@asu.edu; pia@asu.edu; troy.mcdaniel@asu.edu;
   laura.bratton@asu.edu; panch@asu.edu
OI McDaniel, Troy/0000-0003-0284-8921
CR [Anonymous], IEEE ASSP MAGAZINE
   [Anonymous], 2003, TOUCHING KNOWING COG
   Deutsch D., 1975, Short-term memory
   Fritz J P, 1999, IEEE Trans Rehabil Eng, V7, P372, DOI 10.1109/86.788473
   HALE KS, 2004, IEEE COMPUT GRAPH AP, V24
   Johansson Roland S., 1996, P381, DOI 10.1016/B978-012759440-8/50025-6
   Kahol K, 2005, LECT NOTES COMPUT SC, V3665, P102
   KAHOL K, 2005, HUMAN COMPUTER INTER
   KAHOL K, 2004, IEEE INT WORKSH HAPT, V1
   KIPP M, 2001, 7 EUR C SPEECH COMM, P1367
   LEDERMAN SJ, 1987, COGNITIVE PSYCHOL, V19, P342, DOI 10.1016/0010-0285(87)90008-9
   LEDERMAN SJ, 1996, HAND BRAIN NEW ROOPH
   NIELSEN L, 1995, FURTURE REFLECTIONS, V14, P3
   REVESZ G, 1950, PHYCHOL ART BLIND
   SALISBURY K, 2004, IEEE COMPUT GRAPH, V24
   Tan H.Z., 2003, HAPTICS E, V3, P1
   Tzovaras D, 2004, IEEE T NEUR SYS REH, V12, P266, DOI 10.1109/TNSRE.2004.828756
   ZHONG S, 2002, IEEE INT JOINT C NEU
NR 18
TC 7
Z9 7
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2006
VL 2
IS 3
BP 219
EP 240
DI 10.1145/1152149.1152153
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 238IL
UT WOS:000251440700004
DA 2024-07-18
ER

PT J
AU Li, K
   Li, JX
   Guo, D
   Yang, X
   Wang, M
AF Li, Kun
   Li, Jiaxiu
   Guo, Dan
   Yang, Xun
   Wang, Meng
TI Transformer-Based Visual Grounding with Cross-Modality Interaction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual Grounding; referring expression; cross-modality interaction
AB This article tackles the challenging yet important task of Visual Grounding (VG), which aims to localize a visual region in the given image referred by a natural language query. Existing efforts on the VG task are twofold: (1) two-stage methods first extract region proposals and then rank them according to their similarities with the referring expression, which usually leads to suboptimal results due to the quality of region proposals; (2) one-stage methods usually predict all the possible coordinates of the target region online by leveraging modern object detection architectures, which pay little attention to cross-modality correlations and have limited generalization ability. To better address the task, we present an effective transformer-based end-to-end visual grounding approach, which focuses on capturing the cross-modality correlations between the referring expression and visual regions for accurately reasoning the location of the target region. Specifically, our model consists of a feature encoder, a cross-modality interactor, and a modality-agnostic decoder. The feature encoder is employed to capture the intra-modality correlation, which models the linguistic context in query and the spatial dependency in image respectively. The cross-modality interactor endows the model with the capability of highlighting the localization-relevant visual and textual cues by mutual verification of vision and language, which plays a key role in our model. The decoder learns a consolidated token representation enriched by multi-modal contexts and further directly predicts the box coordinates. Extensive experiments on five public benchmark datasets with quantitative and qualitative analysis clearly demonstrate the effectiveness and rationale of our proposed method.
C1 [Li, Kun; Li, Jiaxiu; Guo, Dan; Wang, Meng] Hefei Univ Technol HFUT, Sch Comp Sci & Informat Engn, Sch Artificial Intelligence, 485 Danxia Rd, Hefei 230601, Anhui, Peoples R China.
   [Guo, Dan; Wang, Meng] Hefei Univ Technol HFUT, Intelligent Interconnected Syst Lab Anhui Prov, 193 Tunxi Rd, Hefei 230009, Anhui, Peoples R China.
   [Guo, Dan; Wang, Meng] HFUT, Minist Educ, Key Lab Knowledge Engn Big Data, 485 Danxia Rd, Hefei 230601, Anhui, Peoples R China.
   [Guo, Dan; Wang, Meng] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, 5089 Wangjiang West Rd, Hefei 230026, Anhui, Peoples R China.
   [Yang, Xun] Univ Sci & Technol China, Sch Informat Sci & Technol, 96 JinZhai Rd Hefei, Hefei 230026, Anhui, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Guo, D (corresponding author), Hefei Univ Technol HFUT, Intelligent Interconnected Syst Lab Anhui Prov, 193 Tunxi Rd, Hefei 230009, Anhui, Peoples R China.; Guo, D (corresponding author), HFUT, Minist Educ, Key Lab Knowledge Engn Big Data, 485 Danxia Rd, Hefei 230601, Anhui, Peoples R China.; Guo, D (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, 5089 Wangjiang West Rd, Hefei 230026, Anhui, Peoples R China.; Yang, X (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, 96 JinZhai Rd Hefei, Hefei 230026, Anhui, Peoples R China.
EM kunli.hfut@gmail.com; jiaxiuli@gmail.com; guodan@hfut.edu.cn;
   xyang21@ustc.edu.cn; eric.mengwang@gmail.com
RI Li, Kun/KCZ-1653-2024
OI Li, Kun/0000-0001-5083-2145
FU National Natural Science Foundation of China [62272144, U20A20183,
   62020106007, 72188101, 62272435, U22A2094]; Major Project of Anhui
   Province [202203a05020011]
FX This work was supported by the National Natural Science Foundation of
   China (62272144, U20A20183, 62020106007, 72188101, 62272435, and
   U22A2094), the Major Project of Anhui Province (202203a05020011).
CR Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen XP, 2018, Arxiv, DOI arXiv:1812.03426
   Cirik V, 2018, AAAI CONF ARTIF INTE, P6756
   Dan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10052, DOI 10.1109/CVPR42600.2020.01007
   Deng J., 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision, P1769
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Du Ye, 2022, P IEEE INT C MULTIME
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1823, DOI 10.1145/3343031.3350881
   Guo D, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152121
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong RC, 2022, IEEE T PATTERN ANAL, V44, P684, DOI 10.1109/TPAMI.2019.2911066
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Jin WK, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321505
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Kovvuri R., 2018, P AS C COMP VIS, P451
   Li K, 2021, AAAI CONF ARTIF INTE, V35, P1902
   Liao Y., 2020, P IEEE CVF C COMP VI, P10877, DOI 10.24963/ijcai.2018/155
   Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477
   Liu JY, 2017, IEEE I CONF COMP VIS, P4866, DOI 10.1109/ICCV.2017.520
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205
   Loshchilov I, 2019, Arxiv, DOI arXiv:1711.05101
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Plummer BA, 2018, LECT NOTES COMPUT SC, V11216, P258, DOI 10.1007/978-3-030-01258-8_16
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Sadhu A, 2019, IEEE I CONF COMP VIS, P4693, DOI 10.1109/ICCV.2019.00479
   Shen L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356316
   Sibei Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P589, DOI 10.1007/978-3-030-58529-7_35
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5119, DOI 10.1145/3474085.3475620
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Wang S, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3314577
   Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Yang X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1939, DOI 10.1145/3394171.3413610
   Yang X, 2022, IEEE T IMAGE PROCESS, V31, P1204, DOI 10.1109/TIP.2022.3140611
   Yang X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1339, DOI 10.1145/3397271.3401151
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478
   Ye JB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1702, DOI 10.1145/3474085.3475313
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Yu Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1114
   Yuankai Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9979, DOI 10.1109/CVPR42600.2020.01000
   Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437
   Zheng Qi, 2021, ACMTRANSACTIONS ONMU
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
   Zhuang BH, 2018, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR.2018.00447
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 58
TC 2
Z9 2
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 183
DI 10.1145/3587251
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200006
DA 2024-07-18
ER

PT J
AU Alwaely, B
   Abhayaratne, C
AF Alwaely, Basheer
   Abhayaratne, Charith
TI GHOSM: Graph-based Hybrid Outline and Skeleton Modelling for Shape
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Graph matching; spectral graph partitioning; static hand gesture
ID HAND GESTURE RECOGNITION; OBJECT RECOGNITION; DESCRIPTORS;
   REPRESENTATION
AB An efficient and accurate shape detection model plays a major role in many research areas. With the emergence of more complex shapes in real-life applications, shape recognitionmodels need to capture the structure with more effective features to achieve high accuracy rates for shape recognition. This article presents a new method for 2D/3D shape recognition based on graph spectral domain handcrafted features, which are formulated by exploiting both an outline and a skeleton shape through the global outline and internal details. A fully connected graph is generated over the shape outline to capture the global outline representation while a hierarchically clustered graph with adaptive connectivity is formed on the skeleton to capture the structural descriptions of the shape. We demonstrate the ability of the Fiedler vector to provide the graph partitioning of the skeleton graph. The performance evaluation demonstrates the efficiency of the proposed method compared to state-of-the-art studies with increments of 4.09%, 2.2%, and 14.02% for 2D static hand gestures, 2D shapes, and 3D shapes, respectively.
C1 [Alwaely, Basheer; Abhayaratne, Charith] Univ Sheffield, Dept Elect & Elect Engn, Sheffield, S Yorkshire, England.
C3 University of Sheffield
RP Alwaely, B (corresponding author), Univ Sheffield, Dept Elect & Elect Engn, Sheffield, S Yorkshire, England.
EM Basheer.alwaely@sheffield.ac.uk; abhayaratne@sheffield.ac.uk
RI Alwaely, Basheer/AAC-3307-2020
OI Alwaely, Basheer/0000-0002-1845-2641
FU European Research Consortium for Informatics and Mathematics Alain
   Bensoussan Fellowship Program
FX The work of Dr. Alwaely was supported by the European Research
   Consortium for Informatics and Mathematics Alain Bensoussan Fellowship
   Program.
CR Aghasi A, 2018, IEEE T IMAGE PROCESS, V27, P3513, DOI 10.1109/TIP.2018.2817041
   Akimaliev M, 2015, PATTERN RECOGN, V48, P3504, DOI 10.1016/j.patcog.2015.05.010
   Alwaely B., 2019, P EUROPEAN SIGNAL PR, P1
   Alwaely B, 2020, IEEE ACCESS, V8, P182260, DOI 10.1109/ACCESS.2020.3028696
   Alwaely B, 2019, IEEE ACCESS, V7, P159661, DOI 10.1109/ACCESS.2019.2950643
   [Anonymous], 2007, P INT C IMAGE PROCES
   [Anonymous], 2011, Res Lett Inf Math Sci
   Aowal MA, 2014, TENCON IEEE REGION
   Asad M, 2013, IEEE IMAGE PROC, P3735, DOI 10.1109/ICIP.2013.6738770
   Aslan C, 2008, IEEE T PATTERN ANAL, V30, P2188, DOI 10.1109/TPAMI.2007.70842
   Ayzenberg V, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-45268-y
   Bai X, 2007, IEEE T PATTERN ANAL, V29, P449, DOI 10.1109/TPAMI.2007.59
   Bai X, 2009, IEEE I CONF COMP VIS, P575, DOI 10.1109/ICCV.2009.5459188
   Baker N, 2018, J EXP PSYCHOL GEN, V147, P1295, DOI 10.1037/xge0000409
   Brincat SL, 2004, NAT NEUROSCI, V7, P880, DOI 10.1038/nn1278
   Carcassoni M., 2002, P BRIT MACHINE VISIO, P1
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Connor CE, 2007, CURR OPIN NEUROBIOL, V17, P140, DOI 10.1016/j.conb.2007.03.002
   Cour T., 2007, Advances in Neural Information Processing Systems, V19, P313
   da Silva G. E., 2018, P EUROPEAN SIGNAL PR, P608
   Demirci MF, 2009, IEEE T PATTERN ANAL, V31, P944, DOI 10.1109/TPAMI.2008.267
   Dibra E, 2018, IEEE COMPUT SOC CONF, P1188, DOI 10.1109/CVPRW.2018.00155
   Emmert-Streib F, 2016, INFORM SCIENCES, V346, P180, DOI 10.1016/j.ins.2016.01.074
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   FIEDLER M, 1975, CZECH MATH J, V25, P619
   Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y
   Han ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3707, DOI 10.1109/TIP.2017.2704426
   Horaud R, 1995, PATTERN RECOGN, V28, P1855, DOI 10.1016/0031-3203(95)00048-8
   Jomma HD, 2016, CAN J ELECT COMPUT E, V39, P274, DOI 10.1109/CJECE.2016.2574745
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Lavoué G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li SF, 2014, IEEE T CYBERNETICS, V44, P2099, DOI 10.1109/TCYB.2014.2301193
   Li YY, 2018, ADV NEUR IN, V31
   Liu JY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3365212
   Luo L, 2015, IEEE T IMAGE PROCESS, V24, P273, DOI 10.1109/TIP.2014.2376188
   Luo XF, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3381086
   MARTINETZ T, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P397
   Masoumi M, 2017, J VIS COMMUN IMAGE R, V43, P198, DOI 10.1016/j.jvcir.2017.01.001
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Memo A., 2015, SMART TOOLS APPS GRA, P1, DOI DOI 10.2312/STAG.20151288
   Murray SO, 2003, CEREB CORTEX, V13, P508, DOI 10.1093/cercor/13.5.508
   Nanni L, 2012, PATTERN RECOGN LETT, V33, P2254, DOI 10.1016/j.patrec.2012.07.007
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Reppa I, 2019, ATTEN PERCEPT PSYCHO, V81, P1589, DOI 10.3758/s13414-019-01698-4
   Roberto P, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311748
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924
   Shamaie A, 2001, 30TH APPLIED IMAGERY PATTERN RECOGNITION WORKSHOP, PROCEEDINGS, P67, DOI 10.1109/AIPR.2001.991205
   SHAPIRO LS, 1992, IMAGE VISION COMPUT, V10, P283, DOI 10.1016/0262-8856(92)90043-3
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Taati B, 2011, COMPUT VIS IMAGE UND, V115, P681, DOI 10.1016/j.cviu.2010.11.021
   Tan YS, 2021, NEURAL COMPUT APPL, V33, P5339, DOI 10.1007/s00521-020-05337-0
   UMEYAMA S, 1988, IEEE T PATTERN ANAL, V10, P695, DOI 10.1109/34.6778
   Vranic DV, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P177, DOI 10.1109/ICME.2002.1035747
   Wang B., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P983, DOI 10.1109/ICPR.2010.246
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang X, 2020, ACM T MULTIM COMPUT, V16, DOI [10.1145/3397513, 10.1145/3397340]
   WATT RJ, 1982, VISION RES, V22, P449, DOI 10.1016/0042-6989(82)90193-6
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie J, 2015, PROC CVPR IEEE, P1275, DOI 10.1109/CVPR.2015.7298732
   Yasseen Z, 2016, PATTERN RECOGN, V57, P115, DOI 10.1016/j.patcog.2016.03.022
   ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949
   Zheng Y, 2019, IEEE T IMAGE PROCESS, V28, P5366, DOI 10.1109/TIP.2019.2919195
   Zhi SF, 2018, COMPUT GRAPH-UK, V71, P199, DOI 10.1016/j.cag.2017.10.007
   Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1774, DOI 10.1109/TPAMI.2015.2501802
   Zhou Y, 2017, INFORM SCIENCES, V409, P101, DOI 10.1016/j.ins.2017.05.009
NR 72
TC 2
Z9 2
U1 2
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 86
DI 10.1145/3554922
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300011
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Tan, CH
   Wong, K
   Baskaran, VM
   Adhinugraha, K
   Taniar, D
AF Tan, Chong Hong
   Wong, Koksheik
   Baskaran, Vishnu Monn
   Adhinugraha, Kiki
   Taniar, David
TI Is it Violin or Viola? Classifying the Instruments' Music Pieces using
   Descriptive Statistics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Musical information retrieval; musical pieces classification
ID CLASSIFICATION
AB Classifying music pieces based on their instrument sounds is pivotal for analysis and application purposes. Given its importance, techniques using machine learning have been proposed to classify violin and viola music pieces. The violin and viola are two different instruments with three overlapping strings of the same notes, and it is challenging for ordinary people or even musicians to distinguish the sound produced by these instruments. However, the classification of musical instrument pieces was barely performed by prior research. To solve this problem, we propose a technique using descriptive statistics to reliably distinguish between violin and viola music pieces. Likewise, a similar technique on the basis of histogram is introduced alongside the main descriptive statistics approach. These approaches are derived based on the nature of the instruments' strings and the range of their pieces. We also solve the problem in the current literature which divide the audio into segments for processing instead of managing the whole song. Thereby, we compile a dataset of recordings that comprises of violin and viola solo pieces from the Baroque, Classical, Romantic, and Modern eras. Experiment results suggest that our approach achieves high accuracy on solo pieces as compared to other methods with 0.97 accuracy on Baroque pieces.
C1 [Tan, Chong Hong; Wong, Koksheik; Baskaran, Vishnu Monn] Monash Univ Malaysia, Jalan Lagoon Selatan, Subang Jaya 47500, Selangor, Malaysia.
   [Adhinugraha, Kiki] La Trobe Univ, Plenty Rd, Bundoora, Vic 3086, Australia.
   [Taniar, David] Monash Univ Australia, Wellington Rd, Clayton, Vic 3800, Australia.
C3 Monash University; Monash University Malaysia; La Trobe University;
   Monash University
RP Wong, K (corresponding author), Monash Univ Malaysia, Jalan Lagoon Selatan, Subang Jaya 47500, Selangor, Malaysia.
EM tan.chonghong@monash.edu; wong.koksheik@monash.edu;
   vishnu.monn@monash.edu; K.Adhinugraha@latrobe.edu.au;
   david.taniar@monash.edu
RI ; Wong, KokSheik/B-9796-2011
OI Adhinugraha, Kiki Maulana/0000-0001-5884-1409; Wong,
   KokSheik/0000-0002-4893-2291; Baskaran, Vishnu Monn/0000-0001-6809-5817;
   Taniar, David/0000-0002-8862-3960
FU Advanced Engineering Platform's Cluster Funding [AEP-2021-Cluster-04];
   Monash University Malaysia
FX This work is supported by Advanced Engineering Platform's Cluster
   Funding (account number AEP-2021-Cluster-04), Monash University
   Malaysia.
CR Agostini G, 2003, EURASIP J APPL SIG P, V2003, P5, DOI 10.1155/S1110865703210118
   [Anonymous], 2013, P INT S COMP MUS MUL
   [Anonymous], 2023, ACM T MULTIM COMPUT, V19
   Aucouturier JJ, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P105, DOI 10.1109/ICME.2002.1035729
   Avci Kemal, 2018, 26 SIGNAL PROCESSING, P1, DOI [10.1109/SIU.2018.8404422, DOI 10.1109/SIU.2018.8404422]
   Bartsch MA, 2005, IEEE T MULTIMEDIA, V7, P96, DOI 10.1109/TMM.2004.840597
   Bittner R. M., 2014, P 15 INT SOC MUS INF, V14, P155
   Bosch J. J., 2012, ISMIR, P559, DOI DOI 10.5281/ZENODO.1416075
   Brown JC, 2001, J ACOUST SOC AM, V109, P1064, DOI 10.1121/1.1342075
   Chou SY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3336
   Durian Eric, 1997, U IOWA MUSICAL INSTR
   Eronen A, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P19, DOI 10.1109/ASPAA.2001.969532
   GEORGE A, 1995, EARLY MUSIC, V23, P341
   Goto M., 2002, P 3 INT C MUS INF RE, V2, P287
   Han Y, 2017, IEEE-ACM T AUDIO SPE, V25, P208, DOI 10.1109/TASLP.2016.2632307
   Hung Y., 2018, PROC INT SOC MUSIC I, P135
   Jolliffe I., 2011, International Encyclopedia of Statistical Science, P1094, DOI [DOI 10.1007/978-3-642-04898-2_455, 10.1007/978-3-642-04898-2_455]
   Kaminskyj I, 2005, J INTELL INF SYST, V24, P199, DOI 10.1007/s10844-005-0323-7
   Kostek B, 2004, P IEEE, V92, P712, DOI 10.1109/JPROC.2004.825903
   Kurth F, 2008, IEEE T AUDIO SPEECH, V16, P382, DOI 10.1109/TASL.2007.911552
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li PT, 2015, Arxiv, DOI [arXiv:1511.05520, DOI 10.48550/ARXIV.1511.05520]
   Lidy T, 2005, P 6 INT C MUS INF RE, P34
   McFee B., 2015, P PYTH SCI C AUST TX, P18, DOI 10.25080/Majora-7b98e3ed-003
   McKay Cory., 2006, Proceedings of the 7th International Conference on Music Information Retrieval, P101
   Müller M, 2005, IEEE WORK APPL SIG, P275, DOI 10.1109/ASPAA.2005.1540223
   Park T, 2015, Arxiv, DOI arXiv:1512.07370
   Parr Freya, 2018, WHATS DIFFERENCE VIO
   Peeters G., 2000, Proceedings of International Computer Music Conference, P166
   Roche E, 2019, EARLY MUSIC, V47, P132
   StringOvation Team, 2018, ROMANTIC PERIOD OFMU
   Subotnik Eva E., 2015, ESTATE PLANNING EJOU, V29, P82
   Tseng YH, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P176, DOI 10.1145/312624.312675
   Van Balen Jan, 2013, P 14 SOC MUSIC INFOR, P107
   Wang Q, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9060950
   Western Michigan University Team, 2012, MODERN ART MUSIC
   Yu Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3281746
NR 37
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 93
DI 10.1145/3563218
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300018
DA 2024-07-18
ER

PT J
AU Song, D
   Zhang, CM
   Zhao, XQ
   Wang, T
   Nie, WZ
   Li, XY
   Liu, AA
AF Song, Dan
   Zhang, Chu-Meng
   Zhao, Xiao-Qian
   Wang, Teng
   Nie, Wei-Zhi
   Li, Xuan-Ya
   Liu, An-An
TI Self-supervised Image-based 3D Model Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image-based 3D model retrieval; self-supervised learning; contrastive
   learning; domain adaptation; discriminative feature representation
AB Image-based 3D model retrieval aims at organizing unlabeled 3D models according to the relevance to the labeled 2D images. With easy accessibility of 2D images and wide applications of 3D models, image-based 3D model retrieval attracts more and more attentions. However, it is still a challenging problem due to the modality gap between 2D images and 3D models. In spite of the remarkable progress brought by domain adaptation techniques for this research topic, which usually propose to align the global distribution statistics of two domains, these methods are limited in learning discriminative features for target samples due to the lack of label information in target domain. In this article, besides utilizing the label information of 2D image domain and the adversarial domain alignment, we additionally incorporate self-supervision to address crossdomain 3D model retrieval problem. Specifically, we simultaneously optimize the adversarial adaptation for both domains based on visual features and the contrastive learning for unlabeled 3D model domain to help the feature extractor to learn discriminative feature representations. The contrastive learning is used to map view representations of the identical model nearby while view representations of different models far apart. To guarantee adequate and high-quality negative samples for contrastive learning, we design a memory bank to store and update representative view for each 3D model based on entropy minimization principle. Comprehensive experimental results on the public image-based 3D model retrieval datasets, i.e., MI3DOR and MI3DOR-2, have demonstrated the effectiveness of the proposed method.
C1 [Song, Dan; Zhang, Chu-Meng; Zhao, Xiao-Qian; Wang, Teng; Nie, Wei-Zhi; Liu, An-An] Tianjin Univ, Weijin Rd 92, Tianjin 300072, Peoples R China.
   [Song, Dan; Liu, An-An] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
   [Li, Xuan-Ya] Baidu, Beijing 100085, Peoples R China.
C3 Tianjin University; Baidu
RP Li, XY (corresponding author), Baidu, Beijing 100085, Peoples R China.
EM dan.song@tju.edu.cn; zcm0520@tju.edu.cn; zhaoxiaoqian96@tju.edu.cn;
   wt18233772226@tju.edu.cn; weizhinie@tju.edu.cn; lixuanya@baidu.com;
   anan0422@gmail.com
OI nie, weizhi/0000-0002-0578-8138
FU National Nature Science Foundation of China [61902277, U21B2024];
   National Key Research and Development Program of China [2020YFB1406602];
   Baidu Program
FX This work was supported in part by the National Nature Science
   Foundation of China (61902277, U21B2024), the National Key Research and
   Development Program of China (2020YFB1406602), and the Baidu Program.
CR Cao C, 2019, PROCEEDINGS WEB3D 2019: THE 24TH INTERNATIONAL ACM CONFERENCE ON 3D WEB TECHNOLOGY, DOI 10.1145/3329714.3338130
   Caron M, 2019, IEEE I CONF COMP VIS, P2959, DOI 10.1109/ICCV.2019.00305
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chen T, 2020, PR MACH LEARN RES, V119
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao Y, 2020, NEUROCOMPUTING, V410, P174, DOI 10.1016/j.neucom.2020.05.032
   Gao Z, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377876
   Gidaris S., 2018, P 6 INT C LEARNING R
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   Hamdi A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1, DOI 10.1109/ICCV48922.2021.00007
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kim D, 2018, IEEE WINT CONF APPL, P793, DOI 10.1109/WACV.2018.00092
   Kolesnikov A, 2019, PROC CVPR IEEE, P1920, DOI 10.1109/CVPR.2019.00202
   Li JC, 2021, PROC CVPR IEEE, P2505, DOI 10.1109/CVPR46437.2021.00253
   Li W, 2019, 12 EUR WORKSH 3D OBJ, P103, DOI DOI 10.2312/3DOR.20191068
   Liu AA, 2021, INFORM SCIENCES, V547, P984, DOI 10.1016/j.ins.2020.09.057
   Long M., 2018, ADV NEURAL INFORM PR, P1647
   Long MS, 2017, PR MACH LEARN RES, V70
   Lu X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P715, DOI 10.1145/3331184.3331217
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Mundhenk TN, 2018, PROC CVPR IEEE, P9339, DOI 10.1109/CVPR.2018.00973
   Nie WZ, 2022, IEEE T CIRC SYST VID, V32, P992, DOI 10.1109/TCSVT.2021.3070969
   Nie WZ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3344684
   Nie WZ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P908, DOI 10.1145/3343031.3351009
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Saito K, 2017, PR MACH LEARN RES, V70
   Saito Kuniaki, 2018, P 6 INT C LEARNING R
   Song D, 2021, IEEE T MULTIMEDIA, V23, P2721, DOI 10.1109/TMM.2020.3015554
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3387920
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, 10.48550/arXiv.1412.3474, DOI 10.48550/ARXIV.1412.3474]
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wu L, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486251
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xia HF, 2020, PROC CVPR IEEE, P4363, DOI 10.1109/CVPR42600.2020.00442
   Xie SA, 2018, PR MACH LEARN RES, V80
   Xu JL, 2019, IEEE ACCESS, V7, P156694, DOI 10.1109/ACCESS.2019.2949697
   Xu YZ, 2020, IEEE T MULTIMEDIA, V22, P2950, DOI 10.1109/TMM.2020.2966882
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yuan D, 2021, IEEE T IMAGE PROCESS, V30, P976, DOI 10.1109/TIP.2020.3037518
   Yuan J., 2019, P EUR WORKSH 3D OBJ, P33, DOI DOI 10.2312/3DOR.20191059
   Zellinger Werner, 2017, P 5 INT C LEARNING R
   Zhang DW, 2021, Arxiv, DOI arXiv:2104.07918
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76
   Zhou HY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1667, DOI 10.1145/3343031.3351011
NR 55
TC 2
Z9 2
U1 6
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 70
DI 10.1145/3548690
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000020
DA 2024-07-18
ER

PT J
AU Xue, F
   Yang, T
   Liu, K
   Hong, ZK
   Cao, MW
   Guo, D
   Hong, RC
AF Xue, Feng
   Yang, Tian
   Liu, Kang
   Hong, Zikun
   Cao, Mingwei
   Guo, Dan
   Hong, Richang
TI LCSNet: End-to-end Lipreading with Channel-aware Feature Selection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Lipreading; deep neural network; channel attention mechanism; selective
   feature fusion module
AB Lipreading is a task of decoding the movement of the speaker's lip region into text. In recent years, lipreading methods based on deep neural network have attracted widespread attention, and the accuracy has far surpassed that of experienced human lipreaders. The visual differences in some phonemes are extremely subtle and pose a great challenge to lipreading. Most of the lipreading existing methods do not process the extracted visual features, which mainly suffer from two problems. First, the extracted features contain lot of useless information such as noise caused by differences in speech speed and lip shape, for example. In addition, the extracted features are not abstract enough to distinguish phonemes with similar pronunciation. These problems have a bad effect on the performance of lipreading. To extract features from the lip regions that are more distinguishable and more relevant to the speech content, this article proposes an end-to-end deep neural networkbased lipreading model (LCSNet). The proposed model extracts the short-term spatio-temporal features and the motion trajectory features from the lip region in the video clips. The extracted features are filtered by the channel attention module to eliminate the useless features and then used as input to the proposed Selective Feature Fusion Module (SFFM) to extract the high-level abstract features. Afterwards, these features are used as input to the bidirectional GRU network in time order for temporal modeling to obtain the long-term spatiotemporal features. Finally, a Connectionist Temporal Classification (CTC) decoder is used to generate the output text. The experimental results show that the proposed model achieves a 1.0% CER and 2.3% WER on the GRID corpus database, which, respectively, represents an improvement of 52% and 47% compared to LipNet.
C1 [Xue, Feng] Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Intelligent Interconnected Syst Lab Anhui Prov, Minist Educ, 485 Danxia Rd, Hefei 230031, Anhui, Peoples R China.
   [Yang, Tian; Liu, Kang; Hong, Zikun; Guo, Dan; Hong, Richang] Hefei Univ Technol, Sch Informat Sci & Technol, 485 Danxia Rd, Hefei 230031, Anhui, Peoples R China.
   [Cao, Mingwei] Anhui Univ, Sch Informat Sci & Technol, 485 Danxia Rd, Hefei 230031, Anhui, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology; Anhui
   University
RP Xue, F (corresponding author), Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Intelligent Interconnected Syst Lab Anhui Prov, Minist Educ, 485 Danxia Rd, Hefei 230031, Anhui, Peoples R China.
EM feng.xue@hfut.edu.cn; 2019111009@mail.hfut.edu.cn;
   kangliu1225@gmail.com; hongzikun@mail.hfut.edu.cn; cmwqq2008@163.com;
   guodan@hfut.edu.cn; hongrc.hfut@gmail.com
OI Xue, Feng/0000-0003-4962-9734; liu, kang/0000-0001-6789-1811
FU seventh special support plan for innovation and entrepreneurship of
   Anhui Province, China
FX This work is supported by the seventh special support plan for
   innovation and entrepreneurship of Anhui Province, China.
CR Alizadeh S, 2008, INT CONF SIGN PROCES, P561, DOI 10.1109/ICOSP.2008.4697195
   Amodio A., 2018, IEEE T INTELL TRANSP, V2018, P1
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bai SJ, 2018, Arxiv, DOI [arXiv:1803.01271, DOI 10.48550/ARXIV.1803.01271]
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen JY, 2008, LECT NOTES COMPUT SC, V5359, P236, DOI 10.1007/978-3-540-89646-3_23
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Graves A, 2014, PR MACH LEARN RES, V32, P1764
   Hao MF, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7988, DOI 10.1109/ICASSP39728.2021.9414659
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hilder S., 2009, C AUDIO VISUAL SPEEC
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang YY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2492, DOI 10.1145/3474085.3475420
   Jie H., 2017, PROC CVPR IEEE, V99
   Kingma D. P, 2015, International Conference on Learning Representations
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li M, 2008, 2008 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY, VOLS 1 AND 2, PROCEEDINGS, P361, DOI 10.1109/CIS.2008.214
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liu JL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4328, DOI 10.1145/3394171.3413740
   Assael YM, 2016, Arxiv, DOI arXiv:1611.01599
   Ma PC, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7608, DOI 10.1109/ICASSP39728.2021.9415063
   Martinez B, 2020, INT CONF ACOUST SPEE, P6319, DOI [10.1109/ICASSP40776.2020.9053841, 10.1109/icassp40776.2020.9053841]
   Noda K, 2014, INTERSPEECH, P1149
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Potamianos G, 2001, INT CONF ACOUST SPEE, P165, DOI 10.1109/ICASSP.2001.940793
   Shaikh A. A., 2010, 2010 3rd International Congress on Image and Signal Processing (CISP 2010), P327, DOI 10.1109/CISP.2010.5646264
   Srivastava R.K., 2015, ARXIV
   Stafylakis T, 2017, INTERSPEECH, P3652, DOI 10.21437/Interspeech.2017-85
   Sutskever I, 2014, ADV NEUR IN, V27
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu K, 2018, IEEE INT CONF AUTOMA, P548, DOI 10.1109/FG.2018.00088
   Yang WM, 2019, IEEE SIGNAL PROC LET, V26, P538, DOI 10.1109/LSP.2018.2890770
   Zhang YH, 2020, IEEE INT CONF AUTOMA, P356, DOI 10.1109/FG47880.2020.00134
   Zhao Y., 2019, ACM MULTIMEDIA ASIA
   Zhao Y, 2020, AAAI CONF ARTIF INTE, V34, P6917
   Zhou YAN, 2020, PROC CVPR IEEE, P4776, DOI 10.1109/CVPR42600.2020.00483
NR 42
TC 3
Z9 3
U1 4
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
DI 10.1145/3524620
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800003
DA 2024-07-18
ER

PT J
AU Chen, WD
   Li, GR
   Zhang, XF
   Wang, SH
   Li, L
   Huang, QM
AF Chen, Weidong
   Li, Guorong
   Zhang, Xinfeng
   Wang, Shuhui
   Li, Liang
   Huang, Qingming
TI Weakly Supervised Text-based Actor-Action Video Segmentation by
   Clip-level Multi-instance Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multiple instance learning; weakly supervised learning; video
   actor-action segmentation; cross-modal learning
AB In real-world scenarios, it is common that a video contains multiple actors and their activities. Selectively localizing one specific actor and its action spatially and temporally via a language query becomes a vital and challenging task. Existing fully supervised methods require extensive elaborately annotated data and are sensitive to the class labels, which cannot satisfy real-world applications' needs. Thus, we introduce the task of weakly supervised actor-action video segmentation from a sentence query (AAVSS) in this work, where only the video-sentence pairs are provided. To the best of our knowledge, our work is the first to perform AAVSS under weakly supervised situations. However, this task is extremely challenging not only because the task aims to learn the complex interactions between two heterogeneous modalities but also because the task needs to learn fine-grained analysis of video content without pixel-level annotations. To overcome the challenges, we propose a two-stage network. The network first follows the sentence guidance to localize the candidate region and then performs segmentation to achieve selective segmentation. Specifically, a novel tracker-based clip-level multiple instance learning paradigm is proposed in this article to learn the matches between regions and sentences, which makes our two-stage network robust to the region proposal network. Furthermore, two intrinsic characteristics of the video, temporal consistency and motion information, are utilized in companion with the weak supervision to facilitate the region-query matching. Through extensive experiments, the proposed method achieves comparable performance to state-of-the-art fully supervised approaches on two large-scale benchmarks, including A2D Sentences and J-HMDB Sentences.
C1 [Chen, Weidong; Li, Guorong; Huang, Qingming] UCAS, Sch Comp Sci & Technol, Key Lab Big Data Min & Knowledge Management, Beijing, Peoples R China.
   [Zhang, Xinfeng] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Wang, Shuhui; Li, Liang] Chinese Acad Sci, ICT, Key Lab Intelligent Informat Proc, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences
RP Li, GR (corresponding author), UCAS, Sch Comp Sci & Technol, Beijing 100049, Peoples R China.; Li, GR (corresponding author), UCAS, Key Lab Big Data Min & Knowledge Management, Beijing 100049, Peoples R China.
EM weidong.chen@vipl.ict.ac.cn; liguorong@ucas.ac.cn; xfzhang@ucas.ac.cn;
   wangshuhui@ict.ac.cn; liang.li@ict.ac.cn; qmhuang@ucas.ac.cn
RI Zhang, Xinfeng/X-8148-2019; Weidong, Chen/GPC-8523-2022; Li,
   Guorong/AAG-1594-2020
OI Chen, Weidong/0000-0003-2774-2875; Zhang, Xinfeng/0000-0002-7517-3868
FU Italy-China Collaboration Project TALENT [2018YFE0118400]; National
   Natural Science Foundation of China [61772494, 61620106009, 61872333,
   61931008, 61836002, 61976069, 62022083]; Youth Innovation Promotion
   Association CAS; Fundamental Research Funds for Central Universities;
   China Postdoctoral Science Foundation [2021M691683]
FX This work was supported in part by the Italy-China Collaboration Project
   TALENT under Grant No. 2018YFE0118400; in part by the National Natural
   Science Foundation of China under Grants No. 61772494, No. 61620106009,
   No. 61872333, No. 61931008, No. 61836002, No. 61976069, and No.
   62022083; in part by the Youth Innovation Promotion Association CAS; in
   part by the Fundamental Research Funds for Central Universities; and in
   part by the China Postdoctoral Science Foundation Funded Project No.
   2021M691683.
CR [Anonymous], 2017, PROC CVPR IEEE
   Chen Junwen, 2020, P ACM MM
   Chen K, 2018, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2018.00425
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Chen WD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4053, DOI 10.1145/3474085.3475534
   Chen YH, 2018, LECT NOTES COMPUT SC, V11070, P91, DOI 10.1007/978-3-030-00928-1_11
   Chen Zhenfang, 2019, P ACL
   Dhiman C, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441628
   Fan JS, 2020, PROC CVPR IEEE, P4282, DOI 10.1109/CVPR42600.2020.00434
   Fan Junsong, 2020, P AAAI
   Fan Ruochen, 2018, P ECCV
   Gao J., 2017, P ICCV
   Gavrilyuk K, 2018, PROC CVPR IEEE, P5958, DOI 10.1109/CVPR.2018.00624
   Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21
   Han Tengda, 2020, P NEURIPS
   HaoWang Cheng Deng, 2020, P AAAI
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou Q., 2018, P NEURIPS
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Huang DA, 2018, PROC CVPR IEEE, P5948, DOI 10.1109/CVPR.2018.00623
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Ji WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446792
   Jie Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9898, DOI 10.1109/CVPR42600.2020.00992
   Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee Jungbeom, 2019, PROC CVPR IEEE
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Li Xueyi, 2021, P AAAI
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Liu Xinfang, 2021, ACM T MULTIM COMPUT, V17, P1
   Liu XJ, 2019, IEEE I CONF COMP VIS, P2611, DOI 10.1109/ICCV.2019.00270
   Liu XJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P539, DOI 10.1145/3343031.3351074
   Liu Yongfei, 2021, P CVPR
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   McIntosh Bruce, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9939, DOI 10.1109/CVPR42600.2020.00996
   Ning K, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P948
   Piergiovanni AJ, 2019, PR MACH LEARN RES, V97
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shi Hengcan, 2018, P ECCV
   Shi J, 2019, PROC CVPR IEEE, P10436, DOI 10.1109/CVPR.2019.01069
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song CF, 2019, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2019.00325
   Sun MJ, 2021, IEEE T PATTERN ANAL, V43, P4189, DOI 10.1109/TPAMI.2021.3058684
   Tang M, 2018, LECT NOTES COMPUT SC, V11220, P524, DOI 10.1007/978-3-030-01270-0_31
   Tang PJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303083
   Vernaza P, 2017, PROC CVPR IEEE, P2953, DOI 10.1109/CVPR.2017.315
   Xiankai Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8957, DOI 10.1109/CVPR42600.2020.00898
   Xiao JS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419842
   Xu CL, 2016, PROC CVPR IEEE, P3083, DOI 10.1109/CVPR.2016.336
   Xu CL, 2015, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2015.7298839
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu M., 2020, CVPR, P10156
   Yamaguchi M, 2017, IEEE I CONF COMP VIS, P1462, DOI 10.1109/ICCV.2017.162
   Yan Y, 2017, PROC CVPR IEEE, P1022, DOI 10.1109/CVPR.2017.115
   Yang X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1939, DOI 10.1145/3394171.3413610
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zeng Yu, 2019, PROC CVPR IEEE
   Zhang BF, 2020, AAAI CONF ARTIF INTE, V34, P12765
   Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437
   Zhou Luowei, 2018, P BMVC
   Zhu SY, 2020, ACM T SENSOR NETWORK, V16, DOI 10.1145/3399678
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
NR 69
TC 4
Z9 4
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 12
DI 10.1145/3514250
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400012
DA 2024-07-18
ER

PT J
AU Wu, XT
   Yao, P
AF Wu, Xiaotian
   Yao, Peng
TI Boolean-based Two-in-One Secret Image Sharing by Adaptive Pixel Grouping
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Secret sharing; secret image sharing; visual cryptography; Boolean
   operation
ID VISUAL CRYPTOGRAPHY; SCHEME; STEGANOGRAPHY; IMPROVEMENTS
AB The two-in-one secret image sharing (TiOSIS) technique is a hybrid scheme that protects a secret image by combining visual cryptography (VCS) and polynomial-based secret image sharing (PSIS). There are two decoding methods available in TiOSIS: stacking-to-see decryption and lossless image recovery. However, the majority of current TiOSIS methods use Lagrange interpolation to precisely reconstruct the secret, which would result in intense computations. In this article, an efficient TiOSIS scheme using Boolean XOR operation for lossless image recovery is proposed. The proposed scheme consists of three building blocks: shared data generation, shadow construction, and image decryption. In shared data generation, the grayscale secret image is processed by a Boolean-based SIS to derive the shared bits. In shadow construction, an adaptive pixel grouping (APG) strategy is utilized to determine a grouping pattern. The halftone image adjustment algorithm is adopted to generate a suitable halftone image. With the grouping pattern and halftone image, we construct the shadows via the group-pixel embedding and sharing approach. In image decryption, we can reveal the secret image by stacking-to-see decoding or Boolean-based lossless image recovery. Extensive experiments and comparisons are illustrated to show the effectiveness and benefits of the proposed scheme.
C1 [Wu, Xiaotian; Yao, Peng] Jinan Univ, Dept Comp Sci, Guangzhou, Peoples R China.
C3 Jinan University
RP Wu, XT (corresponding author), Jinan Univ, Dept Comp Sci, Guangzhou, Peoples R China.
EM wxt.sysu@gmail.com; 983689542@qq.com
FU National Natural Science Foundation of China [61972179]; Guangdong Basic
   and Applied Basic Research Foundation [2020A1515011476]
FX This work was partially supported by National Natural Science Foundation
   of China (Grant No. 61972179), Guangdong Basic and Applied Basic
   Research Foundation (Grant No. 2020A1515011476).
CR Arce GR., 2018, MODERN DIGITAL HALFT, DOI [10.1201/9781315219790, DOI 10.1201/9781315219790]
   Ateniese G, 1996, INFORM COMPUT, V129, P86, DOI 10.1006/inco.1996.0076
   Blundo C, 2001, DESIGN CODE CRYPTOGR, V24, P255, DOI 10.1023/A:1011271120274
   Chang CC, 2008, PATTERN RECOGN, V41, P3130, DOI 10.1016/j.patcog.2008.04.006
   Fu MS, 2002, IEEE T IMAGE PROCESS, V11, P477, DOI 10.1109/TIP.2002.999680
   Jia XX, 2018, IEEE T CIRC SYST VID, V28, P1056, DOI 10.1109/TCSVT.2016.2631404
   Li P, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102911
   Li P, 2018, J REAL-TIME IMAGE PR, V14, P41, DOI 10.1007/s11554-016-0621-z
   Li P, 2013, J VIS COMMUN IMAGE R, V24, P1380, DOI 10.1016/j.jvcir.2013.09.010
   Li P, 2013, J VIS COMMUN IMAGE R, V24, P1106, DOI 10.1016/j.jvcir.2013.07.005
   Li P, 2012, J VIS COMMUN IMAGE R, V23, P441, DOI 10.1016/j.jvcir.2012.01.003
   Lin CC, 2004, J SYST SOFTWARE, V73, P405, DOI 10.1016/S0164-1212(03)00239-5
   Lin SJ, 2007, PATTERN RECOGN, V40, P3652, DOI 10.1016/j.patcog.2007.04.001
   Liu YX, 2021, IEEE T INTELL TRANSP, V22, P3952, DOI 10.1109/TITS.2020.2994386
   Liu YX, 2017, SIGNAL PROCESS-IMAGE, V58, P49, DOI 10.1016/j.image.2017.06.011
   Liu ZQ, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3418212
   Naor M., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P1, DOI 10.1007/BFb0053419
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Shyu SJ, 2015, IEEE T CIRC SYST VID, V25, P1557, DOI 10.1109/TCSVT.2015.2389372
   Shyu SJ, 2011, IEEE T INF FOREN SEC, V6, P960, DOI 10.1109/TIFS.2011.2158096
   Thien CC, 2002, COMPUT GRAPH-UK, V26, P766
   ULICHNEY R, 1993, P SOC PHOTO-OPT INS, V1913, P332, DOI 10.1117/12.152707
   Wang RZ, 2007, SIGNAL PROCESS-IMAGE, V22, P363, DOI 10.1016/j.image.2006.12.012
   Wu X, 2012, IET INFORM SECUR, V6, P299, DOI 10.1049/iet-ifs.2012.0046
   Wu XT, 2020, MULTIMED TOOLS APPL, V79, P25657, DOI 10.1007/s11042-020-09253-2
   Wu XT, 2019, DIGIT SIGNAL PROCESS, V93, P22, DOI 10.1016/j.dsp.2019.06.016
   Wu XT, 2019, J VIS COMMUN IMAGE R, V61, P74, DOI 10.1016/j.jvcir.2019.03.020
   Wu XT, 2013, SIGNAL PROCESS, V93, P977, DOI 10.1016/j.sigpro.2012.11.014
   Yan XH, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419750
   Yan XH, 2021, IEEE T CIRC SYST VID, V31, P2896, DOI 10.1109/TCSVT.2020.3025527
   Yan XH, 2019, SIGNAL PROCESS-IMAGE, V71, P66, DOI 10.1016/j.image.2018.11.002
   Yang CN, 2007, J SYST SOFTWARE, V80, P1070, DOI 10.1016/j.jss.2006.11.022
   Yang CN, 2021, IEEE T CIRC SYST VID, V31, P2465, DOI 10.1109/TCSVT.2020.3017126
   Yang CN, 2016, J SYST SOFTWARE, V116, P22, DOI 10.1016/j.jss.2015.01.031
   Yang CN, 2012, OPT COMMUN, V285, P1725, DOI 10.1016/j.optcom.2011.12.003
   Yang CN, 2010, IMAGE VISION COMPUT, V28, P1600, DOI 10.1016/j.imavis.2010.04.003
   Yang CN, 2010, OPT COMMUN, V283, P1750, DOI 10.1016/j.optcom.2009.12.077
   Yang CN, 2004, PATTERN RECOGN LETT, V25, P481, DOI 10.1016/j.patrec.2003.12.011
   Zarepour-Ahmadabadi J, 2016, INFORM SCIENCES, V369, P467, DOI 10.1016/j.ins.2016.07.001
NR 39
TC 7
Z9 7
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 14
DI 10.1145/3517140
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400014
DA 2024-07-18
ER

PT J
AU Duan, MX
   Li, KL
   Deng, JY
   Xiao, B
   Tian, Q
AF Duan, Mingxing
   Li, Kenli
   Deng, Jiayan
   Xiao, Bin
   Tian, Qi
TI A Novel Multi-Sample Generation Method for Adversarial Attacks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Black-box attacks; GAN; multi-task; substitute model
AB Deep learning models are widely used in daily life, which bring great convenience to our lives, but they are vulnerable to attacks. How to build an attack system with strong generalization ability to test the robustness of deep learning systems is a hot issue in current research, among which the research on black-box attacks is extremely challenging. Most current research on black-box attacks assumes that the input dataset is known. However, in fact, it is difficult for us to obtain detailed information for those datasets. In order to solve the above challenges, we propose a multi-sample generation model for black-box model attacks, called MsGM. MsGM is mainly composed of three parts: multi-sample generation, substitute model training, and adversarial sample generation and attack. Firstly, we design a multi-task generation model to learn the distribution of the original dataset. The model first converts an arbitrary signal of a certain distribution into the shared features of the original dataset through deconvolution operations, and then according to different input conditions, multiple identical sub-networks generate the corresponding targeted samples. Secondly, the generated sample features achieve different outputs through querying the black-box model and training the substitute model, which are used to construct different loss functions to optimize and update the generator and substitute model. Finally, some common white-box attack methods are used to attack the substitute model to generate corresponding adversarial samples, which are utilized to attack the black-box model. We conducted a large number of experiments on the MNIST and CIFAR-10 datasets. The experimental results show that under the same settings and attack algorithms, MsGM achieves better performance than the based models.
C1 [Duan, Mingxing] Hunan Univ, Sch Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
   [Li, Kenli] Hunan Univ, Sch Logist Informat, Changsha 410000, Hunan, Peoples R China.
   [Deng, Jiayan] Hunan Modern Logist Coll, Sch Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
   [Xiao, Bin] Hong Kong Polytech Univ, Dept Comp, Hong Kong 99907, Peoples R China.
   [Tian, Qi] Huawei, Cloud BU, Shenzhen 518129, Peoples R China.
C3 Hunan University; Hunan University; Hong Kong Polytechnic University;
   Huawei Technologies
RP Li, KL (corresponding author), Hunan Univ, Sch Logist Informat, Changsha 410000, Hunan, Peoples R China.; Deng, JY (corresponding author), Hunan Modern Logist Coll, Sch Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China.
EM duanmingxing@hnu.edu.cn; lkl@hnu.edu.cn; dengjiayan2016@163.com;
   b.xiao@polyu.edu.hk; tian.qi1@huawei.com
RI Xiao, Bin/AAZ-4848-2020
OI Xiao, Bin/0000-0003-4223-8220; Mingxing, Duan/0000-0003-2281-0672
FU National Key-Research and Development Program of China [2020YFB2104003];
   Open Fund of Science and Technology on Parallel and Distributed
   Processing Laboratory [6142110200205]; Shenzhen Excellent Technological
   and Innovative Talent Training Foundation [RCBS20200714114941176];
   Science and Education Joint Project of Natural Science Foundation of
   Hunan Province [2020JJ7056]; Hong Kong Scholars Program [XJ2020032]
FX This work was supported in part by the National Key-Research and
   Development Program of China under Grant No. 2020YFB2104003, in part by
   the Open Fund of Science and Technology on Parallel and Distributed
   Processing Laboratory under Grant 6142110200205, in part by the Shenzhen
   Excellent Technological and Innovative Talent Training Foundation under
   Grant RCBS20200714114941176, in part by the Science and Education Joint
   Project of Natural Science Foundation of Hunan Province under Grant
   2020JJ7056. This article is funded by the Hong Kong Scholars Program
   under Grants XJ2020032.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Abouelnaga Y, 2016, 2016 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE & COMPUTATIONAL INTELLIGENCE (CSCI), P1192, DOI [10.1109/CSCI.2016.0225, 10.1109/CSCI.2016.224]
   Apruzzese G, 2020, IEEE TETCI, V4, P427, DOI 10.1109/TETCI.2019.2961157
   Brendel W., 2018, ICLR
   Brunner T, 2019, IEEE I CONF COMP VIS, P4957, DOI 10.1109/ICCV.2019.00506
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen JH, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1739, DOI 10.1145/3394486.3403225
   Chen PY, 2018, AAAI CONF ARTIF INTE, P10
   Chen PY, 2017, P 10 ACM WORKSH ART, P15, DOI [10.1145/3128572.3140448, DOI 10.1145/3128572.3140448]
   Chen X., 2020, P P IEEECVF C COMPUT, P10176
   Duan MX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P264, DOI 10.1145/3474085.3475542
   Duan MX, 2021, IEEE T CIRC SYST VID, V31, P608, DOI 10.1109/TCSVT.2020.2981117
   Duan MX, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3379449
   Duan MX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355542
   Duan MX, 2018, IEEE T INF FOREN SEC, V13, P758, DOI 10.1109/TIFS.2017.2766583
   Duan RJ, 2020, PROC CVPR IEEE, P997, DOI 10.1109/CVPR42600.2020.00108
   Fan Yanbo, 2020, P 16 EUR C COMP VIS
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goodfellow Ian., 2015, STAT-US
   Hang J, 2020, PATTERN RECOGN, V101, DOI 10.1016/j.patcog.2019.107184
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huai MD, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P472, DOI 10.1145/3394486.3403089
   Ilyas A., 2018, PR MACH LEARN RES, P2137
   Jandial S., 2019, P IEEE CVF INT C COM
   Karim F, 2021, IEEE T PATTERN ANAL, V43, P3309, DOI 10.1109/TPAMI.2020.2986319
   Kurakin Alexey, 2017, INT C LEARN REPR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li GL, 2020, PROC CVPR IEEE, P797, DOI 10.1109/CVPR42600.2020.00088
   Li PC, 2018, IEEE DATA MINING, P1200, DOI 10.1109/ICDM.2018.00159
   Liu Y, 2017, 5 INT C LEARNING REP
   Lu YT, 2020, PROC CVPR IEEE, P937, DOI 10.1109/CVPR42600.2020.00102
   Madry A., 2018, ARXIV
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Orekondy T, 2019, PROC CVPR IEEE, P4949, DOI 10.1109/CVPR.2019.00509
   Pang R, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1899, DOI 10.1145/3394486.3403241
   Papernot N, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P506, DOI 10.1145/3052973.3053009
   Shi YC, 2020, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR42600.2020.00111
   Shi YC, 2019, PROC CVPR IEEE, P6512, DOI 10.1109/CVPR.2019.00668
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su D, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1045, DOI 10.1145/3394486.3403148
   Tang RX, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P218, DOI 10.1145/3394486.3403064
   Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131
   Wang JY, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P15, DOI 10.1145/3394486.3403044
   Wang Y, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P95, DOI 10.1145/3394486.3403052
   Wu JQ, 2020, IEEE T INF FOREN SEC, V15, P2282, DOI 10.1109/TIFS.2019.2963764
   Xiao CW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3905
   Xiao WL, 2020, 2020 INFORMATION COMMUNICATION TECHNOLOGIES CONFERENCE (ICTC), P141, DOI [10.1109/ICTC49638.2020.9123270, 10.1109/ictc49638.2020.9123270]
   Zelun Kong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14242, DOI 10.1109/CVPR42600.2020.01426
   Zhang YH, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2989, DOI 10.1145/3394486.3403349
   Zheng HZ, 2020, PROC CVPR IEEE, P1178, DOI 10.1109/CVPR42600.2020.00126
   Zhou MY, 2020, PROC CVPR IEEE, P231, DOI 10.1109/CVPR42600.2020.00031
   Zügner D, 2020, ACM T KNOWL DISCOV D, V14, DOI 10.1145/3394520
NR 52
TC 16
Z9 17
U1 2
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 112
DI 10.1145/3506852
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600023
DA 2024-07-18
ER

PT J
AU Kumar, SS
   Nandhini, M
AF Kumar, S. Sambath
   Nandhini, M.
CA Alzheimers Dis Neuroimaging Initia
TI Entropy Slicing Extraction and Transfer Learning Classification for
   Early Diagnosis of Alzheimer Diseases with sMRI
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Alzheimer's disease; ConvNets; medical diagnostic imaging; sMRI; deep
   learning; convolutional neural network; transfer learning
ID CONVOLUTIONAL NEURAL-NETWORKS; FEATURE-SELECTION; PROGRESSION; GRAY
AB Alzheimer's Disease (AD) is an irreversible neurogenerative disorder that undergoes progressive decline in memory and cognitive function and is characterized by structural brain Magnetic Resonance Images (sMRI). In recent years, sMRI data has played a vital role in the evaluation of brain anatomical changes, leading to early detection of AD through deep networks. The existing AD problems such as preprocessing complexity and unreliability are major concerns at present. To overcome these, a model (FEESCTL) has been proposed with an entropy slicing for feature extraction and Transfer Learning for classification. In the present study, the entropy image slicing method is attempted for selecting the most informative MRI slices during training stages. The ADNI dataset is trained on Transfer Learning adopted by VGG-16 network for classifying the AD with normal individuals. The experimental results reveal that the proposed model has achieved an accuracy level of 93.05%, 86.39%, 92.00% for binary classifications (AD/MCI, MCI/CN, AD/CN) and 93.12% for ternary classification (AD/MCI/CN), respectively, and henceforth the efficiency in diagnosing AD is proved through comparative analysis.
C1 [Kumar, S. Sambath; Nandhini, M.] Pondicherry Univ, Dept Comp Sci, Pondicherry 605014, India.
C3 Pondicherry University
RP Kumar, SS (corresponding author), Pondicherry Univ, Dept Comp Sci, Pondicherry 605014, India.
EM sambathkumars06@gmail.com; mnandhini2005@yahoo.com
RI kumar, sambath/IAN-9539-2023
OI kumar, sambath/0009-0003-4283-736X
FU Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes
   of Health) [U01 AG024904]; DOD ADNI (Department of Defense)
   [W81XWH-12-2-0012]; National Institute on Aging; National Institute of
   Biomedical Imaging and Bioengineering; AbbVie; Alzheimer's Association;
   Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica,
   Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate;
   Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company;
   EuroImmun; F. Hoffmann-La Roche Ltd and its Genentech, Inc.; Fujirebio;
   GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research &
   Development, LLC.; Johnson & Johnson Pharmaceutical Research &
   Development LLC.; Lumosity; Lundbeck; Merck Co., Inc.; Meso Scale
   Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis
   Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier;
   Takeda Pharmaceutical Company; Transition Therapeutics; Canadian
   Institutes of Health Research
FX Data collection and sharing for this project was funded by the
   Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes
   of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award
   number W81XWH-12-2-0012). ADNI is funded by the National Institute on
   Aging, the National Institute of Biomedical Imaging and Bioengineering,
   and through generous contributions from the following: AbbVie,
   Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon
   Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company;
   CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli
   Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its
   affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO
   Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.;
   Johnson & Johnson Pharmaceutical Research & Development LLC.; Lumosity;
   Lundbeck; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx
   Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation;
   Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company;
   and Transition Therapeutics. The Canadian Institutes of Health Research
   is providing funds to support ADNI clinical sites in Canada. Private
   sector contributions are facilitated by the Foundation for the National
   Institutes of Health (www.fnih.org).The grantee organization is the
   Northern California Institute for Research and Education, and the study
   is coordinated by the Alzheimer's Therapeutic Research Institute at the
   University of Southern California. ADNI data are disseminated by the
   Laboratory for Neuro Imaging at the University of Southern California,
   Los Angeles.
CR Agarwal A, 2012, ANN STAT, V40, P1171, DOI 10.1214/12-AOS1000
   Alberdi A, 2016, ARTIF INTELL MED, V71, P1, DOI 10.1016/j.artmed.2016.06.003
   Ashburner J, 2000, NEUROIMAGE, V11, P805, DOI 10.1006/nimg.2000.0582
   Beheshti I, 2016, COMPUT METH PROG BIO, V137, P177, DOI 10.1016/j.cmpb.2016.09.019
   Cheng B, 2019, BRAIN IMAGING BEHAV, V13, P138, DOI 10.1007/s11682-018-9846-8
   Coupé P, 2012, NEUROIMAGE-CLIN, V1, P141, DOI 10.1016/j.nicl.2012.10.002
   Cuingnet R, 2011, NEUROIMAGE, V56, P766, DOI 10.1016/j.neuroimage.2010.06.013
   Daoud M, 2019, ARTIF INTELL MED, V97, P204, DOI 10.1016/j.artmed.2019.01.006
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1, DOI DOI 10.1007/978-0-387-84858-7
   Gao XHW, 2016, PROCEEDINGS OF THE 2016 SAI COMPUTING CONFERENCE (SAI), P28, DOI 10.1109/SAI.2016.7555958
   Gaugler J, 2019, ALZHEIMERS DEMENT, V15, P321, DOI 10.1016/j.jalz.2019.01.010
   Gauthier S, 2006, LANCET, V367, P1262, DOI 10.1016/S0140-6736(06)68542-5
   Gopalakrishnan K, 2017, CONSTR BUILD MATER, V157, P322, DOI 10.1016/j.conbuildmat.2017.09.110
   Gunawardena KANNP, 2017, I C MECH MACH VIS PR, P173
   Hanyu H, 2010, J NEUROL SCI, V290, P96, DOI 10.1016/j.jns.2009.10.022
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang YC, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00509
   Jack CR, 2008, J MAGN RESON IMAGING, V27, P685, DOI 10.1002/jmri.21049
   Kingma D. P., 2014, arXiv
   Kononenko I, 2001, ARTIF INTELL MED, V23, P89, DOI 10.1016/S0933-3657(01)00077-X
   Kruthika K. R., 2019, Informatics in Medicine Unlocked, V14, P34, DOI 10.1016/j.imu.2018.12.003
   Kumar SS, 2019, IIOAB J, V10, P21, DOI 10.15740/has/irjaes/10.1/21-26
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lei BY, 2016, FRONT AGING NEUROSCI, V8, DOI 10.3389/fnagi.2016.00077
   Li F, 2018, COMPUT MED IMAG GRAP, V70, P101, DOI 10.1016/j.compmedimag.2018.09.009
   Li H., 2017, PRUNING FILTERS EFFI, DOI DOI 10.48550/ARXIV.1608.08710
   Li XJ, 2017, LECT NOTES ARTIF INT, V10604, P519, DOI 10.1007/978-3-319-69179-4_36
   Liu MH, 2018, NEUROINFORMATICS, V16, P295, DOI 10.1007/s12021-018-9370-4
   Liu SQ, 2015, IEEE T BIO-MED ENG, V62, P1132, DOI 10.1109/TBME.2014.2372011
   Lu SY, 2019, J COMPUT SCI-NETH, V30, P41, DOI 10.1016/j.jocs.2018.11.008
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Maguire EA, 2000, P NATL ACAD SCI USA, V97, P4398, DOI 10.1073/pnas.070039597
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Phillips JS, 2018, NEUROBIOL AGING, V63, P75, DOI 10.1016/j.neurobiolaging.2017.11.008
   Previtali F, 2017, COMPUT METH PROG BIO, V143, P89, DOI 10.1016/j.cmpb.2017.03.006
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salat DH, 1999, ARCH NEUROL-CHICAGO, V56, P338, DOI 10.1001/archneur.56.3.338
   Schmitter D, 2015, NEUROIMAGE-CLIN, V7, P7, DOI 10.1016/j.nicl.2014.11.001
   Suk HI, 2014, NEUROIMAGE, V101, P569, DOI 10.1016/j.neuroimage.2014.06.077
   Suk HI, 2013, LECT NOTES COMPUT SC, V8184, P131, DOI 10.1007/978-3-319-02267-3_17
   Talo M, 2019, COGN SYST RES, V54, P176, DOI 10.1016/j.cogsys.2018.12.007
   Walhovd KB, 2010, AM J NEURORADIOL, V31, P347, DOI 10.3174/ajnr.A1809
   Wu GR, 2015, NEUROIMAGE, V106, P34, DOI 10.1016/j.neuroimage.2014.11.025
   Ye TT, 2016, BRAIN IMAGING BEHAV, V10, P739, DOI 10.1007/s11682-015-9437-x
   Zhang DQ, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0033182
   Zhang DQ, 2012, NEUROIMAGE, V59, P895, DOI 10.1016/j.neuroimage.2011.09.069
   Zhang DQ, 2011, NEUROIMAGE, V55, P856, DOI 10.1016/j.neuroimage.2011.01.008
   Zhou K, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8081372
NR 50
TC 19
Z9 22
U1 4
U2 45
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 40
DI 10.1145/3383749
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000003
DA 2024-07-18
ER

PT J
AU Cui, CR
   Lin, PG
   Nie, XS
   Jian, MW
   Yin, YL
AF Cui, Chaoran
   Lin, Peiguang
   Nie, Xiushan
   Jian, Muwei
   Yin, Yilong
TI Social-sensed Image Aesthetics Assessment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image aesthetics assessment; user perception modeling; multi-task
   learning; social sense
AB Image aesthetics assessment aims to endow computers with the ability to judge the aesthetic values of images, and its potential has been recognized in a variety of applications. Most previous studies perform aesthetics assessment purely based on image content. However, given the fact that aesthetic perceiving is a human cognitive activity, it is necessary to consider users' perception of an image when judging its aesthetic quality. In this article, we regard users' social behavior as the reflection of their perception of images and harness these additional clues to improve image aesthetics assessment. Specifically, we first merge the raw social interactions between users and images into clusters as the social labels of images, so the collective social behavioral information associated with an image can be well represented over a structured and compact space. Then, we develop a novel deep multi-task network to jointly learn social labels in different modalities from social images and apply it to common web images. In this manner, our approach is readily generalized to web images without social behavioral information. Finally, we introduce a high-level fusion sub-network to the aesthetics model, in which the social and visual representations of images are well balanced for aesthetics assessment. Experimental results on two benchmark datasets well verify the effectiveness of our approach and highlight the benefits of different types of social behavioral information for image aesthetics assessment.
C1 [Cui, Chaoran; Lin, Peiguang; Jian, Muwei] Shandong Univ Finance & Econ, Jinan, Shandong, Peoples R China.
   [Nie, Xiushan] Shandong Jianzhu Univ, Jinan 250101, Peoples R China.
   [Yin, Yilong] Shandong Univ, Jinan, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong Jianzhu University;
   Shandong University
RP Cui, CR (corresponding author), Shandong Univ Finance & Econ, Jinan, Shandong, Peoples R China.; Yin, YL (corresponding author), Shandong Univ, Jinan, Peoples R China.
EM crcui@sdufe.edu.cn; llpwgh@163.com; niexiushan@163.com;
   jianmuweihk@163.com; ylyin@sdu.edu.cn
RI Jian, Muwei/Q-8319-2018
OI Jian, Muwei/0000-0002-4249-2264
FU National Natural Science Foundation of China [61701281, 61876098];
   National Key R&D Program of China [2018YFC0830100, 2018YFC0830102];
   Natural Science Foundation of Shandong Province [ZR2017QF009]; Fostering
   Project of Dominant Discipline and Talent Team of Shandong Province
   Higher Education Institutions
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61701281 and 61876098, the National Key R&D Program
   of China under Grants 2018YFC0830100 and 2018YFC0830102, the Natural
   Science Foundation of Shandong Province under Grant ZR2017QF009, and the
   Fostering Project of Dominant Discipline and Talent Team of Shandong
   Province Higher Education Institutions.
CR Albarracín D, 2000, J PERS SOC PSYCHOL, V79, P5, DOI 10.1037//0022-3514.79.1.5
   [Anonymous], 2017, IEEE C COMPUT VIS PA
   [Anonymous], 2013, TMM
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90
   [Anonymous], 2014, TOIS
   Azam S, 2016, 2016 IEEE 15TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC), P535, DOI 10.1109/ICCI-CC.2016.7862089
   Cui CR, 2020, INFORM SCIENCES, V512, P780, DOI 10.1016/j.ins.2019.10.011
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Cui P, 2016, IEEE MULTIMEDIA, V23, P92, DOI 10.1109/MMUL.2016.8
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Gao Y, 2019, PROC CVPR IEEE, P3200, DOI 10.1109/CVPR.2019.00332
   Geng B., 2011, Proceedings of the 19th ACM international conference on multimedia, P63
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jin X, 2018, AAAI CONF ARTIF INTE, P77
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Kawakami R, 2019, IEEE IMAGE PROC, P3636, DOI [10.1109/icip.2019.8803687, 10.1109/ICIP.2019.8803687]
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Khosla A, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P867, DOI 10.1145/2566486.2567996
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Kucer M, 2018, IEEE T IMAGE PROCESS, V27, P5100, DOI 10.1109/TIP.2018.2845100
   Liu A, 2018, P 27 INT JOINT C ART, P821, DOI DOI 10.24963/IJCAI.2018/114
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu SW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P109, DOI 10.1145/2733373.2806247
   Liu SW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P617, DOI 10.1145/2647868.2654905
   Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197
   Lovato P, 2014, IEEE T INF FOREN SEC, V9, P364, DOI 10.1109/TIFS.2014.2298370
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Negoescu R.A., 2008, Proc. Content-based Image and Video Retrieval, P417, DOI DOI 10.1145/1386352.1386406
   Nie L., 2020, ACM T INF SYST, V38, P1
   Nie L., 2019, SYNTHESIS LECT IMAGE, V9, P1
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Ruder S, 2017, arXiv preprint arXiv, DOI DOI 10.48550/ARXIV.1706.05098
   Ruder Sebastian, 2017, stat, V1050, P23
   Segalin C, 2017, IEEE T AFFECT COMPUT, V8, P268, DOI 10.1109/TAFFC.2016.2516994
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   van Zwol Roelof., 2010, Multimedia, P1015
   Wang Y., 2012, Proceedings of the 20th ACM International Conference on Multimedia, P865, DOI [DOI 10.1145/2393347.239633216, 10.1145/2393347.239633216]
   Wei YW, 2020, IEEE T IMAGE PROCESS, V29, P1, DOI 10.1109/TIP.2019.2923608
   Xu N, 2019, IEEE T CIRC SYST VID, V29, P2482, DOI 10.1109/TCSVT.2018.2867286
   Zhao YL, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2502415
NR 55
TC 1
Z9 1
U1 0
U2 36
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 3
SU S
AR 103
DI 10.1145/3414843
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2RB
UT WOS:000612586300011
DA 2024-07-18
ER

PT J
AU Gutterman, C
   Guo, K
   Arora, S
   Gilliland, T
   Wang, XY
   Wu, L
   Katz-Bassett, E
   Zussman, G
AF Gutterman, Craig
   Guo, Katherine
   Arora, Sarthak
   Gilliland, Trey
   Wang, Xiaoyang
   Wu, Les
   Katz-Bassett, Ethan
   Zussman, Gil
TI Requet: Real-Time QoE Metric Detection for Encrypted YouTube Traffic
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Machine learning; HTTP adaptive streaming
AB As video traffic dominates the Internet, it is important for operators to detect video quality of experience (QoE) to ensure adequate support for video traffic. With wide deployment of end-to-end encryption, traditional deep packet inspection based traffic monitoring approaches are becoming ineffective. This poses a challenge for network operators to monitor user QoE and improve upon their experience. To resolve this issue, we develop and present a system for REal-time QUality of experience metric detection for Encrypted Traffic-Requet-which is suitable for network middlebox deployment. Requet uses a detection algorithm that we develop to identify video and audio chunks from the IP headers of encrypted traffic. Features extracted from the chunk statistics are used as input to a machine learning algorithm to predict QoE metrics, specifically buffer warning (low buffer, high buffer), video state (buffer increase, buffer decay, steady, stall), and video resolution. We collect a large YouTube dataset consisting of diverse video assets delivered over various WiFi and LTE network conditions to evaluate the performance. We compare Requet with a baseline system based on previous work and show that Requet outperforms the baseline system in accuracy of predicting buffer low warning, video state, and video resolution by 1.12x, 1.53x, and 3.14x, respectively.
C1 [Gutterman, Craig; Arora, Sarthak; Gilliland, Trey; Katz-Bassett, Ethan; Zussman, Gil] Columbia Univ, Dept Elect Engn, 500 W 120 St,Room 1300, New York, NY 10027 USA.
   [Guo, Katherine; Wang, Xiaoyang; Wu, Les] Nokia Bell Labs, 600 Mt Ave Bldg 5, New Providence, NJ 07974 USA.
C3 Columbia University; Nokia Corporation; Nokia Bell Labs
RP Gutterman, C (corresponding author), Columbia Univ, Dept Elect Engn, 500 W 120 St,Room 1300, New York, NY 10027 USA.
EM clg2168@columbia.edu; katherine.guo@nokia-bell-labs.com;
   sa3522@columbia.edu; jlg2266@columbia.edu;
   xiaoyang.wang@nokia-bell-labs.com; les.j.wu@nokia-bell-labs.com;
   ethan@ee.columbia.edu; gil.zussman@columbia.edu
FU NSF [CNS-1650685, CNS-1910757, DGE 16-44869]
FX This work was supported in part by NSF grants CNS-1650685, CNS-1910757,
   and DGE 16-44869.
CR 3GPP, 2010, 26234 3GPP TS
   Aggarwal Vaneet, 2014, P ACM HOT MOB
   Ahmed Adnan, 2017, P IEEE ICNP
   Amann Johanna, 2017, P ACM IMC C
   [Anonymous], 2018, 2018 10 INT C QUAL M
   [Anonymous], 2011, P ACM SIGCOMM
   Armasu Lucian, 2016, NETFLIX ADOPTS EFFIC
   Bronzino Francesco, 2020, ARXIV190105800
   Casas Pedro, 2013, Performance Evaluation Review, V41, P44
   Cisco, 2019, CISC ENCR TRAFF AN
   CISCO, CISC ANN INT REP 201
   Cofano G., 2016, P ACM MMSYS
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cui Y, 2017, IEEE INTERNET COMPUT, V21, P72, DOI 10.1109/MIC.2017.44
   Dimopoulos Giorgos, 2016, P ACM IMC
   Durumeric Zakir, 2017, P NDSS
   Fielding R., 2014, Hypertext Transfer Protocol: Message Syntax and Routing, DOI [10.17487/rfc7230, DOI 10.17487/RFC7230]
   Fortune, 2016, GOOGL IS MAK YOUTUBE
   Galetto S., 2017, P IEEE MN
   Guarnieri T, 2017, IEEE GLOB COMM CONF
   Gutterman Craig, 2019, P ACM MMSYS
   Ho Tin Kam, 1995, P IEEE ICDAR
   Huang T.-Y., 2014, P ACM SIGCOMM
   Kakhki Arash Molavi, 2017, P ACM IMC
   Krishnamoorthi V., 2017, P ACM MMSYS
   Law Will, 2018, TECHNICAL REPORT
   Li Feng, 2018, P ACM NOSSDAV
   Lin Y.-T., 2017, P IEEE ICC
   Madanapalli Sharat Chandra, 2019, P IEEE TMA
   Mangla T., 2018, P IEEE TMA
   Mangla Tarun, 2017, P IEEE TMA
   Mansy Ahmed, 2014, P ACM MOVID
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mazhar M. H., 2018, P IEEE INFOCOM
   Mondal Abhijit, 2017, P NOSSDAV
   Nicolas Weil, STATE MPEG DASH 2016
   Orsolic I, 2016, IEEE GLOBE WORK
   Petrangeli S, 2017, J NETW COMPUT APPL, V94, P78, DOI 10.1016/j.jnca.2017.07.009
   Razaghpanah Abbas, 2017, P ACM CONEXT
   Reed Andrew, 2017, P CODASPY
   Sarah Wassermann Michael, 2019, P ACM MOBICOM INT QO
   Schmitt P., 2018, P TPRC46
   Schwarzmann Susanna, 2019, P ACM MOBICOM INT QO
   Seufert Michael, 2019, P IEEE INFOCOM NETW
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Telerik, TEL FIDDL FREE WEB D
   Tsilimantos D, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P138, DOI 10.1145/3204949.3204955
   Tsilimantos Dimitrios, 2018, P ACM MMSYS
   Vasilev V, 2018, IEEE ICC
   Wamser Florian, 2015, P EUCNC
NR 50
TC 11
Z9 11
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 71
DI 10.1145/3394498
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FH
UT WOS:000583712600014
DA 2024-07-18
ER

PT J
AU Wu, LX
   Xu, M
   Qian, SS
   Cui, JW
AF Wu, Lingxiang
   Xu, Min
   Qian, Shengsheng
   Cui, Jianwei
TI Image to Modern Chinese Poetry Creation via a Constrained Topic-aware
   Model
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image captioning; poetry generation; semantic consistency
ID HITCH HAIKU
AB Artificial creativity has attracted increasing research attention in the field of multimedia and artificial intelligence. Despite the promising work on poetry/painting/music generation, creating modern Chinese poetry from images, which can significantly enrich the functionality of photo-sharing platforms, has rarely been explored. Moreover, existing generation models cannot tackle three challenges in this task: (1) Maintaining semantic consistency between images and poems; (2) preventing topic drift in the generation; (3) avoidance of certain words appearing frequently. These three points are even common challenges in other sequence generation tasks. In this article, we propose a Constrained Topic-aware Model (CTAM) to create modern Chinese poetries from images regarding the challenges above. Without image-poetry paired dataset, we construct a visual semantic vector to embed visual contents via image captions. For the topic-drift problem, we propose a topic-aware poetry generation model. Additionally, we design an Anti-frequency Decoding (AFD) scheme to constrain high-frequency characters in the generation. Experimental results show that our model achieves promising performance and is effective in poetry's readability and semantic consistency.
C1 [Wu, Lingxiang; Xu, Min] Univ Technol Sydney, Sch Elect & Data Engn, 15 Broadway, Ultimo, NSW 2007, Australia.
   [Qian, Shengsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, 95 Zhongguancun Rd, Beijing, Peoples R China.
   [Qian, Shengsheng] Univ Chinese Acad Sci, 95 Zhongguancun Rd, Beijing, Peoples R China.
   [Cui, Jianwei] Xiaomi Corp, 68 Qinghezhong St, Beijing, Peoples R China.
C3 University of Technology Sydney; Chinese Academy of Sciences; Institute
   of Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Wu, LX (corresponding author), Univ Technol Sydney, Sch Elect & Data Engn, 15 Broadway, Ultimo, NSW 2007, Australia.
EM Lingxiang.Wu@student.uts.edu.au; Min.Xu@uts.edu.au;
   shengsheng.qian@nlpr.ia.ac.cn; cuijianwei@xiaomi.com
RI cui, jian/IAN-2010-2023
OI Xu, Min/0000-0001-9581-8849
CR [Anonymous], 2015, ICLR
   [Anonymous], 2012, Computational Creativity, Concept Invention, and General Intelligence
   [Anonymous], 2017, CVPR
   [Anonymous], 2013, P INT C LEARN REPR I
   [Anonymous], 2012, ICCC
   [Anonymous], 2015, 3 INT C LEARNING REP
   [Anonymous], 2004, TEXT SUMMARIZATION B
   [Anonymous], 2014, P 2014 C EMPIRICAL M
   [Anonymous], 2016, P 26 INT C COMP LING
   [Anonymous], 2016, P INT C LEARN REPR I
   Bahl L. R., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P49
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Choi Y, 2011, P 15 C COMPUTATIONAL, P220
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Ghazvininejad Marjan, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, P1183, DOI 10.18653/v1/D16-1126
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He C., 2019, ACM T MULTIM COMPUT, V15, P1
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hopkins J, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P168, DOI 10.18653/v1/P17-1016
   Kalchbrenner Nal, 2016, P INT C LEARN REPR I
   Kulkarni Girish, 2011, P 24 C COMP VIS PATT
   Li J, 2016, RES ASTRON ASTROPHYS, V16, DOI 10.1088/1674-4527/16/7/110
   Liu B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P783, DOI 10.1145/3240508.3240587
   Liu SH, 2018, SMALL METHODS, V2, DOI 10.1002/smtd.201800040
   Liu Y, 2017, AAAI CONF ARTIF INTE, P1445
   Mathews A, 2016, AAAI CONF ARTIF INTE, P3574
   Mihalcea Rada, 2004, EMNLP
   Netzer Yael., 2009, Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, P32, DOI DOI 10.3115/1642011.1642016
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Qixin Wang Tianyi, 2016, P INT JOINT C ART IN
   Sturm B., 2016, ARXIV160408723
   Sutskever I, 2014, ADV NEUR IN, V27
   Tosa N, 2008, LECT NOTES COMPUT SC, V5309, P209
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang AQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3226037
   Wang BY, 2016, 2016 ASIA-PACIFIC CONFERENCE ON INTELLIGENT ROBOT SYSTEMS (ACIRS 2016), P26, DOI 10.1109/ACIRS.2016.7556182
   WILLIAMS N, 1992, DESIGN, P5
   Wu J, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3271485
   Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815
   Wu XF, 2009, LECT NOTES COMPUT SC, V5709, P191
   Xing C, 2017, AAAI CONF ARTIF INTE, P3351
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu Linli, 2018, P 32 AAAI C ART INT
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Yang ZL, 2016, ADV NEUR IN, V29
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang JY, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1364, DOI 10.18653/v1/P17-1125
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 55
TC 7
Z9 7
U1 3
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 53
DI 10.1145/3381858
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600014
DA 2024-07-18
ER

PT J
AU More, A
   Chaudhuri, S
AF More, Amit
   Chaudhuri, Subhasis
TI A Pseudo-likelihood Approach for Geo-localization of Events from
   Crowd-sourced Sensor-Metadata
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Pseudo-likelihood; HMM; von-Mises distribution; GPS; digital compass;
   smartphones; crowd sourcing; video analysis; event localization
ID RETRIEVAL; PHOTOS; MODEL
AB Events such as live concerts, protest marches, and exhibitions are often video recorded by many people at the same time, typically using smartphone devices. In this work, we address the problem of geo-localizing such events from crowd-generated data. Traditional approaches for solving such a problem using multiple video sequences of the event would require highly complex computer vision (CV) methods, which are computation intensive and are not robust under the environment where visual data are collected through crowd-sourced medium. In the present work, we approach the problem in a probabilistic framework using only the sensor metadata obtained from smartphones. We model the event location and camera locations and orientations (camera parameters) as the hidden states in a Hidden Markov Model. The sensor metadata from GPS and the digital compass from user smartphones are used as the observations associated with the hidden states of the model. We have used a suitable potential function to capture the complex interaction between the hidden states (i.e., event location and camera parameters). The non-Gaussian densities involved in the model, such as the potential function involving hidden states, make the maximum-likelihood estimation intractable. We propose a pseudo-likelihood-based approach to maximize the approximate-likelihood, which provides a tractable solution to the problem. The experimental results on the simulated as well as real data show correct event geo-localization using the proposed method. When compared with several baselines the proposed method shows a superior performance. The overall computation time required is much smaller, since only the sensor metadata are used instead of visual data.
C1 [More, Amit; Chaudhuri, Subhasis] Indian Inst Technol, Dept Elect Engn, EE112, Mumbai 400076, Maharashtra, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Bombay
RP More, A (corresponding author), Indian Inst Technol, Dept Elect Engn, EE112, Mumbai 400076, Maharashtra, India.
EM amitmore@ee.iitb.ac.in; sc@ee.iitb.ac.in
CR [Anonymous], 2013, P 21 ACM INT C MULT, DOI DOI 10.1145/2502081.2502151
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2016, P 7 INT C MULT SYST
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2007, Proc. Int. Conf. Mach. Learn
   Arth C, 2012, INT C PATT RECOG, P2152
   BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Bui TH, 2017, MULTIMED TOOLS APPL, V76, P23435, DOI 10.1007/s11042-016-4114-7
   Cai YH, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0165-6
   Cao LL, 2010, INT CONF ACOUST SPEE, P2274, DOI 10.1109/ICASSP.2010.5495905
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Crandall DJ, 2016, ADV COMPUT VIS PATT, P121, DOI 10.1007/978-3-319-25781-5_7
   Cui HN, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2449557
   Dargahi F, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON SERVICES COMPUTING (SCC 2014), P1, DOI 10.1109/SCC.2014.10
   Erdem AT, 2015, IEEE T IMAGE PROCESS, V24, P538, DOI 10.1109/TIP.2014.2380176
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933
   GYGLI M, 2015, PROC CVPR IEEE, P3090, DOI DOI 10.1109/CVPR.2015.7298928
   Han ZG, 2016, T GIS, V20, P701, DOI 10.1111/tgis.12175
   Hao J, 2014, IEEE T MULTIMEDIA, V16, P1929, DOI 10.1109/TMM.2014.2330802
   Irschara A., 2011, Proc. of IEEE Conference on Computer Vision and Pattern Recognition Workshops, P21
   Ji RR, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2597181
   Jiang K, 2013, NEUROCOMPUTING, V119, P17, DOI 10.1016/j.neucom.2012.02.049
   Kisilevich Slava, 2010, P ACM INT C EXH COMP, V38
   Kurz D, 2011, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2011.5995339
   Lacerda Y.A., 2012, Proceedings_of_the_18th_Brazilian_Symposium on_Multimedia_and_the_Web, WebMedia'12, P281
   Lee I, 2014, EXPERT SYST APPL, V41, P397, DOI 10.1016/j.eswa.2013.07.065
   Li ZC, 2015, IEEE T IMAGE PROCESS, V24, P5343, DOI 10.1109/TIP.2015.2479560
   Li ZC, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2488732
   Lu Y, 2016, GEOINFORMATICA, V20, P829, DOI 10.1007/s10707-016-0250-5
   Luo JB, 2011, MULTIMED TOOLS APPL, V51, P187, DOI 10.1007/s11042-010-0623-y
   Majid A, 2015, DATA KNOWL ENG, V95, P66, DOI 10.1016/j.datak.2014.11.001
   Mardia K.V., 2000, Directional Statistics, V2
   Min WQ, 2014, IEEE IMAGE PROC, P3112, DOI 10.1109/ICIP.2014.7025629
   Min WQ, 2014, IEEE MULTIMEDIA, V21, P20, DOI 10.1109/MMUL.2014.1
   More A, 2016, TENTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS AND IMAGE PROCESSING (ICVGIP 2016), DOI 10.1145/3009977.3009993
   Ouyang RW, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P23, DOI 10.1145/2493432.2493455
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Ramachandran M, 2011, IEEE T PATTERN ANAL, V33, P186, DOI 10.1109/TPAMI.2010.163
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P921, DOI 10.1109/TMM.2013.2237896
   Thanh-Hieu Bui, 2016, International Conference on Information Science and Applications (ICISA) 2016. LNEE 376, P1013, DOI 10.1007/978-981-10-0557-2_97
   Thomee B, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2854004
   Wang JL, 2016, Adv Inform Managemen, P1097, DOI 10.1109/IMCEC.2016.7867381
   Yang YY, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P883
   Yu L, 2016, MULTIMED TOOLS APPL, V75, P3199, DOI 10.1007/s11042-014-2430-3
   Zhang Y, 2016, IEEE T MULTIMEDIA, V18, P418, DOI 10.1109/TMM.2016.2520827
   Zhu C, 2015, AAAI CONF ARTIF INTE, P3878
   Zhu C, 2015, IEEE T IMAGE PROCESS, V24, P5619, DOI 10.1109/TIP.2015.2483376
   Zhu Chao, 2016, P AAAI C ART INT AI
NR 50
TC 1
Z9 1
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2019
VL 15
IS 3
AR 75
DI 10.1145/3321701
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ6YR
UT WOS:000494301200007
DA 2024-07-18
ER

PT J
AU Tang, PJ
   Wang, HL
   Li, QY
AF Tang, Pengjie
   Wang, Hanli
   Li, Qinyu
TI Rich Visual and Language Representation with Complementary Semantics for
   Video Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video captioning; long short term memory; convolutional neural network;
   sequential voting; complementary features
AB It is interesting and challenging to translate a video to natural description sentences based on the video content. In this work, an advanced framework is built to generate sentences with coherence and rich semantic expressions for video captioning. A long short term memory (LSTM) network with an unproved factored way is first developed, which takes the inspiration of LSTM with a conventional factored way and a common practice to feed multi-modal features into LSTM at the first time step for visual description. Then, the incorporation of the LSTM network with the proposed improved factored way and un-factored way is exploited, and a voting strategy is utilized to predict candidate words. In addition, for robust and abstract visual and language representation, residuals are employed to enhance the gradient signals that are learned from the residual network (ResNet), and a deeper LSTM network is constructed. Furthermore, three convolutional neural network based features extracted from GoogLeNet, ResNet101, and ResNet152, are fused to catch more comprehensive and complementary visual information. Experiments are conducted on two benchmark datasets, including MSVD and MSR-VTT2016, and competitive performances are obtained by the proposed techniques as compared to other state-of-the-art methods.
C1 [Tang, Pengjie; Wang, Hanli; Li, Qinyu] Tongji Univ, Dept Comp Sci & Technol, Shanghai 201804, Peoples R China.
   [Tang, Pengjie] Jinggangshan Univ, Coll Math & Phys, Jian 343009, Jiangxi, Peoples R China.
   [Li, Qinyu] Lanzhou City Univ, Dept Comp Sci, Lanzhou 730070, Gansu, Peoples R China.
C3 Tongji University; Jinggangshan University; Lanzhou City University
RP Wang, HL (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Shanghai 201804, Peoples R China.
EM 5tang-pengjie@tongji.edu.cn; hanliwang@tongji.edu.cn;
   qinyu.li@tongji.edu.cn
RI Wang, Hanli/G-5111-2014; yuan, lin/JDW-7387-2023
OI Wang, Hanli/0000-0002-9999-4871; 
FU National Natural Science Foundation of China [61622115, 61472281];
   Program for Professor of Special Appointment (Eastern Scholar) at
   Shanghai Institutions of Higher Learning [GZ2015005]; Shanghai
   Engineering Research Center of Industrial Vision Perception &
   Intelligent Computing [17DZ2251600]; IBM Shared University Research
   Awards Program
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61622115 and 61472281, Program for Professor of
   Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher
   Learning (No. GZ2015005), Shanghai Engineering Research Center of
   Industrial Vision Perception & Intelligent Computing (17DZ2251600), and
   IBM Shared University Research Awards Program.
CR [Anonymous], P EUR C COMP VIS
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2018, IEEE INT CON MULTI
   [Anonymous], N AM CHAPTER ASS COM
   [Anonymous], 2016, PROC 24 ACM INT C MU, DOI [DOI 10.1145/2964284.2984065, 10.1145/2964284.2984065]
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2016, P 24 ACM INT C MULTI, DOI DOI 10.1145/2964284.2984066
   [Anonymous], 2004, ANN M ASS COMP LING
   [Anonymous], 2019, IEEE T CYBERNETICS, DOI DOI 10.1109/TCYB.2018.2831447
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], P EUR C COMP VIS
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2017, NEW MODEL KINETICS D
   [Anonymous], ADAPTIVE FEATURE ABS
   [Anonymous], 2017, CVPR
   Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339
   Biswas P, 2005, I CONF VLSI DESIGN, P651
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong JF, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1082, DOI 10.1145/2964284.2984064
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Krishnamoorthy N., 2013, P AAAI C ART INT, P541
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mnih A., 2007, P 24 INT C MACH LEAR, P641, DOI [DOI 10.1145/1273496.1273577, 10.1145/1273496.1273577]
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Shaw D, 2011, IMPACT OF THE ECONOMIC CRISIS ON EAST ASIA: POLICY RESPONSES FROM FOUR ECONOMIES, P190
   Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212
   Shetty R., 2016, P 24 ACM INT C MULTI, P1073
   SONG J, 2017, ARXIV170802478
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang PJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1871, DOI 10.1145/3123266.3127895
   Thomason J., 2014, COLING, P1218
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang HZ, 2018, PROC CVPR IEEE, P4962, DOI 10.1109/CVPR.2018.00521
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Wei SK, 2010, IEEE T KNOWL DATA EN, V22, P1191, DOI 10.1109/TKDE.2009.145
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K., 2015, COMPUTER SCI, P2048
   Xu KS, 2017, IEEE INT CON MULTI, P361, DOI 10.1109/ICME.2017.8019408
   Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zhang MX, 2019, IEEE T IMAGE PROCESS, V28, P32, DOI 10.1109/TIP.2018.2855415
NR 57
TC 15
Z9 16
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 31
DI 10.1145/3303083
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IM3ZV
UT WOS:000477935400002
DA 2024-07-18
ER

PT J
AU Floris, A
   Ahmad, A
   Atzori, L
AF Floris, Alessandro
   Ahmad, Arslan
   Atzori, Luigi
TI QoE-Aware OTT-ISP Collaboration in Service Management: Architecture and
   Approaches
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Over The Top service providers; OTT; Internet service providers; ISP;
   quality of experience; QoE; QoE management; OTT-ISP collaboration
ID QUALITY; EXPERIENCE
AB It is a matter of fact that quality of experience (QoE) has become one of the key factors determining whether a new multimedia service will be successfully accepted by the final users. Accordingly, several QoE models have been developed with the aim of capturing the perception of the user by considering as many influencing factors as possible. However, when it comes to adopting these models in the management of the services and networks, it frequently happens that no single provider has access to all of the tools to either measure all influencing factors parameters or control over the delivered quality. In particular, it often happens to the over-the-top (OTT) and Internet service providers (ISPs), which act with complementary roles in the service delivery over the Internet. On the basis of this consideration, in this article we first highlight the importance of a possible OTT-ISP collaboration for a joint service management in terms of technical and economic aspects. Then we propose a general reference architecture for a possible collaboration and information exchange among them. Finally, we define three different approaches, namely joint venture, customer lifetime value based, and QoE fairness based. The first aims to maximize the revenue by providing better QoE to customers paying more. The second aims to maximize the profit by providing better QoE to the most profitable customers (MPCs). The third aims to maximize QoE fairness among all customers. Finally, we conduct simulations to compare the three approaches in terms of QoE provided to the users, profit generated for the providers, and QoE fairness.
C1 [Floris, Alessandro; Ahmad, Arslan; Atzori, Luigi] Univ Cagliari, DIEE, I-09123 Cagliari, Italy.
C3 University of Cagliari
RP Floris, A (corresponding author), Univ Cagliari, DIEE, I-09123 Cagliari, Italy.
EM alessandro.floris@diee.unica.it; arslan.ahmad@diee.unica.it;
   l.atzori@ieee.org
RI Floris, Alessandro/L-6707-2018; Ahmad, Arslan/W-4581-2019
OI Floris, Alessandro/0000-0002-8745-1327; Ahmad,
   Arslan/0000-0002-3979-5870
FU European Union's Horizon 2020 research and innovation program under the
   Marie Sklodowska-Curie grant [643072]; Network QoE-Net; Italian Ministry
   of University and Research (MIUR), within the Smart Cities framework
   (project Netergit) [PON04a200490]
FX This work was funded by the European Union's Horizon 2020 research and
   innovation program under the Marie Sklodowska-Curie grant (agreement
   643072), Network QoE-Net (http://qoenet-itn.eu), and by the Italian
   Ministry of University and Research (MIUR), within the Smart Cities
   framework (project Netergit, ID: PON04a200490).
CR Abdeljaouad I, 2015, J NETW COMPUT APPL, V54, P1, DOI 10.1016/j.jnca.2015.04.006
   Accenture, 2017, CUST SERV NOT PRIC R
   Ahmad A, 2016, COMPUT NETW, V110, P168, DOI 10.1016/j.comnet.2016.09.022
   Alimi R, 2014, RFC 7285
   [Anonymous], 2012, EUROPEAN NETWORK QUA
   [Anonymous], P IEEE INT S BROADB
   [Anonymous], 2017, 2300952017 ISOIEC SA
   Barakovic S., 2013, J. Comput. Netw. Commun., P28
   Bernet Y., 2000, 2998 NWG RFC
   Bernstein D, 2009, 2009 FOURTH INTERNATIONAL CONFERENCE ON INTERNET AND WEB APPLICATIONS AND SERVICES, P328, DOI 10.1109/ICIW.2009.55
   Cisco, 2017, Cisco7 Feb.
   Cole RG, 2001, ACM SIGCOMM COMP COM, V31, P9, DOI 10.1145/505666.505669
   Envision, 2012, CO OPT OV APPL UND N
   FierceWireless, 2017, IPHONE US CIT VID BU
   Glady N, 2009, EUR J OPER RES, V197, P402, DOI 10.1016/j.ejor.2008.06.027
   Gomez Gerardo, 2013, J COMPUTER NETWORKS, V2013
   Hoekstra J C., 1999, Journal of Market Focused Management, V3, P257, DOI [10.1023/A:1009842805871, DOI 10.1023/A:1009842805871]
   Hossfeld Tobias, 2013, Data Traffic Monitoring and Analysis. From Measurement, Classification, and Anomaly Detection to Quality of Experience, P264, DOI 10.1007/978-3-642-36784-7_11
   Hossfeld T., 2017, P 9 INT C QUAL MULT
   Hossfeld T, 2017, IEEE COMMUN LETT, V21, P184, DOI 10.1109/LCOMM.2016.2616342
   Hossfeld T, 2015, PROCEEDINGS OF THE 2015 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM), P1274, DOI 10.1109/INM.2015.7140480
   Hossfeld T, 2015, PROCEEDINGS OF THE 2015 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM), P1249, DOI 10.1109/INM.2015.7140476
   ITU, 2015, Recommendation ITU-T G.107
   Lingfen Sun, 2004, 2004 IEEE International Conference on Communications (IEEE Cat. No.04CH37577), P1478, DOI 10.1109/ICC.2004.1312757
   Liotou E, 2015, IEEE COMMUN MAG, V53, P145, DOI 10.1109/MCOM.2015.7158278
   Nokia, 2017, NOK WHIT PAP QUAL EX
   Oliver-Balsalobre P., 2016, EURASIP J WIREL COMM, V2016, P1
   Passarella A, 2012, COMPUT COMMUN, V35, P1, DOI 10.1016/j.comcom.2011.10.005
   Ramneek, 2015, INT CONF ADV COMMUN, P663, DOI 10.1109/ICACT.2015.7224879
   REICHL P, 2015, P 7 INT WORKSH QUAL
   Schwarzmann S, 2016, 2016 28TH INTERNATIONAL TELETRAFFIC CONGRESS (ITC 28), VOL 3, P13, DOI 10.1109/ITC-28.2016.310
   Seppänen J, 2014, J VIS COMMUN IMAGE R, V25, P565, DOI 10.1016/j.jvcir.2013.11.010
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Stanik Alexander, 2016, P 7 INT C INF INT SY
   StreamingMediaBlog. com, 2017, CHART SHOWS WHICH CO
   Tsolkas D, 2017, J NETW COMPUT APPL, V77, P1, DOI 10.1016/j.jnca.2016.10.016
   Varela M, 2015, IEEE INT CONF COMM, P1741, DOI 10.1109/ICCW.2015.7247432
   Vega MT, 2017, MULTIMED TOOLS APPL, V76, P22303, DOI 10.1007/s11042-017-4831-6
   Vega MT, 2017, IEEE SIGNAL PROC LET, V24, P736, DOI 10.1109/LSP.2017.2691160
   Vega MT, 2017, SIGNAL PROCESS-IMAGE, V52, P20, DOI [10.1016/j.image.2016.12.001, 10.1016/jimage.2016.12.001]
NR 40
TC 13
Z9 14
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 36
DI 10.1145/3183517
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TD
UT WOS:000434634700009
OA Green Published
DA 2024-07-18
ER

PT J
AU Skorin-Kapov, L
   Varela, M
   Hossfeld, T
   Chen, KT
AF Skorin-Kapov, Lea
   Varela, Martin
   Hossfeld, Tobias
   Chen, Kuan-Ta
TI A Survey of Emerging Concepts and Challenges for QoE Management of
   Multimedia Services
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE QoE modeling; crowdsourcing; QoE monitoring; QoE management; data
   analytics; SDN; NFV; monitoring probes; encrypted traffic
ID QUALITY ASSESSMENT; EXPERIENCE; NETWORK; OPTIMIZATION; MODEL
AB Quality of Experience (QoE) has received much attention over the past years and has become a prominent issue for delivering services and applications. A significant amount of research has been devoted to understanding, measuring, and modelling QoE for a variety of media services. The next logical step is to actively exploit that accumulated knowledge to improve and manage the quality of multimedia services, while at the same time ensuring efficient and cost-effective network operations. Moreover, with many different players involved in the end-to-end service delivery chain, identifying the root causes of QoE impairments and finding effective solutions for meeting the end users' requirements and expectations in terms of service quality is a challenging and complex problem. In this article, we survey state-of-the-art findings and present emerging concepts and challenges related to managing QoE for networked multimedia services. Going beyond a number of previously published survey articles addressing the topic of QoE management, we address QoE management in the context of ongoing developments, such as the move to softwarized networks, the exploitation of big data analytics and machine learning, and the steady rise of new and immersive services (e.g., augmented and virtual reality). We address the implications of such paradigm shifts in terms of new approaches in QoE modeling and the need for novel QoE monitoring and management infrastructures.
C1 [Skorin-Kapov, Lea] Univ Zagreb, Fac Elect Engn & Comp, Unska 3, Zagreb 10000, Croatia.
   [Varela, Martin] EXFO, Helsinki, Finland.
   [Hossfeld, Tobias] Univ Duisburg Essen, Modeling Adapt Syst, Duisburg, Germany.
   [Chen, Kuan-Ta] Acad Sinica, Inst Informat Sci, 128,Sec 2,Acad Rd, Taipei 115, Taiwan.
   [Varela, Martin] Callstats Io, Annankatu 31-33 C 42, Helsinki, Finland.
   [Hossfeld, Tobias] Univ Wurzburg Hubland, Commun Networks, D-97074 Wurzburg, Germany.
C3 University of Zagreb; University of Duisburg Essen; Academia Sinica -
   Taiwan; University of Wurzburg
RP Skorin-Kapov, L (corresponding author), Univ Zagreb, Fac Elect Engn & Comp, Unska 3, Zagreb 10000, Croatia.
EM lea.skorin-kapov@fer.hr; martin@callstats.io;
   tobias.hossfeld@uni-wuerzburg.de; swc@iis.sinica.edu.tw
RI Skorin-Kapov, Lea/AAJ-4737-2021
OI Berretti, Stefano/0000-0003-1219-4386; Hossfeld,
   Tobias/0000-0003-0173-595X
FU Croatian Science Foundation [UIP-2014-09-5605]; Deutsche
   Forschungsgemeinschaft (DFG) [HO 4770/1-2]
FX This work was partly funded by the Croatian Science Foundation, project
   no. UIP-2014-09-5605 (Q-MANIC), and Deutsche Forschungsgemeinschaft
   (DFG) under grants HO 4770/1-2 (DFG OekOnet).
CR Aggarwal Vaneet., 2014, P 15 WORKSHOP MOBILE, P18
   Ahmad A., 2017, P IEEE INT C COMM IC, P1
   Ahmad A, 2016, COMPUT NETW, V110, P168, DOI 10.1016/j.comnet.2016.09.022
   Aijaz A, 2017, IEEE WIREL COMMUN, V24, P82, DOI 10.1109/MWC.2016.1500157RP
   [Anonymous], 2016, TECHNICAL REPORT
   [Anonymous], MOBILE INFO SYST
   [Anonymous], 2013, ARXIV13060221
   [Anonymous], P WORKSH PERC QUAL S
   [Anonymous], 26909 3GPP TR
   [Anonymous], P ACM SIGCOMM WORKSH
   [Anonymous], 5G INFR PUBL PRIV PA
   [Anonymous], P 2016 23 INT C TEL
   [Anonymous], P ACM SPEC INT GROUP
   [Anonymous], P 2016 IEEE GLOB COM
   [Anonymous], IEEE JSAC
   [Anonymous], P 21 ITC SPEC SEM MU
   [Anonymous], SPIE IS T ELECT IMAG
   [Anonymous], P GLOB COMM C GLOBEC
   [Anonymous], CORR
   [Anonymous], ARCH WHIT PAP
   [Anonymous], P 8 INT C QUAL MULT
   [Anonymous], P 4 INT WORKSH PERC
   [Anonymous], SPIE OPTICAL ENG APP
   [Anonymous], P 14 USENIX S NETW S
   [Anonymous], CISC VIS NETW IND GL
   [Anonymous], 2010, P ANN REL MAINT S JA
   [Anonymous], NFV MANO NETW FUNCT
   [Anonymous], SDN NFV REF ARCH V1
   [Anonymous], 2015, QUALITY MULTIMEDIA E
   [Anonymous], 2014, 2014 IEEE NETW OP MA
   [Anonymous], DELIVER HIGHER MOBIL
   [Anonymous], 2017, 2017 9 INT C QUAL MU
   [Anonymous], 2015, INT J INFORM COMMUN
   [Anonymous], 2010, ACTIVE LEARNING LIT
   [Anonymous], QUAL USER EXPER
   [Anonymous], P 1 IFIP IEEE INT WO
   [Anonymous], P 2015 IFIP IEEE INT
   [Anonymous], 103294V111 ETSI TS
   [Anonymous], 2016, P 8 INT C QUAL MULT
   [Anonymous], WEBINAR FUTURE QOE M
   [Anonymous], IN SITU QUALITY EXPE
   [Anonymous], J COMPUT NETWORKS CO
   Aroussi S, 2014, 2014 INTERNATIONAL CONFERENCE ON COMPUTING, MANAGEMENT AND TELECOMMUNICATIONS (COMMANTEL), P200, DOI 10.1109/ComManTel.2014.6825604
   Awobuluyi Olatunde, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P1657, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.250
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Bao YN, 2017, IEEE J SEL AREA COMM, V35, P1062, DOI 10.1109/JSAC.2017.2680918
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Bouten N, 2014, IEEE T MULTIMEDIA, V16, P2281, DOI 10.1109/TMM.2014.2362856
   Bustamante FE, 2017, ACM SIGCOMM COMP COM, V47, P55, DOI 10.1145/3041027.3041035
   Casas P., 2016, TMA, P1
   Casas P, 2016, IEEE T NETW SERV MAN, V13, P181, DOI 10.1109/TNSM.2016.2537645
   Casas P, 2014, COMPUT NETW, V68, P149, DOI 10.1016/j.comnet.2014.01.008
   Chen KT, 2010, IEEE NETWORK, V24, P28, DOI 10.1109/MNET.2010.5430141
   Chen QA, 2014, PROCEEDINGS OF THE 2014 ACM INTERNET MEASUREMENT CONFERENCE (IMC'14), P151, DOI 10.1145/2663716.2663726
   Chen Wei, 2012, 26 AAAI C ART INT, P592
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   Chu BH, 2007, KNOWL-BASED SYST, V20, P703, DOI 10.1016/j.knosys.2006.10.003
   Clark LA, 1995, PSYCHOL ASSESSMENT, V7, P309, DOI 10.1037/1040-3590.7.3.309
   Cui LZ, 2016, IEEE NETWORK, V30, P58, DOI 10.1109/MNET.2016.7389832
   Dimopoulos G., 2016, P 2016 INT MEAS C SA, P513, DOI DOI 10.1145/2987443.2987459
   Dinh Laurent, 2017, 5 INT C LEARN REPR I
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Dobrijevic O, 2015, INT CONF NETW SER, P274, DOI 10.1109/CNSM.2015.7367371
   Engelke U, 2007, 2007 NEXT GENERATION INTERNET NETWORKS, P190, DOI 10.1109/NGI.2007.371215
   Engelke U, 2017, IEEE J-STSP, V11, P6, DOI 10.1109/JSTSP.2016.2609843
   Ernst JB, 2014, PHYS COMMUN-AMST, V13, P61, DOI 10.1016/j.phycom.2014.04.009
   Fiedler M, 2010, IEEE NETWORK, V24, P36, DOI 10.1109/MNET.2010.5430142
   Gandy M., 2010, 2010 9th IEEE International Symposium on Mixed and Augmented Reality (ISMAR). Science & Technology Papers, P127, DOI 10.1109/ISMAR.2010.5643560
   Garcia M.-N, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P141, DOI 10.1109/QoMEX.2014.6982310
   Gardlo Bruno., 2015, Proceedings of the Fourth International Workshop on Crowdsourcing for Multimedia. CrowdMM'15, P15
   Ge C, 2016, PROCEEDINGS OF THE 2016 3RD ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ACM-ICN '16), P237, DOI 10.1145/2984356.2988522
   Ghinea G, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2617994
   Ghinea G, 2010, IEEE T SYST MAN CY A, V40, P657, DOI 10.1109/TSMCA.2010.2041224
   Goel U, 2016, IEEE COMMUN SURV TUT, V18, P105, DOI 10.1109/COMST.2015.2485979
   Gupta R, 2017, IEEE J-STSP, V11, P22, DOI 10.1109/JSTSP.2016.2638538
   Hadden J, 2007, COMPUT OPER RES, V34, P2902, DOI 10.1016/j.cor.2005.11.007
   Hamam A, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540991
   Han B, 2015, IEEE COMMUN MAG, V53, P90, DOI 10.1109/MCOM.2015.7045396
   Hewage CTER, 2013, IEEE COMMUN MAG, V51, P101, DOI 10.1109/MCOM.2013.6515053
   Hosfeld T., 2011, Proceedings of the 2011 23rd International Teletraffic Congress (ITC 2011), P103
   HoSSfeld T., 2014, Measurement, Modelling, and Evaluation of Computing Systems and Dependability and Fault Tolerance, P1
   Hossfeld T, 2015, PROCEEDINGS OF THE 2015 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM), P1274, DOI 10.1109/INM.2015.7140480
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Hossfeld T, 2012, IEEE COMMUN MAG, V50, P28, DOI 10.1109/MCOM.2012.6178831
   Hossmann Theus., 2011, Proceedings of the 3rd Extreme Conference on Communication: The Amazon Expedition, page, P1, DOI DOI 10.1145/2414393.2414394
   Hu Y., 2015, MOBILE EDGE COMPUTIN, P1
   Ivesic K, 2014, J NETW COMPUT APPL, V46, P336, DOI 10.1016/j.jnca.2014.09.010
   Jalal L, 2017, IEEE INT SYM BROADB, P199
   Jarschel Michael, 2013, 2013 Second European Workshop on Software Defined Networks (EWSDN), P87, DOI 10.1109/EWSDN.2013.21
   Kassler A., 2012, Software, Telecommunications and Computer Networks (Soft-COM), 2012 20th International Conference on, IEEE, P1
   Khan N, 2016, EURASIP J WIREL COMM, DOI 10.1186/s13638-016-0584-6
   Kroupi E, 2014, EUR SIGNAL PR CONF, P2135
   Laghari KUR, 2013, 2013 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2013), P1300
   Le Callet P., 2013, Qualinet White Paper on Definitions of Quality of Experience
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Liu CZ, 2016, C IND ELECT APPL, P1818, DOI 10.1109/ICIEA.2016.7603882
   Maggioni E, 2017, INT WORK QUAL MULTIM
   Mäki T, 2016, 2016 IFIP NETWORKING CONFERENCE (IFIP NETWORKING) AND WORKSHOPS, P476, DOI 10.1109/IFIPNetworking.2016.7497243
   Martini MG, 2012, IEEE J SEL AREA COMM, V30, P1153, DOI 10.1109/JSAC.2012.120801
   Medhat AM, 2017, IEEE COMMUN MAG, V55, P216, DOI 10.1109/MCOM.2016.1600219RP
   Menkovski V, 2012, SIGNAL PROCESS-IMAGE, V27, P788, DOI 10.1016/j.image.2012.01.004
   Menkovski Vlado, 2011, P INT C MOB MULT COM, P1
   Mitra K, 2015, IEEE T MOBILE COMPUT, V14, P920, DOI 10.1109/TMC.2013.155
   Möller S, 2014, T-LAB SER TELECOMMUN, P73, DOI 10.1007/978-3-319-02681-7_5
   Moens H, 2014, INT CONF NETW SER, P418, DOI 10.1109/CNSM.2014.7014205
   Murray N, 2017, INT WORK QUAL MULTIM
   Murray Niall., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys'13, P162, DOI [https://doi.org/10.1145/2483977.2483999, DOI 10.1145/2483977.2483999]
   Nam H, 2014, IEEE GLOB COMM CONF, P1317, DOI 10.1109/GLOCOM.2014.7036990
   Nam H, 2014, ACM SIGCOMM COMP COM, V44, P111, DOI 10.1145/2740070.2631433
   Orsolic, 2017, MULTIMED TOOLS APPL, P1
   Osting B, 2016, APPL COMPUT HARMON A, V41, P540, DOI 10.1016/j.acha.2016.03.007
   Pallot M, 2013, P VIRTUAL REALITY IN, P4
   Perritaz D, 2009, IEEE SYS MAN CYBERN, P888, DOI 10.1109/ICSMC.2009.5346772
   Puig J, 2012, INT WORK QUAL MULTIM, P188, DOI 10.1109/QoMEX.2012.6263864
   Rawassizadeh R, 2015, COMMUN ACM, V58, P45, DOI 10.1145/2629633
   Redi Judith., 2015, Fourth International Workshop on Crowdsourcing for Multimedia. CrowdMM'15, P33, DOI [DOI 10.1145/2810188.2810194, 10.1145/2810188.2810194]
   Reichl P., 2013, P 2013 ACM SIGCOMM W, P33
   Reichl P, 2015, INT WORK QUAL MULTIM, DOI 10.1109/QoMEX.2015.7148138
   Reichl P, 2010, IEEE ICC
   Reiter U, 2014, T-LAB SER TELECOMMUN, P55, DOI 10.1007/978-3-319-02681-7_4
   Robitza Werner, 2017, Multimedia Tools and Applications, V76, P22243, DOI 10.1007/s11042-017-4870-z
   Robitza W., 2016, 5 ISCADEGA WORKSHOP, P39
   Rodríguez DZ, 2014, IEEE T CONSUM ELECTR, V60, P436, DOI 10.1109/TCE.2014.6937328
   Rubino G., 2005, Design and operations of communication networks: a review of wired and wireless modeling and management challenges
   Rubino G, 2006, LECT NOTES COMPUT SC, V4131, P303
   Schatz Raimund, 2013, Data Traffic Monitoring and Analysis. From Measurement, Classification, and Anomaly Detection to Quality of Experience, P219, DOI 10.1007/978-3-642-36784-7_10
   Schatz R, 2017, INT WORK QUAL MULTIM
   Schatz R, 2014, T-LAB SER TELECOMMUN, P411, DOI 10.1007/978-3-319-02681-7_28
   Schwarzmann S, 2016, 2016 28TH INTERNATIONAL TELETRAFFIC CONGRESS (ITC 28), VOL 3, P13, DOI 10.1109/ITC-28.2016.310
   Seufert M., 2017, P 9 INT C QUAL MULT
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Shafiq M. Zubair, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P367, DOI 10.1145/2591971.2591975
   Skorin-Kapov Lea, 2012, 2012 35th International Convention on Information and Communication Technology, Electronics and Microelectronics, P662
   Stallings W., 2015, FDN MODERN NETWORKIN
   Sulema Y., 2016, P 2016 INT C SYSTEMS, P1
   Timmerer C, 2014, T-LAB SER TELECOMMUN, P351, DOI 10.1007/978-3-319-02681-7_24
   Timmerer C, 2014, COMPUTER, V47, P67, DOI 10.1109/MC.2014.76
   Tselios C, 2016, 2016 IEEE 21ST INTERNATIONAL WORKSHOP ON COMPUTER AIDED MODELLING AND DESIGN OF COMMUNICATION LINKS AND NETWORKS (CAMAD), P159, DOI 10.1109/CAMAD.2016.7790351
   Tsolkas D, 2017, J NETW COMPUT APPL, V77, P1, DOI 10.1016/j.jnca.2016.10.016
   Varela M, 2015, IEEE INT CONF COMM, P1741, DOI 10.1109/ICCW.2015.7247432
   Wamser F, 2015, 2015 EUROPEAN CONFERENCE ON NETWORKS AND COMMUNICATIONS (EUCNC), P239, DOI 10.1109/EuCNC.2015.7194076
   Wang Y, 2017, IEEE WIREL COMMUN, V24, P102, DOI 10.1109/MWC.2016.1500184WC
   Wanmin Wu, 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P220, DOI 10.1109/ISM.2010.39
   Weldon MK, 2016, FUTURE X NETWORK: A BELL LABS PERSPECTIVE, P1, DOI 10.1201/b21038
   Wu CC, 2013, IEEE T MULTIMEDIA, V15, P1121, DOI 10.1109/TMM.2013.2241043
   Ye P, 2014, PROC CVPR IEEE, P4249, DOI 10.1109/CVPR.2014.541
   Yuan ZH, 2014, INT WIREL COMMUN, P1142, DOI 10.1109/IWCMC.2014.6906515
   Yuan ZH, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2661329
   Zhao TS, 2017, IEEE COMMUN SURV TUT, V19, P285, DOI 10.1109/COMST.2016.2619982
   Zheng K, 2016, IEEE NETWORK, V30, P44, DOI 10.1109/MNET.2016.7389830
NR 151
TC 65
Z9 67
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 26
DI 10.1145/3176648
PG 29
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TD
UT WOS:000434634700002
DA 2024-07-18
ER

PT J
AU Li, XP
   Cheung, M
   She, J
AF Li, Xiaopeng
   Cheung, Ming
   She, James
TI A Distributed Streaming Framework for Connection Discovery Using Shared
   Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Social networks; connection discovery; bag-of-features tagging; user
   shared videos; computation framework; streaming
ID RECOMMENDATION
AB With the advances in mobile devices and the popularity of social networks, users can share multimedia content anytime, anywhere. One of the most important types of emerging content is video, which is commonly shared on platforms such as Instagram and Facebook. User connections, which indicate whether two users are follower/followee or have the same interests, are essential to improve services and information relevant to users for many social media applications. But they are normally hidden due to users' privacy concerns or are kept confidential by social media sites. Using user-shared content is an alternative way to discover user connections. This article proposes to use user-shared videos for connection discovery with the Bag of Feature Tagging method and proposes a distributed streaming computation framework to facilitate the analytics. Exploiting the uniqueness of shared videos, the proposed framework is divided into Streaming processing and Online and Offline Computation. With experiments using a dataset from Twitter, it has been proved that the proposed method using user-shared videos for connection discovery is feasible. And the proposed computation framework significantly accelerates the analytics, reducing the processing time to only 32% for follower/followee recommendation. It has also been proved that comparable performance can be achieved with only partial data for each video and leads to more efficient computation.
C1 [Li, Xiaopeng] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
   [Cheung, Ming; She, James] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology; Hong Kong University of
   Science & Technology
RP Li, XP (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
OI Li, Xiaopeng/0000-0003-4916-1131
FU HKUST-NIE Social Media Lab., HKUST
FX This work is supported by HKUST-NIE Social Media Lab., HKUST.
CR Agarwal D., 2010, Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '10, P703
   Agarwal V, 2013, SOC NETW ANAL MIN, V3, P359, DOI 10.1007/s13278-012-0083-7
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], 2012, CSCW, DOI DOI 10.1145/2145204.2145360
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], P 2011 VIS INF COMM
   Cheung M., 2014, P 4 INT C ADV INF MI, P83
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Gilbert E, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P211
   Hsu W., 2006, AAAI SPRING S SERIES
   Ji JZ, 2003, IEEE/WIC INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, PROCEEDINGS, P68
   Jie Zhanming, 2015, P IEEE 4 S NETW CLOU
   Jones JJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0052168
   Kuschnig R., 2011, MMSYS, P245
   Li XP, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P931, DOI 10.1109/BigData.2016.7840689
   Liu GT, 2009, PROCEEDINGS OF THE 8TH IEEE/ACIS INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE, P1126, DOI 10.1109/ICIS.2009.124
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matousek J, 2000, DISCRETE COMPUT GEOM, V24, P61, DOI 10.1007/s004540010019
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Rogati M., 2010, Proceedings of the 19th international conference on World wide web, P981, DOI DOI 10.1145/1772690.1772790
   Si Xiance., 2009, Journal of Computational Information Systems, V6, P23
   Song Y, 2008, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE 2007, VOL 6, PTS A AND B, P515, DOI 10.1145/1390334.1390423
   Sujatha C., 2011, 2011 Proceedings of International Conference on Computational Intelligence and Communication Networks (CICN 2011), P73, DOI 10.1109/CICN.2011.15
   Xing Xie, 2010, Proceedings of the 2010 IEEE/ACM Int'l Conference on Green Computing and Communications (GreenCom) and Int'l Conference on Cyber, Physical and Social Computing (CPSCom), P831, DOI 10.1109/GreenCom-CPSCom.2010.28
   Zhu N, 2015, PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON BIG DATA, P1755, DOI 10.1109/BigData.2015.7363947
NR 25
TC 0
Z9 0
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 59
DI 10.1145/3120996
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300013
DA 2024-07-18
ER

PT J
AU Bharati, S
   Omar, HA
   Zhuang, WH
AF Bharati, Sailesh
   Omar, Hassan Aboubakr
   Zhuang, Weihua
TI Enhancing Transmission Collision Detection for Distributed TDMA in
   Vehicular Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE VANETs; distributed time division multiple access (D-TDMA); medium
   access control (MAC); channel estimation; collision detection;
   transmission failure differentiation
ID ADHOC MAC
AB The increasing number of road accidents has led to the evolution of vehicular ad hoc networks (VANETs), which allow vehicles and roadside infrastructure to continuously broadcast safety messages, including necessary information to avoid undesired events on the road. To support reliable broadcast of safety messages, distributed time division multiple access (D-TDMA) protocols are proposed for medium access control in VANETs. Existing D-TDMA protocols react to a transmission failure without distinguishing whether the failure comes from a transmission collision or from a poor radio channel condition, resulting in degraded performance. In this article, we present the importance of transmission failure differentiation due to a poor channel or due to a transmission collision for D-TDMA protocols in vehicular networks. We study the effects of such a transmission failure differentiation on the performance of a node when reserving a time slot to access the transmission channel. Furthermore, we propose a method for transmission failure differentiation, employing the concept of deep-learning techniques, for a node to decide whether to release or continue using its acquired time slot. The proposed method is based on the application of a Markov chain model to estimate the channel state when a transmission failure occurs. The Markov model parameters are dynamically updated by each node (i.e., vehicle or roadside unit) based on information included in the safety messages that are periodically received from neighboring nodes. In addition, from the D-TDMA protocol headers of received messages, a node approximately determines the error in estimating the channel state based on the proposed Markov model and then uses this channel estimation error to further improve subsequent channel state estimations. Through mathematical analysis, we show that transmission failure differentiation, or transmission collision detection, helps a node to efficiently reserve a time slot even with a large number of nodes contending for time slots. Furthermore, through extensive simulations in a highway scenario, we demonstrate that the proposed solution significantly improves the performance of D-TDMA protocols by reducing unnecessary contention on the available time slots, thus increasing the number of nodes having unique time slots for successful broadcast of safety messages.
C1 [Bharati, Sailesh; Omar, Hassan Aboubakr; Zhuang, Weihua] Univ Waterloo, Dept Elect & Comp Engn, 200 Univ Ave, Waterloo, ON N2L 3G1, Canada.
   [Omar, Hassan Aboubakr] Cairo Univ, Fac Engn, Gamaet El Qahera St, Giza 12613, Egypt.
C3 University of Waterloo; Egyptian Knowledge Bank (EKB); Cairo University
RP Bharati, S (corresponding author), Univ Waterloo, Dept Elect & Comp Engn, 200 Univ Ave, Waterloo, ON N2L 3G1, Canada.
EM sbharati@uWaterloo.ca; h3omar@uWaterloo.ca; wzhuang@uWaterloo.ca
RI Zhuang, Weihua/AAH-2576-2020
OI Zhuang, Weihua/0000-0003-0488-511X
FU Natural Sciences and Engineering Research Council (NSERC) of Canada
FX This work was supported by research grants from the Natural Sciences and
   Engineering Research Council (NSERC) of Canada.
CR [Anonymous], 2007, PROC IEEE VEHICLULAR
   [Anonymous], TECHNICAL REPORT
   [Anonymous], 809859 NHTSA DOT HS
   Bharati S., 2017, IEEE T VEHI IN PRESS
   Bharati S, 2016, IEEE T VEH TECHNOL, V65, P9542, DOI 10.1109/TVT.2016.2598488
   Bharati S, 2013, IEEE J SEL AREA COMM, V31, P470, DOI 10.1109/JSAC.2013.SUP.0513042
   Borgonovo F, 2004, WIREL NETW, V10, P359, DOI 10.1023/B:WINE.0000028540.96160.8a
   Borgonovo F., 2003, P 2003 IEEE VEH TECH
   Cheng L., 2008, P IEEE GLOB COMM C I
   Cheng L, 2007, IEEE J SEL AREA COMM, V25, P1501, DOI 10.1109/JSAC.2007.071002
   Ding WD, 2008, ETRI J, V30, P59, DOI 10.4218/etrij.08.0106.0306
   Hadded M., 2015, TECHNICAL REPORT
   Hadded M., 2016, P 2016 24 INT C SOFT
   Hassan MI, 2011, IEEE T VEH TECHNOL, V60, P3882, DOI 10.1109/TVT.2011.2162755
   Huang J. J., 2013, P 2013 INT S WIR PER
   Jakes W.C., 1994, MICROWAVE MOBILE COM
   Lopez-Martinez FJ, 2013, IEEE T COMMUN, V61, P1404, DOI 10.1109/TCOMM.2013.020412.120413
   Jiang D, 2006, IEEE WIREL COMMUN, V13, P36, DOI 10.1109/WC-M.2006.250356
   Khabbaz MJ, 2012, IEEE T INTELL TRANSP, V13, P1312, DOI 10.1109/TITS.2012.2188519
   Li Deng, 2014, TECHNICAL REPORT
   Li Y, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2725469
   Lin S., 2014, P 6 INT S WIR VEH CO
   Liu H, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2491735
   Omar HA, 2013, IEEE T EMERG TOP COM, V1, P69, DOI 10.1109/TETC.2013.2278705
   Omar HA, 2013, IEEE T MOBILE COMPUT, V12, P1724, DOI 10.1109/TMC.2012.142
   PTV Group, 2015, PTV VISS
   Santos JCS, 2007, IEEE COMMUN LETT, V11, P231, DOI 10.1109/LCOMM.2007.061491
   Silva FA, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2903745
   Wiedemann R., 1974, SCHRIFTENREIHE I VER, V8
   Wu Q, 2016, WIREL NETW, V22, P799, DOI 10.1007/s11276-015-1000-6
NR 30
TC 17
Z9 18
U1 0
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
SU S
AR 37
DI 10.1145/3092833
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FP1UO
UT WOS:000417400400005
DA 2024-07-18
ER

PT J
AU Cofano, G
   De Cicco, L
   Zinner, T
   Anh, NN
   Phuoc, TG
   Mascolo, S
AF Cofano, Giuseppe
   De Cicco, Luca
   Zinner, Thomas
   Anh Nguyen-Ngoc
   Phuoc Tran-Gia
   Mascolo, Saverio
TI Design and Performance Evaluation of Network-assisted Control Strategies
   for HTTP Adaptive Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adaptive video streaming; DASH; network-assistance; control plane;
   quality of experience; fairness
AB This article investigates several network-assisted streaming approaches that rely on active cooperation between video streaming applications and the network. We build a Video Control Plane that enforces Video Quality Fairness among concurrent video flows generated by heterogeneous client devices. For this purpose, a max-min fairness optimization problem is solved at runtime. We compare two approaches to actuate the optimal solution in an Software Defined Networking network: The first one allocates network bandwidth slices to video flows, and the second one guides video players in the video bitrate selection. We assess performance through several QoE-related metrics, such as Video Quality Fairness, video quality, and switching frequency. The impact of client-side adaptation algorithms is also investigated.
C1 [Cofano, Giuseppe; De Cicco, Luca; Mascolo, Saverio] Politecn Bari, Via Orabona 4, I-70126 Bari, Italy.
   [Zinner, Thomas; Anh Nguyen-Ngoc; Phuoc Tran-Gia] Univ Wurzburg, D-97074 Wurzburg, Germany.
C3 Politecnico di Bari; University of Wurzburg
RP Cofano, G (corresponding author), Politecn Bari, Via Orabona 4, I-70126 Bari, Italy.
EM giuseppe.cofano@poliba.it; luca.decicco@poliba.it;
   zinner@informatik.uni-wuerzburg.de;
   anh.nguyen@informatik.uni-wuerzburg.de;
   trangia@informatik.uni-wuerzburg.de; saverio.mascolo@poliba.it
RI mascolo, saverio/AAJ-9246-2021; Zinner, Thomas/AAA-6004-2019; De Cicco,
   Luca/AAU-8801-2020
OI Zinner, Thomas/0000-0002-4179-4105; De Cicco, Luca/0000-0002-8900-175X
FU Italian Ministry of Education, Universities and Research (MIUR) through
   the MAIVISTO project [PAC02L1_00061]; Apulia Region (Italy) through the
   Future in Research project [ACYBEH5]; Deutsche Forschungsgemeinschaft
   (DFG) [ZI 1334/2-1, TR257/43-1]
FX This work has been partially supported by the Italian Ministry of
   Education, Universities and Research (MIUR) through the MAIVISTO project
   (PAC02L1_00061) and by the Apulia Region (Italy) through the Future in
   Research project no. ACYBEH5. This work has been also partly funded by
   Deutsche Forschungsgemeinschaft (DFG) under grants ZI 1334/2-1 and
   TR257/43-1.
CR Akhshabi S, 2012, SIGNAL PROCESS-IMAGE, V27, P271, DOI 10.1016/j.image.2011.10.003
   Akhshabi Saamer, 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video-NOSSDAV'13
   [Anonymous], P IEEE WORKSH MULT Q
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], 2012, P IMC 2012 NOV
   [Anonymous], 2015, P IFIP IEEE NETW
   [Anonymous], 1992, Data networks
   [Anonymous], 2015, 12 USENIX S NETW SYS
   [Anonymous], 2014, P 2014 WORKSHOP DESI
   [Anonymous], P USENIX S NETW SYST
   [Anonymous], P INT WORKSH QUAL MU
   [Anonymous], 2013, P 5 WORKSH MOB VID M
   [Anonymous], 2014, 2014 IEEE NETW OP MA
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], P ACM MULT SYST C MM
   [Anonymous], P ACM INT C SPEC
   [Anonymous], MODELING DE IN PRESS
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Bertsimas D, 2011, OPER RES, V59, P17, DOI 10.1287/opre.1100.0865
   Cofano G, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P24, DOI 10.1145/2910017.2910597
   Cofano G., 2014, P 2014 WORKSHOP DESI, P7
   De Cicco L, 2014, IEEE ACM T NETWORK, V22, P526, DOI 10.1109/TNET.2013.2253797
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   Hossfeld Tobias, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P111, DOI 10.1109/QoMEX.2014.6982305
   Hossfeld T, 2014, LECT NOTES COMPUT SC, V8376, P136, DOI 10.1007/978-3-319-05359-2_10
   Houdaille R., 2012, P 3 MULTIMEDIA SYSTE, P1
   Jarschel Michael, 2013, 2013 Second European Workshop on Software Defined Networks (EWSDN), P87, DOI 10.1109/EWSDN.2013.21
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Kleinrouweler Jan Willem, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P26
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Ni P., 2011, P 19 ACM INT C MULT, P463, DOI DOI 10.1145/2072298.2072359
   Petrangeli S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818361
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seufert M, 2015, INT CONF NETW SER, P256, DOI 10.1109/CNSM.2015.7367367
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Sivaraman V, 2013, PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT '13), P31
   Toni L, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700294
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wilson C, 2011, ACM SIGCOMM COMP COM, V41, P50, DOI 10.1145/2043164.2018443
NR 40
TC 21
Z9 22
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
SU S
AR 42
DI 10.1145/3092836
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FP1UO
UT WOS:000417400400010
DA 2024-07-18
ER

PT J
AU Wilk, S
   Stohr, D
   Effelsberg, W
AF Wilk, Stefan
   Stohr, Denny
   Effelsberg, Wolfgang
TI A Content-Aware Video Adaptation Service to Support Mobile Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 6th ACM International Conference on Multimedia Systems (MMSys)
   Co-Located with 25th ACM Workshop on Network and Operating Systems
   Support for Digital Audio and Video (NOSSDAV)
CY MAR 18-20, 2015
CL Portland, OR
SP ACM
DE DASH; adaptation; content-awareness; video quality
AB Adaptive video streaming systems rely on the availability of different quality versions of a video. Such a system can dynamically adjust the quality of a video stream during its playback depending on the available network throughput. Even if the necessary throughput is available, mobile users can benefit from limiting the generated data traffic as most cellular network contracts have data caps. Usually, if the cap is reached, the throughput is throttled to a speed that does not allow video streaming. Existing systems react to varying network conditions but often neglect content-specific adaptation needs. Content inspection can help to save data traffic when a higher bitrate representation would not increase the perceived quality. In this work, we present the Video Adaptation Service (VAS), a support service for a content-aware video adaptation for mobile devices. Based on the video content, the adaptation process is improved for both the available network resources and the perception of the user. By leveraging the content properties of a video stream, the system is able to maintain a stable video quality and at the same time reduce the generated data traffic. The system is evaluated with different adaptation schemes and shows that content-specific adaptation can both increase the perceived quality as well as reduce the data traffic. Additionally, we demonstrate the practical feasibility of this approach by integrating the VAS into Dynamic Adaptive Streaming over the Hypertext Transfer Protocol.
C1 [Wilk, Stefan; Stohr, Denny; Effelsberg, Wolfgang] Tech Univ Darmstadt, Distributed Multimedia Syst, Rundeturmstr 10, Darmstadt, Germany.
C3 Technical University of Darmstadt
RP Wilk, S (corresponding author), Tech Univ Darmstadt, Distributed Multimedia Syst, Rundeturmstr 10, Darmstadt, Germany.
EM stefan.wilk@cs.tu-darmstadt.de; denny.stohr@cs.tu-darmstadt.de;
   wolfgang.effelsberg@cs.tu-darmstadt.de
FU DFG [CRC 1053 MAKI]
FX This work has been funded by the DFG as part of the CRC 1053 MAKI
   projects C03 and B01.
CR Adzic V, 2012, IEEE T CONSUM ELECTR, V58, P397, DOI 10.1109/TCE.2012.6227439
   Akhshabi S., 2011, ACM C MULT SYST
   Akyol E, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/10236
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], P 7 INT C MULT SYST
   Cicalò S, 2014, INT CONF ACOUST SPEE
   Claeys M, 2014, CONNECT SCI, V26, DOI 10.1080/09540091.2014.885273
   Colonnese S, 2015, AD HOC NETW, V24, P74, DOI 10.1016/j.adhoc.2014.07.023
   De Vleeschauwer D, 2013, IEEE INFOCOM SER, P989
   Devlic A., 2015, IEEE S WORLD WIR MOB
   Eittenberger P. M., 2013, ACM MULT SYST C
   El Essaili A, 2013, IEEE ICC
   Garcia M. - N., 2014, IEEE QOMEX
   Ghinea G., 1998, ACM INT C MULT
   Hasler D., 2003, IS T SPIE HUMAN VISI, V5007
   Hossfeld T, 2015, COMPUT NETW, V81, P320, DOI 10.1016/j.comnet.2015.02.015
   ITU, 2008, ITU R REC P 910 SUBJ
   Juluri P., 2015, IEEE INT C COMM WORK
   Knoche H, 2008, MULTIMED TOOLS APPL, V36, P145, DOI 10.1007/s11042-006-0076-5
   Lederer S., 2012, IEEE C VIS COMM IM P
   Lederer S., 2013, ACM MULT SYST C
   Li Z., 2014, ACM MULT SYST C
   Manweiler J. G., 2012, ACM C MOB SYST APPL
   McCabe T. J., 1976, IEEE Transactions on Software Engineering, VSE-2, P308, DOI 10.1109/TSE.1976.233837
   Medjiah S, 2014, IEEE J SEL AREA COMM, V32, P734, DOI 10.1109/JSAC.2014.140406
   Miller K., 2012, IEEE PACK VID WORKSH
   Mok R. K. P., 2012, ACM MULT SYST C
   Moorthy A. K., 2012, IEEE J SELECTED TOPI
   Muller C., 2012, ACM WORKSH MOB VID
   Ni P., 2011, ACM INT C MULT
   Nurminen JK, 2003, COMPUT OPER RES, V30, P1121, DOI 10.1016/S0305-0548(02)00060-6
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Rajendran R. K., 2002, IEEE INT S CIRC SYST
   Schwarz H., 2007, IEEE T CIRCUITS SYST
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Stockhammer T., 2011, ACM MULT SYST C
   Tian G., 2012, ACM INT C EM NETW EX
   Tian G., 2013, IEEE PACK VID WORKSH
   Thang TC, 2014, IEEE J SEL AREA COMM, V32, P693, DOI 10.1109/JSAC.2014.140403
   Thang TC, 2012, IEEE T CONSUM ELECTR, V58, P78, DOI 10.1109/TCE.2012.6170058
   Van Den Ende N., 2007, SPRING EUR C INT TV
   van Kester S., 2011, SPIE HUMAN VISION EL, V7865
   Wang XF, 2013, IEEE T MULTIMEDIA, V15, P811, DOI 10.1109/TMM.2013.2239630
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson A. B., 2001, SPIE, V4299
   Wichtlhuber M., 2016, ACM MULT SYST C
   Wilk S., 2015, ACM NETW OP SYST SUP
   Winkler S., 2012, IEEE J SELECT TOP SI, V6
   Zabih R., 1999, SPRINGER MULTIMEDIA, V7
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
   Zink M., 2003, SPRING INT C QUAL SE
   Zinner T., 2010, INT WORKSH QUAL MULT
NR 52
TC 5
Z9 5
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2016
VL 12
IS 5
SU S
AR 82
DI 10.1145/2983636
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EJ0VT
UT WOS:000392929700013
DA 2024-07-18
ER

PT J
AU Li, XL
   Chen, ML
   Wang, Q
AF Li, Xuelong
   Chen, Mulin
   Wang, Qi
TI Measuring Collectiveness via Refined Topological Similarity
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimedia; crowd analysis; collectiveness; manifold; feature extraction
ID CROWD SIMULATION; MOTION; SALIENCY; SYSTEM; MODEL
AB Crowd system has motivated a surge of interests in many areas of multimedia, as it contains plenty of information about crowd scenes. In crowd systems, individuals tend to exhibit collective behaviors, and the motion of all those individuals is called collective motion. As a comprehensive descriptor of collective motion, collectiveness has been proposed to reflect the degree of individuals moving as an entirety. Nevertheless, existing works mostly have limitations to correctly find the individuals of a crowd system and precisely capture the various relationships between individuals, both of which are essential to measure collectiveness. In this article, we propose a collectiveness-measuring method that is capable of quantifying collectiveness accurately. Our main contributions are threefold: (1) we compute relatively accurate collectiveness by making the tracked feature points represent the individuals more precisely with a point selection strategy; (2) we jointly investigate the spatial-temporal information of individuals and utilize it to characterize the topological relationship between individuals by manifold learning; (3) we propose a stability descriptor to deal with the irregular individuals, which influence the calculation of collectiveness. Intensive experiments on the simulated and real world datasets demonstrate that the proposed method is able to compute relatively accurate collectiveness and keep high consistency with human perception.
C1 [Li, Xuelong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, State Key Lab Transient Opt & Photon, Ctr Opt Imagery Anal & Learning OPTIMAL, Xian 710119, Shaanxi, Peoples R China.
   [Chen, Mulin; Wang, Qi] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
   [Chen, Mulin; Wang, Qi] Northwestern Polytech Univ, Ctr Opt Imagery Anal & Learning OPTIMAL, Xian 710072, Shaanxi, Peoples R China.
C3 Chinese Academy of Sciences; Xi'an Institute of Optics & Precision
   Mechanics, CAS; State Key Laboratory of Transient Optics & Photonics;
   Northwestern Polytechnical University; Northwestern Polytechnical
   University
RP Wang, Q (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.; Wang, Q (corresponding author), Northwestern Polytech Univ, Ctr Opt Imagery Anal & Learning OPTIMAL, Xian 710072, Shaanxi, Peoples R China.
EM xuelong_li@opt.ac.cn; chenmulin@mail.nwpu.edu.cn; crabwq@nwpu.edu.cn
RI sun, huan/JEO-7152-2023; li, xiang/GWM-6319-2022; LIU, Qing
   Yu/IWV-1159-2023; Li, Xuelong/ABF-3381-2020; Li, Xuelong/Z-3785-2019
OI Li, Xuelong/0000-0002-0019-4197
FU National Natural Science Foundation of China [61105012, 61379094];
   Natural Science Foundation Research Project of Shaanxi Province
   [2015JM6264]; Fundamental Research Funds for the Central Universities
   [3102014JC02020G07, 3102015BJ(II)JJZ01]; Open Research Fund of Key
   Laboratory of Spectral Imaging Technology, Chinese Academy of Sciences
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61105012 and 61379094; by the Natural Science
   Foundation Research Project of Shaanxi Province under Grant 2015JM6264;
   by the Fundamental Research Funds for the Central Universities under
   Grants 3102014JC02020G07 and 3102015BJ(II)JJZ01; and by the Open
   Research Fund of Key Laboratory of Spectral Imaging Technology, Chinese
   Academy of Sciences.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ali S, 2008, LECT NOTES COMPUT SC, V5303, P1, DOI 10.1007/978-3-540-88688-4_1
   Ali S, 2007, PROC CVPR IEEE, P65
   [Anonymous], 2013, ARXIV PREPRINT ARXIV
   [Anonymous], IEEE C ADV VID SIGN
   [Anonymous], 1991, CMUCS91132
   [Anonymous], P 2008 IEEE COMP SOC, DOI DOI 10.1109/CVPR.2008.4587718
   Ballerini M, 2008, P NATL ACAD SCI USA, V105, P1232, DOI 10.1073/pnas.0711437105
   Benabbas Y., 2011, Journal on Image and Video Processing, V2011, P7
   Best A., 2014, S COMP AN, P97
   Bolei Zhou, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3441, DOI 10.1109/CVPR.2011.5995459
   Buhl J, 2006, SCIENCE, V312, P1402, DOI 10.1126/science.1125142
   Chang MC, 2011, IEEE I CONF COMP VIS, P747, DOI 10.1109/ICCV.2011.6126312
   Couzin ID, 2009, TRENDS COGN SCI, V13, P36, DOI 10.1016/j.tics.2008.10.002
   Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176
   Golas A, 2014, IEEE T VIS COMPUT GR, V20, P1022, DOI 10.1109/TVCG.2013.235
   Guy SJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366209
   Heck R, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198306
   Helbing D, 2000, NATURE, V407, P487, DOI 10.1038/35035023
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hu WM, 2006, IEEE T PATTERN ANAL, V28, P1450, DOI 10.1109/TPAMI.2006.176
   Huang KQ, 2009, IEEE T SYST MAN CY B, V39, P1028, DOI 10.1109/TSMCB.2008.2011815
   Kratz L, 2012, LECT NOTES COMPUT SC, V7575, P558, DOI 10.1007/978-3-642-33765-9_40
   Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771
   KRU S. N., 1970, MAT SB, V81, P228, DOI [DOI 10.1070/SM1970V010N02ABEH002156, 10.1070/SM1970v010n02ABEH002156]
   Lan T, 2012, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2012.6247821
   Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Moussaid M, 2009, TOP COGN SCI, V1, P469, DOI 10.1111/j.1756-8765.2009.01028.x
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Saleemi I, 2010, PROC CVPR IEEE, P2069, DOI 10.1109/CVPR.2010.5539884
   Scovanner P, 2009, IEEE I CONF COMP VIS, P381, DOI 10.1109/ICCV.2009.5459224
   Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285
   Shao L, 2014, IEEE T CYBERNETICS, V44, P817, DOI 10.1109/TCYB.2013.2273174
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   VICSEK T, 1995, PHYS REV LETT, V75, P1226, DOI 10.1103/PhysRevLett.75.1226
   Wang Q, 2013, IEEE T CIRC SYST VID, V23, P1150, DOI 10.1109/TCSVT.2012.2226528
   Wang Q, 2013, IEEE T CYBERNETICS, V43, P660, DOI 10.1109/TSMCB.2012.2214210
   Wang XG, 2011, INT J COMPUT VISION, V95, P287, DOI 10.1007/s11263-011-0459-6
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang Y, 2009, IEEE I CONF COMP VIS, P1669, DOI 10.1109/ICCV.2009.5459376
   Zawidzki M, 2014, PATTERN RECOGN LETT, V44, P88, DOI 10.1016/j.patrec.2013.10.025
   Zhang HP, 2010, P NATL ACAD SCI USA, V107, P13626, DOI 10.1073/pnas.1001651107
   Zhang P, 2015, VISUAL COMPUT, V31, P5, DOI 10.1007/s00371-013-0900-7
   Zhao L, 2015, SIGNAL PROCESS, V108, P36, DOI 10.1016/j.sigpro.2014.07.031
   Zhao X, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2648583
   [郑利平 Zheng Liping], 2014, [图学学报, Journal of Graphics], V35, P110
   Zhou BL, 2012, LECT NOTES COMPUT SC, V7573, P857, DOI 10.1007/978-3-642-33709-3_61
   Zhou BL, 2014, IEEE T PATTERN ANAL, V36, P1586, DOI 10.1109/TPAMI.2014.2300484
   Zhou BL, 2012, PROC CVPR IEEE, P2871, DOI 10.1109/CVPR.2012.6248013
   Zhou DY, 2004, ADV NEUR IN, V16, P169
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhou SP, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1236471.1236474
NR 57
TC 6
Z9 8
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2016
VL 12
IS 2
AR 34
DI 10.1145/2854000
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PK
UT WOS:000373906200008
DA 2024-07-18
ER

PT J
AU Hao, F
   Jiao, MJ
   Min, GY
   Yang, LT
AF Hao, Fei
   Jiao, Mingjie
   Min, Geyong
   Yang, Laurence T.
TI Launching an Efficient Participatory Sensing Campaign: A Smart Mobile
   Device-Based Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; Performance; Participatory sensing; deployment;
   recruitment; tensor; DTA; trajectory
ID REPUTATION
AB Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies.
C1 [Hao, Fei; Jiao, Mingjie; Yang, Laurence T.] Huazhong Univ Sci & Technol, Sch Comp Sci & Engn, Wuhan, Peoples R China.
   [Min, Geyong] Univ Exeter, Dept Math & Comp Sci, Exeter EX4 4QJ, Devon, England.
C3 Huazhong University of Science & Technology; University of Exeter
RP Hao, F (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Engn, Wuhan, Peoples R China.
EM feehao@gmail.com; mingjie.v@gmail.com; g.min@exeter.ac.uk;
   ltyang@gmail.com
RI Laurence T. Yang, FCAE/AAA-1898-2019
OI Laurence T. Yang, FCAE/0000-0002-7986-4244; Hao,
   Fei/0000-0001-5288-5523; Min, Geyong/0000-0003-1395-7314
CR Ahmadi H., 2010, Proceedings of the 8th ACM Conference on Embedded Networked Sensor Systems, P99
   Amintoosi H, 2014, MOBILE NETW APPL, V19, P88, DOI 10.1007/s11036-013-0455-x
   Amintoosi H, 2013, 2013 9TH IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING IN SENSOR SYSTEMS (IEEE DCOSS 2013), P266, DOI 10.1109/DCOSS.2013.29
   Ammari HM, 2012, IEEE T COMPUT, V61, P118, DOI 10.1109/TC.2011.82
   [Anonymous], 2004, ACM Trans Embedded Comput Syst, DOI DOI 10.1145/972627.972631
   [Anonymous], 2010, P 2010 7 IEEE CONSUM, DOI DOI 10.1109/CCNC.2010.5421661
   [Anonymous], 2006, KDD
   Dong YF, 2008, LECT NOTES COMPUT SC, V5067, P140, DOI 10.1007/978-3-540-69170-9_10
   Eisenman SB, 2007, SENSYS'07: PROCEEDINGS OF THE 5TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P87
   Gaonkar S, 2008, MOBISYS'08: PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P174
   Hao F, 2014, IEEE COMMUN MAG, V52, P41, DOI 10.1109/MCOM.2014.6979950
   Kanhere SalilS., 2011, 12 IEEE INT C MOBILE, P3, DOI [10.1109/MDM.2011.16, DOI 10.1109/MDM.2011.16, 10.1109/mdm.2011.16]
   Karaliopoulos Merkouris, 2015, P IEEE INFOCOM 15
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kotovirta V., 2012, 2012 Sixth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS 2012), P155, DOI 10.1109/IMIS.2012.70
   Li L, 2013, WIREL COMMUN MOB COM, V13, P1, DOI [10.1002/wcm.1087, 10.1002/wcm.10]
   Liao WH, 2015, WIRELESS PERS COMMUN, V82, P2135, DOI 10.1007/s11277-015-2338-x
   Lin M, 2013, IEEE T EMERG TOP COM, V1, P353, DOI 10.1109/TETC.2013.2274042
   Liu Yang, 2012, Algorithms and Architectures for Parallel Processing. Proceedings of the 12th International Conference, ICA3PP 2012, P517, DOI 10.1007/978-3-642-33078-0_37
   Luo T, 2014, IEEE INFOCOM SER, P127, DOI 10.1109/INFOCOM.2014.6847932
   Miettinen A. P., 2010, P 2 USENIX C HOT TOP, V10, P4
   Mini S, 2014, IEEE SENS J, V14, P636, DOI 10.1109/JSEN.2013.2286332
   Osmani A, 2009, 2009 1ST INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE, COMMUNICATION SYSTEMS AND NETWORKS(CICSYN 2009), P90, DOI 10.1109/CICSYN.2009.97
   Rana RK, 2010, PROCEEDINGS OF THE 9TH ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P105, DOI 10.1145/1791212.1791226
   Reddy S, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1817
   Reddy S, 2010, LECT NOTES COMPUT SC, V6030, P138, DOI 10.1007/978-3-642-12654-3_9
   Sun Jimeng, 2008, ACM T KNOWL DISCOV D, V2
   TUNCAY GS, 2012, P 18 ANN INT C MOB C, P407
   Wang B, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1978802.1978811
   Wang GL, 2006, IEEE T MOBILE COMPUT, V5, P640, DOI 10.1109/TMC.2006.80
   Wang XL, 2013, IEEE INFOCOM SER, P2517
   Weinschrott Harald, 2010, 2010 IEEE 7th International Conference on Mobile Ad-Hoc and Sensor Systems (MASS 2010), P195, DOI 10.1109/MASS.2010.5663996
   Xu Ya, 2001, Proc. of International Conference on Mobile Computing and Networking, P70, DOI [10.1145/381677.381685, DOI 10.1145/381677.381685]
   Zheng Y, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1436, DOI 10.1145/2487575.2488188
NR 34
TC 12
Z9 12
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 18
DI 10.1145/2808198
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Toni, L
   Aparicio-Pardo, R
   Pires, K
   Simon, G
   Blanc, A
   Frossard, P
AF Toni, Laura
   Aparicio-Pardo, Ramon
   Pires, Karine
   Simon, Gwendal
   Blanc, Alberto
   Frossard, Pascal
TI Optimal Selection of Adaptive Streaming Representations
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Dynamic adaptive streaming over HTTP; content distribution;
   video streaming; integer linear program
AB Adaptive streaming addresses the increasing and heterogeneous demand of multimedia content over the Internet by offering several encoded versions for each video sequence. Each version (or representation) is characterized by a resolution and a bit rate, and it is aimed at a specific set of users, like TV or mobile phone clients. While most existing works on adaptive streaming deal with effective playout-buffer control strategies on the client side, in this article we take a providers' perspective and propose solutions to improve user satisfaction by optimizing the set of available representations. We formulate an integer linear program that maximizes users' average satisfaction, taking into account network dynamics, type of video content, and user population characteristics. The solution of the optimization is a set of encoding parameters corresponding to the representations set that maximizes user satisfaction. We evaluate this solution by simulating multiple adaptive streaming sessions characterized by realistic network statistics, showing that the proposed solution outperforms commonly used vendor recommendations, in terms of user satisfaction but also in terms of fairness and outage probability. The simulation results show that video content information as well as network constraints and users' statistics play a crucial role in selecting proper encoding parameters to provide fairness among users and to reduce network resource usage. We finally propose a few theoretical guidelines that can be used, in realistic settings, to choose the encoding parameters based on the user characteristics, the network capacity and the type of video content.
C1 [Toni, Laura; Frossard, Pascal] EPFL STI IEL LTS4, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
   [Aparicio-Pardo, Ramon; Pires, Karine; Simon, Gwendal; Blanc, Alberto] Inst Mines Telecom, Telecom Bretagne, Rennes, France.
C3 IMT - Institut Mines-Telecom; IMT Atlantique
RP Toni, L (corresponding author), EPFL STI IEL LTS4, Signal Proc Lab LTS4, Stn 11, CH-1015 Lausanne, Switzerland.
EM laura.toni@epfl.ch; ramon.aparicio@telecom-bretagne.eu;
   karine.pires@telecom-bretagne.eu; gwendal.simon@telecom-bretagne.eu;
   alberto.blanc@telecom-bretagne.eu; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019; Simon, Gwendal/Y-6950-2019
OI Simon, Gwendal/0000-0002-7282-918X; Aparicio Pardo,
   Ramon/0000-0002-6069-2841; Toni, Laura/0000-0002-8441-8791
FU Swiss National Science Foundation (SNSF) under the CHISTERA project
   CONCERT (A Context-Adaptive Content Ecosystem Under Uncertainty) [FNS
   20CH21_151569]; Swiss National Science Foundation (SNF) [20CH21_151569]
   Funding Source: Swiss National Science Foundation (SNF)
FX This work was partially funded by the Swiss National Science Foundation
   (SNSF) under the CHISTERA project CONCERT (A Context-Adaptive Content
   Ecosystem Under Uncertainty), project nr FNS 20CH21_151569.
CR Adhikari V. K., 2012, P IEEE INFOCOM
   Akhshabi S, 2012, SIGNAL PROCESS-IMAGE, V27, P271, DOI 10.1016/j.image.2011.10.003
   Akhshabi Saamer, 2012, P ACM INT WORKSH NET
   Ali El Essaili, 2013, P INT C IEEE COMM
   [Anonymous], ARXIV14012209
   [Anonymous], GONE FISHIN JUSTIN T
   Apple (a), BEST PRACT CREAT DEP
   Besson Adrien, 2013, P 2013 IEEE INT C IM
   Cisco, 2012, VIS NETW IND 2011 20
   De Cicco L, 2014, IEEE ACM T NETWORK, V22, P526, DOI 10.1109/TNET.2013.2253797
   DUCHON CE, 1979, J APPL METEOROL, V18, P1016, DOI 10.1175/1520-0450(1979)018<1016:LFIOAT>2.0.CO;2
   Grafl Michal, COMBINED BITRATE SUG
   IBM, ILOG CPLEX OPT STUD
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Joseph Vinay, 2013, ARXIV13077210
   Lee JS, 2012, IEEE COMMUN MAG, V50, P38, DOI 10.1109/MCOM.2012.6178832
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Li Zhi, 2014, P ACM MULT SYST C MM
   Ma Zhan, 2012, ARXIVABS12062625
   Miller Konstantin, 2012, P IEEE PACK VID WORK
   Mok Ricky K. P., 2012, P ACM MULT SYST C MM
   Netflix, ENC STREAM
   Nygren E., 2010, SIGOPS OPER SYST REV, V44, P2, DOI [10.1145/1842733.1842736, DOI 10.1145/1842733.1842736]
   Simone Basso, 2014, P 2013 IEEE INT C IM
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Sodagar Iraj, 2012, ISOIECJTC1SC29WG11
   Stockhammer T., 2011, P ACM MULT SYST C MM
   Toni Laura, 2014, ARXIV14063161
   Toni Laura, 2014, P ACM MULT SYST C MM
   Thang TC, 2014, IEEE J SEL AREA COMM, V32, P693, DOI 10.1109/JSAC.2014.140403
   Tseng Chin-Hsiao, 2018, LIVER INT, V38, P2018, DOI [10.1111/liv.13872, DOI 10.1111/LIV.13872]
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhu Xiaoqing, 2013, P IEEE WORKSH MULT S
NR 33
TC 39
Z9 42
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2015
VL 11
IS 2
SU S
SI SI
AR 43
DI 10.1145/2700294
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC7RV
UT WOS:000350567000003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lathey, A
   Atrey, PK
AF Lathey, Ankita
   Atrey, Pradeep K.
TI Image Enhancement in Encrypted Domain over Cloud
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Cloud computing; image enhancement; secret sharing; encrypted
   domain processing
AB Cloud-based multimedia systems are becoming increasingly common. These systems offer not only storage facility, but also high-end computing infrastructure which can be used to process data for various analysis tasks ranging from low-level data quality enhancement to high-level activity and behavior identification operations. However, cloud data centers, being third party servers, are often prone to information leakage, raising security and privacy concerns. In this article, we present a Shamir's secret sharing based method to enhance the quality of encrypted image data over cloud. Using the proposed method we show that several image enhancement operations such as noise removal, antialiasing, edge and contrast enhancement, and dehazing can be performed in encrypted domain with near-zero loss in accuracy and minimal computation and data overhead. Moreover, the proposed method is proven to be information theoretically secure.
C1 [Lathey, Ankita; Atrey, Pradeep K.] Univ Winnipeg, Dept Appl Comp Sci, Winnipeg, MB R3B 2E9, Canada.
   [Atrey, Pradeep K.] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA.
C3 University of Winnipeg; State University of New York (SUNY) System;
   State University of New York (SUNY) Albany
RP Lathey, A (corresponding author), Univ Winnipeg, Dept Appl Comp Sci, 515 Portage Ave, Winnipeg, MB R3B 2E9, Canada.
EM lathey-a@webmail.uwinnipeg.ca; p.atrey@uwinnipeg.ca
FU Natural Sciences and Engineering Research Council of Canada [371714]
FX The majority of this work was undertaken at the University of Winnipeg
   and supported by the Natural Sciences and Engineering Research Council
   of Canada, Discovery Grant #371714.
CR [Anonymous], P IEEE NAT COMP C
   [Anonymous], INRIA PERSON DATASET
   [Anonymous], LICENSE PLATE DETECT
   [Anonymous], THESIS U ILLES BALEA
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], IEEE SIGNAL PROCESS
   Basu M, 2002, IEEE T SYST MAN CY C, V32, P252, DOI 10.1109/TSMCC.2002.804448
   BENALOH JC, 1987, LECT NOTES COMPUT SC, V263, P251
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Cai YH, 2006, INT C PATT RECOG, P980
   Cha YJ, 2006, IEEE T IMAGE PROCESS, V15, P2315, DOI 10.1109/TIP.2006.875182
   Chang CC, 2008, INFORM SCIENCES, V178, P2433, DOI 10.1016/j.ins.2007.12.016
   Chu K.-Y., 2013, Proceedings of the 21st ACM international conference on Multimedia, P597
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Davis L.S., 1975, Comput. Graph. Image Process, V4, P248, DOI DOI 10.1016/0146-664X(75)90012-X
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Heath MD, 1997, IEEE T PATTERN ANAL, V19, P1338, DOI 10.1109/34.643893
   Hsu Chao-Yung., 2009, ACM International Conference on Multimedia, P637
   Islam N, 2009, LECT NOTES COMPUT SC, V5703, P121, DOI 10.1007/978-3-642-03688-0_13
   Joshi N., 2010, P IEEE INT C COMPUTA, P1
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lathey Ankita., 2013, Proceedings of 7th IEEE International Conference on Semantic Computing (ICSC) (Irvine, CA, USA), P310
   Mohammadi M., 2013, Iranian Conference on Electrical Engineering (ICEE), P1, DOI DOI 10.1109/PESMG.2013.6672514
   Mohanty M., 2012, Proceedings of the 20th ACM international conference on Multimedia, P1105
   Narasimhan S. G., 2003, Interactive (de) weathering of an image using physical models, V6, P1
   Rahulamathavan Y, 2013, IEEE T AFFECT COMPUT, V4, P83, DOI 10.1109/T-AFFC.2012.33
   Rao YB, 2010, OPT ENG, V49, DOI 10.1117/1.3520553
   Sadeghi AR, 2010, LECT NOTES COMPUT SC, V5984, P229
   SaghaianNejadEsfahani SM, 2012, IEEE IMAGE PROC, P253, DOI 10.1109/ICIP.2012.6466843
   Schechner YY, 2001, PROC CVPR IEEE, P325
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Sheikh H.R., 2005, LIVE IMAGE QUALITY A, V2
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Tan KK, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P788, DOI 10.1109/ICIP.2000.899827
   Tan R. T., 2008, PROC IEEE C COMPUT V, P1
   Upmanyu M, 2009, IEEE I CONF COMP VIS, P1639, DOI 10.1109/ICCV.2009.5459370
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
NR 39
TC 34
Z9 35
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2015
VL 11
IS 3
AR 38
DI 10.1145/2656205
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CB8BF
UT WOS:000349852500006
DA 2024-07-18
ER

PT J
AU Lin, CW
   Chen, KW
   Chen, SC
   Chen, CW
   Hung, YP
AF Lin, Chih-Wei
   Chen, Kuan-Wen
   Chen, Shen-Chi
   Chen, Cheng-Wu
   Hung, Yi-Ping
TI Large-Area, Multilayered, and High-Resolution Visual Monitoring Using a
   Dual-Camera System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Security; PTZ camera; wide-angle fixed camera; large-area,
   multilayered, and high-resolution system; dual-camera system; visual
   monitoring
ID FACE DETECTION; PTZ CAMERAS; RECOGNITION; TRACKING; TILT; PAN
AB Large-area, high-resolution visual monitoring systems are indispensable in surveillance applications. To construct such systems, high-quality image capture and display devices are required. Whereas high-quality displays have rapidly developed, as exemplified by the announcement of the 85-inch 4K ultrahigh-definition TV by Samsung at the 2013 Consumer Electronics Show (CES), high-resolution surveillance cameras have progressed slowly and remain not widely used compared with displays. In this study, we designed an innovative framework, using a dual-camera system comprising a wide-angle fixed camera and a high-resolution pan-tilt-zoom (PTZ) camera to construct a large-area, multilayered, and high-resolution visual monitoring system that features multiresolution monitoring of moving objects. First, we developed a novel calibration approach to estimate the relationship between the two cameras and calibrate the PTZ camera. The PTZ camera was calibrated based on the consistent property of distinct pan-tilt angle at various zooming factors, accelerating the calibration process without affecting accuracy; this calibration process has not been reported previously. After calibrating the dual-camera system, we used the PTZ camera and synthesized a large-area and high-resolution background image. When foreground targets were detected in the images captured by the wide-angle camera, the PTZ camera was controlled to continuously track the user-selected target. Last, we integrated preconstructed high-resolution background and low-resolution foreground images captured using the wide-angle camera and the high-resolution foreground image captured using the PTZ camera to generate a large-area, multilayered, and high-resolution view of the scene.
C1 [Lin, Chih-Wei; Chen, Shen-Chi] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
   [Chen, Kuan-Wen; Hung, Yi-Ping] Natl Taiwan Univ, Taipei, Taiwan.
   [Chen, Cheng-Wu] Natl Kaohsiung Marine Univ, Inst Maritime Informat & Technol, Kaohsiung, Taiwan.
C3 National Taiwan University; National Taiwan University; National
   Kaohsiung University of Science & Technology
RP Lin, CW (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
EM d00922004@csie.ntu.edu.tw
RI CHEN, CHEN/HDM-6657-2022; Chen, Tim/W-7970-2019; Chen,
   Kuan-Wen/AAJ-3870-2020
OI Chen, Kuan-Wen/0000-0002-4159-201X
FU Excellent Research Projects of National Taiwan University [NSC
   102-2221-E-002-171-]
FX This work was supported in part by the Excellent Research Projects of
   National Taiwan University, under grant NSC 102-2221-E-002-171-.
CR Ahlborn BA, 2006, P IEEE VIRT REAL ANN, P281, DOI 10.1109/VR.2006.7
   Alahi A, 2008, IEEE IMAGE PROC, P1713
   Andriluka M., 2008, P 12 IEEE INT C COMP, P1511
   [Anonymous], 1999, 1999 IEEE COMP SOC C
   [Anonymous], 2012, P 11 INT C AUT AG MU
   [Anonymous], 2001, INT J COMPUT VIS
   [Anonymous], 2012, 2012 IEEE INT C COMP, DOI [10.1109/ICCIC.2012.6510299, DOI 10.1109/ICCIC.2012.6510299]
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Baudisch P., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P31, DOI 10.1145/502348.502354
   Baudisch P., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P259, DOI 10.1145/503376.503423
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Brown M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1218
   Cai YH, 2013, IEEE WORK APP COMP, P31, DOI 10.1109/WACV.2013.6474996
   Cha Zhang, 2008, 2008 IEEE 10th Workshop on Multimedia Signal Processing (MMSP), P28, DOI 10.1109/MMSP.2008.4665044
   Chen CC, 2009, PROC CVPR IEEE, P1078, DOI 10.1109/CVPRW.2009.5206780
   Chen CH, 2008, IEEE T CIRC SYST VID, V18, P1052, DOI 10.1109/TCSVT.2008.928223
   Chen K. W., 2010, P ACM MULT OCT, P311
   Chen KW, 2011, IEEE T MULTIMEDIA, V13, P1256, DOI 10.1109/TMM.2011.2165055
   Dornaika F, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2168996.2168997
   Elder JH, 2007, INT J COMPUT VISION, V72, P47, DOI 10.1007/s11263-006-8892-7
   Elder J. H, 2005, ATTENTIVE WIDE FIELD
   Ess A., 2008, COMPUTER VISION PATT, V2008, P1, DOI [10.1109/CVPR.2008.4587581, DOI 10.1109/CVPR.2008.4587581]
   Gracias N, 2009, IMAGE VISION COMPUT, V27, P597, DOI 10.1016/j.imavis.2008.04.014
   HU TT, 2008, P 3 IEEE INT WORKSH, P177
   Kang Xue, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2949, DOI 10.1109/ICIP.2011.6116280
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Lalonde  M, 2007, P SPIE, V6575
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Marchesotti L, 2003, IEEE IMAGE PROC, P681
   Mian Adnan Noor, 2008, 2008 IEEE International Parallel & Distributed Processing Symposium, P1
   Micheloni C, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P480
   Micheloni C, 2005, IEE P-VIS IMAGE SIGN, V152, P205, DOI 10.1049/ip-vis:20041256
   Micheloni C, 2010, IEEE SIGNAL PROC MAG, V27, P78, DOI 10.1109/MSP.2010.937333
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   PLAISANT C, 1995, IEEE SOFTWARE, V12, P21, DOI 10.1109/52.368260
   Prince SJD, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P439
   Reale M, 2010, IEEE INT CON MULTI, P280, DOI 10.1109/ICME.2010.5583014
   Sansoni G, 1999, APPL OPTICS, V38, P6565, DOI 10.1364/AO.38.006565
   Singh VK, 2008, MACH VISION APPL, V19, P375, DOI 10.1007/s00138-007-0082-2
   SINGH VK, 2005, P 3 ACM INT WORKSH V, P149
   Sinha SN, 2006, COMPUT VIS IMAGE UND, V103, P170, DOI 10.1016/j.cviu.2006.06.002
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Szeliski R., 2004, MSRTR20042092
   Wang Q, 2012, IEEE T IMAGE PROCESS, V21, P4454, DOI 10.1109/TIP.2012.2205700
   Wang RP, 2012, IEEE T IMAGE PROCESS, V21, P4466, DOI 10.1109/TIP.2012.2206039
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Wheeler Frederick W., 2010, 2010 4 IEEE INT C BI, P1
   Xue K, 2013, MACH VISION APPL, V24, P477, DOI 10.1007/s00138-012-0426-4
   Ye YM, 2000, MACH VISION APPL, V12, P32, DOI 10.1007/s001380050122
   Zhao T, 2003, PROC CVPR IEEE, P459, DOI 10.1109/NSSMIC.2003.1352083
   Zhao T, 2002, IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING (MOTION 2002), PROCEEDINGS, P9, DOI 10.1109/MOTION.2002.1182207
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhou X., 2003, INT WORKSHOP VIDEO S, P113, DOI DOI 10.1145/982452.982467
NR 54
TC 11
Z9 12
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2014
VL 11
IS 2
AR 30
DI 10.1145/2645862
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6DM
UT WOS:000348308800007
DA 2024-07-18
ER

PT J
AU Mou, LT
   Huang, TJ
   Tian, YH
   Jiang, ML
   Gao, W
AF Mou, Luntian
   Huang, Tiejun
   Tian, Yonghong
   Jiang, Menglin
   Gao, Wen
TI Content-Based Copy Detection through Multimodal Feature Representation
   and Temporal Pyramid Matching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Performance; Security;
   Verification; Content-based copy detection; feature representation;
   temporal pyramid matching
ID ROBUST; CLASSIFICATION; SECURE
AB Content-based copy detection (CBCD) is drawing increasing attention as an alternative technology to watermarking for video identification and copyright protection. In this article, we present a comprehensive method to detect copies that are subjected to complicated transformations. A multimodal feature representation scheme is designed to exploit the complementarity of audio features, global and local visual features so that optimal overall robustness to a wide range of complicated modifications can be achieved. Meanwhile, a temporal pyramid matching algorithm is proposed to assemble frame-level similarity search results into sequence-level matching results through similarity evaluation over multiple temporal granularities. Additionally, inverted indexing and locality sensitive hashing (LSH) are also adopted to speed up similarity search. Experimental results over benchmarking datasets of TRECVID 2010 and 2009 demonstrate that the proposed method outperforms other methods for most transformations in terms of copy detection accuracy. The evaluation results also suggest that our method can achieve competitive copy localization preciseness.
   Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Information filtering, Search process; I.4.7 [Image Processing and Computer Vision]: FeatureMeasurement-Feature representation
C1 [Mou, Luntian; Huang, Tiejun; Tian, Yonghong; Jiang, Menglin; Gao, Wen] Peking Univ, Beijing 1000871, Peoples R China.
C3 Peking University
RP Huang, TJ (corresponding author), Peking Univ, 2 Sci Bldg,5 Yiheyuan Rd, Beijing 1000871, Peoples R China.
EM tjhuang@pku.edu.cn
RI Mou, Luntian/ACX-6553-2022; Huang, Tiejun/D-6161-2011
OI Mou, Luntian/0000-0002-1551-4448; 
FU Chinese National Natural Science Foundation [61035001]; National Basic
   Research Program of China [2009CB320906]
FX This work was partially supported by grants from the Chinese National
   Natural Science Foundation under contract no. 61035001 and National
   Basic Research Program of China under contract no. 2009CB320906.
CR Ahmed F, 2010, SIGNAL PROCESS, V90, P1456, DOI 10.1016/j.sigpro.2009.05.024
   BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716
   Cano P, 2005, J VLSI SIG PROC SYST, V41, P271, DOI 10.1007/s11265-005-4151-3
   Cano P., 2002, P AES 112 INT CONV G
   Chen JP, 2008, LECT NOTES COMPUT SC, V5353, P887, DOI 10.1007/978-3-540-89796-5_106
   Chen L, 2008, PATTERN RECOGN LETT, V29, P1824, DOI 10.1016/j.patrec.2008.05.015
   Cheung SCS, 2003, IEEE T CIRC SYST VID, V13, P59, DOI 10.1109/TCSVT.2002.808080
   De Roover C, 2005, IEEE T SIGNAL PROCES, V53, P4020, DOI 10.1109/TSP.2005.855414
   Douze M, 2010, IEEE T MULTIMEDIA, V12, P257, DOI 10.1109/TMM.2010.2046265
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Hampapur A., 2001, P IEEE INT C MULT EX, P737, DOI DOI 10.1109/ICME.2001.1237827
   Hua XS, 2004, IEEE IMAGE PROC, P685
   Huang TJ, 2010, COMPUTER, V43, P28, DOI 10.1109/MC.2010.356
   Iwamoto K, 2006, IEEE IMAGE PROC, P3185, DOI 10.1109/ICIP.2006.313046
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Kim C, 2005, IEEE T CIRC SYST VID, V15, P127
   Law-To J., 2006, P ACM INT C MULTIMED, P835
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lee S, 2006, P ITN C AC SPEECH, V2, P401, DOI DOI 10.1109/ICASSP.2006.1660364
   LI Y., 2010, ONL P TRECVID 2010 W
   Lin CY, 2001, IEEE T CIRC SYST VID, V11, P153, DOI 10.1109/76.905982
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luntian Mou, 2011, 2011 IEEE Consumer Communications and Networking Conference (CCNC 2011), P323, DOI 10.1109/CCNC.2011.5766482
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mou LT, 2012, IEEE INT SYMP CIRC S, P1131
   MPEG, 2002, 1593842002 MPEG ISOI
   Ngoc P, 2008, 2008 INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS, VOLS 1-4, P2038
   Oostveen J., 2002, Recent Advances in Visual Information Systems. 5th International Conference VISUAL 2002. Proceedings (Lecture Notes in Computer Science Vol.2314), P117
   Over P., 2010, P TRECVID
   Radhakrishnan R, 2008, INT CONF ACOUST SPEE, P2245, DOI 10.1109/ICASSP.2008.4518092
   SHIVAKUMAR N, 1999, THESIS STANFORD U
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Swaminathan A, 2006, IEEE T INF FOREN SEC, V1, P215, DOI 10.1109/TIFS.2006.873601
   Wang XY, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1865106.1865109
   Wei SK, 2011, IEEE T CIRC SYST VID, V21, P15, DOI 10.1109/TCSVT.2011.2105554
   Yonghong Tian, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3629, DOI 10.1109/ICIP.2011.6116504
NR 39
TC 16
Z9 16
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2013
VL 10
IS 1
AR 5
DI 10.1145/2542205.2542208
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 280JP
UT WOS:000329025400005
DA 2024-07-18
ER

PT J
AU Roodaki, H
   Hashemi, MR
   Shirmohammadi, S
AF Roodaki, Hoda
   Hashemi, Mahmoud Reza
   Shirmohammadi, Shervin
TI A New Methodology to Derive Objective Quality Assessment Metrics for
   Scalable Multiview 3D Video Coding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Multiview 3D video; mobile 3D video; objective quality
   assessment; scalable modalities
ID COMPRESSION; PREDICTION; DISPARITY
AB With the growing demand for 3D video, efforts are underway to incorporate it in the next generation of broadcast and streaming applications and standards. 3D video is currently available in games, entertainment, education, security, and surveillance applications. A typical scenario for multiview 3D consists of several 3D video sequences captured simultaneously from the same scene with the help of multiple cameras from different positions and through different angles. Multiview video coding provides a compact representation of these multiple views by exploiting the large amount of inter-view statistical dependencies. One of the major challenges in this field is how to transmit the large amount of data of a multiview sequence over error prone channels to heterogeneous mobile devices with different bandwidth, resolution, and processing/battery power, while maintaining a high visual quality. Scalable Multiview 3D Video Coding (SMVC) is one of the methods to address this challenge; however, the evaluation of the overall visual quality of the resulting scaled-down video requires a new objective perceptual quality measure specifically designed for scalable multiview 3D video. Although several subjective and objective quality assessment methods have been proposed for multiview 3D sequences, no comparable attempt has been made for quality assessment of scalable multiview 3D video. In this article, we propose a new methodology to build suitable objective quality assessment metrics for different scalable modalities in multiview 3D video. Our proposed methodology considers the importance of each layer and its content as a quality of experience factor in the overall quality. Furthermore, in addition to the quality of each layer, the concept of disparity between layers (inter-layer disparity) and disparity between the units of each layer (intra-layer disparity) is considered as an effective feature to evaluate overall perceived quality more accurately. Simulation results indicate that by using this methodology, more efficient objective quality assessment metrics can be introduced for each multiview 3D video scalable modalities.
C1 [Roodaki, Hoda; Hashemi, Mahmoud Reza; Shirmohammadi, Shervin] Univ Tehran, Coll Engn, Sch Elect & Comp Engn, Multimedia Proc Lab, Tehran 14174, Iran.
   [Shirmohammadi, Shervin] Univ Ottawa, Sch Informat Technol & Engn, Distributed & Collaborat Virtual Environm Res Lab, DISCOVER Lab, Ottawa, ON K1N 6N5, Canada.
C3 University of Tehran; University of Ottawa
RP Roodaki, H (corresponding author), Univ Tehran, Coll Engn, Sch Elect & Comp Engn, Multimedia Proc Lab, Tehran 14174, Iran.
EM h.roodaki@ut.ac.ir; rhashemi@ut.ac.ir; shervin@site.uottawa.ca
RI Shirmohammadi, Shervin/E-6945-2012; Roodaki, Hoda/N-6891-2019; Hashemi,
   Mahmoud Reza/H-2172-2011
OI Shirmohammadi, Shervin/0000-0002-3973-4445; Hashemi, Mahmoud
   Reza/0000-0002-3518-9195
CR [Anonymous], 2011, P 3DTV C MAY
   [Anonymous], P IEEE COMSOC MMTC E
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], BT812 ITUR
   [Anonymous], 2007, P 3 INT C NETW SERV
   [Anonymous], P SPIE
   [Anonymous], MULT VID COD
   [Anonymous], 2011, P 3DTV C TRUE VIS CA
   [Anonymous], P 3DTV C TRUE VIS CA
   [Anonymous], 1SC29WG112005 ISOIEC
   [Anonymous], 2000, Document ITU-R Recommendation BT.500-11
   [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], T BROADCAST
   Barkowsky Marcus, 2010, 2010 18th International Packet Video Workshop (PV 2010), P193, DOI 10.1109/PV.2010.5706838
   Dan G., 2010, 2010 1 IEEE INT C SM, P1
   Do L, 2010, IEEE INT CON MULTI, P1730, DOI 10.1109/ICME.2010.5583175
   Dodgson NA, 2005, COMPUTER, V38, P31, DOI 10.1109/MC.2005.252
   Feghali R, 2007, IEEE T BROADCAST, V53, P441, DOI 10.1109/TBC.2007.891700
   Ha HZ, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P61
   Hewage CTER, 2008, ELECTRON LETT, V44, P963, DOI 10.1049/el:20081562
   Hewage C.T. E. R., 2010, Future Network MobileSummit 2010 Conference Proceedings, P1
   Hewage CTER, 2011, IEEE T CONSUM ELECTR, V57, P1185, DOI 10.1109/TCE.2011.6018873
   Hewage CTER, 2010, IEEE IMAGE PROC, P4017, DOI 10.1109/ICIP.2010.5653741
   Jin L., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2521, DOI 10.1109/ICIP.2011.6116175
   Joveluro P, 2010, 3DTV CONF
   Kwangsung Ha, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2525, DOI 10.1109/ICIP.2011.6116176
   Leon Gustavo, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P301, DOI 10.1109/3DTV.2008.4547868
   Li CY, 2010, CANCER PREV RES, V3, P246, DOI 10.1158/1940-6207.CAPR-08-0228
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Li YX, 2009, PROCEEDINGS OF 2009 2ND IEEE INTERNATIONAL CONFERENCE ON BROADBAND NETWORK & MULTIMEDIA TECHNOLOGY, P882, DOI 10.1109/ICBNMT.2009.5347811
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liyuan Xing, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3105, DOI 10.1109/ICIP.2011.6116323
   Maalouf Aldo, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P11, DOI 10.1109/QOMEX.2010.5518301
   Micallef B. W., 2010, 2010 28th Picture Coding Symposium (PCS 2010), P38, DOI 10.1109/PCS.2010.5702516
   Mittal A, 2011, 2011 IEEE DIGITAL SIGNAL PROCESSING WORKSHOP AND IEEE SIGNAL PROCESSING EDUCATION WORKSHOP (DSP/SPE), P338, DOI 10.1109/DSP-SPE.2011.5739236
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Rodriguez Erick Jimenez, 2009, 2009 15th Asia-Pacific Conference on Communications (APCC 2009), P790, DOI 10.1109/APCC.2009.5375485
   Saygili G, 2010, IEEE IMAGE PROC, P4009, DOI 10.1109/ICIP.2010.5653339
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shao H, 2009, 3DTV CONF, P25, DOI 10.1109/3DTV.2009.5069619
   Shimizu S, 2007, IEEE T CIRC SYST VID, V17, P1485, DOI 10.1109/TCSVT.2007.903773
   Tanimoto M., 2008, JTC1SC29WG11 ISOIEC
   Tanimoto M, 2009, IEEE INT CON MULTI, P1552, DOI 10.1109/ICME.2009.5202803
   Umar AS, 2011, AFRICON
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wong KM, 2011, INT CONF ACOUST SPEE, P841
   Yamagishi K, 2011, INT WORK QUAL MULTIM, P37, DOI 10.1109/QoMEX.2011.6065709
   Yo-Sung Ho, 2007, 2007 14th International Workshop in Systems, Signals and Image Processing and 6th EURASIP Conference focused on Speech and Image Processing, Multimedia Communications and Services - EC-SIPMCS 2007, P5
   Zhang Y, 2010, INT CONF SIGN PROCES, P1044, DOI 10.1109/ICOSP.2010.5655900
   Zhu YH, 2009, 2009 ASIA-PACIFIC CONFERENCE ON INFORMATION PROCESSING (APCIP 2009), VOL 2, PROCEEDINGS, P31, DOI 10.1109/APCIP.2009.144
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 52
TC 10
Z9 10
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 3
SU S
AR 44
DI 10.1145/2348816.2348823
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021VG
UT WOS:000309912900007
DA 2024-07-18
ER

PT J
AU Shi, S
   Nahrstedt, K
   Campbell, R
AF Shi, Shu
   Nahrstedt, Klara
   Campbell, Roy
TI A Real-Time Remote Rendering System for Interactive Mobile Graphics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Measurement; Remote rendering; interaction latency; 3D image
   warping; mobile devices
ID 3D GRAPHICS; COMPRESSION
AB Mobile devices are gradually changing people's computing behaviors. However, due to the limitations of physical size and power consumption, they are not capable of delivering a 3D graphics rendering experience comparable to desktops. Many applications with intensive graphics rendering workloads are unable to run on mobile platforms directly. This issue can be addressed with the idea of remote rendering: the heavy 3D graphics rendering computation runs on a powerful server and the rendering results are transmitted to the mobile client for display. However, the simple remote rendering solution inevitably suffers from the large interaction latency caused by wireless networks, and is not acceptable for many applications that have very strict latency requirements.
   In this article, we present an advanced low-latency remote rendering system that assists mobile devices to render interactive 3D graphics in real-time. Our design takes advantage of an image based rendering technique: 3D image warping, to synthesize the mobile display from the depth images generated on the server. The research indicates that the system can successfully reduce the interaction latency while maintaining the high rendering quality by generating multiple depth images at the carefully selected viewpoints. We study the problem of viewpoint selection, propose a real-time reference viewpoint prediction algorithm, and evaluate the algorithm performance with real-device experiments.
C1 [Shi, Shu; Nahrstedt, Klara; Campbell, Roy] Univ Illinois, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Shi, S (corresponding author), Ricoh Innovat Inc, 2882 Sand Hill Rd,Suite 115, Menlo Pk, CA 94025 USA.
EM shushi@rii.ricoh.com; klara@illinois.edu; rhc@illinois.edu
RI Campbell, Roy/O-1141-2019
OI Campbell, Roy/0000-0002-3754-7777
FU National Science Foundation [CNS 05-20182, CNS 07-20702]
FX This work was supported by the National Science Foundation under Grant
   CNS 05-20182 and CNS 07-20702.
CR [Anonymous], THESIS U N CAROLINA
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Bao P, 2004, IEE P-VIS IMAGE SIGN, V151, P329, DOI 10.1049/ip-vis:20040749
   Baratto RicardoA., 2005, Proceedings of the 20th ACM Symposium on Operating Systems Principles, SOSP '05, P277, DOI DOI 10.1145/1095810.1095837
   Beigbeder T., 2004, P ACM NETGAMES, P144
   Boukerche A., 2006, ACM Multimedia, P691, DOI DOI 10.1145/1180639.1180785
   Chang CF, 2002, LECT NOTES COMPUT SC, V2532, P1105
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   Duguet F, 2004, IEEE COMPUT GRAPH, V24, P57, DOI 10.1109/MCG.2004.5
   Eisert P, 2008, IEEE IMAGE PROC, P2704, DOI 10.1109/ICIP.2008.4712352
   ENGEL K., 1999, IEEE VISUALIZ
   EPIC GAMES, 2007, UNR TOURN 3
   Karlton Phil., 2005, OPENGL GRAPHICS X WI, V1.4 edition
   Kum S.-U., 2005, ACM T MULTIM COMPUT, V1, P128, DOI DOI 10.1145/1062253.1062255
   Kurillo G, 2008, IEEE INT SYM MULTIM, P111, DOI 10.1109/ISM.2008.32
   Lamberti F, 2007, IEEE T VIS COMPUT GR, V13, P247, DOI 10.1109/TVCG.2007.29
   Levoy M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P21, DOI 10.1145/218380.218392
   Lien JM, 2007, LECT NOTES COMPUT SC, V4841, P714
   Marquez J., 2008, P WCITD 08
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   McMillan L, 1997, IMAGE BASED APPROACH
   Noimark Y, 2003, IEEE COMPUT GRAPH, V23, P58, DOI 10.1109/MCG.2003.1159614
   Ohazama C., 1999, OPENGL VIZSERVER
   Penta SK, 2005, VISUAL COMPUT, V21, P611, DOI 10.1007/s00371-005-0337-8
   Redert A, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P313
   Riva O., 2008, IEEE COMPUT, V41, P77
   SCANVIEW, 2003, SYSTEM REMOTE VISUAL
   Schmalstieg D., 1997, THESIS VIENNA U TECH
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Shi S, 2011, IEEE INT CON MULTI
   Shi Shu., 2011, Proceedings of the 19th ACM International Conference on Multimedia, P103, DOI [10.1145/2072298.2072313, DOI 10.1145/2072298.2072313]
   Singhal S., 1999, Networked Virtual Environments
   Smit F, 2009, IEEE VIRTUAL REALITY 2009, PROCEEDINGS, P27
   Zhu Minhui, 2011, P 19 ACM INT C MULT, P183, DOI DOI 10.1145/2072298.2072324
NR 34
TC 31
Z9 40
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 3
SU S
AR 46
DI 10.1145/2348816.2348825
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 021VG
UT WOS:000309912900009
DA 2024-07-18
ER

PT J
AU Dornaika, F
   Elder, JH
AF Dornaika, Fadi
   Elder, James H.
TI Image Registration for Foveated Panoramic Sensing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Foveated sensing; omnidirectional sensing;
   attention; matching; registration
AB This article addresses the problem of registering high-resolution, small field-of-view images with low-resolution panoramic images provided by a panoramic catadioptric video sensor. Such systems may find application in surveillance and telepresence systems that require a large field of view and high resolution at selected locations. Although image registration has been studied in more conventional applications, the problem of registering panoramic and conventional video has not previously been addressed, and this problem presents unique challenges due to (i) the extreme differences in resolution between the sensors (more than a 16: 1 linear resolution ratio in our application), and (ii) the resolution inhomogeneity of panoramic images. The main contributions of this article are as follows. First, we introduce our foveated panoramic sensor design. Second, we show how a coarse registration can be computed from the raw images using parametric template matching techniques. Third, we propose two refinement methods allowing automatic and near real-time registration between the two image streams. The first registration method is based on matching extracted interest points using a closed form method. The second registration method is featureless and based on minimizing the intensity discrepancy allowing the direct recovery of both the geometric and the photometric transforms. Fourth, a comparison between the two registration methods is carried out, which shows that the featureless method is superior in accuracy. Registration examples using the developed methods are presented.
C1 [Dornaika, Fadi] Univ Basque Country UPV EHU, Madrid, Spain.
   [Dornaika, Fadi] Basque Fdn Sci, IKERBASQUE, Madrid, Spain.
   York Univ, Ctr Vis Res, N York, ON M3J 1P3, Canada.
C3 University of Basque Country; Basque Foundation for Science; York
   University - Canada
RP Dornaika, F (corresponding author), Univ Basque Country, Manuel Lardizabal 1, San Sebastian 20018, Spain.
EM fadi_dornaika@ehu.es; jelder@yorku.ca
CR AMINTABAR A., 2008, P IEEE INT C IM PROC
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 1988, P ALV VIS C MANCH UK
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Boult TE, 2004, IMAGE VISION COMPUT, V22, P515, DOI 10.1016/j.imavis.2003.09.005
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chen JH, 2003, IEEE T SIGNAL PROCES, V51, P230, DOI 10.1109/TSP.2002.806551
   CONROY T. L., 1999, P IEEE C COMP VIS
   DANILIDIS K., 2000, P IEEE INT C PATT RE
   DORNAIKA F., 2002, LECT NOTES COMPUTER, V2353
   DUFOURNEAU Y., 2000, P IEEE C COMP VIS PA
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fletcher R., 1990, PRACTICAL METHODS OP
   HE Q., 2006, LECT NOTES COMPUTER, V4291
   KANATANI K., 1999, P IEEE C COMP VIS
   Kim DH, 2003, PATTERN RECOGN LETT, V24, P2421, DOI 10.1016/S0167-8655(03)00071-0
   Lin SS, 2006, IEEE T PATTERN ANAL, V28, P840, DOI 10.1109/TPAMI.2006.106
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ohta J., 2007, Smart CMOS Image Sensors and Applications
   Peng G. X., 2005, International Journal of Intelligent Systems Technologies and Applications, V1, P18, DOI 10.1504/IJISTA.2005.007305
   PILU M, 1997, P IEEE C COMP VIS PA
   Prime SL, 2007, EXP BRAIN RES, V180, P609, DOI 10.1007/s00221-007-0885-4
   Scaramuzza D, 2007, PRACTICAL TOOLBOX CA, DOI [10.3929/ethz-a-005657929, DOI 10.3929/ETHZ-A-005657929]
   Spacek L, 2007, ROBOT AUTON SYST, V55, P667, DOI 10.1016/j.robot.2007.05.009
   Swaminathan R, 2006, INT J COMPUT VISION, V66, P211, DOI 10.1007/s11263-005-3220-1
   TANAKA K., 2000, P IEEE C COMP VIS PA
   TORRE F., 2005, P IEEE INT C ROB AUT
   Traver VJ, 2010, ROBOT AUTON SYST, V58, P378, DOI 10.1016/j.robot.2009.10.002
   Wu YT, 2000, INT J COMPUT VISION, V38, P129, DOI 10.1023/A:1008101718719
   Zitová B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 30
TC 5
Z9 5
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2012
VL 8
IS 2
AR 17
DI 10.1145/2168996.2168997
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 945BX
UT WOS:000304248900001
DA 2024-07-18
ER

PT J
AU Lin, YR
   Candan, KS
   Sundaram, H
   Xie, LX
AF Lin, Yu-Ru
   Candan, K. Selcuk
   Sundaram, Hari
   Xie, Lexing
TI SCENT: Scalable Compressed Monitoring of Evolving Multirelational Social
   Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Measurement; Algorithms; Human Factors; Social media;
   social network analysis; stream mining; multirelational learning; tensor
   analysis
AB We propose SCENT, an innovative, scalable spectral analysis framework for internet scale monitoring of multirelational social media data, encoded in the form of tensor streams. In particular, a significant challenge is to detect key changes in the social media data, which could reflect important events in the real world, sufficiently quickly. Social media data have three challenging characteristics. First, data sizes are enormous; recent technological advances allow hundreds of millions of users to create and share content within online social networks. Second, social data are often multifaceted (i.e., have many dimensions of potential interest, from the textual content to user metadata). Finally, the data is dynamic; structural changes can occur at multiple time scales and be localized to a subset of users. Consequently, a framework for extracting useful information from social media data needs to scale with data volume, and also with the number and diversity of the facets of the data. In SCENT, we focus on the computational cost of structural change detection in tensor streams. We extend compressed sensing (CS) to tensor data. We show that, through the use of randomized tensor ensembles, SCENT is able to encode the observed tensor streams in the form of compact descriptors. We show that the descriptors allow very fast detection of significant spectral changes in the tensor stream, which also reduce data collection, storage, and processing costs. Experiments over synthetic and real data show that SCENT is faster (17.7x- 159x for change detection) and more accurate (above 0.9 F-score) than baseline methods.
C1 [Lin, Yu-Ru; Candan, K. Selcuk; Sundaram, Hari] Arizona State Univ, Tempe, AZ 85281 USA.
   [Xie, Lexing] IBM TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
C3 Arizona State University; Arizona State University-Tempe; International
   Business Machines (IBM)
RP Lin, YR (corresponding author), Arizona State Univ, Tempe, AZ 85281 USA.
EM yu-ru.lin@asu.edu
OI Xie, Lexing/0000-0001-8319-0118
CR AGGARWAL C, 2003, P ACM SIGMOD INT C M, P586
   Aggarwal CC, 2005, SIAM PROC S, P56
   Allan J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P37, DOI 10.1145/290941.290954
   [Anonymous], 1996, MATRIX COMPUTATION
   [Anonymous], 2006, P 12 ACM SIGKDD INT, DOI DOI 10.1145/1150402.1150445
   [Anonymous], 2006, Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P424, DOI [DOI 10.1145/1150402.1150450, 10.1145/1150402.1150450]
   [Anonymous], 2003, P 26 ANN INT ACM SIG, DOI DOI 10.1145/860435.860495
   [Anonymous], 2010, P 2010 SIAM INT C DA
   [Anonymous], P AAAI FALL S US UNC
   BAJWA W, 2006, P 5 INT C INF PROC S, P142
   BALASUBRAMANYAN R, 2009, P 3 INT AAAI C WEBL
   Barabási AL, 2002, PHYSICA A, V311, P590, DOI 10.1016/S0378-4371(02)00736-7
   Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x
   BECKER H, 2009, P ACM SIGMOD INT C M
   BLEI D., 2006, ICML, P120
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   Candès E, 2007, INVERSE PROBL, V23, P969, DOI 10.1088/0266-5611/23/3/008
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   CASTRO P., 2009, P ACM SIGKDD INT C K
   Chen KK, 2009, VLDB J, V18, P1241, DOI 10.1007/s00778-009-0134-5
   Chi Y., 2006, CIKM, P68
   Chierichetti F, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P219
   Cormode G, 2009, COMMUN ACM, V52, P97, DOI 10.1145/1562764.1562789
   Dasgupta D., 1996, P INT C INT SYST
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   DOMINGOS P, 2000, P 6 ACM SIGKDD INT C, P80
   Drineas P, 2007, LINEAR ALGEBRA APPL, V420, P553, DOI 10.1016/j.laa.2006.08.023
   Harshman R.A., 1970, UCLA Working Papers in Phonetics, V16, P84, DOI DOI 10.1134/S0036023613040165
   Haupt J, 2008, IEEE SIGNAL PROC MAG, V25, P92, DOI 10.1109/MSP.2007.914732
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kolda TG, 2008, IEEE DATA MINING, P363, DOI 10.1109/ICDM.2008.89
   KUMARAN G, 2004, P ANN INT ACM SIGIR, P304
   Leskovec J., 2005, P ACM SIGKDD INT C K
   Leskovec J., 2006, P INT ACM SIGKDD C K, P631, DOI DOI 10.1145/1150402.1150479
   Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   SUNDARAM H, 2008, P INT WORLD WID WEB
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Wang Haixun, 2003, ACM SIGKDD, P226, DOI DOI 10.1145/956750.956778
   Yang Y., 1998, P 21 ANN INT ACM SIG, P28, DOI DOI 10.1145/290941.290953
NR 45
TC 2
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 29
DI 10.1145/2037676.2037686
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 857NR
UT WOS:000297725800010
DA 2024-07-18
ER

PT J
AU Tan, SL
   Bu, JJ
   Chen, C
   Xu, B
   Wang, C
   He, XF
AF Tan, Shulong
   Bu, Jiajun
   Chen, Chun
   Xu, Bin
   Wang, Can
   He, Xiaofei
TI Using Rich Social Media Information for Music Recommendation via
   Hypergraph Model
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Recommender system; music recommendation;
   hypergraph; social media information
AB There are various kinds of social media information, including different types of objects and relations among these objects, in music social communities such as Last. fm and Pandora. This information is valuable for music recommendation. However, there are two main challenges to exploit this rich social media information: (a) There are many different types of objects and relations in music social communities, which makes it difficult to develop a unified framework taking into account all objects and relations. (b) In these communities, some relations are much more sophisticated than pairwise relation, and thus cannot be simply modeled by a graph. We propose a novel music recommendation algorithm by using both multiple kinds of social media information and music acoustic-based content. Instead of graph, we use hypergraph to model the various objects and relations, and consider music recommendation as a ranking problem on this hypergraph. While an edge of an ordinary graph connects only two objects, a hyperedge represents a set of objects. In this way, hypergraph can be naturally used to model high-order relations.
C1 [Tan, Shulong; Bu, Jiajun; Chen, Chun; Xu, Bin; Wang, Can] Zhejiang Univ, Zhejiang Key Lab Serv Robot, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   [He, Xiaofei] Zhejiang Univ, State Key Lab CAD&CG, Coll Comp Sci, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Tan, SL (corresponding author), Zhejiang Univ, Zhejiang Key Lab Serv Robot, Coll Comp Sci, Hangzhou 310027, Peoples R China.
EM shulongtan@zju.edu.cn; bjj@zju.edu.cn; chenc@zju.edu.cn;
   xbzju@zju.edu.cn; wcan@zju.edu.cn; xiaofeihe@cad.zju.edu.cn
RI WANG, CAN/GWV-0969-2022
FU China National Key Technology RD Program [2008BAH26B00, 2007BAH11B06];
   National Natural Science Foundation of China [60875044, 90920303]
FX This work was supported by the China National Key Technology R&D Program
   (2008BAH26B00, 2007BAH11B06) and the National Natural Science Foundation
   of China (60875044, 90920303).
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   Agarwal S., 2006, P 23 INT C MACH LEAR, P17, DOI DOI 10.1145/1143844.1143847
   [Anonymous], 2009, ADV NEURAL INF PROC
   [Anonymous], 2008, P ACM S APPL COMP
   [Anonymous], 2008, P 9 INT C MUS INF RE
   [Anonymous], 2003, ADV NEURAL INF PROC
   [Anonymous], 2007, P ACM C REC SYST
   [Anonymous], 2008, P 31 ACM SIGIR C RES
   [Anonymous], 1998, P 14 C UNC ART INT
   [Anonymous], 2008, P 16 ACM INT C MULT
   [Anonymous], 2009, P 32 ACM SIGIR C RES
   [Anonymous], 2001, ADV NEURAL INF PROC
   [Anonymous], 2007, P 15 INT C MULT
   [Anonymous], 2002, P IEEE INT C MULT EX
   [Anonymous], 1999, P 22 ACM SIGIR C RES
   [Anonymous], 2006, INT C MUS INF RETR
   [Anonymous], 2009, P 32 ACM SIGIR C RES
   [Anonymous], 2009, P 10 INT SOC MUS INF
   [Anonymous], 2004, P 12 ACM INT C MULT
   [Anonymous], 2010, P 18 ACM INT C MULT
   [Anonymous], 2008, P 31 ACM SIGIR C RES
   [Anonymous], 2005, P 7 ACM SIGMM WORKSH
   [Anonymous], 2007, P ACM C REC SYST
   [Anonymous], 2006, P 7 INT C MUS INF RE
   [Anonymous], 2006, P 8 ACM INT WORKSH M
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2009, P 32 ACM SIGIR C RES
   [Anonymous], 2005, P 13 ACM INT C MULT
   [Anonymous], 2006, ADV NEURAL INF PROC
   [Anonymous], 2006, P 5 INT SEM WEB C
   [Anonymous], 2004, P 5 INT C MUS INF RE
   [Anonymous], 2008, P 31 ACM SIGIR C RES
   [Anonymous], 2007, P 7 IEEE INT C DAT M
   [Anonymous], 2008, P 9 INT C MUS INF RE
   Berenzweig A, 2004, COMPUT MUSIC J, V28, P63, DOI 10.1162/014892604323112257
   Diederich J, 2006, P 1 INT WORKSH BUILD
   Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494
   Guan Z., 2010, Proceedings of the 19th international conference on World wide web, P391
   Konstan J., 2001, P 10 INT C WORLD WID, P285
   Li Q, 2007, INFORM PROCESS MANAG, V43, P473, DOI 10.1016/j.ipm.2006.07.005
   Lin YR, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P527
   Logan B., 2001, P IEEE INT C MULT EX, P745
   Lovasz L., 1993, Combinatorics, Paul Erdos is Eighty, V2, P1
   Resnick P., 1994, P ACM C COMP SUPP CO, P175
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sen S., 2009, Proceedings of the 18th international conference on World wide web, P671
   Sun L., 2008, P 14 ACM SIGKDD INT
   Svizhenko A, 2002, J APPL PHYS, V91, P2343, DOI 10.1063/1.1432117
   Zhang ZK, 2010, PHYSICA A, V389, P179, DOI 10.1016/j.physa.2009.08.036
NR 49
TC 54
Z9 60
U1 2
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 22
DI 10.1145/2037676.2037679
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 857NR
UT WOS:000297725800003
DA 2024-07-18
ER

PT J
AU Bouyakoub, S
   Belkhir, A
AF Bouyakoub, Samia
   Belkhir, Abdelkader
TI SMIL Builder: An Incremental Authoring Tool for SMIL Documents
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Reliability; Languages; Theory; Verification; Authoring; SMIL;
   multimedia; Petri nets; modeling; verification
AB We present in this article a temporal SMIL editor with incremental verification capabilities, based on a formal Petri Net-based model. Our authoring tool, named SMIL Builder, allows the author to "build" his document step by step, while insuring at every stage the validity of the current state of the document. These incremental authoring and consistency checking features are based on the H-SMIL-Net model (Hierarchical SMIL Petri Net), a temporal extension of Petri Nets. Our aim is to propose an easy-to-use temporal environment which can satisfy a wide range of users; so we opted for an interface combining simplicity and ergonomics.
C1 [Bouyakoub, Samia; Belkhir, Abdelkader] USTHB Univ, Fac Elect & Comp Sci, Algiers, Algeria.
C3 University Science & Technology Houari Boumediene
RP Bouyakoub, S (corresponding author), USTHB Univ, Fac Elect & Comp Sci, Algiers, Algeria.
EM bouyakoub.s@gmail.com
RI abdelkader, belkhir/P-6367-2016
OI abdelkader, belkhir/0000-0002-1813-3384
CR [Anonymous], P 10 INT C COMP MOD
   [Anonymous], 2008, SMIL 3 0 FLEXIBLE MU
   [Anonymous], P 21 ANN INT C DOC S
   [Anonymous], THESIS MIT
   [Anonymous], SYNCHRONIZED MULTIME
   Ayars Jeff., 2001, Synchronized Multimedia Integration Language (SMIL 2.0)
   Bulterman D, 2005, SYNCHRONIZED MULTIME
   Bulterman Dick., 2008, SYNCHRONIZED MULTIME, p3.0
   Bulterman DCA, 2005, ACM T MULTIM COMPUT, V1, P89, DOI 10.1145/1047936.1047943
   Chung SM, 2003, ITCC 2003: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: COMPUTERS AND COMMUNICATIONS, PROCEEDINGS, P711, DOI 10.1109/ITCC.2003.1197618
   Hoschka P., 1998, SYNCHRONIZED MULTIME
   Jourdan M, 2000, MULTIMED TOOLS APPL, V12, P257, DOI 10.1023/A:1009675809463
   Senac P, 1996, IEEE J SEL AREA COMM, V14, P84, DOI 10.1109/49.481696
   P PACK VID C
   P ACM S DOC ENG
   SMIL AUTHOR AUTHORIN
   P ACM S DOC ENG
   WORLD WIDE WEB J
   J BRAZIL COMPUT SOC
   P INT COMP S WORKSH
   P 7 INT C WORLD WID
   P 4 INT S MULT SOFTW
   P IEEE INT C MULT IC
   P SMIL EUR C
   P 15 INT C MULT
   IEEE J SELECT AREAS
   P 3 INT C WEB INF SY
NR 27
TC 14
Z9 14
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2011
VL 7
IS 1
AR 2
DI 10.1145/1870121.1870123
PG 30
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 712GI
UT WOS:000286653800002
DA 2024-07-18
ER

PT J
AU Money, AG
   Agius, H
AF Money, Arthur G.
   Agius, Harry
TI ELVIS: Entertainment-Led Video Summaries
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Human Factors; Video summarization; video content;
   semantics; personalization; physiological response; affect; emotion
ID RESPIRATORY RESPONSES; PICTURE; MOTION; DIMENSIONS; RETRIEVAL; EMOTION;
   AROUSAL
AB Video summaries present the user with a condensed and succinct representation of the content of a video stream. Usually this is achieved by attaching degrees of importance to low-level image, audio and text features. However, video content elicits strong and measurable physiological responses in the user, which are potentially rich indicators of what video content is memorable to or emotionally engaging for an individual user. This article proposes a technique that exploits such physiological responses to a given video stream by a given user to produce Entertainment-Led VIdeo Summaries (ELVIS). ELVIS is made up of five analysis phases which correspond to the analyses of five physiological response measures: electro-dermal response (EDR), heart rate (HR), blood volume pulse (BVP), respiration rate (RR), and respiration amplitude (RA). Through these analyses, the temporal locations of the most entertaining video subsegments, as they occur within the video stream as a whole, are automatically identified. The effectiveness of the ELVIS technique is verified through a statistical analysis of data collected during a set of user trials. Our results show that ELVIS is more consistent than RANDOM, EDR, HR, BVP, RR and RA selections in identifying the most entertaining video subsegments for content in the comedy, horror/comedy, and horror genres. Subjective user reports also reveal that ELVIS video summaries are comparatively easy to understand, enjoyable, and informative.
C1 [Money, Arthur G.] Brunel Univ, Sch Informat Syst, Uxbridge UB8 3PH, Middx, England.
   [Agius, Harry] Brunel Univ, Sch Engn & Design, Uxbridge UB8 3PH, Middx, England.
C3 Brunel University; Brunel University
RP Money, AG (corresponding author), Brunel Univ, Sch Informat Syst, Uxbridge UB8 3PH, Middx, England.
EM arthur.money@brunel.ac.uk; harry.agius@brunel.ac.uk
OI Money, Arthur/0000-0003-1063-3680
CR AGIUS H, 2008, ENCY MULTIMEDIA, P204
   Aizawa Kiyoharu., 2004, CARPE 04 P THE 1 ACM, P22, DOI DOI 10.1145/1026653.1026656
   Allanson J, 2004, INTERACT COMPUT, V16, P857, DOI 10.1016/j.intcom.2004.08.001
   Amenabar Alejandro., 2001, The Others
   Athanasiadis T, 2007, IEEE T CIRC SYST VID, V17, P298, DOI 10.1109/TCSVT.2007.890636
   Babaguchi N, 2004, IEEE T MULTIMEDIA, V6, P575, DOI [10.1109/TMM.2004.830811, 10.1109/tmm.2004.830811]
   BABAGUCHI N, 2001, P IEEE INT C MULT EX, P800
   BAILER W, 2007, P ACM INT WORKSH TRE, P60
   Barbieri M, 2003, PROC SPIE, V5242, P1, DOI 10.1117/12.515733
   BROWN WA, 1977, AM J PSYCHIAT, V134, P930
   Cacioppo J.T., 1997, Annual Review of Gerontology and Geriatrics, V17, P27
   CACIOPPO JT, 2007, HDB PSYCHPHYSIOGY
   Carlson N., 2001, PSYCHOL BEHAV, V7th
   Christie IC, 2004, INT J PSYCHOPHYSIOL, V51, P143, DOI 10.1016/j.ijpsycho.2003.08.002
   Clark-Carter D., 1997, Doing Quantitative Psychological Research: From Design to Report
   DAMNJANOVIC U, 2007, P 2 INT C SEM DIG ME, P99
   Davidson Richard J., 1995, P361
   de Silva G. C., 2005, 13th Annual ACM International Conference on Multimedia, P820, DOI 10.1145/1101149.1101329
   Detenber BH, 1998, J BROADCAST ELECTRON, V42, P113
   DETYNIECKI M, 2007, P INT WORKSH TRECVID, P65
   deWied M, 1997, COGNITION EMOTION, V11, P481, DOI 10.1080/026999397379890
   EKMAN P, 1983, SCIENCE, V221, P1208, DOI 10.1126/science.6612338
   Frazier TW, 2004, PSYCHOPHYSIOLOGY, V41, P75, DOI 10.1046/j.1469-8986.2003.00131.x
   Fridja NicoH., 1986, EMOTIONS
   Furini M, 2006, CONSUM COMM NETWORK, P1209
   Gleitman Henry., 2007, Psychology, V7th
   Gomez P, 2004, BIOL PSYCHOL, V67, P359, DOI 10.1016/j.biopsycho.2004.03.013
   Gomez P, 2004, INT J PSYCHOPHYSIOL, V53, P91, DOI 10.1016/j.ijpsycho.2004.02.002
   Greenwald M. K., 1989, Journal of Psychophysiology, V3, P51
   GROSS JJ, 1995, COGNITION EMOTION, V9, P87, DOI 10.1080/02699939508408966
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   Hanjalic A, 2003, IEEE IMAGE PROC, P1
   Healey J. A., 2000, THESIS MIT CAMBRIDGE
   Jaimes A, 2002, IEEE IMAGE PROC, P133
   Jonghwa Kim, 2008, 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI 2008), P114, DOI 10.1109/MFI.2008.4648119
   Jung B, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1230812.1230817
   KAWAI Y, 2007, P ACM INT C IM VID R, P49
   Komlodi A., 2004, P CHI 04 EXTENDED AB
   KRAMER AF, 1991, MULTIPLE TASK PERFOR, P329
   LANG A, 1995, J BROADCAST ELECTRON, V39, P313, DOI 10.1080/08838159509364309
   Lang A, 1999, J BROADCAST ELECTRON, V43, P451
   LEE LL, 2008, P 10 ACM INT C UB CO, P44
   LEONHARDT S, 2007, P 4 INT WORKSH WEAR
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li Y, 2006, IEEE SIGNAL PROC MAG, V23, P79
   Lie WN, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P413, DOI 10.1109/ICME.2008.4607459
   MCINTYRE G, 2007, LECT NOTES COMPUTER, V4868
   MILLET C, 2005, P 2 EUR WORKSH INT K, P119
   MONEY A, 2007, JOINT P 2005 2006 20, P142
   MONEY AG, 2008, LECT NOTES COMPUTER, V4868
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Money AG, 2005, PROCEEDINGS OF THE NINTH IASTED INTERNATIONAL CONFERENCE ON INTERNET AND MULTIMEDIA SYSTEMS AND APPLICATIONS, P436
   MORIYAMA T, 2002, SYSTEMS COMPUTERS JA, V33, P1122
   Morrone-Strupinsky JV, 2004, PERS INDIV DIFFER, V36, P1109, DOI 10.1016/S0191-8869(03)00204-6
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Nasoz F., 2003, International Journal of Cognition, Technology, and Work, V6, P1, DOI [10.1007/s10111-003-0143-x, DOI 10.1007/S10111-003-0143-X]
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   OVER P, 2007, P TVS TRECVID BBC RU
   Palomba D., 1993, The structure of emotion, P158
   Philippot P, 2002, COGNITION EMOTION, V16, P605, DOI 10.1080/02699930143000392
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Picard Rosalind W., 1995, Affective Computing
   Piferi RL, 2000, INT J PSYCHOPHYSIOL, V37, P207, DOI 10.1016/S0167-8760(00)00102-1
   POWER M, 1998, COGNITION EMOTION OR
   RIKKARD NS, 2004, PSYCH MUSIC, V32, P371
   Rilling J, 2007, 4TH IEEE INTERNATIONAL WORKSHOP ON VISUALIZING SOFTWARE FOR UNDERSTANDING AND ANALYSIS, PROCEEDINGS, P10
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Rui Y, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P735, DOI 10.1109/MMCS.1999.778576
   Scheirer J, 2002, INTERACT COMPUT, V14, P93, DOI 10.1016/S0953-5438(01)00059-5
   SENE M, 2005, P SPIE C INT IM
   SHIPMAN S., 2007, Proc. IEEE Int. Conf. Consumer Electronic, P1
   Simon HA., 1982, Affect and cognition: The seventeenth annual Carnegie symposium on cognition, P333
   Simons RF, 2000, PSYCHOPHYSIOLOGY, V37, P706, DOI 10.1017/S004857720000158X
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SPIERS B, 1979, FAWLTY TOWERS 2
   Steinbeis N, 2006, J COGNITIVE NEUROSCI, V18, P1380, DOI 10.1162/jocn.2006.18.8.1380
   SUZIKI J, 2004, INT J PSYCHOPHYSIOL, V55, P35
   Takahashi Y, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P1171
   Tjondronegoro D., 2003, Proceedings of the Fifth ACM SIGMM International Workshop on Multimedia Information Retrieval, P201, DOI [10.1145/973264.973296, DOI 10.1145/973264.973296]
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Van Diest I, 2001, PSYCHOPHYSIOLOGY, V38, P961
   van Reekum CM, 2004, COGNITION EMOTION, V18, P663, DOI 10.1080/02699930341000167
   WANG T, 2007, P INT WORKSH TRECVID, P79
   WINTON WM, 1984, J EXP SOC PSYCHOL, V20, P195, DOI 10.1016/0022-1031(84)90047-7
   Wright E, 2004, Shaun of the Dead
   Xu C., 2006, PROC 14 ANN ACM INT, P221, DOI DOI 10.1145/1180639.1180699
NR 86
TC 22
Z9 23
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2010
VL 6
IS 3
SI SI
AR 17
DI 10.1145/1823746.1823751
PG 30
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 649UL
UT WOS:000281799300006
OA Green Published
DA 2024-07-18
ER

PT J
AU Liu, XT
   Corner, M
   Shenoy, P
AF Liu, Xiaotao
   Corner, Mark
   Shenoy, Prashant
TI <i>SEVA</i>: Sensor-Enhanced Video Annotation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Video annotation; sensor-enhanced;
   location-based services; context-based retrieval
AB In this article, we study how a sensor-rich world can be exploited by digital recording devices such as cameras and camcorders to improve a user's ability to search through a large repository of image and video files. We design and implement a digital recording system that records identities and locations of objects (as advertised by their sensors) along with visual images (as recorded by a camera). The process, which we refer to as Sensor-Enhanced Video Annotation (SEVA), combines a series of correlation, interpolation, and extrapolation techniques. It produces a tagged stream that later can be used to efficiently search for videos or frames containing particular objects or people. We present detailed experiments with a prototype of our system using both stationary and mobile objects as well as GPS and ultrasound. Our experiments show that: (i) SEVA has zero error rates for static objects, except very close to the boundary of the viewable area; (ii) for moving objects or a moving camera, SEVA only misses objects leaving or entering the viewable area by 1-2 frames; (iii) SEVA can scale to 10 fast-moving objects using current sensor technology; and (iv) SEVA runs online using relatively inexpensive hardware.
C1 [Liu, Xiaotao; Corner, Mark; Shenoy, Prashant] Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
C3 University of Massachusetts System; University of Massachusetts Amherst
RP Shenoy, P (corresponding author), Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
EM shenoy@cs.umass.edu
OI Shenoy, Prashant/0000-0002-5435-1901
FU NSF [CCR-0219520, EIA-0080119, CNS-0520729, CNS-0519881, CNS-0447877,
   DUE-0416863]
FX This work was supported in part by NSF grants CCR-0219520, EIA-0080119,
   CNS-0520729, CNS-0519881, CNS-0447877 and DUE-0416863.
CR Adams B., 2006, MULTIMEDIA '06, P987, DOI DOI 10.1145/1180639.1180857
   Ahern S, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P357
   Aizawa Kiyoharu., 2004, CARPE 04 P THE 1 ACM, P22, DOI DOI 10.1145/1026653.1026656
   [Anonymous], P 4 INT C INF PROC S
   [Anonymous], 2002, Introduction to MPEG- 7: Multimedia content description interface
   [Anonymous], 2004, Proceedings of the 2nd International Conference on Mobile Systems, Applications, and Services
   [Anonymous], 2004, Proceedings of the 2nd International Conference on Mobile Systems, Applications, and Services (MobiSys)
   [Anonymous], 2004, MULTIMEDIA'04, DOI [10.1145/1027527.1027748, DOI 10.1145/1027527.1027748]
   [Anonymous], P 12 ANN ACM INT C M
   [Anonymous], P ACM MULT
   [Anonymous], 2002, WSNA '02
   Bahl P., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P775, DOI 10.1109/INFCOM.2000.832252
   Bajaj R, 2002, COMPUTER, V35, P92, DOI 10.1109/MC.2002.993780
   BARRY B, 2005, THESIS MIT
   Davis Marc., 2004, P 12 ANN ACM INT C M, P188, DOI DOI 10.1145/1027527.1027572
   Devore J.L., 1999, PROBABILITY STAT ENG, V5th
   Dourish P, 2004, PERS UBIQUIT COMPUT, V8, P19, DOI 10.1007/s00779-003-0253-8
   Ellis D. R., 2004, Proceedings of a Workshop on Equine Recurrent Laryngeal Neuropathy, Stratford-upon-Avon, UK, 7-10 September, 2003, P39, DOI 10.1145/1026653.1026659
   Fan Jianping., 2004, ACM International Conference on Multimedia, P540, DOI [DOI 10.1145/1027527, DOI 10.1145/1027527.1027660]
   Finkenzeller K., 2003, RFID Handbook: Fundamentals and Applications in Contactless Smart Cards and Identification
   Gemmell J., 2004, P THE 1 ACM WORKSHOP, P48
   Gemmell J., 2002, P 10 ACM INT C MULTI, P235, DOI DOI 10.1145/641007.641053
   *GEOC, FIND LAT LONG AN US
   *GPSDRIV, GPSDRIV 2 09
   GRIMM R, 2002, THESIS U WASHINGTON
   Hähnel D, 2004, IEEE INT CONF ROBOT, P1015, DOI 10.1109/ROBOT.2004.1307283
   Harter A., 1999, Proceedings of the 5th Annual ACM/IEEE International Conference on Mobile Computing and Networking. MobiCom'99, P59, DOI DOI 10.1145/313451.313476
   HIGHTOWER J, 2000, 000202 U WASH
   HIGHTOWER J, 2001, IEEE COMPUT, V34, P8
   Hill JL, 2002, IEEE MICRO, V22, P12, DOI 10.1109/MM.2002.1134340
   Jin R., 2004, Proceedings of the 12th annual ACM international conference on Multimedia, P892, DOI DOI 10.1145/1027527.1027732
   JOHANSON B, 2002, IEEE PERVAS COMPUT, V1, P2
   KINDBERG T, 2002, MOBILE NETW, V7, P5
   LIU X, 2006, P 8 INT C UB COMP UB
   Liu Xiaotao., 2005, Proceedings of the ACM International Conference on Multimedia (ACM MM), P618
   LYMBEROPOULOS D, 2005, P INF PROC SENS NETW
   MEALLING M, 2003, AUTOID OBJE IN PRESS
   Naaman M, 2003, LECT NOTES COMPUT SC, V2888, P196
   Naaman M., 2004, P 12 ANN ACM INT C M, P196
   Nack F, 2004, MULTIMED TOOLS APPL, V22, P263, DOI 10.1023/B:MTAP.0000017031.26875.f7
   Ni LM, 2003, PROCEEDINGS OF THE FIRST IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS (PERCOM 2003), P407, DOI 10.1109/PERCOM.2003.1192765
   PRIYANTHA N., 2000, P 6 ANN INT C MOB CO, P32, DOI [10.1145/345910.345917, DOI 10.1145/345910.345917]
   ROMAN M, 2002, ECOOP WORKSH OBJ OR
   Simon D., 2006, OPTIMAL STATE ESTIMA, DOI [10.1002/0470045345, DOI 10.1002/0470045345.CH11]
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Su NM, 2004, SECOND IEEE ANNUAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS, PROCEEDINGS, P3, DOI 10.1109/PERCOM.2004.1276840
   Toyama Kentaro., 2003, P 11 ACM INT C MULTI, P156
   WANT R, 1992, ACM T INFORM SYST, V10, P91, DOI 10.1145/128756.128759
   Zhang L., 2004, MULTIMEDIA 04, P716
NR 49
TC 3
Z9 3
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2009
VL 5
IS 3
AR 24
DI 10.1145/1556134.1556141
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 504DV
UT WOS:000270595600007
DA 2024-07-18
ER

PT J
AU Qi, GJ
   Hua, XS
   Rui, Y
   Tang, JH
   Mei, T
   Wang, M
   Zhang, HJ
AF Qi, Guo-Jun
   Hua, Xian-Sheng
   Rui, Yong
   Tang, Jinhui
   Mei, Tao
   Wang, Meng
   Zhang, Hong-Jiang
TI Correlative Multilabel Video Annotation with Temporal Kernels
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT ACM Multimedia Conference 2007
CY SEP 24-29, 2007
CL Augsburg, GERMANY
SP ACM
DE Algorithms; Theory; Experimentation; Video annotation; multilabeling;
   concept correlation; temporal kernel
AB Automatic video annotation is an important ingredient for semantic-level video browsing, search and navigation. Much attention has been paid to this topic in recent years. These researches have evolved through two paradigms. In the first paradigm, each concept is individually annotated by a pre-trained binary classifier. However, this method ignores the rich information between the video concepts and only achieves limited success. Evolved from the first paradigm, the methods in the second paradigm add an extra step on the top of the first individual classifiers to fuse the multiple detections of the concepts. However, the performance of these methods can be degraded by the error propagation incurred in the first step to the second fusion one. In this article, another paradigm of the video annotation method is proposed to address these problems. It simultaneously annotates the concepts as well as model correlations between them in one step by the proposed Correlative Multilabel (CML) method, which benefits from the compensation of complementary information between different labels. Furthermore, since the video clips are composed by temporally ordered frame sequences, we extend the proposed method to exploit the rich temporal information in the videos. Specifically, a temporal-kernel is incorporated into the CML method based on the discriminative information between Hidden Markov Models (HMMs) that are learned from the videos. We compare the performance between the proposed approach and the state-of-the-art approaches in the first and second paradigms on the widely used TRECVID data set. As to be shown, superior performance of the proposed method is gained.
C1 [Qi, Guo-Jun] Univ Sci & Technol China, Dept Automat, Hefei 230027, Anhui, Peoples R China.
   [Tang, Jinhui; Wang, Meng] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Anhui, Peoples R China.
   [Hua, Xian-Sheng; Rui, Yong; Mei, Tao; Zhang, Hong-Jiang] Microsoft Corp, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Microsoft
RP Qi, GJ (corresponding author), Univ Sci & Technol China, Dept Automat, 96 Jinzhai Rd, Hefei 230027, Anhui, Peoples R China.
EM qgj@mail.ustc.edu.cn; xshua@microsoft.com; yongrui@microsoft.com;
   jhtang@mail.ustc.edu.cn; tmei@microsoft.com; wang-meng@mail.ustc.edu.cn;
   hjzhang@microsoft.com
RI Mei, Tao/GQZ-0596-2022; Qi, Guo-Jun/AAH-8294-2019; Tang,
   Jinhui/KBR-0891-2024
OI Mei, Tao/0000-0002-5990-7307; Qi, Guo-Jun/0000-0003-3508-1851
CR [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], RC23612W0505104 IBM
   [Anonymous], 22220068 COL U ADVEN
   [Anonymous], ACM INT C IM VID
   [Anonymous], P IEEE C COMP VIS PA
   Berg BA, 2005, LECT NOTES SER INST, V7, P1
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   CAMPBELL M, 2006, TREC VID RETR EV TRE
   CHANG SF, 2006, TREC VID RETR EV TRE
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Do MN, 2003, IEEE SIGNAL PROC LET, V10, P115, DOI 10.1109/LSP.2003.809034
   EBADOLLAHI S, 2006, P IEEE INT C MULT EX
   Gauvain JL, 1994, IEEE T SPEECH AUDI P, V2, P291, DOI 10.1109/89.279278
   GODBOLE S, 2004, P PAC AS C ADV KNOWL
   GOLDBERGER J, 2005, P INT C SPOK LANG PR
   HAUPTMANN AG, 2004, TREC VID RETR EV TRE
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   HUA XS, 2006, TREC VID RETR EV TRE
   JIANG W, 2006, P IEEE INT C IM PROC
   Koskela M, 2007, IEEE T MULTIMEDIA, V9, P912, DOI 10.1109/TMM.2007.900137
   KUMAR S, 2003, P IEEE INT C MACH LE
   Lafferty John, 2001, INT C MACH LEARN ICM
   LIU P, 2007, P IEEE INT C AC SPEE
   Marr D., 1982, Vision
   McCallum A, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P662
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   NAPHADE MR, 2002, P IEEE WORKSH MULT S
   NAPHADE MR, 2002, IEEE T CSVT, V12
   PETERSOHN C, 2004, TREC VID RETR EV TRE
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   SMITH JR, 2003, P IEEE INT C MULT EX
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   TANG J, 2007, P ACM INT C MULT
   Thomas T. M.CoverandJ. A., 1991, ELEMENTS INFORM THEO, DOI 10.1002/0471200611
   Tsochantaridis I., 2004, ICML, P104
   WANG D, 2007, P ACM C MULT INF RET
   WANG T, 2006, P IEEE COMP VIS PATT
   Winkler G., 1995, IMAGE ANAL RANDOM FI
   Wu Y, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1003, DOI 10.1109/ICME.2004.1394372
   XIE L, 2002, P IEEE INT C AC SPEE
   YAN R, 2006, P IEEE INT C MULT EX
   Yao YY, 2003, STUD FUZZ SOFT COMP, V119, P115
   ZHA ZJ, 2007, P ACM INT C MULT
NR 44
TC 25
Z9 30
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2008
VL 5
IS 1
AR 3
DI 10.1145/1404880.1404883
PG 27
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 376AS
UT WOS:000261155700003
DA 2024-07-18
ER

PT J
AU Joshi, D
   Wang, JZ
   Li, J
AF Joshi, Dhiraj
   Wang, James Z.
   Li, Jia
TI The story picturing engine-A system for automatic text illustration
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; design; experimentation; human factors; story picturing;
   lexical referencing; image retrieval; mutual reinforcement; Markov chain
ID RETRIEVAL
AB We present an unsupervised approach to automated story picturing. Semantic keywords are extracted from the story, an annotated image database is searched. Thereafter, a novel image ranking scheme automatically determines the importance of each image. Both lexical annotations and visual content play a role in determining the ranks. Annotations are processed using the Wordnet. A mutual reinforcement-based rank is calculated for each image. We have implemented the methods in our Story Picturing Engine (SPE) system. Experiments on large-scale image databases are reported. A user study has been performed and statistical analysis of the results has been presented.
C1 Penn State Univ, University Pk, PA 16802 USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park
RP Joshi, D (corresponding author), Penn State Univ, 310 IST Bldg, University Pk, PA 16802 USA.
EM djoshi@cse.psu.edu
RI Wang, James/JAD-0675-2023
CR AGOSTI M, 2000, LECT NOTES COMPUTER, V1980
   [Anonymous], P 25 ANN INT C RES D
   [Anonymous], 1998, Computer Networks and ISDN Systems, DOI [DOI 10.1016/S0169-7552(98)00110-X, 10.1016/S0169-7552(98)00110-X]
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Barnard K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P408, DOI 10.1109/ICCV.2001.937654
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   BROWN DC, 1981, ACM SIGGRAPH COMPUT, V15, P174
   BUDANTSKY A, 2001, NAACL WORKSH WORDNET
   Carneiro G., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P559, DOI 10.1145/1076034.1076129
   Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800
   Chen CC, 2005, INT J DIGIT LIBRARIE, V5, P275, DOI 10.1007/s00799-004-0097-5
   Chen YX, 2005, IEEE T IMAGE PROCESS, V14, P1187, DOI 10.1109/TIP.2005.849770
   Clay SR, 1996, IEEE COMPUT GRAPH, V16, P31, DOI 10.1109/38.486678
   Coyne B, 2001, COMP GRAPH, P487, DOI 10.1145/383259.383316
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   GARFIELD E, 1972, SCIENCE, V178, P471, DOI 10.1126/science.178.4060.471
   KAHN KM, 1979, THESIS MIT CAMBRIDGE
   Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140
   LAVRENKO V, 2003, P ADV NEUR INF PROC, V16
   Li J, 2004, IEEE T IMAGE PROCESS, V13, P338, DOI 10.1109/TIP.2003.821349
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Li L, 2002, P 11 INT C WORLD WID, P527, DOI 10.1145/511446.511514
   LU R, 2002, LECT NOTES ARTIFICIA, V2160
   Ma WY, 1999, MULTIMEDIA SYST, V7, P184, DOI 10.1007/s005300050121
   Miller G.A., 1990, Int. J. Lexicogr, V3, P235, DOI [DOI 10.1093/IJL/3.4.235, 10.1093/ijl/3.4.235]
   PINSKI G, 1976, INFORM PROCESS MANAG, V12, P297, DOI 10.1016/0306-4573(76)90048-0
   Reynolds C. W., 1982, Computer Graphics, V16, P289, DOI 10.1145/965145.801293
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
NR 29
TC 50
Z9 82
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2006
VL 2
IS 1
BP 68
EP 89
DI 10.1145/1126004.1126008
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IW
UT WOS:000250871200004
DA 2024-07-18
ER

PT J
AU Nie, J
   Huang, L
   Zheng, CY
   Lv, XW
   Wang, R
AF Nie, Jie
   Huang, Lei
   Zheng, Chengyu
   Lv, Xiaowei
   Wang, Rui
TI Cross-scale Graph Interaction Network for Semantic Segmentation of
   Remote Sensing Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Remote sensing; semantic segmentation; cross-scale; graph convolutional
   network; boundary
AB Semantic segmentation of remote sensing (RS) images plays a vital role in a variety of fields, including urban planning, natural disaster monitoring, and land resource management. Due to the complexity and low resolution of RS images, many approaches have been proposed to handle the related task. However, these previously developed approaches dedicate to contextual interaction but ignore the cross-scale semantic correlation and multi-scale boundary information. Therefore, we propose a Cross-scale Graph Interaction Network (CGIN) to address semantic segmentation problems of RS images, which consists of a semantic branch and a boundary branch. In the semantic branch, we first apply atrous convolution to extract multi-scale semantic features of RS images. Particularly, based on the multi-scale semantic features, a Cross-scale Graph Interaction (CGI) module is introduced, which establishes cross-scale graph structures and performs adaptive graph reasoning to capture the cross-scale semantic correlation of RS objects. In the boundary branch, we propose a Multiscale Boundary Feature Extraction (MBFE) module that utilizes atrous convolutions with different dilation rates to extract multi-scale boundary features. Finally, to address the problem of sparse boundary pixels in the fusion process of the two branches, we propose a Multi-scale Similarity-guided Aggregation (MSA) module by calculating the similarity of semantic features and boundary features at the corresponding scale, which can emphasize the boundary information in semantic features. Our proposed CGIN outperforms state-of-the-art approaches in numerical experiments conducted on two benchmark remote sensing datasets.
C1 [Nie, Jie; Huang, Lei; Zheng, Chengyu; Lv, Xiaowei; Wang, Rui] Ocean Univ China, Coll Informat Sci & Engn, Qingdao, Peoples R China.
C3 Ocean University of China
RP Huang, L (corresponding author), Ocean Univ China, Coll Informat Sci & Engn, Qingdao, Peoples R China.
EM niejie@ouc.edu.cn; huangl@ouc.edu.cn; zhengchengyu@stu.ouc.edu.cn;
   lvxiaowei@stu.ouc.edu.cn; wangrui1585@stu.ouc.edu.cn
RI Nie, Jie/ABG-9228-2021
OI Nie, Jie/0000-0003-4952-7666; Huang, Lei/0000-0003-4087-3677; Lv,
   xiaowei/0000-0002-6544-1515
FU National Natural Science Foundation of China [62172376]; Fundamental
   Research Funds for the Central Universities [202042008]; National Key
   Research and Development Program of China [2021YFF0704000]
FX This work was supported in part by the National Natural Science
   Foundation of China (Grant No. 62072418), the National Natural Science
   Foundation of China (Grant No. 62172376), the Fundamental Research Funds
   for the Central Universities (Grant No. 202042008), the National Key
   Research and Development Program of China (Grant No. 2020YFB1711700),
   and the National Key Research and Development Program of China (Grant
   No. 2021YFF0704000).
CR Ao Chen, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12305), P734, DOI 10.1007/978-3-030-60633-6_61
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bertasius G, 2016, PROC CVPR IEEE, P3602, DOI 10.1109/CVPR.2016.392
   Bruna J, 2014, Arxiv, DOI [arXiv:1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Chen KX, 2020, IEEE T NEUR NET LEAR, V31, P1747, DOI 10.1109/TNNLS.2019.2927224
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P4545, DOI 10.1109/CVPR.2016.492
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Cheng DC, 2017, IEEE J-STARS, V10, P5769, DOI 10.1109/JSTARS.2017.2747599
   Defferrard M, 2016, ADV NEUR IN, V29
   Ding HH, 2019, IEEE I CONF COMP VIS, P6818, DOI 10.1109/ICCV.2019.00692
   Ding L, 2021, IEEE T GEOSCI REMOTE, V59, P426, DOI 10.1109/TGRS.2020.2994150
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gilmer J, 2017, PR MACH LEARN RES, V70
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Hanzhe., 2020, European Conference on Computer Vision, P1
   Li R, 2021, ISPRS J PHOTOGRAMM, V181, P84, DOI 10.1016/j.isprsjprs.2021.09.005
   Li Y, 2018, ADV NEUR IN, V31
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu QH, 2020, IEEE COMPUT SOC CONF, P199, DOI 10.1109/CVPRW50498.2020.00030
   Liu W, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3524497
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu Y, 2019, LECT NOTES COMPUT SC, V11554, P97, DOI 10.1007/978-3-030-22796-8_11
   Luo MN, 2018, IEEE T CYBERNETICS, V48, P648, DOI 10.1109/TCYB.2017.2647904
   Ma Haoxiang, 2021, arXiv, DOI DOI 10.48550/ARXIV.2110.14587
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Nogueira K, 2019, IEEE T GEOSCI REMOTE, V57, P7503, DOI 10.1109/TGRS.2019.2913861
   Nogueira K, 2018, IEEE GEOSCI REMOTE S, V15, P1446, DOI 10.1109/LGRS.2018.2845549
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rottensteiner F., 2014, ISPRS Semantic Labeling Contest
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun JX, 2022, INT CONF ACOUST SPEE, P1081, DOI 10.1109/ICASSP43922.2022.9747828
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Wang Q, 2020, IEEE T IMAGE PROCESS, V29, P7549, DOI 10.1109/TIP.2020.3004249
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P435, DOI 10.1007/978-3-030-58520-4_26
   Zhang C, 2019, REMOTE SENS ENVIRON, V221, P173, DOI 10.1016/j.rse.2018.11.014
   Zhang DL, 2020, IEEE T CYBERNETICS, V50, P3033, DOI 10.1109/TCYB.2019.2905157
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang L, 2020, Arxiv, DOI [arXiv:1909.06121, 10.48550/arXiv.1909.06121]
   Zhang X, 2022, IEEE T IMAGE PROCESS, V31, P2695, DOI 10.1109/TIP.2022.3160399
   Zhang YC, 2018, ICMLC 2020: 2020 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING, P145, DOI 10.1145/3383972.3383975
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhen M., 2020, IEEE C COMPUT VIS PA, P13666, DOI DOI 10.48550/ARXIV.2004.07684
NR 48
TC 3
Z9 3
U1 5
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 185
DI 10.1145/3558770
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200008
DA 2024-07-18
ER

PT J
AU Mackowski, M
   Brzoza, P
   Kawulok, M
   Meisel, R
   Spinczyk, D
AF Mackowski, Michal
   Brzoza, Piotr
   Kawulok, Mateusz
   Meisel, Rafal
   Spinczyk, Dominik
TI Multimodal Presentation of Interactive Audio-Tactile Graphics Supporting
   the Perception of Visual Information by Blind People
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Accessibility; audio-tactile graphics; multimodal interfaces;
   perception; touch interface; visually impaired people
ID COGNITIVE LOAD THEORY; DESIGN
AB Due to the limitations in the perception of graphical information by blind people and the need to substitute the sense of sight with other senses, the correct use of multimedia in the presentation of graphics is particularly important. The aim of the authors was to correctly present visual information in the tactile and audio form and to provide contextually selected information to reduce the existing cognitive barriers. In this article, the authors decided to research the method of exploring a tactile picture by a blind person and providing contextual and semantic information about the touched image elements using the developed tool for multimodal presentation of interactive audio-tactile graphics supporting the perception of blind people. The use of multimedia should improve the perception of the conveyed content. Therefore, the effectiveness of interpreting the information contained in the tactile image is verified by analyzing the tactile, audio, and contextual perceptions during the experiments. The following issues were considered in detail from the point of view of the blind and visually impaired: the recognition of shapes of image elements depending on their size and properties, the optimal width of elements displayed on the tablet screen, the time intervals between taps on an image element, and the acceptable length of graphic element audio description. The results from this study suggest the need to select the image presentation parameters in various perception channels to the user's needs. The findings of our research on tactile shape perception of geometric figures indicate potential problems in recognizing the shapes of figures in the case of wrong preparation of tactile pictures. In the case of a small difference in the proportions of the length of the sides of the figure or a slight difference in the measures of the angles of the figure sides, we have noticed that most blind people fail to recognize its shape. Considerable progress in improving such perception can be achieved by increasing the proportions in the lengths of the sides and the measures of the angles between the sides of the figure. Further steps concern introducing an alternative audio description of the properties of the figure so as to improve the interpretability of the figure shape. For most users, the width of a virtual line in a digital image displayed on a tablet should be around 5 mm for a corresponding 1 mm tactile line. The configuration of the time intervals defining the user's gestures (two-taps, three-taps) should be about 340 ms. Applying audio descriptions to tactile picture elements improves the understanding and interpretation of the information presented on it. Most participants in the test group accepted 5 to 10 seconds of voice prompts. Longer messages were incomprehensible, and details were hard to remember. In the proposed solution, the audio description can be divided into two or three different descriptions available to the user after tapping two or three times on an image element. It allows the user to decide on the amount of contextual information needed about the touched tactile picture element. The research attempted to show the typical values of various image presentation parameters and their ranges of values and indicated effective methods of their selection for a specific system user.
C1 [Mackowski, Michal; Brzoza, Piotr; Kawulok, Mateusz; Meisel, Rafal] Silesian Tech Univ, Dept Distributed Syst & Informat Devices, Gliwice, Poland.
   [Spinczyk, Dominik] Silesian Tech Univ, Dept Med Informat & Artificial Intelligence, Zabrze, Poland.
C3 Silesian University of Technology; Silesian University of Technology
RP Mackowski, M (corresponding author), Silesian Tech Univ, Dept Distributed Syst & Informat Devices, Gliwice, Poland.
EM michal.mackowski@polsl.pl; piotr.brzoza@polsl.pl;
   mateusz.kawulok@polsl.pl; rafal.n.meisel@gmail.com;
   dominik.spinczyk@polsl.pl
RI Maćkowski, Michał/T-5854-2018
OI Spinczyk, Dominik/0000-0003-0068-2948; Kawulok,
   Mateusz/0000-0002-5794-7970
CR Aldrich F.K., 2001, British Journal of Visual Impairment, V19, P69, DOI DOI 10.1177/026461960101900303
   [Anonymous], 2005, The Cambridge Handbook of Multimedia Learning, DOI DOI 10.1017/CBO9780511816819.003
   Baker CM, 2016, ACM T ACCESS COMPUT, V8, DOI 10.1145/2854005
   Baker CatherineM., 2014, Proceedings of the 16th International ACM SIGACCESS Conference on Computers Accessibility - ASSETS '14, P75
   Bellik Y, 2017, LECT NOTES COMPUT SC, V10688, P8, DOI 10.1007/978-3-319-72038-8_2
   Bowden JL, 2013, AGE, V35, P1077, DOI 10.1007/s11357-012-9429-3
   Bowers L, 2021, BRIT J VISUAL IMPA, V39, P214, DOI 10.1177/0264619620912771
   Bradley NA, 2002, LECT NOTES COMPUT SC, V2411, P349
   Bradley NA, 2005, PERS UBIQUIT COMPUT, V9, P395, DOI 10.1007/s00779-005-0350-y
   brailleauthority, GUIDELINES STANDARDS
   Brayda L, 2018, MICROMACHINES-BASEL, V9, DOI 10.3390/mi9070351
   Brewster S., 2003, Univ. Access Inf. Soc, V2, P105, DOI [DOI 10.1007/S10209-002-0042-6, 10.1007/s10209-002-0042-6]
   Brzostek-Pawlowska Jolanta, 2019, J TELECOMMUN INF TEC, V2, P92, DOI [10.26636/JTIT.2019.132819, DOI 10.26636/JTIT.2019.132819]
   Burdea GC, 2000, COMPUTER GRAPHICS INTERNATIONAL 2000, PROCEEDINGS, P295, DOI 10.1109/CGI.2000.852345
   Chow JK, 2022, PSYCHOL RES-PSYCH FO, V86, P1262, DOI 10.1007/s00426-021-01560-z
   Dufresne A., 1995, Human-Computer Interaction. Interact '95, P163
   Elgendy M, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9235110
   Garcia GG, 2020, BEHAV RES METHODS, V52, P813, DOI 10.3758/s13428-019-01279-1
   Götzelmann T, 2015, 8TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2015), DOI 10.1145/2769493.2769497
   Gori M, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00010
   Gorlewicz L., 2018, Interactive Multimedia - Multimedia Production and Digital Storytelling, DOI [DOI 10.5772/INTECHOPEN.82289, 10.5772/intechopen.82289]
   Gotzelmann T., 2018, ACM TRANS ACCESS COM, V11, P1
   Holloway L, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173772
   Jain S., 2014, INT J COMPUT APPL, V103, P29, DOI [10.5120/18119-9403, DOI 10.5120/18119-9403]
   Jehoel S, 2006, BRIT J VISUAL IMPA, V24, P67, DOI 10.1177/0264619606063402
   Khusro S, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22010361
   Kirschner PA, 2002, LEARN INSTR, V12, P1, DOI 10.1016/S0959-4752(01)00014-7
   Klatzky RL, 2006, J EXP PSYCHOL-APPL, V12, P223, DOI 10.1037/1076-898X.12.4.223
   Kristjánsson A, 2016, RESTOR NEUROL NEUROS, V34, P769, DOI 10.3233/RNN-160647
   Kuriakose B, 2020, MULTIMODAL TECHNOLOG, V4, DOI 10.3390/mti4040073
   Lin MC, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P598, DOI 10.1109/ROBOT.2002.1013424
   Mackowski M., 2020, Future Perspectives of AT, EAccessibility and EInclusion, P75
   Mackowski M, 2022, LECT NOTES COMPUT SC, P82, DOI 10.1007/978-3-031-08648-9_11
   Mackowski M, 2022, DISABIL REHABIL-ASSI, V17, P559, DOI 10.1080/17483107.2020.1800116
   Mackowski M, 2020, ADV INTELL SYST, V1033, P211, DOI 10.1007/978-3-030-29885-2_19
   Mackowski M, 2018, MULTIMED TOOLS APPL, V77, P6191, DOI 10.1007/s11042-017-4526-z
   Mackowski MS, 2018, COMPUT BIOL MED, V95, P298, DOI 10.1016/j.compbiomed.2017.06.003
   McLinden M., 2004, Journal of Visual Impairment Blindness, V98, P99, DOI [10.1177/0145482x0409800210, DOI 10.1177/0145482X0409800210, 10.1177/0145482X0409800210]
   Melfi G, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376508
   Merabet LB, 2016, LECT NOTES COMPUT SC, V9739, P595, DOI 10.1007/978-3-319-40238-3_57
   Metatla O, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300608
   Mikulowski D, 2020, LECT NOTES COMPUT SC, V12149, P1, DOI 10.1007/978-3-030-49663-0_1
   Minhat Muzaireen, 2017, 2017 8th International Conference on Information Technology (ICIT). Proceedings, P475, DOI 10.1109/ICITECH.2017.8080045
   Morrison GR, 2005, ETR&D-EDUC TECH RES, V53, P94, DOI 10.1007/BF02504801
   Nam CS, 2012, INT J HUM-COMPUT INT, V28, P784, DOI 10.1080/10447318.2012.661357
   Oumard C, 2022, LECT NOTES COMPUT SC, P388, DOI 10.1007/978-3-031-08648-9_45
   Pearl C., 2016, Designing voice user interfaces: Principles of conversational experiences
   Pigeon C, 2019, GAIT POSTURE, V67, P43, DOI 10.1016/j.gaitpost.2018.09.018
   Porcheron M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174214
   Prescher D, 2018, UNIVERSAL ACCESS INF, V17, P391, DOI 10.1007/s10209-017-0538-8
   Qing-Wen Lin, 2013, Human-Computer Interaction. Users and Contexts of Use. 15th International Conference, HCI International 2013. Proceedings: LNCS 8006, P193, DOI 10.1007/978-3-642-39265-8_21
   Duong QV, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49640-w
   Scheller M, 2021, DEVELOPMENTAL SCI, V24, DOI 10.1111/desc.13001
   Senette Caterina, 2013, Universal Access in Human-Computer Interaction. Design Methods, Tools, and Interaction Techniques for eInclusion. 7th International Conference, UAHCI 2013 Held as Part of HCI International 2013. Proceedings. LNCS 8009, P576, DOI 10.1007/978-3-642-39188-0_62
   Senna I, 2021, CURR BIOL, V31, P4879, DOI 10.1016/j.cub.2021.08.060
   Shinoda M, 2022, LECT NOTES COMPUT SC, P261, DOI 10.1007/978-3-031-08648-9_30
   Silva CS, 2016, INT CONF INF AUTOMAT
   Steinmetz JD, 2021, LANCET GLOB HEALTH, V9, pE144, DOI [10.1016/S2214-109X(20)30425-3, 10.1016/S2214-109X(20)30489-7]
   Theurel A, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040251
   World Health Organization, Blindness and vision impairment
   YaserDhaher Robert, 2017, J BLIND INNOV RES, V7, P2, DOI [10.5241/7-123, DOI 10.5241/7-123]
   Zeng LM, 2016, IEEE T HUM-MACH SYST, V46, P88, DOI 10.1109/THMS.2015.2477999
NR 62
TC 1
Z9 1
U1 5
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2023
VL 19
IS 5
SU S
AR 167
DI 10.1145/3586076
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L0SK0
UT WOS:001020437100003
OA Bronze
DA 2024-07-18
ER

PT J
AU Van Rensburg, BJ
   Puteaux, P
   Puech, W
   Pedeboy, JP
AF Van Rensburg, Bianca Jansen
   Puteaux, Pauline
   Puech, William
   Pedeboy, Jean-Pierre
TI 3D Object Watermarking from Data Hiding in the Homomorphic Encrypted
   Domain
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimedia security; high-capacity data hiding; 3D object security;
   Paillier homomorphic encryption; signal processing in the encrypted
   domain; format compliant
AB For over a decade, 3D objects are an increasingly popular form of media. It has become necessary and urgent to secure them during their transmission or archiving. In this article, we propose a new method to obtain a watermarked 3D object from high-capacity data hiding in the encrypted domain. Based on the homomorphic properties of the Paillier cryptosystem, our proposed method allows us to embed several secret messages in the encrypted domain with a high-capacity. These messages can be extracted in the plain-text domain after the 3D object decryption. To the best of our knowledge, we are the first to propose a data hiding method in the encrypted domain where the high-capacity watermark is conserved in the plain-text domain after the 3D object is decrypted. The encryption and the data hiding in the encrypted domain are format compliant and without size expansion, despite the use of the Paillier cryptosystem. Each time a new message is embedded in the encrypted domain, flags are added in order to indicatewhich blocks are still available for the embedding of additional messages. After the decryption of a watermarked encrypted 3D object, our method produces a watermarked 3D object which is visually very similar to the original 3D object. From the decrypted watermarked 3D object, we can then extract all the embedded messages directly in the plain-text domain, without the need for an auxiliary file. Moreover, large keys are used, rending our method secure for real-life applications.
C1 [Van Rensburg, Bianca Jansen; Puech, William] Univ Montpellier, CNRS, LIRMM, F-34095 Montpellier, France.
   [Puteaux, Pauline] Univ Lille, CNRS, CRIStAL, Centrale Lille, F-59000 Lille, France.
   [Pedeboy, Jean-Pierre] Strategies, F-94510 Rungis, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; Universite de Lille; Centrale Lille; Centre National de la
   Recherche Scientifique (CNRS)
RP Van Rensburg, BJ (corresponding author), Univ Montpellier, CNRS, LIRMM, F-34095 Montpellier, France.
EM bianca.jansen-van-rensburg@lirmm.fr; pauline.puteaux@cnrs.fr;
   william.puech@lirmm.fr; jp.pedeboy@cadwin.com
OI Jansen van Rensburg, Bianca/0000-0002-8683-1360; Puech,
   William/0000-0001-9383-2401
CR Armknecht F, 2013, DESIGN CODE CRYPTOGR, V67, P209, DOI 10.1007/s10623-011-9601-2
   Beugnon S., 2018, 2018 IEEE INT C MULT, P1, DOI [10.1109/ICMEW.2018.8551510, DOI 10.1109/ICMEW.2018.8551510]
   Beugnon S, 2019, IEEE T MULTIMEDIA, V21, P2171, DOI 10.1109/TMM.2019.2900905
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Cayre F, 2003, IEEE T SIGNAL PROCES, V51, P939, DOI 10.1109/TSP.2003.809380
   Chen YC, 2014, J VIS COMMUN IMAGE R, V25, P1164, DOI 10.1016/j.jvcir.2014.04.003
   Cox IJ., 2007, DIGITAL WATERMARKING
   Dong L, 2015, IEEE T MULTIMEDIA, V17, P2174, DOI 10.1109/TMM.2015.2484221
   Fallahpour M, 2009, IEEE IMAGE PROC, P4241, DOI 10.1109/ICIP.2009.5413711
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Jeong SW, 2017, IEEE T MULTIMEDIA, V19, P2692, DOI 10.1109/TMM.2017.2710802
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Kumar CV, 2013, 2013 INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND SIGNAL PROCESSING (ICCSP), P730, DOI 10.1109/iccsp.2013.6577152
   Lavoué G, 2010, IEEE T MULTIMEDIA, V12, P636, DOI 10.1109/TMM.2010.2060475
   Lyu WL, 2022, SIGNAL PROCESS, V201, DOI 10.1016/j.sigpro.2022.108686
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Malik A, 2019, J INF SECUR APPL, V48, DOI 10.1016/j.jisa.2019.102374
   Paillier P, 1999, LECT NOTES COMPUT SC, V1592, P223
   Puech W, 2008, PROC SPIE, V6819, DOI 10.1117/12.766754
   Puteaux P, 2021, IEEE T MULTIMEDIA, V23, P636, DOI 10.1109/TMM.2020.2985537
   Puteaux P, 2020, IEEE ACCESS, V8, P108655, DOI 10.1109/ACCESS.2020.3001385
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Qiu YQ, 2020, IEEE ACCESS, V8, P23209, DOI 10.1109/ACCESS.2020.2969252
   Regev O, 2009, J ACM, V56, DOI 10.1145/1568318.1568324
   Shah M, 2018, ARAB J SCI ENG, V43, P8145, DOI 10.1007/s13369-018-3354-4
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Shiu CW, 2015, SIGNAL PROCESS-IMAGE, V39, P226, DOI 10.1016/j.image.2015.09.014
   van Rensburg BJ, 2021, IEEE IMAGE PROC, P3068, DOI 10.1109/ICIP42928.2021.9506320
   Wang K, 2008, IEEE T MULTIMEDIA, V10, P1513, DOI 10.1109/TMM.2008.2007350
   Wu HT, 2019, J VIS COMMUN IMAGE R, V62, P87, DOI 10.1016/j.jvcir.2019.04.015
   Wu HT, 2016, J VIS COMMUN IMAGE R, V40, P765, DOI 10.1016/j.jvcir.2016.08.021
   Xiang SJ, 2018, IEEE T CIRC SYST VID, V28, P3099, DOI 10.1109/TCSVT.2017.2742023
   Xu N, 2022, COGN COMPUT, V14, P1172, DOI 10.1007/s12559-021-09919-5
   Yin Z., 2021, LECT NOTES COMPUTER, V3020
   Zhang XP, 2016, IEEE T CIRC SYST VID, V26, P1622, DOI 10.1109/TCSVT.2015.2433194
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zheng SL, 2021, IEEE T DEPEND SECURE, V18, P692, DOI 10.1109/TDSC.2019.2913422
   Zhou JT, 2016, IEEE T CIRC SYST VID, V26, P441, DOI 10.1109/TCSVT.2015.2416591
   Zhou N, 2020, IEEE ACCESS, V8, P81412, DOI 10.1109/ACCESS.2020.2990903
NR 39
TC 2
Z9 2
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2023
VL 19
IS 5
SU S
AR 175
DI 10.1145/3588573
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L0SK0
UT WOS:001020437100011
OA Bronze, Green Published
DA 2024-07-18
ER

PT J
AU Wang, JX
   Chen, R
   Lv, ZH
AF Wang, Jinxia
   Chen, Rui
   Lv, Zhihan
TI DNA Computing-Based Multi-Source Data Storage Model in Digital Twins
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Digital Twins; DNA computing; multi-source data storage; Huffman coding;
   error correction code
ID CONSTRUCTION; CODES
AB The work aims to study the application of Deoxyribonucleic Acid (DNA) multi-source data storage in Digital Twins (DT). Through the investigation of the research status of DT and DNA computing, the work puts forward the concept of DNA multi-source data storage for DT. Raptor code is improved from the design direction of degree distribution function, and six degree function distribution schemes are proposed in turn in the process of describing the research method. Additionally, a quaternary dynamic Huffman coding method is applied in DNA data storage, combined with the improved concatenated code as the error correction code. Considering the content of cytosine deoxynucleotide (C) and guanine deoxynucleotide Guanine (G) and the distribution of homopolymer in DNA storage, the work proposes and verifies an improved concatenated code algorithm Deoxyribonucleic Acid-Improved Concatenated code (DNA-ICC). The results show that while the Signal-to-Noise Ratio (SNR) increases, the Bit Error Rate (BER) decreases gradually and the trend is similar. But the anti-interference ability of the degree distribution function optimized by the probability transfer method is better. The BER of DNA-ICC scheme decreases with the decrease of error probability, which is stronger than other error correction codes. Compared with the original concatenated code, it saves at least 1.65 s, and has a good control effect on homopolymer. When the size of homopolymer exceeds 4 nt, the probability of homopolymer is only 0.44%. The proposed Quaternary dynamic Huffman code and concatenated error correction code have excellent performance.
C1 [Wang, Jinxia] Shaanxi Fash Engn Univ, Sch Art & Design, 1 Tongwen Rd,Fengxi New Town, Xian 712046, Peoples R China.
   [Chen, Rui] Xian Univ Posts & Telecommun, 563 Changan South Rd, Xian 710061, Peoples R China.
   [Lv, Zhihan] Uppsala Univ, Fac Arts, Dept Game Design, Strandgatan 1b Residenset, Visby, Sweden.
C3 Xi'an University of Posts & Telecommunications; Uppsala University
RP Lv, ZH (corresponding author), Uppsala Univ, Fac Arts, Dept Game Design, Strandgatan 1b Residenset, Visby, Sweden.
EM 1754354519@qq.com; chenrui@xupt.edu.cn; zhihan.lyu@speldesign.uu.se
OI Wang, Jinxia/0000-0003-3555-1147; Chen, Rui/0000-0001-5292-3374
CR Adithya B., 2021, B COMPUTER SCI ELECT, V2, P38
   Aheleroff S, 2021, ADV ENG INFORM, V47, DOI 10.1016/j.aei.2020.101225
   Cai K, 2021, IEEE T INFORM THEORY, V67, P3438, DOI 10.1109/TIT.2021.3049627
   Campbell M, 2020, COMPUTER, V53, P63, DOI 10.1109/MC.2020.2967908
   Cao B, 2021, IEEE T NANOBIOSCI, V20, P212, DOI 10.1109/TNB.2021.3056351
   Cao B, 2020, IEEE ACCESS, V8, P29547, DOI 10.1109/ACCESS.2020.2970838
   Hassantabar S, 2021, IEEE T CONSUM ELECTR, V67, P244, DOI 10.1109/TCE.2021.3130228
   Immink KAS, 2020, IEEE ACCESS, V8, P49523, DOI 10.1109/ACCESS.2020.2980036
   Jiang YC, 2021, PHILOS T R SOC A, V379, DOI 10.1098/rsta.2020.0360
   Kuruppu S, 2012, IEEE ACM T COMPUT BI, V9, P137, DOI 10.1109/TCBB.2011.82
   Li LL, 2021, IEEE ACCESS, V9, P121507, DOI 10.1109/ACCESS.2021.3108218
   Li X, 2020, IEEE T NANOBIOSCI, V19, P299, DOI 10.1109/TNB.2020.2971644
   Li YS, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3190838
   Lischer K., 2021, J MECH ENG JMECHE, V8, P27
   Lu XZ, 2020, IEEE ACCESS, V8, P162892, DOI 10.1109/ACCESS.2020.3021700
   Major P, 2021, IEEE INSTRU MEAS MAG, V24, P39, DOI 10.1109/MIM.2021.9549127
   Mishra P, 2020, IEEE COMMUN LETT, V24, P1602, DOI 10.1109/LCOMM.2020.2991461
   Namasudra S, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3392665
   Namasudra S, 2020, J NETW COMPUT APPL, V172, DOI 10.1016/j.jnca.2020.102835
   Nguyen HX, 2021, IEEE COMMUN MAG, V59, P10, DOI 10.1109/MCOM.001.2000343
   Qi QL, 2021, J MANUF SYST, V58, P3, DOI 10.1016/j.jmsy.2019.10.001
   Qian J, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02025
   Schroeder GN, 2021, P IEEE, V109, P556, DOI 10.1109/JPROC.2020.3032444
   Song WT, 2020, IEEE T INFORM THEORY, V66, P6048, DOI 10.1109/TIT.2020.3002611
   Wanasinghe TR, 2020, IEEE ACCESS, V8, P104175, DOI 10.1109/ACCESS.2020.2998723
   Wang J, 2021, BIG DATA RES, V24, DOI 10.1016/j.bdr.2021.100193
   Wang KJ, 2021, INT J PROD RES, V59, P6471, DOI 10.1080/00207543.2020.1817999
   Weber JH, 2021, IEEE COMMUN LETT, V25, P41, DOI 10.1109/LCOMM.2020.3023826
   Xue F, 2020, ISPRS J PHOTOGRAMM, V167, P418, DOI 10.1016/j.isprsjprs.2020.07.020
   Xue TB, 2020, IEEE ACCESS, V8, P140972, DOI 10.1109/ACCESS.2020.3012688
   Yildiz Emre, 2020, Procedia CIRP, P1, DOI 10.1016/j.procir.2020.04.043
   Zhang BN, 2021, IEEE ACCESS, V9, P74446, DOI 10.1109/ACCESS.2021.3081562
   Zhang C, 2020, IEEE INTERNET THINGS, V7, P11884, DOI 10.1109/JIOT.2020.3005729
   Zheng HD, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3440035
   Zhou QH, 2021, IEEE ACCESS, V9, P104513, DOI 10.1109/ACCESS.2021.3094876
NR 35
TC 0
Z9 0
U1 4
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 127
DI 10.1145/3561823
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700002
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Wang, ZX
   Gao, GW
   Li, JC
   Yan, H
   Zheng, H
   Lu, HM
AF Wang, Zhengxue
   Gao, Guangwei
   Li, Juncheng
   Yan, Hui
   Zheng, Hao
   Lu, Huimin
TI Lightweight Feature De-redundancy and Self-calibration Network for
   Efficient Image Super-resolution
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Lightweight image super-resolution; Feature De-redundancy; feature
   self-calibration
AB In recent years, thanks to the inherent powerful feature representation and learning abilities of the convolutional neural network (CNN), deep CNN-steered single image super-resolution approaches have achieved remarkable performance improvements. However, these methods are often accompanied by large consumption of computing and memory resources, which is difficult to be adopted in real-world application scenes. To handle this issue, we design an efficient Feature De-redundancy and Self-calibration Super-resolution network (FDSCSR). In particular, a Feature De-redundancy and Self-calibration Block (FDSCB) is proposed to reduce the repetitive feature information extracted by the model and further enhance the efficiency of the model. Then, based on FDSCB, a Local Feature Fusion Module is presented to elaborately utilize and fuse the feature information extracted by each FDSCB. Abundant experiments on benchmarks have demonstrated that our FDSCSR achieves superior performance with relatively less computational consumption and storage resource than other state-of-the-art approaches. The code is available at https://github.com/IVIPLab/FDSCSR.
C1 [Wang, Zhengxue] Nanjing Univ Posts & Telecommun, Coll Automat & Coll Artificial Intelligence, Nanjing 210023, Peoples R China.
   [Gao, Guangwei] Nanjing Univ Posts & Telecommun, Inst Adv Technol Carbon Neutral, Nanjing 210023, Peoples R China.
   [Li, Juncheng] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 999077, Peoples R China.
   [Yan, Hui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Zheng, Hao] Nanjing Xiaozhuang Univ, Key Lab Intelligent Informat Proc, Nanjing 211171, Peoples R China.
   [Lu, Huimin] Kyushu Inst Technol, Dept Mech & Control Engn, Kitakyushu, Fukuoka 8048550, Japan.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Posts & Telecommunications; Shanghai University; Nanjing University of
   Science & Technology; Nanjing Xiaozhuang University; Kyushu Institute of
   Technology
RP Gao, GW (corresponding author), Nanjing Univ Posts & Telecommun, Inst Adv Technol Carbon Neutral, Nanjing 210023, Peoples R China.; Zheng, H (corresponding author), Nanjing Xiaozhuang Univ, Key Lab Intelligent Informat Proc, Nanjing 211171, Peoples R China.
EM wzx_0826@163.com; csggao@gmail.com; cvjunchengli@gmail.com;
   yanhui@njust.edu.cn; zhh710@163.com; dr.huimin.lu@ieee.org
RI LEE, YU/JXY-2338-2024; zheng, hao/JQI-4215-2023; Li,
   Juncheng/AHA-3971-2022
OI Zhengxue, Wang/0000-0002-4668-2559; Li, Juncheng/0000-0001-7314-6754;
   Zheng, Hao/0000-0003-0829-9660; Lu, Huimin/0000-0001-9794-3221
FU National Key Research and Development Program of China [2018AAA0100102,
   2018AAA0100100]; National Natural Science Foundation of China [61972212,
   61772568, 62076139, 61833011]; Natural Science Foundation of Jiangsu
   Province [BK20190089]; Six Talent Peaks Project in Jiangsu Province
   [RJFW-011]; Open Fund Project of Key Laboratory of Intelligent
   Information Processing (Nanjing Xiaozhuang University)
FX This work was supported in part by the National Key Research and
   Development Program of China under Project Nos. 2018AAA0100102 and
   2018AAA0100100; the National Natural Science Foundation of China under
   Grants No. 61972212, 61772568, 62076139, and 61833011; the Natural
   Science Foundation of Jiangsu Province under Grant No. BK20190089; the
   Six Talent Peaks Project in Jiangsu Province under Grant No. RJFW-011;
   and the Open Fund Project of Key Laboratory of Intelligent Information
   Processing (Nanjing Xiaozhuang University).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen HG, 2022, INFORM FUSION, V79, P124, DOI 10.1016/j.inffus.2021.09.005
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gao G., 2022, P INT JOINT C ART IN, P913, DOI DOI 10.24963/IJCAI.2022/128
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Gao GW, 2022, AAAI CONF ARTIF INTE, P661
   Gao GW, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107539
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Jiang K, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107475
   Jiang-Jiang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10093, DOI 10.1109/CVPR42600.2020.01011
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P1443, DOI 10.1109/TCYB.2020.2970104
   Li JC, 2024, Arxiv, DOI arXiv:2109.14335
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li W., 2022, arXiv
   Li WB, 2021, Arxiv, DOI arXiv:2105.10422
   Li YC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414838
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mei YQ, 2020, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR42600.2020.00573
   Mylavarapu S, 2020, Arxiv, DOI arXiv:2005.04437
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Wang CF, 2019, Arxiv, DOI arXiv:1904.02358
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   Wang YQ, 2023, Arxiv, DOI arXiv:2206.06214
   Wang Z., 2021, P 2021 IEEE INT C MU, P1, DOI 10.1109/DTPI52967.2021.9540074
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Yi P, 2022, IEEE T PATTERN ANAL, V44, P2264, DOI 10.1109/TPAMI.2020.3042298
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang DY, 2021, IEEE T MULTIMEDIA, V23, P2172, DOI 10.1109/TMM.2020.3008041
   Zhang JQ, 2022, IEEE T CIRC SYST VID, V32, P1020, DOI 10.1109/TCSVT.2021.3071191
   Zhang XD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4034, DOI 10.1145/3474085.3475291
   Zhang XY, 2021, IEEE T MULTIMEDIA, V23, P1924, DOI 10.1109/TMM.2020.3005025
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhu FY, 2019, IEEE INT CONF COMP V, P2453, DOI 10.1109/ICCVW.2019.00300
   Zhu XY, 2022, IEEE T CIRC SYST VID, V32, P1273, DOI 10.1109/TCSVT.2021.3078436
   Zou WB, 2022, IEEE COMPUT SOC CONF, P929, DOI 10.1109/CVPRW56347.2022.00107
NR 54
TC 4
Z9 4
U1 11
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 110
DI 10.1145/3569900
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300010
DA 2024-07-18
ER

PT J
AU Ding, XW
   Pan, YW
   Li, YH
   Yao, T
   Zeng, D
   Mei, T
AF Ding, Xuewei
   Pan, Yingwei
   Li, Yehao
   Yao, Ting
   Zeng, Dan
   Mei, Tao
TI Boosting Relationship Detection in Images with Multi-Granular
   Self-Supervised Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual relationship detection; self-supervised learning
AB Visual and spatial relationship detection in images has been a fast-developing research topic in the multimedia field, which learns to recognize the semantic/spatial interactions between objects in an image, aiming to compose a structured semantic understanding of the scene. Most of the existing techniques directly encapsulate the holistic image feature plus the semantic and spatial features of the given two objects for predicting the relationship, but leave the inherent supervision derived from such structured and thorough image understanding under-exploited. Specifically, the inherent supervision among objects or relations within an image can span different granularities in this hierarchy including, from simple to comprehensive, (1) the object-based supervision that captures the interaction between the semantic and spatial features of each individual object, (2) the inter-object supervision that characterizes the dependency within the relationship triplet (<subject-predicate-object>), and (3) the inter-relation supervision that exploits contextual information among all relationship triplets in an image. These inherent multi-granular supervisions offer a fertile ground for building self-supervised proxy tasks. In this article, we compose a trilogy of exploring the multi-granular supervision in the sequence from object-based, inter-object, and inter-relation perspectives. We integrate the standard relationship detection objective with a series of proposed self-supervised proxy tasks, which is named as Multi-Granular Self-Supervised learning (MGS). Our MGS is appealing in view that it is pluggable to any neural relationship detection models by simply including the proxy tasks during training, without increasing the computational cost at inference. Through extensive experiments conducted on the SpatialSense and VRD datasets, we demonstrate the superiority of MGS for both spatial and visual relationship detection tasks.
C1 [Ding, Xuewei; Zeng, Dan] Shanghai Univ, 99 ShangDa Rd, Shanghai 200444, Peoples R China.
   [Pan, Yingwei; Li, Yehao; Yao, Ting; Mei, Tao] AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
C3 Shanghai University
RP Zeng, D (corresponding author), Shanghai Univ, 99 ShangDa Rd, Shanghai 200444, Peoples R China.; Yao, T (corresponding author), AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
EM xueweid@shu.edu.cn; panyw.ustc@gmail.com; yehaoli.sysu@gmail.com;
   tingyao.ustc@gmail.com; dzeng@shu.edu.cn; tmei@jd.com
RI Pan, Yingwei/T-7649-2019
OI Pan, Yingwei/0000-0002-4344-8898; Yao, Ting/0000-0001-7587-101X
FU National Key R&D Program of China [2020AAA0108600]
FX This work was supported by the National Key R&D Program of China under
   Grant No. 2020AAA0108600.
CR Ahsan U, 2019, IEEE WINT CONF APPL, P179, DOI 10.1109/WACV.2019.00025
   [Anonymous], 2016, P INT JOINT C ARTIFI
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102
   Bin Y, 2019, AAAI CONF ARTIF INTE, P8110
   Cai Qi, 2020, P NEURIPS, V33, P12638
   Chiou MJ, 2021, IEEE ACCESS, V9, P50441, DOI 10.1109/ACCESS.2021.3069041
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, DOI 10.3115/V1/D14-1179, 10.48550/ARXIV.1406.1078, DOI 10.48550/ARXIV.1406.1078]
   Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352
   Delearde R, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108410
   Deng JJ, 2021, IEEE T MULTIMEDIA, V23, P846, DOI 10.1109/TMM.2020.2990070
   Deshpande A, 2015, IEEE I CONF COMP VIS, P567, DOI 10.1109/ICCV.2015.72
   Diomataris Markos, 2021, P IEEECVF INT C COMP, P15911
   Divvala SK, 2014, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2014.412
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Gidaris S, 2018, Arxiv, DOI arXiv:1803.07728
   Goyal P, 2019, IEEE I CONF COMP VIS, P6400, DOI 10.1109/ICCV.2019.00649
   Huang Rui, 2022, ACM T MULTIMEDIA COM
   Inayoshi Sho, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P682, DOI 10.1007/978-3-030-58558-7_40
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Lee W, 2019, PROC CVPR IEEE, P4979, DOI 10.1109/CVPR.2019.00512
   Li JN, 2017, IEEE I CONF COMP VIS, P2669, DOI 10.1109/ICCV.2017.289
   Li Mi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13883, DOI 10.1109/CVPR42600.2020.01390
   Li R., 2022, P IEEE C COMP VIS PA, P19486
   Li R., 2021, P IEEE CVF INT C COM, P2105
   Li YA, 2022, PROC CVPR IEEE, P17969, DOI 10.1109/CVPR52688.2022.01746
   Li YH, 2023, IEEE T PATTERN ANAL, V45, P1489, DOI 10.1109/TPAMI.2022.3164083
   Li YA, 2021, AAAI CONF ARTIF INTE, V35, P8518
   Li YK, 2017, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2017.766
   Liang KM, 2018, AAAI CONF ARTIF INTE, P7098
   Liao WT, 2019, IEEE COMPUT SOC CONF, P444, DOI 10.1109/CVPRW.2019.00058
   Long Fuchen, 2022, PROC IEEECVF C COMPU, P3192
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lu JS, 2016, ADV NEUR IN, V29
   Lu YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15911, DOI 10.1109/ICCV48922.2021.01563
   Mangla P, 2020, IEEE WINT CONF APPL, P2207, DOI [10.1109/wacv45572.2020.9093338, 10.1109/WACV45572.2020.9093338]
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pan YW, 2020, Arxiv, DOI arXiv:2007.02375
   Pan YW, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3448981
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Peyre J, 2017, IEEE I CONF COMP VIS, P5189, DOI 10.1109/ICCV.2017.554
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Ramanathan V, 2015, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.2015.7298713
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711
   Schuster S., 2015, P 4 WORKSHOP VISION, P70
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Su JC, 2019, Arxiv, DOI arXiv:1906.07079
   Sun QR, 2017, PROC CVPR IEEE, P435, DOI 10.1109/CVPR.2017.54
   Tang PJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303083
   Tomas M., 2013, arXiv, DOI DOI 10.48550/ARXIV.1310.4546
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wang L, 2021, NEUROCOMPUTING, V434, P55, DOI 10.1016/j.neucom.2020.12.099
   Xu T, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3416493
   Xu ZW, 2021, IEEE T MULTIMEDIA, V24, P3652, DOI 10.1109/TMM.2021.3104411
   Yang KY, 2019, IEEE I CONF COMP VIS, P2051, DOI 10.1109/ICCV.2019.00214
   Yang ZG, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3374754
   Yao T, 2022, Arxiv, DOI arXiv:2207.04976
   Yao T, 2021, AAAI CONF ARTIF INTE, V35, P10656
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yao Ting, 2022, P EUROPEAN C COMPUTE
   Yin GJ, 2018, LECT NOTES COMPUT SC, V11207, P330, DOI 10.1007/978-3-030-01219-9_20
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121
   YueHu Siheng Chen, 2019, ICML WORKSHOP LEARNI
   Zhan YB, 2019, PROC CVPR IEEE, P5123, DOI 10.1109/CVPR.2019.00527
   Zhang HW, 2017, IEEE I CONF COMP VIS, P4243, DOI 10.1109/ICCV.2017.454
   Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331
   Zhang J, 2019, AAAI CONF ARTIF INTE, P9185
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao ZY, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3410441
   Zheng SP, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P121, DOI 10.1145/3343031.3350962
   Zhuang BH, 2017, IEEE I CONF COMP VIS, P589, DOI 10.1109/ICCV.2017.71
NR 78
TC 0
Z9 0
U1 3
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 88
DI 10.1145/3556978
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300013
DA 2024-07-18
ER

PT J
AU Bai, CY
   Bolonkin, M
   Regunath, V
   Subrahmanian, VS
AF Bai, Chongyang
   Bolonkin, Maksim
   Regunath, Viney
   Subrahmanian, V. S.
TI DIPS: A Dyadic Impression Prediction System for Group Interaction Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Impression prediction; graph neural networks; video analysis;
   computational psychology; multi-layer networks
ID SCALE
AB We consider the problem of predicting the impression that one subject has of another in a video clip showing a group of interacting people. Our novel Dyadic Impression Prediction System (DIPS) contains two major innovations. First, we develop a novel method to align the facial expressions of subjects pi and pj as well as account for the temporal delay that might be involved in pi reacting to pj's facial expressions. Second, we propose the concept of a multilayered stochastic network for impression prediction on top of which we build a novel Temporal Delayed Network graph neural network architecture. Our overall DIPS architecture predicts six dependent variables relating to the impression pi has of pj. Our experiments show that DIPS beats eight baselines from the literature, yielding statistically significant improvements of 19.9% to 30.8% in AUC and 12.6% to 47.2% in F1-score. We further conduct ablation studies showing that our novel features contribute to the overall quality of the predictions made by DIPS.
C1 [Bai, Chongyang; Bolonkin, Maksim; Regunath, Viney] Dartmouth Coll, 6211 Hinman Box, Hanover, NH 03755 USA.
   [Subrahmanian, V. S.] Northwestern Univ, 1800 Sherman Ave,Suite 3-000, Evanston, IL 60201 USA.
C3 Dartmouth College; Northwestern University
RP Bai, CY (corresponding author), Dartmouth Coll, 6211 Hinman Box, Hanover, NH 03755 USA.
EM chongyang.bai.gr@dartmouth.edu; mbolonkin@cs.dartmouth.edu;
   viney.regunath.21@dartmouth.edu; vss@northwestern.edu
RI Subrahmanian, Venkatramanan/ABA-7399-2021; Bai, Chongyang/HRD-8818-2023
OI Subrahmanian, Venkatramanan/0000-0001-7191-0296; Bai,
   Chongyang/0000-0002-1245-9877
FU Army Research Office [W911NF1610342]; ONR [N00014-20-1-2407]; U.S.
   Department of Defense (DOD) [W911NF1610342] Funding Source: U.S.
   Department of Defense (DOD)
FX This work was funded by Army Research Office grant W911NF1610342.
   Equipment support from ONR grant N00014-20-1-2407 is also gratefully
   acknowledged.
CR Anselmi F, 2019, LECT NOTES COMPUT SC, V11751, P421, DOI 10.1007/978-3-030-30642-7_38
   Bai CY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4643
   Bai CY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4504
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Beyan C, 2021, IEEE T AFFECT COMPUT, V12, P1084, DOI 10.1109/TAFFC.2019.2944614
   Beyan C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P311, DOI 10.1145/3240508.3240685
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   Boccaletti S, 2014, PHYS REP, V544, P1, DOI 10.1016/j.physrep.2014.07.001
   BRUCE AJ, 1993, J GEN PSYCHOL, V120, P451, DOI 10.1080/00221309.1993.9711159
   Burgoon J.K., 2015, INT ENCY INTERPERSON, P1, DOI DOI 10.1002/9781118540190.WBEIC102
   Burgoon J.K., 1995, Interpersonal adaptation, DOI [DOI 10.1017/CBO9780511720314, 10.1017/ CBO9780511720314]
   Çeliktutan O, 2017, IEEE T AFFECT COMPUT, V8, P29, DOI 10.1109/TAFFC.2015.2513401
   Celiktutan O, 2014, IEEE IMAGE PROC, P4196, DOI 10.1109/ICIP.2014.7025852
   Chávez-Martínez G, 2015, PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA (MUM 2015), P109, DOI 10.1145/2836041.2836051
   Davydenko M, 2020, PERS INDIV DIFFER, V159, DOI 10.1016/j.paid.2020.109883
   De Domenico M, 2013, PHYS REV X, V3, DOI 10.1103/PhysRevX.3.041022
   Doreian P, 2009, SOC NETWORKS, V31, P1, DOI 10.1016/j.socnet.2008.08.001
   Dorn B., 2021, Detecting trust and deception in group interaction, P57, DOI DOI 10.1007/978-3-030-54383-9_4
   Eloy L, 2019, ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P244, DOI 10.1145/3340555.3353748
   Facchetti G, 2011, P NATL ACAD SCI USA, V108, P20953, DOI 10.1073/pnas.1109521108
   Fang S, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P225, DOI 10.1145/2993148.2993201
   Floyd K, 1999, COMMUN MONOGR, V66, P219, DOI 10.1080/03637759909376475
   Goldberg L. R., 1992, Psychological Assessment, P26
   Hareli S, 2010, COGNITION EMOTION, V24, P128, DOI 10.1080/02699930802613828
   Heider F., 1958, PSYCHOL INTERPERSONA, DOI 10.1037/10628-000
   Heider F, 1946, J PSYCHOL, V21, P107, DOI 10.1080/00223980.1946.9917275
   Hoque M, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P697
   Escalante HJ, 2022, IEEE T AFFECT COMPUT, V13, P894, DOI 10.1109/TAFFC.2020.2973984
   Jia XY, 2019, PROC CVPR IEEE, P9833, DOI 10.1109/CVPR.2019.01007
   Joshi J, 2014, INT C PATT RECOG, P2855, DOI 10.1109/ICPR.2014.492
   Kampman O, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P606
   Kindiroglu AA, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0224-z
   Kivela M, 2014, J COMPLEX NETW, V2, P203, DOI 10.1093/comnet/cnu016
   Kumano S, 2014, IEICE T INF SYST, VE97D, P2008, DOI 10.1587/transinf.E97.D.2008
   Kumano Shiro, 2009, P 2009 INT C MULTIMO, P99, DOI [10.1145/1647314.1647333, DOI 10.1145/1647314.1647333]
   Kumar C., 2021, Proceedings of the Fifteenth International AAAI Conference on Web and Social Media, ICWSM 2021, held virtually, June 7-10, 2021, P339
   Nguyen LS, 2014, IEEE T MULTIMEDIA, V16, P1018, DOI 10.1109/TMM.2014.2307169
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Lin YS, 2020, INT CONF ACOUST SPEE, P8044, DOI [10.1109/icassp40776.2020.9053308, 10.1109/ICASSP40776.2020.9053308]
   Mawalim CO, 2019, LECT NOTES COMPUT SC, V11578, P370, DOI 10.1007/978-3-030-21902-4_27
   McKeown G, 2012, IEEE T AFFECT COMPUT, V3, P5, DOI 10.1109/T-AFFC.2011.20
   Müller P, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P153, DOI 10.1145/3172944.3172969
   Murata A, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0153128
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   NEWCOMB T, 1958, AM SOCIOL REV, V23, P742, DOI 10.2307/2089062
   Nihei F, 2014, Proceedings of the 16th International Conference on Multimodal Interaction, ICMI'14, page, P136
   NISBETT RE, 1989, SOC COGNITION, V7, P67, DOI 10.1521/soco.1989.7.1.67
   Okada S, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3265754
   Okada S, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P169, DOI 10.1145/2993148.2993154
   Otsuka K., 2006, Proc. Conf. Human Factors in Computing Systems, P1175
   Ponce-López V, 2016, LECT NOTES COMPUT SC, V9915, P400, DOI 10.1007/978-3-319-49409-8_32
   Rayner K, 2009, Q J EXP PSYCHOL, V62, P1457, DOI 10.1080/17470210902816461
   Reysen S, 2005, SOC BEHAV PERSONAL, V33, P201, DOI 10.2224/sbp.2005.33.2.201
   Reysen S., 2006, North American Journal of Psychology, V8, P373
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Seiter JS, 2009, COMMUN RES REP, V26, P1, DOI 10.1080/08824090802636959
   Tickle-Degnen Linda, 1990, Psychological Inquiry, V1, P285, DOI DOI 10.1207/S15327965PLI01041
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang YB, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P693, DOI 10.1145/3442381.3450096
   Yan R, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1292, DOI 10.1145/3240508.3240572
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang L, 2022, IEEE T AFFECT COMPUT, V13, P298, DOI 10.1109/TAFFC.2019.2951656
   Zhang Lingyu, 2020, P IEEECVF WINTER C A
NR 63
TC 0
Z9 0
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
AR 43
DI 10.1145/3532865
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800017
DA 2024-07-18
ER

PT J
AU Pang, B
   Zhai, DM
   Jiang, JJ
   Liu, XM
AF Pang, Bo
   Zhai, Deming
   Jiang, Junjun
   Liu, Xianming
TI Fully Unsupervised Person Re-Identification via Selective Contrastive
   Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; unsupervised learning; contrastive learning
AB Person re-identification (ReID) aims at searching the same identity person among images captured by various cameras. Existing fully supervised person ReID methods usually suffer from poor generalization capability caused by domain gaps. Unsupervised person ReID has attracted a lot of attention recently, because it works without intensive manual annotation and thus shows great potential in adapting to new conditions. Representation learning plays a critical role in unsupervised person ReID. In this work, we propose a novel selective contrastive learning framework for fully unsupervised feature learning. Specifically, different from traditional contrastive learning strategies, we propose to use multiple positives and adaptively selected negatives for defining the contrastive loss, enabling to learn a feature embedding model with stronger identity discriminative representation. Moreover, we propose to jointly leverage global and local features to construct three dynamic memory banks, among which the global and local ones are used for pairwise similarity computation and the mixture memory bank are used for contrastive loss definition. Experimental results demonstrate the superiority of our method in unsupervised person ReID compared with the state of the art. Our code is available at https://github.com/pangbo1997/Unsup ReID.git.
C1 [Pang, Bo; Zhai, Deming; Jiang, Junjun; Liu, Xianming] Harbin Inst Technol, Fac Comp, Harbin 150001, Peoples R China.
C3 Harbin Institute of Technology
RP Zhai, DM (corresponding author), Harbin Inst Technol, Fac Comp, Harbin 150001, Peoples R China.
EM cspb@hit.edu.cn; zhaideming@hit.edu.cn; jiangjunjun@hit.edu.cn;
   csxm@hit.edu.cn
RI Jiang, Junjun/L-7087-2019; Pang, Boxue/O-2200-2019
OI Jiang, Junjun/0000-0002-5694-505X; 
FU National Key Research and Development Project [2019YFE0109600]; National
   Science Foundation of China [61922027, 61971165, 61932022]
FX This work was supported in part by National Key Research and Development
   Project under Grant 2019YFE0109600, and in part by the National Science
   Foundation of China under grants 61922027, 61971165, and 61932022.
CR Chen T, 2020, PR MACH LEARN RES, V119
   Chen YC, 2018, PR MACH LEARN RES, V80
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding G., 2019, ARXIV190601308
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, P NIPS, V33, P11309
   Gidaris S., 2018, P 6 INT C LEARNING R
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu GL, 2020, AAAI CONF ARTIF INTE, V34, P12362
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2018, LECT NOTES COMPUT SC, V11211, P176, DOI 10.1007/978-3-030-01234-2_11
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
NR 30
TC 13
Z9 14
U1 2
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 64
DI 10.1145/3485061
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, JH
   Wang, YH
   Weng, NN
   Chai, TR
   Li, AN
   Zhang, FX
   Yu, SS
AF Wang, Jiahao
   Wang, Yunhong
   Weng, Nina
   Chai, Tianrui
   Li, Annan
   Zhang, Faxi
   Yu, Sansi
TI Will You Ever Become Popular? Learning to Predict Virality of Dance
   Clips
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Dance challenge; virality prediction; multi-modal approach
ID VIDEOS
AB Dance challenges are going viral in video communities like TikTok nowadays. Once a challenge becomes popular, thousands of short-form videos will be uploaded within a couple of days. Therefore, virality prediction from dance challenges is of great commercial value and has a wide range of applications, such as smart recommendation and popularity promotion. In this article, a novel multi-modal framework that integrates skeletal, holistic appearance, facial and scenic cues is proposed for comprehensive dance vitality prediction. To model body movements, we propose a pyramidal skeleton graph convolutional network (PSGCN) that hierarchically refines spatio-temporal skeleton graphs. Meanwhile, we introduce a relational temporal convolutional network (RTCN) to exploit appearance dynamics with non-local temporal relations. An attentive fusion approach is finally proposed to adaptively aggregate predictions from different modalities. To validate our method, we introduce a large-scale viral dance video (VDV) dataset, which contains over 4,000 dance clips of eight viral dance challenges. Extensive experiments on the VDV dataset well demonstrate the effectiveness of our approach. Furthermore, we show that short video applications such as multi-dimensional recommendation and action feedback can be derived from our model.
C1 [Wang, Jiahao; Wang, Yunhong; Weng, Nina; Chai, Tianrui; Li, Annan] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Zhang, Faxi; Yu, Sansi] Tencent, Shenzhen 518054, Peoples R China.
C3 Beihang University; Tencent
RP Li, AN (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM jhwang@buaa.edu.cn; yhwang@buaa.edu.cn; wengnn@buaa.edu.cn;
   trchai@buaa.edu.cn; liannan@buaa.edu.cn; micahzhang@tencent.com;
   mionyu@tencent.com
RI wang, jiahao/HLV-7669-2023; Li, Annan/KRP-2299-2024
OI Li, Annan/0000-0003-3497-5052
FU National Natural Science Foundation of China [U20B2069]; Foundation for
   Innovative Research Groups through the National Natural Science
   Foundation of China [61421003]; CCF-Tencent Rhino-Bird Research Fund
FX This work was supported by National Natural Science Foundation of China
   (Grant No. U20B2069), Foundation for Innovative Research Groups through
   the National Natural Science Foundation of China (Grant No. 61421003),
   and CCF-Tencent Rhino-Bird Research Fund.
CR Abu Farha Y, 2019, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2019.00369
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI DOI 10.1145/2433396.2433443
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Bai S., 2018, EMPIRICAL EVALUATION
   Bielski A, 2018, IEEE COMPUT SOC CONF, P2398, DOI 10.1109/CVPRW.2018.00309
   Bielski A, 2018, IEEE ACCESS, V6, P74277, DOI 10.1109/ACCESS.2018.2884831
   Burges C., 2005, ICML, P89
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen JY, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P898, DOI 10.1145/2964284.2964314
   Chen Z, 2018, PR MACH LEARN RES, V80
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Devlin J., 2018, BERT PRE TRAINING DE
   Doughty H, 2019, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2019.00805
   Doughty H, 2018, PROC CVPR IEEE, P6057, DOI 10.1109/CVPR.2018.00634
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao Y., 2014, P IEEE C EXP TRANSP, P3
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   International Olympic Committee, 2021, LIST SUMM WINT OL SP
   Jibin Gao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P222, DOI 10.1007/978-3-030-58577-8_14
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kingma D. P., 2014, arXiv
   Kipf Thomas N., 2017, 5 INT C LEARN REPRES
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Li J, 2018, 2018 INTERNATIONAL CONFERENCE ON ARTS, LINGUISTICS, LITERATURE AND HUMANITIES (ICALLH 2018), P313
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li YJ, 2018, LECT NOTES COMPUT SC, V11165, P125, DOI 10.1007/978-3-030-00767-6_12
   Li Yongjun, 2018, PROC ASIAN C COMPUT
   Martin JA, 1997, BRIT J SURG, V84, P273, DOI 10.1046/j.1365-2168.1997.02502.x
   Mohsin Maryam, 2021, 10 Tiktok Statistics That You Need to Know in 2021
   Nie L., 2019, SYNTHESIS LECT IMAGE, V9, P1
   Niepert M, 2016, PR MACH LEARN RES, V48
   Pan JH, 2019, IEEE I CONF COMP VIS, P6340, DOI 10.1109/ICCV.2019.00643
   Parmar P, 2019, PROC CVPR IEEE, P304, DOI 10.1109/CVPR.2019.00039
   Parmar P, 2019, IEEE WINT CONF APPL, P1468, DOI 10.1109/WACV.2019.00161
   Parmar P, 2017, IEEE COMPUT SOC CONF, P76, DOI 10.1109/CVPRW.2017.16
   Parmar P, 2016, IEEE ENG MED BIO, P2241, DOI 10.1109/EMBC.2016.7591175
   Paszke A, 2019, ADV NEUR IN, V32
   Pirsiavash H, 2014, LECT NOTES COMPUT SC, V8694, P556, DOI 10.1007/978-3-319-10599-4_36
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Su YT, 2020, MULTIMEDIA SYST, V26, P519, DOI 10.1007/s00530-020-00660-x
   Tao L, 2016, COMPUT VIS IMAGE UND, V148, P136, DOI 10.1016/j.cviu.2015.11.016
   Trzcinski T, 2017, IEEE T MULTIMEDIA, V19, P2561, DOI 10.1109/TMM.2017.2695439
   Van Droogenbroeck M., 2012, 2012 IEEE COMP SOC C, P32
   Vaswani A, 2017, ADV NEUR IN, V30
   Venkataraman Vinay, 2015, BMVC, P67
   Wang JH, 2019, IEEE IMAGE PROC, P1585, DOI [10.1109/icip.2019.8803088, 10.1109/ICIP.2019.8803088]
   Wang L., 2016, P ECCV
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wongsuparatkul Ekapol, 2020, ICCCM'20: Proceedings of the 8th International Conference on Computer and Communications Management, P123, DOI 10.1145/3411174.3411186
   Xiang X, 2018, IEEE IMAGE PROC, P928, DOI 10.1109/ICIP.2018.8451364
   Xie JY, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P2542, DOI 10.1145/3366423.3380004
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yansong Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9836, DOI 10.1109/CVPR42600.2020.00986
   Yu F, 2015, P INT C LEARN REPR
   Yuchao Zhang, 2020, Web Services - ICWS 2020. 27th International Conference Held as Part of the Services Conference Federation, SCF 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12406), P61, DOI 10.1007/978-3-030-59618-7_5
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zia A, 2015, LECT NOTES COMPUT SC, V9349, P430, DOI 10.1007/978-3-319-24553-9_53
NR 67
TC 1
Z9 1
U1 4
U2 34
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 54
DI 10.1145/3477533
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cucchiara, R
   Fabbri, M
AF Cucchiara, Rita
   Fabbri, Matteo
TI Fine-grained Human Analysis under Occlusions and Perspective Constraints
   in Multimedia Surveillance
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE People detection; human pose estimation; tracking; 3D localization;
   synthetic dataset
ID TRACKING; PEOPLE
AB Human detection in the wild is a research topic of paramount importance in computer vision, and it is the starting step for designing intelligent systems oriented to human interaction that work in complete autonomy. To achieve this goal, computer vision and machine learning should aim at superhuman capabilities. In this work, we address the problem of fine-grained human analysis under occlusions and perspective constraints. More specifically, we discuss some issues and some possible solutions to effectively detect people using pose estimation methods and to detect humans under occlusions both in the two-dimensional (2D) image plane and in the 3D space exploiting single monocular cameras. Dealing with occlusion can be done at the joint level or pixel level: We discuss two different solutions, the former based on a supervised neural network architecture for detecting occluded joints and the latter based on a semi-supervised specialized GAN that exploits both appearance and human shape attributes to determine the missing parts of the visible shape. To deal with perspective constraints, we further discuss a neural approach based on a double architecture that learns to create an optimal neural representation, which is useful to reconstruct the 3D position of human keypoints starting with simple RGB images. All these approaches have a critical point in common: the need for large annotated datasets. To have large, fair, consistent, transparent, and ethical datasets, we propose the adoption of synthetic datasets as, for example, JTA and MOTSynth. In this article, we discuss the pros and cons of using synthetic datasets while tackling several human-centered Al issues with respect to European GDPR rules for privacy. We further explore and discuss an application in the field of risk assessment by space occupancy estimation during the COVID-19 pandemic called Inter-Homines.
C1 [Cucchiara, Rita; Fabbri, Matteo] Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, Via P Vivarelli 10, I-41125 Modena, Italy.
C3 Universita di Modena e Reggio Emilia
RP Cucchiara, R (corresponding author), Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, Via P Vivarelli 10, I-41125 Modena, Italy.
EM rita.cucchiara@unimore.it; matteo.fabbri@unimore.it
RI Cucchiara, Rita/L-3006-2015
OI Cucchiara, Rita/0000-0002-2239-283X
FU Italian Ministry of University and Research under PRIN Project
   Ptediction of Events in Urban Environments; NVIDIA AI Technology Center
   at UNIMORE
FX We acknowledge the researchers at AimageLab who supported and
   co-authored some of the cited works. The projects discussed in this
   article are supported by the Italian Ministry of University and Research
   under PRIN Project Ptediction of Events in Urban Environments and the
   European projects ARTEMIS Arrowhead Tools, as well as the NVIDIA AI
   Technology Center at UNIMORE.
CR Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   [Anonymous], WHITE PAPER ARTIFICI
   [Anonymous], 2005, P 3 ACM INT WORKSH V
   Arai H, 2018, IEEE ENG MED BIO, P5162, DOI 10.1109/EMBC.2018.8513469
   Bae SH, 2018, IEEE T PATTERN ANAL, V40, P595, DOI 10.1109/TPAMI.2017.2691769
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chu Q, 2017, IEEE I CONF COMP VIS, P4846, DOI 10.1109/ICCV.2017.518
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   Dabral R, 2018, LECT NOTES COMPUT SC, V11213, P679, DOI 10.1007/978-3-030-01240-3_41
   Dendorfer P., 2020, Mot20: A benchmark for multi object tracking in crowded scenes
   Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966
   Fabbri M, 2021, MOTSYNTH CAN SYNTHET
   Fabbri M, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Fabbri M, 2020, PROC CVPR IEEE, P7202, DOI 10.1109/CVPR42600.2020.00723
   Fabbri M, 2018, LECT NOTES COMPUT SC, V11208, P450, DOI 10.1007/978-3-030-01225-0_27
   Fulgeri F, 2019, COMPUT VIS IMAGE UND, V182, P71, DOI 10.1016/j.cviu.2019.03.007
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Huang K, 2016, ARXIV160307054
   Iqbal U, 2017, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2017.495
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Krähenbühl P, 2018, PROC CVPR IEEE, P2955, DOI 10.1109/CVPR.2018.00312
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   Mehta D, 2018, INT CONF 3D VISION, P120, DOI 10.1109/3DV.2018.00024
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Milan A., 2016, ARXIV160300831
   Moon G, 2019, IEEE I CONF COMP VIS, P10132, DOI 10.1109/ICCV.2019.01023
   Popa AI, 2017, PROC CVPR IEEE, P4714, DOI 10.1109/CVPR.2017.501
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Richter SR, 2017, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2017.243
   Rogez G, 2020, IEEE T PATTERN ANAL, V42, P1146, DOI 10.1109/TPAMI.2019.2892985
   Rogez G, 2017, PROC CVPR IEEE, P1216, DOI 10.1109/CVPR.2017.134
   Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41
   Sanchez-Matilla R, 2016, LECT NOTES COMPUT SC, V9914, P84, DOI 10.1007/978-3-319-48881-3_7
   Scholz Matthias, 2008, PRINCIPAL MANIFOLDS
   Solera F, 2015, 2015 12TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252
   Torfason Robert, 2018, P INT C LEARN REPRS
   Voigtlaender P, 2019, PROC CVPR IEEE, P7934, DOI 10.1109/CVPR.2019.00813
   Wang J, 2012, PROCEDIA COMPUT SCI, V13, P120, DOI 10.1016/j.procs.2012.09.120
   Wang Tongzhou, 2018, ARXIV181110959
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Yu FW, 2016, LECT NOTES COMPUT SC, V9914, P36, DOI 10.1007/978-3-319-48881-3_3
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zanfir A., 2018, Advances in Neural Information Processing Systems
   Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou X., 2019, arXiv
NR 54
TC 4
Z9 4
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2022
VL 18
IS 1
SU S
SI SI
AR 32
DI 10.1145/3476839
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5PV
UT WOS:000772639300009
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Tasaka, S
AF Tasaka, Shuji
TI An Empirical Method for Causal Inference of Constructs for QoE in
   Haptic-Audiovisual Communications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Causal inference; Quality of Experience (QoE); media synchronization;
   causation; SEM; construct; latent variables; Bayesian modeling;
   haptic-audiovisual interactive communications; MCMC; OpenBUGS
ID SYNCHRONIZATION; MEDIA; MODEL
AB This article proposes an empirical method for inferring causal directions in multidimensional Quality of Experience (QoE) in multimedia communications, noting that causation in QoE is perceptual. As an example for modeling framework, we pick up a Bayesian structural equation model (SEM) previously built for haptic audiovisual interactive communications. The SEM includes three constructs (Audiovisual quality, Haptic quality, and User experience quality), which are latent variables each representing a group of observed variables with similar characteristics. In the SEM, the causal directions of the constructs were assumed by resorting to the domain knowledge. This article aims at proposing a methodology for inferring causal directions of constructs in general by verifying the assumption of causal directions in the SEM through their observed data alone. For that purpose, we compare six SEMs each with different causal directions of constructs, one of which is the one from the domain knowledge. The proposed method is based on QoE prediction by a Bayesian approach with Markov chain Monte Carlo (MCMC) simulation. Setting observed scores to the indicators of exogenous variables in each SEM, we predict values of all the indicators; we then assess the mean square error (MSE) between predicted QoE and mean opinion score (MOS) from observed scores and estimate the probability distribution of the MSE in each SEM. We can compare any two SEMs to find which is more plausible by examining the probability that the MSE for one SEM is smaller than or equal to that for the other. These probabilities are estimated with MCMC simulation. The method indicates that the causal directions thus inferred for the haptic audiovisual interactive communications adequately support the original ones drawn from the domain knowledge. In addition, we demonstrate that QoE can behave like the "impact-perceive-adapt" model of the effects of delayed haptic and visual feedback on performance in a collaborative environment, which Jay, Glencross, and Hubbold proposed in 2007, and that it accompanies reversal of plausible causal directions like a flip-flop.
C1 [Tasaka, Shuji] Nagoya Ind Sci Res Inst, Chikusa Ku, 1-13 Yotsuya Dori, Nagoya, Aichi 4640819, Japan.
RP Tasaka, S (corresponding author), Nagoya Ind Sci Res Inst, Chikusa Ku, 1-13 Yotsuya Dori, Nagoya, Aichi 4640819, Japan.
EM tasaka@nisri.jp
OI Tasaka, Shuji/0000-0002-2882-1914
FU JSPS KAKENHI [20K04495]; Grants-in-Aid for Scientific Research
   [20K04495] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI (Grant-In-Aid for Scientific
   Research of Japan Society for the Promotion of Science) grant number
   20K04495. The author thanks Eiichi Isomura and Prof. Toshiro Nunome for
   providing the experimental data.
CR Allen J, 2020, ARCHAEOL OCEAN, V55, P1, DOI 10.1002/arco.5207
   [Anonymous], 2013, THE BUGS BOOK
   [Anonymous], 2012, 2012 IEEE COOL CHIPS
   [Anonymous], 2003, P ACM NETGAMES 03 2, DOI DOI 10.1145/963900.963904
   Bartholomew D, 2011, WILEY SER PROBAB ST, P1, DOI 10.1002/9781119970583
   Blakowski G, 1996, IEEE J SEL AREA COMM, V14, P5, DOI 10.1109/49.481691
   Congdon P., 2014, Applied Bayesian modelling
   Fire A, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2809782
   Huang ZX, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490821
   Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751
   Ishibashi Y, 2000, C LOCAL COMPUT NETW, P337, DOI 10.1109/LCN.2000.891066
   Isomura Eiichi, 2013, IEICE Transactions on Communications (Japanese Edition), VJ96-B, P59
   Jay C, 2007, ACM T COMPUT-HUM INT, V14, DOI 10.1145/1275511.1275514
   Joreskog K.G., 1993, LISREL 8 STRUCTURAL
   Levy R, 2016, CH CRC STAT SOC BEHA, P1, DOI 10.1201/9781315374604
   Loehlin J.C, 2017, LATENT VARIABLE MODE
   Mulaik S.A., 2009, Linear causal modeling with structural equations
   Nunome T, 2020, IEICE T COMMUN, VE103B, P1107, DOI 10.1587/transcom.2019EBP3235
   Pearl J., 2009, CAUSALITY MODELS REA
   Peters J, 2017, ADAPT COMPUT MACH LE
   Peters J, 2014, BIOMETRIKA, V101, P219, DOI 10.1093/biomet/ast043
   SensAble Technologies Inc., 2009, OPENHAPTICS TOOLK VE
   Shimizu S, 2006, J MACH LEARN RES, V7, P2003
   Silva JM, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457451
   Silva R, 2006, J MACH LEARN RES, V7, P191
   Song XY, 2012, BASIC ADV BAYESIAN S
   Spirtes P., 2000, Adaptive computation and machine learning
   Steinbach E, 2012, P IEEE, V100, P937, DOI 10.1109/JPROC.2011.2182100
   Steinmetz R, 1996, IEEE J SEL AREA COMM, V14, P61, DOI 10.1109/49.481694
   Tasaka Shuji, 2019, ICC 2019 - 2019 IEEE International Conference on Communications (ICC). Proceedings, DOI 10.1109/ICC.2019.8761784
   Tasaka S, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3375922
   Tasaka S, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511202
   Tasaka S, 2017, IEEE T MULTIMEDIA, V19, P1195, DOI 10.1109/TMM.2017.2652064
   Tatematsu A., 2010, Communications Quality and Reliability (CQR), 2010 IEEE International Workshop Technical Committee on, P1, DOI [DOI 10.1109/CQR.2010.5619913, 10.1109/CQR.2010.5619913]
   Watanabe S, 2010, J MACH LEARN RES, V11, P3571
   Wiedermann W., 2016, Statistics and causality: Methods for Applied Empirical Research, DOI DOI 10.1002/9781118947074
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P957, DOI 10.1109/TMM.2015.2431915
NR 37
TC 2
Z9 2
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 17
DI 10.1145/3473986
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900017
OA Bronze
DA 2024-07-18
ER

PT J
AU Pan, YW
   Chen, Y
   Bao, Q
   Zhang, N
   Yao, T
   Liu, JG
   Mei, T
AF Pan, Yingwei
   Chen, Yue
   Bao, Qian
   Zhang, Ning
   Yao, Ting
   Liu, Jingen
   Mei, Tao
TI Smart Director: An Event-Driven Directing System for Live Broadcasting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Sports-broadcast directing; multi-view event detection; highlight
   detection
AB Live video broadcasting normally requires a multitude of skills and expertise with domain knowledge to enable multi-camera productions. As the number of cameras keeps increasing, directing a live sports broadcast has now become more complicated and challenging than ever before. The broadcast directors need to be much more concentrated, responsive, and knowledgeable, during the production. To relieve the directors from their intensive efforts, we develop an innovative automated sports broadcast directing system, called Smart Director, which aims at mimicking the typical human-in-the-loop broadcasting process to automatically create near-professional broadcasting programs in real-time by using a set of advanced multi-view video analysis algorithms. Inspired by the so-called "three-event" construction of sports broadcast [14], we build our system with an event-driven pipeline consisting of three consecutive novel components: (1) the Multi-View Event Localization to detect events by modeling multi-view correlations, (2) the Multi-View Highlight Detection to rank camera views by the visual importance for view selection, and (3) the Auto-Broadcasting Scheduler to control the production of broadcasting videos. To our best knowledge, our system is the first end-to-end automated directing system for multi-camera sports broadcasting, completely driven by the semantic understanding of sports events. It is also the first system to solve the novel problem of multi-view joint event detection by cross-view relation modeling. We conduct both objective and subjective evaluations on a real-world multi-camera soccer dataset, which demonstrate the quality of our auto-generated videos is comparable to that of the human-directed videos. Thanks to its faster response, our system is able to capture more fast-passing and short-duration events which are usually missed by human directors.
C1 [Pan, Yingwei; Chen, Yue; Bao, Qian; Yao, Ting; Mei, Tao] JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
   [Zhang, Ning; Liu, Jingen] JD AI Res, 675 E Middlefield Rd, Mountain View, CA USA.
RP Pan, YW (corresponding author), JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
EM panyw.ustc@gmail.com; chenyue21@jd.com; baoqian@jd.com;
   ning.zhang@jd.com; tingyao.ustc@gmail.com; jingenliu@gmail.com;
   tmei@jd.com
RI Mei, Tao/GQZ-0596-2022; Pan, Yingwei/T-7649-2019
OI Mei, Tao/0000-0002-5990-7307; Pan, Yingwei/0000-0002-4344-8898; Zhang,
   Ning/0000-0003-0497-1966
FU Migu Culture & Technology Ltd Co.
FX We would like to acknowledge the support from Migu Culture & Technology
   Ltd Co. and The Power (Beijing) Sports Ltd Co. for providing the soccer
   video data in this research.
CR [Anonymous], 2016, P INT JOINT C ARTIFI
   Barnfield A, 2013, COMMUN SPORT, V1, P326, DOI 10.1177/2167479513479107
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Cai Q, 2019, PROC CVPR IEEE, P11449, DOI 10.1109/CVPR.2019.01172
   Chen C., 2013, 2013 IEEE INT C MULT, P1, DOI DOI 10.1109/NSSMIC.2013.6829480
   Chen JH, 2018, IEEE WINT CONF APPL, P427, DOI 10.1109/WACV.2018.00053
   Chen JH, 2019, IEEE COMPUT SOC CONF, P2497, DOI 10.1109/CVPRW.2019.00305
   Chen JH, 2019, IEEE WINT CONF APPL, P1682, DOI 10.1109/WACV.2019.00184
   Chen Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P647, DOI 10.1145/3343031.3350937
   Choi Kyu-Hyoung, 2009, P KOR SOC BROADC ENG, P193
   Daniyal F., 2011, 2011 Conference for Visual Media Production, P11, DOI 10.1109/CVMP.2011.8
   Deng JJ, 2019, IEEE I CONF COMP VIS, P7022, DOI 10.1109/ICCV.2019.00712
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Goldlust J, 2018, PLAYING KEEPS SPORT
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Javed A, 2016, IEEE SIGNAL PROC LET, V23, P954, DOI 10.1109/LSP.2016.2573042
   Jiawei Zuo, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P4545, DOI 10.1145/3394171.3414453
   Kim H, 2018, IEEE T MULTIMEDIA, V20, P2415, DOI 10.1109/TMM.2018.2806224
   Kong Y, 2017, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2017.390
   Leake M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073653
   Lefèvre F, 2018, LECT NOTES COMPUT SC, V10884, P72, DOI 10.1007/978-3-319-94211-7_9
   Li CY, 2019, LECT NOTES COMPUT SC, V11296, P218, DOI 10.1007/978-3-030-05716-9_18
   Li YH, 2020, IEEE T MULTIMEDIA, V22, P1285, DOI 10.1109/TMM.2019.2939711
   Li Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3328994
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Mettes P, 2019, INT J COMPUT VISION, V127, P263, DOI 10.1007/s11263-018-1120-4
   Ning Zhang, 2020, 2020 25th International Conference on Pattern Recognition (ICPR), P4123, DOI 10.1109/ICPR48806.2021.9413110
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Owens Jim, 2015, TELEVISION SPORTS PR
   Pan YW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P233, DOI 10.1145/2647868.2656404
   Pan YW, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P53, DOI 10.1145/2766462.2767725
   Pech-Pacheco JL, 2000, INT C PATT RECOG, P314, DOI 10.1109/ICPR.2000.903548
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Raventós A, 2015, SPRINGERPLUS, V4, DOI 10.1186/s40064-015-1065-9
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Shih HC, 2018, IEEE T CIRC SYST VID, V28, P1212, DOI 10.1109/TCSVT.2017.2655624
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sun C, 2019, PROC CVPR IEEE, P273, DOI 10.1109/CVPR.2019.00036
   Wang J., 2004, P 12 ANN ACM INT C M, P32
   Wang JJ, 2008, MULTIMEDIA SYST, V14, P179, DOI 10.1007/s00530-008-0112-6
   Wang XT, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2964284.2967265
   Wang XT, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P195, DOI 10.1109/ISM.2014.44
   Xiong B, 2019, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2019.00135
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Yao Ting, 2020, ARXIV200800975
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yu B, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P748, DOI 10.1109/ICIP.1997.638604
   Zhang K, 2018, LECT NOTES COMPUT SC, V11212, P391, DOI 10.1007/978-3-030-01237-3_24
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhang SF, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P1, DOI 10.1109/BTAS.2017.8272675
   Zhao JJ, 2019, PROC CVPR IEEE, P9927, DOI 10.1109/CVPR.2019.01017
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
NR 58
TC 11
Z9 11
U1 4
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 119
DI 10.1145/3448981
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hama, K
   Matsubara, T
   Uehara, K
   Cai, JF
AF Hama, Kenta
   Matsubara, Takashi
   Uehara, Kuniaki
   Cai, Jianfei
TI Exploring Uncertainty Measures for Image-caption Embedding-and-retrieval
   Task
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Uncertainty quantification; Bayesian deep learning; semantic embedding;
   image-caption retrieval
ID NETWORKS
AB With the significant development of black-box machine learning algorithms, particularly deep neural networks, the practical demand for reliability assessment is rapidly increasing. On the basis of the concept that "Bayesian deep learning knows what it does not know," the uncertainty of deep neural network outputs has been investigated as a reliabilitymeasure for classification and regression tasks. By considering an embedding task as a regression task, several existing studies have quantified the uncertainty of embedded features and improved the retrieval performance of cutting-edge models by model averaging. However, in image-caption embedding-and-retrieval tasks, well-known samples are not always easy to retrieve. This study shows that the existing method has poor performance in reliability assessment and investigates another aspect of image-caption embedding-and-retrieval tasks. We propose posterior uncertainty by considering the retrieval task as a classification task, which can accurately assess the reliability of retrieval results. The consistent performance of the two uncertainty measures is observed with different datasets (MS-COCO and Flickr30k), different deep-learning architectures (dropout and batch normalization), and different similarity functions. To the best of our knowledge, this is the first study to perform a reliability assessment on image-caption embedding-and-retrieval tasks.
C1 [Hama, Kenta; Matsubara, Takashi] Osaka Univ, Osaka, Japan.
   [Uehara, Kuniaki] Osaka Gakuin Univ, Osaka, Japan.
   [Cai, Jianfei] Monash Univ, Clayton, Vic, Australia.
C3 Osaka University; Monash University
RP Hama, K (corresponding author), Osaka Univ, Osaka, Japan.
EM hama@hopf.sys.es.osaka-u.ac.jp; matsubara@sys.es.osaka-u.ac.jp;
   kuniaki.uehara@ogu.ac.jp; Jianfei.Cai@monash.edu
RI ; Matsubara, Takashi/C-1698-2017
OI hama, kenta/0000-0002-2338-9229; Matsubara, Takashi/0000-0003-0642-4800
FU MIC/SCOPE [172107101]; JSPS KAKENHI [19H04172]; Grants-in-Aid for
   Scientific Research [19H04172] Funding Source: KAKEN
FX This study was partially supported by the MIC/SCOPE #172107101 and JSPS
   KAKENHI (19H04172).
CR [Anonymous], 2018, ADV NEURAL INFORM PR
   [Anonymous], 2017, ICLR 2017 INT C LEAR
   [Anonymous], 2015, PROC INT C MACH LEAR
   [Anonymous], 2015, INT C MACH LEARN ICM
   [Anonymous], 2017, BRIT MACH VIS C BMVC
   [Anonymous], 2018, AS C MACH LEARN ACML
   [Anonymous], 2015, ADV NEURAL INFORM PR
   [Anonymous], 2018, BRIT MACH VIS C BMVC
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   [Anonymous], 2015, INT C LEARN REPR ICL
   [Anonymous], 2016, INT C MACH LEARN ICM
   Atanov A., 2018, INT C LEARN REPR WOR
   Barber David, 1997, ADV NEURAL INFORM PR
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Yoshua., 2012, P ICML WORKSH UNS TR
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Chen W.-Y., 2019, EUR J INORG CHEM
   Chen Y., 2019, ABS190202586 CORR
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Engilberge M, 2018, PROC CVPR IEEE, P3984, DOI 10.1109/CVPR.2018.00419
   Frome A., 2013, P ADV NEUR INF PROC
   Gal Y., 2016, THESIS
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hein M, 2019, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2019.00013
   Hinton Geoffrey E., 1993, ANN C COMP LEARN THE
   Huang P.-Y., 2018, EUR C COMP VIS ECCV
   Jain H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P935, DOI 10.1145/2939672.2939756
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Kendall A, 2017, 31 ANN C NEURAL INFO, V30
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Kiureghian Armen Der, 2009, STRUCT SAF, V31, P2
   Leibig Christian, 2016, NIPS WORKSH
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li YC, 2017, PROC CVPR IEEE, P1837, DOI 10.1109/CVPR.2017.199
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   MacKay David J. C, 1992, Neural computation, V4, P3
   Matsubara Takashi, 2018, IEEE IJCNN
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Rohrbach Anna, 2018, C EMP METH NAT LANG
   Rosasco L, 2004, NEURAL COMPUT, V16, P1063, DOI 10.1162/089976604773135104
   Salimans T., 2017, ADV NEURAL INFORM PR
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Smith L, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P560
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Taha Ahmed, 2019, ABS190107702 CORR
   Watanabe Sumio, 2010, NEURAL NETWORKS, V23, P1
   Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3
   Xiao Yijun, 2019, AAAI C ART INT AAAI
   Yagcioglu Semih, 2018, C EMP METH NAT LANG
   Young M. H. Peter, 2014, T ASS COMPUT LING, V2
   Zhang Hongyi, 2018, 3 INT C LEARNING REP
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang QS, 2018, AAAI CONF ARTIF INTE, P4464
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
NR 60
TC 1
Z9 1
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 46
DI 10.1145/3425663
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sharma, PK
   Ghosh, S
   Sur, A
AF Sharma, Prasen Kumar
   Ghosh, Sujoy
   Sur, Arijit
TI High-quality Frame Recurrent Video De-raining with Multi-contextual
   Adversarial Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video de-raining; deep learning; generative adversarial network
ID STRUCTURAL SIMILARITY; IMAGE; REMOVAL
AB In this article, we address the problem of rain-streak removal in the videos. Unlike the image, challenges in video restoration comprise temporal consistency besides spatial enhancement. The researchers across the world have proposed several effective methods for estimating the de-noised videos with outstanding temporal consistency. However, such methods also amplify the computational cost due to their larger size. By way of analysis, incorporating separate modules for spatial and temporal enhancement may require more computational resources. It motivates us to propose a unified architecture that directly estimates the de-rained frame with maximal visual quality and minimal computational cost. To this end, we present a deep learning-based Frame-recurrent Multi-contextual Adversarial Network for rain-streak removal in videos. The proposed model is built upon a Conditional Generative Adversarial Network (CGAN)-based framework where the generator model directly estimates the de-rained frame from the previously estimated one with the help of its multicontextual adversary. To optimize the proposed model, we have incorporated the Perceptual loss function in addition to the conventional Euclidean distance. Also, instead of traditional entropy loss from the adversary, we propose to use the Euclidean distance between the features of de-rained and clean frames, extracted from the discriminator model as a cost function for video de-raining. Various experimental observations across 11 test sets, with over 10 state-of-the-art methods, using 14 image-quality metrics, prove the efficacy of the proposed work, both visually and computationally.
C1 [Sharma, Prasen Kumar; Ghosh, Sujoy; Sur, Arijit] Indian Inst Technol Guwahati, Dept Comp Sci & Eng, Multimedia Lab, Bongara 781039, Assam, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Guwahati
RP Sharma, PK (corresponding author), Indian Inst Technol Guwahati, Dept Comp Sci & Eng, Multimedia Lab, Bongara 781039, Assam, India.
EM kumar176101005@iitg.ac.in; ghoshsujoy19@iitg.ac.in; arijit@iitg.ac.in
RI Sur, Arijit/AAB-4216-2020
FU Ministry of Human Resource Development, Government of India
   [BT/COE/34/SP28408/2018]
FX We gratefully acknowledge the funding agency, Ministry of Human Resource
   Development, Government of India. Further, for computing resources, we
   acknowledge the Department of Biotechnology, Govt. of India, for the
   financial support for the project BT/COE/34/SP28408/2018.
CR [Anonymous], ACM T MULTIMEDIA COM, V17
   [Anonymous], P INT C MACH LEARN L
   Barnum PC, 2010, INT J COMPUT VISION, V86, P256, DOI 10.1007/s11263-008-0200-2
   Chen J, 2018, PROC CVPR IEEE, P6286, DOI 10.1109/CVPR.2018.00658
   Chen J, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2013.2290595
   Divakar N, 2017, IEEE COMPUT SOC CONF, P1076, DOI 10.1109/CVPRW.2017.145
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Garg K, 2005, IEEE I CONF COMP VIS, P1067
   Garg K, 2004, PROC CVPR IEEE, P528
   Garg K, 2007, INT J COMPUT VISION, V75, P3, DOI 10.1007/s11263-006-0028-6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang TX, 2019, IEEE T IMAGE PROCESS, V28, P2089, DOI 10.1109/TIP.2018.2880512
   Jiang TX, 2017, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2017.301
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim JH, 2015, IEEE T IMAGE PROCESS, V24, P2658, DOI 10.1109/TIP.2015.2428933
   Kingma D. P., 2014, arXiv
   Li MH, 2018, PROC CVPR IEEE, P6644, DOI 10.1109/CVPR.2018.00695
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Lian NX, 2006, IEEE T IMAGE PROCESS, V15, P2575, DOI 10.1109/TIP.2006.877409
   Liu JY, 2018, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2018.00341
   Liu JY, 2019, IEEE T IMAGE PROCESS, V28, P699, DOI 10.1109/TIP.2018.2869722
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   Paszke A., 2017, NIPS 2017 WORKSH AUT
   Ren WH, 2017, PROC CVPR IEEE, P2838, DOI 10.1109/CVPR.2017.303
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Sharma Prasen, 2020, P IEEE WINT C APPL C
   Sharma PK, 2019, IEEE IMAGE PROC, P2796, DOI [10.1109/icip.2019.8803353, 10.1109/ICIP.2019.8803353]
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Simonyan K, 2015, IEEE INT C ICLR
   Sun SH, 2014, IEEE IMAGE PROC, P4482, DOI 10.1109/ICIP.2014.7025909
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wei W, 2017, IEEE I CONF COMP VIS, P2535, DOI 10.1109/ICCV.2017.275
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang WH, 2019, PROC CVPR IEEE, P1661, DOI 10.1109/CVPR.2019.00176
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Zhang H., 2019, IEEE Trans Circ Syst Vid Technol
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P1975, DOI 10.1109/TCSVT.2019.2912145
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P461, DOI 10.1109/ICME.2006.262572
NR 47
TC 1
Z9 1
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 56
DI 10.1145/3444974
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000019
DA 2024-07-18
ER

PT J
AU Chehabeddine, S
   Jamil, MH
   Park, W
   Sefo, DL
   Loomer, PM
   Eid, M
AF Chehabeddine, Said
   Jamil, Muhammad Hassan
   Park, Wanjoo
   Sefo, Dianne L.
   Loomer, Peter M.
   Eid, Mohamad
TI Bi-manual Haptic-based Periodontal Simulation with Finger Support and
   Vibrotactile Feedback
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Periodontal training; dental simulation; vibrotactile feedback
ID EDUCATION
AB The rise of virtual reality and haptic technologies has created exciting new applications in medical training and education. In a dental simulation, haptic technology can create the illusion of substances (teeth, gingiva, bone, etc.) by providing interaction forces within a simulated virtual world of the mouth. In this article, a haptic periodontal training simulation system, named Haptodont, is developed and evaluated for simulating periodontal probing. Thirty-two faculty members from New York University College of Dentistry were recruited and divided into three groups to evaluate three fundamental functionalities: Group 1 evaluated bi-manual 3 Degrees of Freedome (DoF) haptic interaction, Group 2 evaluated bi-manual 3 DoF haptic interaction with a finger support mechanism, and Group 3 evaluated bi-manual 3 DoF haptic interaction with finger support mechanism and vibrotactile feedback. The probe and mirror interactions were simulated with the Geomagic Touch haptic device whereas the finger support was implemented using the Novint Falcon device. The three groups conducted two probing tasks: healthy gingiva scenario with no pockets (2- to 3-mm depth) and periodontitis scenario with deep pockets (4- to 8-mm depth). Results demonstrated that experts performed comparably to clinical settings in terms of probing depth error (within 0.3 to 0.6 mm) and probing forces (less than 0.5 N). Furthermore, the finger support mechanism significantly improved the probing accuracy for periodontitis condition in the lingual region. The argument that probing the lingual region is more difficult than the buccal region is supported by quantitative evidence (significantly higher probing depth error and probing force). Further research is planned to improve the usability of the finger support, integrate the Haptodont system into the pre-clinical curriculum, and evaluate the Haptodont system with dental students as a learning tool.
C1 [Chehabeddine, Said; Jamil, Muhammad Hassan; Park, Wanjoo; Eid, Mohamad] New York Univ Abu Dhabi, POB 129188, Abu Dhabi, U Arab Emirates.
   [Sefo, Dianne L.] NYU, Coll Dent, 345 E 24th St,Room 622S, New York, NY USA.
   [Loomer, Peter M.] Univ Texas San Antonio, Hlth Sci Ctr, Sch Dent, Room 312W Dent Ctr,421 First Ave, San Antonio, TX 78284 USA.
C3 New York University Abu Dhabi; New York University; University of Texas
   System; University of Texas Health Science Center at San Antonio
RP Chehabeddine, S (corresponding author), New York Univ Abu Dhabi, POB 129188, Abu Dhabi, U Arab Emirates.
EM sc7388@nyu.edu; hassan.jamil@nyu.edu; wanjoo@nyu.edu; dlp6@nyu.edu;
   loomer@uthscsa.edu; mohamad.eid@nyu.edu
OI Park, Wanjoo/0000-0003-1467-4156
FU New York University Abu Dhabi Research Enhancement Fund [RE217]
FX This research is funded by the New York University Abu Dhabi Research
   Enhancement Fund (RE217).
CR Abdulali A, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010237
   Al Shayeb Kwthar Nassar A, 2014, Prim Dent J, V3, P25
   [Anonymous], 2021, ACM T MULTIM COMPUT, V17
   [Anonymous], 2019, CHAI3D ONLINE DOCUME
   Barendregt Dick Steven, 2009, THESIS U AMSTERDAM
   Coles T R., 2009, ACHI '09. Second International Conferences, P193
   Culjat M, 2008, IND ROBOT, V35, P449, DOI 10.1108/01439910810893617
   Danieau F, 2013, IEEE T HAPTICS, V6, P193, DOI [10.1109/TOH.2012.70, 10.1109/ToH.2012.70]
   Dong Hui, 2005, J Dent Educ, V69, P453
   FLEISS JL, 1991, J PERIODONTAL RES, V26, P122, DOI 10.1111/j.1600-0765.1991.tb01635.x
   Forsslund Jonas, 2009, 2009 World Haptics Conference (WHC 2009), P391, DOI 10.1109/WHC.2009.4810916
   Hefti AF, 1997, CRIT REV ORAL BIOL M, V8, P336, DOI 10.1177/10454411970080030601
   Hollis Wainscott, 2011, J Tenn Dent Assoc, V91, P14
   Holtfreter B, 2012, J CLIN PERIODONTOL, V39, P1032, DOI 10.1111/j.1600-051X.2012.01941.x
   Huegel Joel C., 2010, 2010 IEEE Haptics Symposium (Formerly known as Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems), P343, DOI 10.1109/HAPTIC.2010.5444632
   Issenberg SB, 2005, MED TEACH, V27, P10, DOI 10.1080/01421590500046924
   Johnson L, 2000, J Dent Educ, V64, P847
   Karafotias G, 2017, 2017 15TH IEEE INTERNATIONAL SYMPOSIUM ON HAPTIC, AUDIO AND VISUAL ENVIRONMENTS AND GAMES (HAVE), P41
   Kolesnikov M, 2009, IEEE INT CONF ROBOT, P3611
   Kuchenbecker KJ, 2017, SIMUL HEALTHC, V12, P148, DOI 10.1097/SIH.0000000000000201
   Lanning Sharon K, 2005, J Dent Educ, V69, P325
   Luciano C, 2009, VIRTUAL REAL-LONDON, V13, P69, DOI 10.1007/s10055-009-0112-7
   McGaghie WC, 2010, MED EDUC, V44, P50, DOI 10.1111/j.1365-2923.2009.03547.x
   Motola I, 2013, MED TEACH, V35, pE1511, DOI 10.3109/0142159X.2013.818632
   NIH Publication, 2013, NIH PUBL, V46, P13
   Online Data Sheet, 2019, PRECISION MICRODRIVE
   OSBORN JB, 1992, J PERIODONTOL, V63, P283, DOI 10.1902/jop.1992.63.4.283
   Palat Milton, 2014, COMPREHENSIVE PERIOD
   Preshaw PM, 2015, BMC ORAL HEALTH, V15, DOI 10.1186/1472-6831-15-S1-S5
   Ranta J. F, 1999, P 4 PHANTOM US GROUP
   Roy E, 2017, SAUDI DENT J, V29, P41, DOI 10.1016/j.sdentj.2017.02.001
   Ruspini D. C., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P345, DOI 10.1145/258734.258878
   Sarakoglou I, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P3587, DOI 10.1109/IROS.2006.281649
   Sigrist R, 2013, PSYCHON B REV, V20, P21, DOI 10.3758/s13423-012-0333-8
   Sunaga M, 2016, J DENT EDUC, V80, P1430
   The American Board of Dental Examiners, 2019, PAT TREATM CLIN EX M
   Thomas G, 2001, COMPUT METH PROG BIO, V64, P53, DOI 10.1016/S0169-2607(00)00089-4
   van Breda Eric, 2017, BMJ Open Sport Exerc Med, V3, pe000216, DOI 10.1136/bmjsem-2016-000216
   Wang DX, 2012, IEEE T HAPTICS, V5, P332, DOI [10.1109/TOH.2011.59, 10.1109/ToH.2011.59]
NR 39
TC 6
Z9 6
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 28
DI 10.1145/3421765
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200008
DA 2024-07-18
ER

PT J
AU Cinar, Y
   Pocta, P
   Chambers, D
   Melvin, H
AF Cinar, Yusuf
   Pocta, Peter
   Chambers, Desmond
   Melvin, Hugh
TI Improved Jitter Buffer Management for WebRTC
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE WebRTC; jitter buffer management; voice quality
AB This work studies the jitter buffer management algorithm for Voice over IP in WebRTC. In particular, it details the core concepts of WebRTC's jitter buffer management. Furthermore, it investigates how jitter buffer management algorithm behaves under network conditions with packet bursts. It also proposes an approach, different from the default WebRTC algorithm, to avoid distortions that occur under such network conditions. Under packet bursts, when the packet buffer becomes full, the WebRTC jitter buffer algorithm may discard all the packets in the buffer to make room for incoming packets. The proposed approach offers a novel strategy to minimize the number of packets discarded in the presence of packet bursts. Therefore, voice quality as perceived by the user is improved. ITU-T Rec. P.863, which also confirms the improvement, is employed to objectively evaluate the listening quality.
C1 [Cinar, Yusuf; Chambers, Desmond; Melvin, Hugh] Natl Univ Ireland, Sch Comp Sci, Univ Rd, Galway, Ireland.
   [Pocta, Peter] Univ Zilina, Fac Elect Engn & Informat Technol, Dept Multimedia & Informat Communicat Technol, Univ 8215-1, SK-01026 Zilina, Slovakia.
C3 Ollscoil na Gaillimhe-University of Galway; University of Zilina
RP Cinar, Y (corresponding author), Natl Univ Ireland, Sch Comp Sci, Univ Rd, Galway, Ireland.
EM cinar.yusuf@gmail.com; peter.pocta@feit.uniza.sk;
   des.chambers@nuigalway.ie; hugh.melvin@nuigalway.ie
RI Pocta, Peter/A-6228-2010; ÇINAR, Yusuf/HCH-3974-2022
OI Pocta, Peter/0000-0001-6791-1325; 
CR ABIResearch, 2013, 4 7 BILL MOB DEV 201
   Al-Ahmadi Mohannad, 2016, P 5 ISCA DEGA WORKSH
   Alvestrand H. T., 2021, 8825 RFC
   [Anonymous], P 9 INT C QUAL MULT
   [Anonymous], 2019, 26448 3GPP TS
   [Anonymous], 2016, COMMUN SCI LETT U ZI
   Bergkvist Adam, 2021, WEBRTC 1 0 REAL TIME
   Cinar Y, 2014, 2014 10TH INTERNATIONAL CONFERENCE ON DIGITAL TECHNOLOGIES (DT), P31
   Côté N, 2011, T-LAB SER TELECOMMUN, P1, DOI 10.1007/978-3-642-18463-5
   Global IP Solutions, 2007, GIPS NETEQ
   ITU-T, 2018, REC ITU T P863 PERC
   ITU-T, 2003, REC IUT T G114 1 WAY
   ITU-T, 2017, REC ITU T P501 TEST
   Liang YJ, 2003, IEEE T MULTIMEDIA, V5, P532, DOI 10.1109/TMM.2003.819095
   Lundin Henrik Fahlberg, 2010, US Patent, Patent No. [7,733,893, 7733893]
   Majed N., 2018, THESIS ECOLE NATL SU
   Melvin B. E. Hugh, 2004, THESIS U COLL DUBLIN
   Oklander B, 2008, IEEE IC COMP COM NET, P75
   Pocta P., 2016, COMMUNICATIONS SCIEN, V18, P17
   Raake A., 2006, Speech Quality of VoIP: Assessment and Prediction
   Skoglund J., 2008, HDB SPEECH PROCESSIN, P307
   WebRTC Team, 2017, WEBRTC
   Wikipedia contributors, 2019, GLOB IP SOL
NR 23
TC 4
Z9 4
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 30
DI 10.1145/3410449
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200010
DA 2024-07-18
ER

PT J
AU Liu, YD
   Yang, SY
   Li, B
   Zhou, WG
   Xu, JZ
   Li, HQ
   Lu, Y
AF Liu, Yiding
   Yang, Siyu
   Li, Bin
   Zhou, Wengang
   Xu, Jizheng
   Li, Houqiang
   Lu, Yan
TI Affinity Derivation for Accurate Instance Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Instance segmentation; semantic segmentation; graph
AB Affinity, which represents whether two pixels belong to a same instance, is an equivalent representation to the instance segmentation labels. Conventional works do not make an explicit exploration on the affinity. In this article, we present two instance segmentation schemes based on pixel affinity information and show the effectiveness of affinity in both aspects. For proposal-free method, we predict pixel affinity for each image and then propose a simple yet effective graph merge algorithm to cluster pixels into instances. It shows that the affinity is powerful as an instance-relevant information to guide the clustering procedure in proposal-free instance segmentation. For proposal-based methods, we extend conventional framework with affinity head and introduce affinity as attached supervision in training phase. Without any additional inference cost, we can improve the performance of existing proposal-based instance segmentation methods, which shows that the affinity can also be applied as an auxiliary loss and training with such extra loss is beneficial to the training progress. Experimental results show that our schemes achieve comparable performance to other state-of-the-art instance segmentation methods. With Cityscapes training data, the proposed proposal-free method achieves 28.8 AP and the proposal-based method gets 27.2 AP both on test sets.
C1 [Liu, Yiding; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
   [Yang, Siyu] Airbnb Informat Technol Beijing Co Ltd, Beijing, Peoples R China.
   [Li, Bin; Xu, Jizheng; Lu, Yan] Microsoft Res, Beijing, Peoples R China.
   [Zhou, Wengang; Li, Houqiang] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Hefei, Anhui, Peoples R China.; Li, B (corresponding author), Microsoft Res, Beijing, Peoples R China.; Zhou, WG; Li, HQ (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Anhui, Peoples R China.
EM liuyd123@mail.ustc.edu.cn; siyu.yang@airbnb.com; libin@microsoft.com;
   zhwg@ustc.edu.cn; jzxu@microsoft.com; lihq@ustc.edu.cn;
   yanlu@microsoft.com
RI Li, Houqiang Li/B-6259-2013; Xu, Jizheng/JDD-5152-2023
FU NSFC [61836011, 61822208, 61632019]; Youth Innovation Promotion
   Association CAS [2018497]
FX This work was supported in part to Dr. Houqiang Li by NSFC under
   contract No. 61836011, and in part to Dr. Wengang Zhou by NSFC under
   contract No. 61822208 & 61632019 and Youth Innovation Promotion
   Association CAS (No. 2018497). Authors' addresses: Y. Liu, University of
   Science and Technology of China, Hefei, Anhui, China; email:
   liuyd123@mail.ustc.edu.cn;S.Yang, Airbnb Information Technology
   (Beijing) Co., Ltd. Beijing, China; email:siyu.yang@airbnb.com;B.Li
   (corresponding author), J. Xu, and Y. Lu, Microsoft Research, Beijing,
   China; emails:
   libin@microsoft.com,jzxu@microsoft.com,yanlu@microsoft.com;W.Zhou
   (corresponding author) and H. Li (corresponding author), University of
   Science and Technology of China, Hefei, Anhui, China, Institute of
   Artificial Intelligence, Hefei Comprehensive National Science Center,
   Hefei, Anhui, China; emails: zhwg@ustc.edu.cn,lihq@ustc.edu.cn.
CR Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Arnab A, 2017, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2017.100
   Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   De Brabandere B, 2017, IEEE COMPUT SOC CONF, P478, DOI 10.1109/CVPRW.2017.66
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Fathi Alireza, 2017, Semantic instance segmentation via deep metric learning
   Fu J., 2017, ARXIV PREPRINT ARXIV
   Gao NY, 2019, IEEE I CONF COMP VIS, P642, DOI 10.1109/ICCV.2019.00073
   Garcia A, 2017, APPR DIGIT GAME STUD, V5, P1
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   Hayder Z, 2017, PROC CVPR IEEE, P587, DOI 10.1109/CVPR.2017.70
   Hayder Zeeshan, 2016, ARXIV PREPRINT ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu Y.-C., 2018, P 2018 INT JOINT C N, P1, DOI DOI 10.1109/IVCNZ.2018.8634799
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Islam MA, 2017, PROC CVPR IEEE, P4877, DOI 10.1109/CVPR.2017.518
   Jin Long, 2016, ARXIV PREPRINT ARXIV
   Ke TW, 2018, LECT NOTES COMPUT SC, V11205, P605, DOI 10.1007/978-3-030-01246-5_36
   Kendall Alex, 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00781
   Keuper M, 2015, IEEE I CONF COMP VIS, P1751, DOI 10.1109/ICCV.2015.204
   Kirillov A, 2017, PROC CVPR IEEE, P7322, DOI 10.1109/CVPR.2017.774
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Levinkov E, 2017, PROC CVPR IEEE, P1904, DOI 10.1109/CVPR.2017.206
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI [10.1007/978-3-030-01240-3_21, 10.1007/978-3-030-01219-9_23]
   Liang XD, 2018, IEEE T PATTERN ANAL, V40, P2978, DOI 10.1109/TPAMI.2017.2775623
   Liang Xiaodan, 2015, ARXIV PREPRINT ARXIV
   Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu S, 2017, IEEE I CONF COMP VIS, P3516, DOI 10.1109/ICCV.2017.378
   Liu S, 2016, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2016.342
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Neven D, 2019, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR.2019.00904
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pinheiro PO, 2015, ADV NEUR IN, V28
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Tu WC, 2018, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2018.00066
   Uhrig J, 2016, LECT NOTES COMPUT SC, V9796, P14, DOI 10.1007/978-3-319-45886-1_2
   Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272
   Xiong YW, 2019, PROC CVPR IEEE, P8810, DOI 10.1109/CVPR.2019.00902
   Yu FS, 2016, Arxiv, DOI [arXiv:1511.07122, DOI 10.48550/ARXIV.1511.07122]
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhuang YQ, 2018, INT C PATT RECOG, P1506, DOI 10.1109/ICPR.2018.8545708
NR 65
TC 0
Z9 0
U1 2
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 34
DI 10.1145/3407090
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200014
DA 2024-07-18
ER

PT J
AU Wang, WJ
   Duan, LY
   Jiang, H
   Jing, PG
   Song, XM
   Nie, LQ
AF Wang, Wenjie
   Duan, Ling-Yu
   Jiang, Hao
   Jing, Peiguang
   Song, Xuemeng
   Nie, Liqiang
TI Market2Dish: Health-aware Food Recommendation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE User health profiling; health-aware food recommendation; recipe
   retrieval
AB With the rising incidence of some diseases, such as obesity and diabetes, the healthy diet is arousing increasing attention. However, most existing food-related research efforts focus on recipe retrieval, user-preference-based food recommendation, cooking assistance, or the nutrition and calorie estimation of dishes, ignoring the personalized health-aware food recommendation. Therefore, in this work, we present a personalized health-aware food recommendation scheme, namely, Market2Dish, mapping the ingredients displayed in the market to the healthy dishes eaten at home. The proposed scheme comprises three components, namely, recipe retrieval, user health profiling, and health-aware food recommendation. In particular, recipe retrieval aims to acquire the ingredients available to the users and then retrieve recipe candidates from a large-scale recipe dataset. User health profiling is to characterize the health conditions of users by capturing the textual health-related information crawled from social networks. Specifically, to solve the issue that the health-related information is extremely sparse, we incorporate a word-class interaction mechanism into the proposed deep model to learn the fine-grained correlations between the textual tweets and pre-defined health concepts. For the health-aware food recommendation, we present a novel category-aware hierarchical memory network-based recommender to learn the health-aware user-recipe interactions for better food recommendation. Moreover, extensive experiments demonstrate the effectiveness of the health-aware food recommendation scheme.
C1 [Wang, Wenjie] Natl Univ Singapore, Singapore 117417, Singapore.
   [Duan, Ling-Yu] Peking Univ, Beijing 100871, Peoples R China.
   [Jiang, Hao; Song, Xuemeng; Nie, Liqiang] Shandong Univ, Qingdao 266237, Peoples R China.
   [Jing, Peiguang] Tianjin Univ, Tianjin, Peoples R China.
C3 National University of Singapore; Peking University; Shandong
   University; Tianjin University
RP Wang, WJ (corresponding author), Natl Univ Singapore, Singapore 117417, Singapore.
EM wenjiewang96@gmail.com; lingyu@pku.edu.cn; jianghaosdu@mail.sdu.edu.cn;
   pgjing@tju.edu.cn; sxmustc@gmail.com; nieliqiang@gmail.com
FU National Key Research and Development Project of New Generation
   Artificial Intelligence [2018AAA0102502]; National Natural Science
   Foundation of China [61772310, U1936203]; Shandong Provincial Natural
   Science Foundation [ZR2019JQ23]; Innovation Teams in Colleges and
   Universities in Jinan [2018GXRC014]; Shandong Provincial Key Research
   and Development Program [2019JZZY010118]
FX This work is supported by the National Key Research and Development
   Project of New Generation Artificial Intelligence, No.:2018AAA0102502;
   the National Natural Science Foundation of China, No.:61772310, and
   No.:U1936203; the Shandong Provincial Natural Science Foundation,
   No.:ZR2019JQ23; the Innovation Teams in Colleges and Universities in
   Jinan, No.:2018GXRC014; the Shandong Provincial Key Research and
   Development Program, No.:2019JZZY010118.
CR Abbar S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3197, DOI 10.1145/2702123.2702153
   An YS, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1269, DOI 10.1145/3123266.3126490
   [Anonymous], 2015, P 9 ACM C REC SYST N
   [Anonymous], 2016, P C N AM CHAPT ASS C, DOI DOI 10.18653/V1/N16-1170
   [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   [Anonymous], 2008, Proceeding of the 16th ACM international conference on Multimedia-MM'08, DOI [DOI 10.1145/1459359, 10.1145/1459359]
   [Anonymous], 2008, P 17 INT C WORLD WID, DOI DOI 10.1145/1367497.1367629
   Iglesias JA, 2012, IEEE T KNOWL DATA EN, V24, P854, DOI 10.1109/TKDE.2011.17
   Barkan Oren, 2016, IEEE INT WORKSHOP MA
   Bayer I, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1341, DOI 10.1145/3038912.3052694
   Blansche Alexandre, 2010, P INT C CAS BAS REAS, P189
   Carvalho M, 2018, ACM/SIGIR PROCEEDINGS 2018, P35, DOI 10.1145/3209978.3210036
   Chen JJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1020, DOI 10.1145/3240508.3240627
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2017, LECT NOTES COMPUT SC, V10132, P588, DOI 10.1007/978-3-319-51811-4_48
   Cheng Z., 2014, Proceedings of international conference on multimedia retrieval p, P185, DOI [DOI 10.1145/2578726.2578751, 10.1145/2578726.2578751]
   Cheng ZY, 2016, ACM T INFORM SYST, V34, DOI 10.1145/2846092
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   De Choudhury M, 2016, ACM CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW 2016), P1157, DOI 10.1145/2818048.2819956
   Du CX, 2019, AAAI CONF ARTIF INTE, P6359
   Elsweiler D, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P575, DOI 10.1145/3077136.3080826
   Farseev A, 2017, ACM T INFORM SYST, V35, DOI 10.1145/3086676
   Feng FL, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1523, DOI 10.1145/3178876.3186064
   Freyne J, 2010, IUI 2010, P321
   Ge M., 2015, P 5 INT C DIGITAL HL, P105, DOI DOI 10.1145/2750511.2750528
   Harashima J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1229, DOI 10.1145/3077136.3080686
   Harvey M, 2013, LECT NOTES COMPUT SC, V8214, P153, DOI 10.1007/978-3-319-02432-5_19
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   He XN, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P549, DOI 10.1145/2911451.2911489
   Herlocker JL, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P230, DOI 10.1145/312624.312682
   Jiang H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2188, DOI 10.1145/3343031.3350594
   Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068
   Kagaya H, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1085, DOI 10.1145/2647868.2654970
   Kusmierczyk T, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P55, DOI 10.1145/2740908.2742752
   Lai SW, 2015, AAAI CONF ARTIF INTE, P2267
   Li YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1464, DOI 10.1145/3343031.3350950
   Mejova Y., 2015, DH 15, P51, DOI [10.1145/2750511.2750524, DOI 10.1145/2750511.2750524]
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Min WQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1331, DOI 10.1145/3343031.3350948
   Min WQ, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329168
   Min WQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P402, DOI 10.1145/3123266.3123272
   Min WQ, 2018, IEEE T MULTIMEDIA, V20, P950, DOI 10.1109/TMM.2017.2759499
   Nie LQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1098, DOI 10.1145/3343031.3350923
   Ofli F, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P509, DOI 10.1145/3038912.3052663
   Pouladzadeh P, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3063592
   Rokicki Markus, P INT C WEB SOC MED, P310
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Sanjo S, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2279, DOI 10.1145/3132847.3133137
   Shidochi Yuka, 2009, ACM 2009 WORKSHOP MU, P9, DOI DOI 10.1145/1630995.1630998MM
   Simonyan K., 2014, 14091556 ARXIV
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Trattner C, 2017, ARXIV
   Trattner C, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P489, DOI 10.1145/3038912.3052573
   Wang W., 2020, ARXIV PREPRINT ARXIV
   Wang WJ, 2018, ACM/SIGIR PROCEEDINGS 2018, P255, DOI 10.1145/3209978.3210061
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wolf Lior., 2017, P 15 C EUR CHAPT ASS, V2, P157
   Wu H, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P172, DOI 10.1145/2964284.2967205
   Yang S, 2010, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2010.5539907
   Yang Z., 2016, P 2016 C N AM CHAPT, P1480, DOI [DOI 10.18653/V1/N16-1174, 10.18653/v1/n16-1174]
   Yi Xun, 2019, T KNOWL DATA ENG, V32, P1572
   Yiming Yang, 1999, Information Retrieval, V1, P69, DOI 10.1023/A:1009982220290
   Zhao Z, 2016, IEEE T KNOWL DATA EN, V28, P2522, DOI 10.1109/TKDE.2016.2569096
NR 63
TC 28
Z9 28
U1 7
U2 39
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 33
DI 10.1145/3418211
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200013
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Fang, LM
   Yin, CC
   Zhu, JC
   Ge, CP
   Tanveer, M
   Jolfaei, A
   Cao, ZH
AF Fang, Liming
   Yin, Changchun
   Zhu, Juncen
   Ge, Chunpeng
   Tanveer, M.
   Jolfaei, Alireza
   Cao, Zehong
TI Privacy Protection for Medical Data Sharing in Smart Healthcare
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Medical data privacy; attribute-based encryption; smart healthcare;
   access control
ID ATTRIBUTE-BASED ENCRYPTION; RECORDS; INFORMATION; REVOCATION; FRAMEWORK
AB In virtue of advances in smart networks and the cloud computing paradigm, smart healthcare is transforming. However, there are still challenges, such as storing sensitive data in untrusted and controlled infrastructure and ensuring the secure transmission of medical data, among others. The rapid development of watermarking provides opportunities for smart healthcare. In this article, we propose a new data-sharing framework and a data access control mechanism. The applications are submitted by the doctors. and the data is processed in the medical data center of the hospital, stored in semi-trusted servers to support the selective sharing of electronic medical records from different medical institutions between different doctors. Our approach ensures that privacy concerns are taken into account when processing requests for access to patients' medical information. For accountability, after data is modified or leaked, both patients and doctors must add digital watermarks associated with their identification when uploading data. Extensive analytical and experimental results are presented that show the security and efficiency of our proposed scheme.
C1 [Fang, Liming] Nanjing Univ Aeronaut & Astronaut & Collaborat In, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Fang, Liming] Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 210023, Peoples R China.
   [Yin, Changchun; Zhu, Juncen; Ge, Chunpeng] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Tanveer, M.] Indian Inst Technol Indore, Indore, Madhya Pradesh, India.
   [Jolfaei, Alireza] Macquarie Univ, Macquarie Pk, Sydney, NSW, Australia.
   [Cao, Zehong] Univ Tasmania, Discipline ICT, Hobart, Tas 7001, Australia.
C3 Nanjing University of Aeronautics & Astronautics; Indian Institute of
   Technology System (IIT System); Indian Institute of Technology (IIT) -
   Indore; Macquarie University; University of Tasmania
RP Cao, ZH (corresponding author), Univ Tasmania, Discipline ICT, Hobart, Tas 7001, Australia.
EM fangliming@nuaa.edu.cn; ycc0801@nuaa.edu.cn; pudding@nuaa.edu.cn;
   gecp@nuaa.edu.cn; mtanveer@iiti.ac.in; alireza.jolfaei@mq.edu.au;
   Zehong.Cao@utas.edu.au
RI Tanveer, Mohammad/I-4585-2013; Jolfaei, Alireza/GQH-6907-2022; Cao,
   Zehong/O-7351-2016
OI Tanveer, Mohammad/0000-0002-5727-3697; Cao, Zehong/0000-0003-3656-0328;
   Jolfaei, Alireza/0000-0001-7818-459X; Yin, Changchun/0000-0002-4153-5963
FU National Key R&D Program of China [2017YFC1201200, 2019YFA0803000];
   National Natural Science Foundation of China [61872181, 61702236];
   National Science Foundation for Postdoctoral Scientists of China
   [2019M651826]; Natural Science Foundation of Jiangsu Province
   [BK20180421]; National Cryptography Development Fund [MMJJ20180105];
   Fundamental Research Funds for the Central Universities [NE2018106];
   JSPS [JP19J15225]; JSPS Kiban(B) [18H03240]; JSPS Kiban(C) [18K11298];
   H2020-SU-ICT-03-2018: CyberSec4Europe [830929]
FX This work was supported in part by the National Key R&D Program of China
   under grants 2017YFC1201200 and 2019YFA0803000, in part by the National
   Natural Science Foundation of China under grants 61872181 and 61702236,
   in part by the National Science Foundation for Postdoctoral Scientists
   of China under grant 2019M651826, in part by the Natural Science
   Foundation of Jiangsu Province under grant BK20180421, in part by the
   National Cryptography Development Fund under grant MMJJ20180105, in part
   by the Fundamental Research Funds for the Central Universities under
   grant NE2018106, in part by JSPS Grant-in-Aid for Scientific Research
   (DC2) under grant JP19J15225, in part by JSPS Kiban(B) under grant
   18H03240, in part by JSPS Kiban(C) under grant 18K11298, and in part by
   H2020-SU-ICT-03-2018: CyberSec4Europe under grant 830929.
CR Attrapadung N, 2011, LECT NOTES COMPUT SC, V6571, P90, DOI 10.1007/978-3-642-19379-8_6
   Attrapadung N, 2009, LECT NOTES COMPUT SC, V5921, P278, DOI 10.1007/978-3-642-10868-6_17
   Au MH, 2017, J COMPUT SYST SCI, V90, P46, DOI 10.1016/j.jcss.2017.03.002
   Bethencourt J, 2007, P IEEE S SECUR PRIV, P321, DOI 10.1109/sp.2007.11
   Boldyreva A, 2008, CCS'08: PROCEEDINGS OF THE 15TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P417
   Boneh D, 2003, SIAM J COMPUT, V32, P586, DOI 10.1137/S0097539701398521
   di Vimercati S.D. C., 2007, P 33 INT C VERY LARG, P123
   Ding WX, 2020, IEEE T DEPEND SECURE, V17, P363, DOI 10.1109/TDSC.2017.2786247
   Goyal V., 2006, P 13 ACM C COMP COMM, P89, DOI DOI 10.1145/1180405.1180418
   Kamran M, 2012, IEEE T KNOWL DATA EN, V24, P1950, DOI 10.1109/TKDE.2011.223
   Lewko A, 2010, LECT NOTES COMPUT SC, V6110, P62, DOI 10.1007/978-3-642-13190-5_4
   Li JG, 2018, IEEE SYST J, V12, P1767, DOI 10.1109/JSYST.2017.2667679
   Li JG, 2019, INFORM SCIENCES, V470, P175, DOI 10.1016/j.ins.2018.07.077
   Li M, 2013, IEEE T PARALL DISTR, V24, P131, DOI 10.1109/TPDS.2012.97
   Liu JH, 2015, FUTURE GENER COMP SY, V52, P67, DOI 10.1016/j.future.2014.10.014
   Mamlin BW, 2016, AM J MED SCI, V351, P59, DOI 10.1016/j.amjms.2015.10.015
   Miao YB, 2016, J MED SYST, V40, DOI 10.1007/s10916-016-0617-z
   Ostrovsky R, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P195, DOI 10.1145/1315245.1315270
   Pirretti M, 2010, J COMPUT SECUR, V18, P799, DOI 10.3233/JCS-2009-0383
   Qian HL, 2015, INT J INF SECUR, V14, P487, DOI 10.1007/s10207-014-0270-9
   Sahai A, 2005, LECT NOTES COMPUT SC, V3494, P457, DOI 10.1007/11426639_27
   Sahai A, 2012, LECT NOTES COMPUT SC, V7417, P199
   Sakr S, 2016, BIG DATA RES, V4, P44, DOI 10.1016/j.bdr.2016.05.002
   Waters B, 2011, LECT NOTES COMPUT SC, V6571, P53, DOI 10.1007/978-3-642-19379-8_4
   Xu SM, 2018, IEEE T INF FOREN SEC, V13, P2101, DOI 10.1109/TIFS.2018.2810065
   Xue KP, 2017, IEEE T INF FOREN SEC, V12, P953, DOI 10.1109/TIFS.2016.2647222
   Xue Y, 2016, ELECTRON J QUAL THEO, P1, DOI 10.14232/ejqtde.2016.1.97
   Yang JJ, 2015, COMPUT IND, V69, P3, DOI 10.1016/j.compind.2015.01.012
   Yang K, 2013, IEEE T PARALL DISTR, V24, P1717, DOI 10.1109/TPDS.2012.278
   Yu C., 2010, P 5 ACM S INF COMP C, P261
NR 30
TC 28
Z9 30
U1 2
U2 37
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 3
SU S
AR 100
DI 10.1145/3408322
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ2RB
UT WOS:000612586300008
DA 2024-07-18
ER

PT J
AU Lin, F
   Li, B
   Zhou, WG
   Li, HQ
   Lu, Y
AF Lin, Feng
   Li, Bin
   Zhou, Wengang
   Li, Houqiang
   Lu, Yan
TI Single-stage Instance Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Instance segmentation; neural networks; single stage; graph merge
AB Albeit the highest accuracy of object detection is generally acquired by multi-stage detectors, like R-CNN and its extension approaches, the single-stage object detectors also achieve remarkable performance with faster execution and higher scalability. Inspired by this, we propose a single-stage framework to tackle the instance segmentation task. Building on a single-stage object detection network in hand, our model outputs the detected bounding box of each instance, the semantic segmentation result, and the pixel affinity simultaneously. After that, we generate the final instance masks via a fast post-processing method with the help of the three outputs above. As far as we know, it is the first attempt to segment instances in a single-stage pipeline on challenging datasets. Extensive experiments demonstrate the efficiency of our post-processing method, and the proposed framework obtains competitive results as a single-stage instance segmentation method. We achieve 32.5 box AP and 26.0 mask AP on the COCO validation set with 500 pixels input scale and 22.9 mask AP on the Cityscapes test set.
C1 [Lin, Feng; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, 96 JinZhai Rd, Hefei, Peoples R China.
   [Li, Bin; Lu, Yan] Microsoft Res Asia, 5 Dan Ling St, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, 96 JinZhai Rd, Hefei, Peoples R China.
EM lin1993@mail.ustc.edu.cn; libin@microsoft.com; zhwg@ustc.edu.cn;
   lihq@ustc.edu.cn; yanlu@microsoft.com
RI Li, Houqiang Li/B-6259-2013
OI Lin, Feng/0000-0001-8541-7400
FU NSFC [61836011]; National Natural Science Foundation of China [61822208,
   61632019]; Youth Innovation Promotion Association CAS [2018497]
FX The work of H. Li was supported by NSFC under contract 61836011. The
   work of W. Zhou was supported in part by the National Natural Science
   Foundation of China under contract 61822208 and 61632019, and in part by
   Youth Innovation Promotion Association CAS (No. 2018497).
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2014, P EUR C COMP VIS ECC
   Arnab A, 2017, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2017.100
   Arnab Anurag, 2016, ARXIV160902583
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bai Min, 2017, P IEEE C COMP VIS PA
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bolya Daniel, 2019, P IEEE INT C COMP VI
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen Pengfei, 2018, P IEEE WINT C APPL C
   Chen Y., 2019, SMALL, V15
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2016, ADV NEUR IN, V29
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Dai Jifeng, 2017, P IEEE C COMP VIS PA
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan R., 2017, ARXIV171107618
   Fathi Alireza, 2017, Semantic instance segmentation via deep metric learning
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Gao NY, 2019, IEEE I CONF COMP VIS, P642, DOI 10.1109/ICCV.2019.00073
   Girshick Ross, 2018, Detectron
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445
   Huang J., 2017, CVPR
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li X, 2017, COMPUTING IN CIVIL ENGINEERING 2017: SMART SAFETY, SUSTAINABILITY, AND RESILIENCE, P17
   Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI [10.1007/978-3-030-01240-3_21, 10.1007/978-3-030-01219-9_23]
   Liang XD, 2018, IEEE T PATTERN ANAL, V40, P2978, DOI 10.1109/TPAMI.2017.2775623
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin T, 2017, PROCEEDINGS OF THE ASME 9TH ANNUAL DYNAMIC SYSTEMS AND CONTROL CONFERENCE, 2016, VOL 1
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SF, 2017, COMPUT RISK MANAG, P17, DOI 10.1007/978-981-10-1841-1_2
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu Wei, 2016, P EUR C COMP VIS ECC
   Liu YD, 2018, LECT NOTES COMPUT SC, V11207, P708, DOI 10.1007/978-3-030-01219-9_42
   Ma Ningning, 2018, P EUR C COMP VIS ECC, DOI [10.1007/978-3-030-01264-9_8, DOI 10.1007/978-3-030-01264-9_8]
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Patterson Trista, 2015, U S Forest Service Pacific Northwest Research Station Research Note PNW-RN, P1
   Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5
   Pinheiro R, 2015, WOMEN'S VOICES IN MANAGEMENT: IDENTIFYING INNOVATIVE AND RESPONSIBLE SOLUTIONS, P15
   Podgornyj YI, 2017, OBRAB METALLOV, P17, DOI 10.17212/1994-6309-2017-2-17-27
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren MY, 2017, PROC CVPR IEEE, P293, DOI 10.1109/CVPR.2017.39
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romera-Paredes B., 2016, ECCV
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shrivastava A., 2016, ARXIV PREPRINT ARXIV
   Sun Ke, 2018, P BRIT MACH VIS BMVC
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Uhrig Jonas, 2016, P GERM C PATT REC GC
   Wei DQ, 2018, PROCEEDINGS OF 2018 INTERNATIONAL SYMPOSIUM - REFORM AND INNOVATION OF HIGHER ENGINEERING EDUCATION, P18
   Wu Z., 2016, ARXIV160506885
   Xie E., 2019, ARXIV190913226
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu WQ, 2019, IEEE I CONF COMP VIS, P5167, DOI 10.1109/ICCV.2019.00527
   Yi L, 2017, PROC CVPR IEEE, P6584, DOI 10.1109/CVPR.2017.697
   Zhang B, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2750780
   Zhang QN, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457454
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang ZM, 2016, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2016.649
NR 68
TC 2
Z9 2
U1 4
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 86
DI 10.1145/3387926
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200012
DA 2024-07-18
ER

PT J
AU Bhowmik, D
   Abhayaratne, C
AF Bhowmik, Deepayan
   Abhayaratne, Charith
TI Embedding Distortion Analysis in Wavelet-domain Watermarking
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Watermarking; embedding distortion; wavelet; MSE
ID HIDDEN MARKOV MODEL; IMAGE WATERMARKING; TRANSFORM; DECOMPOSITION
AB Imperceptibility and robustness are two complementary fundamental requirements of any watermarking algorithm. Low-strength watermarking yields high imperceptibility, but exhibits poor robustness. High-strength watermarking schemes achieve good robustness but often infuse distortions resulting in poor visual quality in host images. This article analyses the embedding distortion for wavelet-based watermarking schemes. We derive the relationship between distortion, measured in mean square error (MSE), and the watermark embedding modification and propose the linear proportionality between MSE and the sum of energy of the selected wavelet coefficients for watermark embedding modification. The initial proposition assumes the orthononnality of discrete wavelet transform. It is further extended for non-orthonormal wavelet kernels using a weighting parameter that follows the energy conservation theorems in wavelet frames. The proposed analysis is verified by experimental results for both non-blind and blind watermarking schemes. Such a model is useful to find the optimum input parameters, including the wavelet kernel, coefficient selection, and subband choices for wavelet domain image watermarking.
C1 [Bhowmik, Deepayan] Univ Stirling, Stirling FK9 4LA, Scotland.
   [Abhayaratne, Charith] Univ Sheffield, Sheffield S1 4ET, S Yorkshire, England.
C3 University of Stirling; University of Sheffield
RP Bhowmik, D (corresponding author), Univ Stirling, Stirling FK9 4LA, Scotland.
EM deepayan.bhowmik@stir.ac.uk; c.abhayaratne@sheffield.ac.uk
OI Bhowmik, Deepayan/0000-0003-1762-1578
FU UK Engineering and Physical Sciences Research Council (EPSRC), through a
   Dorothy Hodgkin Postgraduate Award
FX We acknowledge the support of the UK Engineering and Physical Sciences
   Research Council (EPSRC), through a Dorothy Hodgkin Postgraduate Award.
CR Abhayaratne C., 2011, J REAL TIME IMAGE PR, V6
   Abhayaratne C, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3624491
   Adams MD, 2000, IEEE T IMAGE PROCESS, V9, P1010, DOI 10.1109/83.846244
   Alenizi F, 2019, MULTIMED TOOLS APPL, V78, P14511, DOI 10.1007/s11042-018-6723-9
   Amini M, 2018, IEEE T CIRC SYST VID, V28, P402, DOI 10.1109/TCSVT.2016.2607299
   Amini M, 2017, MIDWEST SYMP CIRCUIT, P611, DOI 10.1109/MWSCAS.2017.8052997
   Amini M, 2017, SIGNAL PROCESS, V137, P213, DOI 10.1016/j.sigpro.2017.01.019
   Barni M, 2001, IEEE T IMAGE PROCESS, V10, P783, DOI 10.1109/83.918570
   Bhatnagar G, 2012, COMPUT ELECTR ENG, V38, P1164, DOI 10.1016/j.compeleceng.2012.02.002
   Bhowmik D., 2009, SPIE WAVELET APPL IN, V7248
   Bhowmik D, 2016, IEEE ACCESS, V4, P8002, DOI 10.1109/ACCESS.2016.2627241
   Bhowmik D, 2016, IEEE T IMAGE PROCESS, V25, P5158, DOI 10.1109/TIP.2016.2599785
   Bhowmik D, 2014, MULTIMEDIA SYST, V20, P239, DOI 10.1007/s00530-013-0334-0
   Bhowmik D, 2009, LECT NOTES COMPUT SC, V5450, P363, DOI 10.1007/978-3-642-04438-0_31
   Charith G, 2003, ISPA 2003: PROCEEDINGS OF THE 3RD INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, PTS 1 AND 2, P239
   Chen PS, 2004, IEEE T CIRC SYST VID, V14, P1183, DOI 10.1109/TCSVT.2004.833165
   Chen TS, 2003, IEEE FIFTH INTERNATIOANL SYMPOSIUM ON MULTIMEDIA SOFTWARE ENGINEERING, PROCEEDINGS, P80
   Coifman R. R., 1995, LECT NOTES STAT, V103, P125, DOI [DOI 10.1007/978-1-4612-2544-7_9, 10.1002/cpa.3160410705, DOI 10.1002/CPA.3160410705]
   Cui XC, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0196306
   Daubechies I, 1998, J FOURIER ANAL APPL, V4, P247, DOI 10.1007/BF02476026
   DAUBECHIES I, 1990, IEEE T INFORM THEORY, V36, P961, DOI 10.1109/18.57199
   Dufaux F, 2004, P SOC PHOTO-OPT INS, V5558, P319, DOI 10.1117/12.564837
   Ejima M, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P546, DOI 10.1109/ICIP.2001.958549
   Fujiyoshi M, 2004, IEEE IMAGE PROC, P2629
   Gong Q, 2005, PDCAT 2005: Sixth International Conference on Parallel and Distributed Computing, Applications and Technologies, Proceedings, P1058
   Hampson FJ, 1996, INT CONF ACOUST SPEE, P1523, DOI 10.1109/ICASSP.1996.544089
   Heijmans HJAM, 2000, IEEE T IMAGE PROCESS, V9, P1897, DOI 10.1109/83.877211
   Huo FF, 2006, IEEE IMAGE PROC, P2573, DOI 10.1109/ICIP.2006.312985
   Jin C., 2006, INFORM TECHNOLOGY J, V5, P358, DOI DOI 10.3923/itj.2006.358.363
   Jong Ryul Kim, 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P226, DOI 10.1109/ICIP.1999.822889
   Koumaras H. G., 2008, BT50011 ITUR
   Kumsawat P, 2005, IEEE T SIGNAL PROCES, V53, P4707, DOI 10.1109/TSP.2005.859323
   Kundur D, 1998, INT CONF ACOUST SPEE, P2969, DOI 10.1109/ICASSP.1998.678149
   Kwon O, 2001, ISIE 2001: IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS PROCEEDINGS, VOLS I-III, P292, DOI 10.1109/ISIE.2001.931801
   Makbol NM, 2016, IET IMAGE PROCESS, V10, P34, DOI 10.1049/iet-ipr.2014.0965
   Mallat S.A., 1999, WAVELET TOUR SIGNAL
   Meerwald P, 2001, INT FED INFO PROC, V64, P69
   Meerwald P, 2009, LECT NOTES COMPUT SC, V5450, P61, DOI 10.1007/978-3-642-04438-0_6
   PIPER A, 2005, P 7 WORKSH MULT SEC, P79, DOI DOI 10.1145/1073170.1073186
   Saxena V., 2013, IUP J TELECOMMUN, V5, P56
   Tan Y, 2019, IEEE ACCESS, V7, P25026, DOI 10.1109/ACCESS.2019.2896304
   Taubman D., 2012, JPEG2000 IMAGE COMPR, V642
   Verma VS, 2015, SIGNAL IMAGE VIDEO P, V9, P1443, DOI 10.1007/s11760-013-0603-6
   Vetterli Martin, 1995, Wavelets and Subband Coding
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson A.B., 1993, Proc. AIAA Computing in Aerospace, V9, P286
   Xia XG, 1998, OPT EXPRESS, V3, P497, DOI 10.1364/OE.3.000497
   Xiao-Long Liu, 2018, IEEE Transactions on Circuits and Systems for Video Technology, V28, P1047, DOI 10.1109/TCSVT.2016.2633878
   Xie LH, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P427, DOI 10.1109/ICIP.1998.723409
   Zhang ZP, 2001, PROC SPIE, V4551, P127, DOI 10.1117/12.442900
NR 50
TC 10
Z9 10
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 108
DI 10.1145/3357333
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Jin, WK
   Zhao, Z
   Li, YM
   Li, J
   Xiao, J
   Zhuang, YT
AF Jin, Weike
   Zhao, Zhou
   Li, Yimeng
   Li, Jie
   Xiao, Jun
   Zhuang, Yueting
TI Video Question Answering via Knowledge-based Progressive
   Spatial-Temporal Attention Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video question answering; spatial-temporal; knowledge; attention
ID WEB
AB Visual Question Answering (VQA) is a challenging task that has gained increasing attention from both the computer vision and the natural language processing communities in recent years. Given a question in natural language, a VQA system is designed to automatically generate the answer according to the referenced visual content. Though there recently has been much intereset in this topic, the existing work of visual question answering mainly focuses on a single static image, which is only a small part of the dynamic and sequential visual data in the real world. As a natural extension, video question answering (VideoQA) is less explored. Because of the inherent temporal structure in the video, the approaches of ImageQA may be ineffectively applied to video question answering. In this article, we not only take the spatial and temporal dimension of video content into account but also employ an external knowledge base to improve the answering ability of the network. More specifically, we propose a knowledge-based progressive spatial-temporal attention network to tackle this problem. We obtain both objects and region features of the video frames from a region proposal network. The knowledge representation is generated by a word-level attention mechanism using the comment information of each object that is extracted from DBpedia. Then, we develop a question-knowledge-guided progressive spatial-temporal attention network to learn the joint video representation for video question answering task. We construct a large-scale video question answering dataset. The extensive experiments based on two different datasets validate the effectiveness of our method.
C1 [Jin, Weike; Zhao, Zhou; Li, Yimeng; Li, Jie; Xiao, Jun; Zhuang, Yueting] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Jin, WK; Zhao, Z (corresponding author), Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
EM weikejin@zju.edu.cn; zhaozhou@zju.edu.cn; aquaird@zju.edu.cn;
   jeli@zju.edu.cn; junx@zju.edu.cn; yzhuang@zju.edu.cn
RI Li, Jie/AAE-6389-2019; LI, 李伊濛/KGM-6775-2024; Zhao, zhuo/JYO-7894-2024;
   zhao, zhao/JAC-1686-2023
FU National Natural Science Foundation of China [61602405, 61836002,
   61572431, 61751209]; Zhejiang Natural Science Foundation [LR19F020002,
   LZ17F020001]; Fundamental Research Funds for the Central Universities;
   Chinese Knowledge Center for Engineering Sciences and Technology and
   Joint Research Program of ZJU - Microsoft Research Asia
FX This work was supported by National Natural Science Foundation of China
   under Grant No. 61602405, No. 61836002, No. 61572431, and No. 61751209;
   Zhejiang Natural Science Foundation (LR19F020002, LZ17F020001); the
   Fundamental Research Funds for the Central Universities; Chinese
   Knowledge Center for Engineering Sciences and Technology and Joint
   Research Program of ZJU; and Hikvision Research Institute. This project
   is partially funded by Microsoft Research Asia.
CR Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   [Anonymous], 2015, ARXIV150705670
   [Anonymous], 2009, Technical report
   [Anonymous], PROC CVPR IEEE
   [Anonymous], T ASS COMPUT LINGUIS
   [Anonymous], P C INN DAT SYST RES
   [Anonymous], 2015, CORR
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2005, P 21 C UNCERTAINTY A, DOI DOI 10.3115/1690219.1690283
   [Anonymous], IEEE T PATTERN ANAL
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Banko M, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2670
   Bollacker K., 2008, P 2008 ACM SIGMOD IN, P1247, DOI 10.1145/1376616.1376746
   Bordes Antoine, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P165, DOI 10.1007/978-3-662-44848-9_11
   Chen XL, 2013, IEEE I CONF COMP VIS, P1409, DOI 10.1109/ICCV.2013.178
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong L, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P260
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688
   Geman D, 2015, P NATL ACAD SCI USA, V112, P3618, DOI 10.1073/pnas.1422953112
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hong RC, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2043612.2043613
   Iyyer M, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1821, DOI 10.18653/v1/P17-1167
   Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149
   Jiang W, 2018, INT POW ELEC APPLICA, P239
   Jiao ZZ, 2018, ADV MATER SCI ENG, V2018, DOI 10.1155/2018/8407380
   Karpathy A, 2014, ADV NEUR IN, V27
   Kim JH, 2016, ADV NEUR IN, V29
   Le Quoc V., 2014, P INT C MACH LEARN I
   Li R., 2016, Advances in Neural Information Processing Systems, V29, P4655
   Liang JW, 2018, PROC CVPR IEEE, P6135, DOI 10.1109/CVPR.2018.00642
   Liao LZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P801, DOI 10.1145/3240508.3240605
   Lin X, 2016, LECT NOTES COMPUT SC, V9906, P261, DOI 10.1007/978-3-319-46475-6_17
   Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d
   Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289
   Ma L, 2016, AAAI CONF ARTIF INTE, P3567
   Malinowski M, 2014, ADV NEUR IN, V27
   Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9
   Na S, 2017, IEEE I CONF COMP VIS, P677, DOI 10.1109/ICCV.2017.80
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Sukhbaatar Sainbayar, 2015, ADV NEURAL INFORM PR, P2440
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Vrandecic D, 2014, COMMUN ACM, V57, P78, DOI 10.1145/2629489
   Wu AM, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1029
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500
   WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133
   Xiao C, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1341
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu K., 2015, COMPUTER SCI, P2048
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Ye YN, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P829, DOI 10.1145/3077136.3080655
   Yu DF, 2017, PROC CVPR IEEE, P4187, DOI 10.1109/CVPR.2017.446
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Zeng KH, 2017, AAAI CONF ARTIF INTE, P4334
   Zhao Z, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1053, DOI 10.1145/3025453.3025982
   Zhao Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3683
   Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 67
TC 13
Z9 13
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 52
DI 10.1145/3321505
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900005
DA 2024-07-18
ER

PT J
AU Spiteri, K
   Sitaraman, R
   Sparacio, D
AF Spiteri, Kevin
   Sitaraman, Ramesh
   Sparacio, Daniel
TI From Theory to Practice: Improving Bitrate Adaptation in the DASH
   Reference Player
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video streaming; video QoE; bitrate adaptation
ID VIDEO
AB Modern video streaming uses adaptive bitrate (ABR) algorithms that run inside video players and continually adjust the quality (i.e., bitrate) of the video segments that are downloaded and rendered to the user. To maximize the quality-of-experience (QoE) of the user, ABR algorithms must stream at a high bitrate with low rebuffering and low bitrate oscillations. Further, a good ABR algorithm is responsive to user and network events and can be used in demanding scenarios such as low-latency live streaming. Recent research papers provide an abundance of ABR algorithms but fall short on many of the above real-world requirements.
   We develop Sabre, an open-source publicly available simulation tool that enables fast and accurate simulation of adaptive streaming environments. We empirically validated Sabre to show that it accurately simulates real-world environments. We used Sabre to design and evaluate BOLA-E and DYNAMIC, two novel ABR algorithms. We also developed a FAST SWITCHING algorithm that can replace segments that have already been downloaded with higher-bitrate (thus, higher-quality) segments. The new algorithms provide higher QoE to the user in terms of higher bitrate, fewer rebuffers, and lesser bitrate oscillations. In addition, these algorithms react faster to user events such as startup and seek, and they respond more quickly to network events such as improvements in throughput. Further, they perform very well for live streams that require low latency, a challenging scenario for ABR algorithms. Overall, our algorithms offer superior video QoE and responsiveness for real-life adaptive video streaming, in comparison to the state-of-the-art. Importantly, all three algorithms presented in this article are now part of the official DASH reference player dash. is and are being used by video providers in production environments. While our evaluation and implementation are focused on the DASH environment, our algorithms are equally applicable to other adaptive streaming formats such as Apple HLS.
C1 [Spiteri, Kevin; Sitaraman, Ramesh] Univ Massachusetts, Coll Informat & Comp Sci, Comp Sci Bldg,140 Governors Dr, Amherst, MA 01003 USA.
   [Sitaraman, Ramesh] Akamai Technol, 150 Broadway, Cambridge, MA 02142 USA.
   [Sparacio, Daniel] CBS Interact, 235 Second St, San Francisco, CA 94105 USA.
C3 University of Massachusetts System; University of Massachusetts Amherst
RP Spiteri, K (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Comp Sci Bldg,140 Governors Dr, Amherst, MA 01003 USA.
EM kspiteri@cs.umass.edu; ramesh@cs.umass.edu; sparacio@cbsinteractive.com
RI Spiteri, Kevin/B-4874-2016
OI Spiteri, Kevin/0000-0001-6754-705X; Sitaraman,
   Ramesh/0000-0003-0558-6875
FU NSF [CNS-1413998, CNS-1763617]
FX This work is supported in part by the NSF under Grants No. CNS-1413998
   and No. CNS-1763617.
CR [Anonymous], BIG BUCK BUNN MOV
   [Anonymous], 2016, RAW DAT MEAS BROADB
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], HTTP DYN STREAM SPEC
   [Anonymous], 2019, CISCO VISUAL NETWORK
   [Anonymous], 2017, HTTP LIVE STREAMING
   [Anonymous], 2016, P 7 INT C MULT SYST
   Beben A, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P13, DOI 10.1145/2910017.2910596
   DASH Industry Forum, 2019, DASH REF CLIENT 3 0
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Jiang J, 2012, 2012 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE AND INTELLIGENT AGENT TECHNOLOGY (WI-IAT 2012), VOL 2, P90, DOI 10.1109/WI-IAT.2012.40
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Maggs BM, 2015, ACM SIGCOMM COMP COM, V45, P52, DOI 10.1145/2805789.2805800
   Mao Hongzi, 2017, P ACM ANN ACM C SPEC
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Spiteri K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P123, DOI 10.1145/3204949.3204953
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Stohr Denny, 2017, P ACM MULT C
   Sun Yi, 2016, P ANN ACM C SPEC INT
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zambelli Alex, 2009, TECHNICAL REPORT
NR 25
TC 53
Z9 58
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 67
DI 10.1145/3336497
PG 29
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900020
OA Bronze
DA 2024-07-18
ER

PT J
AU Zha, ZJ
   Liu, JW
   Yang, TH
   Zhang, YD
AF Zha, Zheng-Jun
   Liu, Jiawei
   Yang, Tianhao
   Zhang, Yongdong
TI Spatiotemporal-Textual Co-Attention Network for Video Question Answering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video question answering; attention mechanism
ID DYNAMIC MEMORY NETWORKS; MODEL
AB Visual Question Answering (VQA) is to provide a natural language answer for a pair of an image or video and a natural language question. Despite recent progress on VQA, existing works primarily focus on image question answering and are suboptimal for video question answering. This article presents a novel Spatiotemporal-Textual Co-Attention Network (STCA-Net) for video question answering. The STCA-Net jointly learns spatially and temporally visual attention on videos as well as textual attention on questions. It concentrates on the essential cues in both visual and textual spaces for answering question, leading to effective question-video representation. In particular, a question-guided attention network is designed to learn question-aware video representation with a spatial-temporal attention module. It concentrates the network on regions of interest within the frames of interest across the entire video. A video-guided attention network is proposed to learn video-aware question representation with a textual attention module, leading to fine-grained understanding of question. The learned video and question representations are used by an answer predictor to generate answers. Extensive experiments on two challenging datasets of video question answering, i.e., MSVD-QA and MSRVTT-QA, have shown the effectiveness of the proposed approach.
C1 [Zha, Zheng-Jun; Liu, Jiawei; Yang, Tianhao; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230029, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Liu, JW (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230029, Anhui, Peoples R China.
EM zhazj@ustc.edu.cn; ljw368@mail.ustc.edu.cn; jshmyth@mail.ustc.edu.cn;
   zhyd73@ustc.edu.cn
RI Zha, Zheng-Jun/AAE-8408-2020; Zha, Zheng-Jun/AAF-8667-2020
OI Zha, Zheng-Jun/0000-0003-2510-8993; 
FU National Key R&D Program of China [2017YFB1300201]; National Natural
   Science Foundation of China (NSFC) [61622211, 61620106009, 61525206];
   Fundamental Research Funds for the Central Universities [WK2100100030]
FX This work was supported by the National Key R&D Program of China under
   Grant 2017YFB1300201, the National Natural Science Foundation of China
   (NSFC) under Grants 61622211, 61620106009, and 61525206 as well as the
   Fundamental Research Funds for the Central Universities under Grant
   WK2100100030.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2016, P OF AAAI
   [Anonymous], CVPR
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chen D, 2018, LECT NOTES COMPUT SC, V11164, P146, DOI 10.1007/978-3-030-00776-8_14
   Chu WQ, 2018, NEUROCOMPUTING, V314, P386, DOI 10.1016/j.neucom.2018.06.069
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Gao H., 2015, INT C NEURAL INF PRO, P2296
   Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hu HX, 2018, PROC CVPR IEEE, P5428, DOI 10.1109/CVPR.2018.00569
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149
   Jiao YF, 2018, IEEE T MULTIMEDIA, V20, P2693, DOI 10.1109/TMM.2018.2815998
   Kim JH, 2016, ADV NEUR IN, V29
   King DB, 2015, ACS SYM SER, V1214, P1
   Kumar A, 2016, PR MACH LEARN RES, V48
   Li ZT, 2018, IEEE T IMAGE PROCESS, V27, P4478, DOI 10.1109/TIP.2018.2839916
   Liang JW, 2018, PROC CVPR IEEE, P6135, DOI 10.1109/CVPR.2018.00642
   Liu DQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1416, DOI 10.1145/3240508.3240632
   Liu JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P737, DOI 10.1145/3240508.3240585
   Liu JW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231741
   Liu Jiawei, 2016, P 24 ACM INT C MULT, P192
   Liu Z, 2017, IEEE INT CONF COMP V, P3056, DOI 10.1109/ICCVW.2017.361
   Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Malinowski M, 2017, INT J COMPUT VISION, V125, P110, DOI 10.1007/s11263-017-1038-2
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Mun J, 2017, IEEE I CONF COMP VIS, P2886, DOI 10.1109/ICCV.2017.312
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Schwartz Idan., 2017, Advances in Neural Information Processing Systems, P3667
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Simonyan K., 2014, 14091556 ARXIV
   Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501
   Wang B, 2018, AAAI CONF ARTIF INTE, P7380
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P5656, DOI 10.1109/TIP.2017.2746267
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu DF, 2017, PROC CVPR IEEE, P4187, DOI 10.1109/CVPR.2017.446
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zha ZJ, 2013, IEEE T CIRC SYST VID, V23, P856, DOI 10.1109/TCSVT.2012.2226526
   Zhang W, 2018, IEEE T CIRC SYST VID, V28, P2768, DOI 10.1109/TCSVT.2017.2718188
   Zhang W, 2018, IEEE T MULTIMEDIA, V20, P880, DOI 10.1109/TMM.2017.2760102
   Zhao Z, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1053, DOI 10.1145/3025453.3025982
   Zhao Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3683
   Zhao Z, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3518
   Zhu B, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE), P1291, DOI 10.1109/ICISCE.2017.268
   Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 61
TC 10
Z9 10
U1 1
U2 37
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 53
DI 10.1145/3320061
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900006
DA 2024-07-18
ER

PT J
AU Li, XG
   Sun, YM
   Yang, YL
   Miao, CY
AF Li, Xianguo
   Sun, Yemei
   Yang, Yanli
   Miao, Changyun
TI Symmetrical Residual Connections for Single Image Super-Resolution
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Single-image super-resolution; vanishing gradients; convolutional neural
   network; residual networks
AB Single-image super-resolution (SISR) methods based on convolutional neural networks (CNN) have shown great potential in the literature. However, most deep CNN models don't have direct access to subsequent layers, seriously hindering the information flow. Furthermore, they fail to make full use of the hierarchical features from different low-level layers, thereby resulting in relatively low accuracy. In this article, we present a new SISR CNN, called SymSR, which incorporates symmetrical nested residual connections to improve both the accuracy and the execution speed. SymSR takes a larger image region for contextual spreading. It symmetrically combines multiple short paths for the forward propagation to improve the accuracy and for the backward propagation of gradient flow to accelerate the convergence speed. Extensive experiments based on open challenge datasets show the effectiveness of symmetrical residual connections. Compared with four other state-of-the-art super-resolution CNN methods, SymSR is superior in both accuracy and runtime.
C1 [Li, Xianguo; Sun, Yemei; Yang, Yanli; Miao, Changyun] Tianjin Polytech Univ, Tianjin 300387, Peoples R China.
   [Li, Xianguo; Sun, Yemei; Yang, Yanli; Miao, Changyun] Tianjin Key Lab Optoelect Detect Technol & Syst, Tianjin 300387, Peoples R China.
C3 Tiangong University
RP Li, XG (corresponding author), Tianjin Polytech Univ, Tianjin 300387, Peoples R China.; Li, XG (corresponding author), Tianjin Key Lab Optoelect Detect Technol & Syst, Tianjin 300387, Peoples R China.
EM lixianguo@tjpu.edu.cn; sunyemei1216@163.com; yangyanli@tjpu.edu.cn;
   miaochangyun@tjpu.edu.cn
RI yang, yang/HGT-7999-2022; Miao, Changyun/ABD-7766-2021; Lang,
   Ming/HIK-0758-2022; Li, XG/GRF-1094-2022; Yang, Ya-Li/GYU-4320-2022
OI Li, Xianguo/0000-0003-3761-8683
FU Tianjin Research Program of Application Foundation and Advanced
   Technology [15JCYBJC16500]; Program for Innovative Research Team in
   University of Tianjin [TD13-5034]
FX This work was supported by the Tianjin Research Program of Application
   Foundation and Advanced Technology [grant number 15JCYBJC16500] and the
   Program for Innovative Research Team in University of Tianjin [grant
   number TD13-5034]. The authors acknowledge the anonymous reviewers for
   their helpful comments on the manuscript.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bevilacqua M, 2013, 18 INT C DIG SIGN PR, P1
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jia K, 2013, IEEE T PATTERN ANAL, V35, P367, DOI 10.1109/TPAMI.2012.95
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai W-S, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh A, 2015, LECT NOTES COMPUT SC, V9004, P552, DOI 10.1007/978-3-319-16808-1_37
   Szegedy C., 2017, AAAI, V4, P12
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yu LQ, 2017, AAAI CONF ARTIF INTE, P66
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang YY, 2018, IEEE T NEUR NET LEAR, V29, P5419, DOI 10.1109/TNNLS.2018.2802650
NR 31
TC 12
Z9 13
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
AR 19
DI 10.1145/3282445
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9JB
UT WOS:000459798800003
DA 2024-07-18
ER

PT J
AU Huang, L
   Ding, BW
   Wang, AN
   Xu, YD
   Zhou, YP
   Li, X
AF Huang, Lei
   Ding, Bowen
   Wang, Aining
   Xu, Yuedong
   Zhou, Yipeng
   Li, Xiang
TI User Behavior Analysis and Video Popularity Prediction on a Large-Scale
   VoD System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video-on-demand; user behavior analysis; popularity prediction; cache
   replacement
ID MEMORY
AB Understanding streaming user behavior is crucial to the design of large-scale Video-on-Demand (VoD) systems. In this article, we begin with the measurement of individual viewing behavior from two aspects: the temporal characteristics and user interest. We observe that active users spend more hours on each active day, and their daily request time distribution is more scattered than that of the less active users, while the interview time distribution differs negligibly between two groups. The common interest in popular videos and the latest uploaded videos is observed in both groups. We then investigate the predictability of video popularity as a collective user behavior through early views. In the light of the limitations of classical approaches, the Autoregressive-Moving-Average (ARMA) model is employed to forecast the popularity dynamics of individual videos at fine-grained time scales, thus achieving much higher prediction accuracy. When applied to video caching, the ARMA-assisted Least Frequently Used (LFU) algorithm can outperform the Least Recently Used (LRU) by 11-16%, the well-tuned LFU by 6-13%, and the LFU is only 2-4% inferior to the offline LFU in terms of hit ratio.
C1 [Huang, Lei; Ding, Bowen; Wang, Aining; Xu, Yuedong] Fudan Univ, Sch Informat Sci & Engn, Shanghai, Peoples R China.
   [Zhou, Yipeng] Macquarie Univ, Dept Comp, N Ryde, NSW, Australia.
   [Li, Xiang] Fudan Univ, Sch Informat Sci & Engn, Res Ctr Smart Networks & Syst, Shanghai, Peoples R China.
C3 Fudan University; Macquarie University; Fudan University
RP Xu, YD (corresponding author), Fudan Univ, Sch Informat Sci & Engn, Shanghai, Peoples R China.
EM 15210720030@fudan.edu.cn; 17210720028@fudan.edu.cn;
   13307130412@fudan.edu.cn; ydxu@fudan.edu.cn; yipeng.job@gmail.com;
   lix@fudan.edu.cn
RI Li, Xiang/AAB-4134-2020; huang, lei/GQP-8739-2022; li,
   xiang/GWM-6319-2022
OI Li, Xiang/0000-0002-6482-2535; Zhou, Yipeng/0000-0003-1533-0865
FU Natural Science Foundation of China [61772139, 71731004]; National
   Natural Science Fund for Distinguished Young Scholar of China
   [61425019]; CERNET Innovation Project [NGII20170209]; Open project of
   State Key Laboratory for Novel Software Technology, Nanjing University
   [KFKT2016B01]
FX This work is supported by Natural Science Foundation of China (No.
   61772139, 71731004), National Natural Science Fund for Distinguished
   Young Scholar of China (No. 61425019), CERNET Innovation Project
   NGII20170209 and Open project of State Key Laboratory for Novel Software
   Technology, Nanjing University (KFKT2016B01).
CR [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI DOI 10.1145/2433396.2433443
   Bunde A, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.048701
   Cai SM, 2009, EPL-EUROPHYS LETT, V87, DOI 10.1209/0295-5075/87/68001
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   Chatzopoulou G., 2010, P IEEE INFOCOM, DOI [DOI 10.1109/INFCOMW.2010.5466701, 10.1109/INFCOMW.2010.5466701]
   Chen L. Y., 2013, CHINESE PHYS B, V22, P1
   Cisco, 2017, Cisco7 Feb.
   Goh KI, 2008, EPL-EUROPHYS LETT, V81, DOI 10.1209/0295-5075/81/48002
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Huang L., 2017, P 27 WORKSH NETW OP, P49
   Li ZY, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P307, DOI 10.1145/2740908.2743054
   Livina VN, 2005, PHYS REV LETT, V95, DOI 10.1103/PhysRevLett.95.208501
   Rawassizadeh R, 2016, IEEE T KNOWL DATA EN, V28, P3098, DOI 10.1109/TKDE.2016.2592527
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Shcherbakov M.V., 2013, World Applied Sciences Journal, V24, P171, DOI [10.5829/idosi.wasj.2013.24.itmies.80032, DOI 10.5829/IDOSI.WASJ.2013.24.ITMIES.80032]
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Traverso S, 2015, IEEE T MULTIMEDIA, V17, P1839, DOI 10.1109/TMM.2015.2458043
   Trzcinski T, 2017, IEEE T MULTIMEDIA, V19, P2561, DOI 10.1109/TMM.2017.2695439
   Wikimapia, 2017, ENTR INF THEOR
   Xu CQ, 2015, IEEE T VEH TECHNOL, V64, P1201, DOI 10.1109/TVT.2014.2329696
   Xu CQ, 2014, IEEE T BROADCAST, V60, P322, DOI 10.1109/TBC.2014.2314791
   Xu YD, 2017, IEEE T MOBILE COMPUT, V16, P2228, DOI 10.1109/TMC.2016.2616402
   Zhang Y, 2012, EPL-EUROPHYS LETT, V98, DOI 10.1209/0295-5075/98/68002
   Zhao ZD, 2011, CHINESE PHYS LETT, V28, DOI 10.1088/0256-307X/28/6/068901
NR 25
TC 6
Z9 6
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
SU S
AR 67
DI 10.1145/3226035
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG9GD
UT WOS:000455314500015
DA 2024-07-18
ER

PT J
AU Demirbilek, E
   Grégoire, JC
AF Demirbilek, Edip
   Gregoire, Jean-Charles
TI Machine Learning-Based Parametric Audiovisual Quality Prediction Models
   for Real-Time Communications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Perceived quality estimation; audiovisual quality dataset; MOS;
   no-reference models; machine learning
ID REGRESSION
AB In order to mechanically predict audiovisual quality in interactive multimedia services, we have developed machine learning-based no-reference parametric models. We have compared Decision Trees-based ensemble methods, Genetic Programming and Deep Learning models that have one and more hidden layers. We have used the Institut national de la recherche scientifique (INRS) audiovisual quality dataset specifically designed to include ranges of parameters and degradations typically seen in real-time communications. Decision Trees-based ensemble methods have outperformed both Deep Learning- and Genetic Programming-based models in terms of Root-Mean-Square Error (RMSE) and Pearson correlation values. We have also trained and developed models on various publicly available datasets and have compared our results with those of these original models. Our studies show that Random Forests-based prediction models achieve high accuracy for both the INRS audiovisual quality dataset and other publicly available comparable datasets.
C1 [Demirbilek, Edip; Gregoire, Jean-Charles] Inst Natl Rech Sci, Montreal, PQ, Canada.
   [Demirbilek, Edip; Gregoire, Jean-Charles] Ctr Energie Mat Telecommun, Pl Bonaventure 800,Bur 6900, Montreal, PQ H5A 1K6, Canada.
C3 University of Quebec; Institut national de la recherche scientifique
   (INRS)
RP Demirbilek, E (corresponding author), Inst Natl Rech Sci, Montreal, PQ, Canada.; Demirbilek, E (corresponding author), Ctr Energie Mat Telecommun, Pl Bonaventure 800,Bur 6900, Montreal, PQ H5A 1K6, Canada.
EM edip.demirbilek@gmail.com; gregoire@emt.inrs.ca
FU Natural Sciences and Engineering Research Council of Canada (NSERC);
   Summit-Tech Multimedia Communications Inc., under Collaborative Research
   and Development (CRD) grant programme
FX This work is supported by the The Natural Sciences and Engineering
   Research Council of Canada (NSERC) and Summit-Tech Multimedia
   Communications Inc., under the Collaborative Research and Development
   (CRD) grant programme.
CR Alpaydin E, 2004, INTRO MACHINE LEARNI
   [Anonymous], 2012, P120112012 ITUT
   [Anonymous], 1998, P9111998 ITUT
   [Anonymous], 1996, P INT TEL UN RAD ASS
   [Anonymous], 2003, G1072003 ITUT
   [Anonymous], 2012, P12012012 ITUT
   [Anonymous], 1999, P9101999 ITUT
   [Anonymous], 2012, G10702012 ITUT
   [Anonymous], 2012, P14012012 ITUT
   [Anonymous], 2012, P120122012 ITUT
   [Anonymous], 2015, G10712015 ITUT
   Beerends JG, 2013, J AUDIO ENG SOC, V61, P366
   Belmudez B., 2015, AUDIOVISUAL QUALITY
   Bengio Yoshua, 2015, DEEP LEARNI IN PRESS
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Comrie AC, 1997, J AIR WASTE MANAGE, V47, P653, DOI 10.1080/10473289.1997.10463925
   Demirbilek E., 2016, GStreamer Multimedia Quality Testbed
   Demirbilek E., 2016, The INRS Audiovisual Quality Dataset
   Demirbilek E, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511101
   Demirbilek Edip, 2016, ACM MULT C 2016
   Demirbilek Edip, 2016, ARXIV160906612
   Demirbilek Edip, 2016, SUBJECTIVE ASSESMENT
   Domingos P, 2012, COMMUN ACM, V55, P78, DOI 10.1145/2347736.2347755
   Du HQ, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT, PROCEEDINGS, P312, DOI 10.1109/ICNIDC.2009.5360833
   Dubin R, 2016, ARXIV160200489
   Fliegel K., 2014, Qualinet multimedia databases v5. 5
   Garcia Marie-Neige, 2016, AUDIO VIDEO DATABASE
   Garcia MN, 2014, T-LAB SER TELECOMMUN, P1, DOI 10.1007/978-3-319-04855-0
   Gastaldo P, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-54
   Goudarzi M., 2010, 2010 IEEE Global Telecommunications Conference GLOBECOM 2010, P1
   GStreamer, 2016, GSTREAMER OP SOURC M
   Hassenzahl M., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P201, DOI 10.1145/332040.332432
   Keimel C, 2012, INT WORK QUAL MULTIM, P97, DOI 10.1109/QoMEX.2012.6263865
   Konuk B, 2015, SIG PROCESS COMMUN, P966, DOI 10.1109/SIU.2015.7129992
   Mäki T, 2013, INT WORK QUAL MULTIM, P6, DOI 10.1109/QoMEX.2013.6603193
   Martinez Jerome, 2016, MEDIAINFO V0 7 74
   Mushtaq MS., 2012, Networks and Optical Communications (NOC), 2012 17th European Conference on, P1, DOI DOI 10.1109/NOC.2012.6249939
   Oza NC, 2005, IEEE SYS MAN CYBERN, P2340
   Pfahringer B, 2007, LECT NOTES COMPUT SC, V4830, P90
   Pinson Margaret, 2013, P 2013 5 INT WORKSH
   Pinson MH, 2013, IEEE SIGNAL PROC MAG, V30, P171, DOI 10.1109/MSP.2013.2258265
   Pinson MH, 2012, IEEE J-STSP, V6, P640, DOI 10.1109/JSTSP.2012.2215306
   Poli R., 2008, A Field Guide to Genetic Programming
   Quan HT, 2011, IEEE T BROADCAST, V57, P1, DOI 10.1109/TBC.2010.2086750
   Raake A, 2011, IEEE SIGNAL PROC MAG, V28, P68, DOI 10.1109/MSP.2011.942472
   Robitza Werner, 2012, P 6 INT WORKSH VID P
   Schmidt M, 2010, GENET EVOL COMPUT, P73, DOI 10.1007/978-1-4419-1626-6_5
   Strobl C, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-25
   You JY, 2010, SIGNAL PROCESS-IMAGE, V25, P482, DOI 10.1016/j.image.2010.02.002
NR 49
TC 17
Z9 17
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2017
VL 13
IS 2
AR 16
DI 10.1145/3051482
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EV1VK
UT WOS:000401537300004
DA 2024-07-18
ER

PT J
AU Liu, Y
   Xiao, MB
   Zhang, M
   Li, X
   Dong, M
   Ma, Z
   Li, ZH
   Guo, L
   Chen, SQ
AF Liu, Yao
   Xiao, Mengbai
   Zhang, Ming
   Li, Xin
   Dong, Mian
   Ma, Zhan
   Li, Zhenhua
   Guo, Lei
   Chen, Songqing
TI Content-Adaptive Display Power Saving for Internet Video Applications on
   Mobile Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 6th ACM International Conference on Multimedia Systems (MMSys)
   Co-Located with 25th ACM Workshop on Network and Operating Systems
   Support for Digital Audio and Video (NOSSDAV)
CY MAR 18-20, 2015
CL Portland, OR
SP ACM
DE Internet mobile streaming; real-time video communication; LCD; display;
   power saving; backlight scaling
AB Backlight scaling is a technique proposed to reduce the display panel power consumption by strategically dimming the backlight. However, for mobile video applications, a computationally intensive luminance compensation step must be performed in combination with backlight scaling to maintain the perceived appearance of video frames. This step, if done by the Central Processing Unit (CPU), could easily offset the power savings via backlight dimming. Furthermore, computing the backlight scaling values requires per-frame luminance information, which is typically too energy intensive to compute on mobile devices.
   In this article, we propose Content-Adaptive Display (CAD) for two typical Internet mobile video applications: video streaming and real-time video communication. CAD uses the mobile device's Graphics Processing Unit (GPU) rather than the CPU to perform luminance compensation at reduced power consumption. For video streaming where video frames are available in advance, we compute the backlight scaling schedule using a more efficient dynamic programming algorithm than existing work. For real-time video communication where video frames are generated on the fly, we propose a greedy algorithm to determine the backlight scaling at runtime. We implement CAD in one video streaming application and one real-time video call application on the Android platform and use a Monsoon power meter to measure the real power consumption. Experiment results show that CAD can save more than 10% overall power consumption for up to 55.7% videos during video streaming and up to 31.0% overall power consumption in real-time video calls.
C1 [Liu, Yao] SUNY Binghamton, Dept Comp Sci, Binghamton, NY 13902 USA.
   [Xiao, Mengbai; Zhang, Ming; Chen, Songqing] George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
   [Li, Xin] Samsung Telecommun Amer, Richardson, TX USA.
   [Dong, Mian] AT Int Inc, Beijing, Peoples R China.
   [Ma, Zhan] Nanjing Univ, Nanjing, Jiangsu, Peoples R China.
   [Li, Zhenhua] Tsinghua Univ, Beijing, Peoples R China.
   [Guo, Lei] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Binghamton; George Mason University; Samsung; Nanjing University;
   Tsinghua University; University System of Ohio; Ohio State University
RP Liu, Y (corresponding author), SUNY Binghamton, Dept Comp Sci, Binghamton, NY 13902 USA.
RI Li, Zhenhua/AAS-8358-2020; Ma, Zhan/HKW-2859-2023; Dong,
   Mianxiong/O-7489-2019
OI Ma, Zhan/0000-0003-3686-4057; Dong, Mianxiong/0000-0002-2788-3451
FU High-Tech Research and Development Program of China ("863 - China Cloud"
   Major Program) [2015AA01A201]; National Natural Science Foundation of
   China (NSFC) [61432002, 61632020, 61471217]; CCF-Tencent Open Fund
   [IAGR20150101, IAGR20160101]; NSF [CNS-1524462]
FX We appreciate constructive comments from anonymous reviewers. The work
   is partially supported by the High-Tech Research and Development Program
   of China ("863 - China Cloud" Major Program) under grant 2015AA01A201,
   by the National Natural Science Foundation of China (NSFC) under grants
   61432002 (State Key Program), 61632020 (State Key Program) and 61471217,
   by the CCF-Tencent Open Fund under grants IAGR20150101 and IAGR20160101,
   and by the NSF under grant CNS-1524462. Preliminary versions of this
   manuscript are published in the proceedings of NOSSDAV 2015 and ISLPED
   2015 [Liu et al. 2015; Xiao et al. 2015].
CR Anand B., 2015, P DAC 15 JUN, P1
   Anand Bhojan., 2011, P 9 INT C MOBILE SYS, P57
   [Anonymous], 2003, P ESTIMEDIA
   Carroll A., 2010, USENIX annual technical conference, P271
   Chang N, 2004, IEEE T VLSI SYST, V12, P837, DOI 10.1109/tvlsi.2004.831472
   Chen L., 2007, ADAPTIVE COMPUTATION, P1
   Cheng WC, 2004, IEEE T CONSUM ELECTR, V50, P25, DOI 10.1109/TCE.2004.1277837
   Cho H, 2009, IEEE T CONSUM ELECTR, V55, P839, DOI 10.1109/TCE.2009.5174463
   Choi I, 2002, ISLPED'02: PROCEEDINGS OF THE 2002 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN, P112, DOI 10.1109/LPE.2002.1029567
   Dong M, 2012, IEEE T MOBILE COMPUT, V11, P724, DOI 10.1109/TMC.2012.40
   Lin CH, 2014, IEEE T COMPUT, V63, P335, DOI 10.1109/TC.2012.210
   LIU Y, 2015, P 25 ACM WKSH NETW, P1, DOI DOI 10.1145/2736084.2736087
   Pi-Cheng Hsiu, 2011, 2011 International Symposium on Low Power Electronics and Design (ISLPED 2011), P309, DOI 10.1109/ISLPED.2011.5993655
   Ruggiero Martino., 2008, P 8 ACM INT C EMBEDD, P109
   Simunic T, 2001, IEEE T COMPUT AID D, V20, P840, DOI 10.1109/43.931003
   Tsai PS, 2009, IEEE T CIRC SYST VID, V19, P574, DOI 10.1109/TCSVT.2009.2014022
   Wee TK, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P573, DOI 10.1145/2493432.2493445
   Xiao MB, 2015, I SYMPOS LOW POWER E, P285, DOI 10.1109/ISLPED.2015.7273528
   Yan ZS, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P431, DOI 10.1145/2733373.2806269
   Yu CG, 2014, IEEE INFOCOM SER, P1456, DOI 10.1109/INFOCOM.2014.6848080
   Zhou Jia., 2011, Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference, P371
NR 21
TC 6
Z9 7
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2016
VL 12
IS 5
SU S
AR 84
DI 10.1145/2996461
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EJ0VT
UT WOS:000392929700015
OA Bronze
DA 2024-07-18
ER

PT J
AU Rana, S
   Sur, A
AF Rana, Shuvendu
   Sur, Arijit
TI Depth-Based View-Invariant Blind 3D Image Watermarking
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D; image; watermarking; DIBR; synthesis view; depth; dependent view
   region
AB With the huge advance in Internet technology as well as the availability of low-cost 3D display devices, 3D image transmission has become popular in recent times. Since watermarking has become regarded as a potential Digital Rights Management (DRM) tools in the past decade, 3D image watermarking is an emerging research topic. With the introduction of the Depth Image-Based Rendering (DIBR) technique, 3D image watermarking is a more challenging task, especially for synthetic view generation. In this article, synthetic view generation is regarded as a potential attack, and a blind watermarking scheme is proposed that can resist it. In the proposed scheme, the watermark is embedded into the low-pass filtered dependent view region of 3D images. Block Discrete Cosine Transformation (DCT) is used for spatial-filtration of the dependent view region to find the DC coefficient with horizontally shifted coherent regions from the left and right view to make the scheme robust against synthesis view attack. A comprehensive set of experiments have been carried out to justify the robustness of the proposed scheme over related existing schemes with respect to Stereo JPEG compression and different noise addition attacks.
C1 [Rana, Shuvendu; Sur, Arijit] Indian Inst Technol, Dept Comp Sci & Engn, Multimedia Lab, Gauhati 781039, Assam, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Guwahati
RP Rana, S (corresponding author), Indian Inst Technol, Dept Comp Sci & Engn, Multimedia Lab, Gauhati 781039, Assam, India.
EM shuvendu@iitg.ernet.in; arijit@iitg.ernet.in
RI Rana, Shuvendu/ACC-7002-2022; Sur, Arijit/AAB-4216-2020; Rana,
   Shuvendu/J-5190-2019
OI Rana, Shuvendu/0000-0002-8372-5669; Rana, Shuvendu/0000-0002-8372-5669;
   Sur, Arijit/0000-0002-9038-8138
CR [Anonymous], P 9 IND C COMP VIS G
   [Anonymous], TEST MODEL 8 3D HEVC
   [Anonymous], 2013, INT C COMP COMM NETW
   [Anonymous], P 23 EUR SIGN PROC C
   [Anonymous], DEPTH IMAGE BASED RE
   [Anonymous], DIGITAL WATERMARKING
   Asikuzzaman M, 2014, IEEE IMAGE PROC, P5497, DOI 10.1109/ICIP.2014.7026112
   Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191
   Bender W, 1996, IBM SYST J, V35, P313, DOI 10.1147/sj.353.0313
   Campisi P, 2008, J ELECTRON IMAGING, V17, DOI 10.1117/1.3009554
   Fehn C, 2006, P IEEE, V94, P524, DOI 10.1109/JPROC.2006.870688
   Franco-Contreras J., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2761, DOI 10.1109/ICIP.2011.6116242
   Guan Y, 2014, I C CONT AUTOMAT ROB, P346, DOI 10.1109/ICARCV.2014.7064330
   Halici E, 2009, IEEE IMAGE PROC, P4217, DOI 10.1109/ICIP.2009.5413525
   Han YM, 2005, IEEE T CIRC SYST VID, V15, P269, DOI 10.1109/TCSVT.2004.841541
   Hoffman DM, 2008, J VISION, V8, DOI 10.1167/8.3.33
   Jaipuria Smita Jagdishprasad, 2014, 2014 International Conference on Communications and Signal Processing (ICCSP), P181, DOI 10.1109/ICCSP.2014.6949824
   Jridi M, 2013, PROC SPIE, V8656, DOI 10.1117/12.2006174
   Kauff P, 2007, SIGNAL PROCESS-IMAGE, V22, P217, DOI 10.1016/j.image.2006.11.013
   Kim HD, 2012, IEEE T BROADCAST, V58, P533, DOI 10.1109/TBC.2012.2206851
   Lin YH, 2011, IEEE T BROADCAST, V57, P602, DOI 10.1109/TBC.2011.2131470
   Özkalayci BO, 2014, IEEE T IMAGE PROCESS, V23, P5222, DOI 10.1109/TIP.2014.2360452
   Trick D, 2013, IEEE INT WORKSH MULT, P418, DOI 10.1109/MMSP.2013.6659325
   Vinod P., 2006, IEE Proceedings-Information Security, V153, P61, DOI 10.1049/ip-ifs:20055088
   Yonggang Fu, 2009, Proceedings of the 2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2009), P437, DOI 10.1109/FSKD.2009.19
   Yu-Cheng Fan, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P325
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
NR 27
TC 3
Z9 3
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2016
VL 12
IS 4
AR 48
DI 10.1145/2957751
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV4EH
UT WOS:000382877500002
DA 2024-07-18
ER

PT J
AU Hu, XP
   Deng, JQ
   Zhao, JD
   Hu, WY
   Ngai, ECH
   Wang, RF
   Shen, J
   Liang, M
   Li, XT
   Leung, VCM
   Kwok, YK
AF Hu, Xiping
   Deng, Junqi
   Zhao, Jidi
   Hu, Wenyan
   Ngai, Edith C. -H.
   Wang, Renfei
   Shen, Johnny
   Liang, Min
   Li, Xitong
   Leung, Victor C. M.
   Kwok, Yu-Kwong
TI SAfeDJ: A Crowd-Cloud Codesign Approach to Situation-Aware Music
   Delivery for Drivers
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; Experimentation; Smartphones; crowdsensing; cloud;
   music mood; context; driving
ID COMPUTING MOTIVATION; MOBILE; SYSTEM; STATE
AB Driving is an integral part of our everyday lives, but it is also a time when people are uniquely vulnerable. Previous research has demonstrated that not only does listening to suitable music while driving not impair driving performance, but it could lead to an improved mood and a more relaxed body state, which could improve driving performance and promote safe driving significantly. In this article, we propose SAfeDJ, a smartphone-based situation-aware music recommendation system, which is designed to turn driving into a safe and enjoyable experience. SAfeDJ aims at helping drivers to diminish fatigue and negative emotion. Its design is based on novel interactive methods, which enable in-car smartphones to orchestrate multiple sources of sensing data and the drivers' social context, in collaboration with cloud computing to form a seamless crowdsensing solution. This solution enables different smartphones to collaboratively recommend preferable music to drivers according to each driver's specific situations in an automated and intelligent manner. Practical experiments of SAfeDJ have proved its effectiveness in music-mood analysis, and mood-fatigue detections of drivers with reasonable computation and communication overheads on smartphones. Also, our user studies have demonstrated that SAfeDJ helps to decrease fatigue degree and negative mood degree of drivers by 49.09% and 36.35%, respectively, compared to traditional smartphone-based music player under similar driving situations.
C1 [Hu, Xiping; Shen, Johnny; Leung, Victor C. M.] Univ British Columbia, Vancouver, BC V5Z 1M9, Canada.
   [Deng, Junqi; Kwok, Yu-Kwong] Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
   [Zhao, Jidi] E China Normal Univ, Shanghai, Peoples R China.
   [Hu, Wenyan] Nankai Univ, Tianjin, Peoples R China.
   [Ngai, Edith C. -H.] Uppsala Univ, Uppsala, Sweden.
   [Wang, Renfei] IBM Canada, Markham, ON, Canada.
   [Li, Xitong] HEC Paris, Paris, France.
C3 University of British Columbia; University of Hong Kong; East China
   Normal University; Nankai University; Uppsala University; International
   Business Machines (IBM); Hautes Etudes Commerciales (HEC) Paris
RP Hu, XP (corresponding author), Univ British Columbia, Vancouver, BC V5Z 1M9, Canada.
EM xipingh@ece.ubc.ca; jdzhao@sem.ecnu.edu.cn
RI Leung, Victor C. M./AGU-2462-2022; xitong, Li/GQA-3559-2022; Kwok, Yu
   Kwong Ricky/C-1581-2009; Hu, Xiping/GSE-5065-2022
OI Leung, Victor C. M./0000-0003-3529-2640; xitong, Li/0000-0003-4807-1556;
   Hu, Xiping/0000-0002-4952-699X
FU Canadian Natural Sciences and Engineering Research Council through the
   NSERC DIVA Strategic Network; TELUS; National Social Science Found of
   China [11ZD174]; Shanghai Pujiang Program; ECNU International
   Publication Program; Vinnova GreenIoT project; STINT grant for
   international collaboration in Sweden
FX This work is supported in part by the Canadian Natural Sciences and
   Engineering Research Council through the NSERC DIVA Strategic Network,
   by TELUS and other industry partners, by the National Social Science
   Found of China under Grant 11&ZD174, Shanghai Pujiang Program and ECNU
   International Publication Program, and by the Vinnova GreenIoT project
   and STINT grant for international collaboration in Sweden.
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   Ahmadi H., 2010, Proceedings of the 8th ACM Conference on Embedded Networked Sensor Systems, P99
   Ahmed E, 2015, J NETW COMPUT APPL, V52, P154, DOI 10.1016/j.jnca.2015.03.001
   Ahmed E, 2015, J NETW COMPUT APPL, V52, P52, DOI 10.1016/j.jnca.2015.02.003
   [Anonymous], 2014, World Report on Road Traffic Injury Prevention
   Araniti G, 2013, IEEE COMMUN MAG, V51, P148, DOI 10.1109/MCOM.2013.6515060
   Ayaki R., 2009, P INT S AUT DEC SYST, P1
   Baltrunas L, 2012, PERS UBIQUIT COMPUT, V16, P507, DOI 10.1007/s00779-011-0417-x
   Battle R, 2012, SEMANT WEB, V3, P355, DOI 10.3233/SW-2012-0065
   Brickley D., 2012, FOAF Vocabulary Specification 0.98
   Cassidy G, 2009, MUSIC SCI, V13, P357, DOI 10.1177/102986490901300207
   Fernandes S, 2012, IEEE COMMUN SURV TUT, V14, P45, DOI 10.1109/SURV.2011.082010.00099
   Fujishima T., 1999, P INT COMP MUS C, P464
   Ganti RK, 2011, IEEE COMMUN MAG, V49, P32, DOI 10.1109/MCOM.2011.6069707
   Gomez E., 2006, THESIS UPF BARCELONA
   Helmholz Patrick, 2014, Advancing the Impact of Design Science: Moving from Theory to Practice. 9th International Conference, DESRIST 2014. Proceedings: LNCS 8463, P393, DOI 10.1007/978-3-319-06701-8_32
   Helmholz P, 2013, LECT NOTES COMPUT SC, V7939, P412
   Howe J, 2006, WIRED, V14, P1, DOI DOI 10.1086/599595
   Hu X., 2010, MUSIC MOOD THEORY RE
   Hu X., 2015, CONTEXT AWARE MOBILE
   Hu X., 2015, IEEE Communications Surveys and Tutorials
   Hu XF, 2013, DEFECT DIFFUS FORUM, V344, P85, DOI 10.4028/www.scientific.net/DDF.344.85
   Hu XP, 2014, IEEE COMMUN MAG, V52, P78, DOI 10.1109/MCOM.2014.6829948
   Hu XP, 2013, IEEE T EMERG TOP COM, V1, P148, DOI 10.1109/TETC.2013.2273359
   Hunter PG, 2011, EMOTION, V11, P1068, DOI 10.1037/a0023749
   Vu K, 2012, IEEE INFOCOM SER, P2399, DOI 10.1109/INFCOM.2012.6195629
   Lee BG, 2012, SENSORS-BASEL, V12, P17536, DOI 10.3390/s121217536
   Ness S.R., 2009, Proceedings of the ACM International Conference on Multimedia, P705, DOI 10.1145/1631272.1631393
   Pauws S., 2004, P INT S MUS INF RETR
   Pedersen Ted., 2004, DEMONSTRATION PAPERS, P38
   Sardis F, 2013, IEEE T MULTIMEDIA, V15, P769, DOI 10.1109/TMM.2013.2240286
   Su JH, 2010, IEEE INTELL SYST, V25, P16, DOI 10.1109/MIS.2010.23
   Suk M, 2014, IEEE COMPUT SOC CONF, P132, DOI 10.1109/CVPRW.2014.25
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   van der Zwaag MD, 2012, ERGONOMICS, V55, P12, DOI 10.1080/00140139.2011.638403
   Wang Xinxi., 2012, Proceedings of the 20th ACM international conference on Multimedia, P99, DOI [DOI 10.1145/2393347.2393368, 10.1145/2393347.2393368]
   Zhang DQ, 2005, APPL MATH COMPUT, V163, P895, DOI 10.1016/j.amc.2004.04.016
NR 37
TC 32
Z9 32
U1 2
U2 36
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 21
DI 10.1145/2808201
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100011
OA Green Published
DA 2024-07-18
ER

PT J
AU Wu, MJ
   Jang, JSR
AF Wu, Ming-Ju
   Jang, Jyh-Shing R.
TI Combining Acoustic and Multilevel Visual Features for Music Genre
   Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Performance; Music genre classification
ID PATTERNS
AB Most music genre classification approaches extract acoustic features from frames to capture timbre information, leading to the common framework of bag-of-frames analysis. However, time-frequency analysis is also vital for modeling music genres. This article proposes multilevel visual features for extracting spectrogram textures and their temporal variations. A confidence-based late fusion is proposed for combining the acoustic and visual features. The experimental results indicated that the proposed method achieved an accuracy improvement of approximately 14% and 2% in the world's largest benchmark dataset (MASD) and Unique dataset, respectively. In particular, the proposed approach won the Music Information Retrieval Evaluation eXchange (MIREX) music genre classification contests from 2011 to 2013, demonstrating the feasibility and necessity of combining acoustic and visual features for classifying music genres.
C1 [Wu, Ming-Ju] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
   [Jang, Jyh-Shing R.] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
C3 National Tsing Hua University; National Taiwan University
RP Wu, MJ (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
EM hsbw@mirlab.org; roger.jang@mirlab.org
RI Wu, MJ/KQV-3644-2024
OI JANG, JYH-SHING/0000-0002-7319-9095
CR Alm JF, 2002, SIAM REV, V44, P457, DOI 10.1137/S00361445003822
   [Anonymous], IEEE T SPEECH AUDIO
   [Anonymous], 2012, ISMIR
   [Anonymous], 2001, P COST G6 C DIG AUD
   [Anonymous], 1993, FUNDAMENTALS SPEECH
   Bergstra J., 2010, ISMIR, P507
   Bertin-Mahieux T., 2011, ISMIR, P591
   Campbell WM, 2006, IEEE SIGNAL PROC LET, V13, P308, DOI 10.1109/LSP.2006.870086
   Cao C., 2009, THINKITS SUBMISSION
   Chang C., 2010, LIBSVM: A library for support vector machines 2010
   Chen ZS, 2011, IEEE T MULTIMEDIA, V13, P1371, DOI 10.1109/TMM.2011.2166380
   Cory McKay, 2010, THESIS MCGILL U CANA
   Costa YMG, 2012, SIGNAL PROCESS, V92, P2723, DOI 10.1016/j.sigpro.2012.04.023
   Daniel F.Pachet., 2000, Proceedings RIAO of Content-Based Multimedia Information Access, P1238
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Downie JS, 2005, ACM-IEEE J CONF DIG, P376, DOI 10.1145/1065385.1065479
   Ellis DPW, 2007, J NEW MUSIC RES, V36, P51, DOI 10.1080/09298210701653344
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Fu ZY, 2011, IEEE T MULTIMEDIA, V13, P303, DOI 10.1109/TMM.2010.2098858
   Gauvain JL, 1994, IEEE T SPEECH AUDI P, V2, P291, DOI 10.1109/89.279278
   Goto Masataka, 2003, P 16 ANN ACM S USER, P31, DOI [10.1145/964696.964700, DOI 10.1145/964696.964700]
   Grosche P., 2012, Proc of the 13th International Society of Music Information Retrieval, number Ismir, P55
   Jiang DN, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P113, DOI 10.1109/ICME.2002.1035731
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Lee CH, 2009, IEEE T MULTIMEDIA, V11, P670, DOI 10.1109/TMM.2009.2017635
   Lidy Thomas, 2005, P INT C MUS INF RETR, P34
   Meng A., 2005, Proceedings of the International Conference on Music Information Retrieval, P604
   Meng A, 2007, IEEE T AUDIO SPEECH, V15, P1654, DOI 10.1109/TASL.2007.899293
   Ming-Ju Wu, 2011, Proceedings of the 2011 Tenth International Conference on Machine Learning and Applications (ICMLA 2011), P124, DOI 10.1109/ICMLA.2011.48
   Müller M, 2011, IEEE J-STSP, V5, P1088, DOI 10.1109/JSTSP.2011.2112333
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Panagakis Y, 2014, IEEE-ACM T AUDIO SPE, V22, P1905, DOI 10.1109/TASLP.2014.2355774
   Panagakis Y, 2010, IEEE T AUDIO SPEECH, V18, P576, DOI 10.1109/TASL.2009.2036813
   Paulus J., 2010, Ismir, P625
   Pei Soo-Chang, 2009, P IEEE INT C AC SPEE, P169
   Ren JM, 2012, IEEE T AUDIO SPEECH, V20, P1134, DOI 10.1109/TASL.2011.2172426
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Seyerlehner K, 2010, THESIS J KEPLER U LI
   Seyerlehner K., 2010, USING BLOCK LEVEL FE
   Seyerlehner Klaus, 2011, REFINED BLO IN PRESS
   Tsunoo E, 2011, IEEE T AUDIO SPEECH, V19, P1003, DOI 10.1109/TASL.2010.2073706
   Tzanetakis G., 2007, Marsyas submissions to MIREX 2007
   Wu TF, 2004, J MACH LEARN RES, V5, P975
   Yeh CCM, 2013, INT CONF ACOUST SPEE, P246, DOI 10.1109/ICASSP.2013.6637646
NR 44
TC 14
Z9 16
U1 1
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2015
VL 12
IS 1
AR 10
DI 10.1145/2801127
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CP9LG
UT WOS:000360215200010
DA 2024-07-18
ER

PT J
AU Shen, LQ
   An, P
   Zhang, ZY
   Hu, QQ
   Chen, ZC
AF Shen, Liquan
   An, Ping
   Zhang, Zhaoyang
   Hu, Qianqian
   Chen, Zhengchuan
TI A 3D-HEVC Fast Mode Decision Algorithm for Real-Time Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; 3D-HEVC; real-time applications; mode decision;
   computational complexity; motion estimation
ID RATE-DISTORTION OPTIMIZATION; CU SIZE DECISION; MULTIVIEW VIDEO; MOTION
   ESTIMATION; FAST DISPARITY; DEPTH; HEVC; TEXTURE; SELECTION; AVC
AB 3D High Efficiency Video Coding (3D-HEVC) is an extension of the HEVC standard for coding of multiview videos and depth maps. It inherits the same quadtree coding structure as HEVC for both components, which allows recursively splitting into four equal-sized coding units (CU). One of 11 different prediction modes is chosen to code a CU in inter-frames. Similar to the joint model of H.264/AVC, the mode decision process in HM (reference software of HEVC) is performed using all the possible depth levels and prediction modes to find the one with the least rate distortion cost using a Lagrange multiplier. Furthermore, both motion estimation and disparity estimation need to be performed in the encoding process of 3D-HEVC. Those tools achieve high coding efficiency, but lead to a significant computational complexity. In this article, we propose a fast mode decision algorithm for 3D-HEVC. Since multiview videos and their associated depth maps represent the same scene, at the same time instant, their prediction modes are closely linked. Furthermore, the prediction information of a CU at the depth level X is strongly related to that of its parent CU at the depth level X-1 in the quadtree coding structure of HEVC since two corresponding CUs from two neighboring depth levels share similar video characteristics. The proposed algorithm jointly exploits the inter-view coding mode correlation, the inter-component (texture-depth) correlation and the inter-level correlation in the quadtree structure of 3D-HEVC. Experimental results show that our algorithm saves 66% encoder runtime on average with only a 0.2% BD-Rate increase on coded views and 1.3% BD-Rate increase on synthesized views.
C1 [Shen, Liquan; An, Ping; Zhang, Zhaoyang; Hu, Qianqian] Shanghai Univ, Shanghai 200041, Peoples R China.
   [Chen, Zhengchuan] Tsinghua Univ, Beijing, Peoples R China.
C3 Shanghai University; Tsinghua University
RP Shen, LQ (corresponding author), Shanghai Univ, Shanghai 200041, Peoples R China.
EM jssiq@shu.edu.cn
RI Shen, Liquan/D-4832-2012; Hu, Qian/GWZ-7617-2022; Zhang,
   Zhaoyang/AFQ-9161-2022
FU Shanghai Rising-Star Program [11QA1402400]; Innovation Program of
   Shanghai Municipal Education Commission [13ZZ069]; National Natural
   Science Foundation of China [60832003, 60902085, 61171084, 61172096,
   61422111, U1301257]
FX This work is sponsored by Shanghai Rising-Star Program (11QA1402400) and
   Innovation Program (13ZZ069) of Shanghai Municipal Education Commission,
   and is supported by the National Natural Science Foundation of China
   under grant No. 60832003, 60902085, 61171084, 61172096, 61422111 and
   U1301257.
CR [Anonymous], ISON11829
   [Anonymous], JCT3VA0044
   [Anonymous], JCTVCF045
   [Anonymous], MOBILE3DTV TECH REP
   [Anonymous], JCT3VA1100
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], P 13 VCEG M33 M
   [Anonymous], P EUR SIGN PROC C
   [Anonymous], JCTVCF092
   Cernigliaro G, 2013, IEEE T CIRC SYST VID, V23, P769, DOI 10.1109/TCSVT.2012.2223632
   Correa G, 2013, 2013 IEEE EUROCON, P81, DOI 10.1109/EUROCON.2013.6624969
   Daribo I, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/258920
   Domanski M, 2013, IEEE T IMAGE PROCESS, V22, P3517, DOI 10.1109/TIP.2013.2266580
   Grewatsch S, 2004, IEEE IMAGE PROC, P3271
   Gu ZY, 2013, IEEE INT CONF MULTI
   Heming Sun, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P1085, DOI 10.1109/ICME.2012.4
   Karczewicz M, 2010, IEEE T CIRC SYST VID, V20, P1698, DOI 10.1109/TCSVT.2010.2092614
   Kim J, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P449, DOI 10.1109/PCS.2012.6213251
   Kim Y, 2007, IEEE T CONSUM ELECTR, V53, P712, DOI 10.1109/TCE.2007.381750
   Lee JY, 2011, IEEE T CIRC SYST VID, V21, P1859, DOI 10.1109/TCSVT.2011.2154730
   Lei JJ, 2015, IEEE T CIRC SYST VID, V25, P275, DOI 10.1109/TCSVT.2014.2335471
   Lin YH, 2011, IEEE T BROADCAST, V57, P542, DOI 10.1109/TBC.2011.2131510
   Ma SW, 2013, IEEE DATA COMPR CONF, P73, DOI 10.1109/DCC.2013.15
   Micallef BW, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P113, DOI 10.1109/PCS.2012.6213299
   Mora EG, 2014, IEEE T CIRC SYST VID, V24, P1554, DOI 10.1109/TCSVT.2013.2283110
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Oh BT, 2014, IEEE T CIRC SYST VID, V24, P1006, DOI 10.1109/TCSVT.2013.2290577
   Oh H, 2006, LECT NOTES COMPUT SC, V4319, P898
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2014, IEEE T IMAGE PROCESS, V23, P4232, DOI 10.1109/TIP.2014.2341927
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen LQ, 2012, IEEE T CONSUM ELECTR, V58, P926, DOI 10.1109/TCE.2012.6311338
   Shen LQ, 2011, IEEE T CIRC SYST VID, V21, P837, DOI 10.1109/TCSVT.2011.2130310
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tao SP, 2009, IEEE INT SYMP CIRC S, P2353, DOI 10.1109/ISCAS.2009.5118272
   Ugur K, 2010, IEEE T CIRC SYST VID, V20, P1688, DOI 10.1109/TCSVT.2010.2092613
   Vanne J, 2012, IEEE T CIRC SYST VID, V22, P1885, DOI 10.1109/TCSVT.2012.2223013
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Yan Y, 2013, PICT COD SYMP, P406, DOI 10.1109/PCS.2013.6737769
   Zhang H, 2013, IEEE INT SYMP CIRC S, P45, DOI 10.1109/ISCAS.2013.6571778
   Zhang J, 2010, IEEE IMAGE PROC, P2865, DOI 10.1109/ICIP.2010.5651934
   Zhang N., 2013, BIOMETRICS THEORY AP, P1
   Zhang QW, 2011, IEEE T CONSUM ELECTR, V57, P1857, DOI 10.1109/TCE.2011.6131164
   Zhu W, 2010, IEEE T CONSUM ELECTR, V56, P1696, DOI 10.1109/TCE.2010.5606315
   Zhu W, 2010, IEEE T CONSUM ELECTR, V56, P957, DOI 10.1109/TCE.2010.5506026
NR 46
TC 33
Z9 34
U1 0
U2 39
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2015
VL 11
IS 3
AR 34
DI 10.1145/2700298
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CB8BF
UT WOS:000349852500002
DA 2024-07-18
ER

PT J
AU Ren, DN
   Xu, YS
   Chan, SHG
AF Ren, Dongni
   Xu, Yisheng
   Chan, S. -H. Gary
TI Beyond 1Mbps Global Overlay Live Streaming: The Case of Proxy Helpers
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurement; Multi-Mbps video; global live streaming; proxy helpers;
   testbed experiment
ID PEER; TCP
AB In order to provide live streaming over the global Internet, a content provider often deploys an overlay network consisting of distributed proxies placed close to user pools. Streaming of multi-Mbps video over such an overlay is challenging because of bandwidth bottlenecks in paths. To effectively overcome these bottlenecks, we consider employing proxy helpers in the overlay to provide rich path diversity. The helpers do not have any attached users, and hence may forward partial video streams (or not at all) if necessary. In this way, the helpers serve as stepping stones to supply full streams to the servers. The issue is how to involve the helpers in the overlay to achieve low streaming delay meeting a certain high streaming bitrate requirement.
   To address the issue, we first formulate the problem which captures various delay and bandwidth components, and show that it is NP-hard. We then propose an efficient algorithm called Stepping-Stones (SS) which can be efficiently implemented in a controller. Given the encouraging simulation results, we develop a novel streaming testbed for SS and explore, through sets of Internet experiments, the effectiveness of helpers to achieve high bitrate (multi-Mbps) global live streaming. In our experiments, proxies are deployed with a reasonably wide global footprint. We collect more than a hundred hours of streaming traces with bitrate ranging from 500kbps to a few Mbps. Our experimental data validates that helpers indeed play an important role in achieving high bitrate in today's Internet. Global multi-Mbps streaming is possible due to their multihop and multipath advantages. Our experimental trials and data also provide valuable insights on the design of a global push-based streaming network. There are strong benefits of using proxy helpers to achieve high bitrate and low delay.
C1 [Ren, Dongni; Xu, Yisheng; Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Ren, DN (corresponding author), Hong Kong Univ Sci & Technol, Rm 2606, Kowloon, Hong Kong, Peoples R China.
EM tonyren@cse.ust.hk; yishengxu@cse.ust.hk; gchan@cse.ust.hk
RI guo, ppdop/KAL-9865-2024
OI Chan, Gary Shueng Han/0000-0003-4207-764X
FU Hong Kong Research Grant Council (RGC) General Research Fund [610713];
   HKUST [FSGRF13EG15]; Hong Kong Innovation and Technology Fund [UIM/246]
FX This work was supported, in part, by Hong Kong Research Grant Council
   (RGC) General Research Fund (610713), HKUST (FSGRF13EG15) and the Hong
   Kong Innovation and Technology Fund (UIM/246).
CR Adler M, 2011, COMPUT NETW, V55, P4007, DOI 10.1016/j.comnet.2011.07.015
   Alessandria E, 2009, IEEE INFOCOM SER, P100, DOI 10.1109/INFCOM.2009.5061911
   Alvarez-Horine R, 2012, IEEE GLOBE WORK, P747
   [Anonymous], 2013, P 4 ACM MULT SYST C
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   Chang HS, 2011, IEEE ACM T NETWORK, V19, P55, DOI 10.1109/TNET.2010.2056382
   Chang H, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P417
   Chen JC, 2001, GLOB TELECOMM CONF, P1963, DOI 10.1109/GLOCOM.2001.965916
   Concolato Cyril., 2011, Proceedings of the second annual ACM conference on Multimedia systems, MMSys '11, P265, DOI [10.1145/1943552.1943587, DOI 10.1145/1943552.1943587]
   Cormen T. H., 2009, Introduction to Algorithms, VSecond
   Dongni Ren, 2012, P 19 INT PACK VID WO
   Dongni Ren, 2008, P ANN JOINT C IEEE C
   Garey M. R., 1979, Computers and intractability. A guide to the theory of NP-completeness
   He YF, 2010, IEEE T CIRC SYST VID, V20, P1638, DOI 10.1109/TCSVT.2010.2077553
   Jiang JWJ, 2012, IEEE T MULTIMEDIA, V14, P1456, DOI 10.1109/TMM.2012.2196509
   Jiang Wenjie, 2010, P 19 INT C COMP COMM
   Jin X, 2009, IEEE T MULTIMEDIA, V11, P1024, DOI 10.1109/TMM.2009.2021804
   Kontothanassis L, 2004, P IEEE, V92, P1408, DOI 10.1109/JPROC.2004.832956
   Kostic D., 2003, Operating Systems Review, V37, P282, DOI 10.1145/1165389.945473
   Kuschnig R., 2010, MMSYS, P157
   Kuschnig R, 2010, CONSUM COMM NETWORK, P200
   Medina A., 2001, P IEEE INT WORKSH MO
   Muller C., 2012, 4th ACM Workshop on Mobile Video (MoVID), P37, DOI DOI 10.1145/2151677.2151686
   Nygren E., 2010, SIGOPS OPER SYST REV, V44, P2, DOI [10.1145/1842733.1842736, DOI 10.1145/1842733.1842736]
   Padhye J., 1998, Computer Communication Review, V28, P303, DOI 10.1145/285243.285291
   Ren D., 2009, IEEE T MULTIMEDIA, V11, P8
   Sengupta S, 2011, IEEE T INFORM THEORY, V57, P5072, DOI 10.1109/TIT.2011.2145630
   Su AJ, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P29
   Vu L, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1865106.1865115
   Wang B, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1352012.1352020
   Wang Jiajun, 2007, P INT WORKSH PEER TO
   Wu C, 2011, IEEE ACM T NETWORK, V19, P1317, DOI 10.1109/TNET.2011.2107563
   Wu C, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1386109.1386112
   Xiuhan Li, 2010, Proceedings of the 2010 5th IEEE International Conference on Nano/Micro Engineered and Molecular Systems (NEMS 2010), P497, DOI 10.1109/NEMS.2010.5592443
   Yin H., 2009, Proceedings of ACM International Conference on Multimedia, P25
   Yin H, 2013, IEEE T MULTIMEDIA, V15, P2114, DOI 10.1109/TMM.2013.2280557
   Yuan XQ, 2013, J SUPERCOMPUT, V64, P1092, DOI 10.1007/s11227-011-0685-2
NR 37
TC 1
Z9 1
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2014
VL 11
IS 2
AR 26
DI 10.1145/2652485
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6DM
UT WOS:000348308800003
DA 2024-07-18
ER

PT J
AU Gonina, E
   Friedland, G
   Battenberg, E
   Koanantakool, P
   Driscoll, M
   Georganas, E
   Keutzer, K
AF Gonina, Ekaterina
   Friedland, Gerald
   Battenberg, Eric
   Koanantakool, Penporn
   Driscoll, Michael
   Georganas, Evangelos
   Keutzer, Kurt
TI Scalable Multimedia Content Analysis on Parallel Platforms Using Python
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Performance
AB In this new era dominated by consumer-produced media there is a high demand for web-scalable solutions to multimedia content analysis. A compelling approach to making applications scalable is to explicitly map their computation onto parallel platforms. However, developing efficient parallel implementations and fully utilizing the available resources remains a challenge due to the increased code complexity, limited portability and required low-level knowledge of the underlying hardware. In this article, we present PyCASP, a Python-based framework that automatically maps computation onto parallel platforms from Python application code to a variety of parallel platforms. PyCASP is designed using a systematic, pattern-oriented approach to offer a single software development environment for multimedia content analysis applications. Using PyCASP, applications can be prototyped in a couple hundred lines of Python code and automatically scale to modern parallel processors. Applications written with PyCASP are portable to a variety of parallel platforms and efficiently scale from a single desktop Graphics Processing Unit (GPU) to an entire cluster with a small change to application code. To illustrate our approach, we present three multimedia content analysis applications that use our framework: a state-of-the-art speaker diarization application, a content-based music recommendation system based on the Million Song Dataset, and a video event detection system for consumer-produced videos. We show that across this wide range of applications, our approach achieves the goal of automatic portability and scalability while at the same time allowing easy prototyping in a high-level language and efficient performance of low-level optimized code.
C1 [Gonina, Ekaterina; Friedland, Gerald; Battenberg, Eric; Koanantakool, Penporn; Driscoll, Michael; Georganas, Evangelos; Keutzer, Kurt] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Friedland, Gerald] Int Comp Sci Inst, Berkeley, CA 94704 USA.
C3 University of California System; University of California Berkeley
RP Gonina, E (corresponding author), Soda Hall,573D, Berkeley, CA 94720 USA.
EM egonina@eecs.berkeley.edu; fractor@icsi.berkleey.edu;
   ericb@eecs.berkeley.edu; penpornk@eecs.berkeley.edu;
   mbdriscoll@eecs.berkeley.edu; egeor@eecs.berkeley.edu
RI Battenberg, Eric/ABD-6264-2020
FU Microsoft [024263]; Intel [024894]; U.C. Discovery [DIG07-10227]; Par
   Lab affiliates National Instruments; Nokia; NVIDIA; Oracle; Samsung;
   Intelligence Advanced Research Projects Activity (IARPA) via Department
   of Interior National Business Center [D11PC20066]; Direct For Computer &
   Info Scie & Enginr [1251258] Funding Source: National Science
   Foundation; Div Of Information & Intelligent Systems [1251258] Funding
   Source: National Science Foundation
FX This research was supported by Microsoft (Award # 024263) and Intel
   (Award # 024894) funding and by matching funding by U.C. Discovery
   (Award # DIG07-10227). Additional support comes from Par Lab affiliates
   National Instruments, Nokia, NVIDIA, Oracle, and Samsung. It was also
   partially supported by the Intelligence Advanced Research Projects
   Activity (IARPA) via Department of Interior National Business Center
   contract number D11PC20066. The U.S. Government is authorized to
   reproduce and distribute reprints for Governmental purposes
   notwithstanding any copyright annotation thereon. The views and
   conclusion contained herein are those of the authors and should not be
   interpreted as necessarily representing the official policies or
   endorsement, either expressed or implied, of IARPA, DOI/NBC, or the U.S.
   Government.
CR Amatriain X., 2002, P 17 ANN C OBJ OR PR
   Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   Miro XA, 2012, IEEE T AUDIO SPEECH, V20, P356, DOI 10.1109/TASL.2011.2125954
   [Anonymous], P PYTH SCI COMP C
   [Anonymous], UCBEECS2010124
   [Anonymous], P WORKSH PROGR MOD E
   [Anonymous], NVIDIA CUDA PROGR GU
   [Anonymous], 2008, OpenMP Application Program Interface
   [Anonymous], 2009, Hadoop: The definitive guide
   [Anonymous], 2006, Tech. rep.
   [Anonymous], 2014, ACM T MULTIMEDIA COM, V10
   Ascher D., 1999, UCRLMA128569 LAWR LI
   Battenberg Eric., 2009, ISMIR, P501
   Bertin-Mahieux T., 2011, P 12 INT S MUS INF R
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Blackford LS, 2002, ACM T MATH SOFTWARE, V28, P135, DOI 10.1145/567806.567807
   Carletta J, 2007, LANG RESOUR EVAL, V41, P181, DOI 10.1007/s10579-007-9040-x
   Casey C. R. Michael, 2008, IEEE T AUDIO SPEECH, V16
   Catanzaro B., 2008, P 25 INT C MACHINE L, P104, DOI DOI 10.1145/1390156.1390170
   Catanzaro B, 2009, IEEE I CONF COMP VIS, P2381, DOI 10.1109/ICCV.2009.5459410
   Chafi H, 2011, ACM SIGPLAN NOTICES, V46, P35, DOI 10.1145/2038037.1941561
   Chang E. Y., 2009, FDN LARGE SCALE MULT, P213
   Charbuillet C., 2011, P 14 INT C DIG AUD E
   Chaudhuri S., 2011, P 11 P ANN C INT SPE
   Chaves JC, 2006, PROCEEDINGS OF THE HPCMP USERS GROUP CONFERENCE 2006, P429
   Chong J., 2009, P 10 ANN C INT SPEEC
   Chong Jike., 2010, P 2 USENIX C HOT TOP, P2
   Cook H., 2011, P USENIX WORKSH HOT
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Elizalde B., 2012, P 1 ACM WORKSH AUD M
   Ferraro Pascal., 2009, ISMIR, P279
   Friedland G, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1865106.1865111
   Gonina E., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P553, DOI 10.1109/ASRU.2011.6163887
   Gonina E., 2011, P 2 INT WORKSH MAPRE, P35
   Goodale T., P 5 INT C HIGH PERF, P26
   Gregory V.W., 2000, DR DOBBS J
   Grinspun E, 2002, ACM T GRAPHIC, V21, P281, DOI 10.1145/566570.566578
   Hudak P., 1994, YALEUDCSRR1049
   Imseng D, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P432, DOI 10.1109/ASRU.2009.5373254
   Intel, CILK 5 4 6 REF MAN V
   Kamil S., 2011, P PYTH SCI COMP C
   Keutzer K., 2010, INTEL TECH J, V4
   Khronos Group, 2010, OPENCL 1 1 SPEC
   Kosner A., 2012, Forbes
   Liu ZY, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961198
   Lu L, 2008, IEEE T MULTIMEDIA, V10, P74, DOI 10.1109/TMM.2007.911304
   Mueller Frank., 1995, PTHREADS LIB INTERFA
   Pangborn AD, 2010, THESIS ROCHESTER I T
   Prechelt L, 2000, COMPUTER, V33, P23, DOI 10.1109/2.876288
   Ramakrishnan L., 2011, P 2 INT WORKSHOP SCI, P49
   Reynolds DA, 2005, INT CONF ACOUST SPEE, P953
   Slaney M., 2010, P INT C MULT
   Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32
   Takács G, 2009, J MACH LEARN RES, V10, P623
   Tzanetakis G., 2007, MARSYAS SUBMISSIONS
   Vuduc R, 2005, J PHYS CONF SER, V16, P521, DOI 10.1088/1742-6596/16/1/071
   Whaley RC, 2005, SOFTWARE PRACT EXPER, V35, P101, DOI 10.1002/spe.626
   Wooters C, 2008, LECT NOTES COMPUT SC, V4625, P509
   Xia R., 2012, UCBEECS2012227
   You K, 2009, IEEE SIGNAL PROC MAG, V26, P124, DOI 10.1109/MSP.2009.934124
NR 60
TC 4
Z9 5
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2014
VL 10
IS 2
AR 18
DI 10.1145/2517151
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB6DY
UT WOS:000331879000003
DA 2024-07-18
ER

PT J
AU Wang, YC
   Lin, TA
   Hsu, CH
   Liu, X
AF Wang, Yichuan
   Lin, Ting-An
   Hsu, Cheng-Hsin
   Liu, Xin
TI Region- and Action-Aware Virtual World Clients
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Virtual worlds; traffic measurement; traffic modeling
ID MODELS
AB We propose region- and action-aware virtual world clients. To develop such clients, we present a parameterized network traffic model, based on a large collection of Second Life traces gathered by us. Our methodology is also applicable to virtual worlds other than Second Life. With the traffic model, various optimization criteria can be adopted, including visual quality, response time, and energy consumption. We use energy consumption as the show case, and demonstrate via trace-driven simulations that, compared to two existing schemes, a mobile client can save up to 36% and 41% communication energy by selectively turning on its WiFi network interface.
C1 [Wang, Yichuan; Liu, Xin] Univ Calif Davis, Davis, CA 95616 USA.
   [Lin, Ting-An; Hsu, Cheng-Hsin] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
C3 University of California System; University of California Davis;
   National Tsing Hua University
RP Hsu, CH (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, 101,Sect 2,Kuang Fu Rd, Hsinchu 30013, Taiwan.
EM chsu@cs.nthu.edu.tw
FU National Science Council (NSC) of Taiwan [100-2218-E-007-015-MY2]
FX C.-H. Hsu and T.-A. Lin. are partially supported by the National Science
   Council (NSC) of Taiwan, grant no. 100-2218-E-007-015-MY2.
CR [Anonymous], 2 LIF EC STAT
   [Anonymous], P 2008 ACM CONEXT C
   [Anonymous], 2 LIF CUST CAS STUD
   [Anonymous], P ACM INT WORKSH NET
   [Anonymous], GALAXY S NEWS
   Antonello R, 2009, MULTIMEDIA SYST, V15, P33, DOI 10.1007/s00530-008-0125-1
   Balasubramanian N, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P280
   Borella MS, 2000, COMPUT COMMUN, V23, P403, DOI 10.1016/S0140-3664(99)00197-8
   Ferreira M, 2010, NOSSDAV 2010: PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P3
   Kinicki J., 2008, Proceedings of the 18th International Workshop on Network and Operating Systems Support for Digital Audio and Video, P69
   Kumar S, 2008, COMPUTER, V41, P46, DOI 10.1109/MC.2008.398
   La C.-A., 2008, Proceedings of the first workshop on Online social networks, WOSP '08, P79
   Liang HG, 2009, MULTIMED TOOLS APPL, V45, P163, DOI 10.1007/s11042-009-0304-x
   Liang HG, 2008, PROCEEDINGS OF THE 2008 14TH IEEE INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED SYSTEMS, P823, DOI 10.1109/ICPADS.2008.74
   PEDERSON SP, 1990, TECHNOMETRICS, V32, P305, DOI 10.2307/1269107
   WANG Y., P IEEE INT C MULTIME, P1
   Wang Y., 2011, Proceedings of the second annual ACM conference on Multimedia systems, P105
NR 17
TC 4
Z9 4
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2013
VL 9
IS 1
AR 6
DI 10.1145/2422956.2422962
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 097EO
UT WOS:000315457000006
DA 2024-07-18
ER

PT J
AU De Rooij, O
   Worring, M
AF De Rooij, Ork
   Worring, Marcel
TI Efficient Targeted Search Using a Focus and Context Video Browser
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Multidimensional browsing;
   conceptual similarity; semantic threads; interactive search; information
   visualization
AB Currently there are several interactive content-based video retrieval techniques and systems available. However, retrieval performance depends heavily on the means of interaction. We argue that effective CBVR requires efficient, specialized user interfaces. In this article we propose guidelines for such an interface, and we propose an effective CBVR engine: the ForkBrowser, which builds upon the principle of focus and context. This browser is evaluated using a combination of user simulation and real user evaluation. Results indicate that the ideas have merit, and that the browser performs very well when compared to the state-of-the-art in video retrieval.
C1 [De Rooij, Ork; Worring, Marcel] Univ Amsterdam, Intelligent Syst Lab Amsterdam, NL-1098 XH Amsterdam, Netherlands.
C3 University of Amsterdam
RP De Rooij, O (corresponding author), Univ Amsterdam, Intelligent Syst Lab Amsterdam, Sci Pk 904, NL-1098 XH Amsterdam, Netherlands.
EM orooij@uva.nl
RI Worring, Marcel/JRW-7059-2023
OI Worring, Marcel/0000-0003-4097-4136
CR Adcock J, 2005, LECT NOTES COMPUT SC, V3568, P205
   Adcock J., 2007, P ACM INT C IM VID R, P644
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2000, Information Visualization: Perception for Design
   [Anonymous], P INT WORKSH MULT IN
   [Anonymous], P ACM INT C MULT AUG
   [Anonymous], P 6 ACM INT C IM VID
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Christel MG, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P1032
   Cord M, 2007, IMAGE VISION COMPUT, V25, P14, DOI 10.1016/j.imavis.2006.01.004
   de Rooij O, 2010, IEEE T MULTIMEDIA, V12, P121, DOI 10.1109/TMM.2009.2037388
   Furnas G. W., 1986, ACM Sigchi Bull., V17, P16, DOI DOI 10.1145/22339.22342
   Gosselin P., 2004, Proceedings of the 1st international workshop on Computer vision meets databases, P51
   Hauptmann A.G., 2004, PROC 12 ANN ACM INT, P668
   Hauptmann A.G., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, P385, DOI DOI 10.1145/1180639.1180721
   Huijbregts M, 2007, LECT NOTES COMPUTER
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Ming-yu Chen, 2005, 13th Annual ACM International Conference on Multimedia, P902, DOI 10.1145/1101149.1101342
   Natsev A., 2005, 13th Annual ACM International Conference on Multimedia, P598, DOI 10.1145/1101149.1101288
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   PETERSOHN C., 2004, P TRECVID WORKSH
   RAUTIAINEN M., 2006, P 14 ANN ACM INT C M, P125
   Robertson G., 1998, 11th Annual Symposium on User Interface Software and Technology. UIST. Proceedings of the ACM Symposium, P153, DOI 10.1145/288392.288596
   SNOEK C. G. M., 2008, P 6 TRECVID WORKSH
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P280, DOI 10.1109/TMM.2006.886275
   SUNDARAM H., 2001, P IEEE INT C MULT EX, P70, DOI DOI 10.1109/ICME.2001.1237709
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   VAN GEMERT J. C., 2010, COMPUTER VI IN PRESS
   Yan R., 2007, WORKSHOP MULTIMEDIA, P13
   Yee Ka-Ping, 2003, P SIGCHI C HUM FACT, P401, DOI DOI 10.1145/642611.642681
   Zavesky E., Proceedings of the 2008 International Conference on Content-based Image and Video Retrieval, ser. CIVR '08. New York, NY, USA: ACM, P617, DOI [10.1145/1386352.1386442, DOI 10.1145/1386352.1386442]
NR 31
TC 5
Z9 5
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2012
VL 8
IS 4
AR 51
DI 10.1145/2379790.2379793
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 052QA
UT WOS:000312211900003
DA 2024-07-18
ER

PT J
AU Patras, P
   Banchs, A
   Serrano, P
AF Patras, Paul
   Banchs, Albert
   Serrano, Pablo
TI A Control Theoretic Scheme for Efficient Video Transmission over IEEE
   802.11e EDCA WLANs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Performance; control theory; EDCA; IEEE 802.11;
   video transmission
ID OPTIMAL CONFIGURATION; PERFORMANCE
AB The EDCA mechanism of the IEEE 802.11 standard has been designed to support, among others, video traffic. This mechanism relies on a number of parameters whose configuration is left open by the standard. Although there are some recommended values for these parameters, they are fixed independent of the WLAN conditions, which results in suboptimal performance. Following this observation, a number of approaches in the literature have been devised to set the EDCA parameters based on an estimation of the WLAN conditions. However, these previous approaches are based on heuristics and hence do not guarantee optimized performance. In this article we propose a novel algorithm to adjust the EDCA parameters to carry video traffic which, in contrast to previous approaches, is sustained on mathematical foundations that guarantee optimal performance. In particular, our approach builds upon (i) an analytical model of the WLAN performance under video traffic, used to derive the optimal point of operation of EDCA, and (ii) a control theoretic designed mechanism which drives the WLAN to this point of operation. Via extensive simulations, we show that the proposed approach performs optimally and substantially outperforms the standard recommended configuration as well as previous adaptive proposals.
C1 [Patras, Paul; Banchs, Albert] Univ Carlos III Madrid, Escuela Politecn Super, Inst IMDEA Networks, E-28911 Leganes, Madrid, Spain.
C3 Universidad Carlos III de Madrid
RP Patras, P (corresponding author), Univ Carlos III Madrid, Escuela Politecn Super, Inst IMDEA Networks, Ave Univ 30, E-28911 Leganes, Madrid, Spain.
EM patras@ieee.org
RI Serrano, Pablo/GLT-9462-2022; Banchs, Albert/G-6307-2015
OI Serrano, Pablo/0000-0002-5176-0013; Patras, Paul/0000-0002-1037-0158
FU EU's Seventh Framework Programme (FP7-ICT) [258053]; Spanish Ministry of
   Science and Innovation under the QUARTET project [TIN2009-13992-C02-01]
FX The research leading to these results was funded by the EU's Seventh
   Framework Programme (FP7-ICT-2009-5) under grant agreement n. 258053
   (MEDIEVAL project) and by the Spanish Ministry of Science and Innovation
   under the QUARTET project (TIN2009-13992-C02-01).
CR [Anonymous], 2005, 80211E IEEE, V802, P11
   [Anonymous], REC G 1010 END US MU
   [Anonymous], 1999, IEEE STD 80211B
   [Anonymous], 2012, 802112012 IEEE
   [Anonymous], 2010, 80211TGAA IEEE
   [Anonymous], 2003, 80211G IEEE
   Argyriou A, 2008, IEEE T MULTIMEDIA, V10, P691, DOI 10.1109/TMM.2008.922776
   Astrom K.J., 1990, COMPUTER CONTROLLED
   BANCHS A, 2003, P 18 INT TEL C ITC18
   Banchs A, 2006, COMPUT NETW, V50, P1749, DOI 10.1016/j.comnet.2005.07.008
   Bianchi G, 2000, IEEE J SEL AREA COMM, V18, P535, DOI 10.1109/49.840210
   Boggia G, 2007, IEEE ACM T NETWORK, V15, P323, DOI 10.1109/TNET.2007.892881
   BUCCIOL P., 2004, P IEEE GLOBECOM 04, V5
   Cavendish D, 2004, IEEE ACM T NETWORK, V12, P893, DOI 10.1109/TNET.2004.836146
   CHEN C.-L., 2007, P ICCCN
   DUFFY K, 2005, IEEE COMM LETT, V9
   Franklin GF, 1997, DIGITAL CONTROL DYNA, V3rd
   Freitag J, 2006, IEEE COMMUN LETT, V10, P611, DOI [10.1109/LCOMM.2006.1665127, 10.1109/LCOMM.2006.060334]
   Grieco LA, 2003, 42ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, PROCEEDINGS, P1586
   He WB, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1413862.1413869
   Heyman D.P., 2004, Stochastic Models in Operations Research, V1
   Hollot CV, 2001, IEEE INFOCOM SER, P1510, DOI 10.1109/INFCOM.2001.916647
   ITU-T, 2007, REC G 1070 OP MOD VI
   Kleinrock L., 1975, Queuing Systems, VI
   Ksentini A, 2006, IEEE COMMUN MAG, V44, P107, DOI 10.1109/MCOM.2006.1580940
   Lambert P, 2006, IEEE T CIRC SYST VID, V16, P134, DOI 10.1109/TCSVT.2005.857783
   MALONE D, 2007, IEEE ACM T NETW, V15
   Nafaa A, 2008, IEEE T PARALL DISTR, V19, P1020, DOI 10.1109/TPDS.2007.70785
   PATRAS P., 2010, IEEE T MOBILE COMPUT, V99
   Patras P, 2009, MOBILE NETW APPL, V14, P697, DOI 10.1007/s11036-008-0121-x
   Serrano P, 2007, GLOB TELECOMM CONF, P5107
   Xiao Y, 2004, IEEE INFOCOM SER, P2152
   Xiao Y, 2007, IEEE T MOBILE COMPUT, V6, P815, DOI 10.1109/TMC.2007.1054
   YANG Y., 2007, P 8 INT WORKSH PERF
   ZHANG Y., 2007, IEEE T CIRC SYST VID, V17
   Zhang Y, 2008, IEEE ICC, P4958, DOI 10.1109/ICC.2008.929
   ZUKERMAN M., 2007, IEEE T WIRE COMM, V6, P1276
NR 37
TC 12
Z9 14
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2012
VL 8
IS 3
AR 29
DI 10.1145/2240136.2240142
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 986AM
UT WOS:000307311700006
DA 2024-07-18
ER

PT J
AU Yang, B
AF Yang, Bo
TI DSI: A Model for Distributed Multimedia Semantic Indexing and Content
   Integration
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Algorithms; Experimentation; Human Factors; Semantic
   representation; image retrieval; distributed indexing
ID TREE
AB Considerable research has been done on the content-based multimedia delivery and access in distributed data repositories. As noted in the literature, there is always a trade-off between multimedia quality and access speed. In addition, the overall performance is greatly determined by the distribution of the multimedia data. In this article, an unsupervised multimedia semantic integration approach for a distributed infrastructure, the Distributed Semantic Indexing (DSI), is presented that addresses both the data quality and search performance. With the ability of summarizing content information and guiding data distribution, the proposed approach is distinguished by: (1) logic-based representation and concise abstraction of the semantic contents of multimedia data, which are further integrated to form a general overview of a multimedia data repository-content signature; (2) application of linguistic relationships to construct a hierarchical metadata based on the content signatures allowing imprecise queries; and (3) achieving the optimal performance in terms of search cost. The fundamental structure of the proposed model is presented. The proposed scheme has been simulated and the simulation results are analyzed and compared against several other approaches that have been advocated in the literature.
C1 Bowie State Univ, Dept Comp Sci, Bowie, MD 20715 USA.
C3 University System of Maryland; Bowie State University
RP Yang, B (corresponding author), Bowie State Univ, Dept Comp Sci, 14000 Jericho Pk Rd, Bowie, MD 20715 USA.
EM byang@bowiestate.edu
FU U.S. National Science Foundation [IIS-0324835]; Chesapeake Information
   Based Aeronautics Consortium
FX This work is supported in part by the U.S. National Science Foundation
   under Grant No. IIS-0324835 and the Chesapeake Information Based
   Aeronautics Consortium.
CR BAO J, 2005, INT J BUSINESS INTEL, V1, P42
   BECKMANN N, 1990, SIGMOD REC, V19, P322, DOI 10.1145/93605.98741
   Castro M, 2004, ACM SIGCOMM COMP COM, V34, P131, DOI 10.1145/972374.972397
   Chang SF, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P443
   Chen Z, 2004, SHOCK, V21, P1
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   Dheap V, 2003, IEEE MILIT COMMUN C, P1048
   Fu AW, 2000, VLDB J, V9, P154, DOI 10.1007/PL00010672
   Grosky WI, 1997, COMMUN ACM, V40, P72, DOI 10.1145/265563.265574
   He J., 2004, P 12 ANN ACM INT C M, P9, DOI DOI 10.1145/1027527.1027531
   HE X, 2003, P IEE INT C COMP VIS
   Hong PY, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P750
   HURSON AR, 2005, ENCY MULTIMEDIA TECH
   Katayama N., 1997, P ACM SIGMOD, P369
   Li J, 2000, IEEE T SIGNAL PROCES, V48, P517, DOI 10.1109/78.823977
   Ratnasamy S., 2001, Proceedings of the 2001 conference on applications, technologies, architectures, and protocols for computer communications, P161
   Salembier P, 2000, IEEE T IMAGE PROCESS, V9, P561, DOI 10.1109/83.841934
   Schneider H, 2002, VIGILIAE CHRISTIAN, V56, P151, DOI 10.1163/15700720260190785
NR 18
TC 3
Z9 3
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2010
VL 6
IS 1
AR 3
DI 10.1145/1671954.1671957
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 563VL
UT WOS:000275163200003
DA 2024-07-18
ER

PT J
AU Nakayama, M
   Takahasi, Y
AF Nakayama, Minoru
   Takahasi, Yosiyuki
TI Estimation of Certainty for Responses to Multiple-Choice Questionnaires
   Using Eye Movements
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human factors; Eye-movements; certainty; scan-path analysis; support
   vector machines
AB To examine the feasibility of estimating the degree of strength of belief (SOB) of responses using eye movements, the scan paths of eye movements were analyzed while subjects reviewed their own responses to multiple choice tasks. All fixation points of eye movements were classified into visual areas, or cells, which corresponded with the positions of answers. Two estimation procedures are proposed using eye-movement data. The first one is identifying SOB using scan-path transitions. By comparing subject's reports of high and low SOB and eye-movement estimations, a significant correct rate of discrimination of SOB was observed. When the threshold of discrimination was controlled, a high rate of correct responses was obtained if it was set at a low level.
   The second procedure is conducting SOB discrimination using support vector machines (SVM) trained with features of fixations. Subject's gazing features were analyzed while they reviewed their own responses. A discrimination model for SOB was trained with several combinations of features to see whether performance of a significant level could be obtained. As a result, a trained model with 3 features (which consist of interval time, vertical difference, and length between fixations) can provide significant discrimination performance for SOB.
   These results provide evidence that strength of belief can be estimated using eye movements.
C1 [Nakayama, Minoru] Tokoy Inst Technol, CRADLE Ctr Res & Org Educ Technol, Meguro Ku, Tokyo 1528552, Japan.
RP Nakayama, M (corresponding author), Tokoy Inst Technol, CRADLE Ctr Res & Org Educ Technol, Meguro Ku, Tokyo 1528552, Japan.
RI Nakayama, Minoru/GSD-2757-2022
OI Nakayama, Minoru/0000-0001-5563-6901
CR [Anonymous], 2003, J JAPANESE SOC ARTIF
   [Anonymous], EL P 2 C COMM GAZ IN
   [Anonymous], P ACM SIGIR 2005
   Brooke J, 1996, USABILITY EVALUATION, V189, P4
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CHOI YS, 1995, OPTOMETRY VISION SCI, V72, P439, DOI 10.1097/00006324-199507000-00003
   Duda R. O., 2000, PATTERN CLASSIFICATI
   IMAI H, 1995, MEMORY, P29
   ISHIZAKI M, 2001, DISCORSE DIALOGUE
   Iwahashi N, 2003, RO-MAN 2003: 12TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P385
   Jackson P., 2002, Natural language processingfor online applications. Text retrieval, extraction and categorization
   *JSVS, 2001, SHIK JYOUH SYOR HDB
   *NAC IM TECHN, 2004, EY MARK DAT AN SOFTW
   *NAC IM TECHN, 2007, EY MARK REC, V8
   Nakayama M., 2007, P EUR S ART NEUR NET, P91
   NOTON D, 1971, SCIENCE, V171, P308, DOI 10.1126/science.171.3968.308
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   SALOJARVI J, 2004, P WORKSH PROC SENS I
   SHIGEMASU K, 1993, BEHAVIORMETRIKA, V20, P161
   Shimodaira H., 2001, NEURAL INFORM PROCES, V14, P921
   SHINOTSUKA H, 1993, JPN J PSYCHOL, V63, P396, DOI 10.4992/jjpsy.63.396
   TOKUNAGA T, 1999, INFORM RETRIEVAL NAT
   TSUDA K, 2003, PATTERN NINSIKI GAKU
   Underwood G, 2005, COGNITIVE PROCESSES
   Zhang Y., 2007, ITEM RESPONSE MODEL
NR 25
TC 4
Z9 4
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2008
VL 5
IS 2
AR 14
DI 10.1145/1413862.1413867
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 376AU
UT WOS:000261155900005
DA 2024-07-18
ER

PT J
AU Chen, SQ
   Chen, SP
   Guo, HP
   Shen, B
   Jajodia, S
AF Chen, Songqing
   Chen, Shiping
   Guo, Huiping
   Shen, Bo
   Jajodia, Sushil
TI Achieving simultaneous distribution control and privacy protection for
   Internet media delivery
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
ID VIDEO
AB Massive Internet media distribution demands prolonged continuous consumption of networking and disk bandwidths in large capacity. Many proxy-based Internet media distribution algorithms and systems have been proposed, implemented, and evaluated to address the scalability and performance issue. However, few of them have been used in practice, since two important issues are not satisfactorily addressed. First, existing proxy-based media distribution architectures lack an efficient media distribution control mechanism. Without copyright protection, content providers are hesitant to use proxy-based fast distribution techniques. Second, little has been done to protect client privacy during content accesses on the Internet. Straightforward solutions to address these two issues independently lead to conflicts. For example, to enforce distribution control, only legitimate users should be granted access rights. However, this normally discloses more information ( such as which object the client is accessing) other than the client identity, which conflicts with the client's desire for privacy protection. In this article, we propose a unified proxy-based media distribution protocol to effectively address these two problems simultaneously. We further design a set of new algorithms in a cooperative proxy environment where our proposed scheme works efficiently and practically. Simulation-based experiments are conducted to extensively evaluate the proposed system. Preliminary results demonstrate the effectiveness of our proposed strategy.
C1 [Chen, Songqing] George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
   [Chen, Shiping] Sybase Inc, Dept ITSG Secur, Dublin, CA 94568 USA.
   [Guo, Huiping] Calif State Univ Los Angeles, Dept Comp Sci, Los Angeles, CA 90032 USA.
   [Shen, Bo] HP Labs, Media & Mobile Syst Lab, Palo Alto, CA 94304 USA.
   [Jajodia, Sushil] George Mason Univ, Ctr Secur Info Syst, Fairfax, VA 22030 USA.
C3 George Mason University; California State University System; California
   State University Los Angeles; Hewlett-Packard; George Mason University
RP Chen, SQ (corresponding author), George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
EM sqchen@cs.gmu.edu; shiping.chen@sybase.com; hpguo@calstatela.edu;
   bo@blueapple.mobi; jajodia@gmu.edu
RI guo, hui/KRO-5200-2024; Chen, Shiping/B-7492-2011
OI Chen, Shiping/0000-0002-4603-0024
CR [Anonymous], P ACM WORKSH PRIV EL
   [Anonymous], 2000, Rethinking public key infrastructures and digital certificates: building in privacy
   [Anonymous], P ACM WORKSH PRIV EL
   [Anonymous], P ACM C COMP COMM SE
   BAO F, 2001, P 6 AUSTR C INF SEC
   BONATTI PA, 2002, J COMPUT SECUR, V10, P3
   BONEH D, 2001, P USENIX SEC WASH DC
   CHAE Y, 2002, IEEE J SELECT AREAS
   CHAUM D, 1985, COMMUN ACM, V24, P2
   CHEN S, 2006, P 14 IEEE INT WORKSH
   CHIU MY, 1997, P 13 INT C DAT ENG B
   Chor B, 1997, P 29 ANN ACM S THEOR
   CORON J, 1999, P CRYPT SANT BARB CA
   DIFFIE W, 1976, IEEE T INFORM THEORY, V22, P644, DOI 10.1109/TIT.1976.1055638
   Dingledine R., 2004, P 13 USENIX SEC S
   FREEDMAN MJ, 2002, P 9 ACM C COMP COMM
   GAMAL T, 1985, IEEE T INFORM THEORY, P4
   GERTNER Y, 1998, P 30 ANN ACM S THEOR
   GOLLE P, 2004, P ACM C COMP COMM SE
   GRIWODZ C, 1998, P 6 ACM MULT C BRIST
   KANGASHARJU J, 2001, P IEEE INF ANCH AK
   MIAO Z, 2002, IEEE J SELECT AREAS
   PARK JS, 1999, P 22 NAT INF SYST SE
   Reiter MK, 1999, COMMUN ACM, V42, P32, DOI 10.1145/293411.293778
   REITER MK, 2004, P ACM C COMP COMM SE
   REJAIE R, 2000, P IEEE INFOCOM TELAV
   ROY S, 2004, P 9 INT WORKSH WEB C
   *RSA LAB, 2002, PKCS 1 V2 1 RSA CRYP
   SCHOJER P, 2003, P WWW BUD HUNG
   SEN S, 1999, P IEEE INFOCOM NEW Y
   SHI C, 1998, P 6 ACM MULT C BRIST
   Stallings W., 1998, CRYPTOGRAPHY NETWORK, Vsecond
   TOSUN AS, 2002, SECURE VIDEO TRANSMI
   VISCONTI PPA, 2000, P C COMP COMM SEC AT
   Wen JT, 2002, IEEE T CIRC SYST VID, V12, P545, DOI 10.1109/TCSVT.2002.800321
   WINSBOROUGH W, 2002, P 3 WORKSH POL DISTR
   WU K, 2001, P WWW HONGK CHIN
   WU Y, 2004, P ACM MULT C NEW YOR
   YANG H, 2004, P IEEE VTC WIR SEC S
   YEUNG S, 2002, P ACM MULT C JUAN LE
   YEUNG S, 2005, IEEE T MULTIMEDIA, V7
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
   [No title captured]
NR 43
TC 2
Z9 2
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 2
AR 9
DI 10.1145/1352012.1352013
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 306RG
UT WOS:000256264900001
OA Bronze
DA 2024-07-18
ER

PT J
AU Chen, RY
   Li, JZ
   Zhang, H
   Sheng, CC
   Liu, L
   Cao, XC
AF Chen, Ruoyu
   Li, Jingzhi
   Zhang, Hua
   Sheng, Changchong
   Liu, Li
   Cao, Xiaochun
TI Sim2Word: Explaining Similarity with Representative Attribute Words via
   Counterfactual Explanations
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Explainable AI; counterfactual explanation; similarity explanation
ID FACE; MODELS
AB Recently, we have witnessed substantial success using the deep neural network in many tasks. Although there still exist concerns about the explainability of decision making, it is beneficial for users to discern the defects in the deployed deep models. Existing explainable models either provide the image-level visualization of attention weights or generate textual descriptions as post hoc justifications. Different from existing models, in this article we propose a new interpretation method that explains the image similarity models by salience maps and attribute words. Our interpretation model contains visual salience maps generation and the counterfactual explanation generation. The former has two branches: global identity relevant region discovery and multi-attribute semantic region discovery. The first branch aims to capture the visual evidence supporting the similarity score, which is achieved by computing counterfactual feature maps. The second branch aims to discover semantic regions supporting different attributes, which helps to understand which attributes in an image might change the similarity score. Then, by fusing visual evidence from two branches, we can obtain the saliencemaps indicating important response evidence. The latter will generate the attribute words that best explain the similarity using the proposed erasing model. The effectiveness of our model is evaluated on the classical face verification task. Experiments conducted on two benchmarks-VGGFace2 and Celeb-A-demonstrate that our model can provide convincing interpretable explanations for the similarity. Moreover, our algorithm can be applied to evidential learning cases, such as finding the most characteristic attributes in a set of face images, and we verify its effectiveness on the VGGFace2 dataset.
C1 [Chen, Ruoyu; Li, Jingzhi; Zhang, Hua; Liu, Li] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China.
   [Chen, Ruoyu; Li, Jingzhi; Zhang, Hua] Univ Chinese Acad Sc, Sch Cyber Secur, 19 Yuquan Rd, Beijing 100049, Peoples R China.
   [Sheng, Changchong; Liu, Li] Natl Univ Def Technol NUDT, Coll Syst Engn, 109 Deya Rd, Changsha 430074, Hunan, Peoples R China.
   [Cao, Xiaochun] Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen Campus ,66 Gongchang Rd, Shenzhen 518107, Guangdong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   National University of Defense Technology - China; Sun Yat Sen
   University
RP Li, JZ (corresponding author), Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China.
EM chenruoyu@iie.ac.cn; lijingzhi@iie.ac.cn; zhanghua@iie.ac.cn;
   shengcc_nudt@163.com; li.liu@oulu.fi; caoxiaochun@mail.sysu.edu.cn
RI yang, le/KFB-5420-2024; Chen, Zheng/KCY-2338-2024; Zhou,
   Xinyi/KGM-6689-2024; ren, jun/KHG-7717-2024; WANG, YANAN/KCL-4840-2024;
   li, feiyang/KHW-5210-2024; Chen, Ruoyu/IYJ-9466-2023; wang,
   rong/KFQ-7187-2024; Liu, Donghua/KEJ-1974-2024; Liu,
   yuqing/KEI-3260-2024; xie, jing/KDO-9486-2024; zhang, can/KHC-5357-2024;
   Huang, Yong/KFA-1191-2024
OI Chen, Ruoyu/0000-0001-7630-7154; Liu, Donghua/0000-0002-5830-9540; Liu,
   li/0000-0002-2011-2873
FU National Key R&D Program of China [2021YFB3100800]; National Natural
   Science Foundation of China [62072454, 62132006]; Beijing Natural
   Science Foundation [4202084, M22006]
FX This work was supported by the National Key R&D Program of China (grant
   2021YFB3100800), National Natural Science Foundation of China (grants
   62072454 and 62132006), and Beijing Natural Science Foundation (grants
   4202084 and M22006).
CR Abujabal A, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1191, DOI 10.1145/3038912.3052583
   An Xiang, 2020, arXiv
   Hendricks LA, 2018, Arxiv, DOI arXiv:1806.09809
   Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Bai Y., 2022, P 2022 SIAM INT C DA, P495
   Bao W., 2021, IEEE CVF ICCV, P13349
   Belkoura S, 2019, EXPERT SYST APPL, V137, P191, DOI 10.1016/j.eswa.2019.07.001
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Choi E, 2016, ADV NEUR IN, V29
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dhurandhar A, 2018, ADV NEUR IN, V31
   Eberle O, 2022, IEEE T PATTERN ANAL, V44, P1149, DOI 10.1109/TPAMI.2020.3020738
   Erhan Dumitru, 2009, VISUALIZING HIGHER L, V1341, P1
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Wilson AG, 2020, Arxiv, DOI arXiv:2001.10995
   Goyal Y, 2019, PR MACH LEARN RES, V97
   Gulshad Sadaf, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P35, DOI 10.1145/3372278.3390672
   Guo P, 2021, Arxiv, DOI arXiv:1805.08969
   Guo P, 2022, NEUROCOMPUTING, V469, P65, DOI 10.1016/j.neucom.2021.10.071
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Hase P, 2021, ADV NEUR IN
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2018, LECT NOTES COMPUT SC, V11206, P269, DOI 10.1007/978-3-030-01216-8_17
   Hendricks LA, 2016, LECT NOTES COMPUT SC, V9908, P3, DOI 10.1007/978-3-319-46493-0_1
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Jiang HR, 2021, IEEE INT CONF COMP V, P1503, DOI 10.1109/ICCVW54120.2021.00175
   Jiang SY, 2021, IEEE T IMAGE PROCESS, V30, P2771, DOI 10.1109/TIP.2021.3052084
   Jiang Songyao, 2019, 2019 11 INT S ADV TO, P1
   Kim B, 2018, PR MACH LEARN RES, V80
   Kim JM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8330, DOI 10.1109/ICCV48922.2021.00824
   Kim J, 2019, PROC CVPR IEEE, P10583, DOI [10.1109/CVP8.2019.01084, 10.1109/CVPR.2019.01084]
   Kim J, 2018, LECT NOTES COMPUT SC, V11206, P577, DOI 10.1007/978-3-030-01216-8_35
   Kim J, 2017, IEEE I CONF COMP VIS, P2961, DOI 10.1109/ICCV.2017.320
   Kim Jinkyu, 2020, PROC IEEECVF C COMPU, P9661
   Lin YS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3469288
   Lin ZH, 2017, Arxiv, DOI [arXiv:1703.03130, DOI 10.48550/ARXIV.1703.03130]
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Luss R, 2021, Arxiv, DOI arXiv:1905.12698
   Mao JH, 2015, Arxiv, DOI arXiv:1412.6632
   Mosca Edoardo, 2020, Explainability of Hate Speech Detection Models
   Obeso AM, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108411
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Park DH, 2018, PROC CVPR IEEE, P8779, DOI 10.1109/CVPR.2018.00915
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Petsiuk V., 2018, RISE RANDOMIZED INPU
   Petsiuk V, 2021, PROC CVPR IEEE, P11438, DOI 10.1109/CVPR46437.2021.01128
   Plummer B.A., 2020, EUR C COMP VIS, P652
   Raman Mrigank, 2020, P INT C LEARNING REP
   Ray A, 2021, Arxiv, DOI [arXiv:2103.14712, DOI 10.48550/ARXIV.2103.14712]
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sendik O, 2019, COMPUT GRAPH FORUM, V38, P405, DOI 10.1111/cgf.13647
   Sensoy M, 2018, ADV NEUR IN, V31
   Terhorst P, 2021, Arxiv, DOI [arXiv:2012.01030, DOI arXiv:2012.01030.v1]
   Tintarev N, 2011, RECOMMENDER SYSTEMS HANDBOOK, P479, DOI 10.1007/978-0-387-85820-3_15
   Ustun B, 2016, MACH LEARN, V102, P349, DOI 10.1007/s10994-015-5528-6
   Vaughan J, 2018, Arxiv, DOI arXiv:1806.01933
   Wang Hanqing, 2022, P IEEECVF C COMPUTER, P15471
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang HJ, 2020, Cambria Sinophone Wo, P111, DOI 10.1109/CVPRW50498.2020.00020
   Wang Pei, 2019, P 2019 ANN C NEURAL, P1372
   Wang Pei, 2020, P IEEE CVF C COMP VI, P8981, DOI 10.1109/CVPR42600.2020.00900
   Williford Jonathan R., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P248, DOI 10.1007/978-3-030-58621-8_15
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Xia BH, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3474557
   Xian YK, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3775, DOI 10.1145/3447548.3467211
   Xian YK, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1645, DOI 10.1145/3340531.3412038
   Yao BZ, 2010, P IEEE, V98, P1485, DOI 10.1109/JPROC.2010.2050411
   Ye K., 2021, P IEEECVF WINTER C A, P3520
   Yiding Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P672, DOI 10.1007/978-3-030-58565-5_40
   Yin BJ, 2019, IEEE I CONF COMP VIS, P9347, DOI 10.1109/ICCV.2019.00944
   Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zheng M, 2020, Arxiv, DOI [arXiv:2008.06035, 10.48550/arXiv.2008.06035, DOI 10.48550/ARXIV.2008.06035]
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu SJ, 2021, IEEE T IMAGE PROCESS, V30, P7593, DOI 10.1109/TIP.2021.3107214
   Zhu Xiangyu, 2022, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Zimmermann Roland, 2021, ADV NEURAL INFORM PR
   Zintgraf Luisa M., 2017, P INT C LEARNING REP
NR 83
TC 0
Z9 0
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 220
DI 10.1145/3563039
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200042
OA Bronze
DA 2024-07-18
ER

PT J
AU Niu, TZ
   Chen, ZD
   Luo, X
   Zhang, PF
   Huang, Z
   Xu, XS
AF Niu, Tian-Zi
   Chen, Zhen-Duo
   Luo, Xin
   Zhang, Peng-Fei
   Huang, Zi
   Xu, Xin-Shun
TI Video Captioning by Learning from Global Sentence and Looking Ahead
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video captioning; future word; global sentence features
AB Video captioning aims to automatically generate natural language sentences describing the content of a video. Although encoder-decoder-based models have achieved promising progress, it is still very challenging to effectively model the linguistic behavior of humans in generating video captions. In this paper, we propose a novel video captioning model by learning from gLobal sEntence and looking AheaD, LEAD for short. Specifically, LEAD consists of two modules: a Vision Module (VM) and a Language Module (LM). There-into, VM is a novel attention network, which can map visual features to high-level language space and model entire sentences explicitly. LM can not only effectively make use of the information of the previous sequence when generating the current word, but also have a look at the future word. Therefore, based on VM and LM, LEAD can obtain global sentence information and future word information to make video captioning more like a fill-in-the-blank task than a word-by-word sentence generation. In addition, we also propose an autonomous strategy and a multi-stage training scheme to optimize the model, which can mitigate the problem of information leakage. Extensive experiments show that LEAD outperforms some state-of-the-art methods on MSR-VTT, MSVD, and VATEX, demonstrating the effectiveness of the proposed approach in video captioning. In addition, we release the code of our proposed model to be publicly available.(1)
C1 [Niu, Tian-Zi; Chen, Zhen-Duo; Luo, Xin; Xu, Xin-Shun] Shan dong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Zhang, Peng-Fei; Huang, Zi] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld, Australia.
C3 Shandong University; University of Queensland
RP Xu, XS (corresponding author), Shan dong Univ, Sch Software, Jinan 250101, Peoples R China.
EM 201920560@mail.sdu.edu.cn; chenzd.sdu@gmail.com; luoxin.lxin@gmail.com;
   mima.zpf@gmail.com; huang@itee.uq.edu.au; xuxin-shun@sdu.edu.cn
RI Luo, Xin/HNR-3191-2023
OI Luo, Xin/0000-0002-6901-5476; Zhang, Peng-Fei/0000-0002-6790-2098;
   HUANG, ZI/0000-0002-9738-4949; Chen, Zhen-Duo/0000-0002-3481-4892
FU National Natural Science Foundation of China [62172256, 62202278,
   62202272]; Natural Science Foundation of Shandong Province [ZR2019ZD06];
   Major Program of the National Natural Science Foundation of China
   [61991411]; Quan Cheng Laboratory
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62172256, 62202278, and 62202272, in
   part by the Natural Science Foundation of Shandong Province under Grant
   ZR2019ZD06, and in part by the Major Program of the National Natural
   Science Foundation of China under Grant 61991411, and was also supported
   in part by the Quan Cheng Laboratory.
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Ballas N., 2016, INT C LEARNING REPRE
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen HR, 2020, FRONT ARTIF INTEL AP, V325, P1079, DOI 10.3233/FAIA200204
   Chen JW, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3539225
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Cherian A, 2020, IEEE WINT CONF APPL, P1606, DOI 10.1109/WACV45572.2020.9093291
   Deng JC, 2022, IEEE T CIRC SYST VID, V32, P880, DOI 10.1109/TCSVT.2021.3063423
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong Shanshan, 2021, ACM T MULTIM COMPUT, V19, P18
   Fang SC, 2021, PROC CVPR IEEE, P7094, DOI 10.1109/CVPR46437.2021.00702
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang YQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4600, DOI 10.1145/3394171.3416290
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Liu F., 2020, ADV NEURAL INFORM PR, V33, P1865
   Liu FL, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P281
   Liu S, 2021, IEEE T PATTERN ANAL, V43, P3259, DOI 10.1109/TPAMI.2019.2940007
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Ni JM, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P1864
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Patrick M., 2021, P INT C LEARNING REP
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Radford A, 2021, PR MACH LEARN RES, V139
   Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514
   Shi XJ, 2015, ADV NEUR IN, V28
   Shi Yaya, 2023, ACM T MULTIM COMPUT, V19, P21
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan GC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P745
   Tang PJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303083
   Thomason J., 2014, COLING, P1218
   Vaidya J, 2022, IEEE WINT CONF APPL, P2442, DOI 10.1109/WACV51458.2022.00250
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang GH, 2022, J EXP THEOR ARTIF IN, V34, P483, DOI 10.1080/0952813X.2021.1883745
   Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Wu AM, 2020, IEEE T CIRC SYST VID, V30, P4299, DOI 10.1109/TCSVT.2019.2956593
   Wu Hanjie, 2022, ACM Trans. Multimedia Comput. Commun. Appl., V18
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Yang B, 2021, AAAI CONF ARTIF INTE, V35, P3119
   Yang L, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3386725
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zhang ZQ, 2021, PROC CVPR IEEE, P9832, DOI 10.1109/CVPR46437.2021.00971
   Zheng Q., 2020, 2020 CVPR, P13093, DOI 10.1109/CVPR42600.2020.01311
   Zheng Y, 2022, IEEE T CIRC SYST VID, V32, P31, DOI 10.1109/TCSVT.2021.3058626
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 66
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2023
VL 19
IS 5
SU S
AR 171
DI 10.1145/3587252
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L0SK0
UT WOS:001020437100007
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Chao, FY
   Hamidouche, W
   Deforges, O
AF Zhang, Yi
   Chao, Fang-Yi
   Hamidouche, Wassim
   Deforges, Olivier
TI PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Audiovisual; saliency; 360 degrees; panorama; segmentation; conditional
   variational auto-encoder
ID OBJECT DETECTION; PREDICTION; NETWORK
AB Object-level audiovisual saliency detection in 360 degrees panoramic real-life dynamic scenes is important for exploring and modeling human perception in immersive environments, also for aiding the development of virtual, augmented, and mixed reality applications in fields such as education, social network, entertainment, and training. To this end, we propose a new task, panoramic audiovisual salient object detection, (PAV-SOD1), which aims to segment the objects grasping most of the human attention in 360 degrees panoramic videos reflecting real-life daily scenes. To support the task, we collect PAVS10K, the first panoramic video dataset for audiovisual salient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting PAV-SOD, and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks. The coarse-to-fine annotations enable multi-perspective analysis regarding PAV-SOD modeling. We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our PAVS10K. Besides, we propose a new baseline network, which takes advantage of both visual and audio cues of 360 degrees video frames by using a new conditional variational auto-encoder (CVAE). Our CVAE-based audiovisual network, namely, CAV-Net, consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network, and audiovisual distribution estimation modules. As a result, our CAV-Net outperforms all competing models and is able to estimate the aleatoric uncertainties within PAVS10K. With extensive experimental results, we gain several findings about PAV-SOD challenges and insights towards PAV-SOD model interpretability. We hope that our work could serve as a starting point for advancing SOD towards immersive media.
C1 [Zhang, Yi; Hamidouche, Wassim; Deforges, Olivier] Univ Rennes, CNRS, INSA Rennes, IETR,UMR 6164, F-6164 Rennes, France.
   [Chao, Fang-Yi] Trin Coll Dublin, Dublin, Ireland.
C3 Institut National des Sciences Appliquees de Rennes; Universite de
   Rennes; Centre National de la Recherche Scientifique (CNRS); CNRS -
   Institute for Engineering & Systems Sciences (INSIS); Trinity College
   Dublin
RP Zhang, Y (corresponding author), Univ Rennes, CNRS, INSA Rennes, IETR,UMR 6164, F-6164 Rennes, France.
EM yi.zhang1@insa-rennes.fr; fang-yi.chao@tcd.ie;
   Wassim.Hamidouche@insa-rennes.fr; Olivier.Deforges@insa-rennes.fr
OI DEFORGES, olivier/0000-0003-0750-0959
CR Abawi Fares, 2021, P 30 INT JOINT C ART
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Afouras T., 2020, P EUR C COMP VIS, V12363, P208, DOI 10.1007/978-3-030- 58523-5-13
   Afouras T, 2022, Arxiv, DOI arXiv:2104.06401
   Agtzidis I, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1007, DOI 10.1145/3343031.3350947
   Nguyen A, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P279, DOI 10.1145/3304109.3325820
   Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27
   Aytar Y, 2016, ADV NEUR IN, V29
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Chao F.-Y., 2020, IEEE INT CONF MULTI, P1, DOI DOI 10.1109/icmew46912.2020.9105956
   Chao FY, 2020, IEEE I C VI COM I PR, P355, DOI 10.1109/vcip49819.2020.9301766
   Chao FY, 2018, IEEE INT CONF MULTI
   Chen CG, 2021, PROC CVPR IEEE, P15511, DOI 10.1109/CVPR46437.2021.01526
   Chen Changan, 2020, LNCS, P17, DOI DOI 10.1007/978-3-030-58539-6_2
   Chen HL, 2021, Arxiv, DOI arXiv:2112.04432
   Chen HL, 2021, PROC CVPR IEEE, P16862, DOI 10.1109/CVPR46437.2021.01659
   Chen HL, 2020, INT CONF ACOUST SPEE, P721, DOI [10.1109/ICASSP40776.2020.9053174, 10.1109/icassp40776.2020.9053174]
   Chen Q, 2021, AAAI CONF ARTIF INTE, V35, P1063
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cheng MM, 2022, IEEE T PATTERN ANAL, V44, P8006, DOI 10.1109/TPAMI.2021.3107956
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cokelek Mert, 2021, P 17 INT C MACHINE V, P1
   Cong RM, 2022, Arxiv, DOI arXiv:2204.08917
   Cong Runmin, 2021, IEEE T GEOSCI REM SE, V2021
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Dahou Yasser, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12663), P305, DOI 10.1007/978-3-030-68796-0_22
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   Deng S., 2021, P IEEECVF INT C COMP, P14030
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Djilali Yasser Abdelaziz Dahou, 2021, P IEEECVF INT C COMP, P3750
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan CL, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329119
   Fan DP, 2018, Arxiv, DOI arXiv:1805.10421
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fan Q, 2021, PROC CVPR IEEE, P12283, DOI 10.1109/CVPR46437.2021.01211
   Fang CW, 2021, Arxiv, DOI arXiv:2102.09133
   Gan C, 2020, IEEE INT CONF ROBOT, P9701, DOI [10.1109/ICRA40945.2020.9197008, 10.1109/icra40945.2020.9197008]
   Gao RH, 2021, Arxiv, DOI arXiv:2109.07991
   Gao RH, 2019, IEEE I CONF COMP VIS, P3878, DOI 10.1109/ICCV.2019.00398
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Garg R., 2021, arXiv
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu YC, 2020, AAAI CONF ARTIF INTE, V34, P10869
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu KJ, 2019, PROC CVPR IEEE, P8838, DOI 10.1109/CVPR.2019.00905
   Hu Di, 2021, IEEE T PATTERN ANAL, V2021
   Hu Di, 2020, ADV NEURAL INFORM PR
   Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang MK, 2020, IEEE SIGNAL PROC LET, V27, P1819, DOI 10.1109/LSP.2020.3028192
   Jain S, 2021, IEEE INT C INT ROBOT, P3520, DOI 10.1109/IROS51168.2021.9635989
   Ji GP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4902, DOI 10.1109/ICCV48922.2021.00488
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Kim H, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425544
   Kingma D. P., 2014, arXiv
   Li C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P932, DOI 10.1145/3240508.3240581
   Li CY, 2019, IEEE T GEOSCI REMOTE, V57, P9156, DOI 10.1109/TGRS.2019.2925070
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li J, 2020, IEEE J-STSP, V14, P38, DOI 10.1109/JSTSP.2019.2957982
   Li J, 2018, IEEE T IMAGE PROCESS, V27, P349, DOI 10.1109/TIP.2017.2762594
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu X, 2022, ARXIV
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Ma GX, 2020, IEEE T VIS COMPUT GR, V26, P3535, DOI 10.1109/TVCG.2020.3023636
   Mahadevan S, 2023, Arxiv, DOI arXiv:2008.11516
   Majumder S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P275, DOI 10.1109/ICCV48922.2021.00034
   MEREDITH MA, 1983, SCIENCE, V221, P389, DOI 10.1126/science.6867718
   MEREDITH MA, 1986, J NEUROPHYSIOL, V56, P640, DOI 10.1152/jn.1986.56.3.640
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Narasimhan Medhini, 2022, P IEEECVFWINTER C AP, P3761
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   PedroMorgado Nuno Nvasconcelos, 2018, ADV NEURAL INF PROCE, V31
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qiao ML, 2021, IEEE T MULTIMEDIA, V23, P748, DOI 10.1109/TMM.2020.2987682
   Tavakoli HR, 2020, Arxiv, DOI arXiv:1905.10693
   Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218
   Ramenahalli S, 2020, AI-BASEL, V1, P487, DOI 10.3390/ai1040030
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ren SC, 2021, PROC CVPR IEEE, P15450, DOI 10.1109/CVPR46437.2021.01520
   Rhee T, 2017, IEEE T VIS COMPUT GR, V23, P1302, DOI 10.1109/TVCG.2017.2657178
   Rouditchenko Andrew, 2019, ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings, P2357, DOI 10.1109/ICASSP.2019.8682467
   Schmidt C., 2022, P IEEE CVF WINT C AP, P1200
   Shang-Hua Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P702, DOI 10.1007/978-3-030-58539-6_42
   Shuaiyang Cheng, 2021, Intelligent Computing Theories and Application: 17th International Conference, ICIC 2021, Proceedings. Lecture Notes in Computer Science, Information Systems and Applications, incl. Internet/Web, and HCI (12837), P510, DOI 10.1007/978-3-030-84529-2_43
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Sohn K, 2015, ADV NEUR IN, V28
   Tang J, 2020, IEEE T CIRC SYST VID, V30, P4421, DOI 10.1109/TCSVT.2019.2951621
   Tian YP, 2021, PROC CVPR IEEE, P2744, DOI 10.1109/CVPR46437.2021.00277
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Tsiami A, 2020, PROC CVPR IEEE, P4765, DOI 10.1109/CVPR42600.2020.00482
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Vasudevan Arun Balajee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P638, DOI 10.1007/978-3-030-58548-8_37
   Wang GT, 2021, PROC CVPR IEEE, P15114, DOI 10.1109/CVPR46437.2021.01487
   Wang KH, 2019, INT CONF ACOUST SPEE, P3642, DOI [10.1109/icassp.2019.8683093, 10.1109/ICASSP.2019.8683093]
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   WenguanWang Qiuxia Lai, 2021, IEEE T PATTERN ANAL, V2021
   Wu YH, 2022, IEEE T PATTERN ANAL, V44, P10261, DOI 10.1109/TPAMI.2021.3134684
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Yan PX, 2019, IEEE I CONF COMP VIS, P7283, DOI 10.1109/ICCV.2019.00738
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang J, 2020, J MULTIMODAL USER IN, V14, P337, DOI 10.1007/s12193-020-00331-1
   Yao SY, 2021, IEEE IMAGE PROC, P1604, DOI 10.1109/ICIP42928.2021.9506089
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yufan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P413, DOI 10.1007/978-3-030-58565-5_25
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7233, DOI 10.1109/ICCV.2019.00733
   Zhang D., 2020, Adv. Neural Info. Process. Syst., V33, P12236
   Zhang J., 2021, arXiv
   Zhang Jing, 2020, P IEEECVF C COMPUTER
   Zhang KH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14749, DOI 10.1109/ICCV48922.2021.01450
   Zhang KH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8761, DOI 10.1109/ICCV48922.2021.00866
   Zhang M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1533, DOI 10.1109/ICCV48922.2021.00158
   Zhang M, 2019, ADV NEUR IN, V32
   Zhang PP, 2021, IEEE T IMAGE PROCESS, V30, P3204, DOI 10.1109/TIP.2020.3045624
   Zhang Q., 2020, Adv. Neural Inf. Process. Syst.
   Zhang Q, 2021, IEEE T CIRC SYST VID, V31, P1804, DOI 10.1109/TCSVT.2020.3014663
   Zhang QJ, 2021, IEEE T IMAGE PROCESS, V30, P1305, DOI 10.1109/TIP.2020.3042084
   Zhang Y, 2021, Arxiv, DOI arXiv:2104.13916
   Zhang Y, 2020, IEEE IMAGE PROC, P3458, DOI 10.1109/ICIP40778.2020.9191158
   Zhang YM, 2017, I C VIRTUAL REALITY, P18, DOI 10.1109/ICVRV.2017.00013
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zhao PY, 2020, AAAI CONF ARTIF INTE, V34, P12959
   Zhao WB, 2021, PROC CVPR IEEE, P16821, DOI 10.1109/CVPR46437.2021.01655
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou TF, 2020, AAAI CONF ARTIF INTE, V34, P13066
   Zhu YC, 2020, IEEE T MULTIMEDIA, V22, P2331, DOI 10.1109/TMM.2019.2957986
NR 145
TC 1
Z9 1
U1 6
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 101
DI 10.1145/3565267
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300001
DA 2024-07-18
ER

PT J
AU Wu, HS
   Wang, ZZ
   Li, ZY
   Wen, ZK
   Qin, J
AF Wu, Huisi
   Wang, Zhaoze
   Li, Zhuoying
   Wen, Zhenkun
   Qin, Jing
TI Context Prior Guided Semantic Modeling for Biomedical Image Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Global context information; spatial-semantic gap; biomedical image
   segmentation
ID NETWORK
AB Most state-of-the-art deep networks proposed for biomedical image segmentation are developed based on U-Net. While remarkable success has been achieved, its inherent limitations hinder it from yielding more precise segmentation. First, its receptive field is limited due to the fixed kernel size, which prevents the network from modeling global context information. Second, when spatial information captured by shallower layer is directly transmitted to higher layers by skip connections, the process inevitably introduces noise and irrelevant information to feature maps and blurs their semantic meanings. In this article, we propose a novel segmentation network equipped with a new context prior guidance (CPG) module to overcome these limitations for biomedical image segmentation, namely context prior guidance network (CPG-Net). Specifically, we first extract a set of context priors under the supervision of a coarse segmentation and then employ these context priors to model the global context information and bridge the spatial-semantic gap between high-level features and low-level features. The CPG module contains two major components: context prior representation (CPR) and semantic complement flow (SCF). CPR is used to extract pixels belonging to the same objects and hence produce more discriminative features to distinguish different objects. We further introduce deep semantic information for each CPR by the SCF mechanism to compensate the semantic information diluted during the decoding. We extensively evaluate the proposed CPG-Net on three famous biomedical image segmentation tasks with diverse imaging modalities and semantic environments. Experimental results demonstrate the effectiveness of our network, consistently outperforming state-of-the-art segmentation networks in all the three tasks. Codes are available at https://github.com/zzw- szu/CPGNet.
C1 [Wu, Huisi; Wang, Zhaoze; Li, Zhuoying; Wen, Zhenkun; Qin, Jing] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Qin, Jing] Hong Kong Polytech Univ, Ctr Smart Hlth, Sch Nursing, Kowloon, Hong Kong, Peoples R China.
C3 Shenzhen University; Hong Kong Polytechnic University
RP Wu, HS (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM hswu@szu.edu.cn; harry.qin@polyu.edu.hk
RI Qin, Jing/J-9807-2016; Qin, Jing/JMC-1371-2023
OI Qin, Jing/0000-0002-7059-0929; Wang, Zhaoze/0000-0002-9600-3235; Wu,
   Huisi/0000-0002-0399-9089; Li, Zhuoying/0000-0002-9174-1140
FU National Natural Science Foundation of China [61973221]; Natural Science
   Foundation of Guangdong Province, China [2019A1515011165]; COVID-19
   Prevention Project of Guangdong Province, China [2020KZDZX1174]; Hong
   Kong Research Grant Council under General Research Fund Scheme
   [15205919]; Major Project of the New Generation of Artificial
   Intelligence [2018AAA0102900]
FX This work was supported partly by National Natural Science Foundation of
   China (No. 61973221), Natural Science Foundation of Guangdong Province,
   China (No. 2019A1515011165), the COVID-19 Prevention Project of
   Guangdong Province, China (No. 2020KZDZX1174), the Major Project of the
   New Generation of Artificial Intelligence (No. 2018AAA0102900), and the
   Hong Kong Research Grant Council under General Research Fund Scheme
   (Project no. 15205919).
CR Caicedo JC, 2019, NAT METHODS, V16, P1247, DOI 10.1038/s41592-019-0612-7
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Codella N, 2019, Arxiv, DOI arXiv:1902.03368
   Crum WR, 2006, IEEE T MED IMAGING, V25, P1451, DOI 10.1109/TMI.2006.880587
   Ding HH, 2019, PROC CVPR IEEE, P8877, DOI 10.1109/CVPR.2019.00909
   Ding HH, 2020, IEEE T IMAGE PROCESS, V29, P3520, DOI 10.1109/TIP.2019.2962685
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Feng SL, 2020, IEEE T MED IMAGING, V39, P3008, DOI 10.1109/TMI.2020.2983721
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025
   Isensee F, 2021, NAT METHODS, V18, P203, DOI 10.1038/s41592-020-01008-z
   Jaeger S, 2014, QUANT IMAG MED SURG, V4, P475, DOI 10.3978/j.issn.2223-4292.2014.11.20
   Kampffmeyer M, 2019, IEEE T IMAGE PROCESS, V28, P2518, DOI 10.1109/TIP.2018.2886997
   Kingma Diederik, 2015, P 3 INT C LEARN REPR
   Li XT, 2020, AAAI CONF ARTIF INTE, V34, P11418
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Oktay O, 2018, Arxiv, DOI arXiv:1804.03999
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Punn NS, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3376922
   Rajput AS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3430806
   Ren XH, 2020, IEEE T IMAGE PROCESS, V29, P7497, DOI 10.1109/TIP.2020.3003735
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen ZQ, 2020, IEEE T PATTERN ANAL, V42, P398, DOI 10.1109/TPAMI.2019.2922181
   Soltaninejad M, 2020, IEEE T IMAGE PROCESS, V29, P6667, DOI 10.1109/TIP.2020.2992893
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tanwar VK, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3380743
   Tschandl P, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.161
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XH, 2020, IEEE T IMAGE PROCESS, V29, P3039, DOI 10.1109/TIP.2019.2955297
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu HS, 2021, IEEE T MED IMAGING, V40, P357, DOI 10.1109/TMI.2020.3027341
   Yang ZZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446618
   Yu LQ, 2017, IEEE T MED IMAGING, V36, P994, DOI 10.1109/TMI.2016.2642839
   Alom MZ, 2018, Arxiv, DOI arXiv:1802.06955
   Zhang F, 2019, IEEE I CONF COMP VIS, P6797, DOI 10.1109/ICCV.2019.00690
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang ZJ, 2019, LECT NOTES COMPUT SC, V11764, P442, DOI 10.1007/978-3-030-32239-7_49
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou SH, 2020, IEEE T IMAGE PROCESS, V29, P461, DOI 10.1109/TIP.2019.2919937
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 48
TC 0
Z9 0
U1 4
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 90
DI 10.1145/3558520
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300015
DA 2024-07-18
ER

PT J
AU Yanagi, R
   Togo, R
   Ogawa, T
   Haseyama, M
AF Yanagi, Rintaro
   Togo, Ren
   Ogawa, Takahiro
   Haseyama, Miki
TI Interactive Re-ranking via Object Entropy-Guided Question Answering for
   Cross-Modal Image Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modal image retrieval; re-ranking; question answering
ID INFORMATION; COMPLETION
AB Cross-modal image-retrieval methods retrieve desired images from a query text by learning relationships between texts and images. Such a retrieval approach is one of the most effective ways of achieving the easiness of query preparation. Recent cross-modal image-retrieval methods are convenient and accurate when users input a query text that can be used to uniquely identify the desired image. However, in reality, users frequently input ambiguous query texts, and these ambiguous queries make it difficult to obtain desired images. To overcome these difficulties, in this study, we propose a novel interactive cross-modal image-retrieval method based on question answering. The proposed method analyzes candidate images and asks users questions to obtain information that can narrow down retrieval candidates. By only answering questions generated by the proposed method, users can reach their desired images, even when using an ambiguous query text. Experimental results show the proposed method's effectiveness.
C1 [Yanagi, Rintaro] Hokkaido Univ, Grad Sch Informat Sci & Technol, Kita Ku, N-14,W-9, Sapporo, Hokkaido 0600814, Japan.
   [Togo, Ren] Hokkaido Univ, Educ & Res Ctr Math & Data Sci, Kita Ku, N-12,W-7, Sapporo, Hokkaido 0600812, Japan.
   [Ogawa, Takahiro; Haseyama, Miki] Hokkaido Univ, Fac Informat Sci & Technol, Div Media & Network Technol, Kita Ku, N-14,W-9, Sapporo, Hokkaido 0600814, Japan.
C3 Hokkaido University; Hokkaido University; Hokkaido University
RP Yanagi, R (corresponding author), Hokkaido Univ, Grad Sch Informat Sci & Technol, Kita Ku, N-14,W-9, Sapporo, Hokkaido 0600814, Japan.
EM yanagi@lmd.ist.hokudai.ac.jp; togo@lmd.ist.hokudai.ac.jp;
   ogawa@lmd.ist.hokudai.ac.jp; miki@ist.hokuhdai.ac.jp
FU JSPS KAKENHI [JP17H01744]
FX This work was partly supported by JSPS KAKENHI Grant Numbers JP17H01744.
CR Aliannejadi M, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P475, DOI 10.1145/3331184.3331265
   Aliannejadi Mohammad, 2020, ConvAI3: Generating clarifying questions for open-domain dialogue systems (ClariQ)
   Datta D, 2017, EXPERT SYST APPL, V68, P81, DOI 10.1016/j.eswa.2016.09.039
   Devlin J., 2018, BERT PRE TRAINING DE
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Fu Bin, 2021, P 30 INT JOINT C ART, P4483
   Giacinto G., 2007, CIVR, P456, DOI [10.1145/1282280.1282347, DOI 10.1145/1282280.1282347]
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Guo KH, 2016, INFORM SCIENCES, V358, P151, DOI 10.1016/j.ins.2016.04.001
   Guo XX, 2018, ADV NEUR IN, V31
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Ji Z, 2019, IEEE I CONF COMP VIS, P5753, DOI 10.1109/ICCV.2019.00585
   Kaiye Wang, 2016, ARXIV160706215
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Liang S, 2008, PATTERN RECOGN LETT, V29, P1733, DOI 10.1016/j.patrec.2008.05.004
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin WC, 2015, NEUROCOMPUTING, V166, P26, DOI 10.1016/j.neucom.2015.04.037
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Lu D, 2016, IEEE T MULTIMEDIA, V18, P1628, DOI 10.1109/TMM.2016.2568099
   Mandal D, 2017, PATTERN RECOGN LETT, V98, P110, DOI 10.1016/j.patrec.2017.09.008
   Mao XL, 2018, NEUROCOMPUTING, V274, P3, DOI 10.1016/j.neucom.2016.06.096
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Piras L, 2017, INFORM FUSION, V37, P50, DOI 10.1016/j.inffus.2017.01.003
   Putzu L, 2020, MULTIMED TOOLS APPL, V79, P26995, DOI 10.1007/s11042-020-09292-9
   Qian XM, 2017, IEEE T IMAGE PROCESS, V26, P3734, DOI 10.1109/TIP.2017.2699623
   Qu C, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P539, DOI 10.1145/3397271.3401110
   Radlinski F, 2017, CHIIR'17: PROCEEDINGS OF THE 2017 CONFERENCE HUMAN INFORMATION INTERACTION AND RETRIEVAL, P117, DOI 10.1145/3020165.3020183
   Rossetto L, 2021, IEEE T MULTIMEDIA, V23, P243, DOI 10.1109/TMM.2020.2980944
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Sun YM, 2018, ACM/SIGIR PROCEEDINGS 2018, P235, DOI 10.1145/3209978.3210002
   Tan Fuwen, 2019, ADV NEURAL INFORM PR, P2651
   Vendrov I., 2016, ICLR, P1
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wang T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P12, DOI 10.1145/3343031.3350875
   Wei W, 2020, IEEE ACCESS, V8, P84642, DOI 10.1109/ACCESS.2020.2992187
   Wu L, 2013, IEEE T PATTERN ANAL, V35, P716, DOI 10.1109/TPAMI.2012.124
   Xu B, 2015, IEEE T KNOWL DATA EN, V27, P102, DOI 10.1109/TKDE.2013.70
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Yanagi Rintaro, 2021, P ACM INT C MULT AS
   Yang Liu, 2017, P 30 INT JOINT C ART
   Yu XJ, 2019, IEEE INT CONF COMP V, P1799, DOI 10.1109/ICCVW.2019.00223
   Zamani H, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P418, DOI 10.1145/3366423.3380126
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang L, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490823
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhang YF, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P177, DOI 10.1145/3269206.3271776
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhou W., 2017, Recent Advance in Content-based Image Retrieval: A Literature Survey
   Zhu Fengbin, 2021, arXiv preprint arXiv:2101.00774
NR 55
TC 8
Z9 8
U1 3
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 68
DI 10.1145/3485042
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600002
DA 2024-07-18
ER

PT J
AU Han, Q
   Liu, HT
   Min, WD
   Huang, TM
   Lin, DY
   Wang, Q
AF Han, Qing
   Liu, Huiting
   Min, Weidong
   Huang, Tiemei
   Lin, Deyu
   Wang, Qi
TI 3D Skeleton and Two Streams Approach to Person Re-identification Using
   Optimized Region Matching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; two streams; optimized region matching; 3D
   skeleton
ID ATTENTION NETWORK
AB Person re-identification (Re-ID) is a challenging and arduous task due to non-overlapping views, complex background, and uncontrollable occlusion in video surveillance. An existing method for capturing pedestrian local region information is to divide person regions into horizontal stripes, which may lead to invalid features and erroneous learning. To solve this problem, this paper proposes a 3D skeleton and a two-stream approach to person Re-ID. The first stream of the method uses the 3D skeleton for background filtering and region segmentation. The second stream uses Siamese net to extract the global descriptor. The features of the two streams are fused to preserve the integrity of the person. An optimized region matching method for metric learning is designed. Extensive comparing experiments were conducted with state-of-the-art Re-ID methods on the Market-1501, CUHK03, and DukeMTMC-reID datasets. Experimental results show that the proposed method outperforms the existing methods in recognition accuracy.
C1 [Han, Qing; Liu, Huiting; Min, Weidong] Nanchang Univ, Sch Math & Comp Sci, 999 Xuefu Ave, Nanchang 330031, Jiangxi, Peoples R China.
   [Min, Weidong] Nanchang Univ, Inst Metaverse, 999 Xuefu Ave, Nanchang 330031, Jiangxi, Peoples R China.
   [Min, Weidong] Jiangxi Key Lab Smart City, 999 Xuefu Ave, Nanchang 330031, Jiangxi, Peoples R China.
   [Huang, Tiemei] Nanchang Univ, Sch Informat Engn, 999 Xuefu Ave, Nanchang 330031, Jiangxi, Peoples R China.
   [Lin, Deyu; Wang, Qi] Nanchang Univ, Sch Software, 235 Nanjingdong Rd, Nanchang 330031, Jiangxi, Peoples R China.
C3 Nanchang University; Nanchang University; Nanchang University; Nanchang
   University
RP Min, WD (corresponding author), Nanchang Univ, Sch Math & Comp Sci, 999 Xuefu Ave, Nanchang 330031, Jiangxi, Peoples R China.; Min, WD (corresponding author), Nanchang Univ, Inst Metaverse, 999 Xuefu Ave, Nanchang 330031, Jiangxi, Peoples R China.; Min, WD (corresponding author), Jiangxi Key Lab Smart City, 999 Xuefu Ave, Nanchang 330031, Jiangxi, Peoples R China.
EM hanqing@ncu.edu.cn; 411014519040@email.ncu.edu.cn;
   minweidong@ncu.edu.cn; 406130717275@email.ncu.edu.cn;
   dashing_lin@126.com; qiwang@email.ncu.edu.cn
RI Han, Qing-Long/B-6635-2013; han, qing/KCZ-0174-2024; Lin,
   Deyu/AAC-7486-2019; Min, Weidong/D-4585-2017
OI Han, Qing-Long/0000-0002-7207-0716; Lin, Deyu/0000-0003-1400-4769; Min,
   Weidong/0000-0003-2526-2181
CR Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Baltieri D, 2015, INT J COMPUT VISION, V111, P345, DOI 10.1007/s11263-014-0747-z
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Cheng D, 2020, IEEE T CYBERNETICS, V50, P561, DOI 10.1109/TCYB.2018.2869739
   Cho YJ, 2016, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2016.151
   Ding GD, 2020, PATTERN RECOGN LETT, V137, P91, DOI 10.1016/j.patrec.2019.02.015
   Ge YX, 2018, Arxiv, DOI arXiv:1810.02936
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu HM, 2019, IEEE T CIRC SYST VID, V29, P2809, DOI 10.1109/TCSVT.2018.2869898
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kang JK, 2019, IEEE ACCESS, V7, P57972, DOI 10.1109/ACCESS.2019.2914670
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Lin YT, 2020, IEEE T IMAGE PROCESS, V29, P5481, DOI 10.1109/TIP.2020.2982826
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo JH, 2019, IEEE IMAGE PROC, P165, DOI [10.1109/ICIP.2019.8802961, 10.1109/icip.2019.8802961]
   Ma F, 2020, IEEE T INF FOREN SEC, V15, P115, DOI 10.1109/TIFS.2019.2917160
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Min WD, 2018, IEEE T INTELL TRANSP, V19, P174, DOI 10.1109/TITS.2017.2756989
   Perwiaz N, 2018, IEEE ACCESS, V6, P77334, DOI 10.1109/ACCESS.2018.2882254
   Quispe R, 2019, IMAGE VISION COMPUT, V92, DOI 10.1016/j.imavis.2019.07.009
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen C, 2019, IEEE T CIRC SYST VID, V29, P3016, DOI 10.1109/TCSVT.2018.2872503
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Simon K., 2005, CIKM 05, P381, DOI 10.1145/1099554.1099672
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang Q, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2811-8
   We L, 2020, IEEE T IMAGE PROCESS, V29, P4942, DOI 10.1109/TIP.2020.2975712
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wu L, 2016, Arxiv, DOI arXiv:1601.07255
   Wu SX, 2016, IEEE WINT CONF APPL
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Yang H, 2021, IEEE T MULTIMEDIA, V23, P572, DOI 10.1109/TMM.2020.2985536
   Yang X, 2020, AAAI CONF ARTIF INTE, V34, P287
   Yang X, 2018, IEEE T IMAGE PROCESS, V27, P791, DOI 10.1109/TIP.2017.2765836
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Yang Y., 2019, INT CONF BIOMETR, P1
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou LH, 2020, IEEE T VEH TECHNOL, V69, P3604, DOI 10.1109/TVT.2020.2969427
   Zhu JQ, 2018, IEEE T CIRC SYST VID, V28, P3183, DOI 10.1109/TCSVT.2017.2734740
NR 62
TC 2
Z9 2
U1 2
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 129
DI 10.1145/3538490
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000017
DA 2024-07-18
ER

PT J
AU Li, YH
   Fan, JH
   Pan, YW
   Yao, T
   Lin, WY
   Mei, T
AF Li, Yehao
   Fan, Jiahao
   Pan, Yingwei
   Yao, Ting
   Lin, Weiyao
   Mei, Tao
TI Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular
   Vision-Language Pre-training
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Vision-language pre-training; encoder-decoder networks
AB Vision-language pre-training has been an emerging and fast-developing research topic, which transfers multi-modal knowledge from rich-resource pre-training task to limited-resource downstream tasks. Unlike existing works that predominantly learn a single generic encoder, we present a pre-trainable Universal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language perception (e.g., visual question answering) and generation (e.g., image captioning). Uni-EDEN is a two-stream Transformer-based structure, consisting of three modules: object and sentence encoders that separately learns the representations of each modality and sentence decoder that enables both multi-modal reasoning and sentence generation via inter-modal interaction. Considering that the linguistic representations of each image can span different granularities in this hierarchy including, from simple to comprehensive, individual label, a phrase, and a natural sentence, we pre-train Uni-EDEN through multi-granular vision-language proxy tasks: Masked Object Classification, Masked Region Phrase Generation, Image-Sentence Matching, and Masked Sentence Generation. In this way, Uni-EDEN is endowed with the power of both multi-modal representation extraction and language modeling. Extensive experiments demonstrate the compelling generalizability of Uni-EDEN by fine-tuning it to four vision-language perception and generation downstream tasks.
C1 [Li, Yehao; Pan, Yingwei; Yao, Ting; Mei, Tao] JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
   [Fan, Jiahao; Lin, Weiyao] Shanghai Jiao Tong Univ, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Yao, T (corresponding author), JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
EM yehaoli.sysu@gmail.com; jiahaofan@sjtu.edu.cn; panyw.ustc@gmail.com;
   tingyao.ustc@gmail.com; wylin@sjtu.edu.cn; tmei@jd.com
RI lin, yuxi/HKF-6212-2023; Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307; Lin, Weiyao/0000-0001-8307-7107
FU National Key R&D Program of China [2020AAA0108600]
FX This work was supported by the National Key R&D Program of China under
   Grant No. 2020AAA0108600.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chen JW, 2019, AAAI CONF ARTIF INTE, P8167
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Chen Y.-C., 2020, ECCV
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J., 2018, BERT PRE TRAINING DE
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang P.-Y., 2021, P 2021 C N AM CHAPT, P2443
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim JH, 2018, ADV NEUR IN, V31
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Lewis Mike, 2020, P ANN M ASS COMP LIN, DOI DOI 10.18653/V1/2020.ACL-MAIN.703
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li Liunian Harold, 2019, ARXIV190803557
   Li YH, 2019, PROC CVPR IEEE, P12489, DOI 10.1109/CVPR.2019.01278
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Li YF, 2021, AAAI CONF ARTIF INTE, V35, P8547
   Lu JS, 2019, ADV NEUR IN, V32
   Luo Jianjie, 2021, ACM MM
   Mei T., 2020, ARXIV200702375
   Nie LQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1098, DOI 10.1145/3343031.3350923
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Paszke A, 2019, ADV NEUR IN, V32
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Qu Leigang, 2020, ACM MM
   Radford Alec, 2018, IMPROVING LANGUAGE U, DOI DOI 10.18653/V1/N18-1202
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Song KT, 2019, PR MACH LEARN RES, V97
   Su WJ, 2019, ANN NUTR METAB, V75, P31, DOI 10.1159/000501710
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wang TC, 2020, PROC CVPR IEEE, P4115, DOI 10.1109/CVPR42600.2020.00417
   Yang Xiaoshan, 2019, ACM T MULTIM COMPUT, V15, P1
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
NR 50
TC 5
Z9 5
U1 2
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 48
DI 10.1145/3473140
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhai, GT
   Sun, W
   Min, XK
   Zhou, JT
AF Zhai, Guangtao
   Sun, Wei
   Min, Xiongkuo
   Zhou, Jiantao
TI Perceptual Quality Assessment of Low-light Image Enhancement
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Low-light image enhancement; quality assessment; light-enhanced image;
   structure similarity
ID FREE-ENERGY PRINCIPLE; EXPOSURE FUSION; SIMILARITY; REPRESENTATION;
   ILLUMINATION; INDEX
AB Low-light image enhancement algorithms (LIEA) can light up images captured in dark or back-lighting conditions. However, LIEA may introduce various distortions such as structure damage, color shift, and noise into the enhanced images. Despite various LIEAs proposed in the literature, few efforts have been made to study the quality evaluation of low-light enhancement. In this article, we make one of the first attempts to investigate the quality assessment problem of low-light image enhancement. To facilitate the study of objective image quality assessment (IQA), we first build a large-scale low-light image enhancement quality (LIEQ) database. The LIEQ database includes 1,000 light-enhanced images, which are generated from 100 low-light images using 10 LIEAs. Rather than evaluating the quality of light-enhanced images directly, which is more difficult, we propose to use the multi-exposure fused (MEF) image and stack-based high dynamic range (HDR) image as a reference and evaluate the quality of low-light enhancement following a full-reference (FR) quality assessment routine. We observe that distortions introduced in low-light enhancement are significantly different from distortions considered in traditional image IQA databases that are well-studied, and the current state-of-the-art FR IQA models are also not suitable for evaluating their quality. Therefore, we propose a new FR low-light image enhancement quality assessment (LIEQA) index by evaluating the image quality from four aspects: luminance enhancement, color rendition, noise evaluation, and structure preserving, which have captured the most key aspects of low-light enhancement. Experimental results on the LIEQ database show that the proposed LIEQA index outperforms the state-of-the-art FR IQA models. LIEQA can act as an evaluator for various low-light enhancement algorithms and systems. To the best of our knowledge, this article is the first of its kind comprehensive low-light image enhancement quality assessment study.
C1 [Zhai, Guangtao; Sun, Wei; Min, Xiongkuo] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
   [Zhou, Jiantao] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
C3 Shanghai Jiao Tong University; University of Macau
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
EM zhaiguangtao@sjtu.edu.cn; sunguwei@sjtu.edu.cn; minxiongkuo@sjtu.edu.cn;
   jtzhou@umac.mo
RI Sun, Wei/HMV-5798-2023; Zhai, Guangtao/X-5949-2019; Min,
   Xiongkuo/A-7097-2019
OI Sun, Wei/0000-0001-8162-1949; Zhai, Guangtao/0000-0001-8165-9322; Min,
   Xiongkuo/0000-0001-5693-0416
FU National Natural Science Foundation of China [61831015, 61771305,
   61901260, 61971476, U1908210]; Open Research Project of the State Key
   Laboratory of Media Convergence and Communication, Communication
   University of China, China [GZ2004]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61831015, Grant 61771305, Grant
   61901260, Grant 61971476, and Grant U1908210, and in part by the Open
   Research Project of the State Key Laboratory of Media Convergence and
   Communication, Communication University of China, China (No. GZ2004).
CR [Anonymous], 1995, MACHINE VISION
   Barten PGJ, 2004, P SOC PHOTO-OPT INS, V5294, P231, DOI 10.1117/12.537476
   Bruce NDB, 2014, COMPUT GRAPH-UK, V39, P12, DOI 10.1016/j.cag.2013.10.001
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chien Y, 1974, IEEE T AUTOMAT CONTR, V19, P462, DOI [10.1109/TAC.1974.1100577, DOI 10.1109/TAC.1974.1100577]
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong XC, 2011, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2011.115
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Feifan Lv., 2018, P BRIT MACH VIS C
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Gao ZP, 2020, DISPLAYS, V65, DOI 10.1016/j.displa.2020.101972
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hu J, 2013, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2013.154
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Jiang QP, 2019, IEEE T CIRC SYST VID, V29, P323, DOI 10.1109/TCSVT.2017.2783938
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kou F, 2017, IEEE INT CON MULTI, P1105, DOI 10.1109/ICME.2017.8019529
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lyu SW, 2008, PROC CVPR IEEE, P3721
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   Mahmoudpour Saeed, 2019, IEEE T MULTIMEDIA, V2019, P1
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Muthukrishnan R., 2011, International Journal of Computer Science & Information Technology, V3, P259, DOI 10.5121/ijcsit.2011.3620
   Nafchi HZ, 2015, IEEE SIGNAL PROC LET, V22, P1026, DOI 10.1109/LSP.2014.2381458
   Oh TH, 2015, IEEE T PATTERN ANAL, V37, P1219, DOI 10.1109/TPAMI.2014.2361338
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Photomatrix, 2020, COMM AV HDR PROC SOF
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Raman S., 2009, P 26 INT C MACHINE L, P1
   RECOMMENDATION ITU-R BT, 2002, Methodology for the subjective assessment of the quality of television pictures
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Roberts L., 1963, MACHINE PERCEPTION 3
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shen JB, 2014, IEEE T CYBERNETICS, V44, P1579, DOI 10.1109/TCYB.2013.2290435
   Shen L., 2017, ARXIV PREPRINT ARXIV, P171102488
   Shen R, 2011, IEEE T IMAGE PROCESS, V20, P3634, DOI 10.1109/TIP.2011.2150235
   Song Q, 2018, IEEE T IMAGE PROCESS, V27, P4901, DOI 10.1109/TIP.2018.2846686
   Sun W., 2020, IEEE Trans. Mob. Comput.
   Sun W., 2018, 2018 IEEE International Conference on Communications (ICC), P1, DOI 10.1109/MMSP.2018.8547102
   Sun W, 2020, IEEE J-STSP, V14, P64, DOI 10.1109/JSTSP.2019.2955024
   Sun W, 2017, IEEE IMAGE PROC, P3450, DOI 10.1109/ICIP.2017.8296923
   Sun W, 2017, IEEE INT CON MULTI, P25, DOI 10.1109/ICME.2017.8019511
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Xiang T, 2020, IEEE T MULTIMEDIA, V22, P1259, DOI 10.1109/TMM.2019.2938612
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yan ZS, 2018, IEEE T MOBILE COMPUT, V17, P2536, DOI 10.1109/TMC.2018.2812852
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Ying ZQ, 2017, LECT NOTES COMPUT SC, V10425, P36, DOI 10.1007/978-3-319-64698-5_4
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhai GT, 2019, DIGIT SIGNAL PROCESS, V91, P11, DOI 10.1016/j.dsp.2019.02.017
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang W, 2012, IEEE T IMAGE PROCESS, V21, P2318, DOI 10.1109/TIP.2011.2170079
   Zheng YQ, 2020, PROC CVPR IEEE, P6748, DOI 10.1109/CVPR42600.2020.00678
NR 90
TC 39
Z9 39
U1 7
U2 78
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 130
DI 10.1145/3457905
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800014
DA 2024-07-18
ER

PT J
AU Shorfuzzaman, M
   Hossain, MS
   El Saddik, A
AF Shorfuzzaman, Mohammad
   Hossain, M. Shamim
   El Saddik, Abdulmotaleb
TI An Explainable Deep Learning Ensemble Model for Robust Diagnosis of
   Diabetic Retinopathy Grading
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Explainable deep CNN; diabetic retinopathy diagnosis; ensemble model;
   transfer learning; retinal fundus images
ID CONVOLUTIONAL NEURAL-NETWORKS
AB Diabetic retinopathy (DR) is one of the most common causes of vision loss in people who have diabetes for a prolonged period. Convolutional neural networks (CNNs) have become increasingly popular for computer-aided DR diagnosis using retinal fundus images. While these CNNs are highly reliable, their lack of sufficient explainability prevents them from being widely used in medical practice. In this article, we propose a novel explainable deep learning ensemble model where weights from different models are fused into a single model to extract salient features from various retinal lesions found on fundus images. The extracted features are then fed to a custom classifier for the final diagnosis of DR severity level. The model is trained on an APTOS dataset containing retinal fundus images of various DR grades using a cyclical learning rates strategy with an automatic learning rate finder for decaying the learning rate to improve model accuracy. We develop an explainability approach by leveraging gradient-weighted class activation mapping and shapely adaptive explanations to highlight the areas of fundus images that are most indicative of different DR stages. This allows ophthalmologists to view our model's decision in a way that they can understand. Evaluation results using three different datasets (APTOS, MESSIDOR, IDRiD) show the effectiveness of our model, achieving superior classification rates with a high degree of precision (0.970), sensitivity (0.980), and AUC (0.978). We believe that the proposed model, which jointly offers state-of-the-art diagnosis performance and explainability, will address the black-box nature of deep CNN models in robust detection of DR grading.
C1 [Shorfuzzaman, Mohammad] Taif Univ, Coll Comp & Informat Technol, Dept Comp Sci, At Taif 21944, Saudi Arabia.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, POB 51178, Riyadh 11543, Saudi Arabia.
   [El Saddik, Abdulmotaleb] Univ Ottawa, Sch Elect Engn & Comp Sci EECS, 800 King Edward, Ottawa, ON K1N 6N5, Canada.
C3 Taif University; King Saud University; University of Ottawa
RP Shorfuzzaman, M (corresponding author), Taif Univ, Coll Comp & Informat Technol, Dept Comp Sci, At Taif 21944, Saudi Arabia.
EM m.shorf@tu.edu.sa; mshossain@ksu.edu.sa; elsaddik@uottawa.ca
RI Hossain, M. Shamim/K-1362-2014; Shorfuzzaman, Mohammad/AAZ-1145-2020
OI Hossain, M. Shamim/0000-0001-5906-9422; Shorfuzzaman,
   Mohammad/0000-0002-8050-8431
FU Taif University, Taif, Saudi Arabia [TURSP-2020/79]
FX Authors are grateful to the Taif University Researchers Supporting
   Project Number (TURSP-2020/79), Taif University, Taif, Saudi Arabia for
   funding this work.
CR Alyoubi W.L., 2020, INFORM MED UNLOCKED, DOI 10.1016/j.imu.2020.100377
   APTOS, 2019, APTOS 2019 BLINDN DE
   Chetoui M, 2020, IEEE ENG MED BIO, P1966, DOI 10.1109/EMBC44109.2020.9175664
   Cheung N, 2010, LANCET, V376, P124, DOI 10.1016/S0140-6736(09)62124-3
   Colomer A, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041005
   Decencière E, 2014, IMAGE ANAL STEREOL, V33, P231, DOI 10.5566/ias.1155
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gargeya R, 2017, OPHTHALMOLOGY, V124, P962, DOI 10.1016/j.ophtha.2017.02.008
   Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216
   Hacisoftaoglu RE, 2020, PATTERN RECOGN LETT, V135, P409, DOI 10.1016/j.patrec.2020.04.009
   Hagos M.T., 2019, ARXIV190507203
   Hossain MS, 2020, IEEE NETWORK, V34, P126, DOI 10.1109/MNET.011.2000458
   Hossain MS, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3241056
   Hossain MS, 2019, IEEE T IND INFORM, V15, P1027, DOI 10.1109/TII.2018.2875149
   Hossain MS, 2017, IEEE SYST J, V11, P118, DOI 10.1109/JSYST.2015.2470644
   Kenstler B., CYCLICAL LEARNING RA
   Kind A, 2019, LECT NOTES COMPUT SC, V11678, P457, DOI 10.1007/978-3-030-29888-3_37
   Krause J., 2017, ABS171001711 CORR
   Lam Carson, 2018, AMIA Jt Summits Transl Sci Proc, V2017, P147
   Li JQ, 2020, EUR J EPIDEMIOL, V35, P11, DOI 10.1007/s10654-019-00560-z
   Li YH, 2019, MOB INF SYST, V2019, DOI 10.1155/2019/6142839
   Lundberg SM, 2017, ADV NEUR IN, V30
   Mahiba C, 2019, MEASUREMENT, V135, P762, DOI 10.1016/j.measurement.2018.12.032
   Mateen M, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/5801870
   Muhammad G, 2021, IEEE J SEL AREA COMM, V39, P603, DOI 10.1109/JSAC.2020.3020654
   Conde PP, 2012, INT CONF INTELL SYST, P826, DOI 10.1109/ISDA.2012.6416644
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Porwal P, 2020, MED IMAGE ANAL, V59, DOI 10.1016/j.media.2019.101561
   Pratt H, 2016, PROCEDIA COMPUT SCI, V90, P200, DOI 10.1016/j.procs.2016.07.014
   Priya R. P. R., 2012, Int. J. Comput. Appl., V41, P6, DOI 10.5120/5503-7503
   Rahim SS, 2014, COMM COM INF SC, V459, P113
   Rahman MA, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3421725
   Rajalakshmi R, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0138285
   Sarki R, 2019, 763136 BIORXIV
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Singh A, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3415155
   Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58
   Stolte S, 2020, MED IMAGE ANAL, V64, DOI 10.1016/j.media.2020.101742
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tseng VS, 2020, TRANSL VIS SCI TECHN, V9, DOI 10.1167/tvst.9.2.41
   Tymchenko B, 2020, ICPRAM: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P501, DOI 10.5220/0008970805010509
   Wan SH, 2018, COMPUT ELECTR ENG, V72, P274, DOI 10.1016/j.compeleceng.2018.07.042
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 44
TC 20
Z9 20
U1 3
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 113
DI 10.1145/3469841
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600015
DA 2024-07-18
ER

PT J
AU Du, GM
   Wu, JT
   Cao, HF
   Xing, K
   Li, ZM
   Zhang, DL
   Wang, XL
AF Du, Gaoming
   Wu, Jiting
   Cao, Hongfang
   Xing, Kun
   Li, Zhenmin
   Zhang, Duoli
   Wang, Xiaolei
TI A Real-Time Effective Fusion-Based Image Defogging Architecture on FPGA
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Single gray channel; fusion-based defogging; FPGA
ID RETINEX
AB Foggy weather reduces the visibility of photographed objects, causing image distortion and decreasing overall image quality. Many approaches (e.g., image restoration, image enhancement, and fusion-based methods) have been proposed to work out the problem. However, most of these defogging algorithms are facing challenges such as algorithm complexity or real-time processing requirements. To simplify the defogging process, we propose a fusional defogging algorithm on the linear transmission of gray single-channel. This method combines gray single-channel linear transform with high-boost filtering according to different proportions. To enhance the visibility of the defogging image more effectively, we convert the RGB channel into a grayscale single channel without decreasing the defogging results. After gray-scale fusion, the data in the grayscale domain should be linearly transmitted. With the increasing real-time requirements for clear images, we also propose an efficient real-time FPGA defogging architecture. The architecture optimizes the data path of the guided filtering to speed up the defogging speed and save area and resources. Because the pixel reading order of mean and square value calculations are identical, the shift register in the box filter after the average and the computation of the square values is separated from the box filter and put on the input terminal for sharing, saving the storage area. What's more, using LUTs instead of the multiplier can decrease the time delays of the square value calculation module and increase efficiency. Experimental results show that the linear transmission can save 66.7% of the total time. The architecture we proposed can defog efficiently and accurately, meeting the real-time defogging requirements on 1920 x 1080 image size.
C1 [Du, Gaoming; Wu, Jiting; Cao, Hongfang; Xing, Kun; Li, Zhenmin; Zhang, Duoli; Wang, Xiaolei] Hefei Univ Technol, 193 Tunxi Rd, Hefei, Peoples R China.
C3 Hefei University of Technology
RP Du, GM (corresponding author), Hefei Univ Technol, 193 Tunxi Rd, Hefei, Peoples R China.
EM dugaoming@hfut.edu.cn; wujingting@hfut.edu.cn; caohongfang@hfut.edu.cn;
   k.xing@hfut.edu.cn; zhenmin.li@hfut.edu.cn; zhangduoli@hfut.edu.cn;
   wangxiaolei@hfut.edu.cn
RI xing, k/JEP-6928-2023
FU National Key Research and Development Program [2018YFB2202604];
   University Synergy Innovation Program of Anhui Province [GXXT-2019-030]
FX This work was supported in part by the National Key Research and
   Development Program under grant 2018YFB2202604, and the University
   Synergy Innovation Program of Anhui Province under grant GXXT-2019-030.
CR Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Dudhane A, 2020, IEEE T IMAGE PROCESS, V29, P628, DOI 10.1109/TIP.2019.2934360
   El Mezeni D, 2020, J REAL-TIME IMAGE PR, V17, P511, DOI 10.1007/s11554-018-0802-z
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fu XY, 2014, IEEE INT WORKSH MULT
   Gao Y, 2014, PROC SPIE, V9159, DOI 10.1117/12.2064391
   Guo L, 2018, PROC SPIE, V10615, DOI 10.1117/12.2302633
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang SC, 2014, IEEE T CIRC SYST VID, V24, P1814, DOI 10.1109/TCSVT.2014.2317854
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kim JY, 2001, IEEE T CIRC SYST VID, V11, P475, DOI 10.1109/76.915354
   Kumar R, 2017, PROC SPIE, V10396, DOI 10.1117/12.2274682
   Kumar WK, 2019, IET IMAGE PROCESS, V13, P2467, DOI 10.1049/iet-ipr.2018.5812
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lee YH, 2019, IEEE T CIRC SYST VID, V29, P2146, DOI 10.1109/TCSVT.2018.2862906
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Liang ZF, 2014, IEICE ELECTRON EXPR, V11, DOI 10.1587/elex.11.20141002
   Liu H, 2017, NEUROCOMPUTING, V269, P97, DOI 10.1016/j.neucom.2016.09.139
   Ma ZL, 2016, NEUROCOMPUTING, V173, P1257, DOI 10.1016/j.neucom.2015.08.084
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Park Y, 2017, IEEE GLOB CONF SIG, P779, DOI 10.1109/GlobalSIP.2017.8309066
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Schaul L, 2009, IEEE IMAGE PROC, P1629, DOI 10.1109/ICIP.2009.5413700
   Shen Y, 2019, SPECTROSC SPECT ANAL, V39, P1420, DOI 10.3964/j.issn.1000-0593(2019)05-1420-08
   Shiau YH, 2013, IEEE T CIRC SYST VID, V23, P1369, DOI 10.1109/TCSVT.2013.2243650
   Singh D, 2018, MULTIMED TOOLS APPL, V77, P9595, DOI 10.1007/s11042-017-5321-6
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Varalakshmi J, 2020, ADV INTELL SYST COMP, V1108, P624, DOI 10.1007/978-3-030-37218-7_71
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xu L, 2019, INFORM SCIENCES, V489, P50, DOI 10.1016/j.ins.2019.02.058
   Xu Z., 2009, 2009 INT C COMP INT, P1
   Yin Gao, 2018, 2018 IEEE 4th International Conference on Computer and Communications (ICCC). Proceedings, P1665, DOI 10.1109/CompComm.2018.8780924
   Zhao D, 2019, SIGNAL PROCESS-IMAGE, V74, P253, DOI 10.1016/j.image.2019.02.004
   Zhongli Ma, 2013, Advances in Swarm Intelligence. 4th International Conference, ICSI 2013. Proceedings, P436, DOI 10.1007/978-3-642-38715-9_52
   Zuiderveld K., 1994, Graphics Gems, P474, DOI 10.1016/B978-0-12-336156-1.50061-6
NR 40
TC 2
Z9 3
U1 4
U2 45
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 93
DI 10.1145/3446241
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400017
DA 2024-07-18
ER

PT J
AU Jin, X
   Xu, JF
   Tasaka, K
   Chen, ZB
AF Jin, Xin
   Xu, Jianfeng
   Tasaka, Kazuyuki
   Chen, Zhibo
TI Multi-task Learning-based All-in-one Collaboration Framework for
   Degraded Image Super-resolution
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Degraded image super-resolution; multi-task learning; all-in-one
   collaboration; optimal combination
AB In this article, we address the degraded image super-resolution problem in a multi-task learning (MU) manner. To better share representations between multiple tasks, we propose an all-in-one collaboration framework (ACF) with a learnable "junction" unit to handle two major problems that exist in MU-"How to share" and "How much to share." Specifically, ACF consists of a sharing phase and a reconstruction phase. Considering the intrinsic characteristic of multiple image degradations, we propose to first deal with the compression artifact, motion blur, and spatial structure information of the input image in parallel under a three-branch architecture in the sharing phase. Subsequently, in the reconstruction phase, we up-sample the previous features for high-resolution image reconstruction with a channel-wise and spatial attention mechanism. To coordinate two phases, we introduce a learnable "junction" unit with a dual-voting mechanism to selectively filter or preserve shared feature representations that come from sharing phase, learning an optimal combination for the following reconstruction phase. Finally, a curriculum learning-based training scheme is further proposed to improve the convergence of the whole framework. Extensive experimental results on synthetic and real-world low-resolution images show that the proposed all-in-one collaboration framework not only produces favorable high-resolution results while removing serious degradation, but also has high computational efficiency, outperforming state-of-the-art methods. We also have applied ACF to some image-quality sensitive practical task, such as pose estimation, to improve estimation accuracy of low-resolution images.
C1 [Jin, Xin; Chen, Zhibo] Univ Sci & Technol China, Hefei, Peoples R China.
   [Xu, Jianfeng; Tasaka, Kazuyuki] KDDI Res Inc, Media Recognit Lab, Fujimino, Japan.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; KDDI Corporation; KDDI Research, Inc.
RP Chen, ZB (corresponding author), Univ Sci & Technol China, Hefei, Peoples R China.
EM jinxustc@mail.ustc.edu.cn; ji-xu@kddi-research.jp;
   ka-tasaka@kddi-research.jp; chenzhibo@ustc.edu.cn
RI jin, xin/GQZ-5811-2022
FU NSFC [61571413, 61632001, 61390514]
FX This work was supported in part by NSFC under Grant nos. 61571413,
   61632001, 61390514.
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   [Anonymous], 2016, Journal of WSCG
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Han W, 2018, PROC CVPR IEEE, P1654, DOI 10.1109/CVPR.2018.00178
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Zewei, 2018, IEEE T CIRC SYST TEC, V29, P8
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu Z, 2014, PROC CVPR IEEE, P2893, DOI 10.1109/CVPR.2014.370
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang JZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4107
   Huang JJ, 2017, IEEE T CIRC SYST VID, V27, P937, DOI 10.1109/TCSVT.2015.2513661
   Jancsary J, 2012, LECT NOTES COMPUT SC, V7578, P112, DOI 10.1007/978-3-642-33786-4_9
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Khan Faisal, 2011, ADV NEURAL INFORM PR, V24
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mao XJ, 2016, ADV NEUR IN, V29
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Noroozi M, 2017, LECT NOTES COMPUT SC, V10496, P65, DOI 10.1007/978-3-319-66709-6_6
   Obozinski G, 2010, STAT COMPUT, V20, P231, DOI 10.1007/s11222-008-9111-x
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Park D, 2018, IEEE COMPUT SOC CONF, P995, DOI 10.1109/CVPRW.2018.00133
   Polatkan G, 2015, IEEE T PATTERN ANAL, V37, P346, DOI 10.1109/TPAMI.2014.2321404
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi Y, 2017, IEEE T MULTIMEDIA, V19, P2804, DOI 10.1109/TMM.2017.2711263
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Teterwak Piotr, 2014, SHARED ROOTS REGULAR, P92
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu O, 2016, IEEE T MULTIMEDIA, V18, P1062, DOI 10.1109/TMM.2016.2538722
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yim J, 2015, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2015.7298667
   Yu K., 2016, arXiv
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2019, PROC CVPR IEEE, P1671, DOI 10.1109/CVPR.2019.00177
   Zhang Xinyi., 2018, P BMVC
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao W, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1205
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 65
TC 5
Z9 5
U1 2
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 21
DI 10.1145/3417333
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200001
DA 2024-07-18
ER

PT J
AU Li, YY
   Yao, HT
   Zhang, TZ
   Xu, CS
AF Li, Yaoyu
   Yao, Hantao
   Zhang, Tianzhu
   Xu, Changsheng
TI Part-based Structured Representation Learning for Person
   Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; representation learning; graph convolutional
   network
AB Person re-identification aims to match person of interest under non-overlapping camera views. Therefore, how to generate a robust and discriminative representation is crucial for person re-identification. Mining local clues from human body parts to describe pedestrians has been extensively studied in existing methods. However, existing methods locate human body parts coarsely and do not consider the relations among different local parts. To address the above problem, we propose a Part-based Structured Representation Learning (PSRL) for better exploiting local clues to improve the person representation. There are two important modules in our architecture: Local Semantic Feature Extraction and Structured Person Representation Learning. The Local Semantic Feature Extraction module is designed to extract local features from human body semantic regions. After obtaining the local features, the Structured Person Representation Learning is proposed to fuse the local features by considering the person structure. To model the underlying person structure, a graph convolutional network is employed to capture the relations of different semantic regions. The generated structured feature encodes underlying person structure information, and local semantic feature can solve the misalignment problem caused by pose variations in feature matching. By combining them together, we can improve the descriptive ability of the generated representation. Extensive evaluations on four standard benchmarks show that our proposed method achieves competitive performance against state-of-the-art methods.
C1 [Li, Yaoyu; Yao, Hantao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, NLPR, Beijing, Peoples R China.
   [Li, Yaoyu; Yao, Hantao; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Zhang, Tianzhu] Univ Sci & Technol China, 1202 Room Sci & Technol West Bldg,Huangshan Rd, Hefei, Anhui, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Peng Cheng Laboratory
RP Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, 95 Zhongguancun East Rd, Beijing, Peoples R China.; Xu, CS (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
EM yaoyu.li@nlpr.ia.ac.cn; hantao.yao@nlpr.ia.ac.cn; tzzhang10@gmail.com;
   csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; Zhang, Tianzhu/AGY-9389-2022; xu,
   cj/HJZ-3488-2023
OI Zhang, Tianzhu/0000-0003-0764-6106; 
FU National Key Research and Development Program of China [2018AAA0102200];
   National Natural Science Foundation of China [61902399, 61721004,
   U1705262, 61532009, 61832002, 61720106006]; Key Research Program of
   Frontier Sciences, CAS [QYZDJ-SSW-JSC039]; National Postdoctoral
   Programme for Innovative Talents [BX20180358]
FX This work was supported by National Key Research and Development Program
   of China (No. 2018AAA0102200), National Natural Science Foundation of
   China (61902399, 61721004, U1705262, 61532009, 61832002, 61720106006),
   and also was supported by the Key Research Program of Frontier Sciences,
   CAS (QYZDJ-SSW-JSC039) and National Postdoctoral Programme for
   Innovative Talents (BX20180358).
CR [Anonymous], 2019, ARXIV PREPRINT ARXIV
   Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083
   Bruna J., 2014, P INT C LEARN REPR
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen DP, 2018, PROC CVPR IEEE, P8649, DOI 10.1109/CVPR.2018.00902
   Chen D, 2018, INT CONF CLOUD COMPU, P507, DOI 10.1109/CCIS.2018.8691205
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Duvenaud David K., 2015, ADV NEURAL INFORM PR, P2224, DOI DOI 10.48550/ARXIV.1509.09292
   Gao JY, 2018, IEEE T IMAGE PROCESS, V27, P3074, DOI 10.1109/TIP.2018.2813166
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Gong S., 2018, CVPR, P2285, DOI DOI 10.1109/CVPR.2018.00243
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henaff M., 2015, ARXIV150605163
   Hermans Alexander, 2017, ARXIV170307737
   Jiang Bo, 2019, CORR
   Jin H, 2018, CURR MOL MED, V18, P365, DOI 10.2174/1566524018666181109120023
   Jing Xu, 2018, ATTENTION AWARE COMP
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Kingma D. P., 2014, arXiv
   Kipf TN, 2016, ARXIV
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li S., 2019, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, P3595
   Li W, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2194
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li Wei, 2012, AS C COMP VIS ACCV 2, P31
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu Hao, 2016, ARXIV160604404
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   Liu XC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P338, DOI 10.1145/3343031.3350857
   Niepert M, 2016, PR MACH LEARN RES, V48
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Schumann A, 2017, IEEE COMPUT SOC CONF, P1435, DOI 10.1109/CVPRW.2017.186
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun Y., 2017, CORR, DOI DOI 10.1002/EP.10350
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Tang YB, 2018, IEEE T MULTIMEDIA, V20, P2276, DOI 10.1109/TMM.2018.2802644
   Tao Y, 2017, CHIN CONTR CONF, P4288, DOI 10.23919/ChiCC.2017.8028032
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang XB, 2019, CF '19 - PROCEEDINGS OF THE 16TH ACM INTERNATIONAL CONFERENCE ON COMPUTING FRONTIERS, P190, DOI 10.1145/3310273.3323070
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wu L., 2016, ARXIV160107255
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Yan S., 2018, AAAI, P1
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Yao H., 2017, ARXIV170700798
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yi D., 2014, DEEP METRIC LEARNING
   Yin BS, 2019, 2019 2ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND BIG DATA (ICAIBD 2019), P19, DOI [10.1109/ICAIBD.2019.8837016, 10.1109/icaibd.2019.8837016]
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang M, 2019, IEEE INT CON MULTI, P1618, DOI 10.1109/ICME.2019.00279
   Zhang Xiangyu, 2017, CoRRabs/1711.08184
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zheng HT, 2016, IEEE T MULTIMEDIA, V18, P2407, DOI 10.1109/TMM.2016.2598140
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhu FQ, 2017, IEEE T IMAGE PROCESS, V26, P4806, DOI 10.1109/TIP.2017.2695101
NR 82
TC 11
Z9 11
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 134
DI 10.1145/3412384
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800018
DA 2024-07-18
ER

PT J
AU Luo, GL
   Deng, ZG
   Zhao, X
   Jin, XG
   Zeng, W
   Xie, WQ
   Seo, H
AF Luo, Guoliang
   Deng, Zhigang
   Zhao, Xin
   Jin, Xiaogang
   Zeng, Wei
   Xie, Wenqiang
   Seo, Hyewon
TI Spatio-temporal Segmentation Based Adaptive Compression of Dynamic Mesh
   Sequences
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Dynamic mesh sequences; animation compression; adaptive spatio-temporal
   segmentation; data compression
ID MOTION CAPTURE DATA; ALGORITHM; MATRIX
AB With the recent advances in data acquisition techniques, the compression of various dynamic mesh sequence data has become an important topic in the computer graphics community. In this article, we present a new spatio-temporal segmentation-based approach for the adaptive compression of the dynamic mesh sequences. Given an input dynamic mesh sequence, we first compute an initial temporal cut to obtain a small subsequence by detecting the temporal boundary of dynamic behavior. Then, we apply a two-stage vertex clustering on the resulting subsequence to classify the vertices into groups with optimal intra-affinities. After that, we design a temporal segmentation step based on the variations of the principal components within each vertex group prior to performing a PCA-based compression. Furthermore, we apply an extra step on the lossless compression of the PCA bases and coefficients to gain more storage saving. Our approach can adaptively determine the temporal and spatial segmentation boundaries to exploit both temporal and spatial redundancies. We have conducted extensive experiments on different types of 3D mesh animations with various segmentation configurations. Our comparative studies show the advantages of our approach for the compression of 3D mesh animations.
C1 [Luo, Guoliang; Zhao, Xin] East China Jiaotong Univ, 808 Shuanggang East AV, Nanchang, Jiangxi, Peoples R China.
   [Deng, Zhigang] Univ Houston, 3551 Cullen Blvd, Houston, TX 77004 USA.
   [Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.
   [Zeng, Wei; Xie, Wenqiang] Jiangxi Normal Univ, 99 Ziyang AV, Nanchang, Jiangxi, Peoples R China.
   [Seo, Hyewon] Univ Strasbourg, 5 Rue Kirschleger, Strasbourg, France.
C3 East China Jiaotong University; University of Houston System; University
   of Houston; Zhejiang University; Jiangxi Normal University; Universites
   de Strasbourg Etablissements Associes; Universite de Strasbourg
RP Luo, GL (corresponding author), East China Jiaotong Univ, 808 Shuanggang East AV, Nanchang, Jiangxi, Peoples R China.; Deng, ZG (corresponding author), Univ Houston, 3551 Cullen Blvd, Houston, TX 77004 USA.
EM luoguoliang@ecjtu.edu.cn; zdeng4@uh.edu
RI Liu, Gui/JHU-8707-2023; Yang, Ying/ABD-2481-2022
OI Deng, Zhigang/0000-0002-0452-8676; Deng, Zhigang/0000-0003-2571-5865
FU National Natural Science Foundation of China [61962021, 61602222,
   61972344, 61732015]; China Postdoctoral Science Foundation
   [2019M662261]; Key Research and the Development Program of Jiangxi
   Province [20192BBE50079]; Key Research and the Development Program of
   Zhejiang Province [2018C01090]; US NSF [IIS-1524782]
FX This work has been supported in part by the National Natural Science
   Foundation of China (No. 61962021, 61602222, 61972344, 61732015), the
   China Postdoctoral Science Foundation (2019M662261), the Key Research
   and the Development Program of Jiangxi Province (No. 20192BBE50079), the
   Key Research and the Development Program of Zhejiang Province (No.
   2018C01090). Zhigang Deng is supported in part by US NSF IIS-1524782.
CR Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   [Anonymous], 2018, P COMP GRAPH INT 201
   [Anonymous], 2010, THESIS
   Au OKC, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360643
   Barbic J, 2004, PROC GRAPH INTERF, P185
   Beaudoin Philippe, 2007, Proceedings Graphics Interface 2007, P313, DOI 10.1145/1268517.1268568
   Chattopadhyay S, 2007, IEEE T VIS COMPUT GR, V13, P5, DOI 10.1109/TVCG.2007.13
   Chen J.L., 2017, HUSB VET MED, V6, P33
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Cordier F, 2005, COMPUT GRAPH FORUM, V24, P173, DOI 10.1111/j.1467-8659.2005.00841.x
   de Aguiar E, 2008, COMPUT GRAPH FORUM, V27, P389, DOI 10.1111/j.1467-8659.2008.01136.x
   Deutsch P, 1996, REQ COMMENTS, V1950, P1
   Firouzmanesh A, 2011, IEEE T MULTIMEDIA, V13, P829, DOI 10.1109/TMM.2011.2129497
   George D, 2018, GRAPH MODELS, V96, P1, DOI 10.1016/j.gmod.2018.01.001
   George David, 2018, ARXIV180706551
   Gong D, 2012, LECT NOTES COMPUT SC, V7574, P229, DOI 10.1007/978-3-642-33712-3_17
   Gu Q, 2009, COMPUT GRAPH FORUM, V28, P1, DOI 10.1111/j.1467-8659.2008.01309.x
   Hajizadeh M, 2016, COMPUT ANIMAT VIRT W, V27, P556, DOI 10.1002/cav.1685
   Hijiri T., 2000, Proceedings Web3D - VRML 2000. Fifth Symposium on the Virtual Reality Modeling Language, P95, DOI 10.1145/330160.330193
   Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677
   Hou JH, 2017, IEEE T CIRC SYST VID, V27, P1043, DOI 10.1109/TCSVT.2015.2513698
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   James DL, 2005, ACM T GRAPHIC, V24, P399, DOI 10.1145/1073204.1073206
   Kalogerakis E, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778839
   Karni Z, 2004, COMPUT GRAPH-UK, V28, P25, DOI 10.1016/j.cag.2003.10.002
   Kavan L, 2010, COMPUT GRAPH FORUM, V29, P327, DOI 10.1111/j.1467-8659.2009.01602.x
   Khan MA, 2016, MULTIDIM SYST SIGN P, V27, P121, DOI 10.1007/s11045-014-0293-4
   Khodakovsky A, 2004, MATH VISUAL, P189
   Kwak D, 2011, SCI COMPUT, P1, DOI 10.1007/978-94-007-0193-9
   Lalos Aris S., 2017, VISUAL COMPUT, V33, P1
   Le BH, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601161
   Lee PF, 2007, IEICE T INF SYST, VE90D, P1073, DOI 10.1093/ietisy/e90-d.7.1073
   Lee TY, 2006, VISUAL COMPUT, V22, P729, DOI 10.1007/s00371-006-0059-6
   Liu X, 2013, SIAM J SCI COMPUT, V35, pA1641, DOI 10.1137/120871328
   Luo GL, 2019, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2019), DOI 10.1145/3306131.3317017
   Luo GL, 2017, VISUAL COMPUT, V33, P1279, DOI 10.1007/s00371-016-1313-1
   Luo GL, 2013, COMPUT ANIMAT VIRT W, V24, P365, DOI 10.1002/cav.1522
   Maglo A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2693443
   Mamou K, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1121, DOI 10.1109/ICME.2008.4607636
   Payan F, 2007, COMPUT GRAPH-UK, V31, P77, DOI 10.1016/j.cag.2006.09.009
   Ramanathan S, 2008, IMAGE VISION COMPUT, V26, P1012, DOI 10.1016/j.imavis.2007.11.005
   Ren ZL, 2013, PROC CVPR IEEE, P2011, DOI 10.1109/CVPR.2013.262
   Sattler M., 2005, P 2005 ACM SIGGRAPH, P209
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   STEFANOSKI N, 2007, 3DTV C 2007, P1, DOI DOI 10.1109/3DTV.2007.4379461
   Stefanoski N, 2010, COMPUT GRAPH FORUM, V29, P101, DOI 10.1111/j.1467-8659.2009.01547.x
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Tevs A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159517
   Tsuchie S, 2014, COMPUT AIDED DESIGN, V46, P69, DOI 10.1016/j.cad.2013.08.019
   Vása L, 2014, COMPUT GRAPH FORUM, V33, P145, DOI 10.1111/cgf.12304
   Vása L, 2009, COMPUT GRAPH FORUM, V28, P1529, DOI 10.1111/j.1467-8659.2008.01304.x
   Vása L, 2013, IEEE T VIS COMPUT GR, V19, P1467, DOI 10.1109/TVCG.2013.22
   Vása L, 2011, IEEE T VIS COMPUT GR, V17, P220, DOI 10.1109/TVCG.2010.38
   Vasilakis AA, 2014, COMPUT GRAPH FORUM, V33, P293, DOI 10.1111/cgf.12327
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   Wang PJ, 2013, INFORM SCIENCES, V232, P1, DOI 10.1016/j.ins.2013.01.007
   Wuhrer S, 2010, VISUAL COMPUT, V26, P147, DOI 10.1007/s00371-009-0394-5
   Yang BL, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3300198
   Yang Bailin, 2018, COMPUTER ANIMATION V, V30, P6
   Yang JH, 2002, IEEE T CIRC SYST VID, V12, P1178, DOI 10.1109/TCSVT.2002.806814
   Yuan YZ, 2017, DIGIT SIGNAL PROCESS, V69, P204, DOI 10.1016/j.dsp.2017.06.028
   Zhu M Y, 2012, P 2012 ACM SIGGRAPH, P183, DOI [10.2312/SCA/SCA12/183-192, DOI 10.2312/SCA/SCA12/183-192]
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
NR 63
TC 7
Z9 7
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 14
DI 10.1145/3377475
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhong, SH
   Wang, YT
   Ren, TW
   Zheng, MJ
   Liu, Y
   Wu, GS
AF Zhong, Sheng-Hua
   Wang, Yuantian
   Ren, Tongwei
   Zheng, Mingjie
   Liu, Yan
   Wu, Gangshan
TI Steganographer Detection via Multi-Scale Embedding Probability
   Estimation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Steganographer detection; embedding probability estimation; steganalytic
   feature extraction; Gaussian vote; multimedia security
ID ADAPTIVE STEGANALYSIS
AB Steganographer detection aims to identify the guilty user who utilizes steganographic methods to hide secret information in the spread of multimedia data, especially image data, from a large amount of innocent users on social networks. A true embedding probability map illustrates the probability distribution of embedding secret information in the corresponding images by specific steganographic methods and settings, which has been successfully used as the guidance for content-adaptive steganographic and steganalytic methods. Unfortunately, in real-world situation, the detailed steganographic settings adopted by the guilty user cannot be known in advance. It thus becomes necessary to propose an automatic embedding probability estimation method. In this article, we propose a novel content-adaptive steganographer detection method via embedding probability estimation. The embedding probability estimation is first formulated as a learning-based saliency detection problem and the multi-scale estimated map is then integrated into the CNN to extract steganalytic features. Finally, the guilty user is detected via an efficient Gaussian vote method with the extracted steganalytic features. The experimental results prove that the proposed method is superior to the state-of-the-art methods in both spatial and frequency domains.
C1 [Zhong, Sheng-Hua; Zheng, Mingjie] Shenzhen Univ, Coll Comp Sci & Software Engn, 3688 Nanhai Ave, Shenzhen, Guangdong, Peoples R China.
   [Wang, Yuantian; Ren, Tongwei; Wu, Gangshan] Nanjing Univ, State Key Lab Novel Software Technol, 163 Xianlin Ave, Nanjing, Jiangsu, Peoples R China.
   [Liu, Yan] Hong Kong Polytech Univ, Dept Comp, Hung Hom, Kowloon, Hong Kong, Peoples R China.
C3 Shenzhen University; Nanjing University; Hong Kong Polytechnic
   University
RP Zhong, SH (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, 3688 Nanhai Ave, Shenzhen, Guangdong, Peoples R China.
EM csshzhong@szu.edu.cn; wangyt@smail.nju.edu.cn; rentw@nju.edu.cn;
   zhengmingjie@email.szu.edu.cn; csyliu@comp.polyu.edu.hk; gswu@nju.edu.cn
RI liu, yan/HGV-1365-2022
FU Natural Science Foundation of Guangdong Province [2016A030310053];
   Shenzhen high-level overseas talents program; National Science
   Foundation of China [61202320]; Science, Technology and Innovation
   Commission of Shenzhen Municipality [JCYJ20180307151516166];
   Collaborative Innovation Center of Novel Software Technology and
   Industrialization
FX This work was supported by the Natural Science Foundation of Guangdong
   Province (2016A030310053), the Shenzhen high-level overseas talents
   program, the National Science Foundation of China (61202320), the
   Science, Technology and Innovation Commission of Shenzhen Municipality
   (JCYJ20180307151516166), and the Collaborative Innovation Center of
   Novel Software Technology and Industrialization.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2015, B ENTOMOL RES
   [Anonymous], 2014, P 2 ACM INF HID WORK
   [Anonymous], 2016, ELECT IMAGING, DOI DOI 10.2352/ISSN.2470-1173.2016.8.MWSF-078
   [Anonymous], 2014, BIOMED RES INT
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Borji Ali, 2019, [Computational Visual Media, 计算可视媒体], V5, P117
   Breunig M. M., 2000, SIGMOD Record, V29, P93, DOI 10.1145/335191.335388
   Cogranne R, 2013, SIGNAL PROCESS, V93, P1724, DOI 10.1016/j.sigpro.2013.01.014
   Denemark T, 2014, IEEE INT WORKS INFOR, P48, DOI 10.1109/WIFS.2014.7084302
   Denemark Tomas, 2017, IEEE T INF FOREN SEC, V11, P1736
   Fillatre L, 2012, IEEE T SIGNAL PROCES, V60, P556, DOI 10.1109/TSP.2011.2174231
   Filler T, 2010, IEEE T INF FOREN SEC, V5, P705, DOI 10.1109/TIFS.2010.2077629
   Fridrich J, 2005, LECT NOTES COMPUT SC, V3727, P204
   Fridrich J, 2007, PROC SPIE, V6505, DOI 10.1117/12.697471
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu DH, 2017, SECUR COMMUN NETW, DOI 10.1155/2017/2314860
   Hu P, 2017, PROC CVPR IEEE, P540, DOI 10.1109/CVPR.2017.65
   Ker AD, 2012, P MULT SEC, P1
   Ker AD, 2007, LECT NOTES COMPUT SC, V4437, P265
   Ker AD, 2007, P SOC PHOTO-OPT INS, V6505, P50504, DOI 10.1117/12.703334
   Ker AD, 2014, IEEE T INF FOREN SEC, V9, P1424, DOI 10.1109/TIFS.2014.2336380
   Ker Andrew D., 2012, P SOC PHOTO-OPT INS, P265
   Ker Andrew D., 2012, P SOC PHOTO-OPT INS, V7880, P87
   Krishna O, 2018, IEEE IMAGE PROC, P2326, DOI 10.1109/ICIP.2018.8451835
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li F, 2017, EVID-BASED COMPL ALT, V2017, DOI 10.1155/2017/4837839
   Li FY, 2016, IEEE T INF FOREN SEC, V11, P344, DOI 10.1109/TIFS.2015.2496910
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li L, 2019, MULTIMED TOOLS APPL, V78, P8041, DOI 10.1007/s11042-018-6582-4
   Liao X, 2016, SECUR COMMUN NETW, V9, P5756, DOI 10.1002/sec.1734
   Lin GS, 2010, IEEE T MULTIMEDIA, V12, P345, DOI 10.1109/TMM.2010.2051243
   Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P33, DOI 10.1007/978-3-642-14267-3_2
   Luo WQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3190575
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Martin D., 2001, P ICCV, P416, DOI [DOI 10.1109/ICCV.2001.937655, 10.1109/ICCV.2001.937655]
   Molok N. N. A., 2013, NAT PROTOC, V4, P102
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Pevny T, 2008, IEEE T INF FOREN SEC, V3, P635, DOI 10.1109/TIFS.2008.2002936
   Qian YL, 2018, MULTIMED TOOLS APPL, V77, P19633, DOI 10.1007/s11042-017-5326-1
   Qian YL, 2015, PROC SPIE, V9409, DOI 10.1117/12.2083479
   Sedighi V, 2016, IEEE T INF FOREN SEC, V11, P221, DOI 10.1109/TIFS.2015.2486744
   Singh P, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3077140
   Tang W, 2014, P 2 ACM WORKSH INF H, P91, DOI [10.1145/2600918.2600935, DOI 10.1145/2600918.2600935]
   Tang WS, 2016, J SENSORS, V2016, DOI 10.1155/2016/8128651
   Tang WX, 2017, IEEE SIGNAL PROC LET, V24, P1547, DOI 10.1109/LSP.2017.2745572
   Wu SJ, 2017, INT J PHOTOENERGY, V2017, DOI 10.1155/2017/1504857
   Wu ST, 2018, MULTIMED TOOLS APPL, V77, P10437, DOI 10.1007/s11042-017-4440-4
   Xia ZH, 2016, MULTIMED TOOLS APPL, V75, P1947, DOI 10.1007/s11042-014-2381-8
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Yang JH, 2017, LECT NOTES COMPUT SC, V10431, P263, DOI 10.1007/978-3-319-64185-0_20
   Yang M, 2015, IEEE IMAGE PROC, P402, DOI 10.1109/ICIP.2015.7350829
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   Yin H, 2012, IEEE T MULTIMEDIA, V14, P178, DOI 10.1109/TMM.2011.2170556
   Zhang P, 2017, NEUROCOMPUTING, V257, P115, DOI 10.1016/j.neucom.2016.10.073
   Zhang X, 2018, IEEE WIREL COMMUN, V25, P12, DOI 10.1109/MWC.2018.1700327
   Zheng MJ, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P300, DOI 10.1145/3206025.3206031
   Zheng MJ, 2017, IEEE INT CON MULTI, P235, DOI 10.1109/ICME.2017.8019320
   Zhou H, 2017, IEEE T IMAGE PROCESS, V26, P1623, DOI 10.1109/TIP.2017.2657886
NR 63
TC 2
Z9 2
U1 0
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 103
DI 10.1145/3352691
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800003
DA 2024-07-18
ER

PT J
AU Yarnagula, HK
   Juluri, P
   Mehr, SK
   Tamarapalli, V
   Medhi, D
AF Yarnagula, Hema Kumar
   Juluri, Parikshit
   Mehr, Sheyda Kiani
   Tamarapalli, Venkatesh
   Medhi, Deep
TI QoE for Mobile Clients with Segment-aware Rate Adaptation Algorithm
   (SARA) for DASH Video Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Dynamic adaptive streaming over HTTP; bitrate adaptation algorithm;
   segment awareness; quality of experience; parametric QoE model;
   subjective quality assessment
ID HTTP
AB Dynamic adaptive streaming over HTTP (DASH) is widely used for video streaming on mobile devices. Ensuring a good quality of experience (QoE) for mobile video streaming is essential, as it severely impacts both the network and content providers' revenue. Thus, a good rate adaptation algorithm at the client end that provides high QoE is critically important. Recently, a segment size-aware rate adaptation (SARA) algorithm was proposed for DASH clients. However, its performance on mobile clients has not been investigated so far. The main contributions of this article are twofold: (1) We discuss SARA's implementation for mobile clients to improve the QoE in mobile video streaming, one that accurately predicts the download time for the next segment and makes an informed bitrate selection, and (2) we developed a new parametric QoE model to compute a cumulative score that helps in fair comparison of different adaptation algorithms. Based on our subjective and objective evaluation, we observed that SARA for mobile clients outperforms others by 17% on average, in terms of the Mean Opinion Score, while achieving, on average, a 76% improvement in terms of the interruption ratio. The score obtained from our new parametric QoE model also demonstrates that the SARA algorithm for mobile clients gives a better QoE among all the algorithms.
C1 [Yarnagula, Hema Kumar; Tamarapalli, Venkatesh] Indian Inst Technol Guwahati, Dept Comp Sci & Engn, Gauhati, India.
   [Juluri, Parikshit] Akamai Technol, Cambridge, MA USA.
   [Mehr, Sheyda Kiani; Medhi, Deep] Univ Missouri, Kansas City, MO 64110 USA.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Guwahati; University of Missouri System; University
   of Missouri Kansas City
RP Yarnagula, HK (corresponding author), Indian Inst Technol Guwahati, Dept Comp Sci & Engn, Gauhati, India.
EM h.yarnagula@iitg.ac.in; pjuluri@akamai.com; skkv6@mail.umkc.edu;
   tvenkat@iitg.ac.in; dmedhi@umkc.edu
RI Venkatesh, T/B-5029-2009
OI Tamarapalli, Venkatesh/0000-0002-2156-6885
CR Akhshabi Saamer, 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video-NOSSDAV'13
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], 2007, 12LS62E COM ITU T ST
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Bouten N, 2015, COMPUT NETW, V81, P96, DOI 10.1016/j.comnet.2015.02.007
   Bruneau-Queyreix J, 2017, CONSUM COMM NETWORK, P427, DOI 10.1109/CCNC.2017.7983147
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Cisco Systems Inc, 2017, CISCO VISUAL NETWORK
   Cofano G, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092836
   Coverdale P, 2011, IEEE SIGNAL PROC MAG, V28, P91, DOI 10.1109/MSP.2011.942467
   De Cicco L., 2011, P 2 ANN ACM C MULTIM, P145
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Heidemann J, 1997, IEEE ACM T NETWORK, V5, P616, DOI 10.1109/90.649564
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huang TC, 2013, 2013 INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND BIG DATA (CLOUDCOM-ASIA), P9, DOI 10.1109/CLOUDCOM-ASIA.2013.97
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Juluri P, 2016, IEEE COMMUN SURV TUT, V18, P401, DOI 10.1109/COMST.2015.2401424
   Juluri P, 2016, IEEE IFIP NETW OPER, P129, DOI 10.1109/NOMS.2016.7502805
   Juluri P, 2015, IEEE INT CONF COMM, P1765, DOI 10.1109/ICCW.2015.7247436
   Jumisko-Pyykkö S, 2008, INT J DIGIT MULTIMED, V2008, DOI 10.1155/2008/712380
   Juszka D., 2015, P 7 INT WORKSH QUAL, P1
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Kua J, 2017, IEEE COMMUN SURV TUT, V19, P1842, DOI 10.1109/COMST.2017.2685630
   Lederer S., 2012, P 3 MULT SYST C, P89
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   Ozfatura E, 2018, IEEE T BROADCAST, V64, P247, DOI 10.1109/TBC.2018.2823644
   Petrangeli S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818361
   Ries M, 2007, IEEE WCNC, P2670
   Riiser H, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240137
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Watson M., 2011, P ACM MULT SYST C
   Xiang S., 2012, ACM MMSys '12, P167
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhao S, 2017, 2017 IEEE CONFERENCE ON NETWORK FUNCTION VIRTUALIZATION AND SOFTWARE DEFINED NETWORKS (NFV-SDN), P129
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 41
TC 11
Z9 14
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 36
DI 10.1145/3311749
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IM3ZV
UT WOS:000477935400007
DA 2024-07-18
ER

PT J
AU Sun, J
   Huang, D
   Wang, YH
   Chen, LM
AF Sun, Jia
   Huang, Di
   Wang, Yunhong
   Chen, Liming
TI Expression Robust 3D Facial Landmarking via Progressive Coarse-to-Fine
   Tuning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D facial landmarking; curvature analysis; active normal model; cascaded
   regression; coarse-to-fine tuning
ID FACE; RECOGNITION
AB Facial landmarking is a fundamental task in automatic machine-based face analysis. The majority of existing techniques for such a problem are based on 2D images; however, they suffer from illumination and pose variations that may largely degrade landmarking performance. The emergence of 3D data theoretically provides an alternative to overcome these weaknesses in the 2D domain. This article proposes a novel approach to 3D facial landmarking, which combines both the advantages of feature-based methods as well as model-based ones in a progressive three-stage coarse-to-fine manner (initial, intermediate, and fine stages). For the initial stage, a few fiducial landmarks (i.e., the nose tip and two inner eye corners) are robustly detected through curvature analysis, and these points are further exploited to initialize the subsequent stage. For the intermediate stage, a statistical model is learned in the feature space of three normal components of the facial point-cloud rather than the smooth original coordinates, namely Active Normal Model (ANM). For the fine stage, cascaded regression is employed to locally refine the landmarks according to their geometry attributes. The proposed approach can accurately localize dozens of fiducial points on each 3D face scan, greatly surpassing the feature-based ones, and it also improves the state of the art of the model-based ones in two aspects: sensitivity to initialization and deficiency in discrimination. The proposed method is evaluated on the BU-3DFE, Bosphorus, and BU-4DFE databases, and competitive results are achieved in comparison with counterparts in the literature, clearly demonstrating its effectiveness.
C1 [Sun, Jia; Huang, Di; Wang, Yunhong; Chen, Liming] Beihang Univ, Beijing Adv Innovat Ctr Big Data & Brain Comp, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
   [Chen, Liming] Ecole Cent Lyon, LIRIS, F-69134 Lyon, France.
C3 Beihang University; Institut National des Sciences Appliquees de Lyon -
   INSA Lyon; Ecole Centrale de Lyon
RP Huang, D (corresponding author), Beihang Univ, Beijing Adv Innovat Ctr Big Data & Brain Comp, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
EM sunjia@buaa.edu.cn; dhuang@buaa.edu.cn; yhwang@buaa.edu.cn;
   liming.chen@ec-lyon.fr
RI Huang, Di/JBJ-3541-2023
OI Huang, Di/0000-0001-7877-7301
FU National Natural Science Foundation of China [61673033]; French Research
   Agency, l'Agence Nationale de Recherche, through the Jemime project
   [ANR-13-CORD-0004-02]; Microsoft Research Asia Collaborative Program
   [FY17-RES-THEME-033]
FX This work is partly supported by the National Natural Science Foundation
   of China (No. 61673033), the French Research Agency, l'Agence Nationale
   de Recherche, through the Jemime project under grant
   ANR-13-CORD-0004-02, and the Microsoft Research Asia Collaborative
   Program (FY17-RES-THEME-033).
CR [Anonymous], P 2 EUR WORKSH 3D OB
   [Anonymous], 1995, COMPUTER VISION IMAG
   Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x
   BESL PJ, 1986, COMPUT VISION GRAPH, V33, P33, DOI 10.1016/0734-189X(86)90220-3
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Breiman L., 2001, Mach. Learn., V45, P5
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Colbry D., 2005, P 2005 IEEE COMPUTER, V03, P118
   Colombo A, 2006, PATTERN RECOGN, V39, P444, DOI 10.1016/j.patcog.2005.09.009
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Creusot C, 2013, INT J COMPUT VISION, V102, P146, DOI 10.1007/s11263-012-0605-9
   Dibeklioglu H, 2008, 2008 IEEE SECOND INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS), P382
   Fanelli G, 2013, IEEE INT CONF AUTOMA
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Fang TH, 2012, IMAGE VISION COMPUT, V30, P738, DOI 10.1016/j.imavis.2012.02.004
   Gilani SZ, 2017, PATTERN RECOGN, V69, P238, DOI 10.1016/j.patcog.2017.04.013
   Gupta SD, 2010, METHODS MOL BIOL, V589, P97, DOI [10.1007/978-1-60327-114-1_10, 10.1109/SSIAI.2010.5483908]
   Gupta S, 2010, INT J COMPUT VISION, V90, P331, DOI 10.1007/s11263-010-0360-8
   Huang D, 2012, IEEE T INF FOREN SEC, V7, P1551, DOI 10.1109/TIFS.2012.2206807
   Huang D, 2011, LECT NOTES COMPUT SC, V6523, P206
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Li H, 2011, PROCEEDINGS OF THE SEVENTH INTERNATIONAL SYMPOSIUM ON VITICULTURE AND ENOLOGY (2011), P1
   Li HB, 2015, COMPUT VIS IMAGE UND, V140, P83, DOI 10.1016/j.cviu.2015.07.005
   Li HB, 2015, INT J COMPUT VISION, V113, P128, DOI 10.1007/s11263-014-0785-6
   Lu XG, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P585
   Mian AS, 2007, IEEE T PATTERN ANAL, V29, P1927, DOI 10.1109/TPAMI.2007.1105
   Nair P, 2009, IEEE T MULTIMEDIA, V11, P611, DOI 10.1109/TMM.2009.2017629
   PENTLAND A, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P84, DOI 10.1109/CVPR.1994.323814
   Perakis P, 2013, IEEE T PATTERN ANAL, V35, P1552, DOI 10.1109/TPAMI.2012.247
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Romero-Huertas M., 2008, 2 IEEE INT C BIOMETR, P1
   Sauer P, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.30
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Segundo MP, 2010, IEEE T SYST MAN CY B, V40, P1319, DOI 10.1109/TSMCB.2009.2038233
   Song ML, 2014, IEEE T IMAGE PROCESS, V23, P5108, DOI 10.1109/TIP.2014.2361204
   Sukno FM, 2015, IEEE T CYBERNETICS, V45, P1717, DOI 10.1109/TCYB.2014.2359056
   Sun J., 2014, P IEEE INT JOINT C B, P1, DOI [10.1109/BTAS.2014.6996267, DOI 10.1109/BTAS.2014.6996267]
   Szeptycki P, 2009, 2009 IEEE 3RD INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P32, DOI 10.1109/BTAS.2009.5339052
   VINCENT JM, 1992, IEE PROC-F, V139, P405, DOI 10.1049/ip-f-2.1992.0058
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yao YQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3131345
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Yin LJ, 2008, IEEE INT CONF AUTOMA, P116
   Zhang J, 2015, IEEE I CONF COMP VIS, P3801, DOI 10.1109/ICCV.2015.433
   Zhao X, 2011, IEEE T SYST MAN CY B, V41, P1417, DOI 10.1109/TSMCB.2011.2148711
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 49
TC 3
Z9 3
U1 3
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
AR 21
DI 10.1145/3282833
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9JB
UT WOS:000459798800005
DA 2024-07-18
ER

PT J
AU Bahirat, K
   Lai, CY
   Mcmahan, RP
   Prabhakaran, B
AF Bahirat, Kanchan
   Lai, Chengyuan
   Mcmahan, Ryan P.
   Prabhakaran, Balakrishnan
TI Designing and Evaluating a Mesh Simplification Algorithm for Virtual
   Reality
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Mesh simplification; quadric error metric; virtual reality
ID QUALITY; ERROR; MODEL; JND
AB With the increasing accessibility of the mobile head-mounted displays (HMDs), mobile virtual reality (VR) systems are finding applications in various areas. However, mobile HMDs are highly constrained with limited graphics processing units (GPUs) and low processing power and onboard memory. Hence, VR developers must be cognizant of the number of polygons contained within their virtual environments to avoid rendering at low frame rates and inducing simulator sickness. The most robust and rapid approach to keeping the overall number of polygons low is to use mesh simplification algorithms to create low-poly versions of preexisting, high-poly models. Unfortunately, most existing mesh simplification algorithms cannot adequately handle meshes with lots of boundaries or nonmanifold meshes, which are common attributes of many 3D models.
   In this article, we present QEM(4)VR, a high-fidelity mesh simplification algorithm specifically designed for VR. This algorithm addresses the deficiencies of prior quadric error metric (QEM) approaches by leveraging the insight that the most relevant boundary edges lie along curvatures while linear boundary edges can be collapsed. Additionally, our algorithm preserves key surface properties, such as normals, texture coordinates, colors, and materials, as it preprocesses 3D models and generates their low-poly approximations offline.
   We evaluated the effectiveness of our QEM(4)VR algorithm by comparing its simplified-mesh results to those of prior QEM variations in terms of geometric approximation error, texture error, progressive approximation errors, frame rate impact, and perceptual quality measures. We found that QEM(4)VR consistently yielded simplified meshes with less geometric approximation error and texture error than the prior QEM variations. It afforded better frame rates than QEM variations with boundary preservation constraints that create unnecessary lower bounds on overall polygon count reduction. Our evaluation revealed that QEM(4)VR did not fair well in terms of existing perceptual distance measurements, but human-based inspections demonstrate that these algorithmic measurements are not suitable substitutes for actual human perception. In turn, we present a user-based methodology for evaluating the perceptual qualities of mesh simplification algorithms.
C1 [Bahirat, Kanchan; Lai, Chengyuan; Mcmahan, Ryan P.; Prabhakaran, Balakrishnan] Univ Texas Dallas, Erik Jonsson Sch Engn & Comp Sci, Dept Comp Sci, 800 W Campbell Rd,MS EC31, Richardson, TX 75080 USA.
C3 University of Texas System; University of Texas Dallas
RP Bahirat, K (corresponding author), Univ Texas Dallas, Erik Jonsson Sch Engn & Comp Sci, Dept Comp Sci, 800 W Campbell Rd,MS EC31, Richardson, TX 75080 USA.
EM Kanchan.Bahirat@utdallas.edu; Chengyuan.Lai@utdallas.edu;
   rymcmaha@utdallas.edu; bprabhakaran@utdallas.edu
RI Lai, Chengyuan/AAG-4995-2019
OI Lai, Chengyuan/0000-0003-4219-4803; McMahan, Ryan/0000-0001-9357-9696
FU National Science Foundation (NSF) [1012975]; US Army Research Office
   (ARO) [W911NF-17-1-0299]; Direct For Computer & Info Scie & Enginr;
   Division Of Computer and Network Systems [1012975] Funding Source:
   National Science Foundation
FX This material is based on work supported by the National Science
   Foundation (NSF) under Grant No. 1012975 and US Army Research Office
   (ARO) W911NF-17-1-0299. Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the authors and
   do not necessarily reflect the views of NSF and ARO.
CR [Anonymous], 1993, Modeling in Computer Graphics
   [Anonymous], 2008, MESHLAB OPEN SOURCE, DOI DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136
   [Anonymous], 2016, IEEE, DOI DOI 10.1109/QOMEX.2016.7498964
   Bahirat K, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P50, DOI 10.1145/3083187.3083188
   Boos K., 2016, MOBISYS, P291
   Bowman DA, 2007, COMPUTER, V40, P36, DOI 10.1109/MC.2007.257
   Cheng I, 2005, IEEE T CIRC SYST VID, V15, P1234, DOI 10.1109/TCSVT.2005.854234
   Cheng I, 2006, IEEE INT SYM MULTIM, P533
   Chengyuan Lai, 2016, Virtual, Augmented and Mixed Reality. 8th International Conference, VAMR 2016, held as part of HCI International 2016. Proceedings: LNCS 9740, P702, DOI 10.1007/978-3-319-39907-2_67
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Cignoni P, 1998, COMPUT GRAPH-UK, V22, P37, DOI 10.1016/S0097-8493(97)00082-4
   Cohen J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P119, DOI 10.1145/237170.237220
   Ebrahimi T., 2009, P 17 ACM INT C MULTI, P3
   Eck M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P173, DOI 10.1145/218380.218440
   Gargantini A., 2015, Proceedings of the 3rd 2015 Workshop on ICTs for improving Patients Rehabilitation Research Techniques, P81, DOI [10.1145/2838944.2838964, DOI 10.1145/2838944.2838964]
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   GARLAND M, 1999, CMUCS99105
   Goldman R, 2005, COMPUT AIDED GEOM D, V22, P632, DOI 10.1016/j.cagd.2005.06.005
   Guéziec A, 1998, VISUALIZATION '98, PROCEEDINGS, P383, DOI 10.1109/VISUAL.1998.745327
   Hachicha W, 2013, IEEE IMAGE PROC, P113, DOI 10.1109/ICIP.2013.6738024
   He TS, 1995, VISUALIZATION '95 - PROCEEDINGS, P296, DOI 10.1109/VISUAL.1995.485142
   Hoppe H., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P59, DOI 10.1109/VISUAL.1999.809869
   Hoppe H., 1993, Computer Graphics Proceedings, P19, DOI 10.1145/166117.166119
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Hosseini M., 2012, Proceedings of the 3rd Multimedia Systems Conference, P143
   Keighrev C., 2017, Quality of Multimedia Experience (QoMEX), 2017 Ninth International Conference on, P1
   Lavoue G., 2006, SPIE OPTICS PHOTONIC, p63 120L
   Lavoué G, 2011, COMPUT GRAPH FORUM, V30, P1427, DOI 10.1111/j.1467-8659.2011.02017.x
   Lecuyer A., 2000, Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048), P83, DOI 10.1109/VR.2000.840369
   Lindstrom P, 1998, VISUALIZATION '98, PROCEEDINGS, P279, DOI 10.1109/VISUAL.1998.745314
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Luebke D., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P199, DOI 10.1145/258734.258847
   Luebke DP, 2001, IEEE COMPUT GRAPH, V21, P24, DOI 10.1109/38.920624
   McMahan RP, 2012, IEEE T VIS COMPUT GR, V18, P626, DOI 10.1109/TVCG.2012.43
   Meehan M, 2002, ACM T GRAPHIC, V21, P645, DOI 10.1145/566570.566630
   Microsoft Developers Resources, PERF REC MICR HOL
   Ovreiu E., 2012, THESIS
   Pohl D, 2016, P IEEE VIRT REAL ANN, P267, DOI 10.1109/VR.2016.7504756
   Pojar Erik., 2003, Proceedings of the 2003 symposium on Interactive 3D graphics, P127
   Pruett C., SQUEEZING PERFORMANC
   Puig J, 2012, INT WORK QUAL MULTIM, P188, DOI 10.1109/QoMEX.2012.6263864
   Qin SL, 2009, J SOC INF DISPLAY, V17, P687, DOI 10.1889/JSID17.8.687
   Raake A, 2014, T-LAB SER TELECOMMUN, P11, DOI 10.1007/978-3-319-02681-7_2
   SCHROEDER WJ, 1992, COMP GRAPH, V26, P65, DOI 10.1145/142920.134010
   Steed A, 2016, IEEE T VIS COMPUT GR, V22, P1406, DOI 10.1109/TVCG.2016.2518135
   Timmerer C, 2015, COMPUTER, V48, P108, DOI 10.1109/MC.2015.89
   Tong X., 2015, International Symposium on Pervasive Computing Paradigms for Mental Health, P284
   Torkhani F., 2014, MACH GRAPHICS VIS, V23, P1
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Wang HQ, 2017, J VIS COMMUN IMAGE R, V46, P292, DOI 10.1016/j.jvcir.2017.04.009
   Wang K., 2012, Source code for fast mesh perceptual distance (fmpd)
   Wang K, 2012, COMPUT GRAPH-UK, V36, P808, DOI 10.1016/j.cag.2012.06.004
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WATSON AB, 1990, PERCEPT PSYCHOPHYS, V47, P87, DOI 10.3758/BF03208169
   Wu JH, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P12, DOI 10.1109/PCCGA.2001.962853
   Wu W., 2011, P 19 ACM INT C MULTI, P13
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Zhang XH, 2008, J VIS COMMUN IMAGE R, V19, P30, DOI 10.1016/j.jvcir.2007.06.001
   Zhao Y, 2011, IEEE SIGNAL PROC LET, V18, P19, DOI 10.1109/LSP.2010.2090041
   Zielinski DJ, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P133, DOI 10.1109/3DUI.2016.7460043
NR 62
TC 21
Z9 22
U1 2
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
SU S
AR 63
DI 10.1145/3209661
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG9GD
UT WOS:000455314500011
OA Bronze
DA 2024-07-18
ER

PT J
AU Hou, XS
   Lu, Y
   Dey, S
AF Hou, Xueshi
   Lu, Yao
   Dey, Sujit
TI Novel Hybrid-Cast Approach to Reduce Bandwidth and Latency for
   Cloud-Based Virtual Space
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cloud-based mobile multimedia; virtual reality; virtual space;
   hybrid-cast
AB In this article, we explore the possibility of enabling cloud-based virtual space applications for better computational scalability and easy access from any end device, including future lightweight wireless head-mounted displays. In particular, we investigate virtual space applications such as virtual classroom and virtual gallery, in which the scenes and activities are rendered in the cloud, with multiple views captured and streamed to each end device. A key challenge is the high bandwidth requirement to stream all the user views, leading to high operational cost and potential large delay in a bandwidth-restricted wireless network. We propose a novel hybrid-cast approach to save bandwidth in a multi-user streaming scenario. We identify and broadcast the common pixels shared by multiple users, while unicasting the residual pixels for each user. We formulate the problem of minimizing the total bitrate needed to transmit the user views using hybrid-casting and describe our approach. A common view extraction approach and a smart grouping algorithm are proposed and developed to achieve our hybrid-cast approach. Simulation results show that the hybrid-cast approach can significantly reduce total bitrate by up to 55% and avoid congestion-related latency, compared to traditional cloud-based approach of transmitting all the views as individual unicast streams, hence addressing the bandwidth challenges of the cloud, with additional benefits in cost and delay.
C1 [Hou, Xueshi; Lu, Yao; Dey, Sujit] Univ Calif San Diego, Dept Elect & Comp Engn, Mobile Syst Design Lab, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Hou, XS (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, Mobile Syst Design Lab, La Jolla, CA 92093 USA.
EM x7hou@ucsd.edu; luyao@ucsd.edu; dey@ece.ucsd.edu
RI Lu, Yao/R-2982-2017
OI Hou, Xueshi/0000-0003-3083-7656
CR Ahn S.H., 2018, OPENGL TRANSFORMATIO
   Amazon, 2018, AM EC2 PRIC
   [Anonymous], 2018, AV1
   [Anonymous], 2018, REAL TIM TRANSP PROT
   [Anonymous], 2001, CISC VIS NETW IND GL
   [Anonymous], 2018, REND PIP
   [Anonymous], IEEE COMMUNICATIONS
   [Anonymous], 2018, VP9 VIDEO CODEC
   [Anonymous], 1995, VIDEO CODING LOW BIT
   [Anonymous], 2018, RES RES PROT
   [Anonymous], 2018, IP MULT
   [Anonymous], 2018, 5G XCAST PROJ
   [Anonymous], 1990, VIDEO CODEC AUDIOVIS
   [Anonymous], 2018, PRAGM GEN MULT
   Carbone M, 2010, ACM SIGCOMM COMP COM, V40, P13, DOI 10.1145/1764873.1764876
   Checko A, 2015, IEEE COMMUN SURV TUT, V17, P405, DOI 10.1109/COMST.2014.2355255
   Dinh HT, 2013, WIREL COMMUN MOB COM, V13, P1587, DOI 10.1002/wcm.1203
   Hou XS, 2016, IEEE INT SYM MULTIM, P533, DOI 10.1109/ISM.2016.38
   HTC, 2018, HTC VIV
   ISO/ IEC JTC 1, 1999, 11999 ISOIEC JTC
   ISO/ IEC JTC 1, 1993, 11993 ISOIEC JTC
   ITU-T and ISO/ IEC JTC, 2013, 230082 ISOIEC HEVC
   ITU-T ISO/ IEC JTC 1, 1994, 11994 ITUT ISOIEC JT
   ITU-T ISO/ IEC JTC 1, 2003, 12003 ITUT ISOIEC JT
   Jin X, 2013, PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT '13), P163, DOI 10.1145/2535372.2535377
   Liu Y, 2014, IEEE J EM SEL TOP C, V4, P43, DOI 10.1109/JETCAS.2014.2298921
   Lu Y, 2016, IEEE J EM SEL TOP C, V6, P544, DOI 10.1109/JETCAS.2016.2602246
   Lu Y, 2015, IEEE J-STSP, V9, P517, DOI 10.1109/JSTSP.2015.2396475
   Miller S.K., 1998, STARBURST MULTICAST
   Oculus, 2018, PERF HEAD UP DISPL
   Patait A., 2018, HIGH PERFORMANCE VID
   Samsung, 2018, SAMSUNG GEAR VR
   Song Ho Ahn, 2018, OPENGL PROJECTION MA
   Wang SX, 2013, IEEE T MULTIMEDIA, V15, P870, DOI 10.1109/TMM.2013.2240674
   Wei Cai, 2012, 2012 IEEE 4th International Conference on Cloud Computing Technology and Science (CloudCom). Proceedings, P640, DOI 10.1109/CloudCom.2012.6427515
   Wong F., 2016, INTEL VIRTUAL REALIT
   Xu Y, 2013, IEEE WIREL COMMUN, V20, P46, DOI 10.1109/MWC.2013.6549282
NR 37
TC 0
Z9 1
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
SU S
AR 58
DI 10.1145/3205864
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG9GD
UT WOS:000455314500006
DA 2024-07-18
ER

PT J
AU Jiang, SH
   Wu, Y
   Fu, Y
AF Jiang, Shuhui
   Wu, Yue
   Fu, Yun
TI Deep Bidirectional Cross-Triplet Embedding for Online Clothing Shopping
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Clothing retrieval; accessory recommendation; triplet embedding; deep
   learning; cross domain
AB In this article, we address the cross-domain (i.e., street and shop) clothing retrieval problem and investigate its real-world applications for online clothing shopping. It is a challenging problem due to the large discrepancy between street and shop domain images. We focus on learning an effective feature-embedding model to generate robust and discriminative feature representation across domains. Existing triplet embedding models achieve promising results by finding an embedding metric in which the distance between negative pairs is larger than the distance between positive pairs plus a margin. However, existing methods do not address the challenges in the cross-domain clothing retrieval scenario sufficiently. First, the intradomain and cross-domain data relationships need to be considered simultaneously. Second, the number of matched and nonmatched cross-domain pairs are unbalanced. To address these challenges, we propose a deep cross-triplet embedding algorithm together with a cross-triplet sampling strategy. The extensive experimental evaluations demonstrate the effectiveness of the proposed algorithms well. Furthermore, we investigate two novel online shopping applications, clothing trying on and accessories recommendation, based on a unified cross-domain clothing retrieval framework.
C1 [Jiang, Shuhui; Wu, Yue] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
   [Fu, Yun] Northeastern Univ, Coll Engn, Dept Elect & Comp Engn, Boston, MA 02115 USA.
   [Fu, Yun] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
C3 Northeastern University; Northeastern University; Northeastern
   University
RP Jiang, SH (corresponding author), Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
EM shjiang@ece.neu.edu; yuewu@ece.neu.edu; yunfu@ece.neu.edu
RI Jiang, Shuhui/W-6907-2019
CR [Anonymous], 2012, AS C COMP VIS
   [Anonymous], 2012, P ACM INT C MULT
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], 2015, ARXIV151106452
   Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Chen Q, 2015, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR.2015.7299169
   Chen TY, 2013, IEEE INT CONF COMMUN, P11, DOI 10.1109/ICCChinaW.2013.6670558
   Cheng Chueh Min, 2008, CHI 08 EXTENDED ABST, P2787
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Di W, 2013, IEEE COMPUT SOC CONF, P8, DOI 10.1109/CVPRW.2013.6
   Ding ZM, 2014, IEEE DATA MINING, P110, DOI 10.1109/ICDM.2014.29
   Ding ZM, 2015, IEEE T IMAGE PROCESS, V24, P4322, DOI 10.1109/TIP.2015.2462023
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Iwata Tomoharu, 2011, P 22 INT JOINT C ON, V3, P2262
   Jagadeesh V, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1925, DOI 10.1145/2623330.2623332
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang SH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3721
   Jiang SH, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P52, DOI 10.1145/2964284.2967182
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Jiang SH, 2013, IEEE INT SYMP CIRC S, P881, DOI 10.1109/ISCAS.2013.6571988
   Jiang Shuhui, 2016, P 30 AAAI C ART INT
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li J, 2017, IEEE T NEUR NET LEAR, V28, P1425, DOI 10.1109/TNNLS.2016.2541681
   Li J, 2015, AAAI CONF ARTIF INTE, P3804
   Liu S, 2015, PROC CVPR IEEE, P1419, DOI 10.1109/CVPR.2015.7298748
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Luo W, 2018, IEEE T NEUR NET LEAR, V29, P3289, DOI 10.1109/TNNLS.2017.2712793
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen E, 2007, 2007 INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P365
   Shuhui Jiang, 2016, IEEE Transactions on Big Data, V2, P43, DOI 10.1109/TBDATA.2016.2541160
   Simo-Serra E, 2015, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2015.7298688
   Tao Z., 2017, P C INT JOINT C ART P IJCAI AUG, P2843, DOI DOI 10.24963/IJCAI.2017/396
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Vittayakorn S, 2015, IEEE WINT CONF APPL, P951, DOI 10.1109/WACV.2015.131
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang X, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P7, DOI 10.1145/2911996.2912002
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
NR 48
TC 16
Z9 16
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2018
VL 14
IS 1
AR 5
DI 10.1145/3152114
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW9HO
UT WOS:000425646500005
DA 2024-07-18
ER

PT J
AU Wu, JY
   Cheng, B
   Yang, Y
   Wang, M
   Chen, JL
AF Wu, Jiyan
   Cheng, Bo
   Yang, Yuan
   Wang, Ming
   Chen, Junliang
TI Delay-Aware Quality Optimization in Cloud-Assisted Video Streaming
   System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cloud-assisted video streaming; delay-awareness; quality optimization;
   burst loss; differentiated transmission
ID TRANSMISSION; CONGESTION; FRAMEWORK
AB Cloud-assisted video streaming has emerged as a new paradigm to optimize multimedia content distribution over the Internet. This article investigates the problem of streaming cloud-assisted real-time video to multiple destinations (e.g., cloud video conferencing, multi-player cloud gaming, etc.) over lossy communication networks. The user diversity and network dynamics result in the delay differences among multiple destinations. This research proposes Differentiated cloud-Assisted VIdeo Streaming (DAVIS) framework, which proactively leverages such delay differences in video coding and transmission optimization. First, we analytically formulate the optimization problem of joint coding and transmission to maximize received video quality. Second, we develop a quality optimization framework that integrates the video representation selection and FEC (Forward Error Correction) packet interleaving. The proposed DAVIS is able to effectively perform differentiated quality optimization for multiple destinations by taking advantage of the delay differences in cloud-assisted video streaming system. We conduct the performance evaluation through extensive experiments with the Amazon EC2 instances and Exata emulation platform. Evaluation results show that DAVIS outperforms the reference cloud-assisted streaming solutions in video quality and delay performance.
C1 [Wu, Jiyan; Cheng, Bo; Wang, Ming; Chen, Junliang] Beijing Univ Posts & Telecommun, State Key Lab Networking & SwitchingTechnol, Beijing 100876, Peoples R China.
   [Yang, Yuan] Southeast Univ, Sch Instrument Sci & Engn, Nanjing 211189, Jiangsu, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Southeast University -
   China
RP Cheng, B (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & SwitchingTechnol, Beijing 100876, Peoples R China.
EM wujiyan@bupt.edu.cn; chengbo@bupt.edu.cn; 101011978@seu.edu.cn;
   wangming_bupt@bupt.edu.cn; chjl@bupt.edu.cn
RI Chen, John/GPW-8839-2022
FU National High-tech R&D Program of China (863 Program) [2013AA102301];
   National Natural Science Foundation of China [61601123]; Natural Science
   Foundation of Jiangsu Province of China [BK20160697]
FX This work was supported in part by the National High-tech R&D Program of
   China (863 Program) under Grant No. 2013AA102301, the National Natural
   Science Foundation of China under Grant No. 61601123, and the Natural
   Science Foundation of Jiangsu Province of China under Grant No.
   BK20160697.
CR Akamai, 2013, AK INV SUMM
   Badia L, 2010, IEEE J SEL AREA COMM, V28, P488, DOI 10.1109/JSAC.2010.100419
   Cen S, 2003, IEEE ACM T NETWORK, V11, P703, DOI 10.1109/TNET.2003.818187
   Chen X., 2011, Proc. 19th ACM International Conference on Multimedia (MM '11), P493
   Cisco, 2016, VIS NETW IND FOR MET
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   EXata, 2015, EXATA NETW EM SOFTW
   Fang ZY, 2016, PROCEEDINGS OF 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA ANALYSIS (ICBDA), P1
   Fraleigh C, 2003, IEEE NETWORK, V17, P6, DOI 10.1109/MNET.2003.1248656
   Frossard P, 2001, IEEE COMMUN LETT, V5, P122, DOI 10.1109/4234.913160
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Hajiesmaili MH, 2015, INT CON DISTR COMP S, P103, DOI 10.1109/ICDCS.2015.19
   Hameed A, 2016, IEEE T MULTIMEDIA, V18, P764, DOI 10.1109/TMM.2016.2525862
   He J, 2013, IEEE T CIRC SYST VID, V23, P1717, DOI 10.1109/TCSVT.2013.2255423
   Huang CY, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2537855
   Li Y, 2007, IEEE INFOCOM SER, P830, DOI 10.1109/INFCOM.2007.102
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Mahy R., 2010, 5766 IETF RFC
   Mukerjee MK, 2015, ACM SIGCOMM COMP COM, V45, P311, DOI 10.1145/2829988.2787475
   Oliveira T, 2011, IEEE INFOCOM SER, P2390, DOI 10.1109/INFCOM.2011.5935059
   QualNet, 2015, QUALNET NETW SIM SOF
   Ribeiro V. J., 2003, PROC PASSIVE ACTIVE, P1
   Sarkar UK, 2003, IEEE ACM T NETWORK, V11, P638, DOI 10.1109/TNET.2003.815292
   Sharma Vicky, P 2008 IEEE INT C CO, P1
   Si XB, 2012, IEEE INT WORKS INFOR, P1, DOI 10.1109/WIFS.2012.6412616
   Srebrny Piotr, 2010, Proceedings of the 2010 IEEE 30th International Conference on Distributed Computing Systems. ICDCS 2010, P209, DOI 10.1109/ICDCS.2010.29
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Sun M.-T., 2000, Compressed Video over Networks, V1st
   Toni L, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700294
   Wang F, 2012, IEEE INFOCOM SER, P199, DOI 10.1109/INFCOM.2012.6195578
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wu JY, 2016, IEEE T CIRC SYST VID, V26, P711, DOI 10.1109/TCSVT.2015.2412774
   Xu Y., 2012, P 2012 INTERNET MEAS, P371, DOI DOI 10.1145/2398776.2398816
   Xue NN, 2015, IEEE T MULTIMEDIA, V17, P1617, DOI 10.1109/TMM.2015.2450014
   Yu CG, 2014, IEEE INFOCOM SER, P1456, DOI 10.1109/INFOCOM.2014.6848080
   Zhang XG, 2012, IEEE INFOCOM SER, P621, DOI 10.1109/INFCOM.2012.6195805
   Zhu C, 2016, IEEE T VEH TECHNOL, V65, P1506, DOI 10.1109/TVT.2015.2413790
   Zhu XQ, 2005, SIGNAL PROCESS-IMAGE, V20, P773, DOI 10.1016/j.image.2005.05.005
   Zhu ZQ, 2013, IEEE T MULTIMEDIA, V15, P758, DOI 10.1109/TMM.2013.2238908
NR 39
TC 5
Z9 5
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2018
VL 14
IS 1
AR 4
DI 10.1145/3152116
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW9HO
UT WOS:000425646500004
DA 2024-07-18
ER

PT J
AU Hu, Y
   Zhao, C
   Cai, D
   He, XF
   Li, XL
AF Hu, Yao
   Zhao, Chen
   Cai, Deng
   He, Xiaofei
   Li, Xuelong
TI Atom Decomposition with Adaptive Basis Selection Strategy for Matrix
   Completion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Matrix completion; atom decomposition;
   basis selection
ID ALGORITHM
AB Estimating missing entries in matrices has attracted much attention due to its wide range of applications like image inpainting and video denoising, which are usually considered as low-rank matrix completion problems theoretically. It is common to consider nuclear norm as a surrogate of the rank operator since it is the tightest convex lower bound of the rank operator under certain conditions. However, most approaches based on nuclear norm minimization involve a number of singular value decomposition (SVD) operations. Given a matrix X is an element of R-mxn, the time complexity of the SVD operation is O(mn(2)), which brings prohibitive computational burden on large-scale matrices, limiting the further usage of these methods in real applications. Motivated by this observation, a series of atom-decomposition-based matrix completion methods have been studied. The key to these methods is to reconstruct the target matrix by pursuit methods in a greedy way, which only involves the computation of the top SVD and has great advantages in efficiency compared with the SVD-based matrix completion methods. However, due to gradually serious accumulation errors, atom-decomposition-based methods usually result in unsatisfactory reconstruction accuracy. In this article, we propose a new efficient and scalable atom decomposition algorithm for matrix completion called Adaptive Basis Selection Strategy (ABSS). Different from traditional greedy atom decomposition methods, a two-phase strategy is conducted to generate the basis separately via different strategies according to their different nature. At first, we globally prune the basis space to eliminate the unimportant basis as much as possible and locate the probable subspace containing the most informative basis. Then, another group of basis spaces are learned to improve the recovery accuracy based on local information. In this way, our proposed algorithm breaks through the accuracy bottleneck of traditional atom-decomposition-based matrix completion methods; meanwhile, it reserves the innate efficiency advantages over SVD-based matrix completion methods. We empirically evaluate the proposed algorithm ABSS on real visual image data and large-scale recommendation datasets. Results have shown that ABSS has much better reconstruction accuracy with comparable cost to atom-decomposition-based methods. At the same time, it outperforms the state-of-the-art SVD-based matrix completion algorithms by similar or better reconstruction accuracy with enormous advantages on efficiency.
C1 [Hu, Yao; Zhao, Chen; Cai, Deng; He, Xiaofei] Zhejiang Univ, State Key Lab CAD & CG, 388 Yu Hang Tang Rd, Hangzhou 310058, Zhejiang, Peoples R China.
   [Li, Xuelong] Chinese Acad Sci, Ctr OPT IMagery Anal & Learning OPTIMAL, State Key Lab Transient Opt & Photon, Xian Inst Opt & Precis Mech, Xian 710119, Shaanxi, Peoples R China.
C3 Zhejiang University; Chinese Academy of Sciences; Xi'an Institute of
   Optics & Precision Mechanics, CAS; State Key Laboratory of Transient
   Optics & Photonics
RP Hu, Y (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, 388 Yu Hang Tang Rd, Hangzhou 310058, Zhejiang, Peoples R China.
EM huyao001@gmail.com; chenzhaozju@gmail.com; dengcai@gmail.com;
   xiaofeihe@gmail.com; xuelongli@opt.ac.cn
RI Li, Xuelong/Z-3785-2019; Li, Xuelong/ABF-3381-2020; Hu,
   Yao/KEH-3649-2024; li, xiang/GWM-6319-2022
OI Li, Xuelong/0000-0002-0019-4197
FU National Basic Research Program of China (973 Program) [2013CB336500];
   National Nature Science Foundation of China [61222207, 61233011]
FX This work is supported by the National Basic Research Program of China
   (973 Program) (Grant 2013CB336500) and National Nature Science
   Foundation of China (Grant No: 61222207,61233011).
CR [Anonymous], 2002, THESIS STANFORD U
   [Anonymous], 1997, ACM T MATH SOFT TOMS, DOI DOI 10.1145/279232
   [Anonymous], INT C MACH LEARN
   [Anonymous], P 15 INT C ART INT S
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2005, MATRIX PRECONDITIONI
   [Anonymous], ARXIV14045692
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Bennett J., 2007, P KDD CUP WORKSHOP, P35
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343
   Candès EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candès EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Cremonesi P., 2010, P 4 ACM C REC SYST, P39, DOI [DOI 10.1145/1864708.1864721, 10.1145/1864708.1864721]
   Hsieh CJ, 2014, PR MACH LEARN RES, V32
   Hu Y, 2012, PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON INNOVATION AND MANAGEMENT, P298
   Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Ji H, 2010, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2010.5539849
   Keshavan RH, 2009, IEEE INT SYMP INFO, P324, DOI 10.1109/ISIT.2009.5205567
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Liu Ji, 2014, P INT C MACHINE LEAR
   Lu CY, 2014, PROC CVPR IEEE, P4130, DOI 10.1109/CVPR.2014.526
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Miller BM, 2003, IFAC WORK S, P263, DOI 10.1145/604045.604094
   Mohan K., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P653, DOI 10.1109/ALLERTON.2010.5706969
   Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865
   Shao L, 2014, IEEE T CYBERNETICS, V44, P1001, DOI 10.1109/TCYB.2013.2278548
   Srebro N, 2005, LECT NOTES COMPUT SC, V3559, P545, DOI 10.1007/11503415_37
   Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766
   Toh KC, 2010, PAC J OPTIM, V6, P615
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Tutuncu RH., 2001, SDPT3 A MATLAB SOFTW
   Wang S., 2013, Int. Jt. Conf. Artif. Intell, P1764
   Wang Z, 2014, IEEE INFOCOM SER, P91, DOI 10.1109/INFOCOM.2014.6847928
   Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892
   Zhang DB, 2012, PROC CVPR IEEE, P2192, DOI 10.1109/CVPR.2012.6247927
   Zhang T., 2009, Advances in Neural Information Processing Systems, V21, P1921
   Zhang X., 2012, ADV NEURAL INFORM PR, P2906
   Zhang Yi., 2007, P 30 ANN INT ACM SIG, P47, DOI [10.1145/1277741.1277752, DOI 10.1145/1277741.1277752]
   Zhuang JF, 2015, ACM T INTEL SYST TEC, V5, DOI 10.1145/2668110
NR 47
TC 8
Z9 8
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2016
VL 12
IS 3
AR 43
DI 10.1145/2903716
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YO
UT WOS:000379425400009
DA 2024-07-18
ER

PT J
AU Tyson, G
   Elkhatib, Y
   Sastry, N
   Uhlig, S
AF Tyson, Gareth
   Elkhatib, Yehia
   Sastry, Nishanth
   Uhlig, Steve
TI Measurements and Analysis of a Major Adult Video Portal
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adult video content; Porn 2.0; media streaming; user behaviour
AB Today, the Internet is a large multimedia delivery infrastructure, with websites such as YouTube appearing at the top of most measurement studies. However, most traffic studies have ignored an important domain: adult multimedia distribution. Whereas, traditionally, such services were provided primarily via bespoke websites, recently these have converged towards what is known as "Porn 2.0". These services allow users to upload, view, rate, and comment on videos for free (much like YouTube). Despite their scale, we still lack even a basic understanding of their operation. This article addresses this gap by performing a large-scale study of one of the most popular Porn 2.0 websites: YouPorn. Our measurements reveal a global delivery infrastructure that we have repeatedly crawled to collect statistics (on 183k videos). We use this data to characterise the corpus, as well as to inspect popularity trends and how they relate to other features, for example, categories and ratings. To explore our discoveries further, we use a small-scale user study, highlighting key system implications.
C1 [Tyson, Gareth; Uhlig, Steve] Queen Mary Univ London, Peter Landin Bldg, London E1 4NS, England.
   [Elkhatib, Yehia] Univ Lancaster, Infolab21, Lancaster LA1 4WA, England.
   [Sastry, Nishanth] Kings Coll London, Dept Informat, London WC2R 2LS, England.
C3 University of London; Queen Mary University London; Lancaster
   University; University of London; King's College London
RP Tyson, G (corresponding author), Queen Mary Univ London, Peter Landin Bldg, London E1 4NS, England.
RI Uhlig, Steve/P-3202-2019; Elkhatib, Yehia/G-9800-2013
OI Uhlig, Steve/0000-0001-6251-6836; Elkhatib, Yehia/0000-0003-4639-436X;
   Sastry, Nishanth/0000-0002-4053-0386
FU EPSRC [EP/K024914/1] Funding Source: UKRI
CR Abrahamsson H., 2012, P 2012 INT MEAS C IM, P199
   [Anonymous], 2011, A billion wicked thoughts: What the worlds largest experiment reveals about human desire
   [Anonymous], 2013, P INT MEAS C
   [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI DOI 10.1145/2433396.2433443
   [Anonymous], P 8 ACM SIGCOMM C IN
   [Anonymous], 2002, HPL2002260
   [Anonymous], 2010, P WORKSH EC INF SEC
   [Anonymous], 2016, CISC VIS NETW IND GL
   Anthony Sebastian., 2012, Just How Big Are Porn Sites?
   Attwood Feona., 2010, Porn. Com: Making Sense of Online Pornography, V48
   Borghol Y., 2012, ACM SIGKDD International Knowledge Discovery and Data Mining, P1186, DOI [DOI 10.1145/2339530.2339717, 10.1145/2339530.2339717]
   Brampton A, 2009, MULTIMEDIA SYST, V15, P3, DOI 10.1007/s00530-008-0126-0
   Carroll JS, 2008, J ADOLESCENT RES, V23, P6, DOI 10.1177/0743558407306348
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   Chatzopoulou G., 2010, P IEEE INFOCOM, DOI [DOI 10.1109/INFCOMW.2010.5466701, 10.1109/INFCOMW.2010.5466701]
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Cooper A., 1998, CYBERPSYCHOL BEHAV, V1, P187, DOI [DOI 10.1089/CPB.1998.1.187, 10.1089/cpb.1998.1.187]
   Coopersmith J., 2006, HIST TECHNOL, V22, P1, DOI DOI 10.1080/07341510500508610
   Crane R, 2008, P NATL ACAD SCI USA, V105, P15649, DOI 10.1073/pnas.0803685105
   Cunningham S.J., 2008, Proceedings of the 8th ACM/IEEE-CSjoint conference on Digital libraries, P201
   Daneback K, 2013, SEX HEALTH, V10, P26, DOI 10.1071/SH11023
   Dines Gail., 2010, Pornland: How Porn has Hijacked our Sexuality
   Elkhatib Y, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P965, DOI 10.1145/2647868.2654980
   Gareth Tyson, 2013, P 2013 C INT MEAS C
   Guo L, 2008, PODC'08: PROCEEDINGS OF THE 27TH ANNUAL ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P283, DOI 10.1145/1400751.1400789
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Hu WM, 2007, IEEE T PATTERN ANAL, V29, P1019, DOI 10.1109/TPAMI.2007.1133
   Hurley Ryan, 2013, P WORLD WID WEB C
   Kupka Anna, 2010, YOUTUBE REACHES 4 BI
   Lange PatriciaG., 2007, ANN C SOC APPL ANTHR
   Liikkanen LA, 2015, COMPUT HUM BEHAV, V50, P108, DOI 10.1016/j.chb.2015.01.067
   Mehta MD, 1997, INFORM SOC, V13, P153, DOI 10.1080/019722497129179
   Nencioni Gianfranco, 2015, IEEE ACM T NETWORKIN
   Sastry Nishanth, 2012, P C WEBL SOC MED
   Schuhmacher M., 2013, Search and Exploration of X-rated Information: WSDM'13 Workshop Proceedings, P27
   Schulze Hendrik., 2009, Internet Study 2007
   Siersdorfer S., 2010, Proceedings of the 19th International Conference on World Wide Web, WWW'10, P891, DOI DOI 10.1145/1772690.1772781
   Suler J, 2004, CYBERPSYCHOL BEHAV, V7, P321, DOI 10.1089/1094931041291295
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Thelwall M, 2012, J AM SOC INF SCI TEC, V63, P616, DOI 10.1002/asi.21679
   Tyson G., 2015, P 9 INT AAAI C WEB S
   Wattenhofer Mirjam, 2010, P WORKSH SOC INT AN
   Zink M, 2009, COMPUT NETW, V53, P501, DOI 10.1016/j.comnet.2008.09.022
NR 44
TC 11
Z9 12
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2016
VL 12
IS 2
AR 35
DI 10.1145/2854003
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DJ0PK
UT WOS:000373906200009
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Ahn, J
   Williamson, J
   Gartrell, M
   Han, R
   Lv, Q
   Mishra, S
AF Ahn, Junho
   Williamson, James
   Gartrell, Mike
   Han, Richard
   Lv, Qin
   Mishra, Shivakant
TI Supporting Healthy Grocery Shopping via Mobile Augmented Reality
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Augmented reality; mobile health; recommendation; grocery shopping;
   nutrition
ID SYSTEMS
AB Augmented reality (AR) applications have recently become popular on modern smartphones. We explore the effectiveness of this mobile AR technology in the context of grocery shopping, in particular as a means to assist shoppers in making healthier decisions as they decide which grocery products to buy. We construct an AR-assisted mobile grocery-shopping application that makes real-time, customized recommendations of healthy products to users and also highlights products to avoid for various types of health concerns, such as allergies to milk or nut products, low-sodium or low-fat diets, and general caloric intake. We have implemented a prototype of this AR-assisted mobile grocery shopping application and evaluated its effectiveness in grocery store aisles. Our application's evaluation with typical grocery shoppers demonstrates that AR overlay tagging of products reduces the search time to find healthy food items, and that coloring the tags helps to improve the user's ability to quickly and easily identify recommended products, as well as products to avoid. We have evaluated our application's functionality by analyzing the data we collected from 15 in-person actual grocery-shopping subjects and 104 online application survey participants.
C1 [Ahn, Junho; Williamson, James; Gartrell, Mike; Han, Richard; Lv, Qin; Mishra, Shivakant] Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.
C3 University of Colorado System; University of Colorado Boulder
RP Mishra, S (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.
EM mishras@cs.colorado.edu
CR Agapito G, 2014, 2014 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING & SIMULATION (HPCS), P616, DOI 10.1109/HPCSim.2014.6903744
   Ahn Junho, 2012, ARFUSION INDOOR MOBI, P12
   Ahn Junho, 2011, P IEEE AS PAC SERV C
   Anderson ES, 2001, ANN BEHAV MED, V23, P88, DOI 10.1207/S15324796ABM2302_3
   [Anonymous], P 4 WORKSH POS NAV C
   [Anonymous], REAL FULL POT HLTH I
   Barton Amy, 2006, CARDIOVASCULAR RISK
   Bobadilla J, 2013, KNOWL-BASED SYST, V46, P109, DOI 10.1016/j.knosys.2013.03.012
   Botella C, 2011, COMPUT HUM BEHAV, V27, P217, DOI 10.1016/j.chb.2010.07.043
   DanKam, 2010, DANKAM AR APPL COL B
   DietGuideline, 2010, 2010 DIET GUID AM
   Freyne J, 2010, IUI 2010, P321
   Fuchs C, 2011, PERVASIVE MOB COMPUT, V7, P1, DOI 10.1016/j.pmcj.2010.07.001
   Ganapathy S, 2011, P IEEE VIRT REAL ANN, P207, DOI 10.1109/VR.2011.5759471
   Golfscape, 2014, GOLFSC GPS AR RANG F
   Gorgu, 2010, Proc. of the 8th Intl. Conf. on Advances in Mobile Computing and Multimedia, P173
   Gu YY, 2009, IEEE COMMUN SURV TUT, V11, P13, DOI 10.1109/SURV.2009.090103
   Hervás R, 2011, LECT NOTES COMPUT SC, V6693, P17, DOI 10.1007/978-3-642-21303-8_3
   Hong Hwajung., 2010, CHI EA '10, P3577
   IQEngines, 2013, IQENGINES IM REC VIS
   Jia-Kuan Lin, 2011, 2011 IEEE 13th International Conference on e-Health Networking, Applications and Services (Healthcom 2011), P197, DOI 10.1109/HEALTH.2011.6026743
   Kalnikaite Vaiva, 2011, P UBICOMP 11
   Katz David, 2009, AM J HLTH PROMOT NOV
   Ladstaetter Stefan., 2010, MobileHCI, P395
   Lobstein T., 2009, PUBLIC HLTH NUTR
   Mankoff J., 2002, UbiComp 2002: Ubiquitous Computing. 4th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2498), P371
   Mechanical Turk, 2014, AM MECH TURK SURV SE
   Mhurchu CN, 2007, PUBLIC HEALTH NUTR, V10, P608, DOI 10.1017/S136898000735249X
   Mulrooney Barry., 2006, CHI '06 extended abstracts on Human factors in computing systems, CHI EA '06, P1855, DOI 10.1145/1125451.1125802
   Oh Y, 2010, J SUPERCOMPUT, V54, P61, DOI 10.1007/s11227-009-0314-5
   Phanich M., 2010, Information Science and Applications (ICISA), 2010 International Conference on, P1, DOI [DOI 10.1109/ICISA.2010.5480416, 10.1109/ICISA.2010.5480416.]
   Winett RA, 1997, COMPUT HUM BEHAV, V13, P371, DOI 10.1016/S0747-5632(97)00015-0
   WordLens, 2014, WORDLENS AUGM REAL L
NR 33
TC 55
Z9 61
U1 3
U2 52
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2015
VL 12
IS 1
SU S
SI SI
AR 16
DI 10.1145/2808207
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CU6PH
UT WOS:000363654100006
DA 2024-07-18
ER

PT J
AU Hamam, A
   El Saddik, A
   Alja'Am, J
AF Hamam, Abdelwahab
   El Saddik, Abdulmotaleb
   Alja'Am, Jihad
TI A Quality of Experience Model for Haptic Virtual Environments
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Haptic I/O; quality of experience; QoE; virtual reality;
   haptic multimedia application; fuzzy logic evaluation
ID FUZZY-LOGIC SYSTEM; PERFORMANCE
AB Haptic-based Virtual Reality (VR) applications have many merits. What is still obscure, from the designer's perspective of these applications, is the experience the users will undergo when they use the VR system. Quality of Experience (QoE) is an evaluation metric from the user's perspective that unfortunately has received limited attention from the research community. Assessing the QoE of VR applications reflects the amount of overall satisfaction and benefits gained from the application in addition to laying the foundation for ideal user-centric design in the future. In this article, we propose a taxonomy for the evaluation of QoE for multimedia applications and in particular VR applications. We model this taxonomy using a Fuzzy logic Inference System (FIS) to quantitatively measure the QoE of haptic virtual environments. We build and test our FIS by conducting a users' study analysis to evaluate the QoE of a haptic game application. Our results demonstrate that the proposed FIS model reflects the user's estimation of the application's quality significantly with low error and hence is suited for QoE evaluation.
C1 [Hamam, Abdelwahab; El Saddik, Abdulmotaleb] Univ Ottawa, Sch Elect Engn & Comp Sci, Multimedia Commun Res Lab, Ottawa, ON K1N 6N5, Canada.
   [Alja'Am, Jihad] Univ Qatar, Dept Comp Sci & Engn, Doha, Qatar.
C3 University of Ottawa; Qatar University
RP Hamam, A (corresponding author), Univ Ottawa, Sch Elect Engn & Comp Sci, Multimedia Commun Res Lab, Ottawa, ON K1N 6N5, Canada.
EM elsaddik@uottawa.ca
RI /D-4159-2009
OI /0000-0002-7690-8547
FU Qatar National Research Fund [NPRP 09-052-5-003]
FX This publication was made possible by a grant from the Qatar National
   Research Fund under its award NPRP 09-052-5-003. Its contents are solely
   the responsibility of the authors and do not necessarily represent the
   official views of the Qatar National Research Fund.
CR Al Osman H., 2008, P 16 S HAPT INT VIRT
   Alamri A, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE FOR MEASUREMENT SYSTEMS AND APPLICATIONS, P14, DOI 10.1109/CIMSA.2008.4595824
   [Anonymous], 2012, 2012 IEEE Aerospace Conference, DOI [10.1109/AERO.2012.6187425, DOI 10.1109/AERO.2012.6187425]
   [Anonymous], 2007, INT C ADV COMP ENT T
   Basdogan C., 2000, ACM Transactions on Computer-Human Interaction, V7, P443, DOI 10.1145/365058.365082
   Behr KM, 2005, PRESENCE-TELEOP VIRT, V14, P668, DOI 10.1162/105474605775196535
   Bhattacharya A., 2011, P 19 ACM INT C MULT, P929, DOI DOI 10.1145/2072298.2071905
   Black K., 2004, BUSINESS STAT CONT D
   Cribbie RA, 2004, J CLIN PSYCHOL, V60, P1, DOI 10.1002/jclp.10217
   Danieau F., 2012, 2012 IEEE Haptics Symposium (HAPTICS), P541, DOI 10.1109/HAPTIC.2012.6183844
   De Marez L., 2007, OBSERVATORIO, V1, P1
   DeVleeschouwer C, 1997, SIGNAL PROCESS-IMAGE, V10, P115, DOI 10.1016/S0923-5965(97)00021-0
   Ebrahimi T., 2009, P 17 ACM INT C MULTI, P3
   El Saddik A, 2007, IEEE INSTRU MEAS MAG, V10, P10, DOI 10.1109/MIM.2007.339540
   Gaggioli A, 2003, EMERG COMMUNICAT, V5, P121
   Guerraz A., 2003, P EUR, P137
   Gutiérrez F, 2007, STUD HEALTH TECHNOL, V125, P155
   Hajshirmohammadi I, 2007, PRESENCE-TELEOP VIRT, V16, P603, DOI 10.1162/pres.16.6.603
   Hamam A., 2008, HAS '08: Proceedings of the 2008 Ambi-Sys Workshop on Haptic User Interfaces in Ambient Media Systems, P1
   Hamam A, 2008, LECT NOTES COMPUT SC, V5024, P129, DOI 10.1007/978-3-540-69057-3_14
   Hamam A, 2013, IEEE T INSTRUM MEAS, V62, P3315, DOI 10.1109/TIM.2013.2272859
   Hamam A, 2013, MULTIMED TOOLS APPL, V67, P455, DOI 10.1007/s11042-012-0990-7
   Ida Y., 2010, Network and Systems Support for Games (NetGames), 2010 9th Annual Workshop on, P1
   Iwata K., 2010, COMPUT ENTERTAIN, V8, P12
   Jain R, 2004, IEEE MULTIMEDIA, V11, P96, DOI 10.1109/MMUL.2004.1261114
   Jones LA, 2008, IEEE T HAPTICS, V1, P53, DOI 10.1109/ToH.2008.2
   Karam M, 2009, IEEE T HAPTICS, V2, P160, DOI 10.1109/ToH.2009.32
   Kusunose Y., 2010, Network and Systems Support for Games (NetGames), 2010 9th Annual Workshop on, P1
   Mamdani EH, 1999, INT J HUM-COMPUT ST, V51, P135, DOI 10.1006/ijhc.1973.0303
   MATLAB Documentation, 2001, MATLAB DOC FUZZ LOG
   Miller V G, 1993, West J Nurs Res, V15, P595, DOI 10.1177/019394599301500506
   Nacke L., 2008, P 2008 C FUT PLAY, P81
   Nacke L. E., 2010, LOADING, V3, P5
   Park K. S., 1999, P IEEE C VIRT REAL
   Preston CC, 2000, ACTA PSYCHOL, V104, P1, DOI 10.1016/S0001-6918(99)00050-5
   Ramsey A., 1997, P INT WORKSH MOT SIC, P26
   Roid G. H., 2004, P INT TEST US C AUST
   Rowe LA, 2005, ACM T MULTIM COMPUT, V1, P3, DOI 10.1145/1047936.1047938
   SEROUSSI R, 1989, IEEE T BIO-MED ENG, V36, P284, DOI 10.1109/10.16476
   Srinivasan MA, 1997, COMPUT GRAPH-UK, V21, P393, DOI 10.1016/S0097-8493(97)00030-7
   Streiner DL, 2003, CAN J PSYCHIAT, V48, P756, DOI 10.1177/070674370304801108
   Strickland D, 1997, COMMUN ACM, V40, P34, DOI 10.1145/257874.257881
   Sweetser P, 2005, COMPUTERS ENTERTAINM, V3, P3, DOI [10.1145/1077246.1077253, DOI 10.1145/1077246.1077253]
   Tatematsu A., 2010, Communications Quality and Reliability (CQR), 2010 IEEE International Workshop Technical Committee on, P1, DOI [DOI 10.1109/CQR.2010.5619913, 10.1109/CQR.2010.5619913]
   Wang CH, 2010, IEEE INT SYMP INFO, P2028, DOI 10.1109/ISIT.2010.5513368
   WANG LX, 1992, IEEE T SYST MAN CYB, V22, P1414, DOI 10.1109/21.199466
   Whalen TE, 2003, VECIMS'03: 2003 IEEE INTERNATIONAL SYMPOSIUM ON VIRTUAL ENVIRONMENTS, HUMAN-COMPUTER INTERFACES AND MEASUREMENT SYSTEMS, P8
   Wu W., 2009, MM 09, P481, DOI DOI 10.1145/1631272.1631338
NR 48
TC 19
Z9 33
U1 0
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2014
VL 10
IS 3
AR 28
DI 10.1145/2540991
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA AF3AM
UT WOS:000334583800006
DA 2024-07-18
ER

PT J
AU Yang, Y
   Ivrissimtzis, I
AF Yang, Ying
   Ivrissimtzis, Ioannis
TI Mesh Discriminative Features for 3D Steganalysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Triangle meshes; discriminative feature vector; data
   embedding; steganalysis
ID WATERMARKING; STEGANOGRAPHY; COMPRESSION
AB We propose a steganalytic algorithm for triangle meshes, based on the supervised training of a classifier by discriminative feature vectors. After a normalization step, the triangle mesh is calibrated by one step of Laplacian smoothing and then a feature vector is computed, encoding geometric information corresponding to vertices, edges and faces. For a given steganographic or watermarking algorithm, we create a training set containing unmarked meshes and meshes marked by that algorithm, and train a classifier using Quadratic Discriminant Analysis. The performance of the proposed method was evaluated on six well-known watermarking/steganographic schemes with satisfactory accuracy rates.
C1 [Yang, Ying] Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA.
   [Ivrissimtzis, Ioannis] Univ Durham, Sch Engn & Comp Sci, Durham DH1 3HP, England.
C3 Yale University; Durham University
RP Yang, Y (corresponding author), Yale Univ, Dept Comp Sci, POB 2158, New Haven, CT 06520 USA.
EM ying.yang.yy368@yale.edu; ioannis.ivrissimtzis@durham.ac.uk
OI Ivrissimtzis, Ioannis/0000-0002-3380-1889
CR [Anonymous], 2001, INF HID 4 INT WORKSH, DOI 10.1007/3-540-
   Bogomjakov A, 2008, COMPUT GRAPH FORUM, V27, P637, DOI 10.1111/j.1467-8659.2008.01161.x
   Bollobas Bela, 2013, Modern Graph Theory, V184
   Cayre F, 2003, IEEE T SIGNAL PROCES, V51, P939, DOI 10.1109/TSP.2003.809380
   Chao MW, 2009, IEEE T VIS COMPUT GR, V15, P274, DOI 10.1109/TVCG.2008.94
   Chen XB, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531379
   Cho JW, 2007, IEEE T SIGNAL PROCES, V55, P142, DOI 10.1109/TSP.2006.882111
   Farid H, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P905
   FREEDMAN D, 1981, Z WAHRSCHEINLICHKEIT, V57, P453, DOI 10.1007/BF01025868
   Fridrich J, 2003, LECT NOTES COMPUT SC, V2578, P310
   Fridrich Jessica., 2007, STATISTICALLY UNDETE, P3
   Golub G.H., 1996, Matrix computations, V3
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Harmsen JJ, 2003, PROC SPIE, V5020, P131, DOI 10.1117/12.476813
   Higuchi Y, 2001, J GRAPH THEOR, V38, P220, DOI 10.1002/jgt.10004
   Hoppe H., 1996, Proc. ACM SIGGRAPH, P99
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Ker AD, 2005, IEEE SIGNAL PROC LET, V12, P441, DOI 10.1109/LSP.2005.847889
   Kim K, 2010, IEEE T INF FOREN SEC, V5, P721, DOI 10.1109/TIFS.2010.2068546
   Kodovsky J, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P63
   Konstantinides JM, 2009, IEEE T MULTIMEDIA, V11, P23, DOI 10.1109/TMM.2008.2008913
   Krzanowski W., 2000, PRINCIPLES MULTIVARI, V23
   Lin HYS, 2005, IEEE T MULTIMEDIA, V7, P997, DOI 10.1109/TMM.2005.858412
   Liu Y, 2012, IEEE T INF FOREN SEC, V7, P1459, DOI 10.1109/TIFS.2012.2204251
   Liu Y, 2010, COMPUT GRAPH FORUM, V29, P2039, DOI 10.1111/j.1467-8659.2010.01790.x
   Liu Y, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P43
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Luo M, 2011, IEEE T IMAGE PROCESS, V20, P2813, DOI 10.1109/TIP.2011.2142004
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Praun E, 1999, COMP GRAPH, P49, DOI 10.1145/311535.311540
   Tu SC, 2010, VISUAL COMPUT, V26, P1177, DOI 10.1007/s00371-009-0398-1
   Uccheddu F., 2004, PROC ACM MULTIMEDIA, P143
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Wang CM, 2005, COMPUT GRAPH FORUM, V24, P591, DOI 10.1111/j.1467-8659.2005.00884.x
   Wang K, 2008, IEEE T MULTIMEDIA, V10, P1513, DOI 10.1109/TMM.2008.2007350
   Wang Y, 2007, IEEE T INF FOREN SEC, V2, P31, DOI 10.1109/TIFS.2006.890517
   Xuan GR, 2005, LECT NOTES COMPUT SC, V3727, P262
   Yang Y, 2010, COMPUT GRAPH FORUM, V29, P1585, DOI 10.1111/j.1467-8659.2010.01767.x
   Yang Y, 2013, IEEE T VIS COMPUT GR, V19, P45, DOI 10.1109/TVCG.2012.106
   Yeo B. L., 1999, COMPUT GRAPHICS APPL, V19, P36
   Yin KK, 2001, COMPUT GRAPH-UK, V25, P409, DOI 10.1016/S0097-8493(01)00065-6
NR 41
TC 20
Z9 21
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2014
VL 10
IS 3
AR 27
DI 10.1145/2535555
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF3AM
UT WOS:000334583800005
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Song, M
   Lee, Y
   Kim, E
AF Song, Minseok
   Lee, Yeongju
   Kim, Euiseok
TI Saving Disk Energy in Video Servers by Combining Caching and Prefetching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Experimentation; Multimedia Storage Systems;
   Low-Power Systems; Power Management
ID CONTINUOUS MEDIA; STRATEGY
AB Maintenance and upgrades to the significant storage infrastructure in a video server often create a heterogenous disk array. We show how to manage the energy consumption of such an array by combining caching and prefetching techniques. We first examine how seek operations affect disk energy consumption, and then analyze the relationship between the amount of prefetched data and the number of seeks, and the effect of the size of the prefetching buffer on energy consumption. Based on this, we propose a new data prefetching scheme in which the amount of data prefetched for each video stream is dynamically adjusted to allow for the bit-rates of streams and the power characteristics of different disks. We next examine the impact of caching on disk power consumption and propose a new caching scheme that prioritizes each stream based on the ratio of the amount of energy that can be saved to its cache requirement, so as to make effective use of limited caching space. We address the trade-off between caching and prefetching and propose an algorithm that dynamically divides the entire buffer space into prefetching and caching regions, with the aim of minimizing overall disk energy consumption. Experimental results show that our scheme can reduce disk energy consumption between 26% and 31%, compared to a server without prefetching and caching.
C1 [Song, Minseok; Lee, Yeongju; Kim, Euiseok] Inha Univ, Sch Comp Sci & Informat Engn, Inchon, South Korea.
C3 Inha University
RP Song, M (corresponding author), Inha Univ, Sch Comp Sci & Informat Engn, Inchon, South Korea.
EM mssong@inha.ac.kr; celesty0128@gmail.com; esuk42@naver.com
FU Industrial Strategic Technology Development Program [10041971]; Ministry
   of Knowledge Economy (MKE, Korea); IT R&D program of MKE/KEIT
   [10035243]; Inha University
FX This work was supported in part by the Industrial Strategic Technology
   Development Program (10041971, Development of Power Efficient
   High-Performance Multimedia Contents Service Technology using
   Context-Adapting Distributed Transcoding) funded by the Ministry of
   Knowledge Economy (MKE, Korea), in part by the IT R&D program of
   MKE/KEIT under grant 10035243, and in part by Inha University.
CR Agarwal A., 2012, YOUTUBE STAT 2012
   Almeida J., 2001, P SPIE MULTIMEDIA CO, P64
   [Anonymous], 1995, Algorithms for knapsack problems
   Athanasios P., 2004, P USENIX ANN TECHN C, P22
   Bostoen T, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480750
   Butt A., 2008, IEEE T COMPUT, V57, P748
   Cai L, 2006, ISLPED '06: PROCEEDINGS OF THE 2006 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN, P186, DOI 10.1109/LPE.2006.4271833
   Chang E, 1996, IEEE MULTIMEDIA, V3, P56, DOI 10.1109/93.556461
   Chang E, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P496
   Chang E., 1996, THESIS U CALIFORNIA
   Cheng-Fu Chou, 2000, Proceedings 20th IEEE International Conference on Distributed Computing Systems, P64, DOI 10.1109/ICDCS.2000.840908
   Dan A, 1996, MULTIMEDIA SYST, V4, P112, DOI 10.1007/s005300050016
   Dan A, 1997, MULTIMED TOOLS APPL, V4, P279, DOI 10.1023/A:1009637022889
   Ding JW, 2003, MULTIMED TOOLS APPL, V19, P29, DOI 10.1023/A:1021164829330
   Essary D., 2008, ACM T STORAGE, V4, P748
   Forte D., 2011, P IEEE INT C GREEN C, P1
   GE X, 2011, P 25 ANN C NEUR INF, P1
   Guo J, 2008, IEEE T CIRC SYST VID, V18, P937, DOI 10.1109/TCSVT.2008.924905
   Gurumurthi S, 2003, COMPUTER, V36, P59, DOI 10.1109/MC.2003.1250884
   Gurumurthi S., 2005, THESIS PENNSYLVANIA
   Hitachi, 2012, HIT DESKST 7K1000
   Hondroulis A, 2004, MULTIMED TOOLS APPL, V23, P203, DOI 10.1023/B:MTAP.0000031757.02159.ac
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Huang H., 2000, P ACM INT S OP SYST, P64
   IXBT Labs, 2005, HDD DIET POW CONS HE
   Joukov N, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P69, DOI 10.1145/1357010.1352600
   Kim M, 2012, IEEE T CIRC SYST VID, V22, P567, DOI 10.1109/TCSVT.2011.2170112
   Kim T., 2003, ELECTRON LETT, V21, P1555
   Liu JC, 2004, IEEE COMMUN MAG, V42, P88, DOI 10.1109/MCOM.2004.1321397
   Manzanres A, 2009, 2009 8TH IEEE INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS, P90, DOI 10.1109/NCA.2009.29
   Pinheiro E., 2006, Performance Evaluation Review, V34, P15, DOI 10.1145/1140103.1140281
   Pinheiro E., 2004, P ACM IEEE C SUP JUN, P88
   Rao A., 2009, P USENIX ANN TECHN C, P24
   Riska Alma, 2009, Performance Evaluation Review, V37, P43, DOI 10.1145/1710115.1710124
   Song M., 2007, Proceedings of the 15th international conference on Multimedia, MULTIMEDIA '07, P755, DOI DOI 10.1145/1291233.1291403
   Song M., 2013, P ACM WORKSH NETW OP, P1
   Song M, 2006, LECT NOTES COMPUT SC, V4096, P193
   Song M, 2008, IEEE INT SYM MULTIM, P196, DOI 10.1109/ISM.2008.25
   TANG W., 2003, Proceedings of the 13th international workshop on Network and operating systems support for digital audio and video, P12
   Vandebogart S., 2009, P USENIX ANN TECHN C, P24
   Wang J, 2008, IEEE T COMPUT, V57, P359, DOI 10.1109/TC.2007.70821
   Wang J, 2008, IEEE T COMPUT, V57, P733, DOI 10.1109/TC.2008.43
   WEDDLE C., 2007, ACM T STORAGE, V3, P3
   Xie T, 2008, IEEE T COMPUT, V57, P748, DOI 10.1109/TC.2008.27
   Yuan H, 2012, INT C PAR DISTRIB SY, P644, DOI 10.1109/ICPADS.2012.92
   Zhu Q, 2005, P 20 ACM S OP SYST P, P177, DOI DOI 10.1145/1095809.1095828
   Zhu QB, 2005, IEEE T COMPUT, V54, P587
   Zimmermann R, 2004, IEEE T MULTIMEDIA, V6, P886, DOI 10.1109/TMM.2004.837231
NR 48
TC 5
Z9 6
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2014
VL 10
IS 1
SU S
SI SI
AR 15
DI 10.1145/2537856
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA2EF
UT WOS:000330907200007
DA 2024-07-18
ER

PT J
AU Abrams, A
   Pless, R
AF Abrams, Austin
   Pless, Robert
TI Web-Accessible Geographic Integration and Calibration of Webcams
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Camera calibration; geospatial Web services; participatory
   GIS; pan-tilt-zoom cameras; social computing; voluntary geographic
   information; webcams
AB A global network of webcams offers unique viewpoints from tens of thousands of locations. Understanding the geographic context of this imagery is vital in using these cameras for quantitative environmental monitoring or surveillance applications. We derive robust geo-calibration constraints that allow users to geo-register static or pan-tilt-zoom cameras by specifying a few corresponding points, and describe our Web interface suitable for novices. We discuss design decisions that support our scalable, publicly accessible Web service that allows webcam textures to be displayed live on 3D geographic models. Finally, we demonstrate several multimedia applications for geo-calibrated cameras.
C1 [Abrams, Austin; Pless, Robert] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
C3 Washington University (WUSTL)
RP Abrams, A (corresponding author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
EM abramsa@cse.wustl.edu
RI Pless, Robert B/I-4698-2013
FU National Science Foundation [IIS-0546383, EF-1065734, DEB-1053554,
   IIS-1111398]; Direct For Biological Sciences; Emerging Frontiers
   [1065734] Funding Source: National Science Foundation; Direct For
   Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems
   [1111415, 1111398] Funding Source: National Science Foundation; Division
   Of Environmental Biology; Direct For Biological Sciences [1053554]
   Funding Source: National Science Foundation
FX This work was supported in part by the National Science Foundation
   (IIS-0546383, EF-1065734, DEB-1053554, IIS-1111398).
CR Abrams A., 2010, P ACM MULT C
   Abrams A., 2010, P 1 INT C COMP GEOSP
   Aguera y Arcas B., 2010, BLAISE AGUERA ARCAS
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2009, ACM SIGSPATIAL INT C
   Gibbons PB, 2003, IEEE PERVAS COMPUT, V2, P22, DOI 10.1109/MPRV.2003.1251166
   Gliet J., 2008, WORKING C ADV VISUAL, P287
   Grosky WI, 2007, IEEE MULTIMEDIA, V14, P8, DOI 10.1109/MMUL.2007.82
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Jacobs N., 2007, P INT C COMP VIS ICC
   Jacobs N., 2009, P IEEE COMP VIS PATT
   Jacobs N., 2008, P IEEE WORKSH APPL C
   Kim K., 2009, P IEEE INT S MIX AUG
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Lalonde J.-F., 2008, P EUR C COMP VIS ECC
   Nath S., 2006, 1 WORKSHOP WORLDSENS, P3
   Sankaranarayanan K., 2008, P IEEE INT C ADV VID
   Sawhney HS, 2002, P 13 EUR WORKSH REND
   Sebe I. O., 2003, P 1 ACM INT WORKSH V
   Sunkavalli K., 2008, P IEEE C COMP VIS PA
NR 20
TC 4
Z9 6
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2013
VL 9
IS 1
AR 8
DI 10.1145/2422956.2422964
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 097EO
UT WOS:000315457000008
DA 2024-07-18
ER

PT J
AU Yang, R
   Qu, ZH
   Huang, JW
AF Yang, Rui
   Qu, Zhenhua
   Huang, Jiwu
TI Exposing MP3 Audio Forgeries Using Frame Offsets
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Algorithms; Verification; MP3 audio forgery; forgery
   detection; audio authentication
ID DIGITAL AUDIO
AB Audio recordings should be authenticated before they are used as evidence. Although audio watermarking and signature are widely applied for authentication, these two techniques require accessing the original audio before it is published. Passive authentication is necessary for digital audio, especially for the most popular audio format: MP3. In this article, we propose a passive approach to detect forgeries of MP3 audio. During the process of MP3 encoding the audio samples are divided into frames, and thus each frame has its own frame offset after encoding. Forgeries lead to the breaking of framing grids. So the frame offset is a good indication for locating forgeries, and it can be retrieved by the identification of the quantization characteristic. In this way, the doctored positions can be automatically located. Experimental results demonstrate that the proposed approach is effective in detecting some common forgeries, such as deletion, insertion, substitution, and splicing. Even when the bit rate is as low as 32 kbps, the detection rate is above 99%.
C1 [Yang, Rui; Qu, Zhenhua; Huang, Jiwu] Sun Yat Sen Univ, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Huang, JW (corresponding author), State Key Lab Informat Secur, Beijing 100190, Peoples R China.
EM isshjw@mail.sysu.edu.cn
RI huang, jw/KVY-9917-2024; Qu, Zhenhua/GLV-2397-2022
OI Qu, Zhenhua/0000-0002-8089-2795
FU 973 Program in China [2011CB302204]; NSFC [U1135001, 61202497]
FX The work was supported in part by 973 Program (2011CB302204) in China
   and NSFC (U1135001, 61202497).
CR [Anonymous], 2003, P DIG FOR RES WORKSH
   [Anonymous], 2012, MP3 ENC
   BOEHM R., 2004, P 6 ACM MULT SEC WOR
   Farid H, 1999, AIM1657 MIT AI
   Fu D., 2007, P SPIE C SEC STEG WA
   Grigoras C, 2005, INT J SPEECH LANG LA, V12, P63, DOI 10.1558/sll.2005.12.1.63
   HERRE J., 2000, P 109 AES CONV
   HERRE J., 2002, P 112 AES CONV
   *ISO, 1992, 111723 ISOIEC
   KRAETZER C., 2007, P 9 ACM MULT SEC WOR
   Painter T, 2000, P IEEE, V88, P451, DOI 10.1109/5.842996
   Popescu A.C., 2004, P 6 INT WORKSH INF H
   Qu Z., 2008, P IEEE INT C AC SPEE, P4244
   WANG Y., 2003, AES J, V51, P51
   WANG Y., 2000, P 16 IFIP WORLD COMP
   YANG R., 2008, P 10 ACM MULT SEC WO
NR 16
TC 12
Z9 13
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 2
SU S
AR 35
DI 10.1145/2344436.2344441
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 011KE
UT WOS:000309162800005
DA 2024-07-18
ER

PT J
AU Mandel, MI
   Pascanu, R
   Eck, D
   Bengio, Y
   Aiello, LM
   Schifanella, R
   Menczer, F
AF Mandel, Michael I.
   Pascanu, Razvan
   Eck, Douglas
   Bengio, Yoshua
   Aiello, Luca M.
   Schifanella, Rossano
   Menczer, Filippo
TI Contextual Tag Inference
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Performance; Autotagging; clips; context; music;
   smoothing; tags
AB This article examines the use of two kinds of context to improve the results of content-based music taggers: the relationships between tags and between the clips of songs that are tagged. We show that users agree more on tags applied to clips temporally "closer" to one another; that conditional restricted Boltzmann machine models of tags can more accurately predict related tags when they take context into account; and that when training data is "smoothed" using context, support vector machines can better rank these clips according to the original, unsmoothed tags and do this more accurately than three standard multi-label classifiers.
C1 [Mandel, Michael I.] Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ H3C 3J7, Canada.
   [Aiello, Luca M.; Schifanella, Rossano] Univ Turin, I-10124 Turin, Italy.
   [Menczer, Filippo] Indiana Univ, Bloomington, IN 47405 USA.
C3 Universite de Montreal; University of Turin; Indiana University System;
   Indiana University Bloomington
RP Mandel, MI (corresponding author), Univ Montreal, Dept Comp Sci & Operat Res, CP 6128,Succ Ctr Ville, Montreal, PQ H3C 3J7, Canada.
EM mandelm@iro.umontreal.ca
RI Michael Mandel, Professor/AAK-1239-2021; Schifanella,
   Rossano/C-6682-2011; Aiello, Luca Maria/ABB-2507-2021
OI Michael Mandel, Professor/0000-0003-2073-5357; Aiello, Luca
   Maria/0000-0002-0654-2527; Menczer, Filippo/0000-0003-4384-2876;
   SCHIFANELLA, ROSSANO/0000-0002-3745-5792
FU project Social Integration of Semantic Annotations for Web Applications;
   National Science Foundation [IIS-0811994]
FX This work was partly supported by the project Social Integration of
   Semantic Annotations for Web Applications, funded by National Science
   Foundation award IIS-0811994.
CR [Anonymous], P C ADV NEUR INF PRO
   [Anonymous], P INT C AC SPEECH SI
   [Anonymous], P C ADV NEUR INF PRO
   [Anonymous], P INT S MUS INF RETR
   [Anonymous], 2010, P 11 INT SOC MUS INF
   AUCOUTURIER JJ, 2007, P 8 ISMIR C VIENN AU, P425
   Bertin-Mahieux T, 2008, J NEW MUSIC RES, V37, P115, DOI 10.1080/09298210802479250
   BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Chen L, 2010, PROC CVPR IEEE, P3440, DOI 10.1109/CVPR.2010.5539988
   Eck D., 2008, Advances in neural information processing systems, P385
   Han YH, 2010, AAAI CONF ARTIF INTE, P469
   Hand DJ, 2009, MACH LEARN, V77, P103, DOI 10.1007/s10994-009-5119-5
   Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5
   Kang F., 2006, CVPR, V2, P1719
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536
   Lee JinHa., 2010, ISMIR, P183
   Mandel M., 2011, Autotagging music with conditional restricted Boltzmann machines
   Mandel M, 2008, J NEW MUSIC RES, V37, P151, DOI 10.1080/09298210802479300
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Markines B., 2009, P 18 INT C WORLD WID, P641
   Miotto Riccardo., 2010, INT SOC MUSIC INFORM, P297
   Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986
   Rasiwasia N, 2009, PROC CVPR IEEE, P1889, DOI 10.1109/CVPRW.2009.5206826
   Salakhutdinov R., 2007, P 24 INT C MACH LEAR, P791, DOI DOI 10.1145/1273496.1273596
   Schifanella R., 2010, Proceedings of the 3rd ACM Int'l Conf. on Web Search and Data Mining, P271
   Smolensky P, 1986, P 1986 PARALLEL DIST
   Snow R, 2008, P 2008 C EMP METH NA, P254, DOI DOI 10.3115/1613715.1613751
   SOROKIN A., 2008, P WORKSHOP INTERNET, P1
   Taylor GW, 2007, ADV NEURAL INFORM PR, P1345, DOI DOI 10.7551/MITPRESS/7503.003.0173
   Tingle D., 2010, P ISMIR, P55
   Tsoumakas G, 2007, LECT NOTES ARTIF INT, V4701, P406
   Tsoumakas G, 2011, J MACH LEARN RES, V12, P2411
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   Whitehill J., 2009, Advances in Neural Information Processing Systems, P2035
   Whitman B, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P153
   Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
NR 40
TC 8
Z9 9
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 32
DI 10.1145/2037676.2037689
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 857NR
UT WOS:000297725800013
DA 2024-07-18
ER

PT J
AU Milani, S
   Calvagno, G
AF Milani, Simone
   Calvagno, Giancarlo
TI A Cognitive Approach for Effective Coding and Transmission of 3D Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Measurements; Performance; Cross-layer optimization;
   source coding; joint source-channel coding; 3D video; cognitive source
   coding
AB Future multimedia applications will rely on the transmission of 3D video contents within heterogeneous fruition scenarios, and as a matter of fact, the reliable delivery of 3D video signals proves to be a crucial issue in such communications. To this purpose, multimedia communication experts have been designing cross-layer strategies to improve the quality of the perceived 3D experience. This article presents a new cross-layer strategy, called Cognitive Source Coding (CSC), that defines a new 3D video system able to identify the different elements of the 3D scene and choose the most appropriate coding strategy.
C1 [Milani, Simone; Calvagno, Giancarlo] Univ Padua, Dept Informat Engn, I-35131 Padua, Italy.
C3 University of Padua
RP Milani, S (corresponding author), Univ Padua, Dept Informat Engn, Via Gradenigo 6-B, I-35131 Padua, Italy.
EM simone.milani@dei.unipd.it; calvagno@dei.unipd.it
OI Milani, Simone/0000-0001-8266-5839
CR Aaron A, 2002, CONF REC ASILOMAR C, P240
   ADIKARI ABB, 2008, P IEEE FUT MULT NETW, P439
   Aksay A., 2006, P EUR SIGN PROC C EU
   Alregib G, 2005, ACM T GRAPHIC, V24, P182, DOI 10.1145/1061347.1061349
   [Anonymous], P MIDW S CIRC SYST A
   [Anonymous], 1994, GEN COD MOV PICT A 2
   [Anonymous], 1995, VID COD LOW BITR COM
   Artigas X, 2007, P PICT COD S PCS
   Balter R, 2006, IEEE T MULTIMEDIA, V8, P1147, DOI 10.1109/TMM.2006.879873
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Boughorbel S, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P113, DOI 10.1109/ICME.2005.1521373
   Bremond R., 2010, P MED RET WORKSH CON
   CRAVE O, 2008, P EUR SIGN PROC C EU
   FAN Y, 2003, P DCC 2008 SNOWB UT, P515
   Farber N., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P550, DOI 10.1109/ICIP.1999.822956
   FEHN C, 2004, P PICT COD S PCS
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   FRAUNHOFER HHI, 2011, REPOSITORY FHG HHI 3
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Haykin S, 2005, IEEE J SEL AREA COMM, V23, P201, DOI 10.1109/JSAC.2004.839380
   *ISO IEC, 2001, JTC1 ISOIEC
   Jagmohan A, 2003, IEEE DATA COMPR CONF, P213
   KARIM HA, 2007, P PRET COD S PCS
   Katsaggelos AK, 2005, P IEEE, V93, P135, DOI 10.1109/JPROC.2004.839621
   LIAO J, 2010, IEEE T CIRCUITS SYST, V10, P30
   *MICR RES, 2011, MSR 3D VID
   MILANI S, 2010, P ACM MULT 2010
   MILANI S, 2010, P 11 INT WORKSH IM A
   MILANI S, 2009, P EUR SIGN PROC C EU, P1824
   Milani S, 2010, IEEE SIGNAL PROC LET, V17, P51, DOI 10.1109/LSP.2010.2051619
   *MOBILE3DTV PROJ, 2011, 3D VID DAT
   Norkin A, 2006, LECT NOTES COMPUT SC, V4105, P730
   PURI R, 2002, P ALL C 2002, P402
   Reusens E, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P377, DOI 10.1109/ICIP.1996.560838
   Rosenberg J., 1999, An rtp payload format for generic forward error correction
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   Schulzrinne H., 1996, Rtp: a transport protocol for real-time applications
   SHI S, 2009, P 2 INT C IMM TEL IM
   Wang AH, 2009, SCI CHINA SER F, V52, P206, DOI 10.1007/s11432-009-0037-5
   Wang J, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P1584, DOI 10.1109/ISIT.2006.261543
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WIEGAND T, 2004, P 12 JVT M
   Wu M, 2004, P SOC PHOTO-OPT INS, V5600, P120, DOI 10.1117/12.577069
   YEO C, 2008, P IEEE VIS COMM IM P, V6508
NR 46
TC 6
Z9 6
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 23
DI 10.1145/2037676.2037680
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 857NR
UT WOS:000297725800004
DA 2024-07-18
ER

PT J
AU Jin, YH
   Prabhakaran, B
AF Jin, Yohan
   Prabhakaran, Balakrishnan
TI Knowledge Discovery from 3D Human Motion Streams through Semantic
   Dimensional Reduction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; 3D motion capture; singular value decomposition; Gaussian
   mixture model; EM; string matching
ID CONVERGENCE PROPERTIES; MAXIMUM-LIKELIHOOD; EM; RETRIEVAL
AB 3D human motion capture is a form of multimedia data that is widely used in entertainment as well as medical fields ( such as orthopedics, physical medicine, and rehabilitation where gait analysis is needed). These applications typically create large repositories of motion capture data and need efficient and accurate content-based retrieval techniques. 3D motion capture data is in the form of multidimensional time-series data. To reduce the dimensions of human motion data while maintaining semantically important features, we quantize human motion data by extracting spatio-temporal features through SVD and translate them onto a symbolic sequential representation through our proposed sGMMEM (semantic Gaussian Mixture Modeling with EM). In order to handle variations in motion capture data due to human body characteristics and speed of motion, we transform the semantically quantized values into a histogram representation. This representation is used as a signature for classification and similarity-based retrieval. We achieved good classification accuracies for "coarse" human motion categories (such as walking 92.85%, run 91.42%, and jump 94.11%) and even for subtle categories (such as dance 89.47%, laugh 83.33%, basketball signal 85.71%, golf putting 80.00%). Experiments also demonstrated that the proposed approach outperforms earlier techniques such as the wMSV (weighted Motion Singular Vector) approach and LB Keogh method.
C1 [Prabhakaran, Balakrishnan] Univ Texas Dallas, Dept Comp Sci, Dallas, TX 75230 USA.
C3 University of Texas System; University of Texas Dallas
EM yjin@myspace-inc.com
CR ADJEROH DA, 1998, P IAPR INT WORKSH MU, P80
   ADJEROH DA, 1998, P INT WORKSH MULT DA
   Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   CARDLE M, 2003, P 29 ACM SIGGRAPH SK
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Duda R. O., 2001, PATTERN CLASSIFICATI, P517
   Golub G.H., 1989, MATRIX COMPUTATIONS
   GUERRA G, 2007, IEEE COMPUT MAG, V40, P5
   Guerra-Filho G, 2006, IEEE-RAS INT C HUMAN, P69, DOI 10.1109/ICHR.2006.321365
   JIN Y, 2008, P INT MULT MOD C MMM
   KEOGH EJ, 2004, VLDB, P780
   KOHONEN T, 1992, P INT JOINT C NEUR N, P725
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   LI C, 2007, INT J MULTIMEDIA TOO, V35, P1
   LI C, 2004, P 2 ACM INT WORKSH M, P75
   Li CJ, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1236471.1236475
   Liu F, 2003, COMPUT VIS IMAGE UND, V92, P265, DOI 10.1016/j.cviu.2003.06.001
   LIU G, 2005, P ACM SIGMOD INT C M
   Meredith M., MOTION CAPTURE FILE
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   PRADHAN GN, 2007, P INT MULT MOD C MMM
   Puzicha J, 2000, IEEE T IMAGE PROCESS, V9, P666, DOI 10.1109/83.841942
   REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129
   YANG CF, 2003, P 26 ANN INT ACM SIG, P267
   YANG K, 2005, P TIME C IEEE COMP S
   YU C, 2001, VLDB, P421
NR 29
TC 7
Z9 8
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2011
VL 7
IS 2
AR 9
DI 10.1145/1925101.1925104
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 729AT
UT WOS:000287919200003
DA 2024-07-18
ER

PT J
AU Chandra, S
   Yu, XW
AF Chandra, Surendar
   Yu, Xuwen
TI An Empirical Analysis of Serendipitous Media Sharing among Campus-Wide
   Wireless Users
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Measurement; Performance; Local media sharing;
   peer-to-peer
AB Contemporary systems use centralized as well as peer-to-peer mechanisms for the large scale distribution of media objects. In this work, we investigate a serendipitous mechanism for directly sharing media objects among a local community of wireless users. This localized sharing is attractive when wide area network connectivity is undesirable, expensive or unavailable; especially when the shared media objects are large. With some restrictions, such localized sharing of media objects is also acceptable to content owners. However, localized sharing has to contend with far fewer media providers who may also not offer the variety of objects available from wide-area services. We collected empirical data from the widely deployed Apple iTunes application for our analysis. We showed that users are already making a significant amount of media objects available for serendipitous sharing. Our analysis showed that the shared object annotations exhibited a Zipfian long tail distribution. The availability patterns of wireless iTunes users and the object annotations makes serendipitous sharing inappropriate for scenarios that require access to a specific object. Instead, mechanisms that allow the user to specify classes of interesting objects are better suited for such users. Also, given the smaller scale of these systems, serendipitous sharing can benefit from approaches that allow users to disseminate a compact representation of their shared objects. Though the wireless user availability rates was not as high as what was observed in a corporate desktop setting, a large fraction of the users showed high temporal consistency. This allows for high availability with reasonable replication during weekday daytime hours. We answer important questions regarding the viability of a campus-wide media sharing system.
EM surendar@acm.org
FU U.S. National Science Foundation [CNS-0447671]
FX Supported in part by the U.S. National Science Foundation (CNS-0447671).
CR ACOSTA W, 2007, P 8 PASS ACT MEAS C
   ACOSTA W, 2008, P IEEE C HOT TOP PEE
   ACOSTA W, 2008, P ACM SPIE C MULT CO
   Adya A, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   [Anonymous], Proceedings of the 2005 ACM SIGCOMM workshop on Delay-tolerant networking, DOI DOI 10.1145/1080139.1080142
   [Anonymous], P 6 ANN ACM IEEE INT
   [Anonymous], 2017, BENTHAM OPEN, DOI DOI 10.1145/378993.379239
   [Anonymous], P IEEE INFOCOM BARC, DOI DOI 10.1109/INFOCOM.2006.172
   [Anonymous], 2001, Pastry: Scalable, decentralized object location, and routing for large-scale peer-to-peer systems, DOI DOI 10.1007/3-540-45518-3_18
   [Anonymous], US TODAY 0612
   AUBREY S, 2003, ADVENTURES HIGHER ED
   Balazinska M, 2003, PROCEEDINGS OF MOBISYS 2003, P303, DOI 10.1145/1066116.1066127
   Bolosky WJ, 2000, PERF E R SI, V28, P34, DOI 10.1145/345063.339345
   Broder Andrei, 2003, Internet Math, V1, DOI DOI 10.1080/15427951.2004.10129096
   CHANDRA S, 2007, P ACM SPIE C MULT CO
   CHAWATHE Y, 2003, SIGCOMM 03, P407
   Chazelle B., 2004, P THEFIFTEENTH ANN A, P30
   Davies Chris., Applerecords
   GUMMADI KP, 2003, P 19 ACM S OP SYST P, P314
   Guttman E, 2001, IEEE INTERNET COMPUT, V5, P81, DOI 10.1109/4236.935181
   Henderson T, 2008, COMPUT NETW, V52, P2690, DOI 10.1016/j.comnet.2008.05.003
   HSU WJ, 2005, 05858 USC
   Hui P, 2007, FIFTH ANNUAL IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS WORKSHOPS, PROCEEDINGS, P65, DOI 10.1109/PERCOMW.2007.55
   KIROVSKI D, 2006, P ACM INT WORKSH NET, P118
   Kotz D., 2002, 8 INT C MOBILE COMPU, P107
   LOVITT R, 2008, IN FLIGHT INTERNET A
   Manolios P., BLOOM FILTER CALCULA
   MASSOULIE L, 2006, P 25 ANN ACM S PRINC, P123, DOI DOI 10.1145/1146381.1146402
   Mickens JamesW., 2005, WISE 05, P77, DOI DOI 10.1145/1080793.1080806
   *MISS, 2009, AUD VID PLAYER WEB B
   Mitzenmacher M., 2001, Proceedings of the 20th Annual ACM Symposium on Principles of Distributed Computing, P144
   SAROIU S, 2002, P ACM SPIE C MULT CO
   SCHILLER P, 2009, MACWORLD EXPO 2009 K
   SONG L, 2007, P ACM MOBICOM WORKSH
   Stoica I, 2004, IEEE ACM T NETWORK, V12, P205, DOI 10.1109/TNET.2004.826279
   Stoica I, 2001, ACM SIGCOMM COMP COM, V31, P149, DOI 10.1145/964723.383071
   STUTZBACH D, 2007, IEEE ACM T NETW
   TIMIRAOS N, 2006, WALL STREET J   0706, pB1
   Voida Amy., 2005, CHI 05, P191, DOI [DOI 10.1145/1054972.1054999, 10.1145/1054972.1054999]
   Yan Guanhua., 2007, Proceedings of ACM Symposium on Information, Computer and Communications Security (ASIACCS), P32, DOI DOI 10.1145/1229285.1229294
   YU X, 2008, P ACM SPIE C MULT CO
   ZAHARIA MA, 2007, P 6 WORKSH PEER TO P
   ZHAO S, 2006, P ACM SPIE C MULT CO
   Zhuang S, 2003, PROCEEDINGS OF MOBISYS 2003, P129, DOI 10.1145/1066116.1189042
NR 44
TC 3
Z9 3
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2011
VL 7
IS 1
AR 6
DI 10.1145/1870121.1870127
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 712GI
UT WOS:000286653800006
DA 2024-07-18
ER

PT J
AU Pan, L
   Zhang, CN
AF Pan, Leon
   Zhang, Chang N.
TI A Criterion-Based Multilayer Access Control Approach for Multimedia
   Applications and the Implementation Considerations
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Multilayer access control; security criterion; secure user;
   secure object; secure permission
ID MODEL
AB In this article, a novel criterion-based multilayer access control (CBMAC) approach is presented to enhance existing access control models such as Role-Based, Mandatory, and Discretionary Access Control models to support multilayer (multilevel) access control. The proposed approach is based on a set of predefined security criteria which are extracted from authorization rules. The security attributes of objects and users are specified by security criterion expressions (serving as locks) and the elements (serving as keys) of security criterion subsets respectively. An object embedded with a number of security criterion expressions becomes a secure object while a user associated with a security criterion subset is called a secure user. The multilayer access control is achieved by evaluating the embedded security criterion expressions (actuating locks) by the elements (keys) in a user's security criterion subset. The paper also provides the details of integrating the proposed approach with existing access control models and presents the implementation considerations of Criterion-Based Role-Based Multilayer Access Control, the integration of CBMAC and Role-Based Access Control.
C1 [Pan, Leon] Univ Regina, Dept Comp Sci, Regina, SK S4S 3S4, Canada.
C3 University of Regina
RP Pan, L (corresponding author), Univ Regina, Dept Comp Sci, Regina, SK S4S 3S4, Canada.
EM panli111@cs.uregina.ca; zhang@cs.uregina.ca
FU Telecommunications Research Labs (TRLabs); NSERC
FX This research was partly supported by Telecommunications Research Labs
   (TRLabs) and NSERC.
CR ADAM N, 2002, IEEE T KNOWL DATA EN, V14
   [Anonymous], 2002, Introduction to MPEG- 7: Multimedia content description interface
   [Anonymous], 2005, P IEEE INT C WEB SER
   BELL D, 1996, J COMPUTER SECURITY, V4, P239
   Bertino E., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P336, DOI 10.1145/354756.354838
   Bertino E, 2003, ACM T INFORM SYST, V21, P155, DOI 10.1145/763693.763695
   BERTINO E, 2002, MAX ACCESS CONTROL S
   DAMIANI E, 2005, P 5 IEEE INT S SIGN
   DAMIANI E, 2002, P 16 ANN IFIP WG11 3
   Fernández-Medina E, 2003, LECT NOTES COMPUT SC, V2889, P741
   KODALI N, MULTIMEDIA ACCESS CO
   Kosch H., 2004, DISTRIBUTED MULTIMED
   *NAT COMP SEC CTR, 1987, GUID UND DISCR ACC C
   PAN L, 2006, P IEEE INT S MULT IS
   PARK J, 2004, ACM T INFORM SYST SE, V7
   RABITTI F, 1991, ACM T DATABASE SYST, V16, P88, DOI 10.1145/103140.103144
   Salembier P, 2001, IEEE T CIRC SYST VID, V11, P748, DOI 10.1109/76.927435
   Sandhu RS, 1996, COMPUTER, V29, P38, DOI 10.1109/2.485845
   *W3C REC, 2004, XML SCHEM 1
   *W3C REC, 2004, XML SCHEM 2
   *W3C REC, 2004, XML SCHEM 0
   Walmsley Priscilla., 2002, DEFINITIVE XML SCHEM
   Wang L., 2004, P 2004 ACM WORKSH FO P 2004 ACM WORKSH FO
NR 23
TC 8
Z9 9
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2008
VL 5
IS 2
AR 17
DI 10.1145/1413862.1413870
PG 29
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 376AU
UT WOS:000261155900008
DA 2024-07-18
ER

PT J
AU Ip, ATS
   Lui, JCS
   Liu, JC
AF Ip, Alan T. S.
   Lui, John C. S.
   Liu, Jiangchuan
TI A revenue-rewarding scheme of providing incentive for cooperative proxy
   caching for media streaming systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE performance; game-theoretic analysis; incentive mechanism; pricing; Nash
   equilibrium; resource allocation
ID SERVERS
AB Network entities cooperating together can improve system performance of media streaming. In this paper, we address the "incentive issue" of a cooperative proxy caching system and how to motivate each proxy to provide cache space to the system. To encourage proxies to participate, we propose a "revenue-rewarding scheme" to credit the cooperative proxies according to the resources they contribute. A game-theoretic model is used to analyze the interactions among proxies under the revenue-rewarding scheme. We propose two cooperative game settings that lead to optimal situations. In particular, (1) We propose a distributed incentive framework for peers to participate in resource contribution for media streaming; (2) Proxies are encouraged to cooperate under the revenue-rewarding scheme; (3) Profit and social welfare are maximized in these cooperative games; and (4) Cost-effective resource allocation is achieved in these cooperative games. Large scale simulation is carried out to validate and verify the merits of our proposed incentive schemes.
C1 [Ip, Alan T. S.; Lui, John C. S.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Liu, Jiangchuan] Simon Fraser Univ, Dept Comp Sci, Vancouver, BC, Canada.
C3 Chinese University of Hong Kong; Simon Fraser University
RP Ip, ATS (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM alanip@gmail.com; cslui@cse.cuhk.edu.hk; jcliu@cs.sfu.ca
CR Adar E., 2000, First Monday, V5, DOI 10.5210/fm.v5i10.792
   Adler M., 2004, P 2 WORKSH EC PEER T
   [Anonymous], 1999, SIAM Series in Classics in Applied Mathematics
   BASAR T, 2002, P IEEE INFOCOM 2002
   Buragohain C., 2003, P 3 INT C PEER TO PE
   CAMPOSNANEZ E, 2003, P IEEE INFOCOM 2003
   FELDMAN M, 2004, P ACM C EL COMM EC 0
   GOLLE P, 2001, P ACM C EL COMM EC 0
   Golubchik L, 1996, MULTIMEDIA SYST, V4, P140, DOI 10.1007/s005300050019
   Golubchik L, 1998, PARALLEL COMPUT, V24, P123, DOI 10.1016/S0167-8191(97)00119-1
   GUPTA M, 2003, P INT WORKSH NETW OP
   GUPTA R, 2004, P IEEE INT C NETW SI
   HABIB A, 2004, P INT WORKSH QUAL SE
   HARDIN G, 1968, SCIENCE, V162, P1248
   HE L, 2005, P IEEE INFOCOM 2005
   HEFEDA MM, 2003, 200237 CERIAS PURD U
   Ip ATS, 2007, IEEE T PARALL DISTR, V18, P70, DOI 10.1109/TPDS.2007.253282
   IP ATS, 2007, P IFIP NETW 2005, V18, P70, DOI DOI 10.1109/TPDS.2007.253282
   JIANG J, 2003, P GRID COOPERATIVE C
   JIANG JWJ, 2005, PERF EVAL J, V62, P1
   Kolda TG, 2003, SIAM REV, V45, P385, DOI [10.1137/S003614450242889, 10.1137/S0036144502428893]
   Lau SW, 1998, MULTIMEDIA SYST, V6, P29, DOI 10.1007/s005300050074
   Lie PWK, 2000, MULTIMED TOOLS APPL, V11, P35, DOI 10.1023/A:1009673332611
   LUI S, 2002, P 35 ANN HAW INT C S
   Ma RTB, 2006, IEEE ACM T NETWORK, V14, P978, DOI 10.1109/TNET.2006.882904
   Ma RTB, 2004, P INT C DISTR COMP S
   MA RTB, 2004, P ACM SIGM PERF C BA
   RANGANATHAN K, 2003, P WORKSH EC PEER TO
   Tamilmani K., 2004, P 2 WORKSH EC PEER T
   TURNER DA, 2004, P 7 INT C EL COMM
   UZAWA H, 1960, REV EC STUD
   WANG W, 2003, P INT WORKSH QUAL SE
   YE S, 2004, P ACM INT C MULT NEW
   ZHANG X, 2005, P IEEE INFOCOM MIAM
NR 34
TC 9
Z9 10
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2008
VL 4
IS 1
AR 5
DI 10.1145/1324287.1324292
PG 32
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 264UK
UT WOS:000253315700005
DA 2024-07-18
ER

PT J
AU Rachovides, D
   Walkerdine, J
   Phillips, P
AF Rachovides, Dorothy
   Walkerdine, James
   Phillips, Peter
TI The conductor interaction method
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE design; experimentation; human factors; human-computer interaction;
   gaze- and gesture-based interfaces
AB Computers have increasingly become part of our everyday lives, with many activities either involving their direct use or being supported by one. This has prompted research into developing methods and mechanisms to assist humans in interacting with computers (human-computer interaction, or HCI). A number of HCI techniques have been developed over the years, some of which are quite old but continue to be used, and some more recent and still evolving. Many of these interaction techniques, however, are not natural in their use and typically require the user to learn a new means of interaction. Inconsistencies within these techniques and the restrictions they impose on user creativity can also make such interaction techniques difficult to use, especially for novice users.
   This article proposes an alternative interaction method, the conductor interaction method (CIM), which aims to provide a more natural and easier-to-learn interaction technique. This novel interaction method extends existing HCI methods by drawing upon techniques found in human-human interaction. It is argued that the use of a two-phased multimodal interaction mechanism, using gaze for selection and gesture for manipulation, incorporated within a metaphor-based environment, can provide a viable alternative for interacting with a computer (especially for novice users). Both the model and an implementation of the CIM within a system are presented in this article. This system formed the basis of a number of user studies that have been performed to assess the effectiveness of the CIM, the findings of which are discussed in this work.
C1 [Rachovides, Dorothy] Univ Surrey, Digital World Res Ctr, Guildford GU2 7XH, Surrey, England.
   [Walkerdine, James; Phillips, Peter] Univ Lancaster, Dept Comp, Lancaster LA1 4YW, England.
C3 University of Surrey; Lancaster University
RP Rachovides, D (corresponding author), Univ Surrey, Digital World Res Ctr, Guildford GU2 7XH, Surrey, England.
EM drachovides@acm.org; j.walkerdine@lancaster.ac.uk;
   phillips@comp.lancs.ac.uk
RI Phillips, Peter/D-8686-2016
OI Phillips, Peter/0000-0002-7473-6040
CR AOKI T, 1999, P INT WORKSH MONJUNO
   Argyle M., 1996, Bodily communication
   BAUER B, 2002, LECT NOTES COMPUTER, V2298
   BAUML BJ, 1997, DICT WORLDWIDE GESTR
   BENFORD S, 1995, P C EUR MAASTR NETH
   Bolt R. A., 1980, P 7 ANN C COMP GRAPH
   BOLT RA, 1992, P 5 ANN ACM S US INT
   BORCHERS O, 1997, P SIGCHI C HUM FACT
   CHEN WC, 2000, P IEEE VIS C SALT LA
   Dix A., 2004, Human-computer interaction
   FARID MM, 2002, J SOC INF DISPLAY
   FARID MM, 2002, P SPIE REG M OPT PHO
   Gips J., 1996, P 11 INT C TECHN PER, VVolume 1
   GRIBBS SJ, 1999, MULTIMEDIA SYST, V7, P214
   Hart S G., 1987, The Practical Assessment of Pilot Workload, P90
   INSTANCE H, 1994, P C HUM COMP INT PEO, P195
   JACOB RJK, 1991, ACM T INFORM SYST, V9, P3
   KAM KS, 2002, EYE MOVEMENT MAILING
   Kauff Peter, 2002, P 4 INT C COLLABORAT, P105
   Kendon A., 1992, Rethinking Context: Language as an Interactive Phenomenon
   MARRIN T, 1997, P INT C MUS C THESS
   *NAT POINT, 1997, OPT TRACK SYST
   PARK E, 2006, P SIGCHI C HUM FACT, P255
   Qvarfordt P., 2005, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, New York, NY, USA, P221
   SALVUCCI DD, 1999, P SIGCHI C HUM FAC C
   Shneiderman B., 2005, DESIGNING USER INTER
   Sibert L. E., 2000, P SIGCHI C HUM FACT
   SOWA T, 1999, LECT NOTES COMPUTER, V1739
   THORISSON KR, 1998, P AUT AG C MINN
   Vertegaal R., 2001, P SIGCHI C HUM FACT
   Vertegaal R., 1999, P SIGCHI C HUM FACT
   Zhai S., 1999, P SIGCHI C HUM FACT
NR 32
TC 2
Z9 6
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 4
AR 27
DI 10.1145/1314303.1314312
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 250QU
UT WOS:000252315900009
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Adams, B
   Venkatesh, S
   Jain, R
AF Adams, Brett
   Venkatesh, Svetha
   Jain, Ramesh
TI IMCE: Integrated Media Creation Environment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Human Factors; Measurement; Home movie; video collection;
   semantics; computational media aesthetics
AB We discuss the design goals for an integrated media creation environment (IMCE) aimed at enabling the average user to create media artifacts with professional qualities. The resulting requirements are implemented and we demonstrate the efficacy of the resulting system with the generation of two simple home movies. The significance for the average user seeking to create home movies lies in the flexible and automatic application of film principles to the task, removal of tedious low-level editing by means of well-formed media transformations in terms of high-level film constructs (e.g., tempo), and content repurposing powered by those same transformations added to the rich semantic information maintained at each phase of the process.
C1 [Adams, Brett; Venkatesh, Svetha] Curtin Univ Technol, Dept Comp Sci, Perth, WA 6109, Australia.
   [Jain, Ramesh] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
   [Jain, Ramesh] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
C3 Curtin University; University System of Georgia; Georgia Institute of
   Technology; University System of Georgia; Georgia Institute of
   Technology
RP Adams, B (corresponding author), Curtin Univ Technol, Dept Comp Sci, Kent St, Perth, WA 6109, Australia.
EM adamsb@cs.curtin.edu.au; svetha@cs.curtin.edu.au; jain@ece.gatech.edu
OI Venkatesh, Svetha/0000-0001-8675-6631
CR *ACD SYST, ACD VIDEOMAGIC
   Adams B, 2002, IEEE T MULTIMEDIA, V4, P472, DOI 10.1109/TMM.2002.802016
   Agamanolis S, 2003, IEEE MULTIMEDIA, V10, P88, DOI 10.1109/MMUL.2003.1218260
   Aristotle Malcolm., 1997, POETICS
   Baecker R., 1996, Proceedings ACM Multimedia 96, P31, DOI 10.1145/244130.244142
   Bailey BrianP., 2001, P 9 ACM INT C MULTIM, P241
   BARRY B, 2003, P 2003 INT C MULT EX
   BEAL J, 1974, CINE CRAFT
   BOBICK A, 1995, P ICCV 95 WORKSH CON, P13
   BROOKS K, 1999, THESIS MIT CAMBRIDGE
   BRUCKMAN A, 1991, THESIS MIT CAMBRIDGE
   Burch Noel., 1973, THEORY FILM PRACTICE
   Casares Juan, 2002, P 4 C DES INT SYST P, P157
   CHANAN M, 1998, FILMWAVES, V4
   Chatman Seymour., 1978, STORY DISCOURSE
   CHUA TS, 1995, ACM T INFORM SYST, V13, P373, DOI 10.1145/211430.211431
   Davis M, 2003, IEEE MULTIMEDIA, V10, P54, DOI 10.1109/MMUL.2003.1195161
   Field Syd., 1994, Screenplay: The Foundations of Screenwriting
   GIRGENSOHN A, 2000, P UIST 00, P81
   GIRGENSOHN A, 2003, P 11 ACM INT C MULT, P92
   Hua X.S., 2003, P 11 ACM INT C MULTI, P490
   LANG R, 1999, P AAAI FALL S NARR I, P134
   LINDLEY CA, 2001, INSR0101 CWI
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   McKee R., 1997, STORY SUBSTANCE STRU
   Mehring M., 1990, The screenplay: A blend of film form and content
   *MUV TECHN, 2004, MUV AUTOPRODUCER
   Nack F, 2001, P 9 ACM INT C MULT, P251
   PFEIFFER RLS, 1997, COMMUN ACM, V40, P54
   SCHULTZ E, 1972, MAKE EXCITING MOVIES
   Sharff Stefan., 1982, The Elements of Cinema: Toward a Theory of Cinesthetic Impact
   Suchman L. A., 1987, Plans and situated actions: The problem of human-machine communication
   Sundaram Hari., 2002, COMUNICA O APRESENTA, DOI [DOI 10.1145/641007.641042, 10.1145/641007.641042]
   Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951
   Zacks JM, 2001, PSYCHOL BULL, V127, P3, DOI 10.1037//0033-2909.127.1.3
   2004, OPINIONJOURNAL F T W
   2004, VIDEOMAKER MAGAZINE
NR 37
TC 8
Z9 9
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2005
VL 1
IS 3
PG 37
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DW
UT WOS:000205012400001
DA 2024-07-18
ER

PT J
AU Deng, ZJ
   He, XT
   Peng, YX
AF Deng, Zijun
   He, Xiangteng
   Peng, Yuxin
TI LFR-GAN: Local Feature Refinement based Generative Adversarial Network
   for Text-to-Image Generation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Local feature refinement; text-to-image generation; generative
   adversarial network
AB Text-to-image generation aims to generate images from text descriptions. Its main challenge lies in two aspects: (1) Semantic consistency, i.e., the generated images should be semantically consistent with the input text; and (2) Visual reality, i.e., the generated images should look like real images. To ensure text-image consistency, existing works mainly learn to establish the cross-modal representations via a text encoder and image encoder. However, due to the limited representation capability of the fixed-length embeddings and the flexibility of the free-form text descriptions, the learned text-to-image model is incapable of maintaining the semantic consistency between image local regions and fine-grained descriptions. As a result, the generated images sometimes miss some fine-grained attributes of the generated object, such as the color or shape of a part of the object. To address this issue, this paper proposes a Local Feature Refinement Based Generative Adversarial Network (LFR-GAN), which first divides the text into some independent fine-grained attributes and generates an initial image, then refines the image details based on these attributes. The main contributions are three-fold: (1) An attribute modeling approach is proposed to model the fine-grained text descriptions by mapping them into representations of independent attributes, which provides more fine-grained details for image generation. (2) A local feature refinement approach is proposed to enable the generated image to form a complete reflection of the fine-grained attributes contained in the text description. (3) A multi-stage generation approach is proposed to realize the fine-grained manipulation of complex images progressively, which aims to improve the performance of the refinement and generate photo-realistic images. Extensive experiments on the CUB and Oxford102 datasets show the effectiveness of our LFR-GAN approach in both text-to-image generation and text-guided image manipulation tasks. Our LFR-GAN approach shows superior performance compared to the state-of-the-art methods. The codes will be released at https://github.com/PKU-ICST-MIPL/LFR-GAN_TOMM2023.
C1 [Deng, Zijun; He, Xiangteng; Peng, Yuxin] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
   [Deng, Zijun; He, Xiangteng; Peng, Yuxin] Peking Univ, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
C3 Peking University; Peking University
RP Peng, YX (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.; Peng, YX (corresponding author), Peking Univ, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
OI Deng, Zijun/0000-0003-1590-7863; He, Xiangteng/0000-0001-8502-5685
FU National Natural Science Foundation of China [62132001, 61925201,
   U21B2025, 62272013, U22B2048]
FX This work was supported by the grants from the National Natural Science
   Foundation of China (62132001, 61925201, U21B2025, 62272013, U22B2048).
CR Akbik A, 2019, NAACL HLT 2019: THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES: PROCEEDINGS OF THE DEMONSTRATIONS SESSION, P54
   Bouma H., 1980, IPO ANN PROGR REPORT, V15, P83
   Cheng J., 2020, IEEE C COMP VIS PATT, P10911
   Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608
   Fu Y., 2019, P 12 INT C NATURAL L, P24, DOI DOI 10.18653/V1/W19-8604
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jobling P., 1996, GRAPHIC DESIGN REPRO
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li B., 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P7880
   Li B., 2020, Advances in Neural Information Processing Systems, V33, P22020, DOI [10.48550/arxiv.2010.12136, DOI 10.48550/ARXIV.2010.12136]
   Luan Y, 2021, T ASSOC COMPUT LING, V9, P329, DOI 10.1162/tacl_a_00369
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Melnyk I, 2017, Arxiv, DOI arXiv:1711.09395
   Nam S, 2018, ADV NEUR IN, V31
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Ruan SL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13940, DOI 10.1109/ICCV48922.2021.01370
   Salimans T, 2016, ADV NEUR IN, V29
   Sang E. F., 2000, ARXIV
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang G., 2022, P IEEECVF C COMPUTER, P10707
   Wang H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P630, DOI 10.1145/3474085.3475226
   Wang Ke., 2019, Adv Neural Inf Process Syst, V32, P11034
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Xu Minzhang, 2022, WORLD WIDE WEB, P1
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang YH, 2021, IEEE T IMAGE PROCESS, V30, P2798, DOI 10.1109/TIP.2021.3055062
   Yang ZC, 2018, 32 C NEURAL INFORM P, V31
   Yi XY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3801
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhou Y., 2022, Computer Vision and Pattern Recognition, P17907
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 39
TC 0
Z9 0
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 207
DI 10.1145/3589002
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200030
DA 2024-07-18
ER

PT J
AU Xie, JY
   Chen, JL
   Cai, Y
   Huang, QB
   Li, Q
AF Xie, Jiayuan
   Chen, Jiali
   Cai, Yi
   Huang, Qingbao
   Li, Qing
TI Visual Paraphrase Generation with Key Information Retained
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodal; visual paraphrase generation; VisualBERT
ID LANGUAGE
AB Visual paraphrase generation task aims to rewrite a given image-related original sentence into a new paraphrase, where the paraphrase needs to have the same expressed meaning as the original sentence but have a difference in expression form. Existing studies mainly extract two semantic vectors to represent the entire image and the entire original sentence, respectively, for paraphrase generation. However, these semantic vectors for an image or a sentence may lead to the model failing to focus on some key objects in the original sentence, which may generate semantically inconsistent sentences by changing key object information. In this article, we propose an object-level paraphrase generation model, which generates paraphrases by adjusting the permutation of key objects and modifying their associated descriptions. To adjust the permutation of key objects, an object-sorting module aims to obtain new object sequences based on the key object information and original sentences. Then, a sequence generation module sequentially generates paraphrases based on the permutation of the newly object sequences. Each generation step focuses on different image features associated with different key objects to generate descriptions with differences. Furthermore, we use a semantic discriminator module to promote the generated paraphrase to be semantically close to the original sentence. Specifically, the loss function of the discriminator penalizes the excessive distance between the paraphrase and the original sentence. Extensive experiments on the MS COCO dataset show that the proposed model outperforms the baselines.
C1 [Xie, Jiayuan; Chen, Jiali; Cai, Yi] South China Univ Technol, Sch Software Engn, Guangzhou, Peoples R China.
   [Xie, Jiayuan; Chen, Jiali] MOE China, Key Lab Big Data & Intelligent Robot SCUT, Shenzhen, Peoples R China.
   [Xie, Jiayuan; Chen, Jiali] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Huang, Qingbao] Guangxi Univ, Sch Elect Engn, Nanning, Guangxi, Peoples R China.
   [Li, Qing] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
C3 South China University of Technology; Peng Cheng Laboratory; Guangxi
   University; Hong Kong Polytechnic University
RP Cai, Y (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou, Peoples R China.
EM sexiejiayuan@mail.scut.edu.cn; ycai@scut.edu.cn
RI Wang, lingyu/JLM-2013-2023; liu, peng/JSL-1931-2023; Wang,
   Chao/JHT-6081-2023; Chen, Yu/JLL-0171-2023; WANG, YANG/JFA-8821-2023;
   Li, Qing/JMH-1365-2023
OI Li, Qing/0000-0003-3370-471X; Chen, Jiali/0000-0001-8064-1577; Huang,
   Qingbao/0000-0001-7691-347X
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Babakov N, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022): STUDENT RESEARCH WORKSHOP, P300
   Bander E, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P596
   Cai H., 2020, P 8 INT C LEARNING R
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Edunov S, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P489
   Eisenberger M, 2022, PROC CVPR IEEE, P499, DOI 10.1109/CVPR52688.2022.00059
   Fan Z., 2021, PROC INT JOINT C ART, P657
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Fang ZW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282469
   Goyal T, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P238
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hosking T, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P2489
   Hu RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1419, DOI 10.1109/ICCV48922.2021.00147
   Huang Zhiheng., 2015, Bidirectional LSTM-CRF models for sequence tagging
   Jin WK, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321505
   Kim J, 2017, INTERSPEECH, P1591, DOI 10.21437/Interspeech.2017-477
   Kingma Diederik P., 2015, P 3 INT C LEARN
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li Liunian Harold, 2020, WEAKLY SUPERVISED VI
   Li Q, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3489142
   Li W, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P3187
   Li W, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2592
   Li YA, 2022, PROC CVPR IEEE, P17969, DOI 10.1109/CVPR52688.2022.01746
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin Z, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P1548
   Liu LX, 2019, IEEE I CONF COMP VIS, P4239, DOI 10.1109/ICCV.2019.00434
   Ma S, 2018, P C N AM ASS CHAPT A, V1, P196, DOI [DOI 10.18653/V1/N18-1018, 10.18653/v1/N18-1018]
   Mena Gonzalo., 2018, Proceedings of the 6th International Conference on Learning Representation
   Ormazabal A, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P1621
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Patro BN, 2021, NEUROCOMPUTING, V420, P149, DOI 10.1016/j.neucom.2020.08.022
   Patro Badri Narayana, 2018, P 27 INT C COMPUTATI, P2715
   Peng QW, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P5579
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tang JH, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3291925
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Weston J, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P4052
   WHITAKER EE, 1993, COLL COMPOS COMMUN, V44, P509, DOI 10.2307/358386
   Xing C, 2017, AAAI CONF ARTIF INTE, P3351
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhao X, 2022, 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, NAACL-HLT 2022, P79
   Zhou L., 2018, DESIGN IMPLEMENTATIO
   Zhuang YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3366710
NR 49
TC 2
Z9 3
U1 10
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 184
DI 10.1145/3585010
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200007
DA 2024-07-18
ER

PT J
AU Li, ZQ
   Xia, PF
   Rui, X
   Li, B
AF Li, Ziqiang
   Xia, Pengfei
   Rui, Xue
   Li, Bin
TI Exploring the Effect of High-frequency Components in GANs Training
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Generative Adversarial Networks; frequency; contrastive learning; data
   augmentation
AB Generative Adversarial Networks (GANs) have the ability to generate images that are visually indistinguishable from real images. However, recent studies have revealed that generated and real images share significant differences in the frequency domain. In this article, we argue that the frequency gap is caused by the high-frequency sensitivity of the discriminator. According to our observation, during the training of most GANs, severe high-frequency differences make the discriminator focus on high-frequency components excessively, which hinders the generator from fitting the low-frequency components that are important for learning images' content. Then, we propose two simple yet effective image pre-processing operations in the frequency domain for eliminating the side effects caused by high-frequency differences in GANs training: High-frequency Confusion (HFC) and High-frequency Filter (HFF). The proposed operations are general and can be applied to most existing GANs at a fraction of the cost. The advanced performance of the proposed operations is verified on multiple loss functions, network architectures, and datasets. Specifically, the proposed HFF achieves significant improvements of 42.5% FID on CelebA (128*128) unconditional generation based on SNGAN, 30.2% FID on CelebA unconditional generation based on SSGAN, and 69.3% FID on CelebA unconditional generation based on InfoMAXGAN. Furthermore, we also adopt HFF as the first attempt at data augmentation in the frequency domain for contrastive learning, achieving state-of-the-art performance on unconditional generation. Code is available at https://github.com/iceli1007/HFC-and-HFF.
C1 [Li, Ziqiang; Xia, Pengfei; Rui, Xue; Li, Bin] Univ Sci & Technol China, 96 Jinzhai Rd, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Li, B (corresponding author), Univ Sci & Technol China, 96 Jinzhai Rd, Hefei 230026, Anhui, Peoples R China.
EM iceli@mail.ustc.edu.cn; xpengfei@mail.ustc.edu.cn;
   ruixue27@mail.ustc.edu.cn; binli@ustc.edu.cn
RI ziqiang, li/P-7648-2017
OI ziqiang, li/0000-0001-9484-2310; Xia, Pengfei/0000-0001-8268-2057; li,
   bin/0000-0002-2332-3959; Rui, Xue/0000-0002-8116-0334
FU National Natural Science Foundation of China [U19B2044, 61836011]
FX The work is partially supported by the National Natural Science
   Foundation of China under Grants No. U19B2044 and No. 61836011.
CR Arjovsky M, 2017, Arxiv, DOI [arXiv:1701.07875, DOI 10.48550/ARXIV.1701.07875]
   Basri R, 2019, Arxiv, DOI arXiv:1906.00425
   Bińkowski M, 2021, Arxiv, DOI [arXiv:1801.01401, DOI 10.48550/ARXIV.1801.01401]
   Cao Y, 2020, Arxiv, DOI arXiv:1912.01198
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen T, 2019, PROC CVPR IEEE, P12146, DOI 10.1109/CVPR.2019.01243
   Chen YQ, 2021, AAAI CONF ARTIF INTE, V35, P1105
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Duan MX, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3506852
   Durall R., 2020, P IEEE CVF C COMP VI, P7890, DOI DOI 10.1109/CVPR42600.2020.00791
   Dzanic T, 2020, Arxiv, DOI arXiv:1911.06465
   Frank J, 2020, Arxiv, DOI arXiv:2003.08685
   Fuoli D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2340, DOI 10.1109/ICCV48922.2021.00236
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Haohan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8681, DOI 10.1109/CVPR42600.2020.00871
   Hensel M, 2017, ADV NEUR IN, V30
   Lim JH, 2017, Arxiv, DOI arXiv:1705.02894
   Gulrajani I, 2017, ADV NEUR IN, V30
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Jeong J, 2021, Arxiv, DOI arXiv:2103.09742
   Jiang Liming, 2021, P 35 C NEURAL INFORM
   John Xu Zhi-Qin, 2018, arXiv, DOI DOI 10.48550/ARXIV.1811.10146
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Karras Tero, 2020, ADV NEURAL INFO PROC, V33
   Khayatkhoei M, 2020, Arxiv, DOI arXiv:2010.01473
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lee KS, 2021, IEEE WINT CONF APPL, P3941, DOI 10.1109/WACV48630.2021.00399
   Li JS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3418217
   Li ZQ, 2022, LECT NOTES COMPUT SC, V13675, P598, DOI 10.1007/978-3-031-19784-0_35
   Li ZQ, 2022, Arxiv, DOI arXiv:2204.08329
   Li ZQ, 2021, Arxiv, DOI [arXiv:2008.08930, 10.1145/3569928]
   Li ZQ, 2023, IEEE T EM TOP COMP I, V7, P178, DOI 10.1109/TETCI.2022.3193373
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Miyato T, 2018, Arxiv, DOI arXiv:1802.05957
   Nowozin S, 2016, ADV NEUR IN, V29
   Patel P, 2021, IEEE WINT CONF APPL, P3188, DOI 10.1109/WACV48630.2021.00323
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Rahaman N, 2019, PR MACH LEARN RES, V97
   Salimans T, 2016, ADV NEUR IN, V29
   Shamai G, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3337067
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Shin YG, 2021, IEEE T NEUR NET LEAR, V32, P252, DOI 10.1109/TNNLS.2020.2978501
   Sun T, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3475872
   Tancik M, 2020, Arxiv, DOI arXiv:2006.10739
   Tao RT, 2019, IEEE ACCESS, V7, P132594, DOI 10.1109/ACCESS.2019.2941272
   Tran NT, 2021, IEEE T IMAGE PROCESS, V30, P1882, DOI 10.1109/TIP.2021.3049346
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Wang CY, 2018, IEEE T IMAGE PROCESS, V27, P4066, DOI 10.1109/TIP.2018.2836316
   Wang XP, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355397
   Wong CH, 2022, LECT NOTES COMPUT SC, V13682, P682, DOI 10.1007/978-3-031-20047-2_39
   Xia BH, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108552
   Xu ZQJ, 2024, Arxiv, DOI arXiv:1901.06523
   Xu ZQJ, 2019, LECT NOTES COMPUT SC, V11953, P264, DOI 10.1007/978-3-030-36708-4_22
   Xu Zhiqin John, 2018, arXiv
   Yamaguchi Shinya, 2021, ARXIV
   Yang Ceyuan, 2021, ADV NEUR IN, V34
   Yang MP, 2022, LECT NOTES COMPUT SC, V13675, P1, DOI 10.1007/978-3-031-19784-0_1
   Yi-LunWu Hong-Han, 2021, P IEEE CVF INT C COM, P6373
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Yu Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3424116
   Zhang X, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035107
   Zhang Yaoyu, 2019, arXiv
   Zhao Shengyu, 2020, ADV NEURAL INFO PROC, V33
   Zhao ZL, 2020, Arxiv, DOI arXiv:2006.02595
   Zhao ZW, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491225
   Ziqiang Li, 2021, IEEE Transactions on Artificial Intelligence, V2, P58, DOI 10.1109/TAI.2021.3071642
NR 70
TC 3
Z9 3
U1 6
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 153
DI 10.1145/3578585
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300002
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Ma, X
   Yang, XS
   Xu, CS
AF Ma, Xuan
   Yang, Xiaoshan
   Xu, Changsheng
TI Multi-Source Knowledge Reasoning Graph Network for Multi-Modal
   Commonsense Inference
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Knowledge reasoning; multi-modal commonsense inference; graph neural
   network
AB As a crucial part of natural language processing, event-centered commonsense inference task has attracted increasing attention. With a given observed event, the intention and reaction of the people involved in the event are required to be inferred with artificial intelligent algorithms. To solve this problem, sequence-to-sequence methods are widely studied, where the event is first encoded into a specific representation and then decoded to generate the results. However, all the existing methods learn the event representation only with the textual information, while the visual information is ignored, which is actually helpful for the commonsense reference. In this article, we first define a new task of multi-modal commonsense reference with both textual and visual information. A new event-centered multi-modal dataset is also provided. Then we propose a multi-source knowledge reasoning graph network to solve this task, where three kinds of relational knowledge are considered. Multi-modal correlations are learned to get the event's multi-modal representation from a global perspective. Intra-event object relations are explored to capture the fine-grained event feature with an object graph. Inter-event semantic relations are also explored through the external knowledge to understand the semantic associations among events with an event graph. We conduct extensive experiments on the new dataset, and the results show the effectiveness of our method.
C1 [Ma, Xuan; Yang, Xiaoshan; Xu, Changsheng] Univ Chinese Acad Sci, Inst Automat, Chinese Acad Sci, Natl Lab Pattern Recognit, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Ma, Xuan; Yang, Xiaoshan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Ma, Xuan; Yang, Xiaoshan; Xu, Changsheng] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; University of
   Chinese Academy of Sciences, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Peng Cheng Laboratory
RP Ma, X (corresponding author), Univ Chinese Acad Sci, Inst Automat, Chinese Acad Sci, Natl Lab Pattern Recognit, 95 Zhongguancun East Rd, Beijing, Peoples R China.; Ma, X (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, 95 Zhongguancun East Rd, Beijing, Peoples R China.; Ma, X (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
EM maxuan2018@ia.ac.cn; xiaoshan.yang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI yang, xiaoshan/HSE-6093-2023
FU National Key Research and Development Program of China [2018AAA0100604];
   National Natural Science Foundation of China [61720106006, 62036012,
   62072455, 61721004, U1836220, 61872424]; Beijing Natural Science
   Foundation [L201001]
FX This work was supported by National Key Research and Development Program
   of China (No. 2018AAA0100604), National Natural Science Foundation of
   China (No. 61720106006, 62036012, 62072455, 61721004, U1836220,
   61872424), Beijing Natural Science Foundation (L201001).
CR Atwood J, 2016, ADV NEUR IN, V29
   Bajaj M, 2019, IEEE I CONF COMP VIS, P4280, DOI 10.1109/ICCV.2019.00438
   Baker C.F., 1998, P 36 ANN M ASS COMP, P86, DOI [DOI 10.3115/980845.980860, DOI 10.3115/980451.980860]
   Bosselut A, 2019, Arxiv, DOI arXiv:1906.05317
   Cao SS, 2016, AAAI CONF ARTIF INTE, P1145
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Defferrard M, 2016, ADV NEUR IN, V29
   Du L, 2019, Arxiv, DOI arXiv:1909.08824
   Freitag M, 2017, Arxiv, DOI arXiv:1702.01806
   Gallicchio C, 2010, IEEE IJCNN
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Granroth-Wilding M, 2016, AAAI CONF ARTIF INTE, P2727
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henaff M, 2015, Arxiv, DOI arXiv:1506.05163
   Hou JY, 2020, AAAI CONF ARTIF INTE, V34, P10973
   Huang X, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P105, DOI 10.1145/3289600.3290956
   Huang X, 2021, IEEE T CYBERNETICS, V51, P5692, DOI 10.1109/TCYB.2019.2956975
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   LENAT DB, 1995, COMMUN ACM, V38, P33, DOI 10.1145/219717.219745
   Li YG, 2018, Arxiv, DOI [arXiv:1707.01926, DOI 10.48550/ARXIV.1707.01926]
   Li ZY, 2018, Arxiv, DOI [arXiv:1805.05081, DOI 10.48550/ARXIV.1805.05081]
   Lukovnikov D, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1211, DOI 10.1145/3038912.3052675
   Lv SW, 2020, AAAI CONF ARTIF INTE, V34, P8449
   Meyers Adam., 2004, P WORKSH FRONT CORP, P24
   Mikolov T, 2013, Arxiv, DOI [arXiv:1301.3781, 10.48550/arXiv.1301.3781]
   Mostafazadeh Nasrin, 2016, P 2016 C N AM CHAPT, P839, DOI [DOI 10.18653/V1/N16-1098, 10.18653/v1/N16-1098]
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Nian FD, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P411, DOI 10.1145/3123266.3123443
   Palmer M, 2005, COMPUT LINGUIST, V31, P71, DOI 10.1162/0891201053630264
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pichotta K, 2016, AAAI CONF ARTIF INTE, P2800
   Pitner G, 2020, INT EL DEVICES MEET, DOI 10.1109/IEDM13553.2020.9371899
   Radford A, Improving language understanding by generative pre-training
   Rashkin H, 2019, Arxiv, DOI arXiv:1805.06939
   Rashkin H, 2018, Arxiv, DOI arXiv:1805.06533
   Sap M, 2019, AAAI CONF ARTIF INTE, P3027
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Seo Y, 2018, LECT NOTES COMPUT SC, V11301, P362, DOI 10.1007/978-3-030-04167-0_33
   Serban IV, 2017, AAAI CONF ARTIF INTE, P3295
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Vaswani A, 2023, Arxiv, DOI [arXiv:1706.03762, DOI 10.48550/ARXIV.1706.03762, 10.48550/arXiv.1706.03762]
   Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753
   Wang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P950, DOI 10.1145/3292500.3330989
   Wang Z., 2017, P 2017 C EMP METH NA, P57
   Wen Z, 2021, IEEE T CIRC SYST VID, V31, P1042, DOI 10.1109/TCSVT.2020.2991866
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Yang Li, 2022, PROC IEEECVF C COMPU, P9499
   Yang Y., 2021, P IEEE CVF INT C COM, P1867
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yifei Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14021, DOI 10.1109/CVPR42600.2020.01404
   Yuan CX, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1763, DOI 10.1145/3340531.3411895
   Zhang HM, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P201, DOI 10.1145/3366423.3380107
NR 52
TC 0
Z9 0
U1 2
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2023
VL 19
IS 4
AR 141
DI 10.1145/3573201
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FX8
UT WOS:001011937600003
OA Bronze
DA 2024-07-18
ER

PT J
AU Wu, J
   Zhu, TL
   Zhu, JH
   Li, TY
   Wang, CZ
AF Wu, Jun
   Zhu, Tianliang
   Zhu, Jiahui
   Li, Tianyi
   Wang, Chunzhi
TI A Optimized BERT for Multimodal Sentiment Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE HG-BERT; multi-head self-attention mechanism; multimodal sentiment
   analysis; gate channel; tensor fusion network
AB Sentiment analysis of one modality (e.g., text or image) has been broadly studied. However, not much attention has been paid to the sentiment analysis of multi-modal data. As the research on and applications of multimodal data analysis are becoming more and more broad, it is necessary to optimize BERT internal structure. This article proposes a hierarchical multi-head self-attention and gate channel BERT, which is an optimized BERT model. The model is composed of three modules: the hierarchical multi-head self-attention module realizes the hierarchical extraction process of features; the gate channel module replaces BERT's original Feed Forward layer to realize information filtering; and the tensor fusion model based on a self-attention mechanism is utilized to implement the fusion process of different modal features. Experiments show that our method achieves promising results and improves accuracy by 5-6% when compared with traditional models on the CMU-MOSI dataset.
C1 [Wu, Jun; Zhu, Tianliang; Zhu, Jiahui; Li, Tianyi; Wang, Chunzhi] Hubei Univ Technol, Sch Comp Sci, Wuhan 430068, Hubei, Peoples R China.
C3 Hubei University of Technology
RP Wu, J (corresponding author), Hubei Univ Technol, Sch Comp Sci, Wuhan 430068, Hubei, Peoples R China.
EM wujun@whut.edu.cn; 913829070@qq.com; 314328625@qq.com; 469216198@qq.com;
   lavazza@foxmail.com
OI WU, Jun/0000-0002-9683-053X
FU National Natural Science Foundation of China [61602161, 61772180]; Hubei
   Province Science and Technology Support Project [2020BAB012];
   Fundamental Research Funds for the Research Fund of Key Lab of Traffic
   and Internet of Things [WUT:2015-015-A03]
FX This work is supported by the National Natural Science Foundation of
   China (Grant No. 61602161, 61772180), Hubei Province Science and
   Technology Support Project (Grant No: 2020BAB012), The Fundamental
   Research Funds for the Research Fund of Key Lab of Traffic and Internet
   of Things (WUT:2015-015-A03).
CR [Anonymous], 2018, DEEP CONTEXTUALIZED, DOI DOI 10.18653/V1/N18-1202
   Arjmand M, 2021, Arxiv, DOI arXiv:2109.05522
   Chen Minghai, 2017, P 19 ACM INT C MULTI, P163, DOI [10.1145/3136755.3136801, DOI 10.1145/3136755.3136801]
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, DOI 10.3115/V1/D14-1179, 10.48550/ARXIV.1406.1078, DOI 10.48550/ARXIV.1406.1078]
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   He Pengcheng, 2020, arXiv, DOI 10.48550/arXiv.2006.03654
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang FR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3388861
   Tsai YHH, 2019, Arxiv, DOI arXiv:1806.06176
   Leng XL, 2021, MULTIMED TOOLS APPL, V80, P12581, DOI 10.1007/s11042-020-10336-3
   Liu L, 2021, ACM T ASIAN LOW-RESO, V20, DOI 10.1145/3464425
   Liu RY, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3300939
   Liu YH, 2019, Arxiv, DOI arXiv:1907.11692
   Liu Z, 2018, Arxiv, DOI [arXiv:1806.00064, 10.18653/v1/p18-1209, DOI 10.48550/ARXIV.1806.00064]
   Morency L.-P., 2011, P 13 INT C MULT INT, P169, DOI DOI 10.1145/2070481.2070509
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Liang PP, 2018, Arxiv, DOI arXiv:1808.03920
   Pu XJ, 2019, MULTIMEDIA SYST, V25, P21, DOI 10.1007/s00530-017-0550-0
   Ruiqi Chen, 2019, IOP Conference Series: Materials Science and Engineering, V490, DOI 10.1088/1757-899X/490/6/062063
   Sun CG, 2021, ACM T ASIAN LOW-RESO, V20, DOI 10.1145/3402886
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Usama M, 2020, FUTURE GENER COMP SY, V113, P571, DOI 10.1016/j.future.2020.07.022
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang W, 2019, Arxiv, DOI arXiv:1908.04577
   Wöllmer M, 2013, IEEE INTELL SYST, V28, P46, DOI 10.1109/MIS.2013.34
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3458281
   Yang KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P521, DOI 10.1145/3394171.3413690
   Zadeh A, 2017, Arxiv, DOI [arXiv:1707.07250, DOI 10.48550/ARXIV.1707.07250]
   Zadeh A, 2016, Arxiv, DOI arXiv:1606.06259
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
NR 30
TC 4
Z9 5
U1 17
U2 45
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 91
DI 10.1145/3566126
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300016
OA hybrid
DA 2024-07-18
ER

PT J
AU Qiao, Y
   Liu, YH
   Wei, ZQ
   Wang, YX
   Cai, Q
   Zhang, GF
   Yang, X
AF Qiao, Yu
   Liu, Yuhao
   Wei, Ziqi
   Wang, Yuxin
   Cai, Qiang
   Zhang, Guofeng
   Yang, Xin
TI Hierarchical and Progressive Image Matting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image matting; alpha matte; hierarchical; progressive; attention
AB Most matting research resorts to advanced semantics to achieve high-quality alpha mattes, and a direct low-level features combination is usually explored to complement alpha details. However, we argue that appearance-agnostic integration can only provide biased foreground (FG) details and that alpha mattes require different-level feature aggregation for better pixel-wise opacity perception. In this article, we propose an end-to-end hierarchical and progressive attention matting network (HAttMatting++), which can better predict the opacity of the FG from single RGB imageswithout additional input. Specifically, we utilize channel-wise attention (CA) to distill pyramidal features and employ spatial attention (SA) at different levels to filter appearance cues. This progressive attention mechanism can estimate alpha mattes from adaptive semantics and semantics-indicated boundaries. We also introduce a hybrid loss function fusing structural similarity, mean square error, adversarial loss, and sentry supervision to guide the network to further improve the overall FG structure. In addition, we construct a large-scale and challenging image matting dataset comprised of 59,000 training images and 1,000 test images (a total of 646 distinct FG alpha mattes), which can further improve the robustness of our hierarchical and progressive aggregationmodel. Extensive experiments demonstrate that the proposed HAttMatting++ can capture sophisticated FG structures and achieve state-of-the-art performance with single RGB images as input.
C1 [Qiao, Yu; Liu, Yuhao; Wang, Yuxin; Yang, Xin] Dalian Univ Technol, 2 Linggong Rd, Dalian 116024, Liaoning, Peoples R China.
   [Wei, Ziqi] Inst Automat, CAS Key Lab Mol Imaging, Beijing 100190, Peoples R China.
   [Cai, Qiang] Beijing Technol & Business Univ, Beijing 100048, Peoples R China.
   [Zhang, Guofeng] Wonxing Technol, 14 Haitian 2 Rd, Shenzhen 518110, Guangdong, Peoples R China.
C3 Dalian University of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Beijing Technology & Business University
RP Wang, YX (corresponding author), Dalian Univ Technol, 2 Linggong Rd, Dalian 116024, Liaoning, Peoples R China.; Wei, ZQ (corresponding author), Inst Automat, CAS Key Lab Mol Imaging, Beijing 100190, Peoples R China.
EM qiaoyu2020@mail.dlut.edu.cn; yuhaoliu7456@gmail.com; ziqi.wei@ia.ac.cn;
   wyx@dlut.edu.cn; caiq@th.btbu.edu.cn; zhangguofeng@wonxing.com;
   xinyang@dlut.edu.cn
RI Wang, Yuxin/ABF-2724-2020; Guofeng, Zhang/J-9769-2016
OI Guofeng, Zhang/0000-0003-1180-4170; Wei, Ziqi/0000-0001-9402-8386; Liu,
   Yuhao/0000-0003-0550-4788
FU National Natural Science Foundation of China [61972067/U 21A20491];
   Innovation Technology Funding of Dalian [2020JJ26GX036]; Open Research
   Fund of Beijing Key Laboratory of Big Data Technology for Food Safety
   [BTBD-2018KF]; National Key Research and Development Program of China
   [2022ZD0210500]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61972067/U 21A20491, the Innovation
   Technology Funding of Dalian (2020JJ26GX036), the Open Research Fund of
   Beijing Key Laboratory of Big Data Technology for Food Safety (Project
   No. BTBD-2018KF), and the National Key Research and Development Program
   of China (No. 2022ZD0210500).
CR Aksoy Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201275
   Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32
   [Anonymous], 2019, P IEEECVF INT C COMP
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P618, DOI 10.1145/3240508.3240610
   Cho D, 2019, IEEE T IMAGE PROCESS, V28, P1054, DOI 10.1109/TIP.2018.2872925
   Cho D, 2017, IEEE T PATTERN ANAL, V39, P1504, DOI 10.1109/TPAMI.2016.2606397
   Dai YT, 2021, PROC CVPR IEEE, P6837, DOI 10.1109/CVPR46437.2021.00677
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guan Y, 2006, COMPUT GRAPH FORUM, V25, P567, DOI 10.1111/j.1467-8659.2006.00976.x
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495
   Karacan L, 2015, IEEE I CONF COMP VIS, P424, DOI 10.1109/ICCV.2015.56
   Lee P., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2193, DOI 10.1109/CVPR.2011.5995665
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P1699, DOI 10.1109/TPAMI.2008.168
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li YY, 2020, AAAI CONF ARTIF INTE, V34, P11450
   Lin SC, 2021, PROC CVPR IEEE, P8758, DOI 10.1109/CVPR46437.2021.00865
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2015, Arxiv, DOI [arXiv:1506.04579, 10.48550/arXiv.1506.04579]
   Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336
   Lutz S., 2018, BMVC
   Mei HY, 2022, IEEE T CIRC SYST VID, V32, P1378, DOI 10.1109/TCSVT.2021.3069848
   Mei HY, 2020, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR42600.2020.00374
   Qiao Y, 2020, COMPUT GRAPH FORUM, V39, P565, DOI 10.1111/cgf.14168
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR.2018.00395
   Sengupta S, 2020, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR42600.2020.00236
   Shahrian E, 2013, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2013.88
   Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6
   Sun YA, 2021, PROC CVPR IEEE, P11115, DOI 10.1109/CVPR46437.2021.01097
   Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317
   Tian X, 2022, INT J COMPUT VISION, V130, P729, DOI 10.1007/s11263-021-01553-w
   Tian Xin, 2020, P BRIT MACHINE VISIO
   Tian Xin, 2022, P IEEE C COMPUTER VI
   Wan RJ, 2018, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2018.00502
   Wang J, 2007, PROC CVPR IEEE, P281
   Wang Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P999
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei T., 2021, P IEEE C COMP VIS PA, P15374
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiangli Yuanbo, 2020, P INT C LEARNING REP
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Yang X, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3408323
   Yang X, 2018, ADV NEUR IN, V31
   Yu HC, 2021, AAAI CONF ARTIF INTE, V35, P3217
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13673, DOI 10.1109/CVPR42600.2020.01369
   Yu QH, 2021, PROC CVPR IEEE, P1154, DOI 10.1109/CVPR46437.2021.00121
   Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765
   Zheng YJ, 2009, IEEE I CONF COMP VIS, P889, DOI 10.1109/ICCV.2009.5459326
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 60
TC 5
Z9 5
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 52
DI 10.1145/3540201
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Qin, QP
   Jung, C
AF Qin, Qipu
   Jung, Cheolkon
TI Quality Enhancement of Compressed 360-Degree Videos Using Viewport-based
   Deep Neural Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 360-degree video; video compression; viewport prediction; viewport
   quality enhancement; offset field; residual enhancement
ID RATE-DISTORTION OPTIMIZATION; SALIENCY PREDICTION; ARTIFACT REDUCTION;
   CNN
AB 360-degree video provides omnidirectional views by a bounding sphere, thus also called omnidirectional video. For omnidirectional video, people can only see specific content in the viewport through head movement, i.e., only a small portion of the 360-degree content is exposed at a given time. Therefore, the viewport quality is of particular importance for 360-degree videos. In this article, we propose a quality enhancement of compressed 360-degree videos using viewport-based deep neural networks, named V-DNN. V-DNN is mainly composed of two modules: viewport prediction network (VPN) and viewport quality enhancement network (VQEN). VPN based on spherical convolution and 2D convolution generates potential viewports for omnidirectional video. VQEN takes the current viewport and its reference viewports as the input and enhances residual for the current viewport based on bidirectional offset prediction and Spatio-temporal deformable convolutions. Compared with HM16.16 baseline at QP = 37 under the Low Delay P (LDP) configuration, experimental results show that V-DNN achieves an average 0.605 dB and 0.0139 gains in viewport-based.PSNR and.MS-SSIM, respectively, and is 0.379 dB (59.63%) and 0.0073 (110.61%) higher than the multiframe quality enhancement (MFQE-2.0) scheme at QP = 37, respectively. Moreover, V-DNN consistently outperforms MFQE-1.0, MFQE-2.0, and HM16.16 baseline at the other QPs in terms of Delta PSNR, Delta WS-PSNR, and.MS-SSIM.
C1 [Qin, Qipu; Jung, Cheolkon] Xidian Univ, 2 South Taibai Rd, Xian 710071, Peoples R China.
C3 Xidian University
RP Jung, C (corresponding author), Xidian Univ, 2 South Taibai Rd, Xian 710071, Peoples R China.
EM kippqin@stu.xidian.edu.cn; zhengzk@xidian.edu.cn
FU National Natural Science Foundation of China [61872280, 62111540272]
FX This work was supported by the National Natural Science Foundation of
   China (Nos. 61872280 and 62111540272).
CR Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Carreira J, 2021, IEEE IMAGE PROC, P2149, DOI 10.1109/ICIP42928.2021.9506358
   Chang HB, 2014, IEEE T SIGNAL PROCES, V62, P718, DOI 10.1109/TSP.2013.2290508
   Chen HG, 2022, IEEE T NEUR NET LEAR, V33, P430, DOI 10.1109/TNNLS.2021.3124370
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cohen T. S., 2018, P INT C LEARNING REP
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Ding DD, 2020, IEEE T CIRC SYST VID, V30, P1871, DOI 10.1109/TCSVT.2019.2935508
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Fu XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4066, DOI 10.1109/ICCV48922.2021.00405
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Guo HW, 2020, IEEE T BROADCAST, V66, P113, DOI 10.1109/TBC.2019.2917402
   Han QL, 2015, IEEE IMAGE PROC, P1905, DOI 10.1109/ICIP.2015.7351132
   He YW, 2018, PICT COD SYMP, P313, DOI 10.1109/PCS.2018.8456280
   Huang HY, 2021, IEEE T CIRC SYST VID, V31, P2100, DOI 10.1109/TCSVT.2020.3018230
   Jancsary J, 2012, LECT NOTES COMPUT SC, V7578, P112, DOI 10.1007/978-3-642-33786-4_9
   Jung C, 2012, SIGNAL PROCESS-IMAGE, V27, P663, DOI 10.1016/j.image.2012.03.002
   Kells LymanMorse., 1940, PLANE SPHERICAL TRIG
   Kim TH, 2018, LECT NOTES COMPUT SC, V11207, P111, DOI 10.1007/978-3-030-01219-9_7
   Kim YW, 2017, IEEE I CONF COMP VIS, P4753, DOI 10.1109/ICCV.2017.508
   Lebreton P, 2018, SIGNAL PROCESS-IMAGE, V69, P69, DOI 10.1016/j.image.2018.03.006
   Li C, 2019, PROC CVPR IEEE, P10169, DOI 10.1109/CVPR.2019.01042
   Li C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P932, DOI 10.1145/3240508.3240581
   Li JW, 2020, IEEE T IMAGE PROCESS, V29, P8842, DOI 10.1109/TIP.2020.3020389
   Li K, 2017, IEEE INT CON MULTI, P1320, DOI 10.1109/ICME.2017.8019416
   Li L, 2020, IEEE J-STSP, V14, P130, DOI 10.1109/JSTSP.2019.2963154
   Li L, 2017, IEEE INT CON MULTI, P55, DOI 10.1109/ICME.2017.8019393
   Li Y, 2021, IEEE T BROADCAST, V67, P500, DOI 10.1109/TBC.2021.3068871
   Li YM, 2019, IEEE T CIRC SYST VID, V29, P1767, DOI 10.1109/TCSVT.2018.2846042
   Li YM, 2017, IEEE INT CON MULTI, P709, DOI 10.1109/ICME.2017.8019492
   Ling J, 2018, SIGNAL PROCESS-IMAGE, V69, P60, DOI 10.1016/j.image.2018.03.007
   Ma D, 2021, IEEE J-STSP, V15, P378, DOI 10.1109/JSTSP.2020.3043064
   Pan ZQ, 2020, IEEE T IMAGE PROCESS, V29, P5352, DOI 10.1109/TIP.2020.2982534
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Shang XW, 2022, IEEE T CIRC SYST VID, V32, P802, DOI 10.1109/TCSVT.2021.3062402
   Snyder J. P., 1987, Report 1395, V1395
   Storch I, 2022, IEEE T CIRC SYST VID, V32, P3235, DOI 10.1109/TCSVT.2021.3096752
   Su YC, 2022, IEEE T PATTERN ANAL, V44, P8371, DOI 10.1109/TPAMI.2021.3113612
   Su YC, 2021, IEEE T PATTERN ANAL, V43, P2697, DOI 10.1109/TPAMI.2020.2974472
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun MD, 2020, NEUROCOMPUTING, V384, P335, DOI 10.1016/j.neucom.2019.12.015
   Sun Y., 2017, P IEEE C VIS COMM IM, P1, DOI [10.1109/VCIP.2017.8305087, DOI 10.1109/VCIP.2017.8305087]
   Sun Y. L., 2016, JOINT VIDEO EXPLORAT
   Tan TK, 2016, IEEE T CIRC SYST VID, V26, P76, DOI 10.1109/TCSVT.2015.2477916
   Wang DZ, 2021, IEEE T IMAGE PROCESS, V30, P4198, DOI 10.1109/TIP.2021.3068638
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu M, 2022, IEEE T PATTERN ANAL, V44, P2198, DOI 10.1109/TPAMI.2020.3028509
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang R, 2019, IEEE T CIRC SYST VID, V29, P2039, DOI 10.1109/TCSVT.2018.2867568
   Yang R, 2018, PROC CVPR IEEE, P6664, DOI 10.1109/CVPR.2018.00697
   Yang R, 2018, IEEE T BROADCAST, V64, P865, DOI 10.1109/TBC.2018.2795459
   Yang R, 2017, IEEE INT CON MULTI, P817, DOI 10.1109/ICME.2017.8019299
   Yoo SB, 2014, IEEE T MULTIMEDIA, V16, P1536, DOI 10.1109/TMM.2014.2327563
   Zhang J, 2016, IEEE DATA COMPR CONF, P91, DOI 10.1109/DCC.2016.105
   Zhang XF, 2013, IEEE T IMAGE PROCESS, V22, P4613, DOI 10.1109/TIP.2013.2274386
   Zhu YC, 2022, IEEE T CIRC SYST VID, V32, P4188, DOI 10.1109/TCSVT.2021.3126590
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 63
TC 1
Z9 1
U1 7
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 74
DI 10.1145/3551641
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000024
DA 2024-07-18
ER

PT J
AU Zhou, W
   Xia, ZW
   Dou, P
   Su, T
   Hu, HF
AF Zhou, Wei
   Xia, Zhiwu
   Dou, Peng
   Su, Tao
   Hu, Haifeng
TI Aligning Image Semantics and Label Concepts for Image Multi-Label
   Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-label classification; transformer; self-attention; salient
   features; label correlation; visual analysis
ID NETWORK
AB Image multi-label classification task is mainly to correctly predict multiple object categories in the images. To capture the correlation between labels, graph convolution network based methods have to manually count the label co-occurrence probability from training data to construct a pre-defined graph as the input of graph network, which is inflexible and may degrade model generalizability. Moreover, most of the current methods cannot effectively align the learned salient object features with the label concepts, so that the predicted results of model may not be consistent with the image content. Therefore, how to learn the salient semantic features of images and capture the correlation between labels, and then effectively align them is one of the key to improve the performance of image multi-label classification task. To this end, we propose a novel image multi-label classification framework which aims to align Image Semantics with Label Concepts (ISLC). Specifically, we propose a residual encoder to learn salient object features in the images, and exploit the selfattention layer in aligned decoder to automatically capture the correlation between labels. Then, we leverage the cross-attention layers in aligned decoder to align image semantic features with label concepts, so as to make the labels predicted by model more consistent with image content. Finally, the output features of the last layer of residual encoder and aligned decoder are fused to obtain the final output feature for classification. The proposed ISLC model achieves good performance on various prevalent multi-label image datasets such as MS-COCO 2014, PASCAL VOC 2007, VG-500, and NUS-WIDE with 87.2%, 96.9%, 39.4%, and 64.2%, respectively.
C1 [Zhou, Wei; Xia, Zhiwu; Dou, Peng; Su, Tao; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM zhouw75@mail2.sysu.edu.cn; xiazhw@mail2.sysu.edu.cn;
   doupg@mail2.sysu.edu.cn; sutao@mail.sysu.edu.cn; huhaif@mail.sysu.edu.cn
RI Peng/AAP-6413-2021
OI Zhou, Wei/0000-0002-9237-7205; Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [62076262, 61673402,
   61273270, 60802069]; Science and Technology Program of Guangdong
   Province [2021B1101270007, 2019B010140002]
FX This work was supported in part by the National Natural Science
   Foundation of China (62076262, 61673402, 61273270, 60802069), and in
   part by the Science and Technology Program of Guangdong Province
   (2021B1101270007, 2019B010140002).
CR Cevikalp Hakan, 2019, CVPR WORKSHOPS, P9
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Chu XX, 2021, Arxiv, DOI [arXiv:2102.10882, DOI 10.48550/ARXIV.2102.10882]
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Dai ZH, 2019, Arxiv, DOI [arXiv:1901.02860, DOI 10.48550/ARXIV.1901.02860]
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duo Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P615, DOI 10.1007/978-3-030-58589-1_37
   Durand T, 2019, IEEE T PATTERN ANAL, V41, P337, DOI 10.1109/TPAMI.2017.2788435
   Durand T, 2017, PROC CVPR IEEE, P5957, DOI 10.1109/CVPR.2017.631
   Dutta Ayushi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P191, DOI 10.1007/978-3-030-58526-6_12
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao BB, 2021, IEEE T IMAGE PROCESS, V30, P5920, DOI 10.1109/TIP.2021.3088605
   Gong YC, 2014, Arxiv, DOI arXiv:1312.4894
   Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082
   Guo JY, 2020, AAAI CONF ARTIF INTE, V34, P10885
   Guo JY, 2021, IEEE T CIRC SYST VID, V31, P1114, DOI 10.1109/TCSVT.2020.2996231
   Guo JY, 2020, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR42600.2020.00158
   Gupta S, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3436494
   Hassanin M, 2022, J VIS COMMUN IMAGE R, V83, DOI 10.1016/j.jvcir.2022.103448
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2021, Arxiv, DOI arXiv:2012.11747
   Hu YT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446208
   Huang FR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3388861
   Huang ZC, 2020, Arxiv, DOI arXiv:2004.00849
   Ji WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446792
   Jin JR, 2016, INT C PATT RECOG, P2452, DOI 10.1109/ICPR.2016.7900004
   Jin Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P649, DOI 10.1007/978-3-030-58589-1_39
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lanchantin J, 2021, PROC CVPR IEEE, P16473, DOI 10.1109/CVPR46437.2021.01621
   Li JB, 2020, LECT NOTES COMPUT SC, V12396, P736, DOI 10.1007/978-3-030-61609-0_58
   Li L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3359753
   Li Q, 2019, Arxiv, DOI arXiv:1909.13005
   Li ZX, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3426974
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu LQ, 2017, IEEE T PATTERN ANAL, V39, P2335, DOI 10.1109/TPAMI.2017.2651061
   Liu LC, 2019, INT CONF ACOUST SPEE, P1682, DOI 10.1109/ICASSP.2019.8683665
   Lyu F, 2018, IEEE INT SM C CONF
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Meng Q., 2019, PROC ACM MULTIMEDIA, P1
   Nguyen HD, 2021, AAAI CONF ARTIF INTE, V35, P9092
   Pu T, 2023, Arxiv, DOI arXiv:2204.03795
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sun Dengdi, 2022, COGN COMPUT, V9, P1
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Vu XS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2299, DOI 10.1145/3394171.3414047
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Xiaomei, 2021, 2021 IEEE INT C MULT, P1
   Wang Y, 2020, AAAI CONF ARTIF INTE, V34, P12265
   Wang YT, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1575, DOI 10.1145/3340531.3411880
   Wang Z, 2022, IEEE T CIRC SYST VID, V32, P1848, DOI 10.1109/TCSVT.2021.3083978
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Wu XP, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P284, DOI 10.1145/3394171.3414046
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan Z, 2019, IEEE ACCESS, V7, P98005, DOI 10.1109/ACCESS.2019.2929512
   Yang H, 2016, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2016.37
   Yazici Vacit Oguz, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13437, DOI 10.1109/CVPR42600.2020.01345
   You RC, 2020, AAAI CONF ARTIF INTE, V34, P12709
   Yu WJ, 2019, PATTERN RECOGN, V91, P322, DOI 10.1016/j.patcog.2019.03.006
   Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhao HY, 2020, IEEE ACCESS, V8, P225539, DOI 10.1109/ACCESS.2020.3044446
   Zhao JW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P163, DOI 10.1109/ICCV48922.2021.00023
   Zhao LC, 2021, IEEE T CIRC SYST VID, V31, P4735, DOI 10.1109/TCSVT.2021.3102025
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou FT, 2022, IEEE T CIRC SYST VID, V32, P4513, DOI 10.1109/TCSVT.2021.3128054
   Zhou W, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3519030
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
   Zhu K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P184, DOI 10.1109/ICCV48922.2021.00025
NR 74
TC 3
Z9 3
U1 5
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 75
DI 10.1145/3550278
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000025
DA 2024-07-18
ER

PT J
AU Yadav, A
   Vishwakarma, DK
AF Yadav, Ashima
   Vishwakarma, Dinesh Kumar
TI A Deep Multi-level Attentive Network for Multimodal Sentiment Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Attention; deep learning; multimodal analysis; sentiment analysis
ID FUSION
AB Multimodal sentiment analysis has attracted increasing attention with broad application prospects. Most of the existing methods have focused on a single modality, which fails to handle social media data due to its multiple modalities. Moreover, in multimodal learning, most of the works have focused on simply combining the two modalities without exploring the complicated correlations between them. This resulted in dissatisfying performance for multimodal sentiment classification. Motivated by the status quo, we propose a Deep Multi-level Attentive network (DMLANet), which exploits the correlation between image and text modalities to improve multimodal learning. Specifically, we generate the bi-attentive visual map along the spatial and channel dimensions to magnify Convolutional neural network representation power. Then, we model the correlation between the image regions and semantics of the word by extracting the textual features related to the bi-attentive visual features by applying semantic attention. Finally, self-attention is employed to fetch the sentiment-rich multimodal features for the classification automatically. We conduct extensive evaluations on four real-world datasets, namely, MVSA-Single, MVSA-Multiple, Flickr, and Getty Images, which verify our method's superiority.
C1 [Yadav, Ashima] Bennett Univ, Dept Comp Sci & Engn, Plot 8-11,Tech Zone 2, Greater Noida 201310, Uttar Pradesh, India.
   [Vishwakarma, Dinesh Kumar] Delhi Technol Univ, Dept Informat Technol, Bawana Rd, New Delhi 110042, India.
C3 Delhi Technological University
RP Yadav, A (corresponding author), Bennett Univ, Dept Comp Sci & Engn, Plot 8-11,Tech Zone 2, Greater Noida 201310, Uttar Pradesh, India.
EM ashimayadavdtu@gmail.com; dvishwakarma@gmail.com
RI Yadav, Ashima/AAS-1004-2021; VISHWAKARMA, DINESH KUMAR/L-3815-2018
OI Yadav, Ashima/0000-0002-1467-1601; VISHWAKARMA, DINESH
   KUMAR/0000-0002-1026-0047
CR Akbari H., 2021, 35 C NEURAL INFORM P
   Ankita S., 2020, Procedia Comput. Sci, V173, P325, DOI [10.1016/j.procs.2020.06.038, DOI 10.1016/J.PROCS.2020.06.038]
   [Anonymous], 2013, 21 ACM INT C MULTIME
   Baecchi C, 2016, MULTIMED TOOLS APPL, V75, P2507, DOI 10.1007/s11042-015-2646-x
   Barrett M., 2018, P 22 C COMP NAT LANG, P302, DOI 10.18653/v1/k18
   Chan SWK, 2017, DECIS SUPPORT SYST, V94, P53, DOI 10.1016/j.dss.2016.10.006
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Dai S., 2018, IEEE C MULTIMEDIA IN
   Denecke K, 2015, ARTIF INTELL MED, V64, P17, DOI 10.1016/j.artmed.2015.03.006
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Huang FR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3388861
   Huang FR, 2019, KNOWL-BASED SYST, V167, P26, DOI 10.1016/j.knosys.2019.01.019
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Jiang T., 2020, PACIFICASIA C KNOWLE
   Lu JS, 2019, ADV NEUR IN, V32
   Ma HJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3109
   Niu T., 2016, INT C MULTIMEDIA MOD
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.178, 10.1109/ICDM.2016.0055]
   Radford A, 2021, PR MACH LEARN RES, V139
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Selvaraju R. R., 2016, Grad-cam: Why did you say that? arXiv preprint arXiv:1611.07450
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vadicamo L, 2017, IEEE INT CONF COMP V, P308, DOI 10.1109/ICCVW.2017.45
   Battaglia PW, 2018, Arxiv, DOI [arXiv:1806.01261, DOI 10.48550/ARXIV.1806.01261, 10.48550/arXiv.1806.01261]
   Wang JB, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107075
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xi C, 2020, ICMLSC 2020: PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND SOFT COMPUTING, P34, DOI 10.1145/3380688.3380693
   Xu J, 2019, KNOWL-BASED SYST, V178, P61, DOI 10.1016/j.knosys.2019.04.018
   Xu J, 2019, APPL SOFT COMPUT, V80, P387, DOI 10.1016/j.asoc.2019.04.010
   Xu JZ, 2021, IMPACT ASSESS PROJ A, V39, P429, DOI [10.1109/TR.2020.3040191, 10.1080/14615517.2020.1848242]
   Xu N., 2017, IEEE 2 INT C BIG DAT
   Xu N., 2018, 41 INT ACM SIGIR C R
   Xu N., 2017, IEEE INT C INTELLIGE
   Xu N, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2399, DOI 10.1145/3132847.3133142
   Yadav A, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106624
   Yadav A, 2020, MULTIMEDIA SYST, V26, P431, DOI 10.1007/s00530-020-00656-7
   Yadav A, 2020, ARTIF INTELL REV, V53, P4335, DOI 10.1007/s10462-019-09794-5
   Yang KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P521, DOI 10.1145/3394171.3413690
   You Q., 2016, P 9 ACM INT C WEB SE
   Yu JF, 2020, IEEE-ACM T AUDIO SPE, V28, P429, DOI 10.1109/TASLP.2019.2957872
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zhang K, 2020, IEEE ACCESS, V8, P35276, DOI 10.1109/ACCESS.2020.2975036
   Zhang W, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3292059
   Zhao S., 2020, ACM T MULTIM COMPUT, V15, P1
   Zhao ZY, 2019, INFORM PROCESS MANAG, V56, DOI 10.1016/j.ipm.2019.102097
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 47
TC 16
Z9 16
U1 26
U2 90
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 15
DI 10.1145/3517139
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, SG
   Wang, HX
   Pei, M
AF Liu, Shiguang
   Wang, Huixin
   Pei, Min
TI Facial-expression-aware Emotional Color Transfer Based on Convolutional
   Neural Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Facial-expression; emotional color transfer; emotion classification
   network; Face-Emotion database
ID IMAGE; VALENCE
AB Emotional color transfer aims to change the evoked emotion of a source image to that of a target image by adjusting color distribution. Most of existing emotional color transfer methods only consider the low-level visual features of an image and ignore the facial expression features when the image contains a human face, which would cause incorrect emotion evaluation for the given image. In addition, previous emotional color transfer methods may easily result in ambiguity between the emotion of resulting image and target image. For example, if the background of the target image is dark while the facial expression is happiness, then previous methods would directly transfer dark color to the source image, neglecting the facial emotion in the image. To solve this problem, we propose a new facial-expression-aware emotional color transfer framework. Given a target image with facial expression features, we first predict the facial emotion label of the image through the emotion classification network. Then, facial emotion labels are matched with pre-trained emotional color transfer models. Finally, we use the matched emotion model to transfer the color of the target image to the source image. Considering none of the existing emotion image databases, which focus on images that contain face and background, we built an emotion database for our new emotional color transfer framework that is called "Face-Emotion database." Experiments demonstrate that our method can successfully capture and transfer facial emotions, outperforming state-of-the-art methods.
C1 [Liu, Shiguang; Wang, Huixin; Pei, Min] Tianjin Univ, Coll Intelligence & Comp, Sch Comp Sci & Technol, 135 Yaguan Rd,Haihe Educ Pk, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Sch Comp Sci & Technol, 135 Yaguan Rd,Haihe Educ Pk, Tianjin 300350, Peoples R China.
EM lsg@tju.edu.cn; huixinw@tju.edu.cn; minmin@tju.edu.cn
FU Natural Science Foundation of China [62072328, 61672375]
FX This work was partly supported by the Natural Science Foundation of
   China under Grants No. 62072328 and No. 61672375.
CR Alameda-Pineda X, 2016, PROC CVPR IEEE, P5240, DOI 10.1109/CVPR.2016.566
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Cao X, 2014, SCI CHINA EARTH SCI, V57, P2330, DOI 10.1007/s11430-014-4929-x
   Chang FJ, 2018, IEEE INT CONF AUTOMA, P122, DOI 10.1109/FG.2018.00027
   Dan-Glauser ES, 2011, BEHAV RES METHODS, V43, P468, DOI 10.3758/s13428-011-0064-1
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   EKMAN P, 1986, MOTIV EMOTION, V10, P159, DOI 10.1007/BF00992253
   Fried O, 2017, COMPUT GRAPH FORUM, V36, P183, DOI 10.1111/cgf.13284
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Habimana O, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-018-9941-6
   Han Y, 2017, IEEE T MULTIMEDIA, V19, P80, DOI 10.1109/TMM.2016.2608000
   He L, 2015, SIGNAL IMAGE VIDEO P, V9, P1965, DOI 10.1007/s11760-014-0691-y
   Huang YF, 2021, IEEE T MULTIMEDIA, V23, P176, DOI 10.1109/TMM.2020.2981994
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kim HR, 2018, IEEE T MULTIMEDIA, V20, P2980, DOI 10.1109/TMM.2018.2827782
   Lang P. J., 1999, A4 U FLOR CTR RES PS
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Liu D, 2018, PATTERN RECOGN LETT, V110, P16, DOI 10.1016/j.patrec.2018.03.015
   Liu MY, 2015, LECT NOTES COMPUT SC, V9006, P143, DOI 10.1007/978-3-319-16817-3_10
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Rao TR, 2016, IEEE IMAGE PROC, P634, DOI 10.1109/ICIP.2016.7532434
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ryoo S., 2014, Int. J. Softw. Eng. Appl, V8, P227
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Solli M., 2010, Proceedings of the ACM International Conference on Image and Video Retrieval, P398
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Sreedharan NPN, 2018, IET BIOMETRICS, V7, P490, DOI 10.1049/iet-bmt.2017.0160
   Su YY, 2019, SOFT COMPUT, V23, P1007, DOI 10.1007/s00500-017-2814-1
   Wang XH, 2013, IEEE IMAGE PROC, P3230, DOI 10.1109/ICIP.2013.6738665
   Wang XH, 2013, VISUAL COMPUT, V29, P1121, DOI 10.1007/s00371-012-0755-3
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Xiao Xuezhong, 2006, PACM INT C VIRT REAL, P305, DOI DOI 10.1145/1128923.1128974
   Xiao Y, 2013, IEEE T MULTIMEDIA, V15, P549, DOI 10.1109/TMM.2012.2233725
   Yang CK, 2008, IEEE COMPUT GRAPH, V28, P52, DOI 10.1109/MCG.2008.24
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   Zhao SC, 2020, IEEE T AFFECT COMPUT, V11, P574, DOI [10.1109/TAFFC.2018.2818685, 10.1109/TAFFC.2016.2628787]
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 41
TC 4
Z9 5
U1 4
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 8
DI 10.1145/3464382
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ZY5OX
UT WOS:000772636900008
DA 2024-07-18
ER

PT J
AU Jiang, WT
   Wang, WX
   Hu, HF
AF Jiang, Weitao
   Wang, Weixuan
   Hu, Haifeng
TI Bi-Directional Co-Attention Network for Image Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image captioning; attention mechanism; bi-directional co-attention
   mechanism; multivariate residual strategy
ID LANGUAGE
AB Image Captioning, which automatically describes an image with natural language, is regarded as a fundamental challenge in computer vision. In recent years, significant advance has been made in image captioning through improving attention mechanism. However, most existing methods construct attention mechanisms based on singular visual features, such as patch features or object features, which limits the accuracy of generated captions. In this article, we propose a Bidirectional Co-Attention Network (BCAN) that combines multiple visual features to provide information from different aspects. Different features are associated with predicting different words, and there are a priori relations between these multiple visual features. Based on this, we further propose a bottom-up and top-down bi-directional co-attention mechanism to extract discriminative attention information. Furthermore, most existing methods do not exploit an effective multimodal integration strategy, generally using addition or concatenation to combine features. To solve this problem, we adopt the Multivariate Residual Module (MRM) to integrate multimodal attention features. Meanwhile, we further propose a Vertical MRM to integrate features of the same category, and a Horizontal MRM to combine features of the different categories, which can balance the contribution of the bottom-up co-attention and the top-down co-attention. In contrast to the existing methods, the BCAN is able to obtain complementary information from multiple visual features via the bi-directional co-attention strategy, and integrate multimodal information via the improved multivariate residual strategy. We conduct a series of experiments on two benchmark datasets (MSCOCO and Flickr30k), and the results indicate that the proposed BCAN achieves the superior performance.
C1 [Jiang, Weitao; Wang, Weixuan; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM jiangwt5@mail2.sysu.edu.cn; wangwx25@mail2.sysu.edu.cn;
   huhaif@mail.sysu.edu.cn
RI Wang, Xiaoman/JYP-1144-2024
OI Hu, Haifeng/0000-0002-4884-323X; Jiang, Weitao/0000-0002-7168-9357
FU National Natural Science Foundation of China [62076262, 61673402,
   61273270, 60802069]; National Key Research and Development Program of
   China [2018YFB1601101, 2018YFB1601100]; Natural Science Foundation of
   Guangdong Province [2017A030311029]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076262, and Grant 61673402, Grant
   61273270, and Grant 60802069; in part by the National Key Research and
   Development Program of China under Grant 2018YFB1601101 and Grant
   2018YFB1601100; in part by the Natural Science Foundation of Guangdong
   Province under Grant 2017A030311029.
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], 2016, ADV NEURAL INFORM PR
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Biswas P, 2005, I CONF VLSI DESIGN, P651
   Cai DL, 2018, LEC NO MULTI IND ENG, P499, DOI 10.1007/978-3-319-59280-0_40
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   He C, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3292058
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Kalimuthu Marimuthu, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12666), P381, DOI 10.1007/978-3-030-68780-9_32
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Karpathy Andrej, 2014, ADV NEURAL INFORM PR, P1889
   Ke L, 2019, IEEE I CONF COMP VIS, P8887, DOI 10.1109/ICCV.2019.00898
   Kim J.-H., 2016, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Lu P, 2018, AAAI CONF ARTIF INTE, P7218
   Mao Junhua, 2014, ARXIV14126632
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Parikh, 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7299087
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Ranzato MarcAurelio, 2015, ARXIV151106732
   Ren FY, 2004, IEEE SYMP COMP COMMU, P748, DOI 10.1109/ISCC.2004.1358630
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang WX, 2019, AAAI CONF ARTIF INTE, P8957
   Wang YF, 2017, PROC CVPR IEEE, P7378, DOI 10.1109/CVPR.2017.780
   WeixuanWang Zhihong Chen, 2018, P AS C COMP VIS, P587
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang L, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3386725
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yiwu Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P211, DOI 10.1007/978-3-030-58568-6_13
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang L, 2017, ARXIV PREPRINT ARXIV
NR 51
TC 25
Z9 25
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 125
DI 10.1145/3460474
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800009
DA 2024-07-18
ER

PT J
AU Ma, X
   Yang, XS
   Gao, JY
   Xu, CS
AF Ma, Xuan
   Yang, Xiaoshan
   Gao, Junyu
   Xu, Changsheng
TI Health Status Prediction with Local-Global Heterogeneous Behavior Graph
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Health status prediction; graph neural networks; individual behavior
ID NEUROBIOLOGICAL MARKERS; CLASSIFICATION; VALIDATION; MODELS
AB Health management is getting increasing attention all over the world. However, existing health management mainly relies on hospital examination and treatment, which are complicated and untimely. The emergence of mobile devices provides the possibility to manage people's health status in a convenient and instant way. Estimation of health status can be achieved with various kinds of data streams continuously collected from wearable sensors. However, these data streams are multi-source and heterogeneous, containing complex temporal structures with local contextual and global temporal aspects, which makes the feature learning and data joint utilization challenging. We propose to model the behavior-related multi-source data streams with a local-global graph, which contains multiple local context sub-graphs to learn short-term local context information with heterogeneous graph neural networks and a global temporal sub-graph to learn long-term dependency with self-attention networks. Then health status is predicted based on the structure-aware representation learned from the local-global behavior graph. We take experiments on the StudentLife dataset, and extensive results demonstrate the effectiveness of our proposed model.
C1 [Ma, Xuan; Yang, Xiaoshan; Gao, Junyu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Ma, Xuan; Yang, Xiaoshan; Gao, Junyu; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Ma, Xuan; Yang, Xiaoshan; Gao, Junyu; Xu, Changsheng] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory
RP Ma, X (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, 95 Zhongguancun East Rd, Beijing, Peoples R China.; Ma, X (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, 95 Zhongguancun East Rd, Beijing, Peoples R China.; Ma, X (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
EM maxuan2018@ia.ac.cn; xiaoshan.yang@nlpria.ac.cn; gaojunyu2015@ia.ac.cn;
   csxu@nlpria.ac.cn
RI Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023; Ma, Xuan/HPG-8311-2023;
   Gao, Junyu/HDO-5516-2022
FU National Key Research and Development Program of China [2018AAA0100604];
   National Natural Science Foundation of China [61720106006, 62072455,
   61721004, U1836220, U1705262, 61872424]
FX This work was supported by the National Key Research and Development
   Program of China (No. 2018AAA0100604) and the National Natural Science
   Foundation of China (No. 61720106006, 62072455, 61721004, U1836220,
   U1705262, 61872424).
CR Acharya UR, 2018, COMPUT BIOL MED, V100, P270, DOI 10.1016/j.compbiomed.2017.09.017
   [Anonymous], 1997, NEURAL COMPUT
   Asselbergs J, 2016, J MED INTERNET RES, V18, DOI 10.2196/jmir.5505
   Atwood J, 2016, ADV NEUR IN, V29
   Bánhalmi A, 2018, J HEALTHC ENG, V2018, DOI 10.1155/2018/4038034
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Burns MN, 2011, J MED INTERNET RES, V13, DOI 10.2196/jmir.1838
   Cai HS, 2018, COMPLEXITY, DOI 10.1155/2018/5238028
   Canzian L, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P1293, DOI 10.1145/2750858.2805845
   Cao SS, 2016, AAAI CONF ARTIF INTE, P1145
   Chao Che, 2017, P 2017 SIAM INT C DA, P198, DOI DOI 10.1137/1.9781611974973.23
   Chen HX, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1177, DOI 10.1145/3219819.3219986
   Chen YY, 2016, PROC EUR S-STATE DEV, P432, DOI 10.1109/ESSDERC.2016.7599678
   Costafreda SG, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006353
   Defferrard M, 2016, ADV NEUR IN, V29
   Dong YX, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P135, DOI 10.1145/3097983.3098036
   Fu CHY, 2008, BIOL PSYCHIAT, V63, P656, DOI 10.1016/j.biopsych.2007.08.020
   Futoma J, 2015, J BIOMED INFORM, V56, P229, DOI 10.1016/j.jbi.2015.05.016
   Gallicchio C, 2010, IEEE IJCNN
   Gao JY, 2021, IEEE T PATTERN ANAL, V43, P3476, DOI 10.1109/TPAMI.2020.2985708
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Gao JY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P690, DOI 10.1145/3240508.3240566
   Gao Junyu, 2011, P AAAI C ARTIFICIAL, V33, P8303
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Goel M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P5675, DOI 10.1145/2858036.2858401
   Hahn T, 2011, ARCH GEN PSYCHIAT, V68, P361, DOI 10.1001/archgenpsychiatry.2010.178
   Henaff M., 2015, ARXIV150605163
   Huang Y, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409332
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35
   Kam HJ, 2017, COMPUT BIOL MED, V89, P248, DOI 10.1016/j.compbiomed.2017.08.015
   Kho AN, 2012, J AM MED INFORM ASSN, V19, P212, DOI 10.1136/amiajnl-2011-000439
   Kipf TN, 2016, ARXIV
   Koenig N, 2016, TELEMED E-HEALTH, V22, P631, DOI 10.1089/tmj.2015.0212
   Laaksonen Jorma, 2002, P INT C NEUR NETW IC
   Lane Nicholas D, 2011, P 5 INT ICST C PERV, P23, DOI DOI 10.4108/ICST.PERVASIVEHEALTH.2011.246161
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li HG, 2019, MICROELECTRON J, V88, P164, DOI 10.1016/j.mejo.2018.01.015
   Liu Yan., 2017, ARXIV170701926, V1707, P01926
   Machado IP, 2015, INFORM PROCESS MANAG, V51, P204, DOI 10.1016/j.ipm.2014.07.008
   McGrath CL, 2013, JAMA PSYCHIAT, V70, P821, DOI 10.1001/jamapsychiatry.2013.143
   Min JK, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P477, DOI 10.1145/2556288.2557220
   Min WQ, 2018, IEEE T MULTIMEDIA, V20, P950, DOI 10.1109/TMM.2017.2759499
   Mwangi B, 2012, J MAGN RESON IMAGING, V35, P64, DOI 10.1002/jmri.22806
   Nag N, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1993, DOI 10.1145/3240508.3241913
   Nandakumar R., 2015, ACM MOBISYS, DOI [DOI 10.1145/2867070.2867078, DOI 10.1145/2742647.2742674]
   Natale V, 2012, SLEEP BIOL RHYTHMS, V10, P287, DOI 10.1111/j.1479-8425.2012.00575.x
   Niepert M, 2016, PR MACH LEARN RES, V48
   Peissig PL, 2012, J AM MED INFORM ASSN, V19, P225, DOI 10.1136/amiajnl-2011-000456
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Pollak JP, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P725
   Qi F, 2021, IEEE T MULTIMEDIA, V23, P3999, DOI 10.1109/TMM.2020.3035285
   Rajkomar A, 2018, NPJ DIGIT MED, V1, DOI 10.1038/s41746-018-0029-1
   Ridge PG, 2018, ALZHEIMERS DEMENT, V14, P514, DOI 10.1016/j.jalz.2017.11.013
   Rosa MJ, 2015, NEUROIMAGE, V105, P493, DOI 10.1016/j.neuroimage.2014.11.021
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Seo Y, 2018, LECT NOTES COMPUT SC, V11301, P362, DOI 10.1007/978-3-030-04167-0_33
   Stafford M, 2016, 2016 IEEE FIRST INTERNATIONAL CONFERENCE ON CONNECTED HEALTH: APPLICATIONS, SYSTEMS AND ENGINEERING TECHNOLOGIES (CHASE), P332, DOI 10.1109/CHASE.2016.70
   Sun Y., 2013, SIGKDD Explorations, V14, P20, DOI DOI 10.1145/2481244.2481248
   Tin Kam Ho, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P278, DOI 10.1109/ICDAR.1995.598994
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753
   Wang R, 2014, UBICOMP'14: PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P3, DOI 10.1145/2632048.2632054
   Wang X, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2022, DOI 10.1145/3308558.3313562
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Yebda T, 2019, INT WORK CONTENT MUL
   Zhang Weiming, 2019, Proceedings of the ACM Multimedia Asia, P1
   Ziwei Zhang, 2022, IEEE Transactions on Knowledge and Data Engineering, V34, P249, DOI 10.1109/TKDE.2020.2981333
NR 69
TC 0
Z9 0
U1 2
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 129
DI 10.1145/3457893
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA YP8GI
UT WOS:000748857800013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Messina, N
   Amato, G
   Esuli, A
   Falchi, F
   Gennaro, C
   Marchand-Maillet, S
AF Messina, Nicola
   Amato, Giuseppe
   Esuli, Andrea
   Falchi, Fabrizio
   Gennaro, Claudio
   Marchand-Maillet, Stephane
TI Fine-Grained Visual Textual Alignment for Cross-Modal Retrieval Using
   Transformer Encoders
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; cross-modal retrieval; multi-modal matching; computer
   vision; natural language processing
ID LANGUAGE; GENOME
AB Despite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching remains a challenging task. In this work, we tackle the task of cross-modal retrieval through image-sentence matching based on word-region alignments, using supervision only at the global image-sentence level. Specifically, we present a novel approach called Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences (i.e., image regions and words, respectively) to preserve the informative richness of both modalities. TERAN obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover, on MS-COCO, it also outperforms current approaches on the sentence retrieval task. 000Focusing on scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual data pipelines well separated. Cross-attention links invalidate any chance to separately extract visual and textual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In this respect, TERAN merges the information from the two domains only during the final alignment phase, immediately before the loss computation. We argue that the fine-grained alignments produced by TERAN pave the way toward the research for effective and efficient methods for large-scale cross-modal information retrieval. We compare the effectiveness of our approach against relevant state-of-the-art methods. On the MS-COCO 1K test set, we obtain an improvement of 5.7% and 3.5% respectively on the image and the sentence retrieval tasks on the Recall@1 metric.
C1 [Messina, Nicola; Amato, Giuseppe; Esuli, Andrea; Falchi, Fabrizio; Gennaro, Claudio] ISTI CNR, Pisa, Italy.
   [Marchand-Maillet, Stephane] Univ Geneva, VIPER Grp, Geneva, Switzerland.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR); University
   of Geneva
RP Messina, N (corresponding author), ISTI CNR, Pisa, Italy.
EM nicola.messina@isti.cnr.it; giuseppe.amato@isti.cnr.it;
   andrea.esuli@isti.cnr.it; fabrizio.falchi@isti.cnr.it;
   claudio.gennaro@isti.cnr.it; stephane.marchand-maillet@unige.ch
RI Amato, Giuseppe/F-2227-2013; Gennaro, Claudio/AAH-5171-2019; Falchi,
   Fabrizio/J-2920-2012; Esuli, Andrea/B-6343-2015; Messina,
   Nicola/AAB-3309-2022
OI Amato, Giuseppe/0000-0003-0171-4315; Gennaro,
   Claudio/0000-0002-3715-149X; Falchi, Fabrizio/0000-0001-6258-5313;
   Esuli, Andrea/0000-0002-5725-4322; Messina, Nicola/0000-0003-3011-2487;
   Marchand-Maillet, Stephane/0000-0002-4875-6101
FU "Intelligenza Artificiale per il Monitoraggio Visuale dei Siti
   Culturali" (AI4CHSites) CNR4C program [CUP B15J19001040004]; AI4EU
   project - EC (H2020) [825619]; AI4Media [GA 951911]
FX This work was partially supported by "Intelligenza Artificiale per il
   Monitoraggio Visuale dei Siti Culturali" (AI4CHSites) CNR4C program, CUP
   B15J19001040004, by the AI4EU project, funded by the EC (H2020 -Contract
   n. 825619), and AI4Media under GA 951911.
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], 2018, P EUR C COMP VIS ECC, DOI DOI 10.3892/MMR.2018.9013
   [Anonymous], 2020, ARXIV200414255, DOI DOI 10.1145/3397271.3401093
   [Anonymous], 2016, 4 INT C LEARN REPR I
   Carrara F, 2018, INFORM RETRIEVAL J, V21, P208, DOI 10.1007/s10791-017-9318-6
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen Y., 2019, ARXIV
   Cornia M, 2019, PROC CVPR IEEE, P8299, DOI 10.1109/CVPR.2019.00850
   Devlin J., 2018, BERT PRE TRAINING DE
   Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Guo Yawen, 2020, APPL SCI, V10, P5516
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Huang F., 2018, IEEE T IMAGE PROCESS, V28, P2008
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang Y, 2019, IEEE I CONF COMP VIS, P5773, DOI 10.1109/ICCV.2019.00587
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767
   Huang Zhicheng, 2020, Pixel-bert: Aligning image pixels with text by deep multi-modal transformers
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Z, 2019, IEEE I CONF COMP VIS, P5753, DOI 10.1109/ICCV.2019.00585
   Ji Z, 2020, IEEE ACCESS, V8, P38438, DOI 10.1109/ACCESS.2020.2975594
   Ji Zhong, 2020, IEEE T CYBERNETICS
   Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee Kuang-Huei, 2019, CoRR
   Lee K, 2018, CONSERV GENET RESOUR, V10, P201, DOI 10.1007/s12686-017-0798-x
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li YK, 2018, LECT NOTES COMPUT SC, V11205, P346, DOI 10.1007/978-3-030-01246-5_21
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin X, 2016, LECT NOTES COMPUT SC, V9906, P261, DOI 10.1007/978-3-319-46475-6_17
   Liu C, 2020, P IEEECVF C COMPUTER, p10 918
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Lu JS, 2019, ADV NEUR IN, V32
   Messina Nicola, 2019, INT J MULTIMEDIA INF, V9, P113
   Messina Nicola, 2018, P EUR C COMP VIS ECC
   Messina Nicola, 2020, P INT C PATT REC ICP
   Qi D, 2020, CoRR
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Ren FY, 2004, IEEE SYMP COMP COMMU, P748, DOI 10.1109/ISCC.2004.1358630
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Santoro A, 2017, ADV NEUR IN, V30
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Su Weijie, 2020, INT C LEARN REPR
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wei KM, 2020, IEEE ACCESS, V8, P96237, DOI 10.1109/ACCESS.2020.2996407
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597
   Yan Huang, 2018, IEEE T PAMI, V42, P636
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
NR 65
TC 55
Z9 59
U1 7
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 128
DI 10.1145/3451390
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ni, TG
   Ding, Y
   Xue, J
   Xia, KJ
   Gu, XQ
   Jiang, YZ
AF Ni, Tongguang
   Ding, Yan
   Xue, Jing
   Xia, Kaijian
   Gu, Xiaoqing
   Jiang, Yizhang
TI Local Constraint and Label Embedding Multi-layer Dictionary Learning for
   Sperm Head Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human sperm head image; morphological classification; multi-layer
   framework; dictionary learning
ID DISCRIMINATIVE DICTIONARY; GOLD-STANDARD; K-SVD
AB Morphological classification of human sperm heads is a key technology for diagnosing male infertility. Due to its sparse representation and learning capability, dictionary learning has shown remarkable performance in human sperm head classification. To promote the discriminability of the classification model, a novel local constraint and label embedding multi-layer dictionary learning model called LCLM-MDL is proposed in this study. Based on the multi-layer dictionary learning framework, two dictionaries are built on the basis of Laplacian regularized constraint and label embedding term in each layer, and the two dictionaries are approximated to each other as much as possible, so as to well exploit the nonlinear structure and discriminability features of the morphology of human sperm heads. In addition, to promote the robustness of the model, the asymmetric Huber loss is adopted in the last layer of LCLM-MDL, which approximates the misclassification error by using the absolute error function. Finally, the experimental results on HuSHeM dataset demonstrate the validity of the LCLM-MDL.
C1 [Ni, Tongguang; Gu, Xiaoqing] Changzhou Univ, Sch Comp Sci & Artificial Intelligence, Changzhou 213164, Peoples R China.
   [Ding, Yan] Changshu Inst Technol, Dept Logist Support, Changshu 215500, Jiangsu, Peoples R China.
   [Xue, Jing] Nanjing Med Univ, Dept Nephrol, Affiliated Wuxi Peoples Hosp, Wuxi 214023, Jiangsu, Peoples R China.
   [Xia, Kaijian] Soochow Univ, Cent Res Lab, Affiliated Changshu Hosp, Changshu 1 Peoples Hosp, Changshu 215500, Jiangsu, Peoples R China.
   [Jiang, Yizhang] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
C3 Changzhou University; Changshu Institute of Technology; Nanjing Medical
   University; Soochow University - China; Jiangnan University
RP Xia, KJ (corresponding author), Soochow Univ, Cent Res Lab, Affiliated Changshu Hosp, Changshu 1 Peoples Hosp, Changshu 215500, Jiangsu, Peoples R China.
EM hbxtntg-12@163.com; dyan@cslg.edu.cn; xuejing@njmu.edu.cn;
   xiakaijian@163.com; czxygu@163.com; yzjiang@jiangnan.edu.cn
RI Jiang, Yizhang/V-2171-2019; Ni, Tongguang/GPP-6949-2022; Gu,
   Xiaoqing/GPP-6913-2022
OI Jiang, Yizhang/0000-0002-4558-9803; 
FU Jiangsu Committee of Health [H2018071]; Changshu Committee of Health
   [csws201820]; Changzhou Scientific and Technological Support Social
   Development Project [CE20215032]; Natural Science Foundation of Jiangsu
   Province [BK 20211333]; National Natural Science Foundation of China
   [61806026, 62171203]
FX This work was supported in part by the Jiangsu Committee of Health under
   Grant H2018071, Changshu Committee of Health under Grant csws201820,
   Changzhou Scientific and Technological Support Social Development
   Project CE20215032, Natural Science Foundation of Jiangsu Province under
   Grant BK 20211333, and National Natural Science Foundation of China
   under Grants 61806026 and 62171203.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Bai J, 2018, SOFT COMPUT, V22, P1467, DOI 10.1007/s00500-017-2853-7
   Balasundaram S, 2020, APPL SOFT COMPUT, V97, DOI 10.1016/j.asoc.2020.106708
   Balasundaram S, 2019, NEURAL PROCESS LETT, V49, P1399, DOI 10.1007/s11063-018-9875-8
   Batuwita R, 2010, IEEE T FUZZY SYST, V18, P558, DOI 10.1109/TFUZZ.2010.2042721
   Boumaza K., 2018, MEDICAL TECHNOLOGIES, V2, P301, DOI DOI 10.26415/2572-004XVOL2ISS4P301-307
   Chang V, 2017, COMPUT BIOL MED, V84, P205, DOI 10.1016/j.compbiomed.2017.03.029
   Chang V, 2017, COMPUT BIOL MED, V83, P143, DOI 10.1016/j.compbiomed.2017.03.004
   Chang V, 2014, COMPUT METH PROG BIO, V117, P225, DOI 10.1016/j.cmpb.2014.06.018
   Davis G, 1997, CONSTR APPROX, V13, P57, DOI 10.1007/BF02678430
   Ghasemian F, 2015, COMPUT METH PROG BIO, V122, P409, DOI 10.1016/j.cmpb.2015.08.013
   Gu XQ, 2020, INT J MACH LEARN CYB, V11, P33, DOI 10.1007/s13042-019-00936-3
   Gu XQ, 2020, J MED IMAG HEALTH IN, V10, P463, DOI 10.1166/jmihi.2020.2900
   Guo L, 2010, J NEUROSCI METH, V191, P101, DOI 10.1016/j.jneumeth.2010.05.020
   Gupta D, 2020, NEURAL COMPUT APPL, V32, P12971, DOI 10.1007/s00521-020-04741-w
   Ilhan HO, 2020, MED BIOL ENG COMPUT, V58, P1047, DOI 10.1007/s11517-019-02101-y
   Iqbal I, 2020, DIAGNOSTICS, V10, DOI 10.3390/diagnostics10050325
   Javadi S, 2019, COMPUT BIOL MED, V109, P182, DOI 10.1016/j.compbiomed.2019.04.030
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Leski JM, 2010, B POL ACAD SCI-TECH, V58, P171, DOI 10.2478/v10175-010-0018-2
   Li HF, 2018, PATTERN RECOGN, V79, P130, DOI 10.1016/j.patcog.2018.02.005
   Li ZM, 2017, IEEE T NEUR NET LEAR, V28, P278, DOI 10.1109/TNNLS.2015.2508025
   Lu SY, 2021, NEURAL COMPUT APPL, V33, P10799, DOI 10.1007/s00521-020-05082-4
   Lu SY, 2019, J COMPUT SCI-NETH, V30, P41, DOI 10.1016/j.jocs.2018.11.008
   McCallum C, 2019, COMMUN BIOL, V2, DOI 10.1038/s42003-019-0491-6
   Meng QL, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3350840
   Mirsky SK, 2017, CYTOM PART A, V91A, P893, DOI 10.1002/cyto.a.23189
   Papyan V, 2018, IEEE SIGNAL PROC MAG, V35, P72, DOI 10.1109/MSP.2018.2820224
   Riordon J, 2019, COMPUT BIOL MED, V111, DOI 10.1016/j.compbiomed.2019.103342
   Shaker F, 2017, COMPUT BIOL MED, V91, P181, DOI 10.1016/j.compbiomed.2017.10.009
   Shaker I., 2017, Human sperm head morphology dataset
   Song JQ, 2019, PATTERN RECOGN, V91, P135, DOI 10.1016/j.patcog.2019.02.018
   Srinivas M, 2015, NEUROCOMPUTING, V168, P880, DOI 10.1016/j.neucom.2015.05.036
   Thirumalaraju P, 2018, FERTIL STERIL, V110, pE432, DOI 10.1016/j.fertnstert.2018.08.039
   Tim SCW, 2016, LECT NOTES COMPUT SC, V10016, P522, DOI 10.1007/978-3-319-48680-2_46
   WHO, 2010, WHO LAB MAN EX PROC
NR 36
TC 5
Z9 5
U1 3
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 100
DI 10.1145/3458927
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600002
DA 2024-07-18
ER

PT J
AU Singh, AK
   Anand, A
   Lv, Z
   Ko, H
   Mohan, A
AF Singh, A. K.
   Anand, A.
   Lv, Z.
   Ko, H.
   Mohan, A.
TI A Survey on Healthcare Data: A Security Perspective
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Smart healthcare system; medical records; cybersecurity; security
   issues; watermarking; cryptography; biometrics; blockchain
ID WIRELESS SENSOR NETWORKS; SVD-BASED ROBUST; MEDICAL IMAGES;
   AUTHENTICATION PROTOCOL; MULTIPLE WATERMARKING; PATIENT PRIVACY; SCHEME;
   ENCRYPTION; DWT; INFORMATION
AB With the remarkable development of internet technologies, the popularity of smart healthcare has regularly come to the fore. Smart healthcare uses advanced technologies to transform the traditional medical system in an all-round way, making healthcare more efficient, more convenient, and more personalized. Unfortunately, medical data security is a serious issue in the smart healthcare systems. It becomes a fundamental challenge that requires the development of efficient innovative strategies towards fulfilling the healthcare needs and supporting secure healthcare transfer and delivery. This article provides a comprehensive survey on state-of-the-art techniques for health data security and their new trends for solving challenges in real-world applications. We survey the various notable cryptography, biometrics, watermarking, and blockchain-based security techniques for healthcare applications. A comparative analysis is also performed to identify the contribution of reviewed techniques in terms of their objective, methodology, type of medical data, important features, and limitations. At the end, we discuss the open issues and research directions to explore the promising areas for future research.
C1 [Singh, A. K.; Anand, A.] NIT Patna, Dept CSE, Patna 800005, Bihar, India.
   [Lv, Z.] Qingdao Univ, Sch Data Sci & Software Engn, Qingdao 266071, Peoples R China.
   [Ko, H.] Chosun Univ, IT Res Inst, Gwangju 61452, South Korea.
   [Mohan, A.] IIT BHU, Dept Elect Engn, Varanasi, Uttar Pradesh, India.
   [Mohan, A.] Banaras Hindu Univ, Dept Elect Engn, Indian Inst Technol, Varanasi 221005, Uttar Pradesh, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Patna; Qingdao University; Chosun University; Indian
   Institute of Technology System (IIT System); Indian Institute of
   Technology BHU Varanasi (IIT BHU Varanasi); Banaras Hindu University
   (BHU); Indian Institute of Technology System (IIT System); Indian
   Institute of Technology BHU Varanasi (IIT BHU Varanasi)
RP Singh, AK (corresponding author), NIT Patna, Dept CSE, Patna 800005, Bihar, India.
EM amit.singh@nitp.ac.in; ashima1795@gmail.com; lvzhihan@gmail.com;
   skoh21@chosun.ac.kr; profanandmohan@gmail.com
RI Lv, Zhihan/GLR-6000-2022; Lyu, Zhihan/I-3187-2014; Singh, Amit
   Kumar/D-1300-2015
OI Lv, Zhihan/0000-0003-2525-3074; Lyu, Zhihan/0000-0003-2525-3074; Singh,
   Amit Kumar/0000-0001-7359-2068
CR Abd-Eldayem MM, 2013, EGYPT INFORM J, V14, P1, DOI 10.1016/j.eij.2012.11.002
   Acharya R, 2004, COMPUT METH PROG BIO, V76, P13, DOI 10.1016/j.cmpb.2004.02.009
   Acharya R, 2003, COMPUT BIOL MED, V33, P303, DOI 10.1016/S0010-4825(02)00083-5
   Agarwal N, 2019, MULTIMED TOOLS APPL, V78, P8603, DOI 10.1007/s11042-018-7128-5
   Agbo CC, 2019, HEALTHCARE-BASEL, V7, DOI 10.3390/healthcare7020056
   Al-Haj A, 2015, IET INFORM SECUR, V9, P365, DOI 10.1049/iet-ifs.2014.0245
   Al-Haj A, 2014, J DIGIT IMAGING, V27, P737, DOI 10.1007/s10278-014-9709-9
   Ali R, 2018, ARAB J SCI ENG, V43, P7837, DOI 10.1007/s13369-018-3220-4
   Alshehri S., 2012, Proceedings of the 2012 IEEE International Conference on Data Engineering Workshops (ICDEW 2012), P143, DOI 10.1109/ICDEW.2012.68
   Anand A, 2020, IEEE MULTIMEDIA, V27, P133, DOI 10.1109/MMUL.2020.2993269
   Anand A, 2020, IEEE MULTIMEDIA, V27, P66, DOI 10.1109/MMUL.2020.2985973
   Anand A, 2021, MULTIMED TOOLS APPL, V80, P30165, DOI 10.1007/s11042-020-08801-0
   Anand A, 2020, COMPUT COMMUN, V152, P72, DOI 10.1016/j.comcom.2020.01.038
   [Anonymous], 2008, HDB BIOMETRICS
   [Anonymous], 2020, GREATEST CYBERSECUR
   [Anonymous], 2018, International Journal of Engineering & Technology, DOI DOI 10.14419/IJET.V7I1.9.9729
   [Anonymous], 2017, INT J COMPUTER APPL
   Arunkumar S, 2019, MEASUREMENT, V139, P426, DOI 10.1016/j.measurement.2019.02.069
   Awad A. I., 2012, 2012 IIAI International Conference on Advanced Applied Informatics (IIAIAAI 2012), P129, DOI 10.1109/IIAI-AAI.2012.34
   Awasthi AK, 2011, COMPUT ELECTR ENG, V37, P869, DOI 10.1016/j.compeleceng.2011.09.015
   Boucherkha S, 2007, PROC WRLD ACAD SCI E, V1, P100
   Brunese L, 2019, PROCEDIA COMPUT SCI, V159, P1787, DOI 10.1016/j.procs.2019.09.350
   Burke W, 2019, PROCEEDINGS OF THE AUSTRALASIAN COMPUTER SCIENCE WEEK MULTICONFERENCE (ACSW 2019), DOI 10.1145/3290688.3290721
   Byun JW, 2015, SECUR COMMUN NETW, V8, P3028, DOI 10.1002/sec.1229
   Cao F, 2003, COMPUT MED IMAG GRAP, V27, P185, DOI 10.1016/S0895-6111(02)00073-3
   Casino F, 2019, TELEMAT INFORM, V36, P55, DOI 10.1016/j.tele.2018.11.006
   Chang CC, 2008, PATTERN RECOGN, V41, P3130, DOI 10.1016/j.patcog.2008.04.006
   Chen Y, 2019, J MED SYST, V43, DOI 10.1007/s10916-018-1121-4
   Chhajed P, 2015, 2015 INTERNATIONAL CONFERENCE ON GREEN COMPUTING AND INTERNET OF THINGS (ICGCIOT), P1035, DOI 10.1109/ICGCIoT.2015.7380616
   Chirakkarottu S, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-019-1685-8
   Choi Y, 2014, SENSORS-BASEL, V14, P10081, DOI 10.3390/s140610081
   Demchenko, 2020, TRENDS HEALTHCARE 20
   Dridi M, 2014, 2014 GLOBAL SUMMIT ON COMPUTER & INFORMATION TECHNOLOGY (GSCIT)
   El Bireki MFM, 2016, INT J SECUR APPL, V10, P107, DOI 10.14257/ijsia.2016.10.5.10
   Eswaraiah R, 2014, INT CONF COMM SYST, P896, DOI 10.1109/CSNT.2014.184
   Garg N, 2020, IEEE ACCESS, V8, P95956, DOI 10.1109/ACCESS.2020.2995917
   Giri D, 2015, J MED SYST, V39, DOI 10.1007/s10916-014-0145-7
   Gupta, 2020, HDB COMPUTER NETWORK
   Hamidi H, 2019, FUTURE GENER COMP SY, V91, P434, DOI 10.1016/j.future.2018.09.024
   Hathaliya JJ, 2019, COMPUT ELECTR ENG, V76, P398, DOI 10.1016/j.compeleceng.2019.04.017
   He DB, 2015, IEEE COMMUN MAG, V53, P71, DOI 10.1109/MCOM.2015.7010518
   Hussein Nidal Hassan, 2016, INT J COMPUT SCI MOB, P186
   Ichikawa D, 2017, JMIR MHEALTH UHEALTH, V5, DOI 10.2196/mhealth.7938
   Islam SKH, 2016, INT J COMMUN SYST, V29, P1708, DOI 10.1002/dac.2793
   Iuon-Chang Lin, 2017, International Journal of Network Security, V19, P653, DOI 10.6633/IJNS.201709.19(5).01
   Jahan Sharmin, 2019, International Journal of Computers and Applications, V41, P233, DOI 10.1080/1206212X.2018.1437651
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jang CS, 2011, WIREL COMMUN MOB COM, V11, P277, DOI 10.1002/wcm.884
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang Q, 2016, J NETW COMPUT APPL, V76, P37, DOI 10.1016/j.jnca.2016.10.001
   Jindong Xu, 2010, Journal of Software Engineering and Applications, V3, P939, DOI 10.4236/jsea.2010.310111
   Kanan HR, 2014, EXPERT SYST APPL, V41, P6123, DOI 10.1016/j.eswa.2014.04.022
   Kaur H, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-1007-5
   Kavitha K., 2019, INT J INTELL SYST TE, V18, P271, DOI DOI 10.1504/IJISTA.2019.099344
   Kumar A., 2010, Int. J Comput. Appl., V9, P19
   Kumar B., 2011, WORLD ACAD SCI ENG T, V55, P95, DOI DOI 10.5281/ZENODO.1059548
   Kumar B., 2011, J INF SECUR, V2, P91, DOI DOI 10.4236/JIS.2011.22009
   Kumari S, 2014, SECUR COMMUN NETW, V7, P1921, DOI 10.1002/sec.906
   Lee J, 2019, INT J PROD RES, V57, P7123, DOI 10.1080/00207543.2019.1578430
   Li HH, 2018, J FOOD PROCESS PRES, V42, DOI [10.1007/s10916-018-0993-7, 10.1111/jfpp.13348]
   Li MY, 2005, COMPUT MED IMAG GRAP, V29, P367, DOI 10.1016/j.compmedimag.2005.02.003
   Li XQ, 2020, FUTURE GENER COMP SY, V107, P841, DOI 10.1016/j.future.2017.08.020
   Li X, 2018, IEEE T IND INFORM, V14, P3599, DOI 10.1109/TII.2017.2773666
   Li X, 2018, J NETW COMPUT APPL, V103, P194, DOI 10.1016/j.jnca.2017.07.001
   Li X, 2017, COMPUT NETW, V129, P429, DOI 10.1016/j.comnet.2017.03.013
   Lili Liu, 2012, Proceedings of the 2012 International Conference on Computer Science and Electronics Engineering (ICCSEE 2012), P707, DOI 10.1109/ICCSEE.2012.435
   Matyas V., 2010, 2010 International Conference on Computer Information Systems and Industrial Management Applications (CISIM 2010), P19, DOI 10.1109/CISIM.2010.5643698
   Merabet F, 2020, PEER PEER NETW APPL, V13, P439, DOI 10.1007/s12083-019-00782-8
   Mirjalol S, 2018, I C INF COMM TECH CO, P424, DOI 10.1109/ICTC.2018.8539365
   Mishra R, 2015, J MED SYST, V39, DOI 10.1007/s10916-015-0215-5
   Mothi R, 2019, MEASUREMENT, V136, P67, DOI 10.1016/j.measurement.2018.12.030
   Nagaraju, 2018, INT J INNOV ENG MANA, V7, P236, DOI [10.22214/ijraset.2017.4212, DOI 10.22214/IJRASET.2017.4212]
   Pandey R, 2016, MULTIMED TOOLS APPL, V75, P14381, DOI 10.1007/s11042-016-3536-6
   Parah SA, 2017, MULTIMED TOOLS APPL, V76, P10599, DOI 10.1007/s11042-015-3127-y
   Park U, 2008, PROC SPIE, V6944, DOI 10.1117/12.778804
   Parvaz R, 2018, OPT LASER TECHNOL, V101, P30, DOI 10.1016/j.optlastec.2017.10.024
   Qadir A.M., 2019, 2019 7TH INTERNATIONAL SYMPOSIUM ON DIGITAL FORENSICS AND SECURITY (ISDFS), P1
   Ramli DA, 2016, PROCEDIA COMPUT SCI, V96, P314, DOI 10.1016/j.procs.2016.08.143
   Salameh JNB, 2019, INT J COMPUT SCI NET, V19, P28
   Sankaran KS, 2019, 2019 INT C COMM SIGN, P0568
   Sarker M. I. H., 2013, INT J COMPUT SCI, V10, P3
   Shaki KA, 2020, J KING SAUD UNIV-COM, V32, P57, DOI 10.1016/j.jksuci.2017.07.001
   Shankar K, 2018, IEEE ACCESS, V6, P77145, DOI 10.1109/ACCESS.2018.2874026
   Shanthini B., 2012, Proceedings of the 2012 International Conference on Recent Advances in Computing and Software Systems (RACSS), P180, DOI 10.1109/RACSS.2012.6212720
   Sharma A, 2015, PROCEDIA COMPUT SCI, V70, P778, DOI 10.1016/j.procs.2015.10.117
   Shen M, 2019, IEEE NETWORK, V33, P27, DOI 10.1109/MNET.001.1800503
   Silva Hugo, 2011, Information Quality in e-Health. Proceedings 7th Conference of the Workgroup Human-Computer Interaction and Usability Engineering of the Austrian Computer Society, USAB 2011, P121, DOI 10.1007/978-3-642-25364-5_12
   Singh AK, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3382772
   Singh Amit Kumar, 2020, IT Professional, V22, P45, DOI 10.1109/MITP.2019.2961898
   Singh AK, 2017, MULTIMED SYST APPL, P1, DOI 10.1007/978-3-319-57699-2
   Singh AK, 2018, IEEE ACCESS, V6, P79005, DOI 10.1109/ACCESS.2018.2885256
   Singh AK, 2015, J MED IMAG HEALTH IN, V5, P607, DOI 10.1166/jmihi.2015.1432
   Singh AK, 2015, J MED IMAG HEALTH IN, V5, P406, DOI 10.1166/jmihi.2015.1407
   Singh AK, 2014, NATL ACAD SCI LETT, V37, P351, DOI 10.1007/s40009-014-0241-8
   Singh I, 2019, PROCEEDINGS 2019 AMITY INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AICAI), P930, DOI [10.1109/aicai.2019.8701387, 10.1109/AICAI.2019.8701387]
   Singh P, 2017, MULTIMED TOOLS APPL, V76, P3871, DOI 10.1007/s11042-016-4048-0
   Srinivas J, 2020, IEEE T DEPEND SECURE, V17, P1133, DOI 10.1109/TDSC.2018.2857811
   Sujatha S, 2013, INT J COMPUT APPL, V78, P27
   Sun JY, 2011, INT CON DISTR COMP S, P373, DOI 10.1109/ICDCS.2011.83
   Sundaravadivel P, 2018, IEEE CONSUM ELECTR M, V7, P19, DOI 10.1109/MCE.2017.2755378
   Thakkar FN, 2017, MULTIMED TOOLS APPL, V76, P3669, DOI 10.1007/s11042-016-3928-7
   Thakur S, 2020, LECT NOTES ELECTR EN, V587, P897, DOI 10.1007/978-981-32-9775-3_80
   Thakur S., 2018, Cryptographic and Information Security Approaches for Images and Videos, P467
   Thakur S, 2019, MULTIMED TOOLS APPL, V78, P3457, DOI 10.1007/s11042-018-6263-3
   Thanki R, 2016, ADV INTELL SYST, V425, P133, DOI 10.1007/978-3-319-28658-7_12
   Theodouli A, 2018, IEEE TRUST BIG, P1374, DOI 10.1109/TrustCom/BigDataSE.2018.00190
   Tian S., 2019, Global HealthJournal3, V3, P62, DOI [DOI 10.1016/J.GLOHJ.2019.07.001, 10.1016/j.glohj.2019.07.001]
   Toughi S, 2017, SIGNAL PROCESS, V141, P217, DOI 10.1016/j.sigpro.2017.06.010
   Vengadapurvaja AM, 2017, PROCEDIA COMPUT SCI, V115, P643, DOI 10.1016/j.procs.2017.09.150
   Wu CC, 2011, J SYST SOFTWARE, V84, P2196, DOI 10.1016/j.jss.2011.06.021
   Wu SH, 2019, PROCEEDINGS OF 2019 THE 3RD INTERNATIONAL CONFERENCE ON CRYPTOGRAPHY, SECURITY AND PRIVACY (ICCSP 2019) WITH WORKSHOP 2019 THE 4TH INTERNATIONAL CONFERENCE ON MULTIMEDIA AND IMAGE PROCESSING (ICMIP 2019), P13, DOI 10.1145/3309074.3309079
   Xhafa F, 2015, MULTIMED TOOLS APPL, V74, P3441, DOI 10.1007/s11042-013-1829-6
   Yang H, 2019, PATTERN RECOGN, V85, P1, DOI 10.1016/j.patcog.2018.07.028
   Yang K, 2013, IEEE INFOCOM SER, P2895
   Yang WC, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11020141
   Yue ZJ, 2021, ACM T INTERNET TECHN, V21, DOI 10.1145/3383779
   Zhang CE, 2008, IEEE T INF FOREN SEC, V3, P611, DOI 10.1109/TIFS.2008.2004288
   Zhao ZG, 2014, J MED SYST, V38, DOI 10.1007/s10916-014-0013-5
   Zhou R, 2013, SENSORS-BASEL, V13, P3142, DOI 10.3390/s130303142
   Zhou YC, 2009, IEEE ENG MED BIO, P3707, DOI 10.1109/IEMBS.2009.5334799
   Zhu LH, 2019, 2019 IEEE 5TH INTL CONFERENCE ON BIG DATA SECURITY ON CLOUD (BIGDATASECURITY) / IEEE INTL CONFERENCE ON HIGH PERFORMANCE AND SMART COMPUTING (HPSC) / IEEE INTL CONFERENCE ON INTELLIGENT DATA AND SECURITY (IDS), P113, DOI 10.1109/BigDataSecurity-HPSC-IDS.2019.00030
   Zhu Y, 2018, AS C COMP VIS, P363, DOI DOI 10.1007/978-3-030-20893-6
NR 122
TC 54
Z9 57
U1 9
U2 45
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 59
DI 10.1145/3422816
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100002
DA 2024-07-18
ER

PT J
AU Czekierda, L
   Zielinski, K
   Zielinski, S
AF Czekierda, Lukasz
   Zielinski, Krzysztof
   Zielinski, Slawomir
TI Automated Orchestration of Online Educational Collaboration in
   Cloud-based Environments
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Computer-supported collaboration; virtual learning environment; model
   driven architecture; orchestration; cloud-based collaboration;
   integrated collaboration environment
AB Integrated collaboration environments (ICEs) are widely used by corporations to increase productivity by fostering groupwide and interpersonal collaboration. In this article, we discuss the enhancements of such environment needed to build an educational ICE (E-ICE) that addresses the specific needs of educational users. The motivation for the research was the Malopolska Educational Cloud (MEC) project conducted by AGH University and its partners.
   The E-ICE developed by MEC project fosters collaboration between universities and high schools by creating an immersive virtual collaboration space. MEC is a unique project due to its scale and usage domain. Multiple online collaboration events are organized weekly between over 150 geographically scattered institutions. Such events, aside from videoconferencing, require various services. The MEC E-ICE is a complex composition of a significant number of services and various terminals that require very specific configuration and management.
   In this article, we focus on a model-driven approach to automating the organization of online meetings in their preparation, execution, and conclusion phases. We present a conceptual model of E-ICE-supported educational courses, introduce a taxonomy of online educational services, identify planes and modes of their operation, as well as discuss the most common collaboration patterns.
   The MEC E-ICE, which we present as a case study, is built in accordance with the presented, model-driven approach. MEC educational services are described in a way that allows for converting the declarative specification of E-ICE application models into platform-independent models, platform-specific models, and, finally, working sets of orchestrated service instances. Such approach both reduces the level of technical knowledge required from the end-users and considerably speeds up the construction of online educational collaboration environments.
C1 [Czekierda, Lukasz; Zielinski, Krzysztof; Zielinski, Slawomir] AGH Univ Sci & Technol, Fac Comp Sci Elect & Telecommun, Al Mickiewicza 30, PL-30059 Krakow, Poland.
C3 AGH University of Krakow
RP Czekierda, L (corresponding author), AGH Univ Sci & Technol, Fac Comp Sci Elect & Telecommun, Al Mickiewicza 30, PL-30059 Krakow, Poland.
EM luke@agh.edu.pl; kz@agh.edu.pl; slawek@agh.edu.pl
RI Zielinski, Slawomir/C-2522-2013
OI Zielinski, Slawomir/0000-0002-0824-2608; Zielinski,
   Krzysztof/0000-0002-0506-3073
CR Alim E. S., 2017, P 2 INT C INF TECHN, DOI [10.1109/ICITISEE.2017.8285563, DOI 10.1109/ICITISEE.2017.8285563]
   Balanskat A., 2006, European Schoolnet
   Balta N, 2019, EDUC INF TECHNOL, V24, P307, DOI 10.1007/s10639-018-9773-8
   Cai L., 2004, P IEEE INT C E COMM, DOI [10.1109/CECEAST.2004.4, DOI 10.1109/CECEAST.2004.4]
   Chia HP, 2014, COMPUT EDUC, V79, P1, DOI 10.1016/j.compedu.2014.07.005
   Collazos CA, 2019, J AMB INTEL HUM COMP, V10, P4789, DOI 10.1007/s12652-018-1165-9
   Czekierda L, 2017, 2017 IEEE 26TH INTERNATIONAL CONFERENCE ON ENABLING TECHNOLOGIES - INFRASTRUCTURE FOR COLLABORATIVE ENTERPRISES (WETICE), P80, DOI 10.1109/WETICE.2017.57
   Delgado V. Agredo, 2017, COLLABORATION INCREA, DOI [10.1007/978-3-319- 58562-8_2, DOI 10.1007/978-3-319-58562-8_2]
   El Mhouti A., 2016, P 3 INT C SYST COLL, DOI [10.1109/SYSCO.2016.7831340, DOI 10.1109/SYSCO.2016.7831340]
   Engelmann T, 2009, COMPUT HUM BEHAV, V25, P949, DOI 10.1016/j.chb.2009.04.004
   Ficheman I. K., 2002, P S IB AM COMP GRAF
   Giesbers B, 2013, COMPUT HUM BEHAV, V29, P285, DOI 10.1016/j.chb.2012.09.005
   Gutwin C., 2002, Computer Supported Cooperative Work: The Journal of Collaborative Computing, V11, P411, DOI 10.1023/A:1021271517844
   Hargreaves A., 2018, Collaborative professionalism: When teaching together means learning for all
   Hu H., 2015, E-LEARNING DIGITAL M, V12, P17, DOI DOI 10.1177/2042753014558373
   Kobbe L, 2007, INT J COMP-SUPP COLL, V2, P211, DOI 10.1007/s11412-007-9014-4
   Kolfschoten G. L., 2004, LECT NOTES COMPUTER, V3198, DOI [10.1007/ 978- 3- 540- 30112- 7_12, DOI 10.1007/978-3-540-30112-7_12]
   Lesko Jr C. J., 2012, VIRTUAL REALITY ENV
   Luna W., 2015, CREAT ED, V6, P1435
   Mahapatra J, 2016, 13TH WEB FOR ALL CONFERENCE MONTREAL, CANADA 2016, DOI 10.1145/2899475.2899485
   Meyer B, 2015, INTERACT TECHNOL SMA, V12, P270, DOI 10.1108/ITSE-09-2015-0027
   Psannis KE, 2019, IEEE T SUST COMPUT, V4, P77, DOI 10.1109/TSUSC.2018.2817043
   Raths D., 2015, 6 WAYS VIDEOCONFEREN
   Rhazali Y, 2016, INT J CLOUD APPL COM, V6, P11, DOI 10.4018/IJCAC.2016040102
   Rivera LFZ, 2016, INT J ONLINE ENG, V12, P14, DOI 10.3991/ijoe.v12i09.6129
   Rodriguez Dario, 2014, Lecture Notes on Software Engineering, V2, P76, DOI 10.7763/LNSE.2014.V2.98
   Savolainen J., 2016, THESIS JAMK U APPL S
   Seewungkum D., 2012, 1st International Conference on Future Generation Communication Technologies, FGCT 2012, P156, DOI [DOI 10.1109/FGCT.2012.6476574, 10.1109/FGCT.2012.6476574]
   Stefan L, 2012, PROCD SOC BEHV, V51, P1056, DOI 10.1016/j.sbspro.2012.08.287
   Tang Y, 2017, COMPUT EDUC, V106, P97, DOI 10.1016/j.compedu.2016.12.004
   Treffz H., 2010, VIRTUAL COLLABORATIV
   Wahls N., 2017, COLLABORATIVE LEARNI
   Wang AI, 2015, COMPUT EDUC, V82, P217, DOI 10.1016/j.compedu.2014.11.004
   Wang Y, 2013, P ACM SIGITE C INF T, DOI [10.1145/2512276.2512279, DOI 10.1145/2512276.2512279]
   Webb M., 2004, Technology, Pedagogy and Education, V13, P235, DOI 10.1080/14759390400200183
   Williams J., 2015, COLLABORATIVE LEARNI
NR 36
TC 1
Z9 1
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 31
DI 10.1145/3412381
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200011
DA 2024-07-18
ER

PT J
AU Qi, LY
   Song, HB
   Zhang, XY
   Srivastava, GT
   Xu, XL
   Yu, S
AF Qi, Lianyong
   Song, Houbing
   Zhang, Xuyun
   Srivastava, Gautam
   Xu, Xiaolong
   Yu, Shui
TI Compatibility-Aware Web API Recommendation for Mashup Creation via
   Textual Description Mining
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Text document; textual description mining; APIs recommendation;
   compatibility; mashup creation
ID SERVICE; THINGS
AB With the ever-increasing prosperity of web Application Programming Interface (API) sharing platforms, it is becoming an economic and efficient way for software developers to design their interested mashups through web API re-use. Generally, a software developer can browse, evaluate, and select his or her preferred web APIs from the API's sharing platforms to create various mashups with rich functionality. The big volume of candidate APIs places a heavy burden on software developers' API selection decisions. This, in turn, calls for the support of intelligent API recommender systems. However, existing API recommender systems often face two challenges. First, they focus more on the functional accuracy of APIs while neglecting the APIs' actual compatibility. This then creates incompatible mashups. Second, they often require software developers to input a set of keywords that can accurately describe the expected functions of the mashup to be developed. This second challenge tests partial developers who have little background knowledge in the fields. To tackle the above-mentioned challenges, in this article we propose a compatibility-aware and text description-driven web API recommendation approach (named WARtext). WARtext guarantees the compatibility among the recommended APIs by utilizing the APIs' composition records produced by historicalmashup creations. Besides, WARtext entitles a software developer to type a simple text document that describes the expected mashup functions as input. Then through textual description mining, WARtext can precisely capture the developers' functional requirements and then return a set of APIs with the highest compatibility. Finally, through a realworld mashup dataset ProgrammableWeb, we validate the feasibility of our novel approach.
C1 [Qi, Lianyong] Qufu Normal Univ, Sch Comp Sci, Rizhao, Shandong, Peoples R China.
   [Qi, Lianyong; Xu, Xiaolong] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
   [Song, Houbing] Embry Riddle Aeronaut Univ, Dept Elect Engn & Comp Sci, Daytona Beach, FL USA.
   [Zhang, Xuyun] Macquarie Univ, Dept Comp, Sydney, NSW, Australia.
   [Srivastava, Gautam] Brandon Univ, Dept Math & Comp Sci, Brandon, MB, Canada.
   [Srivastava, Gautam] China Med Univ, Res Ctr Interneural Comp, Taichung, Taiwan.
   [Xu, Xiaolong] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing, Jiangsu, Peoples R China.
   [Yu, Shui] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW, Australia.
C3 Qufu Normal University; Nanjing University; Embry-Riddle Aeronautical
   University; Macquarie University; Brandon University; China Medical
   University Taiwan; Nanjing University of Information Science &
   Technology; University of Technology Sydney
RP Xu, XL (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.; Xu, XL (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing, Jiangsu, Peoples R China.
EM lianyongqi@gmail.com; h.song@ieee.org; xuyun.zhang@mq.edu.au;
   srivastavag@brandonu.ca; njuxlxu@gmail.com; Shui.Yu@uts.edu.au
RI Song, Houbing Herbert/E-3628-2010; Qi, Lianyong/AAO-2681-2020; song,
   hu/JVO-3838-2024; zhang, xu/GYE-3558-2022; Yu, Shui/AFL-2699-2022;
   Srivastava, Gautam/N-5668-2019; ZHANG, XUCHEN/KBB-7989-2024
OI Song, Houbing Herbert/0000-0003-2631-9223; Yu, Shui/0000-0003-4485-6743;
   Srivastava, Gautam/0000-0001-9851-4103; Xu,
   Xiaolong/0000-0003-4879-9803; Zhang, Xuyun/0000-0001-7353-4159
FU National Natural Science Foundation of China [61872219, 61672276];
   Natural Science Foundation of Shandong Province [ZR2019MF001]; Open
   Project of State Key Laboratory for Novel Software Technology
   [KFKT2020B08]; Fundamental Research Funds for the Central Universities
   [30919011282]
FX This work was supported by the National Natural Science Foundation of
   China (No. 61872219, No. 61672276), the Natural Science Foundation of
   Shandong Province (ZR2019MF001), and the Open Project of State Key
   Laboratory for Novel Software Technology (No. KFKT2020B08). The
   Fundamental Research Funds for the Central Universities under Grant No.
   30919011282.
CR Almarimi N, 2019, APPL SOFT COMPUT, V85, DOI 10.1016/j.asoc.2019.105830
   Buqing Cao, 2020, IEEE Transactions on Services Computing, V13, P99, DOI 10.1109/TSC.2017.2686390
   Cai TT, 2022, IEEE T KNOWL DATA EN, V34, P1993, DOI 10.1109/TKDE.2020.3003047
   Chen NX, 2018, IEEE T SERV COMPUT, V11, P49, DOI 10.1109/TSC.2016.2533348
   Chen W., P IEEE INT C BIG DAT, P4573
   Cheng B, 2017, IEEE COMMUN MAG, V55, P115, DOI 10.1109/MCOM.2017.1600309
   Chi XX, 2022, CONCURR COMP-PRACT E, V34, DOI 10.1002/cpe.5681
   Deng S., IEEE INT C WEB SERV, P708
   Ding ST, 2020, NEUROCOMPUTING, V398, P520, DOI 10.1016/j.neucom.2019.04.095
   Gao W, 2017, 2017 IEEE 24TH INTERNATIONAL CONFERENCE ON WEB SERVICES (ICWS 2017), P65, DOI 10.1109/ICWS.2017.17
   Gao Z., 2020, ACM T MULTIMEDIA COM
   Gao Z., 2020, NEURAL NETWORKS, V2020
   Gao Z, 2019, IEEE INTERNET THINGS, V6, P9280, DOI 10.1109/JIOT.2019.2911669
   Gu Q, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON WEB SERVICES (ICWS), P452, DOI 10.1109/ICWS.2016.65
   He Q, 2014, IEEE T SOFTWARE ENG, V40, P192, DOI 10.1109/TSE.2013.2297911
   Hsu CF, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2886778
   Huang G, 2015, IEEE T SERV COMPUT, V8, P494, DOI 10.1109/TSC.2014.2347293
   Kou H., 2020, KNOWL-BASED SYST, V2020
   Li JX, 2020, INFORM SYST, V92, DOI 10.1016/j.is.2020.101522
   Liang TT, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON WEB SERVICES (ICWS), P436, DOI 10.1109/ICWS.2016.63
   Liu HW, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/2085638
   Liu HW, 2019, EURASIP J WIREL COMM, V2019, DOI 10.1186/s13638-019-1561-7
   Marcantoni F, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3063, DOI 10.1145/3308558.3313539
   Michel F, 2019, COMPANION OF THE WORLD WIDE WEB CONFERENCE (WWW 2019 ), P883, DOI 10.1145/3308560.3317073
   Ota K, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092831
   Pan WF, 2018, FUTURE GENER COMP SY, V87, P267, DOI 10.1016/j.future.2018.04.052
   Qi LY, 2022, IEEE T BIG DATA, V8, P685, DOI 10.1109/TBDATA.2020.2975587
   Qi LY, 2019, IEEE T COMPUT SOC SY, V6, P1063, DOI 10.1109/TCSS.2019.2906925
   Shi M, 2019, IEEE T PARALL DISTR, V30, P1077, DOI 10.1109/TPDS.2018.2877363
   Tan W, 2016, IEEE INTERNET COMPUT, V20, P64, DOI 10.1109/MIC.2016.74
   Wan S., 2020, IEEE T MULTIMEDIA, V2020
   Yao LN, 2021, IEEE T SERV COMPUT, V14, P502, DOI 10.1109/TSC.2018.2803171
   Zhong WY, 2020, COMPUT COMMUN, V157, P116, DOI 10.1016/j.comcom.2020.04.018
   Zhong Y, 2018, IEEE T AUTOM SCI ENG, V15, P468, DOI 10.1109/TASE.2016.2624310
   Zhou CJ, 2020, EXPERT SYST APPL, V151, DOI 10.1016/j.eswa.2020.113361
   Zhou XK, 2021, IEEE ACM T COMPUT BI, V18, P912, DOI 10.1109/TCBB.2020.2994780
   Zhou XK, 2020, IEEE INTERNET THINGS, V7, P6429, DOI 10.1109/JIOT.2020.2985082
   Zhou XK, 2021, IEEE T EMERG TOP COM, V9, P246, DOI 10.1109/TETC.2018.2860051
NR 38
TC 45
Z9 45
U1 3
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 20
DI 10.1145/3417293
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900020
DA 2024-07-18
ER

PT J
AU Nie, WZ
   Liang, Q
   Wang, YX
   Wei, X
   Su, YT
AF Nie, Weizhi
   Liang, Qi
   Wang, Yixin
   Wei, Xing
   Su, Yuting
TI MMFN: Multimodal Information Fusion Networks for 3D Model Classification
   and Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D retrieval; multimodal; feature fusion; point cloud
ID CONVOLUTIONAL NEURAL-NETWORKS; OBJECT RETRIEVAL; REPRESENTATION;
   FEATURES
AB In recent years, research into 3D shape recognition in the field of multimedia and computer vision has attracted wide attention. With the rapid development of deep learning, various deep models have achieved state-of-the-art performance based on different representations. There are many modalities for representing a 3D model, such as point cloud, multiview, and panorama view. Deep learning models based on these different modalities have different concerns, and all of them have achieved high performance for 3D shape recognition. However, all of these methods ignore the multimodality information in conditions where the same 3D model is represented by different modalities. Thus, we can obtain a better descriptor by guiding the training to consider these multiple representations. In this article, we propose MMFN, a novel multimodal fusion network for 3D shape recognition that employs correlations between the different modalities to generate a fused descriptor, which is more robust. In particular, we design two novel loss functions to help the model learn the correlation information during training. The first is correlation loss, which focuses on the correlations among different descriptors generated from different structures. This approach reduces the training time and improves the robustness of the fused descriptor of the 3D model The second is instance loss, which preserves the independence of each modality and utilizes feature differentiation to guide model learning during the training process. More specifically, we use the weighted fusion method, which applies statistical methods to obtain robust descriptors that maximize the advantages of the information from the different modalities. We evaluated the proposed method on the Model-Net40 and ShapeNetCore55 datasets for 3D shape classification and retrieval tasks. The experimental results and comparisons with state-of-the-art methods demonstrate the superiority of our approach.
C1 [Nie, Weizhi; Liang, Qi; Wang, Yixin; Su, Yuting] Tianjin Univ, Tianjin, Peoples R China.
   [Wei, Xing] Guilin Univ Aerosp Technol, Guilin, Peoples R China.
C3 Tianjin University; Guilin University of Aerospace Technology
RP Liang, Q (corresponding author), Tianjin Univ, Tianjin, Peoples R China.; Wei, X (corresponding author), Guilin Univ Aerosp Technol, Guilin, Peoples R China.
EM weizhinie@tju.edu.cn; tjuliangqi@tju.edu.cn; tjuwangyixin@163.com;
   wxaiqiqi@163.com; ytsu@tju.edu.cn
RI Liang, Qi/ABF-4426-2021; Nie, Weizhi/ABF-5316-2021
OI nie, weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China [61772359, 61572356,
   61872267]; 2019 Tianjin New Generation Artificial Intelligence Major
   Program; 2018 Tianjin New Generation Artificial Intelligence Major
   Program [18ZXZNGX00150]; Open Project Program of the State Key Lab of
   CAD & CG, Zhejiang University [A1907]; Elite Scholar Program of Tianjin
   University [2019XRX-0035]
FX This work was supported in part by the National Natural Science
   Foundation of China (61772359, 61572356, 61872267), a grant from the
   2019 Tianjin New Generation Artificial Intelligence Major Program, a
   grant from the 2018 Tianjin New Generation Artificial Intelligence Major
   Program (18ZXZNGX00150), the Open Project Program of the State Key Lab
   of CAD & CG (A1907)H, Zhejiang University, and a grant from the Elite
   Scholar Program of Tianjin University (2019XRX-0035).
CR [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai G., 2018, IEEE T IMAGE PROCESS, V27, P3374
   DYN N, 1990, ACM T GRAPHIC, V9, P160, DOI 10.1145/78956.78958
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   González A, 2017, IEEE T CYBERNETICS, V47, P3980, DOI 10.1109/TCYB.2016.2593940
   Guo HY, 2016, IEEE T IMAGE PROCESS, V25, P5526, DOI 10.1109/TIP.2016.2609814
   Han ZZ, 2019, IEEE I CONF COMP VIS, P10441, DOI 10.1109/ICCV.2019.01054
   Han ZZ, 2019, AAAI CONF ARTIF INTE, P8376
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   Han ZZ, 2019, IEEE T CYBERNETICS, V49, P481, DOI 10.1109/TCYB.2017.2778764
   Han ZZ, 2018, IEEE T IMAGE PROCESS, V27, P3049, DOI 10.1109/TIP.2018.2816821
   Han ZZ, 2017, IEEE T NEUR NET LEAR, V28, P2268, DOI 10.1109/TNNLS.2016.2582532
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Hinton Geoffrey, 2014, NIPS DEEP LEARN WORK
   Johnson Rie, 2013, P 26 INT C NEURAL IN, P315
   Kazhdan M., 2003, P EUR ACM SIGGRAPH S, V6, P156
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Li B, 2013, MULTIMED TOOLS APPL, V62, P821, DOI 10.1007/s11042-011-0873-3
   Li Y., 2018, ADV NEURAL INFORM PR, P820, DOI DOI 10.48550/ARXIV.1801.07791
   Liu AA, 2019, IEEE T CIRC SYST VID, V29, P868, DOI 10.1109/TCSVT.2018.2810191
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Nie WZ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P908, DOI 10.1145/3343031.3351009
   Pahwa RS, 2018, IEEE T CIRC SYST VID, V28, P626, DOI 10.1109/TCSVT.2016.2616143
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Papaioannidis C, 2020, IEEE T CIRC SYST VID, V30, P2683, DOI 10.1109/TCSVT.2019.2929600
   Peng J, 2019, INT CONF ACOUST SPEE, P4115, DOI 10.1109/ICASSP.2019.8682753
   Poria S, 2017, IEEE DATA MINING, P1033, DOI 10.1109/ICDM.2017.134
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Savva M., 2017, EUR WORKSH 3D OBJ RE, P39
   Sfikas K., 2017, EXPLOITING PANORAMA, P1, DOI 10.2312/3dor.20171045
   Sfikas K, 2018, COMPUT GRAPH-UK, V71, P208, DOI 10.1016/j.cag.2017.12.001
   Shi HY, 2019, IEEE T NEUR NET LEAR, V30, P2963, DOI 10.1109/TNNLS.2018.2869747
   Socher R., 2012, NIPS, V3, P8
   Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tatsuma A, 2009, VISUAL COMPUT, V25, P785, DOI 10.1007/s00371-008-0304-2
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Xie J, 2017, PROC CVPR IEEE, P3615, DOI 10.1109/CVPR.2017.385
   Xie J, 2017, IEEE T PATTERN ANAL, V39, P1335, DOI 10.1109/TPAMI.2016.2596722
   Yoon GJ, 2017, NEUROCOMPUTING, V267, P556, DOI 10.1016/j.neucom.2017.06.034
   You HX, 2019, AAAI CONF ARTIF INTE, P9119
   You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702
   Zhang ZZ, 2018, IEEE T IMAGE PROCESS, V27, P5957, DOI 10.1109/TIP.2018.2862625
   Zhu Z., 2018, IEEE T CIRCUITS SYST, V29, P3317
NR 56
TC 3
Z9 4
U1 2
U2 28
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 131
DI 10.1145/3410439
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800015
DA 2024-07-18
ER

PT J
AU Zhang, DY
   Shao, J
   Shen, HT
AF Zhang, Dongyang
   Shao, Jie
   Shen, Heng Tao
TI Kernel Attention Network for Single Image Super-Resolution
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image super-resolution; kernel attention; receptive field; multi-scale
   features
ID CONVOLUTIONAL NETWORK
AB Recently, attention mechanisms have shown a developing tendency toward convolutional neural network (CNN), and some representative attention mechanisms, i.e., channel attention (CA) and spatial attention (SA) have been fully applied to single image super-resolution (SISR) tasks. However, the existing architectures directly apply these attention mechanisms to SISR without much consideration of the nature characteristic, resulting in less strong representational power. In this article, we propose a novel kernel attention module (KAM) for SISR, which enables the network to adjust its receptive field size corresponding to various scales of input by dynamically selecting the appropriate kernel. Based on this, we stack multiple kernel attention modules with group and residual connection to constitute a novel architecture for SISR, which enables our network to learn more distinguishing representations through filtering the information under different receptive fields. Thus, our network is more sensitive to multi-scale features, which enables our single network to deal with multi-scale SR task by predefining the upscaling modules. Besides, other attention mechanisms in super-resolution are also investigated and illustrated in detail in this article. Thanks to the kernel attention mechanism, the extensive benchmark evaluation shows that our method outperforms the other state-of-theart methods.
C1 [Zhang, Dongyang; Shao, Jie; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Ctr Future Media, Chengdu 611731, Peoples R China.
   [Zhang, Dongyang; Shao, Jie; Shen, Heng Tao] Sichuan Artificial Intelligence Res Inst, Yibin 644000, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Shao, J (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Ctr Future Media, Chengdu 611731, Peoples R China.; Shao, J (corresponding author), Sichuan Artificial Intelligence Res Inst, Yibin 644000, Peoples R China.
EM dyzhang@std.uestc.edu.cn; shaojie@std.uestc.edu.cn;
   shenhengtao@std.uestc.edu.cn
RI Shen, Heng Tao/ABD-5331-2021
OI zhang, dongyang/0000-0002-4839-0234
FU National Natural Science Foundation of China [61832001, 61672133,
   61632007]; Sichuan Science and Technology Program [2019YFG0535]
FX This work was supported by the National Natural Science Foundation of
   China (No. 61832001, No. 61672133, and No. 61632007), and Sichuan
   Science and Technology Program (No. 2019YFG0535).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N., 2018, P EUR C COMP VIS ECC, P252, DOI DOI 10.1007/978-3-030-01249-6_16
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Bulat A, 2018, LECT NOTES COMPUT SC, V11210, P187, DOI 10.1007/978-3-030-01231-1_12
   Chen J, 2020, PATTERN RECOGN LETT, V132, P62, DOI 10.1016/j.patrec.2018.06.030
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Guo Jianting, 2017, TOMCCAP, V13, P61
   Han W, 2018, PROC CVPR IEEE, P1654, DOI 10.1109/CVPR.2018.00178
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He Chen, 2019, TOMCCAP, V15, P26
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu YT, 2020, IEEE T CIRC SYST VID, V30, P3911, DOI 10.1109/TCSVT.2019.2915238
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Irani M., 1991, GRAPH MODEL IM PROC, V53, P3
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim JH, 2020, NEUROCOMPUTING, V402, P38, DOI 10.1016/j.neucom.2020.03.069
   Kingma D. P., 2014, arXiv
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li JZ, 2018, C ELECT INSUL DIEL P, P527, DOI 10.1109/CEIDP.2018.8544775
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li Xianguo, 2019, TOMCCAP, V15, P19
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liao Q, 2016, ARXIV160403640
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu H, 2018, NEUROCOMPUTING, V282, P52, DOI 10.1016/j.neucom.2017.12.014
   Mao XJ, 2016, ADV NEUR IN, V29
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mnih V, 2014, ADV NEUR IN, V27
   Park J., 2018, BRIT MACH VIS C, P147
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Wang Anqi, 2018, TOMCCAP, V14, P73
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yang JQ, 2008, ITESS: 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES, PT 1, P1, DOI 10.1109/CVPR.2008.4587647
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zhang DY, 2017, LECT NOTES COMPUT SC, V10636, P217, DOI 10.1007/978-3-319-70090-8_23
   Zhang K., 2017, PROC CVPR IEEE, P3929, DOI [DOI 10.1109/CVPR.2017.300, 10.1109/CVPR.2017.300]
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang L, 2019, COMPLEXITY, DOI 10.1155/2019/1671340
   Zhang YW, 2018, SMALL, V14, DOI 10.1002/smll.201800691
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 63
TC 20
Z9 20
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 90
DI 10.1145/3398685
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200016
DA 2024-07-18
ER

PT J
AU Zhang, RX
   Ma, M
   Huang, TC
   Pang, HT
   Yao, X
   Wu, CL
   Sun, LF
AF Zhang, Rui-Xiao
   Ma, Ming
   Huang, Tianchi
   Pang, Haitian
   Yao, Xin
   Wu, Chenglei
   Sun, Lifeng
TI A Practical Learning-based Approach for Viewer Scheduling in the
   Crowdsourced Live Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Crowdsourced live streaming; reinforcement learning; scheduling
AB Scheduling viewers effectively among different Content Delivery Network (CDN) providers is challenging owing to the extreme diversity in the crowdsourced live streaming (CLS) scenarios. Abundant algorithms have been proposed in recent years, which, however, suffer from a critical limitation: Due to their inaccurate feature engineering or naive rules, they cannot optimally schedule viewers. To address this concern, we put forward LTS (Learn to Schedule), a novel scheduling algorithm that can adapt to the dynamics from both viewer traffics and CDN performance. In detail, we first propose LTS-RL, an approach that schedules CLS viewers based on deep reinforcement learning (DRL). Since LTS-RL is trained in an end-to-end way, it can automatically learn scheduling algorithms without any pre-programmed models or assumptions about the environment dynamics. At the same time, to practically deploy LTS-RL. we then use the decision tree and imitation learning to convert LTS-RL into a more light-weighted and interpretable model, which is denoted as Fast-LTS. After the extensive evaluation of the real data from a leading CLS platform in China, we demonstrate that our proposed model (both LTS-RL and Fast-LTS) can improve the average quality of experience (QoE) over state-of-the-art approaches by 8.71-15.63%. At the same time, we also demonstrate that Fast-LTS can faithfully convert the complicated LTS-RL with slight performance degradation (<2%), while significantly reducing the decision time (x7-10).
C1 [Zhang, Rui-Xiao; Huang, Tianchi; Pang, Haitian; Sun, Lifeng] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing, Peoples R China.
   [Ma, Ming] Kuaishou Technol Co Ltd, Beijing, Peoples R China.
   [Yao, Xin; Wu, Chenglei] Minist Educ, Key Lab Pervas Comp, Beijing, Peoples R China.
C3 Tsinghua University
RP Sun, LF (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing, Peoples R China.
EM zhangrx17@mails.tsinghua.edu.cn; maming@kuaishou.com;
   htc19@mails.tsinghua.edu.cn; pht14@mails.tsinghua.edu.cn;
   yaox16@mails.tsinghua.edu.cn; wucl18@mails.tsinghua.edu.cn;
   sunlf@tsinghua.edu.cn
FU NSFC [61936011, 61521002]; National Key R&D Program of China
   [2018YFB1003703]; Beijing Key Lab of Networked Multimedia
FX This work is supported by the NSFC under Grant 61936011, 61521002,
   National Key R&D Program of China (No. 2018YFB1003703), and Beijing Key
   Lab of Networked Multimedia.
CR Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   Adhikari VK, 2012, IEEE CONF COMPUT, P7, DOI 10.1109/INFCOMW.2012.6193524
   Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   [Anonymous], 2018, Adv Neural Inf Process Syst
   [Anonymous], 2018, J FORESTRY RES
   Blockeel H, 1998, ARTIF INTELL, V101, P285, DOI 10.1016/S0004-3702(98)00034-4
   Breiman L., 2017, Classification and Regression Trees, DOI [10.1201/9781315139470-8, DOI 10.1201/9781315139470-8, DOI 10.1201/9781315139470]
   Chen F, 2015, IEEE T MULTIMEDIA, V17, P1471, DOI 10.1109/TMM.2015.2460193
   Dethise A, 2019, NETAI'19: PROCEEDINGS OF THE 2019 ACM SIGCOMM WORKSHOP ON NETWORK MEETS AI & ML, P29, DOI 10.1145/3341216.3342210
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Fu R, 2016, 2016 31ST YOUTH ACADEMIC ANNUAL CONFERENCE OF CHINESE ASSOCIATION OF AUTOMATION (YAC), P324, DOI 10.1109/YAC.2016.7804912
   Garivier A., 2008, ARXIV08053415
   Guo WB, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P364, DOI 10.1145/3243734.3243792
   Huang T., 2018, ARXIV180502482
   Huang TC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P429, DOI 10.1145/3343031.3351014
   Jiang JC, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P137
   Jiang Junchen, 2017, NSDI, V1, P3
   Klein Jessica, 2018, TWITCH ENDED 2017 15
   Liu HH, 2012, ACM SIGCOMM COMP COM, V42, P371, DOI 10.1145/2377677.2377753
   Liu X, 2012, ACM SIGCOMM COMP COM, V42, P359, DOI 10.1145/2377677.2377752
   Lundberg SM, 2017, ADV NEUR IN, V30
   Mann ZA, 2015, ACM COMPUT SURV, V48, DOI 10.1145/2797211
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Meng ZL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2431, DOI 10.1145/3343031.3350866
   Meng Zili, 2019, ARXIV191003835
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Pang HT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1217, DOI 10.1145/3240508.3240642
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Ross S, 2011, P 14 INT C ART INT S, P627
   Silver D, 2014, PR MACH LEARN RES, V32
   Stemm M., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P285, DOI 10.1109/INFCOM.2000.832198
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Tang FX, 2018, IEEE WIREL COMMUN, V25, P154, DOI 10.1109/MWC.2017.1700244
   Torres R, 2011, INT CON DISTR COMP S, P248, DOI 10.1109/ICDCS.2011.43
   Wang JM, 2014, IEEE ICC, P3118, DOI 10.1109/ICC.2014.6883800
   Wei Y, 2018, BIOMED RES INT, V2018, DOI 10.1155/2018/1012789
   Xu CZ, 2012, J PARALLEL DISTR COM, V72, P95, DOI 10.1016/j.jpdc.2011.10.003
   Xu Z., 2018, P C COMP COMM
   Yan B, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P73, DOI 10.1145/3123266.3123283
   Yan FY, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P731
   Zhang H, 2019, FOOD BIOSCI, V29, P55, DOI 10.1016/j.fbio.2019.03.009
   Zhu YF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1372, DOI 10.1145/3123266.3123384
NR 43
TC 3
Z9 4
U1 1
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 67
DI 10.1145/3397226
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FH
UT WOS:000583712600010
DA 2024-07-18
ER

PT J
AU Zhang, HY
   Zhang, HK
   Pirbhulal, S
   Wu, WQ
   De Albuquerque, VHC
AF Zhang, Hongyi
   Zhang, Haoke
   Pirbhulal, Sandeep
   Wu, Wanqing
   De Albuquerque, Victor Hugo C.
TI Active Balancing Mechanism for Imbalanced Medical Data in Deep
   Learning-Based Classification Models
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Biomedical; imbalanced data; myocardial infarction; Gaussian naive
   Bayes; entropy
ID SUPPORT VECTOR MACHINE; MYOCARDIAL-INFARCTION; SMOTE
AB Imbalanced data always has a serious impact on a predictive model, and most under-sampling techniques consume more time and suffer from loss of samples containing critical information during imbalanced data processing, especially in the biomedical field. To solve these problems, we developed an active balancing mechanism (ABM) based on valuable information contained in the biomedical data. ABM adopts the Gaussian naive Bayes method to estimate the object samples and entropy as a query function to evaluate sample information and only retains valuable samples of the majority class to achieve under-sampling. The Physikalisch Technische Bundesanstalt diagnostic electrocardiogram (ECG) database, including 5,173 normal ECG samples and 26,654 myocardial infarction ECG samples, is applied to verify the validity of ABM. At imbalance rates of 13 and 5, experimental results reveal that ABM takes 7.7 seconds and 13.2 seconds, respectively. Both results are significantly faster than five conventional under-sampling methods. In addition, at the imbalance rate of 13, ABM-based data obtained the highest accuracy of 92.23% and 97.52% using support vector machines and modified convolutional neural networks (MCNNs) with eight layers, respectively. At the imbalance rate of 5, the processed data by ABM also achieved the best accuracy of 92.31% and 98.46% based on support vector machines and MCNNs, respectively. Furthermore, ABM has better performance than two compared methods in F1-measure, G-means, and area under the curve. Consequently, ABM could be a useful and effective approach to deal with imbalanced data in general, particularly biomedical myocardial infarction ECG datasets, and the MCNN can also achieve higher performance compared to the state of the art.
C1 [Zhang, Hongyi; Zhang, Haoke] Xiamen Univ Technol, 600 Ligong Rd, Xiamen 361024, Fujian, Peoples R China.
   [Pirbhulal, Sandeep] Chinese Acad Sci, Shenzhen Inst Adv Technol, 1068 Xueyuan Ave, Shenzhen 518055, Peoples R China.
   [Wu, Wanqing] Sun Yat Sen Univ, 135 Xingang Xi Rd, Guangzhou 510275, Peoples R China.
   [De Albuquerque, Victor Hugo C.] Univ Fortaleza, Fortaleza, Ceara, Brazil.
C3 Xiamen University of Technology; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS; Sun Yat Sen University;
   Universidade Fortaleza
RP Wu, WQ (corresponding author), Sun Yat Sen Univ, 135 Xingang Xi Rd, Guangzhou 510275, Peoples R China.
EM zhanghongyi@xmut.edu.cn; hk.zhang@gmail.com; sandeep@siat.ac.cn;
   wuwanqing8133@gmail.com; victor.albuquerque@unifor.br
RI de Albuquerque, Victor Hugo C./C-3677-2016; Zhang, Haoke/AFH-2781-2022;
   zhang, hongyi/GWQ-7770-2022; Pirbhulal, Sandeep/X-7191-2019; Zhang,
   Hongyi/HNR-5465-2023
OI de Albuquerque, Victor Hugo C./0000-0003-3886-4309; Zhang,
   Haoke/0000-0001-7309-2506; Pirbhulal, Sandeep/0000-0003-0843-8974; Wu,
   Wanqing/0000-0001-7594-1487
FU General Logistics Department of PLA [BLB19J005]; National Natural
   Science Foundation of China [61873349, U180120019]; Guangdong Province
   Natural Science Fund [2017B010125001]; Guangzhou Science and Technology
   Planning Project [201704020079]; China Postdoctoral Science Foundation
   [2018M643256]; Brazilian National Council for Research and Development
   CNPq [304315/2017-6]
FX This work was supported in part by the General Logistics Department of
   PLA (BLB19J005) the National Natural Science Foundation of China (under
   grants 61873349, U180120019), the Guangdong Province Natural Science
   Fund (2017B010125001), the Guangzhou Science and Technology Planning
   Project (201704020079), and the China Postdoctoral Science Foundation
   (2018M643256). V. H. C. de Albuquerque received support from the
   Brazilian National Council for Research and Development CNPq (grant
   304315/2017-6).
CR Ahmad K, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3199668
   [Anonymous], 2007, IEEE T SYSTEMS MAN C, DOI DOI 10.1109/TSMC.1976.4309523
   [Anonymous], 2000, P 2000 INT C ART INT
   Baloglu UB, 2019, PATTERN RECOGN LETT, V122, P23, DOI 10.1016/j.patrec.2019.02.016
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen J, 2019, IEEE T INTELL TRANSP, V20, P4450, DOI 10.1109/TITS.2018.2886280
   Codella NCF, 2017, IBM J RES DEV, V61, DOI 10.1147/JRD.2017.2708299
   D'Addabbo A, 2015, PATTERN RECOGN LETT, V62, P61, DOI 10.1016/j.patrec.2015.05.008
   Drummond C., 2003, WORKSH LEARN IMB DAT, V11, P1
   Guo HX, 2017, EXPERT SYST APPL, V73, P220, DOI 10.1016/j.eswa.2016.12.035
   Han C, 2019, COMPUT METH PROG BIO, V175, P9, DOI 10.1016/j.cmpb.2019.03.012
   HART PE, 1968, IEEE T INFORM THEORY, V14, P515, DOI 10.1109/TIT.1968.1054155
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   Hossain MS, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3241056
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Kang Q, 2018, IEEE T NEUR NET LEAR, V29, P4152, DOI 10.1109/TNNLS.2017.2755595
   Li FL, 2018, INFORM SCIENCES, V422, P242, DOI 10.1016/j.ins.2017.09.013
   Li J, 2017, COATINGS, V7, DOI 10.3390/coatings7070094
   Liu CL, 2020, IEEE T KNOWL DATA EN, V32, P1543, DOI 10.1109/TKDE.2019.2905559
   Maratea A, 2014, INFORM SCIENCES, V257, P331, DOI 10.1016/j.ins.2013.04.016
   Nekooeimehr I, 2016, EXPERT SYST APPL, V46, P405, DOI 10.1016/j.eswa.2015.10.031
   PAN J, 1985, IEEE T BIO-MED ENG, V32, P230, DOI 10.1109/TBME.1985.325532
   Pirbhulal S., 2017, SENSOR SWITZERLAND, V17, P1, DOI DOI 10.3390/S17030606
   Pirbhulal S, 2018, IEEE T BIO-MED ENG, V65, P2751, DOI 10.1109/TBME.2018.2815155
   Provost Foster, 1997, P 15 INT C MACH LEAR, P445
   Prusa Joseph D., 2016, P 29 INT FLAIRS C
   Richhariya B, 2018, APPL SOFT COMPUT, V71, P418, DOI 10.1016/j.asoc.2018.07.003
   Sadhukhan D, 2018, IEEE T INSTRUM MEAS, V67, P2303, DOI 10.1109/TIM.2018.2816458
   Soleymani R, 2018, EXPERT SYST APPL, V101, P271, DOI 10.1016/j.eswa.2018.01.023
   Sun YM, 2006, IEEE DATA MINING, P592, DOI 10.1109/icdm.2006.29
   Susan S, 2019, APPL SOFT COMPUT, V78, P141, DOI 10.1016/j.asoc.2019.02.028
   TOMEK I, 1976, IEEE T SYST MAN CYB, V6, P769, DOI 10.1109/tsmc.1976.4309452
   Tsai CF, 2019, INFORM SCIENCES, V477, P47, DOI 10.1016/j.ins.2018.10.029
   Wang YD, 2019, IEEE ACCESS, V7, P39974, DOI 10.1109/ACCESS.2019.2902846
   Wu WQ, 2019, IEEE J BIOMED HEALTH, V23, P703, DOI 10.1109/JBHI.2018.2832069
   Wu WQ, 2018, FUTURE GENER COMP SY, V86, P515, DOI 10.1016/j.future.2018.04.024
   Wu WQ, 2015, IEEE SENS J, V15, P7087, DOI 10.1109/JSEN.2015.2470638
   Xiao YW, 2018, COMPUT METH PROG BIO, V153, P1, DOI 10.1016/j.cmpb.2017.09.005
   Yan YL, 2015, IEEE INT SYM MULTIM, P483, DOI 10.1109/ISM.2015.126
   Yen SJ, 2009, EXPERT SYST APPL, V36, P5718, DOI 10.1016/j.eswa.2008.06.108
   Yonglong Tian, 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3264947
   Yu HL, 2019, IEEE T FUZZY SYST, V27, P2353, DOI 10.1109/TFUZZ.2019.2898371
   Zhang C, 2019, IEEE T NEUR NET LEAR, V30, P109, DOI 10.1109/TNNLS.2018.2832648
   Zhang JMI, 2003, P ICML 2003 WORKSH L
   Zhang QZ, 2018, J IND INTEGR MANAG, V3, DOI 10.1142/S2424862218500112
   Zhang XX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2055, DOI 10.1145/2939672.2939854
   Zhang YF, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2768, DOI 10.1145/3219819.3219948
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
NR 48
TC 32
Z9 33
U1 1
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 39
DI 10.1145/3357253
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300021
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Xing, M
   Feng, ZY
   Su, Y
   Zhang, JH
AF Xing, Meng
   Feng, Zhiyong
   Su, Yong
   Zhang, Jianhai
TI An Image Cues Coding Approach for 3D Human Pose Estimation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Ambiguity in 3D pose recovery; human perception mechanism; image cues;
   code of 3D pose; matching mechanism
AB Although Deep Convolutional Neural Networks (DCNNs) facilitate the evolution of 3D human pose estimation, ambiguity remains the most challenging problem in such tasks. Inspired by the Human Perception Mechanism (HPM), we propose an image-to-pose coding method to fill the gap between image cues and 3D poses, thereby alleviating the ambiguity of 3D human pose estimation. First, in 3D pose space, we divide the whole 3D pose space into multiple subregions named pose codes, turning a disambiguation problem into a classification problem. The proposed coding mechanism covers multiple camera views and provides a complete description for 3D pose space. Second, it is noteworthy that the articulated structure of the human body lies on a sophisticated product manifold and the error accumulation in the chain structure will undoubtedly affect the coding performance. Therefore, in image space, we extract the image cues from independent local image patches rather than the whole image. The mapping relationship between image cues and 3D pose codes is established by a set of DCNNs. The image-to-pose coding method transforms the implicit image cues into explicit constraints. Finally, the image-to-pose coding method is integrated into a linear matching mechanism to construct a 3D pose estimation method that effectively alleviates the ambiguity. We conduct extensive experiments on widely used public benchmarks. The experimental results show that our method effectively alleviates the ambiguity in 3D pose recovery and is robust to the variations of view.
C1 [Xing, Meng; Feng, Zhiyong; Su, Yong; Zhang, Jianhai] Tianjin Univ, 135 Yaguan Rd, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Xing, M (corresponding author), Tianjin Univ, 135 Yaguan Rd, Tianjin 300350, Peoples R China.
EM xingmeng@tju.edu.cn; zyfeng@tju.edu.cn; suyong@tju.edu.cn;
   zhangjianhai@tju.edu.cn
RI su, yong/JEO-5411-2023; Feng, Zhi-Yong/I-7541-2016
OI su, yong/0000-0002-6851-4142; Meng, Xing/0000-0001-6082-4675; Zhang,
   Jianhai/0000-0002-0330-6908
FU Shenzhen Science and Technology Foundation [JCYJ20170816093943197]
FX This research was supported by Shenzhen Science and Technology
   Foundation grant no. JCYJ20170816093943197.
CR Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751
   Amin S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.45
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Andriluka M, 2010, PROC CVPR IEEE, P623, DOI 10.1109/CVPR.2010.5540156
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Chen ML, 2015, ASIAPAC SIGN INFO PR, P1041, DOI 10.1109/APSIPA.2015.7415430
   Ching-Hang Chen, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5759, DOI 10.1109/CVPR.2017.610
   Everingham M., 2010, BMVC, V2, P5
   Fan XC, 2014, LECT NOTES COMPUT SC, V8689, P174, DOI 10.1007/978-3-319-10590-1_12
   Fang HS, 2018, AAAI CONF ARTIF INTE, P6821
   Haque A, 2016, LECT NOTES COMPUT SC, V9905, P160, DOI 10.1007/978-3-319-46448-0_10
   Hofmann M, 2012, INT J COMPUT VISION, V96, P103, DOI 10.1007/s11263-011-0451-1
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Kostrikov I., 2014, BMVC, V1, P5
   LEE HJ, 1985, COMPUT VISION GRAPH, V30, P148, DOI 10.1016/0734-189X(85)90094-5
   Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P494, DOI 10.1109/TPAMI.2019.2894422
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie BX, 2017, IEEE I CONF COMP VIS, P3467, DOI 10.1109/ICCV.2017.373
   Parameswaran Vasu, 2004, P 2004 IEEE COMP SOC, V2, pII
   Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138
   Ramakrishna V, 2012, LECT NOTES COMPUT SC, V7575, P573, DOI 10.1007/978-3-642-33765-9_41
   Rogez G., 2016, Adv. Neural Inf. Process. Syst., P3116
   Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Simo-Serra E, 2012, PROC CVPR IEEE, P2673, DOI 10.1109/CVPR.2012.6247988
   Su Y, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3180420
   Taylor CJ, 2000, PROC CVPR IEEE, P677, DOI 10.1109/CVPR.2000.855885
   Tekin B, 2016, PROC CVPR IEEE, pCP8, DOI 10.1109/CVPR.2016.113
   Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603
   Wei XLK, 2009, IEEE I CONF COMP VIS, P1873, DOI 10.1109/ICCV.2009.5459415
   Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535
   Zhou XW, 2019, IEEE T PATTERN ANAL, V41, P901, DOI 10.1109/TPAMI.2018.2816031
   Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
NR 42
TC 2
Z9 2
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 113
DI 10.1145/3368066
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800013
DA 2024-07-18
ER

PT J
AU Zhao, RW
   Zhang, Q
   Wu, ZX
   Li, JG
   Jiang, YG
AF Zhao, Rui-Wei
   Zhang, Qi
   Wu, Zuxuan
   Li, Jianguo
   Jiang, Yu-Gang
TI Visual Content Recognition by Exploiting Semantic Feature Map with
   Attention and Multi-task Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image representation; contextual fusion; image classification; video
   classification
ID LATE FUSION
AB Recent studies have shown that spatial relationships among objects are very important for visual recognition, since they can provide rich clues on object contexts within the images. In this article, we introduce a novel method to learn the Semantic Feature Map (SFM) with attention-based deep neural networks for image and video classification in an end-to-end manner, aiming to explicitly model the spatial object contexts within the images. In particular, we explicitly apply the designed gate units to the extracted object features for important objects selection and noise removal. These selected object features are then organized into the proposed SFM, which is a compact and discriminative representation with the spatial information among objects preserved. Finally, we employ either Fully Convolutional Networks (FCN) or Long-Short Term Memory (LSTM) as the classifiers on top of the SFM for content recognition. A novel multi-task learning framework with image classification loss, object localization loss, and grid labeling loss are also introduced to help better learn the model parameters. We conduct extensive evaluations and comparative studies to verify the effectiveness of the proposed approach on Pascal VOC 2007/2012 and MS-COCO benchmarks for image classification. In addition, the experimental results also show that the SFMs learned from the image domain can be successfully transferred to CCV and FCVID benchmarks for video classification.
C1 [Zhao, Rui-Wei; Zhang, Qi; Jiang, Yu-Gang] Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Sch Comp Sci, Shanghai, Peoples R China.
   [Wu, Zuxuan] Univ Maryland, College Pk, MD 20742 USA.
   [Li, Jianguo] Intel Labs China, Beijing, Peoples R China.
C3 Fudan University; University System of Maryland; University of Maryland
   College Park; Intel Corporation
RP Jiang, YG (corresponding author), Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Sch Comp Sci, Shanghai, Peoples R China.
EM rwzhao14@fudan.edu.cn; qz@fudan.edu.cn; zxwu@cs.umd.edu;
   jianguo.li@intel.com; ygj@fudan.edu.cn
FU NSF China [61572134, 61622204]; STCSM, Shanghai, China [16QA1400500]
FX This work was supported by two projects from NSF China (Grants No.
   61572134 and No. 61622204) and a project from STCSM, Shanghai, China
   (Grant No. 16QA1400500).
CR [Anonymous], P 2017 ACM MULT C AC
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2016, P 24 ACM INT C MULT
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], P INT C LEARN REPR W
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], ACM INT C IM VID
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2015, ARXIV150902470V1
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P BMVC
   [Anonymous], ARXIV150405843
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Dong J, 2013, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2013.112
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jhuo IH, 2014, MACH VISION APPL, V25, P33, DOI 10.1007/s00138-013-0567-0
   Jiang Y., 2011, P ACM INT C MULT RET
   Jiang YG, 2015, IEEE T IMAGE PROCESS, V24, P3781, DOI 10.1109/TIP.2015.2456412
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Li LJ, 2014, INT J COMPUT VISION, V107, P20, DOI 10.1007/s11263-013-0660-x
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu D, 2013, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.2013.109
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ma AJ, 2014, INT J COMPUT VISION, V109, P233, DOI 10.1007/s11263-014-0723-7
   Mettes P, 2016, COMPUT VIS IMAGE UND, V152, P131, DOI 10.1016/j.cviu.2016.07.008
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pang L, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818711
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang M, 2016, IEEE T IMAGE PROCESS, V26, P5678, DOI 10.1109/TIP.2016.2612829
   Wei Y, 2015, EVID-BASED COMPL ALT, V2015, DOI 10.1155/2015/340126
   Wu RB, 2015, IEEE I CONF COMP VIS, P1287, DOI 10.1109/ICCV.2015.152
   Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xu K., 2015, COMPUTER SCI, P2048
   Xu ZW, 2013, IEEE I CONF COMP VIS, P3440, DOI 10.1109/ICCV.2013.427
   Yang XS, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2962719
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
   Zuo Z, 2016, IEEE T IMAGE PROCESS, V25, P2983, DOI 10.1109/TIP.2016.2548241
NR 61
TC 6
Z9 6
U1 4
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 6
DI 10.1145/3231739
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100006
DA 2024-07-18
ER

PT J
AU Zhu, JJ
   Wei, YX
   Feng, YF
   Zhao, XB
   Gao, Y
AF Zhu, Junjie
   Wei, Yuxuan
   Feng, Yifan
   Zhao, Xibin
   Gao, Yue
TI Physiological Signals-based Emotion Recognition via High-order
   Correlation Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Emotion recognition; physiological signals; multi-modal fusion;
   multi-hypergraph neural networks
ID EEG; ATTENTION; MODELS
AB Emotion recognition by physiological signals is an effective way to discern the inner state of human beings and therefore has been widely adopted in many user-centered applications. The majority of current state-of-the-art methods focus on exploring relationship among emotion and physiological signals. Given some particular features of the natural process of emotional expression, it is still a challenging and urgent issue to efficiently combine such high-order correlations among multimodal physiological signals and subjects. To tackle the problem, a novel multi-hypergraph neural networks is proposed, in which one hypergraph is established with one type of physiological signals to formulate inter-subject correlations. Each one of the vertices in a hypergraph stands for one subject with a description of its related stimuli, and the complex correlations among the vertices can be formulated through hyperedges. With the multi-hypergraph structure of the subjects, emotion recognition is translated into classification of vertices in the multi-hypergraph structure. Experimental results with the DEAP dataset and ASCERTAIN dataset demonstrate that the proposed method outperforms the current state-of-the-art methods.
C1 [Zhu, Junjie; Wei, Yuxuan; Zhao, Xibin; Gao, Yue] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Feng, Yifan] Xiamen Univ, Xiamen 361005, Peoples R China.
C3 Tsinghua University; Xiamen University
RP Zhao, XB; Gao, Y (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM zhujj18@mails.tsinghua.edu.cn; weiyx15@mails.tsinghua.edu.cn;
   evanfeng97@gmail.com; zxb@tsinghua.edu.cn; gaoyue@tsinghua.edu.cn
RI zhu, junjie/JDV-8211-2023; Gao, Yue/B-3376-2012
FU National Key R&D Program of China [2017YFC0113000]; National Natural
   Science Funds of China [U1801263, U1701262]
FX This work was supported by National Key R&D Program of China (Grant No.
   2017YFC0113000) and National Natural Science Funds of China (U1801263,
   U1701262).
CR [Anonymous], 2015, P 1 INT WORKSH AFF S
   [Anonymous], 2008, 30th Annual Conference of the Cognitive Science Society
   Babiloni C, 2004, COGNITIVE BRAIN RES, V19, P259, DOI 10.1016/j.cogbrainres.2003.12.010
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Basar E, 2001, INT J PSYCHOPHYSIOL, V39, P241, DOI 10.1016/S0167-8760(00)00145-8
   Berger H, 1929, ARCH PSYCHIAT NERVEN, V87, P527, DOI 10.1007/BF01797193
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Cambria E, 2016, IEEE INTELL SYST, V31, P102, DOI 10.1109/MIS.2016.31
   Chung SY, 2012, 2012 12TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1768
   EKMAN P, 1983, SCIENCE, V221, P1208, DOI 10.1126/science.6612338
   el Kaliouby R, 2005, REAL-TIME VISION FOR HUMAN-COMPUTER INTERACTION, P181
   Feng B, 2018, PROCEEDINGS OF THE 2018 EURO-ASIA CONFERENCE ON ENVIRONMENT AND CSR: TOURISM, SOCIETY AND EDUCATION SESSION (PART III), P18
   Fernandez R, 2003, SPEECH COMMUN, V40, P145, DOI 10.1016/S0167-6393(02)00080-8
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Healey JA, 2005, IEEE T INTELL TRANSP, V6, P156, DOI 10.1109/TITS.2005.848368
   Huang Yung-Fu, 2011, IEEE APEC, V2011, P6
   Jenke R, 2014, IEEE T AFFECT COMPUT, V5, P327, DOI 10.1109/TAFFC.2014.2339834
   Kim J., 2005, P 9 EUR C SPEECH COM
   Kim J, 2008, IEEE T PATTERN ANAL, V30, P2067, DOI 10.1109/TPAMI.2008.26
   Knyazev GG, 2007, NEUROSCI BIOBEHAV R, V31, P377, DOI 10.1016/j.neubiorev.2006.10.004
   Koelstra S, 2013, IMAGE VISION COMPUT, V31, P164, DOI 10.1016/j.imavis.2012.10.002
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Laufs H, 2003, P NATL ACAD SCI USA, V100, P11053, DOI 10.1073/pnas.1831638100
   LEDOUX JE, 1995, ANNU REV PSYCHOL, V46, P209, DOI 10.1146/annurev.ps.46.020195.001233
   Luo Q, 2007, NEUROIMAGE, V34, P839, DOI 10.1016/j.neuroimage.2006.09.023
   Martínez HP, 2013, IEEE COMPUT INTELL M, V8, P20, DOI 10.1109/MCI.2013.2247823
   Nakasone Arturo., 2005, Proc. of the 5th International Workshop on Biosignal Interpretation, P219
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   RAY WJ, 1985, SCIENCE, V228, P750, DOI 10.1126/science.3992243
   Rozgic V, 2013, INT CONF ACOUST SPEE, P1286, DOI 10.1109/ICASSP.2013.6637858
   RUSSELL JA, 1991, PSYCHOL BULL, V110, P426, DOI 10.1037/0033-2909.110.3.426
   RUSSELL JA, 1983, J PERS SOC PSYCHOL, V45, P1281, DOI 10.1037/0022-3514.45.6.1281
   Sauseng P, 2005, EUR J NEUROSCI, V22, P2917, DOI 10.1111/j.1460-9568.2005.04482.x
   Shu YY, 2017, INT CONF ACOUST SPEE, P2871, DOI 10.1109/ICASSP.2017.7952681
   Soleymani M., 2017, Frontiers in ICT, V4, DOI DOI 10.3389/FICT.2017.00001
   Soleymani M, 2016, IEEE T AFFECT COMPUT, V7, P17, DOI 10.1109/TAFFC.2015.2436926
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P211, DOI 10.1109/T-AFFC.2011.37
   Subramanian R, 2018, IEEE T AFFECT COMPUT, V9, P147, DOI 10.1109/TAFFC.2016.2625250
   Tripathi S, 2017, AAAI CONF ARTIF INTE, P4746
   Zhang ZZ, 2018, IEEE T IMAGE PROCESS, V27, P5957, DOI 10.1109/TIP.2018.2862625
   Zhao SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1319, DOI 10.1145/3240508.3240591
   Zhao SC, 2016, AAAI CONF ARTIF INTE, P4284
   Zhao SC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3233184
   Zhao Sicheng, 2019, P C ASS ADV ART INT
   Zhou D, 2007, 2007 IEEE NORTH-EAST WORKSHOP ON CIRCUITS AND SYSTEMS, P167, DOI 10.1109/CADCG.2007.4407875
NR 45
TC 7
Z9 8
U1 6
U2 38
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2019
VL 15
IS 3
SU S
SI SI
AR 95
DI 10.1145/3332374
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA LR5EU
UT WOS:000535718800011
DA 2024-07-18
ER

PT J
AU Zhang, JF
   Hu, HF
AF Zhang, Junfeng
   Hu, Haifeng
TI Joint Head Attribute Classifier and Domain-Specific Refinement Networks
   for Face Alignment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Two-stage estimation; domain-specific; head attribute classifier; face
   alignment
AB In this article, a two-stage refinement network is proposed for facial landmarks detection on unconstrained conditions. Our model can be divided into two modules, namely the Head Attribude Classifier (HAC) module and the Domain-Specific Refinement (DSR) module. Given an input facial image, HAC adopts multi-task learning mechanism to detect the head pose and obtain an initial shape. Based on the obtained head pose, DSR designs three different CNN-based refinement networks trained by specific domain, respectively, and automatically selects the most approximate network for the landmarks refinement. Different from existing two-stage models, HAC combines head pose prediction with facial landmarks estimation to improve the accuracy of head pose prediction, as well as obtaining a robust initial shape. Moreover, an adaptive sub-network training strategy applied in the DSR module can effectively solve the issue of traditional multi-view methods that an improperly selected sub-network may result in alignment failure. The extensive experimental results on two public datasets, AFLW and 300W, confirm the validity of our model.
C1 [Zhang, Junfeng; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM zhangf26@mail2.sysmedu.cn; huhaif@mail.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; Natural Science Foundation of Guangdong Province
   [2017A030311029, 2016B010109002]; Science and Technology Program of
   Guangzhou, China [201704020180]; Fundamental Research Funds for the
   Central Universities of China
FX This work was supported in part by the National Natural Science
   Foundation of China, under Grants 61673402, 61273270, and 60802069; the
   Natural Science Foundation of Guangdong Province (2017A030311029 and
   2016B010109002); the Science and Technology Program of Guangzhou, China,
   under Grant 201704020180; and the Fundamental Research Funds for the
   Central Universities of China.
CR [Anonymous], 2015, ARXIV150703409
   [Anonymous], 1995, COMPUTER VISION IMAG
   Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Cootes T. F., 2002, P IEEE INT C AUT FAC, P227
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cristinacce D., 2006, BRIT MACH VIS C, V1, P3
   Deng J., 2017, IEEE T ACTIONS IMAGE
   Dong XW, 2018, IEEE CONF COMPUT
   Dou P, 2017, PROC CVPR IEEE, P1503, DOI 10.1109/CVPR.2017.164
   Feng ZH, 2017, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2017.392
   Hara K, 2014, LECT NOTES COMPUT SC, V8690, P552, DOI 10.1007/978-3-319-10605-2_36
   Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454
   Jourabloo A, 2015, IEEE I CONF COMP VIS, P3694, DOI 10.1109/ICCV.2015.421
   KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI [DOI 10.1109/CVPR.2014.241, 10.1109/CVPR.2014.241]
   Kittler Josef, 2016, 3D MORPHABLE FACE MO
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krogh A., 1991, Advances in Neural Information Processing Systems
   Kstinger M., 2012, IEEE INT C COMP VIS, P2144
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Liu YJ, 2017, IEEE INT CONF COMP V, P1619, DOI 10.1109/ICCVW.2017.190
   Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393
   Ren SQ, 2016, IEEE T IMAGE PROCESS, V25, P1233, DOI 10.1109/TIP.2016.2518867
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tzimiropoulos G, 2015, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2015.7298989
   Walecki R, 2016, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2016.530
   Weng RL, 2016, IEEE T IMAGE PROCESS, V25, P1163, DOI 10.1109/TIP.2016.2515987
   Wu Y, 2016, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2016.370
   Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4
   Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Xu X, 2017, IEEE INT CONF AUTOMA, P642, DOI 10.1109/FG.2017.81
   Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253
   YANG JL, 2017, PROC CVPR IEEE, P5216, DOI DOI 10.1109/CVPR.2017.554
   Yin X, 2018, IEEE T IMAGE PROCESS, V27, P964, DOI 10.1109/TIP.2017.2765830
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P52, DOI 10.1007/978-3-319-46454-1_4
   Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1
   Zhang JF, 2018, COMPUT VIS IMAGE UND, V171, P95, DOI 10.1016/j.cviu.2018.05.002
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 50
TC 0
Z9 0
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2018
VL 14
IS 4
AR 79
DI 10.1145/3241059
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JB
UT WOS:000457139200001
DA 2024-07-18
ER

PT J
AU Wang, C
   Yang, HJ
   Meinel, C
AF Wang, Cheng
   Yang, Haojin
   Meinel, Christoph
TI Image Captioning with Deep Bidirectional LSTMs and Multi-Task Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; LSTM; multimodal representations; image captioning;
   mutli-task learning
AB Generating a novel and descriptive caption of an image is drawing increasing interests in computer vision, natural language processing, and multimedia communities. In this work, we propose an end-to-end trainable deep bidirectional LSTM (Bi-LSTM (Long Short-Term Memory)) model to address the problem. By combining a deep convolutional neural network (CNN) and two separate LSTM networks, our model is capable of learning long-term visual-language interactions by making use of history and future context information at high-level semantic space. We also explore deep multimodal bidirectional models, in which we increase the depth of nonlinearity transition in different ways to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale, and vertical mirror are proposed to prevent over-fitting in training deep models. To understand how our models "translate" image to sentence, we visualize and qualitatively analyze the evolution of Bi-LSTM internal states over time. The effectiveness and generality of proposed models are evaluated on four benchmark datasets: Flickr8K, Flickr30K, MSCOCO, and Pascal1K datasets. We demonstrate that Bi-LSTM models achieve highly competitive performance on both caption generation and image-sentence retrieval even without integrating an additional mechanism (e.g., object detection, attention model). Our experiments also prove that multi-task learning is beneficial to increase model generality and gain performance. We also demonstrate the performance of transfer learning of the Bi-LSTM model significantly outperforms previous methods on the Pascal1K dataset.
C1 [Wang, Cheng; Yang, Haojin; Meinel, Christoph] Univ Potsdam, Hasso Plattner Inst, Prof Dr Helmert Str 2-3, D-14482 Potsdam, Germany.
C3 University of Potsdam
RP Wang, C (corresponding author), Univ Potsdam, Hasso Plattner Inst, Prof Dr Helmert Str 2-3, D-14482 Potsdam, Germany.
EM Cheng.Wang@hpi.de; Haojin.Yang@hpi.de; Christoph.Meinel@hpi.de
RI ARSLAN, Okan/AAA-3232-2020
CR [Anonymous], 2014, Transactions of the Association for Computational Linguistics
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2015, B ENTOMOL RES
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2014, P 28 INT C NEUR INF
   [Anonymous], 2016, ARXIV160400790
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2011, P ICML
   [Anonymous], 2015, ICML
   [Anonymous], 2014, Learning to execute
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2015, P 3 INT C LEARN REPR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2014, P IEEE C COMP VIS PA
   [Anonymous], 2015, ICLR 2015
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], EMNLP
   Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5
   Chen Wang, 2017, Multimedia Tools and Applications, V76, P6263, DOI 10.1007/s11042-015-3199-8
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Choi Y, 2011, P 15 C COMPUTATIONAL, P220
   Chung JY, 2015, PR MACH LEARN RES, V37, P2067
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kulkarni G, 2011, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2011.5995466
   Kuznetsova Polina, 2012, Association for Computational Linguistics
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Mikolov T, 2011, INT CONF ACOUST SPEE, P5528
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pascanu Razvan, 2013, COMPUT SCI
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Simonyan K., 2014, Very Deep Convolutional Networks for Large-Scale Image Recognition
   Socher R., 2011, PROC INT C MACH LEAR, P129
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang W., 2016, Asian Conference on Computer Vision, P104
   Wang W, 2016, PROC CVPR IEEE, P2378, DOI 10.1109/CVPR.2016.261
   Wang ZY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P451, DOI 10.1145/2733373.2806219
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wu ZZ, 2016, INT CONF ACOUST SPEE, P5140, DOI 10.1109/ICASSP.2016.7472657
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zitnick C.L., 2014, ARXIV14115654
NR 64
TC 71
Z9 80
U1 1
U2 53
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 40
DI 10.1145/3115432
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TD
UT WOS:000434634700013
DA 2024-07-18
ER

PT J
AU Azzakhnini, S
   Ballihi, L
   Aboutajdine, D
AF Azzakhnini, Safaa
   Ballihi, Lahoucine
   Aboutajdine, Driss
TI Combining Facial Parts For Learning Gender, Ethnicity, and Emotional
   State Based on RGB-D Information
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face recognition; facial parts; RGB-D data; kinect
ID 3D FACE RECOGNITION; DIFFERENCE; KINECT
AB With the success of emerging RGB-D cameras such as the Kinect sensor, combining the shape (depth) and texture information to improve the quality of recognition became a trend among computer vision researchers. In this work, we address the problem of face classification in the context of RGB images and depth data. Inspired by the psychological results for human face perception, this article focuses on (i) finding out which facial parts are most effective at making the difference for some social aspects of face perception (gender, ethnicity, and emotional state), (ii) determining the optimal decision by combining the decision rendered by the individual parts, and (iii) extracting the promising features from RGB-D faces to exploit all the potential that this data provide. Experimental results on EurecomKinect Face and CurtinFaces databases show that the proposed approach improves the recognition quality in many use cases.
C1 [Azzakhnini, Safaa; Ballihi, Lahoucine; Aboutajdine, Driss] Mohammed V Univ Rabat, LRIT, Associated Unit, CNRST,URAC 29, BP 1014, Rabat, Morocco.
C3 Mohammed V University in Rabat; Centre National de la Recherche
   Scientifique & Technologique (CNRST)
RP Azzakhnini, S (corresponding author), Mohammed V Univ Rabat, LRIT, Associated Unit, CNRST,URAC 29, BP 1014, Rabat, Morocco.
EM safae.azzakhnini@gmail.com; Ballihi@fsr.ac.ma; aboutaj@fsr.ac.ma
RI aboutajdine, driss/AAP-9051-2020
OI lahoucine, Ballihi/0000-0002-4307-7468
CR Abate AF, 2007, PATTERN RECOGN LETT, V28, P1885, DOI 10.1016/j.patrec.2006.12.018
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Ajmera R., 2014, Proceedings of the 2014 Indian Conference on Computer Vision Graphics and Image Processing, P76
   [Anonymous], 2004, ACCV
   [Anonymous], 2014, VISAPP
   [Anonymous], 2011, CS294A LECT NOTES
   [Anonymous], 2006, BMVC
   [Anonymous], 1987, THESIS
   Ballihi L, 2015, LECT NOTES COMPUT SC, V8912, P109, DOI 10.1007/978-3-319-13737-7_10
   Ballihi L, 2012, IEEE T INF FOREN SEC, V7, P1766, DOI 10.1109/TIFS.2012.2209876
   Berretti S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168759
   Birattari M., 2010, EXPT METHODS ANAL OP, P311, DOI DOI 10.1007/978-3-642
   Boutellaa Elhocine, 2015, PATTERN RECOGN LETT
   Bowyer KW, 2006, COMPUT VIS IMAGE UND, V101, P1, DOI 10.1016/j.cviu.2005.05.005
   BROWN E, 1993, PERCEPTION, V22, P829, DOI 10.1068/p220829
   BRUCE V, 1993, PERCEPTION, V22, P131, DOI 10.1068/p220131
   BURTON AM, 1993, PERCEPTION, V22, P153, DOI 10.1068/p220153
   Neto JBC, 2015, 30TH ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING, VOLS I AND II, P66, DOI 10.1145/2695664.2695807
   Cellerino A, 2004, BRAIN RES BULL, V63, P443, DOI 10.1016/j.brainresbull.2004.03.010
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dai X, 2015, IEEE WINT CONF APPL, P657, DOI 10.1109/WACV.2015.93
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Gallinari P., 1987, P COGNITIVA 87
   Galoogahi H. K., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P224, DOI 10.1109/ICME.2012.128
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goswami G., 2013, 2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems BTAS, P1
   Hayat M, 2016, NEUROCOMPUTING, V171, P889, DOI 10.1016/j.neucom.2015.07.027
   Hg RI, 2012, 8TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS 2012), P42, DOI 10.1109/SITIS.2012.17
   Hsu GS, 2014, IEEE T INF FOREN SEC, V9, P2110, DOI 10.1109/TIFS.2014.2361028
   Huynh Tri, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P133, DOI 10.1007/978-3-642-37410-4_12
   Krishnan P, 2015, PROCEDIA COMPUT SCI, V46, P1653, DOI 10.1016/j.procs.2015.02.102
   Li BYL, 2013, IEEE WORK APP COMP, P186, DOI 10.1109/WACV.2013.6475017
   López-Ibáñez M, 2016, OPER RES PERSPECT, V3, P43, DOI 10.1016/j.orp.2016.09.002
   Lumini A, 2017, INFORM FUSION, V33, P71, DOI 10.1016/j.inffus.2016.05.003
   Lumini Alessandra, 2016, APPL COMPUT INFO
   Milborrow S., 2013, STASM 4 USER MANUAL
   Min R, 2014, IEEE T SYST MAN CY-S, V44, P1534, DOI 10.1109/TSMC.2014.2331215
   Min R, 2012, INT C PATT RECOG, P1739
   Mracek Stepan, 2014, P BIOSIG, P195
   Qiu Jin, 2012, 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery, P1792, DOI 10.1109/FSKD.2012.6234127
   Llonch RS, 2010, PATTERN RECOGN, V43, P824, DOI 10.1016/j.patcog.2009.07.005
   Segundo MP, 2013, IEEE COMPUT SOC CONF, P64, DOI 10.1109/CVPRW.2013.17
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
NR 44
TC 4
Z9 4
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2018
VL 14
IS 1
SU S
AR 19
DI 10.1145/3152125
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GH5ZK
UT WOS:000433517100005
DA 2024-07-18
ER

PT J
AU Akhtar, S
   Beck, A
   Rimac, I
AF Akhtar, Shahid
   Beck, Andre
   Rimac, Ivica
TI Caching Online Video: Analysis and Proposed Algorithm
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Hierarchical cache; online video; hit-rate; LRU; LFU; GDSF
AB Online video presents new challenges to traditional caching with over a thousand-fold increase in number of assets, rapidly changing popularity of assets and much higher throughput requirements.
   We propose a new hierarchical filtering algorithm for caching online video-HiFi. Our algorithm is designed to optimize hit rate, replacement rate and cache throughput. It has an associated implementation complexity comparable to that of LRU.
   Our results show that, under typical operator conditions, HiFi can increase edge cache byte hit rate by 5%-24% over an LRU policy, but more importantly can increase the RAM or memory byte hit rate by 80% to 200% and reduce the replacement rate by more than 100 times! These two factors combined can dramatically increase throughput for most caches. If SSDs are used for storage, the much lower replacement rate may also allow substitution of lower-cost MLC-based SSDs instead of SLC-based SSDs.
   We extend previous multi-tier analytical models for LRU caches to caches with filtering. We analytically show how HiFi can approach the performance of an optimal caching policy and how to tune HiFi to reach as close to optimal performance as the traffic conditions allow. We develop a realistic simulation environment for online video using statistics from operator traces. We show that HiFi performs within a few percentage points from the optimal solution which was simulated by Belady's MIN algorithm under typical operator conditions
C1 [Akhtar, Shahid] Nokia, 601 Data Dr, Plano, TX 75075 USA.
   [Beck, Andre] Nokia, 2000 Lucent Ave, Naperville, IL USA.
   [Rimac, Ivica] Nokia, Lorenzstr 10, D-70435 Stuttgart, Germany.
C3 Nokia Corporation; Nokia Bell Labs; Nokia Corporation; Nokia Bell Labs;
   Nokia Corporation
RP Akhtar, S (corresponding author), Nokia, 601 Data Dr, Plano, TX 75075 USA.
EM shahid.akhtar@nokia.com; andre.beck@nokia-bell-labs.com;
   ivica.rimac@nokia-bell-labs.com
FU Nokia
FX This work was supported by Nokia.
CR Adhikari Vijay Kumar, 2011, P 2012 IEEE INFOCOM
   Andres Ferragut, 2016, P 2016 ACM SIGMETRIC
   [Anonymous], 1997, IFIP NETW C IFIP NET
   [Anonymous], 2013, SANDVINE REPORT 2013
   Balachandran Athula, 2013, P 2013 C INT MEAS C
   BELADY LA, 1966, IBM SYST J, V5, P78, DOI 10.1147/sj.52.0078
   Berger Daniel S., 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P57
   Berger DS, 2014, PERFORM EVALUATION, V79, P2, DOI 10.1016/j.peva.2014.07.001
   Che H, 2002, IEEE J SEL AREA COMM, V20, P1305, DOI 10.1109/JSAC.2002.801752
   Dave Simpson, 2010, MLC VS SLC FLASH ENT
   Fielding R., 1999, Hypertext transfer protocol-http/1.1. Tech. rep
   Fofack N. Choungmo, 2012, P 2012 6 INT C PERF
   Fofack NC, 2014, COMPUT NETW, V65, P212, DOI 10.1016/j.comnet.2014.03.006
   George Karakostas, 2002, P 7 INT S COMP COMM
   Gerry Cole, 2000, 338 TP
   Gil Einziger, 2014, P 2014 22 EUR INT C
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Keqiu Li, 2005, ACM Transactions on Internet Technology, V5, P480, DOI 10.1145/1084772.1084774
   Konstantinos Poularakis, 2012, P 2012 46 ANN C INF
   Konstantinos Poularakis, 2013, P 2013 IEEE INT C CO
   Laoutaris N, 2006, PERFORM EVALUATION, V63, P609, DOI 10.1016/j.peva.2005.05.003
   Llorca Jaime, 2013, P INFOCOM 2013
   Ludmila Cherkasova, 1998, IMPROVING WWW PROXIE
   Neglia Giovanni, 2016, P 2016 28 INT TEL C, V1
   Nikolaos Laoutaris, 2004, P 2004 IEEE INT C PE
   Nimrod Megiddo, 2003, ARC SELF TUNING LOW, V3
   Podlipnig S, 2003, ACM COMPUT SURV, V35, P374, DOI 10.1145/954339.954341
   Rodriguez P, 2001, IEEE ACM T NETWORK, V9, P404, DOI 10.1109/90.944339
   Rosensweig Elisha J., 2009, P INFOCOM 2009 IEEE
   Sem Borst, 2010, 2010 P IEEE IEEE
   Shah K., 2010, CITESEERX, V1, P1
   Shahab Bakhtiyari, 2012, THESIS
   Shahid Akhtar, 2015, P 23 ACM INT C MULT
   Tang W, 2007, COMPUT NETW, V51, P336, DOI 10.1016/j.comnet.2006.05.003
   Theodore Johnson, 1994, P 20 VLDB C
   Tsutsui Tatsuhiro, 2012, P 2012 IEEE INT NAT
   Yuncheng Zhu, 2010, P IEEE C COMP COMM W
   Zhe Li, 2011, P 2011 IEEE INT C CO
NR 38
TC 3
Z9 3
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 48
DI 10.1145/3106157
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300002
DA 2024-07-18
ER

PT J
AU Dang-Nguyen, DT
   Piras, L
   Giacinto, G
   Boato, G
   De Natale, FGB
AF Duc-Tien Dang-Nguyen
   Piras, Luca
   Giacinto, Giorgio
   Boato, Giulia
   De Natale, Francesco G. B.
TI Multimodal Retrieval with Diversification and Relevance Feedback for
   Tourist Attraction Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Diversification; tourist attraction images retrieval
ID SEARCH; SVM
AB In this article, we present a novel framework that can produce a visual description of a tourist attraction by choosing the most diverse pictures from community-contributed datasets, which describe different details of the queried location. The main strength of the proposed approach is its flexibility that permits us to filter out non-relevant images and to obtain a reliable set of diverse and relevant images by first clustering similar images according to their textual descriptions and their visual content and then extracting images from different clusters according to a measure of the user's credibility. Clustering is based on a two-step process, where textual descriptions are used first and the clusters are then refined according to the visual features. The degree of diversification can be further increased by exploiting users' judgments on the results produced by the proposed algorithm through a novel approach, where users not only provide a relevance feedback but also a diversity feedback. Experimental results performed on the MediaEval 2015 "Retrieving Diverse Social Images" dataset show that the proposed framework can achieve very good performance both in the case of automatic retrieval of diverse images and in the case of the exploitation of the users' feedback. The effectiveness of the proposed approach has been also confirmed by a small case study involving a number of real users.
C1 [Duc-Tien Dang-Nguyen; Boato, Giulia; De Natale, Francesco G. B.] Univ Trento, Trento, Italy.
   [Duc-Tien Dang-Nguyen] Dublin City Univ, Dublin, Ireland.
   [Piras, Luca; Giacinto, Giorgio] Univ Cagliari, Dept Elect & Elect Engn, Cagliari, Italy.
   [Duc-Tien Dang-Nguyen] Dublin City Univ, Insight Ctr Data Analyt, Dublin 9, Ireland.
   [Boato, Giulia; De Natale, Francesco G. B.] Univ Trento, Dept Informat Engn & Comp Sci, Trento, Italy.
C3 University of Trento; Dublin City University; University of Cagliari;
   Dublin City University; University of Trento
RP Dang-Nguyen, DT (corresponding author), Univ Trento, Trento, Italy.; Dang-Nguyen, DT (corresponding author), Dublin City Univ, Dublin, Ireland.; Dang-Nguyen, DT (corresponding author), Dublin City Univ, Insight Ctr Data Analyt, Dublin 9, Ireland.
EM duc-tien.dang-nguyen@dcu.ie; luca.piras@diee.unica.it;
   giacinto@diee.unica.it; boato@disi.unitn.it; denatale@ing.unitn.it
RI Piras, Luca/L-9264-2015
OI GIACINTO, GIORGIO/0000-0002-5759-3017
FU Regional Administration of Sardinia, Italy [CUP F71J11000690002]
FX This work has been partially supported by the Regional Administration of
   Sardinia, Italy, within the project "Advanced and secure sharing of
   multimedia data over social networks in the future Internet" (CUP
   F71J11000690002).
CR Anderberg M.R., 1973, Probability and Mathematical Statistics
   [Anonymous], MEDIAEVAL
   [Anonymous], P IEEE INT WORKSH CO
   [Anonymous], MEDIAEVAL
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], MEDIAEVAL
   [Anonymous], 2015, P 13 INT WORKSHOP CO
   [Anonymous], 2013, P WORKSH NEW CHALL D
   [Anonymous], MEDIAEVAL
   [Anonymous], MEDIAEVAL
   [Anonymous], P INT C CROSS LANG E
   [Anonymous], MEDIAEVAL
   [Anonymous], MEDIAEVAL
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Boato G., 2015, MULTIMED TOOLS APPL, V2015, P1
   Boteanu B, 2014, INT C INTELL COMP CO, P47, DOI 10.1109/ICCP.2014.6936979
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Chen T, 2014, IEEE T MULTIMEDIA, V16, P612, DOI 10.1109/TMM.2014.2301978
   Chen YQ, 2001, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2001.958946
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dang- Nguyen D.-T., P IEEE INT C MULTIME
   Ginsca AL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1021, DOI 10.1145/2647868.2655033
   Giordano D, 2016, MULTIMEDIA SYST, V22, P725, DOI 10.1007/s00530-015-0491-4
   Huang JT, 2005, I S INTELL SIG PROC, P157
   Huang Z., 2010, ADV MECH ENG, V2010, P1
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Kim DH, 2005, J SYST SOFTWARE, V78, P9, DOI 10.1016/j.jss.2005.02.005
   Laaksonen J, 2002, IEEE T NEURAL NETWOR, V13, P841, DOI 10.1109/TNN.2002.1021885
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lei Z, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P721, DOI 10.1109/ICIP.2001.958595
   Liang S, 2008, PATTERN RECOGN LETT, V29, P1733, DOI 10.1016/j.patrec.2008.05.004
   Lu D, 2016, IEEE T MULTIMEDIA, V18, P1628, DOI 10.1109/TMM.2016.2568099
   Lu ZW, 2010, IEEE T MULTIMEDIA, V12, P194, DOI 10.1109/TMM.2010.2041100
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Piras L, 2017, INFORM FUSION, V37, P50, DOI 10.1016/j.inffus.2017.01.003
   Piras L, 2009, 2009 10TH INTERNATIONAL WORKSHOP ON IMAGE ANALYSIS FOR MULTIMEDIA INTERACTIVE SERVICES, P238, DOI 10.1109/WIAMIS.2009.5031477
   Qian XM, 2016, IEEE T IMAGE PROCESS, V25, P195, DOI 10.1109/TIP.2015.2497145
   Qian XM, 2015, IEEE T CIRC SYST VID, V25, P1857, DOI 10.1109/TCSVT.2014.2369731
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P921, DOI 10.1109/TMM.2013.2237896
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Rui Y, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P815, DOI 10.1109/ICIP.1997.638621
   Simon I., 2007, P IEEE INT C COMPUTE, P1
   Thomee B, 2012, INT J MULTIMED INF R, V1, P71, DOI 10.1007/s13735-012-0014-4
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   Wang T, 2003, MULTIMEDIA SYST, V9, P131, DOI 10.1007/s00530-003-0084-5
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Zhang RF, 2005, MULTIMEDIA SYST, V10, P529, DOI 10.1007/s00530-005-0180-9
   Zhang T., 1996, BIRCH EFFICIENT DATA, V25, P103, DOI [DOI 10.1145/233269.233324, 10.1145/235968.233324]
   Zhu L, 2015, IEEE T MULTIMEDIA, V17, P981, DOI 10.1109/TMM.2015.2431496
NR 52
TC 14
Z9 14
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 49
DI 10.1145/3103613
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300003
OA Green Published
DA 2024-07-18
ER

PT J
AU Su, Z
   Zeng, K
   Li, HH
   Luo, XN
AF Su, Zhuo
   Zeng, Kun
   Li, Hanhui
   Luo, Xiaonan
TI A Dual-Domain Perceptual Framework for Generating Visual Inconspicuous
   Counterparts
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Object manipulation; image editing; visual perception; bidirectional
   similarity; image quality assessment
ID IMAGE; PATCHMATCH; ALGORITHM; SALIENCY; COLOR
AB For a given image, it is a challenging task to generate its corresponding counterpart with visual inconspicuous modification. The complexity of this problem reasons from the high correlativity between the editing operations and vision perception. Essentially, a significant requirement that should be emphasized is how to make the object modifications hard to be found visually in the generative counterparts. In this article, we propose a novel dual-domain perceptual framework to generate visual inconspicuous counterparts, which applies the perceptual bidirectional similarity metric (PBSM) and appearance similarity metric (ASM) to create the dual-domain perception error minimization model. The candidate targets are yielded by the well-known PatchMatch model with the strokes-based interactions and selective object library. By the dual-perceptual evaluation index, all candidate targets are sorted to select out the best result. For demonstration, a series of objective and subjective measurements are used to evaluate the performance of our framework.
C1 [Su, Zhuo; Zeng, Kun; Li, Hanhui] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Luo, Xiaonan] Sun Yat Sen Univ, Natl Engn Res Ctr Digital Life, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University
RP Zeng, K (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
EM suzhuo3@mail.sysu.edu.cn; zengkun2@mail.sysu.edu.cn;
   lihanhui@mail2.sysu.edu.cn; lnslxn@mail.sysu.edu.cn
RI Li, Hanhui/AAJ-9546-2021; Su, Zhuo/AAO-4506-2020
OI Su, Zhuo/0000-0002-6090-0110
FU National Natural Science Foundation of China [61320106008, 61502541,
   61370186]; Natural Science Foundation of Guangdong Province
   [2016A030310202, 2015A030313129]; Science and Technology Planning
   Project of Guangdong Province [2015B010106005]; Fundamental Research
   Funds for the Central Universities (Sun Yat-sen University) [16lgpy39]
FX This work was supported by the National Natural Science Foundation of
   China (61320106008, 61502541, 61370186), the Natural Science Foundation
   of Guangdong Province (2016A030310202, 2015A030313129), the Science and
   Technology Planning Project of Guangdong Province (2015B010106005), and
   the Fundamental Research Funds for the Central Universities (Sun Yat-sen
   University, No. 16lgpy39).
CR [Anonymous], 2007, IEEE T PATTERN ANAL
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], P SIGGRAPH 2009 TALK
   [Anonymous], P 20 ACM INT C MULT
   [Anonymous], 2009, ACM T GRAPHIC, DOI DOI 10.1145/1618452.1618470
   [Anonymous], ACM T GRAPHICS
   Barnes C., 2010, LECT NOTES COMPUT SC, V6313, P29
   Barnes C, 2011, COMMUN ACM, V54, P103, DOI 10.1145/2018396.2018421
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Cho Taeg Sang, 2008, PROC IEEE C COMPUT V, P1
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597
   Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373
   Goldberg C, 2012, COMPUT GRAPH FORUM, V31, P265, DOI 10.1111/j.1467-8659.2012.03005.x
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508373
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Lu CW, 2014, INT J COMPUT VISION, V110, P222, DOI 10.1007/s11263-014-0732-6
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Shamir Ariel., 2009, ACM SIGGRAPH ASIA 20, P1, DOI DOI 10.1145/1665817.1665828
   Shen XY, 2015, IEEE I CONF COMP VIS, P3406, DOI 10.1109/ICCV.2015.389
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Song ML, 2010, IEEE T PATTERN ANAL, V32, P1537, DOI 10.1109/TPAMI.2009.74
   Su Z, 2014, IEEE T MULTIMEDIA, V16, P988, DOI 10.1109/TMM.2014.2305914
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Sun SY, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818708
   Tao MW, 2013, INT J COMPUT VISION, V103, P178, DOI 10.1007/s11263-012-0579-7
   Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu PH, 2013, IEEE T IMAGE PROCESS, V22, P3614, DOI 10.1109/TIP.2013.2266099
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Xu L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508404
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zhang FL, 2012, IEEE T VIS COMPUT GR, V18, P1849, DOI 10.1109/TVCG.2012.68
   Zhao MT, 2013, ACM T APPL PERCEPT, V10, DOI 10.1145/2422105.2422110
NR 48
TC 1
Z9 1
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2017
VL 13
IS 2
AR 22
DI 10.1145/3068427
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EV1VK
UT WOS:000401537300010
DA 2024-07-18
ER

PT J
AU Xu, JX
   Wah, BW
AF Xu, Jingxi
   Wah, Benjamin W.
TI Consistent Synchronization of Action Order with Least Noticeable Delays
   in Fast-Paced Multiplayer Online Games
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Internet; consistency; human factors; quality of experience
ID PERCEPTION
AB When running multiplayer online games on IP networks with losses and delays, the order of actions may be changed when compared to the order run on an ideal network with no delays and losses. To maintain a proper ordering of events, traditional approaches either use rollbacks to undo certain actions or local lags to introduce additional delays. Both may be perceived by players because their changes are beyond the just-noticeable-difference (JND) threshold. In this article, we propose a novel method for ensuring a strongly consistent completion order of actions, where strong consistency refers to the same completion order as well as the same interval between any completion time and the corresponding ideal reference completion time under no network delay We find that small adjustments within the JND on the duration of an action would not be perceivable, as long as the duration is comparable to the network round-trip time. We utilize this property to control the vector of durations of actions and formulate the search of the vector as a multidimensional optimization problem. By using the property that players are generally more sensitive to the most prominent delay effect (with the highest probability of notiembility P-notice or the probability of correctly noticing a change when compared to the reference), we prove that the optimal solution occurs when P-notice, of the individual adjustments are equal. As this search can be done efficiently in polynomial time (similar to-5ms) with a small amount of space (similar to 160KB), the search can be done at runtime to determine the optimal control. Last, we evaluate our approach on the popular open-source online shooting game BZFlag.
C1 [Xu, Jingxi] Chinese Univ Hong Kong, SHB 115, Sha Tin, Hong Kong, Peoples R China.
   [Wah, Benjamin W.] Chinese Univ Hong Kong, Room 202,Univ Adm Bldg, Sha Tin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong; Chinese University of Hong Kong
RP Xu, JX (corresponding author), Chinese Univ Hong Kong, SHB 115, Sha Tin, Hong Kong, Peoples R China.; Wah, BW (corresponding author), Chinese Univ Hong Kong, Room 202,Univ Adm Bldg, Sha Tin, Hong Kong, Peoples R China.
EM jxxu@cse.cuhk.edu.hk; bwah@cuhk.edu.hk
CR [Anonymous], 1891, Zeitschrift fur psychologie und Physiologie der sinnesorgane
   Beigbeder T., 2004, P ACM NETGAMES, P144
   Bharambe A., 2006, Proceedings of the 3rd conference on Networked Systems Design Implementation - Volume 3, NSDI'06, V3, P12
   Chan Luther., 2007, NETGAMES 07, P37, DOI [10.1145/1326257.1326264, DOI 10.1145/1326257.1326264]
   Chen P., 2011, P 10 ANN WORKSHOP NE, P2
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Dzhafarov EN, 1999, PSYCHON B REV, V6, P239, DOI 10.3758/BF03212329
   Gulliver SR, 2006, ACM T MULTIM COMPUT, V2, P241, DOI 10.1145/1201730.1201731
   Hariri N, 2011, IEEE T INSTRUM MEAS, V60, P1594, DOI 10.1109/TIM.2010.2092871
   Huang Z., 2013, ACM T MULTIM COMPUT, V9, P1, DOI DOI 10.1145/2490821
   Human Benchmark, 2016, REACT TIM TEST
   JAYANT N, 1992, IEEE J SEL AREA COMM, V10, P796, DOI 10.1109/49.138986
   Li FWB, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000493
   Lin YJ, 2002, 10TH IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P155, DOI 10.1109/ICNP.2002.1181396
   Ma L, 2011, SIGNAL PROCESS-IMAGE, V26, P162, DOI 10.1016/j.image.2011.02.002
   Marshall D, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1865106.1865114
   Mauve M, 2004, IEEE T MULTIMEDIA, V6, P47, DOI 10.1109/TMM.2003.819751
   Myers J., 2012, BZFlag 2.4.2
   Raaen K., 2014, 2014 13th Annual Workshop on Network and Systems Support for Games, P1
   Savery C., 2014, P 1 ACM SIGCHI ANN S, P237
   Savery C., 2014, THESIS
   Shi W, 2014, INT J COMPUT GAMES T, V2014, DOI 10.1155/2014/138596
   Smed J, 2005, COMPUT NETW, V49, P27, DOI 10.1016/j.comnet.2005.04.007
   Stuckel D, 2008, CSCW: 2008 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, CONFERENCE PROCEEDINGS, P447
   Teal S. L., 1992, CHI '92 Conference Proceedings. ACM Conference on Human Factors in Computing Systems. Striking a Balance, P295, DOI 10.1145/142750.142818
   Xu J., 2013, PROC 4 ACM MULTIMEDI, P238, DOI 10.1145/2483977.2484006
   Xu JH, 2013, ADV METEOROL, V2013, DOI 10.1155/2013/272715
   Xu JX, 2016, IEEE T MULTIMEDIA, V18, P1330, DOI 10.1109/TMM.2016.2557728
   Xu JX, 2015, IEEE MULTIMEDIA, V22, P14, DOI 10.1109/MMUL.2015.70
   Yahyavi A, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2522968.2522977
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Zhang Kaiwen., 2008, NETGAMES 08, P53
   Zhou SP, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1236471.1236474
NR 33
TC 3
Z9 3
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2017
VL 13
IS 1
AR 8
DI 10.1145/3003727
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM7WJ
UT WOS:000395522700008
DA 2024-07-18
ER

PT J
AU Chen, W
   Ma, LP
   Shen, CC
AF Chen, Wei
   Ma, Liangping
   Shen, Chien-Chung
TI Congestion-Aware MAC Layer Adaptation to Improve Video Telephony over
   Wi-Fi
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 6th ACM International Conference on Multimedia Systems (MMSys)
   Co-Located with 25th ACM Workshop on Network and Operating Systems
   Support for Digital Audio and Video (NOSSDAV)
CY MAR 18-20, 2015
CL Portland, OR
SP ACM
DE Wi-Fi; 802.11 MAC; retry limit; WebRTC; real-time video system; video
   telephony; congestion detection; Google congestion control (GCC)
AB In wireless networks such as those based on IEEE 802.11, packet losses due to fading and interference are often misinterpreted as indications of congestion by the congestion control protocol at higher layers, causing an unnecessary decrease in the data sending rate. For delay-constrained applications such as video telephony, packet losses may result in excessive artifacts or freeze in the decoded video. We propose a simple and yet effective mechanism to detect and reduce channel-caused packet losses by adjusting the retry limit parameter of the IEEE 802.11 protocol while taking into account the delay requirement of the traffic. Since the retry limit is left configurable in the IEEE 802.11 standard and does not require cross-layer coordination, our scheme can be easily implemented and incrementally deployed. Experimental results of applying the proposed scheme to a WebRTC-based real-time video communication prototype show significant performance gain compared to the case where the retry limit is configured statically.
C1 [Chen, Wei; Ma, Liangping] Inter Digital, Wilmington, DE 19809 USA.
   [Shen, Chien-Chung] Univ Delaware, Newark, DE 19716 USA.
   [Chen, Wei; Ma, Liangping] 9710 Scranton Rd,Suite 250, San Diego, CA 92121 USA.
   [Shen, Chien-Chung] 450 Smith Hall, Newark, DE 19716 USA.
C3 InterDigital; University of Delaware
RP Chen, W (corresponding author), Inter Digital, Wilmington, DE 19809 USA.; Chen, W (corresponding author), 9710 Scranton Rd,Suite 250, San Diego, CA 92121 USA.
EM cwei@udel.edu; liangping.ma@interdigital.com; cshen@udel.edu
OI Ma, Liangping/0000-0002-5510-9332
CR Acharya Prashanth Aravinda Kumar, 2008, 2008 5th Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks, P1, DOI 10.1109/SAHCN.2008.11
   Al Islam A., 2011, IEEE WCNC
   Alimi R., 2014, 7285 RFC
   Andrews JG, 2014, IEEE J SEL AREA COMM, V32, P1065, DOI 10.1109/JSAC.2014.2328098
   [Anonymous], 2008, Random networks for communication
   [Anonymous], 1992, Data networks
   Bianchi G, 2000, IEEE J SEL AREA COMM, V18, P535, DOI 10.1109/49.840210
   Bobarshad H, 2012, IEEE T MULTIMEDIA, V14, P401, DOI 10.1109/TMM.2011.2173477
   Bononi L, 2004, IEEE T PARALL DISTR, V15, P66, DOI 10.1109/TPDS.2004.1264787
   Chen C. M., 2010, IEEE CSVT, V20, P11
   Chen Wei, 2015, MMSYS
   Chen Y., 2012, UMCS2012022
   De Cicco L., 2013, P 20 INT PACK VID WO, P12
   Ghadiyaram D., 2014, IEEE GLOBALSIP
   Handley M., 2008, INTERNET ENG TASK FO, V3448, P1
   Holmer S., 2013, IEEE ICIP
   Hu Y., 2004, P ICDCS
   Jardosh A., 2005, P IMC
   Kim H., 2006, P GLOB TEL C GLOBECO
   Lee K. Y., 2006, MASS
   Li An-Chih, 2009, SECON
   Lochert C, 2007, WIREL COMMUN MOB COM, V7, P655, DOI 10.1002/wcm.524
   Lohier S., 2006, P IEEE INT C WIR MOB
   LU M, 2007, WIRELESS COMMUNICATI
   Lundin H, 2014, GOOGLE CONGESTION CO
   Ma L., 2015, IEEE GLOBECOM
   OPNET, 2012, SITL
   Prasanthi S., 2010, ITNG
   Rangwala S., 2011, IEEE T NETWORKING, V19, P6
   Schulzrinne H., 2003, RTP TRANSPORT PROTOC
   Tan K, 2007, IEEE T VEH TECHNOL, V56, P863, DOI 10.1109/TVT.2007.891405
   Tas N. C., 2011, INFOCOM
   Xiao H., 2008, VIE 2008
   Xu T., 2016, ARXIV161100869CSMM
   Zhou H., 2004, P WIR TEL S
NR 35
TC 2
Z9 4
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2016
VL 12
IS 5
SU S
AR 83
DI 10.1145/2983634
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EJ0VT
UT WOS:000392929700014
DA 2024-07-18
ER

PT J
AU Qin, Z
   Yan, JB
   Ren, K
   Chen, CW
   Wang, C
AF Qin, Zhan
   Yan, Jingbo
   Ren, Kui
   Chen, Chang Wen
   Wang, Cong
TI SecSIFT: Secure Image SIFT Feature Extraction in Cloud Computing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image feature detection; SIFT; privacy-preserving; cloud computing
AB The image and multimedia data produced by individuals and enterprises is increasing every day. Motivated by the advances in cloud computing, there is a growing need to outsource such computational intensive image feature detection tasks to cloud for its economic computing resources and on-demand ubiquitous access. However, the concerns over the effective protection of private image and multimedia data when outsourcing it to cloud platform become the major barrier that impedes the further implementation of cloud computing techniques over massive amount of image and multimedia data. To address this fundamental challenge, we study the state-of-the-art image feature detection algorithms and focus on Scalar Invariant Feature Transform (SIFT), which is one of the most important local feature detection algorithms and has been broadly employed in different areas, including object recognition, image matching, robotic mapping, and so on. We analyze and model the privacy requirements in outsourcing SIFT computation and propose Secure Scalar Invariant Feature Transform (SecSIFT), a high-performance privacy-preserving SIFT feature detection system. In contrast to previous works, the proposed design is not restricted by the efficiency limitations of current homomorphic encryption scheme. In our design, we decompose and distribute the computation procedures of the original SIFT algorithm to a set of independent, co-operative cloud servers and keep the outsourced computation procedures as simple as possible to avoid utilizing a computationally expensive homomorphic encryption scheme. The proposed SecSIFT enables implementation with practical computation and communication complexity. Extensive experimental results demonstrate that SecSIFT performs comparably to original SIFT on image benchmarks while capable of preserving the privacy in an efficient way.
C1 [Qin, Zhan; Yan, Jingbo; Ren, Kui; Chen, Chang Wen] SUNY Buffalo, Dept Comp Sci & Engn, 338 Davis Hall, Buffalo, NY 14260 USA.
   [Wang, Cong] City Univ Hong Kong, Dept Comp Sci, 83 Tat Chee Ave, Kowloon, Hong Kong, Peoples R China.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo; City University of Hong Kong
RP Qin, Z (corresponding author), SUNY Buffalo, Dept Comp Sci & Engn, 338 Davis Hall, Buffalo, NY 14260 USA.
EM zhanqin@buffalo.edu; jingboya@buffalo.edu; kuiren@buffalo.edu;
   chencw@buffalo.edu; congwang@cityu.edu.hk
RI Ren, Kui/AGE-3662-2022
OI Ren, Kui/0000-0003-3441-6277; Chen, Chang Wen/0000-0002-6720-234X
FU U.S. National Science Foundation [CNS-1262277, CNS-1262275]; RGC of Hong
   Kong under ECS Grant [9041983]; Direct For Computer & Info Scie &
   Enginr; Division Of Computer and Network Systems [1262277] Funding
   Source: National Science Foundation
FX A preliminary version of this article was presented at the 22nd ACM
   International Conference on Multimedia (MM14), Orlando, USA, November
   3-7, 2014. This work was supported in part by the U.S. National Science
   Foundation under Grants No. CNS-1262277 and No. CNS-1262275 and by RGC
   of Hong Kong under ECS Grant No. 9041983.
CR Agrawal R., 2004, P SIGMOID 04, P563
   [Anonymous], 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587388
   [Anonymous], 2009, FULLY HOMOMORPHIC EN, DOI 10.1145/1536414.1536440
   [Anonymous], 2005, INT J INF SECUR, DOI DOI 10.1007/S10207-005-0070-3
   Atallah MJ, 2001, ADV COMPUT, V54, P215
   Barni M, 2011, IEEE T INF FOREN SEC, V6, P452, DOI 10.1109/TIFS.2011.2108650
   Barni M, 2010, MM&SEC 2010: 2010 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, PROCEEDINGS, P231
   Bianchi T, 2010, IEEE T INF FOREN SEC, V5, P180, DOI 10.1109/TIFS.2009.2036230
   Boldyreva A, 2011, LECT NOTES COMPUT SC, V6841, P578, DOI 10.1007/978-3-642-22792-9_33
   Boldyreva A, 2009, LECT NOTES COMPUT SC, V5479, P224, DOI 10.1007/978-3-642-01001-9_13
   Bringer J, 2013, IEEE SIGNAL PROC MAG, V30, P42, DOI 10.1109/MSP.2012.2230218
   Chen RC, 2013, ACM SIGCOMM COMP COM, V43, P315, DOI 10.1145/2534169.2486013
   Duncan GT, 2000, J AM STAT ASSOC, V95, P720, DOI 10.2307/2669452
   Erkin Z, 2009, LECT NOTES COMPUT SC, V5672, P235, DOI 10.1007/978-3-642-03168-7_14
   Goldwasser S, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P555
   Hsu CY, 2012, IEEE T IMAGE PROCESS, V21, P4593, DOI 10.1109/TIP.2012.2204272
   Hsu Chao-Yung, 2011, P SPIE 11
   Hsu Chao-Yung., 2009, ACM International Conference on Multimedia, P637
   Ke Y., 2004, P CVPR 04, V2, P66
   Leon P.G., 2012, P SIGCHI C HUMAN FAC, P589, DOI DOI 10.1145/2207676.2207759
   Li XH, 2008, IEEE T SIGNAL PROCES, V56, P675, DOI 10.1109/TSP.2007.907820
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu W., 2009, P INT C ULTR TEL ICU, V7254, P1, DOI DOI 10.1145/1533057.1533062
   Lu WJ, 2009, INT CONF ACOUST SPEE, P1533, DOI 10.1109/ICASSP.2009.4959888
   Malheiros M., 2012, the SIGCHI conference on human factors in computing systems, P579, DOI [10.1145/2207676.2207758, DOI 10.1145/2207676.2207758]
   Moskovich B., 2010, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P154
   Naini Farid M., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6250, DOI 10.1109/ICASSP.2014.6854806
   Osadchy M, 2010, P IEEE S SECUR PRIV, P239, DOI 10.1109/SP.2010.39
   Patel S.J., 2014, Journal of Information Security, V5, P12
   Prabhakaran MM, 2013, CRYPTOL INF SEC SER, V10, P1
   Qin Z, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P497, DOI 10.1145/2647868.2654941
   Qin Z, 2014, IEEE GLOB COMM CONF, P710, DOI 10.1109/GLOCOM.2014.7036891
   Troncoso-Pastoriza JR, 2007, MM&SEC'07: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2007, P109
   Sadeghi AR, 2010, LECT NOTES COMPUT SC, V5984, P229
   Shumiao Wang, 2013, Information and Communications Security. 15th International Conference, ICICS 2013. Proceedings: LNCS 8233, P90, DOI 10.1007/978-3-319-02726-5_7
   Subashini S, 2011, J NETW COMPUT APPL, V34, P1, DOI 10.1016/j.jnca.2010.07.006
   Thanh-Toan Do, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P52, DOI 10.1109/MMSP.2010.5661993
   Wang Y, 2011, APPL SOFT COMPUT, V11, P514, DOI 10.1016/j.asoc.2009.12.011
   Yao A. C., 1982, 23rd Annual Symposium on Foundations of Computer Science, P160, DOI 10.1109/SFCS.1982.38
   Yuan XL, 2014, INT CON DISTR COMP S, P198, DOI 10.1109/ICDCS.2014.28
NR 40
TC 9
Z9 10
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2016
VL 12
IS 4
SU S
AR 65
DI 10.1145/2978574
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8DS
UT WOS:000392736100009
DA 2024-07-18
ER

PT J
AU Wilk, S
   Kopf, S
   Effelsberg, W
AF Wilk, Stefan
   Kopf, Stephan
   Effelsberg, Wolfgang
TI Collaborative Annotation of Videos Relying on Weak Consistency
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Hypervideo; weak consistency; eventual consistency; interactive system
AB This work discusses a distributed interactive video system that supports video annotation using simultaneous hyperlinking by multiple users. The users mark and annotate objects within the video with links to other media such as text, images, websites, or other videos. Annotations are visualized on the client user interface as an overlay close to the objects. Our system is intuitive to use; for example, it contains automatic object-tracking functionality that correctly positions the annotations, even when the form or location of an object changes. Thus, our first contribution discusses the adaptive object-tracking algorithm used for this repositioning. It shows improved precision and reliability in comparison to nonadaptive algorithms. A second key issue is to keep the system responsive when the number of concurrent annotators increases. Thus, we rely on the concept of eventual consistency between different network entities. While this weak form of consistency allows temporary inconsistencies, it ensures that a consistent state can be reached. Thus, the second contribution is the design and evaluation of our distributed interactive video system, which relies on the weak consistency paradigm.
C1 [Wilk, Stefan] Tech Univ Darmstadt, Distributed Multimedia Syst, Rundeturmstr 10, D-64283 Darmstadt, Germany.
   [Kopf, Stephan; Effelsberg, Wolfgang] Univ Mannheim, Dept Comp Sci 4, A5 6, D-68159 Mannheim, Germany.
C3 Technical University of Darmstadt; University of Mannheim
RP Wilk, S (corresponding author), Tech Univ Darmstadt, Distributed Multimedia Syst, Rundeturmstr 10, D-64283 Darmstadt, Germany.
EM stefan.wilk@cs.tu-darmstadt.de; kopf@informatik.uni-mannheim.de;
   effelsberg@informatik.uni-mannheim.de
OI Kopf, Stephan/0000-0002-1140-6685
FU DFG [CRC 1053 MAKI]
FX This work has been funded by the DFG as part of the CRC 1053 MAKI.
CR [Anonymous], 2006, P 9 EUR C COMP VIS 1
   [Anonymous], 2004, VIS
   [Anonymous], 2013, ACM QUEUE
   Bermbach David, 2011, ACM WORKSH MIDDL SER
   Bouajjani A, 2014, ACM SIGPLAN NOTICES, V49, P285, DOI 10.1145/2535838.2535877
   Bouillot Nicolas, 2004, ACM SIGOPS OPERATING, V38
   Brewer E., 2000, 19 ANN ACM S PRINC D
   Bulterman Dick, 2004, CREATING PEER LEVEL, V2004
   Burckhardt S, 2012, LECT NOTES COMPUT SC, V7313, P283, DOI 10.1007/978-3-642-31057-7_14
   Cesar Pablo, 2008, ACM INT C MULT
   Chi Pei-Yu, 2011, ACM C HUM FACT COMP
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Girgensohn Andreas, 2003, HUMAN COMPUTER INTER
   Goldman Dan, 2008, ACM S UI SOFTW TECHN
   Grossman Tovi, 2010, ACM C HUM FACT COMP
   Guimaraes Rodrigo Laiola, 2012, ACM BRAZ S MULT WEB
   Guimaraes Rodrigo Laiola, 2010, ACM S DOC ENG
   Gustavsson Sanny, 2002, ACM WORKSH SELF HEAL
   Hellerstein Joseph M., 2011, C INN DAT SYST RES
   Ismail Ari, 2003, IEEE ANAL SIMULATION
   Juhlin Oskar, 2011, ACM C HUM FACT COMP
   Kopf Stephan, 2012, IEEE INT C MULT EXP
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Lucas Bruce D., 1981, ACM INT JOINT C ART
   Martínez JM, 2002, IEEE MULTIMEDIA, V9, P78, DOI 10.1109/93.998074
   McDaniel CD., 1998, Marketing research essentials, V2nd edn
   Meixner B, 2014, MULTIMED TOOLS APPL, V70, P1251, DOI 10.1007/s11042-012-1218-6
   Meixner Britta, 2015, ACM C MULT
   Mirbel I, 2000, VLDB J, V9, P111, DOI 10.1007/PL00010674
   Mu M, 2013, IEEE COMMUN MAG, V51, P112, DOI 10.1109/MCOM.2013.6576348
   Rahman Muntasir Raihan, 2012, USENIX WORKSH HOT TO
   Saito Y, 2005, ACM COMPUT SURV, V37, P42, DOI 10.1145/1057977.1057980
   Sand Peter, 2008, International Journal of Computer Vision, V80, P72, DOI 10.1007/s11263-008-0136-6
   Sawhney Nitin, 1996, ACM INT C HYP
   Weng SK, 2006, J VIS COMMUN IMAGE R, V17, P1190, DOI 10.1016/j.jvcir.2006.03.004
   Stein AN, 2009, IMAGE VISION COMPUT, V27, P514, DOI 10.1016/j.imavis.2008.04.017
   Vogels W., 2008, Queue, V6, P14, DOI [DOI 10.1145/1466443.1466448, 10. 1145/1466443.1466448]
   Wilk S., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P105, DOI 10.1109/WACV.2012.6163022
   Wilk S, 2014, IEEE INT CON MULTI
   Wilk Stefan, 2013, AACE P WORLD C ED MU
   Xiangming Mu, 2003, IEEE C DIG LIB
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
NR 42
TC 0
Z9 0
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2016
VL 12
IS 3
AR 45
DI 10.1145/2907983
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YO
UT WOS:000379425400011
DA 2024-07-18
ER

PT J
AU Pang, L
   Ngo, CW
AF Pang, Lei
   Ngo, Chong-Wah
TI Opinion Question Answering by Sentiment Clip Localization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Performance; Experimentation; Multimedia question answering;
   opinion clip localization; multimodality sentiment analysis
AB This article considers multimedia question answering beyond factoid and how-to questions. We are interested in searching videos for answering opinion-oriented questions that are controversial and hotly debated. Examples of questions include "Should Edward Snowden be pardoned?" and "Obamacare-unconstitutional or not?". These questions often invoke emotional response, either positively or negatively, hence are likely to be better answered by videos than texts, due to the vivid display of emotional signals visible through facial expression and speaking tone. Nevertheless, a potential answer of duration 60s may be embedded in a video of 10min, resulting in degraded user experience compared to reading the answer in text only. Furthermore, a text-based opinion question may be short and vague, while the video answers could be verbal, less structured grammatically, and noisy because of errors in speech transcription. Direct matching of words or syntactic analysis of sentence structure, such as adopted by factoid and how-to question-answering, is unlikely to find video answers. The first problem, the answer localization, is addressed by audiovisual analysis of the emotional signals in videos for locating video segments likely expressing opinions. The second problem, questions and answers matching, is tackled by a deep architecture that nonlinearly matches text words in questions and speeches in videos. Experiments are conducted on eight controversial topics based on questions crawled from Yahoo! Answers and Internet videos from YouTube.
C1 [Pang, Lei] City Univ Hong Kong, Dept Comp Sci, Creat Media Ctr M5001, 83 Tat Chee Ave, Kowloon Tong, Hong Kong, Peoples R China.
   [Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Creat Media Ctr M5003, 83 Tat Chee Ave, Kowloon Tong, Hong Kong, Peoples R China.
C3 City University of Hong Kong; City University of Hong Kong
RP Pang, L (corresponding author), City Univ Hong Kong, Dept Comp Sci, Creat Media Ctr M5001, 83 Tat Chee Ave, Kowloon Tong, Hong Kong, Peoples R China.; Ngo, CW (corresponding author), City Univ Hong Kong, Dept Comp Sci, Creat Media Ctr M5003, 83 Tat Chee Ave, Kowloon Tong, Hong Kong, Peoples R China.
EM leipang3-c@my.cityu.edu.hk; cscwngo@cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 118812, CityU 11210514]
FX The work described in this article was supported by two grants from the
   Research Grants Council of the Hong Kong Special Administrative Region,
   China (CityU 118812 and CityU 11210514).
CR [Anonymous], 1996, PR TREC
   [Anonymous], 2013, P 13 ACM MULT C
   [Anonymous], 1998, NEURAL NETWORKS TRIC
   Baccianella S, 2010, LREC 2010 - SEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION
   Brill E., 2001, P 10 TEXT RETRIEVAL
   Cao Jinwei, 2004, P 4 ACM IEEE CS JOIN
   Chen Tao, 2014, P ACM INT C MULT
   CHUA TS, 2009, P 1 ACM WORKSH LARG
   Everingham M., 2006, P BRIT MACHINE VISIO
   Hermjakob U., 2002, P TREC 2002
   Kacmarcik Gary, 2005, ASIA FEDERATION  NAT
   Khoury Elie, 2013, P 3 ACM C INT C MULT
   Lee YS, 2009, J AM SOC INF SCI TEC, V60, P509, DOI 10.1002/asi.21002
   Li GD, 2010, IEEE MULTIMEDIA, V17, P46, DOI 10.1109/MMUL.2010.47
   Lu Zhengdong, 2013, ADV NEURAL INFORM PR
   Machajdik Jana, 2010, P INT C MULT
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Mimno David, 2007, P 24 INT C MACH LEAR
   Nie Liqiang, 2011, P 34 INT ACM C RES D
   Radev Dragomir R., 2001, P 10 INT C INF KNOWL
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Rouvier M, 2013, INTERSPEECH, P1476
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang Kai, 2009, P 32 INT ACM C RES D
   Wei Zhang, 2012, P 20 ACM INT C MULT
   Wu Wei, 2013, P 6 ACM INT C WEB SE
   Wu YC, 2008, IEEE T CIRC SYST VID, V18, P1411, DOI 10.1109/TCSVT.2008.2002831
   Wu Yu-Chyeh, 2004, P IEEE 6 INT S MULT
   Yang Hui, 2003, P 26 ANN INT ACM C R
   Yeh T, 2008, P 16 ACM INT C MULT
NR 31
TC 3
Z9 3
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2016
VL 12
IS 2
AR 31
DI 10.1145/2818711
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PK
UT WOS:000373906200005
OA Green Published
DA 2024-07-18
ER

PT J
AU Huang, JS
   Liu, S
   Xing, JL
   Mei, T
   Yan, SC
AF Huang, Junshi
   Liu, Si
   Xing, Junliang
   Mei, Tao
   Yan, Shuicheng
TI Circle & Search: Attribute-Aware Shoe Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Shoe retrieval; object
   detection; attribute learning
ID BAG
AB Taking the shoe as a concrete example, we present an innovative product retrieval system that leverages object detection and retrieval techniques to support a brand-new online shopping experience in this article. The system, called Circle & Search, enables users to naturally indicate any preferred product by simply circling the product in images as the visual query, and then returns visually and semantically similar products to the users. The system is characterized by introducing attributes in both the detection and retrieval of the shoe. Specifically, we first develop an attribute-aware part-based shoe detection model. By maintaining the consistency between shoe parts and attributes, this shoe detector has the ability to model high-order relations between parts and thus the detection performance can be enhanced. Meanwhile, the attributes of this detected shoe can also be predicted as the semantic relations between parts. Based on the result of shoe detection, the system ranks all the shoes in the repository using an attribute refinement retrieval model that takes advantage of query-specific information and attribute correlation to provide an accurate and robust shoe retrieval. To evaluate this retrieval system, we build a large dataset with 17,151 shoe images, in which each shoe is annotated with 10 shoe attributes e. g., heel height, heel shape, sole shape, etc.). According to the experimental result and the user study, our Circle & Search system achieves promising shoe retrieval performance and thus significantly improves the users' online shopping experience.
C1 [Huang, Junshi; Liu, Si; Yan, Shuicheng] Natl Univ Singapore, Singapore 117548, Singapore.
   [Xing, Junliang] Chinese Acad Sci, Inst Automat, Beijing 100864, Peoples R China.
   [Mei, Tao] Microsoft Res Asia, Beijing, Peoples R China.
C3 National University of Singapore; Chinese Academy of Sciences; Institute
   of Automation, CAS; Microsoft Research Asia; Microsoft
RP Huang, JS (corresponding author), Natl Univ Singapore, Singapore 117548, Singapore.
EM a0092558@nus.edu.sg
RI Mei, Tao/GQZ-0596-2022; Xing, Junliang/HGE-9630-2022; Yan,
   Shuicheng/HCI-1431-2022
OI Mei, Tao/0000-0002-5990-7307; Xing, Junliang/0000-0001-6801-0510; 
FU Singapore Ministry of Education [MOE2010-T2-1-087]; National Science
   Foundation of China [61303178]
FX This work is supported by the Singapore Ministry of Education under
   research grant MOE2010-T2-1-087. Dr. J. Xing was partially supported by
   the National Science Foundation of China under grant no. 61303178.
CR [Anonymous], P 11 EUR C COMP VI 5
   [Anonymous], 2004, DISTANCE TRANSFORMS
   Arandjelovic R, 2011, IEEE I CONF COMP VIS, P375, DOI 10.1109/ICCV.2011.6126265
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ferrari Vittorio, 2008, P NEUR INF PROC SYST
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Kang H, 2012, LECT NOTES COMPUT SC, V7577, P794, DOI 10.1007/978-3-642-33783-3_57
   Kovashka A, 2012, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2012.6248026
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Lu Shiyang, 2012, P 20 ACM INT C MULT, P1323
   Parikh D, 2011, PROC CVPR IEEE, P1681, DOI 10.1109/CVPR.2011.5995451
   Shen XH, 2012, LECT NOTES COMPUT SC, V7575, P114, DOI 10.1007/978-3-642-33765-9_9
   Siddiquie B, 2011, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2011.5995329
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
NR 20
TC 7
Z9 7
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2014
VL 11
IS 1
AR 3
DI 10.1145/2632165
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AP2XQ
UT WOS:000341939800003
DA 2024-07-18
ER

PT J
AU Wang, XX
   Wang, Y
   Hsu, D
   Wang, Y
AF Wang, Xinxi
   Wang, Yi
   Hsu, David
   Wang, Ye
TI Exploration in Interactive Personalized Music Recommendation: A
   Reinforcement Learning Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Recommender systems; machine learning;
   music; model; application
AB Current music recommender systems typically act in a greedy manner by recommending songs with the highest user ratings. Greedy recommendation, however, is suboptimal over the long term: it does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the needs to explore user preferences and to exploit this information for recommendation. This article presents a new approach to music recommendation by formulating this exploration-exploitation trade-off as a reinforcement learning task. To learn user preferences, it uses a Bayesian model that accounts for both audio content and the novelty of recommendations. A piecewise-linear approximation to the model and a variational inference algorithm help to speed up Bayesian inference. One additional benefit of our approach is a single unified model for both music recommendation and playlist generation. We demonstrate the strong potential of the proposed approach with simulation results and a user study.
C1 [Wang, Xinxi; Hsu, David; Wang, Ye] Natl Univ Singapore, Dept Comp Sci, SG, Singapore 117417, Singapore.
   [Wang, Yi] ASTAR, Inst High Performance Comp, Dept Comp Sci, SG, Singapore 138632, Singapore.
C3 National University of Singapore; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute of High Performance Computing
   (IHPC)
RP Wang, Y (corresponding author), Natl Univ Singapore, Dept Comp Sci, SG, Singapore 117417, Singapore.
EM wangye@comp.nus.edu.sg
RI Wang, Ye/KGL-6405-2024
OI Wang, Ye/0000-0002-0123-1260
FU Singapore National Research Foundation under its International Research
   Centre at Singapore Funding Initiative
FX This research is supported by the Singapore National Research Foundation
   under its International Research Centre at Singapore Funding Initiative
   and administered by the IDM Programme Office.
CR Agrawal S., 2012, P 25 ANN C LEARN THE
   Aizenberg N., 2012, P 21 INT C WORLD WID, P1, DOI DOI 10.1145/2187836.2187838
   [Anonymous], 2011, Proceedings of International Workshop on Diversity in Document Retrieval (DDR)
   [Anonymous], 2013, International Conference on Multimedia
   Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663
   Braunhofer M, 2013, INT J MULTIMED INF R, V2, P31, DOI 10.1007/s13735-012-0032-2
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Cano P., 2005, 13th Annual ACM International Conference on Multimedia, P211, DOI 10.1145/1101149.1101181
   Chen S., 2012, P 18 ACM SIGKDD INT, P714, DOI [DOI 10.1145/2339530.2339643, 10.1145/2339530.2339643]
   Cheng Z., 2014, Proceedings of international conference on multimedia retrieval p, P185, DOI [DOI 10.1145/2578726.2578751, 10.1145/2578726.2578751]
   Chi CY, 2010, CONF TECHNOL APPL, P60, DOI 10.1109/TAAI.2010.21
   Ebbinghaus H., 1913, CONTRIBUTION EXPT PS
   Eck D., 2007, P NEUR INF PROC SYST, V20
   Friedman N, 2009, Probabilistic Graphical Models: Principles and Techniques (Adaptive Computation and Machine Learning series
   Golovin N, 2004, ITCC 2004: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, VOL 1, PROCEEDINGS, P398, DOI 10.1109/ITCC.2004.1286487
   Gunawardana A, 2009, J MACH LEARN RES, V10, P2935
   Hariri N., 2012, P 6 ACM C RECOMMENDE, P131, DOI DOI 10.1145/2365952.2365979
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Hu Y., 2011, P 12 INT SOC MUS INF
   Hung-Chen Chen, 2001, Proceedings of the 2001 ACM CIKM. Tenth International Conference on Information and Knowledge Management, P231, DOI 10.1145/502585.502625
   Joachims T, 1997, INT JOINT CONF ARTIF, P770
   Kaminskas M., 2013, P 7 ACM C REC SYST N, P17
   Karimi R, 2011, PROC INT C TOOLS ART, P1069, DOI 10.1109/ICTAI.2011.182
   Kaufmann E., 2012, Artificial intelligence and statistics, P592
   Knees P, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2542205.2542206
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lathia N, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P210
   Li L., 2012, P 19 INT C WORLD WID, P661
   Li Lihong, 2011, P 4 ACM INT C WEB SE, P297, DOI DOI 10.1145/1935826.1935878
   Liebman E., 2014, DJ MC REINFORCEMENT
   Liu H, 2009, PROCEEDINGS OF THE 2009 INTERNATIONAL CONFERENCE ON COMPUTER TECHNOLOGY AND DEVELOPMENT, VOL 1, P545, DOI 10.1109/ICCTD.2009.246
   Logan B., 2002, ISMIR, V2, P295
   MacKinnon DP, 2007, BEHAV RES METHODS, V39, P384, DOI 10.3758/BF03193007
   May BC, 2012, J MACH LEARN RES, V13, P2069
   Newman MEJ, 2005, CONTEMP PHYS, V46, P323, DOI 10.1080/00107510500052444
   Salakhutdinov Ruslan, 2008, P INT C MACH LEARN, P880, DOI [10.1145/1390156.1390267, DOI 10.1145/1390156.1390267]
   Schedl M., 2014, P 20 INT C MULTIMEDI
   Shani G, 2005, J MACH LEARN RES, V6, P1265
   Shen J., 2013, P ACM MULT C MM 13, P1109
   Silva J., 2012, P 18 ACM SIGKDD INT, P325
   Song Y., 2012, P 9 INT S COMP MUS M
   Srivihok A, 2005, SEVENTH INTERNATIONAL CONFERENCE ON ELECTRONIC COMMERCE, VOLS 1 AND 2, SELECTED PROCEEDINGS, P287
   Sutton R., 1998, Reinforcement Learning: An Introduction
   Szepesvari C., 2010, SYNTHESIS LECT ARTIF, V4
   Taghipour N, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P1164
   Van den Oord A., 2013, P NIPS
   Wang Xinxi., 2012, Proceedings of the 20th ACM international conference on Multimedia, P99, DOI [DOI 10.1145/2393347.2393368, 10.1145/2393347.2393368]
   Xi Chen, 2013, WSDM, P193, DOI DOI 10.1145/2433396.2433420
   Yoshii K., 2006, ISMIR, P296
   Zhang BT, 2001, APPL ARTIF INTELL, V15, P665, DOI 10.1080/088395101750363993
   Zhang Yuan Cao, 2012, P 5 ACM INT C WEB SE, P13, DOI [10.1145/2124295.2124300, DOI 10.1145/2124295.2124300]
   Zheleva Elena., 2010, Proceedings of the 19th international conference on World wide web, WWW '10, P1019
NR 52
TC 22
Z9 22
U1 0
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2014
VL 11
IS 1
AR 7
DI 10.1145/2623372
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AP2XQ
UT WOS:000341939800007
DA 2024-07-18
ER

PT J
AU Zhang, TZ
   Xu, CS
AF Zhang, Tianzhu
   Xu, Changsheng
TI Cross-Domain Multi-Event Tracking via CO-PMHT
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Cross-domain; multi-modality;
   multi-event tracking; PMHT; CO-PMHT
ID VIDEO ANNOTATION; NEWS
AB With the massive growth of events on the Internet, efficient organization and monitoring of events becomes a practical challenge. To deal with this problem, we propose a novel CO-PMHT (CO-Probabilistic Multi-Hypothesis Tracking) algorithm for cross-domain multi-event tracking to obtain their informative summary details and evolutionary trends over time. We collect a large-scale dataset by searching keywords on two domains (Gooogle News and Flickr) and downloading both images and textual content for an event. Given the input data, our algorithm can track multiple events in the two domains collaboratively and boost the tracking performance. Specifically, the bridge between two domains is a semantic posterior probability, that avoids the domain gap. After tracking, we can visualize the whole evolutionary process of the event over time and mine the semantic topics of each event for deep understanding and event prediction. The extensive experimental evaluations on the collected dataset well demonstrate the effectiveness of the proposed algorithm for cross-domain multi-event tracking.
C1 [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   China Singapore Inst Digital Media, Singapore 119613, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Zhang, Tianzhu/AGY-9389-2022
OI Zhang, Tianzhu/0000-0003-0764-6106
FU National Program on Key Basic Research Project (973 Program)
   [2012CB316304]; National Natural Science Foundation of China [61225009,
   61303173]; Singapore National Research Foundation under International
   Research Centre at Singapore Funding Initiative
FX This work is supported in part by the National Program on Key Basic
   Research Project (973 Program, Project No. 2012CB316304) and National
   Natural Science Foundation of China (61225009, 61303173), and also by
   the Singapore National Research Foundation under International Research
   Centre at Singapore Funding Initiative and administered by the IDM
   Programme Office.
CR [Anonymous], 2002, P 27 ANN ACM SIGIR I
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P IEEE INT C COMP VI
   Bar-Shalom Y, 2009, IEEE CONTR SYST MAG, V29, P82, DOI 10.1109/MCS.2009.934469
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chieu H. L., 2004, P 27 ANN ACM SIGIR I
   Cox I., 1979, IEEE T PATTERN ANAL, V24, P843
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Duygulu P., 2005, P ACM MULT OCT, P820
   Kender JR, 2005, PROC CVPR IEEE, P1174
   Knan Z., 2005, IEEE T PATTERN ANAL, V27, P82
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Kumar R., 2004, P 10 ACM SIGKDD INT
   Lin FR, 2008, DECIS SUPPORT SYST, V45, P473, DOI 10.1016/j.dss.2007.06.009
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Liu S, 2012, IEEE T MULTIMEDIA, V14, P361, DOI 10.1109/TMM.2011.2174780
   Makkonen J, 2004, INFORM RETRIEVAL, V7, P347, DOI 10.1023/B:INRT.0000011210.12953.86
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   Streit R., 1995, PROBABILISTIC MULTIH, P428
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xie L, 2004, IEEE IMAGE PROC, P2383
   Xing J., 2009, 5 INT C WIR COMM NET, P1
   Yang YM, 1999, IEEE INTELL SYST APP, V14, P32, DOI 10.1109/5254.784083
   Yu Q, 2009, IEEE T PATTERN ANAL, V31, P2196, DOI 10.1109/TPAMI.2008.253
   Yun Zhai, 2005, 13th Annual ACM International Conference on Multimedia, P2, DOI 10.1145/1101149.1101152
   Zhang T., 2012, P 12 EUR C COMP VIS
   Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z
   Zhang TZ, 2013, IEEE T IND INFORM, V9, P149, DOI 10.1109/TII.2012.2218251
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhang TZ, 2012, IEEE T MULTIMEDIA, V14, P1206, DOI 10.1109/TMM.2012.2191944
NR 31
TC 17
Z9 20
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2014
VL 10
IS 4
AR 31
DI 10.1145/2602633
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AO4FY
UT WOS:000341292900001
DA 2024-07-18
ER

PT J
AU Murray, N
   Qiao, YS
   Lee, B
   Muntean, GM
AF Murray, Niall
   Qiao, Yuansong
   Lee, Brian
   Muntean, Gabriel-Miro
TI User-Profile-Based Perceived Olfactory and Visual Media Synchronization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Design; Experimentation; Olfaction; multimedia
   synchronization; subjective quality assessment; quality of experience
ID PERCEPTION; QUALITY; SMELL
AB As a step towards enhancing users' perceived multimedia quality levels, this article presents the results of a study which looked at user's perception of inter-stream synchronization between scent and video. The ability to detect and the perception of and impact of skew on user's quality of experience is analyzed considering user's age, sex, and culture (user profile). The results indicate that skews beyond a certain level between olfaction and video have a negative impact on user-perceived experience. Olfaction before video is more noticeable to users than olfaction after video, and assessors are more tolerable of olfactory data presented after video.
C1 [Murray, Niall; Qiao, Yuansong; Lee, Brian] Athlone Inst Technol, Atholone, Ireland.
   [Muntean, Gabriel-Miro] Dublin City Univ, Dublin 9, Ireland.
C3 Technological University of the Shannon: Midlands Midwest; Dublin City
   University
RP Murray, N (corresponding author), Athlone Inst Technol, Dublin Rd, Atholone, Ireland.
EM nmurray@research.ait.ie
RI Muntean, Gabriel-Miro/U-6783-2019; Qiao, Yuansong/A-1140-2017
OI Muntean, Gabriel-Miro/0000-0002-9332-4770; Qiao,
   Yuansong/0000-0002-1543-1589; Lee, Brian/0000-0002-8475-4074
FU Enterprise Ireland throught its Applied Research Enhancement (SUNAT);
   Enterprise Ireland throught its Technology Gateway (COMAND)
FX This work was funded by Enterprise Ireland throught its Applied Research
   Enhancement (SUNAT) and Technology Gateway (COMAND) funding programs.
CR Ademoye OA, 2009, IEEE T MULTIMEDIA, V11, P561, DOI 10.1109/TMM.2009.2012927
   [Anonymous], 2011, ACM INT C MULTIMEDIA
   Ariyakul Y, 2011, P IEEE VIRT REAL ANN, P193, DOI 10.1109/VR.2011.5759464
   Ayabe-Kanamura S, 1998, CHEM SENSES, V23, P31, DOI 10.1093/chemse/23.1.31
   Calvert G., 2004, The Handbook of Multisensory Processes.
   Cha J., 2009, P 17 ACM INT C MULT
   Chastrette M, 2002, OLFACTION, TASTE, AND COGNITION, P100, DOI 10.1017/CBO9780511546389.012
   Dann G. M. S., 2003, Tourism Geographies, V5, P3, DOI 10.1080/1461668032000034033
   Eid M, 2011, IEEE T INSTRUM MEAS, V60, P21, DOI 10.1109/TIM.2010.2065530
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   Ghinea G, 2011, MULTIMED TOOLS APPL, V55, P601, DOI 10.1007/s11042-010-0581-4
   Ghinea G, 2010, IEEE T SYST MAN CY A, V40, P657, DOI 10.1109/TSMCA.2010.2041224
   Gulliver SR, 2006, ACM T MULTIM COMPUT, V2, P241, DOI 10.1145/1201730.1201731
   Hoshino S., 2011, P IEEE INT WORKSH TE
   Huang ZX, 2012, IEEE INFOCOM SER, P2786, DOI 10.1109/INFCOM.2012.6195700
   International Telecommunications Union, 2008, ITU T P 910 SUBJ VID
   ISO, 2006, ISO 5496:2006
   ISO, 2007, 8589 ISOIEC
   ISO, 2011, 230053 ISOIEC FDIS
   ISO, 2008, 54922008 ISO
   ITU, 2002, ITU T BT 500 METH SU
   KAYE N, 2001, THESIS MIT CAMBRIDGE
   Lawless H T., 1997, Tasting and Smelling, P125, DOI DOI 10.1016/B978-012161958-9/50005-1
   Murray N., 2013, P ACM MULT SYST C
   Murray N., 2014, ACM COMMUN IN PRESS
   Murray N, 2013, IEEE INT CON MULTI
   Nakamoto T, 2008, IEEE COMPUT GRAPH, V28, P75, DOI 10.1109/MCG.2008.3
   Nakamoto T, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P179
   Narumi T, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P93
   Noguchi D, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P83
   Pair J, 2006, P IEEE VIRT REAL ANN, P67, DOI 10.1109/VR.2006.23
   Ramic B., 2006, P SPRING C COMP GRAP
   Shams L, 2008, TRENDS COGN SCI, V12, P411, DOI 10.1016/j.tics.2008.07.006
   Spencer BS, 2006, IEEE T INF TECHNOL B, V10, P168, DOI 10.1109/TITB.2005.856851
   Steinmetz R, 1996, IEEE J SEL AREA COMM, V14, P61, DOI 10.1109/49.481694
   Steinmetz R., 1995, MULTIMEDIA COMPUTING
   Sugimoto Sayumi., 2010, Proceedings of the 18th ACM International Conference on Multimedia, MM'10, P301, DOI DOI 10.1145/1873951.1873994
   Timmerer C, 2012, SIGNAL PROCESS-IMAGE, V27, P909, DOI 10.1016/j.image.2012.01.016
   Tomono A., 2004, P HUM INT S, P249
   Washburn D. A., 2003, MODELING SIMULATION, V2, P19
   Zixia H., 2012, P 3 MULTIMEDIA SYSTE, P29
NR 41
TC 39
Z9 40
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2014
VL 10
IS 1
SU S
SI SI
AR 11
DI 10.1145/2540994
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AA2EF
UT WOS:000330907200003
DA 2024-07-18
ER

PT J
AU Chen, Y
   Deshpande, AA
   Aygün, RS
AF Chen, Yi
   Deshpande, Abhidnya A.
   Ayguen, Ramazan S.
TI Sprite Generation Using Sprite Fusion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Sprite generation; video processing; video
   standards
ID GLOBAL MOTION ESTIMATION; VIDEO; ROBUST; SEQUENCES; EFFICIENT; MOSAICS
AB There has been related research for sprite or mosaic generation for over 15 years. In this article, we try to understand the methodologies for sprite generation and identify what has not actually been covered for sprite generation. We first identify issues and focus on the domain of videos for sprite generation. We introduce a novel sprite fusion method that blends two sprites. Sprite fusion method produces good results for tracking videos and does not require object segmentation. We present sample results of our experiments.
C1 [Chen, Yi; Deshpande, Abhidnya A.; Ayguen, Ramazan S.] Univ Alabama, Dept Comp Sci, Huntsville, AL 35899 USA.
C3 University of Alabama System; University of Alabama Huntsville
RP Chen, Y (corresponding author), Univ Alabama, Dept Comp Sci, Huntsville, AL 35899 USA.
EM yichen@cs.uah.edu; adeshpan@cs.uah.edu; raygun@cs.uah.edu
RI Chen, Yi/G-6720-2013; Aygun, Ramazan/HTQ-3507-2023
OI Aygun, Ramazan/0000-0001-7244-7475
FU National Science Foundation [0812307]; Direct For Computer & Info Scie &
   Enginr; Div Of Information & Intelligent Systems [0812307] Funding
   Source: National Science Foundation
FX This material is based on work supported by the National Science
   Foundation under Grant No. 0812307.
CR Alzoubi H, 2008, INFORM SCIENCES, V178, P3415, DOI 10.1016/j.ins.2008.05.004
   Asif Muhammad, 2008, 2008 IEEE Fifth International Conference on Advanced Video and Signal Based Surveillance, P60, DOI 10.1109/AVSS.2008.27
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Aygün RS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1503, DOI 10.1109/ICME.2004.1394531
   Aygün RS, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA537
   Bevilacqua A, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P511
   Chen LH, 2006, INT C PATT RECOG, P723
   Chen Y, 2010, INT J MULTIMED DATA, V1, P34, DOI 10.4018/jmdem.2010040103
   Cherng DC, 2007, IEEE INT SYMP CIRC S, P2902, DOI 10.1109/ISCAS.2007.377856
   Cheung HK, 2007, IET IMAGE PROCESS, V1, P13, DOI 10.1049/iet-ipr:20050212
   CHEUNG H.-K., 2002, P IEEE INT S CIRC SY
   Cheung HK, 2008, IEEE T CIRC SYST VID, V18, P522, DOI 10.1109/TCSVT.2008.918549
   Chien SY, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P785
   Coorg S, 2000, INT J COMPUT VISION, V37, P259, DOI 10.1023/A:1008184124789
   Dasu AR, 2004, IEEE T CIRC SYST VID, V14, P244, DOI 10.1109/TCSVT.2003.819185
   Deshpande AA, 2009, PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON DATABASE AND EXPERT SYSTEMS APPLICATION, P231, DOI 10.1109/DEXA.2009.77
   Dufaux F, 2000, IEEE T IMAGE PROCESS, V9, P497, DOI 10.1109/83.826785
   Farin D, 2006, IEEE T CIRC SYST VID, V16, P492, DOI 10.1109/TCSVT.2006.872781
   Geys I, 2006, SIGNAL PROCESS-IMAGE, V21, P709, DOI 10.1016/j.image.2006.08.001
   Grammalidis N., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P477, DOI 10.1109/ICIP.1999.822942
   Hsu CT, 2004, SIGNAL PROCESS-IMAGE, V19, P81, DOI 10.1016/j.image.2003.10.001
   Irani M, 1998, P IEEE, V86, P905, DOI 10.1109/5.664279
   *ISO IEC, 14496102003 ISOIEC
   Krutz Andreas, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P313, DOI 10.1109/3DTV.2008.4547871
   Krutz A, 2006, IEEE IMAGE PROC, P353, DOI 10.1109/ICIP.2006.313166
   Krutz A, 2008, MONOGR COTSEN INST A, P459
   KUNTER M., 2008, P INT C IM PROC ICIP
   LAI J., 2009, P IEEE INT C MULT EX
   Lee MC, 1997, IEEE T CIRC SYST VID, V7, P130, DOI 10.1109/76.554424
   Lu Y, 2003, IEEE T CIRC SYST VID, V13, P394, DOI 10.1109/TCSVT.2003.811607
   Lu Y, 2001, IEEE IMAGE PROC, P473, DOI 10.1109/ICIP.2001.959056
   Lu Y, 2001, LECT NOTES COMPUT SC, V2195, P118
   Marzotto R, 2004, PROC CVPR IEEE, P692
   MPEG4 SOFTWARE, 144967 ISOIEC
   MPEG4-2, 1449622004 ISOIEC
   NAGARAJ R. C., 2001, P SOC PHOTO-OPT INS, V4313, P69
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   PARIKH P., 2007, P IEEE WORKSH MOT VI, P26
   Peleg S, 2000, IEEE T PATTERN ANAL, V22, P1144, DOI 10.1109/34.879794
   RICHTER H., P PICT COD S
   Salembier P., 1998, 4 EUROPEAN SIGNAL PR, P2105
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Smolic A, 1999, IEEE T CIRC SYST VID, V9, P1227, DOI 10.1109/76.809158
   SMOLIC A, 2000, P IEEE INT C IM PROC
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Steedly D, 2005, IEEE I CONF COMP VIS, P1300
   Szeliski R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P251, DOI 10.1145/258734.258861
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   TAUBMAN D., 2002, JPEG2000 IMAGE COMPR
   Teodosio L., 1993, Proceedings ACM Multimedia 93, P39, DOI 10.1145/166266.166270
   Teodosio L, 2005, ACM T MULTIM COMPUT, V1, P16, DOI 10.1145/1047936.1047940
   TO L. T., 2005, THESIS U NEW S WALES
   YE G., 2005, P IEEE INT C IM PROC, P11
   YE G., 2008, P IEEE 10 WORKSH MUL, P70
   Yong Shen, 2004, Proceedings. Third International Conference on Image and Graphics, P560
   ZHU Z., 1999, P IEEE INT C MULT CO
   Zoghlami I, 1997, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.1997.609359
NR 57
TC 3
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2012
VL 8
IS 2
AR 22
DI 10.1145/2168996.2169002
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 945BX
UT WOS:000304248900006
DA 2024-07-18
ER

PT J
AU Feng, WC
   Kaiser, E
   Feng, WC
   Baillif, M
AF Feng, Wu-Chi
   Kaiser, Ed
   Feng, Wu Chang
   Le Baillif, Mikael
TI Panoptes: Scalable Low-Power Video Sensor Networking Technologies
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Measurement; Performance; Video sensor networking; video
   collection; adaptive video
AB Video-based sensor networks can provide important visual information in a number of applications including: environmental monitoring, health care, emergency response, and video security. This article describes the Panoptes video-based sensor networking architecture, including its design, implementation, and performance. We describe two video sensor platforms that can deliver high-quality video over 802.11 networks with a power requirement less than 5 watts. In addition, we describe the streaming and prioritization mechanisms that we have designed to allow it to survive long-periods of disconnected operation. Finally, we describe a sample application and bitmapping algorithm that we have implemented to show the usefulness of our platform. Our experiments include an in-depth analysis of the bottlenecks within the system as well as power measurements for the various components of the system.
C1 [Feng, Wu-Chi; Kaiser, Ed; Feng, Wu Chang; Le Baillif, Mikael] Portland State Univ, Dept Comp Sci, Portland, OR 97207 USA.
C3 Portland State University
RP Feng, WC (corresponding author), Portland State Univ, Dept Comp Sci, POB 751, Portland, OR 97207 USA.
EM wuchi@cs.pdx.edu; edkaiser@cs.pdx.edu; wuchang@cs.pdx.edu;
   mikael.le_baillif@enseirb.fr
CR BULUSU N, 2003, ACM T EMBED COMP MAY
   CORNER M, 2001, P ACM SPIE MULT COMP
   EKICI E, 1999, P INFOCOM 1999
   ESTRIN D, 1992, IEEE PERV COMPUT JAN, P59
   FENG W, 1999, P ACM SPIE MULT COMP
   FENG WC, 2003, P ACM MULT 2003 ACM
   Flinn J, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P48, DOI 10.1145/319344.319155
   Hill J. N., 2000, ARCHITECTURAL SUPPOR, P83
   KRASIC B, 2003, P 13 INT WORKSH NETW
   Kravets R, 2000, WIREL NETW, V6, P263, DOI 10.1023/A:1019149900672
   RHEE I, 1998, P SIGCOMM 1998 ACM N
   Stockdon H.F., 2000, J GEOPHYS RES, V105, p22,01522,033
   TAN W, 1999, IEEE T MULTIMED, V1
NR 13
TC 24
Z9 24
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2005
VL 1
IS 2
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DV
UT WOS:000205012300003
DA 2024-07-18
ER

PT J
AU Teodosio, L
   Bender, W
AF Teodosio, Laura
   Bender, Walter
TI Salient Stills
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Salient stills; semantic image processing; video summary;
   video database; timeprints; media transcoding; shape-time photography;
   synopsis mosaic; video mosaic
AB Salient Stills are a class of images that reflect the aggregation of the temporal changes that occur in a moving-image sequence with the salient features of individual frames preserved. They convey the intended expression of an entire series of moving frames - a visual summary of camera and object movements. The original frames, which may include variations in focal length or field of view, or moving objects, are combined to create a single still image. The still image may have multiresolution patches, a larger field of view, or higher overall resolution than any individual frame in the original image sequence. Salient Stills may also contain selected significant objects from individual or multiple video frames. Salient Stills attempt to retain much of the original content ( detail) and spatial and temporal extent ( context) of the original video or film sequence.
C1 [Teodosio, Laura] Salient Stills Inc, Boston, MA 02210 USA.
   [Bender, Walter] MIT, Media Lab, Cambridge, MA 02139 USA.
C3 Massachusetts Institute of Technology (MIT)
RP Teodosio, L (corresponding author), Salient Stills Inc, 268 Summer St,Fl 6, Boston, MA 02210 USA.
EM laura@salientstills.com; walter@media.mit.edu
OI /0000-0003-1555-8560
CR Adelson E. H., 1984, RCA engineer, V29, P33, DOI 10.1.1.59.9419.
   [Anonymous], 1988, International journal of computer vision
   BENDER W, P USENIX, P329
   Bergen J. R., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P27, DOI 10.1109/ICCV.1990.139486
   Blankinship E, 2004, BT TECHNOL J, V22, P151, DOI 10.1023/B:BTTJ.0000047594.85806.c9
   BOVE VM, 1993, DIGITAL IMAGES HUMAN, P23
   Braun Marta., 1992, PICTURING TIME WORK
   CARRIERI R, 1966, FUTURISM
   Catmull E., 1980, Computer Graphics, V14, P279, DOI 10.1145/965105.807505
   DOYLE T, 1986, INT C CONS EL
   ELLIOT E, 1992, MULTIPLE VIEWS DIGIT
   FREEMAN W, 2003, IEEE COMPUT VIS PATT, V2, P151
   GIROD B, 1989, P OPT SOC AM M UND M, P73
   Hagen M., 1980, PERCEPTION PICTURES
   Hockney D., 1988, Hockney on photography : Conversations with Paul Joyce
   HOLTZMAN H, 1991, THESIS MIT CAMBRIDGE
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   HU A, 1987, THESIS MIT CAMBRIDGE
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   IRANI M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P605, DOI 10.1109/ICCV.1995.466883
   JAIN JR, 1981, IEEE T COMMUN, V29, P1799, DOI 10.1109/TCOM.1981.1094950
   JONES RC, 1999, P ACM MULTIMEDIA C, P29
   Kayafas Gus., 1987, Stopping Time: The Photographs of Harold Edgerton
   LAI KY, 1988, ACM T INFORM SYST, V6, P332, DOI 10.1145/58566.59298
   Lie H., 1997, Cascading Style Sheets
   LIM J, 1975, COMPUT GRAPH IMAGE P, V4
   LIPPMAN A, 1991, COMMUN ACM, V34, P92, DOI 10.1145/103085.103094
   LIPPMAN A, 1992, Patent No. 5262856
   MACHOVER T, 1991, HYPERSTRING TRILOGY
   MANN S, 1993, P 46 ANN IS T C SOC
   Massey M., 1996, Proceedings ACM Multimedia 96, P401, DOI 10.1145/244130.244430
   Massey M, 1996, IBM SYST J, V35, P557, DOI 10.1147/sj.353.0557
   MCLEAN P, 1991, THESIS MIT CAMBRIDGE
   Ritchin Fred., 1990, Our Own Image: The Coming RevolutionIn Photography
   SAWHNEY HS, 1995, P IEEE INT C IM PROC, V1, P322
   SCHARF Aaron., 1968, Art and Photography, VFirst
   SHEPP A, 1989, INTRO IMAGES IMAGING
   Teodosio L., 1993, Proceedings ACM Multimedia 93, P39, DOI 10.1145/166266.166270
   TEODOSIO L, 1992, THESIS MIT CAMBRIDGE
   TEODOSIO L, 1992, Patent No. 5657402
   Teodosio L. A., 1993, Proceedings ACM Multimedia 93, P359, DOI 10.1145/166266.168422
   Toyama K., 1999, The Proceedings of the Seventh IEEE International Conference on Computer Vision, V1, P255
   VASCONCELOS N, 1998, P IEEE COMP SOC C CO, P771
   VERCOE BL, 1991, P INT COMP MUS C
   WATLINGTON J, 1989, THESIS MIT CAMBRIDGE
   WEITZMAN LM, 1995, THESIS MIT CAMBRIDGE
   1998, ESPN MAGAZINE   0420, P42
   1998, ESPN MAGAZINE   0323, P88
NR 48
TC 13
Z9 22
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2005
VL 1
IS 1
BP 16
EP 36
DI 10.1145/1047936.1047940
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DU
UT WOS:000205012200004
DA 2024-07-18
ER

PT J
AU Huang, C
   Peng, XL
   Liu, D
   Lu, Y
AF Huang, Cong
   Peng, Xiulian
   Liu, Dong
   Lu, Yan
TI Text Image Super-Resolution Guided by Text Structure and Embedding
   Priors
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Text image super-resolution; text-structure prior; text-embedding prior
ID NETWORK; RECOGNITION
AB We aim to super-resolve text images from unrecognizable low-resolution inputs. Existing super-resolution methods mainly learn a direct mapping from low-resolution to high-resolution images by exploring low-level features, which usually generate blurry outputs and suffer from severe structure distortion for text parts, especially when the resolution is quite low. Both the visual quality and the readability will suffer. To tackle these issues, we propose a new text super-resolution paradigm by recovering with understanding. Specifically, we extract a text-embedding prior and a text-structure prior from the upsampled image by learning to understand the text. The two priors with rich structure information and text-embedding information are then used as auxiliary information to recover the clear text structure. In addition, we introduce a text-feature loss to guide the training for better text recognizability. Extensive evaluations on both screen and scene text image datasets show that our method largely outperforms the state-of-the-art in both visual quality and recognition accuracy.
C1 [Huang, Cong; Liu, Dong] Univ Sci & Technol China, 96 JinZhai Rd, Hefei, Peoples R China.
   [Peng, Xiulian; Lu, Yan] Microsoft Res Asia, 5 Dan Ling St, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Huang, C (corresponding author), Univ Sci & Technol China, 96 JinZhai Rd, Hefei, Peoples R China.
EM hcy96@mail.ustc.edu.cn; xipe@microsoft.com; dongeliu@ustc.edu.cn;
   yanlu@microsoft.com
RI zhang, yueqi/JXM-4287-2024; Zhang, Can/JUU-9511-2023
OI Huang, Cong/0000-0002-5389-1658; Liu, Dong/0000-0001-9100-2906
CR Baek J, 2019, IEEE I CONF COMP VIS, P4714, DOI 10.1109/ICCV.2019.00481
   Cairong Zhao, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P2908, DOI 10.1145/3474085.3475469
   Chen JY, 2021, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR46437.2021.01185
   Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Jaderberg M, 2015, ADV NEUR IN, V28
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245
   Li JC, 2021, IEEE T CIRC SYST VID, V31, P2547, DOI 10.1109/TCSVT.2020.3027732
   Li XG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282445
   Li YC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414838
   Liang Liao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P683, DOI 10.1007/978-3-030-58583-9_41
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Ma H., 2022, ACM T MULTIM COMPUT, V18, P1
   Ma J., 2021, arXiv
   Ma JQ, 2022, PROC CVPR IEEE, P5901, DOI 10.1109/CVPR52688.2022.00582
   Mei YQ, 2020, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR42600.2020.00573
   Mishra A, 2012, PROC CVPR IEEE, P2687, DOI 10.1109/CVPR.2012.6247990
   Qin Rui, 2022, ARXIV
   Quan YH, 2020, IEEE T COMPUT IMAG, V6, P778, DOI 10.1109/TCI.2020.2981758
   Raghunandan KS, 2019, IEEE T CIRC SYST VID, V29, P1145, DOI 10.1109/TCSVT.2018.2817642
   Ray Anupama, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P59, DOI 10.1109/ICDAR.2019.00019
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang XCA, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3454009
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JR, 2017, ADV DIFFER EQU-NY, DOI 10.1186/s13662-017-1389-6
   Wang WJ, 2019, Arxiv, DOI arXiv:1909.07113
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang YF, 2019, IEEE T CIRC SYST VID, V29, P1259, DOI 10.1109/TCSVT.2018.2839879
   Wenjia Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P650, DOI 10.1007/978-3-030-58607-2_38
   Xiaobin Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P763, DOI 10.1007/978-3-030-58548-8_44
   Yongqiang Mou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P158, DOI 10.1007/978-3-030-58555-6_10
   Zhang Dengyong, 2022, ACM T MULTIM COMPUT
   Zhang DY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3398685
   Zhang HC, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao MY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5646, DOI 10.1145/3474085.3475710
   Zhao Minyi, 2022, P IJCAI, P1707
NR 52
TC 0
Z9 0
U1 11
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 199
DI 10.1145/3595924
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200022
DA 2024-07-18
ER

PT J
AU Feng, DD
   He, XT
   Peng, YX
AF Feng, Duoduo
   He, Xiangteng
   Peng, Yuxin
TI MKVSE: Multimodal Knowledge Enhanced Visual-semantic Embedding for
   Image-text Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image-text retrieval; cross-modal retrieval; visual-semantic embedding;
   multimodal knowledge graph
AB Image-text retrieval aims to take the text (image) query to retrieve the semantically relevant images (texts), which is fundamental and critical in the search system, online shopping, and social network. Existing works have shown the effectiveness of visual-semantic embedding and unimodal knowledge exploiting (e.g., textual knowledge) in connecting the image and text. However, they neglect the implicit multimodal knowledge relations between these two modalities when the image contains information that is not directly described in the text, hindering the ability to connect the image and text with the implicit semantic relations. For instance, an image shows a person next to the "tap" but the pairing text description may only include the word "wash," missing the washing tool "tap." The implicit semantic relation between image object "tap" and text word "wash" can help to connect the above image and text. To sufficiently utilize the implicit multimodal knowledge relations, we propose a Multimodal Knowledge enhanced Visual-Semantic Embedding (MKVSE) approach building a multimodal knowledge graph to explicitly represent the implicit multimodal knowledge relations and injecting it to visual-semantic embedding for image-text retrieval task. The contributions in this article can be summarized as follows: (1) Multimodal Knowledge Graph (MKG) is proposed to explicitly represent the implicit multimodal knowledge relations between the image and text as intra-modal semantic relations and inter-modal co-occurrence relations. Intra-modal semantic relations provide synonymy information that is implicit in the unimodal data such as the text corpus. And inter-modal co-occurrence relations characterize the co-occurrence correlations (such as temporal, causal, and logical) that are implicit in image-text pairs. These two relations help establishing reliable image-text connections in the higher-level semantic space. (2) Multimodal Graph Convolution Networks (MGCN) is proposed to reason on the MKG in two steps to sufficiently utilize the implicit multimodal knowledge relations. In the first step, MGCN focuses on the intra-modal relations to distinguish other entities in the semantic space. In the second step, MGCN focuses on the inter-modal relations to connect multimodal entities based on co-occurrence correlations. The two-step reasoning manner can sufficiently utilize the implicit semantic relations between two modal entities to enhance the embeddings of the image and text. Extensive experiments are conducted on two widely used datasets, namely, Flickr30k and MSCOCO, to demonstrate the superiority of the proposed MKVSE approach in achieving state-of-the-art performances. The codes are available at https://github.com/PKU-ICST-MIPL/MKVSE- TOMM2023.
C1 [Feng, Duoduo; He, Xiangteng; Peng, Yuxin] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
   [Peng, Yuxin] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Peking University; Peng Cheng Laboratory
RP Peng, YX (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.; Peng, YX (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.
EM pengyuxin@pku.edu.cn
OI He, Xiangteng/0000-0001-8502-5685
FU National Natural Science Foundation of China [62132001, 61925201,
   62272013, U22B2048]; 2022 Tencent Wechat Rhino-Bird Focused Research
   Program
FX This work was supported by the grants from the National Natural Science
   Foundation of China (62132001, 61925201, 62272013, U22B2048) and by 2022
   Tencent Wechat Rhino-Bird Focused Research Program.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Arora S, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2650
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bird S., 2009, NATURAL LANGUAGE PRO
   Chen DW, 2021, LECT NOTES COMPUT SC, V12682, P186, DOI 10.1007/978-3-030-73197-7_12
   Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Cheng YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499027
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Ding Y, 2022, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR52688.2022.00503
   Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201
   Faghri F, 2018, Arxiv, DOI arXiv:1707.05612
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Garcia N, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P25, DOI 10.1145/3323873.3325028
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   Hou JY, 2019, IEEE I CONF COMP VIS, P8917, DOI 10.1109/ICCV.2019.00901
   Huang Po-Yao, 2016, P 1 C MACH TRANSL, DOI DOI 10.18653/V1/W16-2360
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Kahou SE, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P467, DOI 10.1145/2818346.2830596
   Kai Shu, 2017, ACM SIGKDD Explorations Newsletter, V19, P22, DOI 10.1145/3137597.3137600
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kipf Thomas N., 2017, 5 INT C LEARN REPRES
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li KP, 2023, IEEE T PATTERN ANAL, V45, P641, DOI 10.1109/TPAMI.2022.3148470
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li X J., 2020, P EUR C COMP VIS, P121, DOI DOI 10.1007/978-3-030-58577-8_8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Long SQ, 2022, IEEE WINT CONF APPL, P2463, DOI 10.1109/WACV51458.2022.00252
   Mai SJ, 2020, AAAI CONF ARTIF INTE, V34, P164
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Nian FD, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P411, DOI 10.1145/3123266.3123443
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qu LG, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1104, DOI 10.1145/3404835.3462829
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shi BT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5182
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Sun R, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1405, DOI 10.1145/3340531.3411947
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wang Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1793, DOI 10.1109/ICCV48922.2021.00183
   Wehrmann P, 2020, AAAI CONF ARTIF INTE, V34, P12313
   Yang JY, 2022, PROC CVPR IEEE, P15650, DOI 10.1109/CVPR52688.2022.01522
   Yang Shiquan, 2021, P INT JOINT C ART IN, P3978
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yin YJ, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3025
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Youze Wang, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P540, DOI 10.1145/3372278.3390713
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang SY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1292, DOI 10.1145/3394171.3413880
   Zhang SA, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3285029
NR 59
TC 5
Z9 6
U1 12
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 162
DI 10.1145/3580501
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300011
OA Bronze
DA 2024-07-18
ER

PT J
AU Yuan, J
   Chen, SK
   Zhang, Y
   Shi, ZC
   Geng, X
   Fan, JP
   Rui, Y
AF Yuan, Jin
   Chen, Shikai
   Zhang, Yao
   Shi, Zhongchao
   Geng, Xin
   Fan, Jianping
   Rui, Yong
TI Graph Attention Transformer Network for Multi-label Image Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Graph neural network; transformer; attention mechanism; multi-label
   classification
AB Multi-label classification aims to recognize multiple objects or attributes from images. The key to solving this issue relies on effectively characterizing the inter-label correlations or dependencies, which bring the prevailing graph neural network. However, current methods often use the co-occurrence probability of labels based on the training set as the adjacency matrix to model this correlation, which is greatly limited by the dataset and affects the model's generalization ability. This article proposes a Graph Attention Transformer Network, a general framework for multi-label image classification by mining rich and effective label correlation. First, we use the cosine similarity value of the pre-trained label word embedding as the initial correlation matrix, which can represent richer semantic information than the co-occurrence one. Subsequently, we propose the graph attention transformer layer to transfer this adjacency matrix to adapt to the current domain. Our extensive experiments have demonstrated that our proposed methods can achieve highly competitive performance on three datasets.
C1 [Yuan, Jin; Chen, Shikai; Geng, Xin] Southeast Univ, Nanjing, Peoples R China.
   [Zhang, Yao] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Shi, Zhongchao; Fan, Jianping; Rui, Yong] Lenovo Res, Beijing, Peoples R China.
C3 Southeast University - China; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Legend Holdings; Lenovo
RP Geng, X (corresponding author), Southeast Univ, Nanjing, Peoples R China.; Rui, Y (corresponding author), Lenovo Res, Beijing, Peoples R China.
EM yuanjin@seu.edu.cn; skchen@seu.edu.cn; zhangyao215@mails.ucas.ac.cn;
   shizc2@lenovo.com; xgeng@seu.edu.cn; jfan1@lenovo.com;
   yongrui@lenovo.com
OI Rui, Yong/0000-0002-9142-5914; Fan, Jianping/0000-0003-2290-1785; Zhang,
   Yao/0000-0002-8759-4811; shi, zhongchao/0000-0002-5216-3827; Yuan,
   Jin/0000-0002-9954-0693
CR Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bellorin J, 2020, Arxiv, DOI arXiv:1905.02546
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Casanova Arantxa, 2018, INT C LEARNING REPRE
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen SF, 2018, AAAI CONF ARTIF INTE, P6714
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Chen TS, 2018, AAAI CONF ARTIF INTE, P6730
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy Alexey, 2021, ICLR
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Galassi A, 2021, IEEE T NEUR NET LEAR, V32, P4291, DOI 10.1109/TNNLS.2020.3019893
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Ge ZY, 2018, Arxiv, DOI arXiv:1807.07247
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   HaidongWang Xuan He, 2022, ACM T MULTIM COMPUT, V2022
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang QH, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2598779
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Lanchantin J, 2021, PROC CVPR IEEE, P16473, DOI 10.1109/CVPR46437.2021.01621
   Li L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1092, DOI 10.1145/3240508.3240649
   Li Q, 2019, Arxiv, DOI arXiv:1909.13005
   Li YN, 2016, LECT NOTES COMPUT SC, V9910, P684, DOI 10.1007/978-3-319-46466-4_41
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu F, 2017, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2017.443
   Liu SL, 2021, Arxiv, DOI arXiv:2107.10834
   Liu WW, 2015, ADV NEUR IN, V28
   Liu Y, 2022, Arxiv, DOI arXiv:2111.06091
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Messina N, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451390
   Nam J., 2019, P 36 INT C MACH LEAR, P4733
   Nath ND, 2019, J INF TECHNOL CONSTR, V24, P511, DOI 10.36680/j.itcon.2019.028
   Nguyen HD, 2021, AAAI CONF ARTIF INTE, V35, P9092
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Radford A, Improving language understanding by generative pre-training
   Quevedo JR, 2012, PATTERN RECOGN, V45, P876, DOI 10.1016/j.patcog.2011.08.007
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Ridnik Tal, 2023, 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), P32, DOI 10.1109/WACV56688.2023.00012
   Shikai Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13981, DOI 10.1109/CVPR42600.2020.01400
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YG, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3524618
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Vaswani A, 2017, ADV NEUR IN, V30
   Vu XS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2299, DOI 10.1145/3394171.3414047
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang X, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2022, DOI 10.1145/3308558.3313562
   Wang Y, 2020, AAAI CONF ARTIF INTE, V34, P12265
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang H, 2016, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2016.37
   Yazici Vacit Oguz, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13437, DOI 10.1109/CVPR42600.2020.01345
   Yun S, 2019, ADV NEUR IN, V32
   Zhang Yong, 2022, ACM T MULTIM COMPUT, V2022
   Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369
   Zhou Wei, 2022, ACM T MULTIM COMPUT
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
   Zhuang N, 2018, PATTERN RECOGN, V80, P225, DOI 10.1016/j.patcog.2018.03.018
NR 61
TC 7
Z9 7
U1 10
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2023
VL 19
IS 4
AR 150
DI 10.1145/3578518
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FX8
UT WOS:001011937600012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yao, XR
   Wang, X
   Liu, Y
   Zhu, WW
AF Yao, Xuanrong
   Wang, Xin
   Liu, Yue
   Zhu, Wenwu
TI Continual Recognition with Adaptive Memory Update
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Continual learning; lifelong; incremental; replay
AB Class incremental continual learning aims to improve the ability of modern classification models to continually recognize new classes without forgetting the previous ones. Prior art in the field has largely considered using a replay buffer. In this article, we start froman observation that the existing replay-based method would fail when the stored exemplars are not hard enough to get a good decision boundary between a previously learned class and a new class. To prevent this situation, we propose a method from the perspective of remedy after forgetting for the first time. In the proposed method, a set of exemplars is preserved as a working memory, which helps to recognize new classes. When the working memory is insufficient to distinguish between new classes, more discriminating samples would be swapped froma long-termmemory, which is built up during the early training process, in an adaptive way. Our continual recognition model with adaptive memory update is capable of overcoming the problem of catastrophic forgetting with various new classes coming in sequence, especially for similar but different classes. Extensive experiments on different real-world datasets demonstrate that the proposed model is superior to existing state-of-the-art algorithms. Moreover, our model can be used as a general plugin for any replay-based continual learning algorithm to further improve their performance.
C1 [Yao, Xuanrong; Wang, Xin; Liu, Yue; Zhu, Wenwu] Tsinghua Univ, Dept Comp Sci & Technol, 30 Shuangqing Rd, Beijing, Peoples R China.
C3 Tsinghua University
RP Wang, X; Zhu, WW (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, 30 Shuangqing Rd, Beijing, Peoples R China.
EM yaoxr17@mails.tsinghua.edu.cn; xin_wang@tsinghua.edu.cn;
   liuyuethu@163.com; wwzhu@tsinghua.edu.cn
RI Wang, Chen/JZE-6385-2024; Wang, Fei/KEH-6292-2024; jing,
   wang/KCZ-2144-2024; Chen, Nuo/JZD-0344-2024; Wang,
   Jiachen/KFT-0161-2024; li, lan/KCJ-5061-2024; ZHOU, YUE/KCJ-8790-2024;
   zhang, zheng/KHY-8870-2024; ZHANG, JING/KHY-1073-2024; lin,
   lin/KFB-9548-2024
OI Wang, Xin/0000-0002-0351-2939
FU National Key Research and Development Program of China [2020AAA0106300];
   National Natural Science Foundation of China [62250008, 62222209,
   62102222]
FX This work was supported in part by the National Key Research and
   Development Program of China (no. 2020AAA0106300) and the National
   Natural Science Foundation of China (nos. 62250008, 62222209, and
   62102222).
CR Abati D, 2020, PROC CVPR IEEE, P3930, DOI 10.1109/CVPR42600.2020.00399
   Aljundi Rahaf, 2019, ADV NEURAL INFORM PR, P11872
   Bartol TM, 2015, ELIFE, V4, DOI 10.7554/eLife.10778
   Belouadah E., 2018, P EUROPEAN C COMPUTE
   Belouadah E, 2019, IEEE I CONF COMP VIS, P583, DOI 10.1109/ICCV.2019.00067
   Caccia L., 2020, INT C MACH LEARN, P1240
   Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15
   Chen Z., 2016, SUSTAINABILITY-BASEL, V10, P1, DOI [10.2200/S00737ED1V01Y201610AIM033, DOI 10.3390/SU10103378]
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhar P, 2019, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR.2019.00528
   Fritzke B., 1995, Advances in Neural Information Processing Systems 7, P625
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, COMPUT SCI, V2
   Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092
   Hung S.C.Y., 2019, Advances in Neural Information Processing Systems (NeurIPS)
   Isele D, 2018, AAAI CONF ARTIF INTE, P3302
   Kingma D. P., 2014, arXiv
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Knoblauch J, 2020, PR MACH LEARN RES, V119
   Krizhevsky A., 2009, Tech. Rep.
   Lee K, 2019, IEEE I CONF COMP VIS, P312, DOI 10.1109/ICCV.2019.00040
   Li XL, 2019, PR MACH LEARN RES, V97
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   van de Ven GM, 2019, Arxiv, DOI arXiv:1904.07734
   Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810
   MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419
   Murre JMJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120644
   Ostapenko O., 2019, PROC CVPR IEEE, P11321, DOI DOI 10.1109/CVPR.2019.01158
   Paszke A, 2019, ADV NEUR IN, V32
   Prabhu Ameya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P524, DOI 10.1007/978-3-030-58536-5_31
   Rajasegaran Jathushan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13585, DOI 10.1109/CVPR42600.2020.01360
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Riemer M, 2019, AAAI CONF ARTIF INTE, P1352
   Shim DS, 2021, AAAI CONF ARTIF INTE, V35, P9630
   Shin H, 2017, ADV NEUR IN, V30
   Smith L, 2005, ARTIF LIFE, V11, P13, DOI 10.1162/1064546053278973
   Wu C., 2018, NEURIPS, P5962
   Wu Y, 2019, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2019.00046
   Xiaoyu Tao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P254, DOI 10.1007/978-3-030-58529-7_16
NR 39
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 134
DI 10.1145/3573202
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700009
OA Bronze
DA 2024-07-18
ER

PT J
AU Yu, HC
   Huang, MQ
   Zhang, JJ
AF Yu, Hongchuan
   Huang, Mengqing
   Zhang, Jian Jun
TI Domain Adaptation Problem in Sketch Based Image Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Few-shot transfer learning; image retrieval; low rank decomposition;
   sketch-based retrieval
AB In this article, we present two algorithms that discover the discriminative structures of sketches, given pairs of sketches and photos in sketch-based image retrieval (SBIR) scenarios. Unlike the existing approaches, we aim at the few-shot and domain adaptation (DA) problems, and set up a module with canonical correlation analysis (CCA) technique in our algorithms to improve retrieval performance. For single source domain settings, our first algorithm can effectively transfer a classifier trained on a known dataset to a new one. For multisource settings, our second algorithm sophisticatedly combines multisource domain data to yield a classifier on the target domain. To the best of our knowledge, these two works are the first research in SBIR field. Experiments on the Sketchy and TU-Berlin sketch benchmark datasets demonstrate the effectiveness of our algorithms and compelling performance. Compared with the state-of-the-art methods, our algorithms do not use text based semantic information, but achieve competitive results. Furthermore, experiments also turn out that feature representation by available trained deep networks has distinct advantages and combination with traditional machine learning methods brings substantial improvements against the state-of-the-art methods on image retrievals. The complete source code of the proposed algorithms will be released on gitHub: [GitHub- ADAM0912/Domain-Adaptation-Problem-in-Sketch-Based-Image-Retrieval: The code repository for the articleDomain Adaptation Problem in Sketch Based Image Retrieval].
C1 [Yu, Hongchuan; Huang, Mengqing; Zhang, Jian Jun] Bournemouth Univ, Natl Ctr Comp Animat, Poole BH125BB, Dorset, England.
C3 Bournemouth University
RP Yu, HC (corresponding author), Bournemouth Univ, Natl Ctr Comp Animat, Poole BH125BB, Dorset, England.
EM hyu@bournemouth.ac.uk; mhuang@bournemouth.ac.uk;
   jzhang@bournemouth.ac.uk
OI Yu, Hongchuan/0000-0002-8508-7488; Zhang, Jian/0000-0002-7069-5771
CR Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dey S, 2019, PROC CVPR IEEE, P2174, DOI 10.1109/CVPR.2019.00228
   Dey S, 2018, INT C PATT RECOG, P916, DOI 10.1109/ICPR.2018.8545452
   Dong CAQ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P716
   Dutta A, 2020, INT J COMPUT VISION, V128, P2684, DOI 10.1007/s11263-020-01350-x
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Garcia V., 2017, arXiv
   google.com, ABOUT US
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu R, 2013, COMPUT VIS IMAGE UND, V117, P790, DOI 10.1016/j.cviu.2013.02.005
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Li JJ, 2022, IEEE T KNOWL DATA EN, V34, P5770, DOI 10.1109/TKDE.2021.3060473
   Li JJ, 2022, IEEE T PATTERN ANAL, V44, P8196, DOI 10.1109/TPAMI.2021.3109287
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li JJ, 2019, IEEE T NEUR NET LEAR, V30, P1381, DOI 10.1109/TNNLS.2018.2868854
   Li JJ, 2019, IEEE T CYBERNETICS, V49, P2144, DOI 10.1109/TCYB.2018.2820174
   Li YN, 2017, PROC CVPR IEEE, P5207, DOI 10.1109/CVPR.2017.553
   Li Yi, 2014, P BRIT MACHINE VISIO
   Lin HW, 2005, COMPUT MATH APPL, V50, P575, DOI 10.1016/j.camwa.2005.01.023
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lu P, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3418, DOI 10.1145/3474085.3475499
   Mancini M., 2020, P EUR C COMP VIS, P466, DOI DOI 10.1007/978-3-030-58592
   Mikolov Tomas, 2013, PROCEEDING INT C LEA
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Qin J, 2017, PROC CVPR IEEE, P6728, DOI 10.1109/CVPR.2017.712
   Saavedra J. M., 2015, BRIT MACH VIS C, P7
   Saavedra JM, 2014, IEEE IMAGE PROC, P2998, DOI 10.1109/ICIP.2014.7025606
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Shen YM, 2018, PROC CVPR IEEE, P3598, DOI 10.1109/CVPR.2018.00379
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JF, 2017, IEEE I CONF COMP VIS, P5552, DOI 10.1109/ICCV.2017.592
   Song YX, 2019, IEEE ACCESS, V7, P32393, DOI 10.1109/ACCESS.2019.2903534
   Su W., 2015, P INT C THEOR INF RE, P349, DOI 10.1145/2808194.2809481
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wang Z., 2021, P 13 INT JOINT C ART, P1143
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   Yelamarthi SK, 2018, LECT NOTES COMPUT SC, V11208, P316, DOI 10.1007/978-3-030-01225-0_19
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Zhang H, 2016, PROC CVPR IEEE, P1105, DOI 10.1109/CVPR.2016.125
   Zhang JY, 2018, LECT NOTES COMPUT SC, V11206, P304, DOI 10.1007/978-3-030-01216-8_19
NR 47
TC 0
Z9 0
U1 2
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 106
DI 10.1145/3565368
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300006
DA 2024-07-18
ER

PT J
AU Zhang, XY
   Gao, W
   Li, G
   Jiang, QP
   Cong, RM
AF Zhang, Xiaoyu
   Gao, Wei
   Li, Ge
   Jiang, Qiuping
   Cong, Runmin
TI Image Quality Assessment-driven Reinforcement Learning for Mixed
   Distorted Image Restoration
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image restoration; reinforcement learning; deep learning
AB Due to the diversity of the degradation process that is difficult to model, the recovery of mixed distorted images is still a challenging problem. The deep learning model trained under certain degradation declines significantly in other degradation situations. In this article, we explore ways to use a combination of tools to deal with the mixed distortion. First, we illustrate the limitations of a single deep network in dealing with multiple distortion types and then introduce a hierarchical toolkit with distinguished powerful tools. Second, we investigate how an efficient representation of images combined with a reinforcement learning (RL) paradigm helps to deal with tool noise in continuous restoration. The proposed method can accurately capture the distortion preferences for selecting the optimal recovery tools by RL agent. Finally, to fully utilize random tools for unknown distortion combinations, we adopt the exploration scheme with various quality evaluation methods to achieve more quality improvements. Experimental results demonstrate that the peak signal-to-noise ratio of the proposed method is 3.30 dB higher than other state-of-the-art RL-based methods on the CSIQ single distortion dataset and 0.95 dB higher on the DIV2K mixed distortion dataset.
C1 [Zhang, Xiaoyu; Gao, Wei; Li, Ge] Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen, Peoples R China.
   [Zhang, Xiaoyu; Gao, Wei] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Jiang, Qiuping] Ningbo Univ, Sch Informat Sci & Engn, Ningbo, Peoples R China.
   [Cong, Runmin] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing, Peoples R China.
C3 Peking University; Peng Cheng Laboratory; Ningbo University; Beijing
   Jiaotong University
RP Gao, W (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen, Peoples R China.; Gao, W (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
EM zxy2019@pku.edu.cn; gaowei262@pku.edu.cn; geli@pku.edu.cn;
   jiangqiuping@nbu.edu.cn; rmcong@bjtu.edu.cn
RI Zhang, xiaoyu/GXA-3206-2022; Jiang, Qiuping/AAL-8273-2020
OI Zhang, Xiaoyu/0000-0003-3458-8706; Qiuping, Jiang/0000-0002-6025-9343
FU Shenzhen Fundamental Research Program
   [GXWD20201231165807007-20200806163656003]; Shenzhen Science and
   Technology Plan Basic Research Project [JCYJ20190808161805519];
   Guangdong Basic and Applied Basic Research Foundation [2019A1515012031];
   Natural Science Foundation of China [61801303, 62031013]
FX This work was supported by Shenzhen Fundamental Research Program
   (GXWD20201231165807007-20200806163656003), Shenzhen Science and
   Technology Plan Basic Research Project (JCYJ20190808161805519),
   Guangdong Basic and Applied Basic Research Foundation (2019A1515012031),
   and Natural Science Foundation of China (61801303, 62031013).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 1989, LEARNING DELAYED REW
   Bertsekas D.P., 2018, Abstract Dynamic Programming
   Bianco Simone, 2021, PATTERN RECOGN LETT
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Buades A, 2011, IMAGE PROCESS ON LIN, V1, P208, DOI 10.5201/ipol.2011.bcm_nlm
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Furuta R, 2020, IEEE T MULTIMEDIA, V22, P1704, DOI 10.1109/TMM.2019.2960636
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   He JW, 2019, PROC CVPR IEEE, P11048, DOI 10.1109/CVPR.2019.01131
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim Y, 2015, IEEE IMAGE PROC, P1404, DOI 10.1109/ICIP.2015.7351031
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li MD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3341728
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu XH, 2012, IEEE IMAGE PROC, P665, DOI 10.1109/ICIP.2012.6466947
   Liu YW, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3425605
   Luo XF, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3381086
   McCaffrey James D., 2017, EPSILON GREEDY ALGOR
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Qian GC, 2021, Arxiv, DOI arXiv:1905.02538
   Suganuma M, 2019, PROC CVPR IEEE, P9031, DOI 10.1109/CVPR.2019.00925
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343
   Wang W, 2019, IEEE I CONF COMP VIS, P4139, DOI 10.1109/ICCV.2019.00424
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE IMAGE PROC, P477
   Wiener N., 1964, Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yang, 2021, ACM T SENSOR NETWORK, V17, P1
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   Yu K, 2018, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2018.00259
   Yu Ke, 2021, IEEE T PATTERN ANAL, V2021
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang XS, 2018, IEEE IMAGE PROC, P390, DOI 10.1109/ICIP.2018.8451694
NR 40
TC 1
Z9 1
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
AR 42
DI 10.1145/3532625
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800016
DA 2024-07-18
ER

PT J
AU Lu, FH
   Chen, H
   Li, K
   Deng, QL
   Zhao, J
   Zhang, KP
   Han, H
AF Lu Feihong
   Chen Hang
   Li Kang
   Deng Qiliang
   Zhao Jian
   Zhang Kaipeng
   Han Hong
TI Toward High-quality Face-Mask Occluded Restoration
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face-mask occluded dataset; face restoration; self-adaptive contextual
   attention; masked face recognition and verification
ID IMAGES; OBJECT
AB Face-mask occluded restoration aims at restoring the masked region of a human face, which has attracted increasing attention in the context of the COVID-19 pandemic. One major challenge of this task is the large visual variance of masks in the real world. To solve it we first construct a large-scale Face-mask Occluded Restoration (FMOR) dataset, which contains 5,500 unmasked images and 5,500 face-mask occluded images with various illuminations, and involves 1,100 subjects of different races, face orientations, and mask types. Moreover, we propose a Face-Mask Occluded Detection and Restoration (FMODR) framework, which can detect face-mask regions with large visual variations and restore them to realistic human faces. In particular, our FMODR contains a self-adaptive contextual attention module specifically designed for this task, which is able to exploit the contextual information and correlations of adjacent pixels for achieving high realism of the restored faces, which are however often neglected in existing contextual attention models. Our framework achieves state-of-the-art results of face restoration on three datasets, including CelebA, AR, and our FMOR datasets. Moreover, experimental results on AR and FMOR datasets demonstrate that our framework can significantly improve masked face recognition and verification performance.
C1 [Lu Feihong; Chen Hang; Li Kang; Deng Qiliang; Han Hong] Xidian Univ, 2 South Taibai Rd, Xian 710071, Shaanxi, Peoples R China.
   [Zhao Jian] Inst North Elect Equipment, 226 North Fourth Ring Rd Middle, Beijing 100000, Peoples R China.
   [Zhang Kaipeng] Univ Tokyo, 1,3 Fan,7-D Mu, Tokyo 1138654, Japan.
C3 Xidian University; University of Tokyo
RP Han, H (corresponding author), Xidian Univ, 2 South Taibai Rd, Xian 710071, Shaanxi, Peoples R China.
EM fhlu@stu.xidian.edu.cn; 1477227041@qq.com; 1989491568@qq.com;
   675213153@qq.com; zhaojian90@u.nus.edu; kp_zhang@foxmail.com;
   hanh@mail.xidian.edu.cn
RI zhao, jian/HTM-3920-2023
OI Zhao, Jian/0000-0002-3508-756X
CR [Anonymous], 2001, Schooling for Tomorrow
   Arya Sunil, 1998, P IEEE CGC WORKSHOP
   Banerjee S, 2020, Arxiv, DOI arXiv:2002.08448
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bornard R., 2002, P ACM INT C MULTIMED, P355
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chen CF, 2021, PROC CVPR IEEE, P11891, DOI 10.1109/CVPR46437.2021.01172
   Chen X, 2018, IEEE ACCESS, V6, P14567, DOI 10.1109/ACCESS.2018.2803787
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Ge SM, 2017, PROC CVPR IEEE, P426, DOI 10.1109/CVPR.2017.53
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gross R, 2005, HANDBOOK OF FACE RECOGNITION, P301, DOI 10.1007/0-387-27257-7_14
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo XF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14114, DOI 10.1109/ICCV48922.2021.01387
   He KM, 2012, LECT NOTES COMPUT SC, V7573, P16, DOI 10.1007/978-3-642-33709-3_2
   He XF, 2004, ADV NEUR IN, V16, P153
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiankang Deng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P741, DOI 10.1007/978-3-030-58621-8_43
   Lee J, 2012, IEEE T CONSUM ELECTR, V58, P553, DOI 10.1109/TCE.2012.6227460
   Lei Z, 2008, IEEE INT CONF AUTOMA, P140, DOI 10.1109/AFGR.2008.4813354
   Li Ang, 2019, P IJCNN, P1
   Li CY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3016, DOI 10.1145/3394171.3413960
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Liu C, 2001, PROC CVPR IEEE, P192
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sun Zhenan, 2016, P BTAS, P1
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
   Wang Q, 2006, J PATTERN RECOGNIT R, V1, P55, DOI 10.13176/11.15
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Wang Y, 2018, ADV NEUR IN, V31
   Wang ZY, 2020, Arxiv, DOI arXiv:2003.09093
   Wen Jian-Ke, 2013, HUM IMMUNOL, V16, P14
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xia WH, 2021, Arxiv, DOI arXiv:2104.08910
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yin BJ, 2019, IEEE I CONF COMP VIS, P9347, DOI 10.1109/ICCV.2019.00944
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu LY, 2022, IEEE T MULTIMEDIA, V24, P2950, DOI 10.1109/TMM.2021.3091863
   Yu T, 2020, AAAI CONF ARTIF INTE, V34, P12733
   Zhang X, 2021, APPL SOFT COMPUT, V111, DOI 10.1016/j.asoc.2021.107626
   Zhao F, 2018, IEEE T IMAGE PROCESS, V27, P778, DOI 10.1109/TIP.2017.2771408
   Zhao J, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4397
   Zhao J, 2017, ADV NEUR IN, V30
   Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235
   Zhou T., 2020, IEEECVF C COMPUTER V, P7680
   Zhou XZ, 2020, IEEE T AFFECT COMPUT, V11, P542, DOI 10.1109/TAFFC.2018.2828819
   Zhu Z, 2021, PROC CVPR IEEE, P10487, DOI 10.1109/CVPR46437.2021.01035
NR 58
TC 4
Z9 4
U1 9
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 24
DI 10.1145/3524137
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400024
DA 2024-07-18
ER

PT J
AU Dogariu, M
   Stefan, LD
   Boteanu, BA
   Lamba, C
   Kim, B
   Ionescu, B
AF Dogariu, Mihai
   Stefan, Liviu-Daniel
   Boteanu, Bogdan Andrei
   Lamba, Claudiu
   Kim, Bomi
   Ionescu, Bogdan
TI Generation of Realistic Synthetic Financial Time-series
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Time-series; synthetic financial data generation; financial data
   prediction; generative models
AB Financial markets have always been a point of interest for automated systems. Due to their complex nature, financial algorithms and fintech frameworks require vast amounts of data to accurately respond to market fluctuations. This data availability is tied to the daily market evolution, so it is impossible to accelerate its acquisition. In this article, we discuss several solutions for augmenting financial datasets via synthesizing realistic time-series with the help of generative models. This problem is complex, since financial time series present very specific properties, e.g., fat-tail distribution, cross-correlation between different stocks, specific autocorrelation, cluster volatility and so on. In particular, we propose solutions for capturing cross-correlations between different stocks and for transitioning from fixed to variable length time-series without resorting to sequence modeling networks, and adapt various network architectures, e.g., fully connected and convolutional GANs, variational autoencoders, and generative moment matching networks. Finally, we tackle the problem of evaluating the quality of synthetic financial time-series. We introduce qualitative and quantitative metrics, along with a portfolio trend prediction framework that validates our generative models' performance. We carry out experiments on real-world financial data extracted from the US stock market, proving the benefits of these techniques.
C1 [Dogariu, Mihai; Stefan, Liviu-Daniel; Boteanu, Bogdan Andrei; Ionescu, Bogdan] Univ Politehn Bucuresti, Bucharest, Romania.
   [Lamba, Claudiu; Kim, Bomi] Hana Inst Technol, Hana TI, Big Data & AI Lab, Seoul, South Korea.
C3 National University of Science & Technology POLITEHNICA Bucharest
RP Dogariu, M (corresponding author), Univ Politehn Bucuresti, Bucharest, Romania.
EM mihai.dogariu@upb.ro; liviu_daniel.stefan@upb.ro; bboteanu@imag.pub.ro;
   lambac@hanafn.com; bomik@tepper.cmu.edu; bogdan.ionescu@upb.ro
RI Ionescu, Bogdan/IWU-7778-2023; Dogariu, Mihai/HHN-6565-2022; Stefan,
   Liviu-Daniel/AAS-9561-2021; Dogariu, Mihai/HHN-6442-2022
OI Stefan, Liviu-Daniel/0000-0001-9174-3923; 
FU project AI4Media "A European Excellence Centre for Media, Society and
   Democracy", H2020 ICT-48-2020 [951911]
FX The work of Mihai Dogariu, Liviu-Daniel Stefan, and Bogdan Ionescu was
   partly funded under project AI4Media "A European Excellence Centre for
   Media, Society and Democracy," grant #951911, H2020 ICT-48-2020.
CR Afshar P, 2019, IEEE SIGNAL PROC MAG, V36, P132, DOI 10.1109/MSP.2019.2900993
   [Anonymous], 2018, ADV NEUR IN
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Baevski A., 2020, Adv. Neural Inf. Process. Syst., V33
   Bansal A, 2018, LECT NOTES COMPUT SC, V11205, P397, DOI 10.1007/978-3-030-01246-5_24
   BOLLERSLEV T, 1986, J ECONOMETRICS, V31, P307, DOI 10.1016/0304-4076(86)90063-1
   Chakraborti A, 2011, QUANT FINANC, V11, P991, DOI 10.1080/14697688.2010.539248
   Challet D, 1997, PHYSICA A, V246, P407, DOI 10.1016/S0378-4371(97)00419-6
   Chen S., 2018, P INT C LEARN REPR
   Cont R, 2001, QUANT FINANC, V1, P223, DOI [10.1080/713665670, 10.1088/1469-7688/1/2/304]
   Diederik P. K, 2014, Proceedings of the International Conference on Learning Representations (ICLR), V1
   Dogariu Mihai, 2021, P 29 EUR SIGN PROC C
   Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258
   ENGLE RF, 1982, ECONOMETRICA, V50, P987, DOI 10.2307/1912773
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Feng FL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5843
   Fu R., 2020, INT J MECH IND ENG, V14, P463
   Ghosh P, 2020, ARXIV PREPRINT ARXIV
   Gibson E, 2018, COMPUT METH PROG BIO, V158, P113, DOI 10.1016/j.cmpb.2018.01.025
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hao YP, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10113961
   Haodong Duan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P670, DOI 10.1007/978-3-030-58555-6_40
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hiransha M., 2018, Procedia Computer Science, V132, P1351, DOI 10.1016/j.procs.2018.05.050
   Hu ZN, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P261, DOI 10.1145/3159652.3159690
   Hussein N, 2019, PROC CVPR IEEE, P254, DOI 10.1109/CVPR.2019.00034
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Kakushadze Z., 2016, J RISK CONTROL, V3, P17, DOI [10.2139/ssrn.2802753, DOI 10.2139/SSRN.2802753]
   KANTOROVICH LV, 1960, MANAGE SCI, V6, P366, DOI 10.1287/mnsc.6.4.366
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim R., 2019, ARXIV PREPRINT ARXIV
   Kim Seokhwan, 2019, ARXIV191106394
   Kinlay Jonathan, 2011, CAN MACHINE LEARNING
   Koshiyama A, 2021, QUANT FINANC, V21, P797, DOI 10.1080/14697688.2020.1790635
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Li X., 2019, P 5 WORKSH NOIS US G, P34
   Li YJ, 2015, PR MACH LEARN RES, V37, P1718
   LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115
   Lior Sidi, 2020, ARXIV PREPRINT ARXIV
   Liu H., 2018, PROC INT C LEARN REP
   Lux T, 1999, NATURE, V397, P498, DOI 10.1038/17290
   MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095
   Miyato Takeru, 2018, P ICLR
   Otto P, 2018, SPAT STAT-NETH, V26, P125, DOI 10.1016/j.spasta.2018.07.005
   Pariente M, 2020, INT CONF ACOUST SPEE, P6364, DOI [10.1109/icassp40776.2020.9053038, 10.1109/ICASSP40776.2020.9053038]
   Paszke A, 2019, ADV NEUR IN, V32
   Pelka O., 2020, CEUR WORKSHOP P, V2696
   Radford A., 2015, ARXIV
   Ratto AP, 2018, 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P2090, DOI 10.1109/SSCI.2018.8628795
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Snyder D, 2019, INT CONF ACOUST SPEE, P5796, DOI 10.1109/ICASSP.2019.8683760
   Takahashi S, 2019, PHYSICA A, V527, DOI 10.1016/j.physa.2019.121261
   Theis L, 2016, INT C LEARN REPR ICL, P1
   Upadhyay Arun., 2012, Journal of Business Studies Quarterly, V3, P16
   Wellman MP, 2017, RSF-RUS SAGE J SOC S, V3, P104, DOI 10.7758/RSF.2017.3.1.06
   Wiese M, 2020, QUANT FINANC, V20, P1419, DOI 10.1080/14697688.2020.1730426
   Yoon J, 2019, ADV NEUR IN, V32
   Zhang Y., 2020, NEURAL INFORM PROCES
   Zhang YZ, 2017, PR MACH LEARN RES, V70
   Zhou XY, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/4907423
NR 66
TC 9
Z9 9
U1 8
U2 35
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 96
DI 10.1145/3501305
PG 27
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 0E1KA
UT WOS:000776441600007
OA Green Published
DA 2024-07-18
ER

PT J
AU Yang, DB
   Zhou, Y
   Shi, W
   Wu, DY
   Wang, WP
AF Yang, Dongbao
   Zhou, Yu
   Shi, Wei
   Wu, Dayan
   Wang, Weiping
TI RD-IOD: Two-Level Residual-Distillation-Based Triple-Network for
   Incremental Object Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Object detection; incremental learning; residual distillation
AB As a basic component in multimedia applications, object detectors are generally trained on a fixed set of classes that are pre-defined. However, new object classes often emerge after the models are trained in practice. Modern object detectors based on Convolutional Neural Networks (CNN) suffer from catastrophic forgetting when fine-tuning on new classes without the original training data. Therefore, it is critical to improve the incremental learning capability on object detection. In this article, we propose a novel Residual-Distillation-based Incremental learning method on Object Detection (RD-IOD). Our approach rests on the creation of a triple-network based on Faster R-CNN. To enable continuous learning from new classes. we use the original model as well as a residual model to guide the learning of the incremental model on new classes while maintaining the previous learned knowledge. To better maintain the discrimination between the features of old and new classes, the residual model is jointly trained with the incremental model on new classes in the incremental learning procedure. In addition, a two-level distillation scheme is designed to guide the training process, which consists of (1) a general distillation for imitating the original model in feature space along with a residual distillation on the features in both image level and instance level, and (2) a joint classification distillation on the output layers. To well preserve the learned knowledge, we design a 2-threshold training strategy to guide the learning of a Region Proposal Network and a detection head. Extensive experiments conducted on VOC2007 and COCO demonstrate that the proposed method can effectively learn to incrementally detect objects of new classes, and the problem of catastrophic forgetting is mitigated. Our code is available at https://github.cotn/yangdb/RD-IOD.
C1 [Yang, Dongbao; Zhou, Yu; Wu, Dayan; Wang, Weiping] Chinese Acad Sci, Inst Informat Engn, A 89,Minzhuang Rd, Beijing 100093, Peoples R China.
   [Yang, Dongbao] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
   [Shi, Wei] Carleton Univ, Sch Informat Technol, 1125 Colonel By Dr, Ottawa, ON, Canada.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Carleton University
RP Zhou, Y (corresponding author), Chinese Acad Sci, Inst Informat Engn, A 89,Minzhuang Rd, Beijing 100093, Peoples R China.
EM yangdongbao@iie.ac.cn; zhouyu@iie.ac.cn; wei.shi@carleton.ca;
   wudayan@iie.ac.cn; wangweiping@iie.ac.cn
RI wu, dayan/HHZ-8311-2022
OI Shi, Wei/0000-0002-3071-8350; ZHOU, Yu/0000-0003-4188-9953
FU Open Research Project of the State Key Laboratory of Media Convergence
   and Communication, Communication University of China, China
   [SKLMCC2020KF004]; Beijing Municipal Science & Technology Commission
   [Z191100007119002]; Key Research Program of Frontier Sciences, CAS
   [ZDBS-LY-7024]; National Natural Science Foundation of China [62006221]
FX This work was supported by the Open Research Project of the State Key
   Laboratory of Media Convergence and Communication, Communication
   University of China, China (No. SKLMCC2020KF004), the Beijing Municipal
   Science & Technology Commission (Z191100007119002), the Key Research
   Program of Frontier Sciences, CAS, Grant No. ZDBS-LY-7024, and the
   National Natural Science Foundation of China (No. 62006221).
CR Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9
   Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Cauwenberghs G, 2001, ADV NEUR IN, V13, P409
   Chen L., 2019, P 2019 INT JOINT C N, P1
   Chen YD, 2021, INT C PATT RECOG, P850, DOI 10.1109/ICPR48806.2021.9412258
   Dean J., 2015, NIPS DEEP LEARNING R
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goodfellow I. J., 2013, Proceedings of the Computer Science
   Hao Y, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P271, DOI 10.1145/3323873.3325033
   Hao Y, 2019, IEEE INT CON MULTI, P1, DOI 10.1109/ICME.2019.00009
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092
   Jung H, 2018, AAAI CONF ARTIF INTE, P3358
   Jung Heechul, 2016, ARXIV160700122
   Kang BY, 2019, IEEE I CONF COMP VIS, P8419, DOI 10.1109/ICCV.2019.00851
   Khosla Prannay, 2020, ADV NEURAL INFORM PR, V33, P18661
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuzborskij I, 2013, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2013.431
   Li DW, 2019, SEC'19: PROCEEDINGS OF THE 4TH ACM/IEEE SYMPOSIUM ON EDGE COMPUTING, P113, DOI 10.1145/3318216.3363317
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CB, 2020, IEEE T MULTIMEDIA, V22, P1785, DOI 10.1109/TMM.2019.2954747
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Luo DZ, 2020, AAAI CONF ARTIF INTE, V34, P11701
   McCloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI [10.1016/S0079-7421(08)60536-8, DOI 10.1016/S0079-7421(08)60536-8]
   Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83
   Perez-Rua Juan-Manuel, 2020, ARXIV PREPRINT ARXIV
   Polikar R, 2001, IEEE T SYST MAN CY C, V31, P497, DOI 10.1109/5326.983933
   Qin XG, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4350, DOI 10.1109/ICASSP39728.2021.9413821
   Qin Xugong, 2019, ICDAR, P559
   Rannen A, 2017, IEEE I CONF COMP VIS, P1329, DOI 10.1109/ICCV.2017.148
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368
   Sun G, 2018, AAAI CONF ARTIF INTE, P4107
   Sun G, 2019, IEEE T CYBERNETICS, V49, P3168, DOI 10.1109/TCYB.2018.2841046
   Wang XR, 2020, IEEE C EVOL COMPUTAT, DOI 10.1109/cec48606.2020.9185810
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P1316, DOI 10.1109/TMM.2020.2995290
   Yao Y, 2020, PROC CVPR IEEE, P6547, DOI 10.1109/CVPR42600.2020.00658
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zhang D., 2021, ARXIV PREPRINT ARXIV
   Zhang JT, 2020, IEEE WINT CONF APPL, P1120, DOI [10.1109/WACV45572.2020.9093365, 10.1109/wacv45572.2020.9093365]
   Zhang YF, 2021, INT C PATT RECOG, P8476, DOI 10.1109/ICPR48806.2021.9412301
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354
   Zhong BN, 2019, IEEE T IMAGE PROCESS, V28, P2331, DOI 10.1109/TIP.2018.2885238
   Zhou QQ, 2020, IEEE T IMAGE PROCESS, V29, P7578, DOI 10.1109/TIP.2020.3004267
   Zhou X., 2019, arXiv
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 54
TC 11
Z9 11
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 18
DI 10.1145/3472393
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900018
DA 2024-07-18
ER

PT J
AU He, ZL
   Li, HS
   Wang, Z
   Xia, ST
   Zhu, WW
AF He, Zhaoliang
   Li, Hongshan
   Wang, Zhi
   Xia, Shutao
   Zhu, Wenwu
TI Adaptive Compression for Online Computer Vision: An Edge Reinforcement
   Learning Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Edge computing; reinforcement learning; adaptive compression; machine
   learning service
ID JPEG; CLOUD
AB With the growth of computer vision-based applications, an explosive amount of images have been uploaded to cloud servers that host such online computer vision algorithms, usually in the form of deep learning models. JPEG has been used as the de facto compression and encapsulation method for images. However, standard JPEG configuration does not always perform well for compressing images that are to be processed by a deep learning model-for example, the standard quality level of JPEG leads to 50% of size overhead (compared with the best quality level selection) on ImageNet under the same inference accuracy in popular computer vision models (e.g., InceptionNet and ResNet). Knowing this, designing a better JPEG configuration for online computer vision-based services is still extremely challenging. First, cloud-based computer vision models are usually a black box to end-users; thus, it is challenging to design JPEG configuration without knowing their model structures. Second, the "optimal" JPEG configuration is not fixed; instead, it is determined by confounding factors, including the characteristics of the input images and the model, the expected accuracy and image size, and so forth. In this article, we propose a reinforcement learning (RL)-based adaptive JPEG configuration framework, AdaCompress. In particular, we design an edge (i.e., user-side) RL agent that learns the optimal compression quality level to achieve an expected inference accuracy and upload image size, only from the online inference results, without knowing details of the model structures. Furthermore, we design an explore-exploit mechanism to let the framework fast switch an agent when it detects a performance degradation, mainly due to the input change (e.g., images captured across daytime and night). Our evaluation experiments using real-world online computer vision-based APIs from Amazon Rekognition, Face++, and Baidu Vision show that our approach outperforms existing baselines by reducing the size of images by one-half to one-third while the overall classification accuracy only decreases slightly. Meanwhile, AdaCompress adaptively re-trains or re-loads the RL agent promptly to maintain the performance.
C1 [He, Zhaoliang; Zhu, Wenwu] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
   [He, Zhaoliang; Wang, Zhi] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Li, Hongshan] Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Shenzhen, Peoples R China.
   [Wang, Zhi; Xia, Shutao] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen, Peoples R China.
C3 Tsinghua University; Peng Cheng Laboratory; Tsinghua University;
   Tsinghua Shenzhen International Graduate School; Tsinghua University;
   Tsinghua Shenzhen International Graduate School
RP Wang, Z (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.; Wang, Z (corresponding author), Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen, Peoples R China.
EM hezl19@mails.tsinghua.edu.cn; lhs17@mails.tsinghua.edu.cn;
   wangzhi@sz.tsinghua.edu.cn; xiast@sz.tsinghua.edu.cn;
   wwzhu@tsinghua.edu.cn
RI Li, Hongshan/K-3086-2019
FU National Key Research and Development Program of China [2020AAA0106300];
   NSFC [61872215, 61771273]; Shenzhen Science and Technology Program
   [RCYX20200714114523079]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant No. 2020AAA0106300, NSFC under
   grant nos. 61872215 and 61771273, and Shenzhen Science and Technology
   Program (Grant No. RCYX20200714114523079).
CR [Anonymous], 2018, FLIR Thermal Dataset for Algorithm Training
   [Anonymous], 2019, AM REK
   [Anonymous], 2015, Mobile cloud visual media computing
   Anwar S, 2017, ACM J EMERG TECH COM, V13, DOI 10.1145/3005348
   Anwar S, 2015, INT CONF ACOUST SPEE, P1131, DOI 10.1109/ICASSP.2015.7178146
   Baidu, 2019, BAID AI OP PLATF
   Balle J, 2018, ICLR
   Baluja Shumeet, 2019, TECHNICAL DISCLOSURE
   Calore M., 2010, WIRED
   Chamain LD, 2019, IEEE INT CON MULTI, P338, DOI 10.1109/ICME.2019.00066
   Chao JS, 2013, IEEE IMAGE PROC, P1675, DOI 10.1109/ICIP.2013.6738345
   Chao JS, 2011, IEEE IMAGE PROC, P301, DOI 10.1109/ICIP.2011.6116299
   Delac K, 2005, LECT NOTES COMPUT SC, V3687, P136
   Dodge S, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Eshratifar AE, 2018, PR GR LAK SYMP VLSI, P111, DOI 10.1145/3194554.3194565
   Evtimov Ivan, 2018, CVPR
   Face++, 2019, FAC COGN SERV
   Flynn R., 2019, LOSSY IMAGE OPTIMIZA
   Ge WF, 2017, PROC CVPR IEEE, P10, DOI 10.1109/CVPR.2017.9
   Gong Y., 2014, INT C LEARN REPR ICL
   Google Inc, 2019, GOOGL EDG TPU
   Gueguen L, 2018, ADV NEUR IN, V31
   Han  S., 2015, ARXIV151000149
   Han S, 2016, Harvard Yenching Ins, V101, P123
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu PF, 2017, IEEE T IND INFORM, V13, P1910, DOI 10.1109/TII.2016.2607178
   Hu Y., 2015, MOBILE EDGE COMPUTIN, P1
   Huawei, 2019, HUAW ATL 500 EDG STA
   Huynh LN, 2017, MOBISYS'17: PROCEEDINGS OF THE 15TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P82, DOI 10.1145/3081333.3081360
   Hwang K, 2014, IEEE WRK SIG PRO SYS, P174
   I. DIS, 1991, CCITT RECOMMENDATION, V81, P6
   Image-Net.org, 2012, IMAGENET LARGE SCALE
   Kang YP, 2017, ACM SIGPLAN NOTICES, V52, P615, DOI 10.1145/3093336.3037698
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee J., 2019, P 7 INT C LEARN REP
   Li HS, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2440, DOI 10.1145/3343031.3350874
   Li HS, 2018, INT C PAR DISTRIB SY, P671, DOI [10.1109/ICPADS.2018.00092, 10.1109/PADSW.2018.8645013]
   Liu Z, 2018, DES AUT CON, DOI 10.1145/3195970.3196022
   Minnen D, 2018, ADV NEUR IN, V31
   Mnih V, 2013, ARXIV
   Ohm J.R., 2018, PICT COD S
   Python Imaging Library, 2019, IM FIL FORM
   Rabbani Majid., 2002, J ELECTRON IMAGING, V11
   Radford Alec, 2018, IMPROVING LANGUAGE U, DOI DOI 10.18653/V1/N18-1202
   Raspberry Pi, 2019, RASPB PI 4 4 MOD B
   Rippel O, 2017, PR MACH LEARN RES, V70
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sadeghi A, 2019, IEEE T COGN COMMUN, V5, P1024, DOI 10.1109/TCCN.2019.2936193
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Satyanarayanan M, 2017, COMPUTER, V50, P30, DOI 10.1109/MC.2017.9
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Simonyan K., 2014, 14091556 ARXIV
   Speedtest, 2019, SPEEDT GLOB IND INT
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Theis L., 2017, ICLR
   Toderici G., 2015, ARXIV PREPRINT ARXIV
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   Torfason R., 2018, P INT C LEARN REPR, P1
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang FX, 2020, IEEE INFOCOM SER, P2499, DOI [10.1109/infocom41043.2020.9155373, 10.1109/INFOCOM41043.2020.9155373]
   Yang Zhaohui, 2020, ACM MM, P1561
   Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zhong C, 2018, 2018 52ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS), DOI 10.1109/ciss.2018.8362276
NR 67
TC 5
Z9 5
U1 3
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 118
DI 10.1145/3447878
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800002
DA 2024-07-18
ER

PT J
AU Mehrabi, A
   Siekkinen, M
   Kämäräinen, T
   Ylä-Jääski, A
AF Mehrabi, Abbas
   Siekkinen, Matti
   Kamarainen, Teemu
   Yla-Jaaski, Antti
TI Multi-Tier CloudVR: Leveraging Edge Computing in Remote Rendered Virtual
   Reality
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-access edge computing (MEC); rendered virtual reality (VR); joint
   optimization; integer nonlinear programming (INLP); greedy algorithm
AB The availability of high bandwidth with low-latency communication in 5G mobile networks enables remote rendered real-time virtual reality (VR) applications. Remote rendering of VR graphics in a cloud removes the need for local personal computer for graphics rendering and augments weak graphics processing unit capacity of stand-alone VR headsets. However, to prevent the added network latency of remote rendering from ruining user experience, rendering a locally navigable viewport that is larger than the field of view of the HMD is necessary. The size of the viewport required depends on latency: Longer latency requires rendering a larger viewport and streaming more content. In this article, we aim to utilize multi-access edge computing to assist the backend cloud in such remote rendered interactive VR. Given the dependency between latency and amount and quality of the content streamed, our objective is to jointly optimize the tradeoff between average video quality and delivery latency. Formulating the problem as mixed integer nonlinear programming, we leverage the interpolation between client's field of view frame size and overall latency to convert the problem to integer nonlinear programming model and then design efficient online algorithms to solve it. The results of our simulations supplemented by real-world user data reveal that enabling a desired balance between video quality and latency, our algorithm particularly achieves the improvements of on average about 22% and 12% in term of video delivery latency and 8% in term of video quality compared to respectively order-of-arrival, threshold-based, and random-location strategies.
C1 [Mehrabi, Abbas] Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne NEI 8ST, Tyne & Wear, England.
   [Siekkinen, Matti; Yla-Jaaski, Antti] Aalto Univ, Dept Comp Sci, Espoo 02150, Finland.
   [Kamarainen, Teemu] Univ Helsinki, Dept Comp Sci, Yliopistonkatu 4, Helsinki 00100, Finland.
C3 Northumbria University; Aalto University; University of Helsinki
RP Mehrabi, A (corresponding author), Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne NEI 8ST, Tyne & Wear, England.
EM abbas.mehrabidavoodabadi@northumbria.ac.uk; matti.siekkinen@aalto.fi;
   teemu.kamarainen@helsinki.fi; antti.ylajaaski@aalto.fi
RI Siekkinen, Matti/H-2447-2018; Kämäräinen, Teemu/GLU-3738-2022;
   Yla-Jaaski, Antti/G-2484-2013
OI Kamarainen, Teemu/0000-0003-4685-6763; Mehrabi,
   Abbas/0000-0002-8758-0882; Yla-Jaaski, Antti/0000-0002-2069-1721
CR Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Boos K., 2016, MOBISYS, P291
   Choy S, 2014, MULTIMEDIA SYST, V20, P503, DOI 10.1007/s00530-014-0367-z
   Choy S, 2012, ANN WORK NETW
   Cofano G, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P24, DOI 10.1145/2910017.2910597
   Corbillon X, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P943, DOI 10.1145/3123266.3123372
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Elbamby MS, 2018, IEEE NETWORK, V32, P78, DOI 10.1109/MNET.2018.1700268
   Erol-Kantarci M, 2018, L N INST COMP SCI SO, V223, P169, DOI 10.1007/978-3-319-74439-1_15
   Hu Y., 2015, MOBILE EDGE COMPUTIN, P1
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Kämäräinen T, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1181, DOI 10.1145/3240508.3240620
   Kua J, 2017, IEEE COMMUN SURV TUT, V19, P1842, DOI 10.1109/COMST.2017.2685630
   Lai ZQ, 2017, PROCEEDINGS OF THE 23RD ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM '17), P409, DOI 10.1145/3117811.3117815
   Li Z., 2016, Visual Communications and Image Processing (VCIP), 2016, P1
   Mehrabi A, 2019, I S WORLD WIREL MOBI, DOI 10.1109/wowmom.2019.8792981
   Mehrabi A, 2019, IEEE T MOBILE COMPUT, V18, P787, DOI 10.1109/TMC.2018.2850026
   Mehrabi A, 2018, IEEE ACCESS, V6, P52261, DOI 10.1109/ACCESS.2018.2870855
   OculusRift-Blog.com, J CARMACKS DELIVERS
   Petrangeli S, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3165266
   Petrangeli S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818361
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Shi S, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P222, DOI 10.1145/3304109.3306217
   Shi S, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2719921
   Shi Shu., 2019, Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services, P130
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Tran TX, 2019, IEEE T MOBILE COMPUT, V18, P1965, DOI 10.1109/TMC.2018.2871147
   Wang C, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P1, DOI 10.1145/2910017.2910593
   Wang DS, 2019, IEEE T SERV COMPUT, V12, P685, DOI 10.1109/TSC.2018.2828426
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Xie J. C., 2017, Int. Geol. Rev., P1
NR 35
TC 15
Z9 16
U1 2
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 49
DI 10.1145/3429441
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000008
OA Green Accepted, Bronze
DA 2024-07-18
ER

PT J
AU Xu, XL
   Fang, ZJ
   Qi, LY
   Zhang, XY
   He, Q
   Zhou, XK
AF Xu, Xiaolong
   Fang, Zijie
   Qi, Lianyong
   Zhang, Xuyun
   He, Qiang
   Zhou, Xiaokang
TI TripRes: Traffic Flow Prediction Driven Resource Reservation for
   Multimedia IoV with Edge Computing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Resource reservation; edge computing; multimedia IoV; traffic flow
   prediction; residual networks
ID INTERNET; THINGS
AB The Internet of Vehicles (IoV) connects vehicles, roadside units (RSUs) and other intelligent objects, enabling data sharing among them, thereby improving the efficiency of urban traffic and safety. Currently, collections of multimedia content, generated by multimedia surveillance equipment, vehicles, and so on, are transmitted to edge servers for implementation, because edge computing is a formidable paradigm for accommodating multimedia services with low-latency resource provisioning. However, the uneven or discrete distribution of the traffic flow covered by edge servers negatively affects the service performance (e.g., overload and underload) of edge servers inmultimedia IoV systems. Therefore, howto accurately schedule and dynamically reserve proper numbers of resources for multimedia services in edge servers is still challenging. To address this challenge, a traffic flowprediction driven resource reservation method, called TripRes, is developed in this article. Specifically, the city map is divided into different regions, and the edge servers in a region are treated as a "big edge server" to simplify the complex distribution of edge servers. Then, future traffic flows are predicted using the deep spatiotemporal residual network (ST-ResNet), and future traffic flows are used to estimate the amount of multimedia services each region needs to offload to the edge servers. With the number of services to be offloaded in each region, their offloading destinations are determined through latency-sensitive transmission path selection. Finally, the performance of TripRes is evaluated using real-world big data with over 100M multimedia surveillance records from RSUs in Nanjing China.
C1 [Xu, Xiaolong; Fang, Zijie] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing, Peoples R China.
   [Xu, Xiaolong] Nanjing Univ Informat Sci & Technol, Jiangsu Engn Ctr Network Monitoring, Nanjing, Peoples R China.
   [Xu, Xiaolong] Minist Educ, Engn Res Ctr Digital Forens, Nanjing, Peoples R China.
   [Xu, Xiaolong] Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing, Peoples R China.
   [Qi, Lianyong] Qufu Normal Univ, Sch Informat Sci & Engn, Qufu, Shandong, Peoples R China.
   [Zhang, Xuyun] Macquarie Univ, Dept Comp, Sydney, NSW, Australia.
   [He, Qiang] Swinburne Univ Technol, Dept Comp Sci & Software Engn, Melbourne, Vic, Australia.
   [Zhou, Xiaokang] Shiga Univ, Fac Data Sci, Hikone, Japan.
   [Zhou, Xiaokang] RIKEN, Ctr Adv Intelligence Project, Tokyo, Japan.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; Nanjing University of
   Information Science & Technology; Qufu Normal University; Macquarie
   University; Swinburne University of Technology; Shiga University; RIKEN
RP Qi, LY (corresponding author), Qufu Normal Univ, Sch Informat Sci & Engn, Qufu, Shandong, Peoples R China.
EM njuxlxu@gmail.com; vison307@gmail.com; lianyongqi@gmail.com;
   xuyun.zhang@mq.edu.au; qhe@swin.edu.au; zhou@biwako.shiga-u.ac.jp
RI Xu, Xiaolong/U-2547-2019; zhang, xu/GYE-3558-2022; ZHANG,
   XUCHEN/KBB-7989-2024; Bennis, Mehdi/ABE-5838-2020; Qi,
   Lianyong/AAO-2681-2020
OI Xu, Xiaolong/0000-0003-4879-9803; Bennis, Mehdi/0000-0003-0261-0171;
   Zhang, Xuyun/0000-0001-7353-4159
FU Financial and Science Technology Plan Project of Xinjiang Production and
   Construction Corps [2020DB005]; National Natural Science Foundation of
   China [61702277, 61872219]; Priority Academic Program Development of
   Jiangsu Higher Education Institutions (PAPD) fund; Jiangsu Collaborative
   Innovation Center on Atmospheric Environment and Equipment Technology
   (CICAEET)
FX This research is supported by the Financial and Science Technology Plan
   Project of Xinjiang Production and Construction Corps under grant no.
   2020DB005, the National Natural Science Foundation of China under grant
   no. 61702277 and no. 61872219, the Priority Academic Program Development
   of Jiangsu Higher Education Institutions (PAPD) fund, and Jiangsu
   Collaborative Innovation Center on Atmospheric Environment and Equipment
   Technology (CICAEET).
CR Abadi A, 2015, IEEE T INTELL TRANSP, V16, P653, DOI 10.1109/TITS.2014.2337238
   Amjad A, 2017, 2017 SECOND INTERNATIONAL CONFERENCE ON FOG AND MOBILE EDGE COMPUTING (FMEC), P194, DOI 10.1109/FMEC.2017.7946430
   Bilal K, 2017, 2017 SECOND INTERNATIONAL CONFERENCE ON FOG AND MOBILE EDGE COMPUTING (FMEC), P68, DOI 10.1109/FMEC.2017.7946410
   Chen WH, 2018, FUTURE GENER COMP SY, V89, P78, DOI 10.1016/j.future.2018.06.021
   Chih-Lin Hu, 2017, 2017 International Conference on Applied System Innovation (ICASI). Proceedings, P610, DOI 10.1109/ICASI.2017.7988497
   Contreras-Castillo J, 2018, IEEE INTERNET THINGS, V5, P3701, DOI 10.1109/JIOT.2017.2690902
   Du BW, 2020, IEEE T INTELL TRANSP, V21, P972, DOI 10.1109/TITS.2019.2900481
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7435, DOI 10.1109/CVPR.2017.786
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ibáñez JAG, 2015, IEEE WIREL COMMUN, V22, P122
   Jia CM, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Jo B, 2019, CMC-COMPUT MATER CON, V61, P439, DOI 10.32604/cmc.2019.08194
   Kong FH, 2019, FUTURE GENER COMP SY, V93, P460, DOI 10.1016/j.future.2018.10.052
   Li H, 2018, IEEE NETWORK, V32, P96, DOI 10.1109/MNET.2018.1700202
   Lin K, 2019, FUTURE GENER COMP SY, V94, P610, DOI 10.1016/j.future.2018.12.045
   Lv YS, 2015, IEEE T INTELL TRANSP, V16, P865, DOI 10.1109/TITS.2014.2345663
   Ngoko Y, 2017, 2017 IEEE 1ST INTERNATIONAL CONFERENCE ON EDGE COMPUTING (IEEE EDGE), P240, DOI 10.1109/IEEE.EDGE.2017.44
   Ning Y., 2018, P INT JOINT C NEUR N, P1
   Ning Z., 2018, IEEE INTERNET THINGS, V5, P2506, DOI DOI 10.1109/JIOT.2017.2764259
   Peng H, 2020, INFORM SCIENCES, V521, P277, DOI 10.1016/j.ins.2020.01.043
   Qi LY, 2019, IEEE T COMPUT SOC SY, V6, P1063, DOI 10.1109/TCSS.2019.2906925
   Ren P, 2018, 2018 THIRD IEEE/ACM SYMPOSIUM ON EDGE COMPUTING (SEC), P349, DOI 10.1109/SEC.2018.00041
   Satyanarayanan M, 2017, COMPUTER, V50, P30, DOI 10.1109/MC.2017.9
   Shi WS, 2016, IEEE INTERNET THINGS, V3, P637, DOI 10.1109/JIOT.2016.2579198
   Shin HK, 2019, CMC-COMPUT MATER CON, V61, P911, DOI 10.32604/cmc.2019.08269
   Wan SH, 2020, J SUPERCOMPUT, V76, P2518, DOI 10.1007/s11227-019-03011-4
   Wang SG, 2019, J PARALLEL DISTR COM, V127, P160, DOI 10.1016/j.jpdc.2018.06.008
   Warabino T., 2005, 2005 IEEE 16th International Symposium on Personal, Indoor and Mobile Radio Communications (IEEE Cat. No. 05TH8889), P795
   Wei YF, 2019, CMC-COMPUT MATER CON, V59, P89, DOI 10.32604/cmc.2019.04836
   Xiao ZY, 2018, IEEE INT CONF MULTI
   Xing M, 2016, IEEE T VEH TECHNOL, V65, P2649, DOI 10.1109/TVT.2015.2424372
   Xu XL, 2019, FUTURE GENER COMP SY, V96, P89, DOI 10.1016/j.future.2019.01.012
   Zhang J, 2017, PROCEEDINGS OF THE ASME 11TH INTERNATIONAL CONFERENCE ON ENERGY SUSTAINABILITY, 2017
NR 33
TC 59
Z9 59
U1 6
U2 39
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 41
DI 10.1145/3401979
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000005
DA 2024-07-18
ER

PT J
AU Yang, X
   Song, XM
   Feng, FL
   Wen, HK
   Duan, LY
   Nie, LQ
AF Yang, Xin
   Song, Xuemeng
   Feng, Fuli
   Wen, Haokun
   Duan, Ling-Yu
   Nie, Liqiang
TI Attribute-wise Explainable Fashion Compatibility Modeling
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fashion analysis; explainable compatibility modeling; attribute-wise
   learning
AB With the boom of the fashion market and people's daily needs for beauty, clothing matching has gained increased research attention. In a sense, tackling this problem lies in modeling the human notions of the compatibility between fashion items, i.e., Fashion Compatibility Modeling (FCM), which plays an important role in a wide bunch of commercial applications, including clothing recommendation and dressing assistant. Recent advances in multimedia processing have shown remarkable effectiveness in accurate compatibility evaluation. However, these studies work like a black box and cannot provide appropriate explanations, which are indeed of importance for gaining users' trust and improving their experience. In fact, fashion experts usually explain the compatibility evaluation through the matching patterns between fashion attributes (e.g., a silk tank top cannot go with a knit dress). Inspired by this, we devise an attribute-wise explainable FCM solution, named ExFCM, which can simultaneously generate the item-level compatibility evaluation for input fashion items and the attribute-level explanations for the evaluation result. In particular, ExFCM consists of two key components: attribute-wise representation learning and attribute interaction modeling. The former works on learning the region-aware attribute representation for each item with the threshold global average pooling. Besides, the latter is responsible for compiling the attribute-level matching signals into the overall compatibility evaluation adaptively with the attentive interaction mechanism. Note that ExFCM is trained without any attribute-level compatibility annotations, which facilitates its practical applications. Extensive experiments on two real-world datasets validate that ExFCM can generate more accurate compatibility evaluations than the existing methods, together with reasonable explanations.
C1 [Yang, Xin; Song, Xuemeng; Wen, Haokun; Nie, Liqiang] Shandong Univ, 72 Binhai Rd, Qingdao 266237, Peoples R China.
   [Feng, Fuli] Natl Univ Singapore, Singapore, Singapore.
   [Duan, Ling-Yu] Peking Univ, Beijing, Peoples R China.
C3 Shandong University; National University of Singapore; Peking University
RP Song, XM; Nie, LQ (corresponding author), Shandong Univ, 72 Binhai Rd, Qingdao 266237, Peoples R China.
EM joeyangbuer@gmail.com; sxmustc@gmail.com; fulifeng93@gmail.com;
   whenhaokun@gmail.com; lingyu@pku.edu.cn; nieliqiang@gmail.com
RI Wen, Haokun/GRO-6431-2022
FU National Key Research and Development Project of New Generation
   Artificial Intelligence [2018AAA0102502]; National Natural Science
   Foundation of China [61772310, 61702300, U1936203]; Shandong Provincial
   Natural Science Foundation [ZR2019JQ23]; Shandong Provincial Key
   Research and Development Program [2019JZZY010118]; Innovation Teams in
   Colleges and Universities in Jinan [2018GXRC014]
FX This work is supported by the National Key Research and Development
   Project of New Generation Artificial Intelligence, No.:2018AAA0102502;
   the National Natural Science Foundation of China, No.:61772310,
   No.:61702300, and No.:U1936203; the Shandong Provincial Natural Science
   Foundation, No.:ZR2019JQ23; the Shandong Provincial Key Research and
   Development Program, No.:2019JZZY010118; the Innovation Teams in
   Colleges and Universities in Jinan, No.:2018GXRC014.
CR Ak KE, 2018, PROC CVPR IEEE, P7708, DOI 10.1109/CVPR.2018.00804
   [Anonymous], 2016, P C N AM CHAPT ASS C, DOI DOI 10.18653/V1/N16-1170
   [Anonymous], 2012, P ACM INT C MULT
   [Anonymous], 2016, P 24 ACM INT C MULT
   Cao D, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P585, DOI 10.1145/3077136.3080779
   Chen C, 2017, PROCEEDINGS OF THE ASME POWER CONFERENCE JOINT WITH ICOPE-17, 2017, VOL 2
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Du CX, 2019, AAAI CONF ARTIF INTE, P6359
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   He XN, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P549, DOI 10.1145/2911451.2911489
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Hu ZT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2410, DOI 10.18653/v1/p16-1228
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Jiang Junxiao, 2018, P ACM INT C MULT RET, P143
   Khachatrian Hrant, 2018, P INT C LEARN REPR
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liao LZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1571, DOI 10.1145/3240508.3240646
   Lin Min, 2014, PROC INT CONF RECON
   Lin YJ, 2020, IEEE T KNOWL DATA EN, V32, P1502, DOI 10.1109/TKDE.2019.2906190
   Liu JH, 2019, NEUROCOMPUTING, V359, P249, DOI 10.1016/j.neucom.2019.05.081
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lu YJ, 2015, IEEE T MULTIMEDIA, V17, P1213, DOI 10.1109/TMM.2015.2438712
   Ma L, 2017, IEEE T MULTIMEDIA, V19, P2545, DOI 10.1109/TMM.2017.2703089
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Rendle Steffen, 2010, P 3 ACM INT C WEB SE, P81, DOI DOI 10.1145/1718487.1718498
   Song SJ, 2018, IEEE MULTIMEDIA, V25, P102, DOI 10.1109/MMUL.2018.2875860
   Song XM, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P320, DOI 10.1145/3343031.3350956
   Song XM, 2018, ACM/SIGIR PROCEEDINGS 2018, P5, DOI 10.1145/3209978.3209996
   Song XM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P753, DOI 10.1145/3123266.3123314
   Sun GL, 2018, MULTIMED TOOLS APPL, V77, P17731, DOI 10.1007/s11042-017-5245-1
   Tangseng Pongsate, 2020, P WINT C APPL COMP V, P2153
   Tintarev N, 2007, I C DATA ENGIN WORKS, P801, DOI 10.1109/ICDEW.2007.4401070
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wang X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P329, DOI 10.1145/3343031.3350909
   Wei-Lin Hsiao, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P7161, DOI 10.1109/CVPR.2018.00748
   Wu Y, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P496, DOI 10.18653/v1/P17-1046
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
   Zhang Hanwang, 2013, P ACM INT C MULT
   Zhang Y, 2018, EXPLAINABLE RECOMMEN
   Zhang YM, 2020, IEEE ACM T NETWORK, V28, P1227, DOI 10.1109/TNET.2020.2979807
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 49
TC 16
Z9 17
U1 4
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 36
DI 10.1145/3425636
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200016
DA 2024-07-18
ER

PT J
AU He, JL
   Yang, GB
   Liu, X
   Ding, XL
AF He, Jiale
   Yang, Gaobo
   Liu, Xin
   Ding, Xiangling
TI Spatio-temporal Saliency-based Motion Vector Refinement for Frame Rate
   Up-conversion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Frame rate up-conversion; video-saliency-based hierarchical motion
   vector refinement; interpolation post-processing; forensics;
   anti-forensics
AB A spatio-temporal saliency-based frame rate up-conversion (FRUC) approach is proposed, which achieves better quality of interpolated frames and invalidates existing texture variation-based FRUC detectors. A spatio-temporal saliency model is designed to select salient frames. After obtaining initial motion vector field by texture- and color-based bilateral motion estimation, two motion vector refining (MVR) schemes are adopted for high and low saliency frames to hierarchically refine the motion vectors, respectively. To produce high-quality interpolated frames, image enhancement are performed for salient frames after frame interpolation. Due to distinct MVR schemes, there are different degrees of texture information in interpolated frames. Some edge and texture information is supplemented into salient frames as post-processing, which can invalidate existing texture variation-based FRUC detectors. Experimental results show that the proposed approach outperforms state-of-the-art works in both objective and subjective qualities of interpolated frames, and achieves the purpose of FRUC anti-forensics.
C1 [He, Jiale; Yang, Gaobo] Hunan Univ, 2 Lushangnan Rd, Changsha 410082, Peoples R China.
   [Liu, Xin] Hunan Appl Technol Univ, Yuxia Rd, Changde, Peoples R China.
   [Ding, Xiangling] Hunan Univ Sci & Technol, Xiangtan, Peoples R China.
C3 Hunan University; Hunan University of Science & Technology
RP He, JL (corresponding author), Hunan Univ, 2 Lushangnan Rd, Changsha 410082, Peoples R China.
EM sophia_hjl@hnu.edu.cn; yanggaobo@hnu.edu.cn; Lusano_710@163.com;
   xianglingding@163.com
OI HE, JIALE/0000-0003-2186-1793; ding, xiangling/0000-0002-6581-4633;
   Yang, Gaobo/0000-0003-2734-659X
FU National Key R&D Program of China [2018YFB1003205]; National Natural
   Science Foundation of China [61572183, 61972143]
FX This work is supported in part by the National Key R&D Program of China
   (No.2018YFB1003205) and the National Natural Science Foundation of China
   (61572183, 61972143).
CR Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487
   Bao WB, 2018, IEEE T IMAGE PROCESS, V27, P3813, DOI 10.1109/TIP.2018.2825100
   Bayer B. E., 1976, U.S. patent, Patent No. 3,971,065
   Benois-Pineau J, 2002, J VIS COMMUN IMAGE R, V13, P363, DOI 10.1006/jvci.2001.0490
   Boyle, 2014, CENGAGE LEARNING
   Choi BD, 2007, IEEE T CIRC SYST VID, V17, P407, DOI 10.1109/TCSVT.2007.893835
   Choi D, 2016, IEEE T CIRC SYST VID, V26, P1789, DOI 10.1109/TCSVT.2015.2473275
   Ding XL, 2019, IEEE T CIRC SYST VID, V29, P1893, DOI 10.1109/TCSVT.2018.2852799
   Ding XL, 2019, MULTIMED TOOLS APPL, V78, P7453, DOI 10.1007/s11042-018-6504-5
   Ding X, 2018, IEEE T CIRC SYST VID, V28, P1497, DOI 10.1109/TCSVT.2017.2676162
   Gao D, 2008, J VISION, V8, DOI 10.1167/8.7.13
   Guo Y, 2016, J DISP TECHNOL, V12, P89, DOI 10.1109/JDT.2015.2466104
   He JL, 2020, J REAL-TIME IMAGE PR, V17, P259, DOI 10.1007/s11554-018-0767-y
   Jacobson N, 2012, IEEE T IMAGE PROCESS, V21, P2198, DOI 10.1109/TIP.2011.2179055
   Jacobson N, 2010, IEEE INT CON MULTI, P778, DOI 10.1109/ICME.2010.5582574
   Jeong SG, 2013, IEEE T IMAGE PROCESS, V22, P4497, DOI 10.1109/TIP.2013.2274731
   Jeong SG, 2012, IEEE IMAGE PROC, P845, DOI 10.1109/ICIP.2012.6466992
   Kang SJ, 2007, IEEE T CONSUM ELECTR, V53, P1759, DOI 10.1109/TCE.2007.4429281
   Kang SJ, 2010, IEEE T CIRC SYST VID, V20, P1909, DOI 10.1109/TCSVT.2010.2087832
   Kang XG, 2016, MULTIMED TOOLS APPL, V75, P13833, DOI 10.1007/s11042-015-2762-7
   Kim US, 2014, IEEE T CIRC SYST VID, V24, P384, DOI 10.1109/TCSVT.2013.2278142
   Lee WH, 2014, IEEE T IMAGE PROCESS, V23, P399, DOI 10.1109/TIP.2013.2288139
   Li R, 2018, IEEE ACCESS, V6, P1905, DOI 10.1109/ACCESS.2017.2780822
   Li R, 2017, SIGNAL PROCESS-IMAGE, V54, P36, DOI 10.1016/j.image.2017.02.010
   Li R, 2016, IEICE T INF SYST, VE99D, P208, DOI 10.1587/transinf.2015EDP7027
   Li Y., 2017, SCI REP, V7, P2017, DOI DOI 10.1155/2017/8248142
   Li YL, 2019, FUTURE INTERNET, V11, DOI 10.3390/fi11020026
   Li Y, 2017, IEEE T BROADCAST, V63, P535, DOI 10.1109/TBC.2017.2704423
   Liu HB, 2012, IEEE T CIRC SYST VID, V22, P1188, DOI 10.1109/TCSVT.2012.2197081
   ORCHARD MT, 1994, IEEE T IMAGE PROCESS, V3, P693, DOI 10.1109/83.334974
   Qu AX, 2016, INT CONF ACOUST SPEE, P1686, DOI 10.1109/ICASSP.2016.7471964
   Stamm MC, 2012, IEEE T INF FOREN SEC, V7, P1315, DOI 10.1109/TIFS.2012.2205568
   Tsai TH, 2016, IEEE T BROADCAST, V62, P426, DOI 10.1109/TBC.2016.2550764
   Tsai TH, 2012, J DISP TECHNOL, V8, P341, DOI 10.1109/JDT.2012.2186555
   Wang C, 2010, IEEE T CIRC SYST VID, V20, P886, DOI 10.1109/TCSVT.2010.2046057
   Xia M, 2017, MULTIMED TOOLS APPL, V76, P8399, DOI 10.1007/s11042-016-3468-1
   Yao YX, 2016, J INF SECUR APPL, V26, P39, DOI 10.1016/j.jisa.2015.12.001
   Yoo DG, 2013, J DISP TECHNOL, V9, P840, DOI 10.1109/JDT.2013.2263374
   Yoon SJ, 2018, IEEE T IMAGE PROCESS, V27, P5918, DOI 10.1109/TIP.2018.2861567
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang HL, 2017, IET COMPUT VIS, V11, P379, DOI 10.1049/iet-cvi.2016.0492
   Zhang LB, 2018, J VIS COMMUN IMAGE R, V51, P56, DOI 10.1016/j.jvcir.2018.01.001
   Zhang Y, 2010, MATER RES SOC SYMP P, V1246, DOI 10.1557/PROC-1246-B04-02
NR 43
TC 10
Z9 10
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 55
DI 10.1145/3382506
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600016
DA 2024-07-18
ER

PT J
AU Wang, QY
   Zhou, Y
   Ding, WP
   Zhang, ZG
   Muhammad, K
   Cao, ZH
AF Wang, Qingyong
   Zhou, Yun
   Ding, Weiping
   Zhang, Zhiguo
   Muhammad, Khan
   Cao, Zehong
TI Random Forest with Self-Paced Bootstrap Learning in Lung Cancer
   Prognosis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Lung cancer; random forest; self-paced learning; bootstrap;
   classification
ID ENSEMBLE; CLASSIFICATION
AB Training gene expression data with supervised learning approaches can provide an alarm sign for early treatment of lung cancer to decrease death rates. However, the samples of gene features involve lots of noises in a realistic environment. In this study, we present a random forest with self-paced learning bootstrap for improvement of lung cancer classification and prognosis based on gene expression data. To be specific, we propose an ensemble learning with random forest approach to improving the model classification performance by selecting multi-classifiers. Then, we investigate the sampling strategy by gradually embedding from high- to low-quality samples by self-paced learning. The experimental results based on five public lung cancer datasets show that our proposed method could select significant genes exactly, which improves classification performance compared to that of existing approaches. We believe that our proposed method has the potential to assist doctors in gene selections and lung cancer prognosis.
C1 [Wang, Qingyong; Zhou, Yun] Natl Univ Def Technol, Sci & Technol Informat Syst Engn Lab, Changsha, Peoples R China.
   [Ding, Weiping] Nantong Univ, Sch Informat Sci & Technol, Nantong, Peoples R China.
   [Zhang, Zhiguo] Shenzhen Univ, Hlth Sci Ctr, Sch Biomed Engn, Shenzhen, Peoples R China.
   [Muhammad, Khan] Sejong Univ, Dept Digital Contents, Seoul, South Korea.
   [Cao, Zehong] Univ Tasmania, Coll Sci & Engn, Sch Technol Environm & Design, Discipline ICT, Hobart, Tas, Australia.
C3 National University of Defense Technology - China; Nantong University;
   Shenzhen University; Sejong University; University of Tasmania
RP Ding, WP (corresponding author), Nantong Univ, Sch Informat Sci & Technol, Nantong, Peoples R China.; Cao, ZH (corresponding author), Univ Tasmania, Coll Sci & Engn, Sch Technol Environm & Design, Discipline ICT, Hobart, Tas, Australia.
EM dwp9988@163.com; zehong.cao@utas.edu.au
RI Muhammad, Khan/L-9059-2016; Cao, Zehong/O-7351-2016; Zhang,
   Zhiguo/JGE-3336-2023; Khan, Muhammad/IXN-8470-2023; Zhou,
   Yun/AAD-9531-2022
OI Muhammad, Khan/0000-0003-4055-7412; Cao, Zehong/0000-0003-3656-0328;
   Zhang, Zhiguo/0000-0001-7992-7965; Zhou, Yun/0000-0001-7328-0275; Wang,
   Qingyong/0000-0001-5115-6425; Muhammad, Khan/0000-0002-5302-1150
FU National Natural Science Foundation of China [61703416, 61976120];
   Natural Science Foundation of Hunan Province, China [2018JJ3614];
   Postgraduate Research Innovation Project fromHunan Provincial Department
   of Education [CX20190040]; Natural Science Foundation of Jiangsu
   Province [BK20191445]; Six Talent Peaks Project of Jiangsu Province
   [XYDXXJS-048]; Qing Lan Project of Jiangsu Province
FX This work was supported by the National Natural Science Foundation of
   China (grant no. 61703416 and no. 61976120), the Natural Science
   Foundation of Hunan Province, China (grant no. 2018JJ3614), the
   Postgraduate Research Innovation Project fromHunan Provincial Department
   of Education (grant no. CX20190040), the Natural Science Foundation of
   Jiangsu Province (grant no. BK20191445), the Six Talent Peaks Project of
   Jiangsu Province (grant no. XYDXXJS-048), and the Qing Lan Project of
   Jiangsu Province.
CR Abdullah A. A., 2012, INFORM ENG LETT, V2, P49
   [Anonymous], 1937, PUBLICATIONS AM STAT
   [Anonymous], 2015, AAAI
   Audibert JY, 2004, ANN I H POINCARE-PR, V40, P685, DOI 10.1016/j.anihpb.2003.11.006
   Baumgartner D, 2013, INT J MACH LEARN CYB, V4, P301, DOI 10.1007/s13042-012-0094-8
   Breiman L., 2001, Mach. Learn., V45, P5
   Cai ZH, 2015, MOL BIOSYST, V11, P791, DOI 10.1039/c4mb00659c
   Cao ZH, 2020, IEEE T FUZZY SYST, V28, P14, DOI 10.1109/TFUZZ.2019.2905823
   Cao ZH, 2019, SCI DATA, V6, DOI 10.1038/s41597-019-0027-4
   Cao ZH, 2018, IEEE T FUZZY SYST, V26, P1032, DOI 10.1109/TFUZZ.2017.2666789
   Cao Zehong, 2019, NEUROCOMPUTING
   Delen D, 2005, ARTIF INTELL MED, V34, P113, DOI 10.1016/j.artmed.2004.07.002
   Delen D, 2009, EXPERT SYST, V26, P100, DOI 10.1111/j.1468-0394.2008.00480.x
   Díaz-Uriarte R, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-3
   Bui DT, 2016, ENVIRON EARTH SCI, V75, DOI 10.1007/s12665-016-5919-4
   Hassanien AE, 2014, APPL SOFT COMPUT, V14, P62, DOI 10.1016/j.asoc.2013.08.011
   Hosny A, 2018, PLOS MED, V15, DOI 10.1371/journal.pmed.1002711
   Huang MW, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0161501
   Khalilia M, 2011, BMC MED INFORM DECIS, V11, DOI 10.1186/1472-6947-11-51
   Kumar A, 2010, ASIA PACIF MICROWAVE, P1189
   Langer D. L., 2010, J MAGNETIC RESONANCE, V30, P327
   Lee H, 2015, EXPERT SYST APPL, V42, P5356, DOI 10.1016/j.eswa.2015.02.005
   Li CS, 2017, AAAI CONF ARTIF INTE, P2175
   Liu B, 2016, BIOINFORMATICS, V32, P2411, DOI 10.1093/bioinformatics/btw186
   Ma F, 2017, PR MACH LEARN RES, V70
   Meng DY, 2017, INFORM SCIENCES, V414, P319, DOI 10.1016/j.ins.2017.05.043
   Mirzaei G, 2016, REV NEUROSCIENCE, V27, P857, DOI 10.1515/revneuro-2016-0029
   Paul R, 2018, J MED IMAGING, V5, DOI 10.1117/1.JMI.5.1.011021
   Paul R, 2016, TOMOGRAPHY, V2, P388, DOI 10.18383/j.tom.2016.00211
   Pedersen JH, 2017, EUR J CARDIO-THORAC, V51, P411, DOI 10.1093/ejcts/ezw418
   Pi T., 2016, P 25 INT JOINT C ART, P1932
   Rodríguez JJ, 2006, IEEE T PATTERN ANAL, V28, P1619, DOI 10.1109/TPAMI.2006.211
   Rodriguez-Galiano VF, 2012, REMOTE SENS ENVIRON, V121, P93, DOI 10.1016/j.rse.2011.12.003
   Samala Ravi K., 2018, MED IMAGING 2018 COM
   Siegel RL, 2021, CA-CANCER J CLIN, V71, P7, DOI [10.3322/caac.21387, 10.3322/caac.20073, 10.3322/caac.21332, 10.3322/caac.21601, 10.3322/caac.21254, 10.3322/caac.21654, 10.3322/caac.20006, 10.3322/caac.21551]
   Tang Y., 2012, P 20 ACM INT C MULT, P833, DOI DOI 10.1145/2393347.2396324
   Torre Lindsey A., 2016, LUNG CANC STAT
   Wang QY, 2018, 2018 IEEE SMARTWORLD, UBIQUITOUS INTELLIGENCE & COMPUTING, ADVANCED & TRUSTED COMPUTING, SCALABLE COMPUTING & COMMUNICATIONS, CLOUD & BIG DATA COMPUTING, INTERNET OF PEOPLE AND SMART CITY INNOVATION (SMARTWORLD/SCALCOM/UIC/ATC/CBDCOM/IOP/SCI), P796, DOI 10.1109/SmartWorld.2018.00149
   Xia LY, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065719500163
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhou Z.-H., 2012, ENSEMBLE METHODS FDN, P414
   Zieba M, 2014, APPL SOFT COMPUT, V14, P99, DOI [10.1016/j.asor.2013.07.016, 10.1016/j.asoc.2013.07.016]
NR 42
TC 23
Z9 24
U1 2
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 34
DI 10.1145/3345314
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300016
OA Green Published, Green Accepted
DA 2024-07-18
ER

PT J
AU Gao, Z
   Li, YM
   Wan, SH
AF Gao, Zan
   Li, Yinming
   Wan, Shaohua
TI Exploring Deep Learning for View-Based 3D Model Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D model retrieval; benchmark; deep learning features; handcrafted
   feature
ID OBJECT RETRIEVAL; DESCRIPTORS; RECOGNITION
AB In recent years, view-based 3D model retrieval has become one of the research focuses in the field of computer vision and machine learning. In fact, the 3D model retrieval algorithm consists of feature extraction and similarity measurement, and the robust features play a decisive role in the similarity measurement. Although deep learning has achieved comprehensive success in the field of computer vision, deep learning features are used for 3D model retrieval only in a small number of works. To the best of our knowledge, there is no benchmark to evaluate these deep learning features. To tackle this problem, in this work we systematically evaluate the performance of deep learning features in view-based 3D model retrieval on four popular datasets (ETH, NTU60, PSB, and MVRED) by different kinds of similarity measure methods. In detail, the performance of hand-crafted features and deep learning features are compared, and then the robustness of deep learning features is assessed. Finally, the difference between single-view deep learning features and multi-view deep learning features is also evaluated. By quantitatively analyzing the performances on different datasets, it is clear that these deep learning features can consistently outperform all of the hand-crafted features, and they are also more robust than the hand-crafted features when different degrees of noise are added into the image. The exploration of latent relationships among different views in multi-view deep learning network architectures shows that the performance of multi-view deep learning outperforms that of single-view deep learning features with low computational complexity.
C1 [Gao, Zan; Li, Yinming] Tianjin Univ Technol, Tianjin Key Lab Intelligence Comp & Novel Softwar, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China.
   [Gao, Zan; Li, Yinming] Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Comp Sci Ctr, Natl Supercomp Ctr Jinan,Shandong Acad Sci, Jinan 250014, Peoples R China.
   [Wan, Shaohua] Zhongnan Univ Econ & Law, Sch Informat & Safety Engn, Wuhan 430073, Peoples R China.
C3 Tianjin University of Technology; Qilu University of Technology;
   Zhongnan University of Economics & Law
RP Wan, SH (corresponding author), Zhongnan Univ Econ & Law, Sch Informat & Safety Engn, Wuhan 430073, Peoples R China.
EM zangaonsh4522@gmail.com; jsjkx_bianwu@163.com; shaohua.wan@ieee.org
RI Wan, Shaohua/L-8492-2019; Wan, Shaohua/B-9243-2014
OI Wan, Shaohua/0000-0001-7013-9081
FU National Natural Science Foundation of China [61872270, 61572357];
   National Key R&D Program of China [2019YFBB1404700]; Jinan's innovation
   team [2018GXRC014]
FX This work was supported in part by the National Natural Science
   Foundation of China (nos. 61872270 and 61572357), National Key R&D
   Program of China (No.2019YFBB1404700), and Jinan's innovation team (no.
   2018GXRC014).
CR [Anonymous], 2001, Intelligent Signal Processing
   Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Daras P, 2010, INT J COMPUT VISION, V89, P229, DOI 10.1007/s11263-009-0277-2
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duan LY, 2019, IEEE MULTIMEDIA, V26, P44, DOI 10.1109/MMUL.2018.2873844
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   DUBUISSON MP, 1994, INT C PATT RECOG, P566, DOI 10.1109/ICPR.1994.576361
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Gao Y, 2014, IEEE T IND ELECTRON, V61, P2088, DOI 10.1109/TIE.2013.2262760
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P2269, DOI 10.1109/TIP.2011.2170081
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Gao Y, 2011, SIGNAL PROCESS-IMAGE, V26, P39, DOI 10.1016/j.image.2010.10.006
   Gao Y, 2010, PATTERN RECOGN, V43, P1142, DOI 10.1016/j.patcog.2009.07.012
   Gao Z, 2019, FUTURE GENER COMP SY, V94, P641, DOI 10.1016/j.future.2018.12.039
   Gao Z, 2018, J VIS COMMUN IMAGE R, V56, P305, DOI 10.1016/j.jvcir.2018.10.007
   Gao Z, 2017, MULTIMED TOOLS APPL, V76, P20125, DOI 10.1007/s11042-017-4384-8
   Gao Z, 2019, IEEE INTERNET THINGS, V6, P9280, DOI 10.1109/JIOT.2019.2911669
   Gao ZZ, 2017, J IMMUNOL RES, V2017, DOI 10.1155/2017/1813086
   Gao ZH, 2018, IEEE INT SEMICONDUCT, P97, DOI 10.1109/ISLC.2018.8516184
   Grabner Alexander, 2018, ARXIV180311493
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leng B, 2015, NEUROCOMPUTING, V168, P761, DOI 10.1016/j.neucom.2015.05.048
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li B, 2013, MULTIMED TOOLS APPL, V62, P821, DOI 10.1007/s11042-011-0873-3
   Liu AA, 2018, IEEE T CYBERNETICS, V48, P916, DOI 10.1109/TCYB.2017.2664503
   Liu AA, 2016, IEEE T IMAGE PROCESS, V25, P2103, DOI 10.1109/TIP.2016.2540802
   Liu A, 2015, INFORM SCIENCES, V320, P429, DOI 10.1016/j.ins.2015.04.042
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P494, DOI 10.1109/TPAMI.2019.2894422
   Liu XL, 2015, IEEE T CYBERNETICS, V45, P2461, DOI 10.1109/TCYB.2014.2374755
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Lu K, 2015, IEEE T IMAGE PROCESS, V24, P1449, DOI 10.1109/TIP.2015.2395961
   Lu K, 2014, INFORM SCIENCES, V281, P703, DOI 10.1016/j.ins.2014.03.079
   Lucas Laurent, 2013, 3D VIDEO CAPTURE DIF, P347
   Mavar-Haramija M, 2015, J MED SYST, V39, DOI 10.1007/s10916-015-0282-7
   Nie WZ, 2016, J VIS COMMUN IMAGE R, V37, P40, DOI 10.1016/j.jvcir.2015.06.011
   Nie Weizhi, 2017, MULTIMEDIA SYSTEMS, V23, P1
   Ohbuchi Ryutarou, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P63, DOI 10.1109/ICCVW.2009.5457716
   PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681
   Polewski P, 2015, ISPRS J PHOTOGRAMM, V105, P252, DOI 10.1016/j.isprsjprs.2015.01.010
   Schiele Bernt, 2003, P 2003 IEEE COMP SOC, V2
   Shih JL, 2007, PATTERN RECOGN, V40, P283, DOI 10.1016/j.patcog.2006.04.034
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Steinbach M., 2000, KDD WORKSH TEXT MIN, P525
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Wan SH, 2020, COMPUT NETW, V168, DOI 10.1016/j.comnet.2019.107036
   Wan SH, 2020, MOBILE NETW APPL, V25, P743, DOI 10.1007/s11036-019-01445-x
   Wan SH, 2020, COMPUT COMMUN, V149, P99, DOI 10.1016/j.comcom.2019.10.012
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xu N, 2019, IEEE T CIRC SYST VID, V29, P2482, DOI 10.1109/TCSVT.2018.2867286
   Yang Luren, 1994, PATTERN RECOGN, V29, P1061
   You Haoxuan, 2018, P ACM C MULT, P1
   Zhou J, 2018, ARXIV181208434
NR 59
TC 93
Z9 93
U1 9
U2 61
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 18
DI 10.1145/3377876
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100017
DA 2024-07-18
ER

PT J
AU Zhang, JX
   Hu, HF
   Lu, XL
AF Zhang, Junxuan
   Hu, Haifeng
   Lu, Xinlong
TI Moving Foreground-Aware Visual Attention and Key Volume Mining for Human
   Action Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual attention; action-relevant key volume; variance-based scheme;
   human action recognition
AB Recently, many deep learning approaches have shown remarkable progress on human action recognition. However, it remains unclear how to extract the useful information in videos since only video-level labels are available in the training phase. To address this limitation, many efforts have been made to improve the performance of action recognition by applying the visual attention mechanism in the deep learning model. In this article, we propose a novel deep model called Moving Foreground Attention (MFA) that enhances the performance of action recognition by guiding the model to focus on the discriminative foreground targets. In our work, MFA detects the moving foreground through a proposed variance-based algorithm. Meanwhile, an unsupervised proposal is utilized to mine the action-related key volumes and generate corresponding correlation scores. Based on these scores, a newly proposed stochastic-out scheme is exploited to train the MFA. Experiment results show that action recognition performance can be significantly improved by using our proposed techniques, and our model achieves state-of-the-art performance on UCF101 and HMDB51.
C1 [Zhang, Junxuan; Hu, Haifeng; Lu, Xinlong] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM zhangjx73@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn;
   luxlong@mail2.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; Natural Science Foundation of Guangdong [2017A030311029];
   National Key R&D Program of China [2018YFB1601101]; Science and
   Technology Program of Guangzhou [201704020180]; Fundamental Research
   Funds for the Central Universities of China [17lgzd08]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants no. 61673402, no. 61273270, and no.
   60802069; in part by the Natural Science Foundation of Guangdong under
   Grants no. 2017A030311029; in part by the National Key R&D Program of
   China under Grant 2018YFB1601101, in part by the Science and Technology
   Program of Guangzhou under Grants no. 201704020180; and in part by the
   Fundamental Research Funds for the Central Universities of China under
   Grant no. 17lgzd08.
CR Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   [Anonymous], VISUAL COGNITION
   Antic B, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P515, DOI 10.1109/ICCVW.2013.73
   BILEN H, 2016, PROC CVPR IEEE, P3034, DOI [10.1109/CVPR.2016.331, DOI 10.1109/CVPR.2016.331]
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Dollar P., 2005, VISUAL SURVEILLANCE, V14, P65, DOI DOI 10.1109/VSPETS.2005.1570899
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Feichtenhofer Christoph., 2016, CoRR
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kuehne H, 2013, HIGH PERFORMANCE COMPUTING IN SCIENCE AND ENGINEERING '12: TRANSACTIONS OF THE HIGH PERFORMANCE COMPUTING CENTER, STUTTGART (HLRS) 2012, P571, DOI 10.1007/978-3-642-33374-3_41
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lin Weiyao, 2017, CORR
   Liu Juncheng., 2017, P IEEE C COMPUTER VI, P792, DOI DOI 10.1109/CVPR.2017.391
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Mnih V., 2014, Neural Information Processing Systems, P2204
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Salakhutdinov, 2015, ARXIV151104119
   Sapienza M, 2014, INT J COMPUT VISION, V110, P30, DOI 10.1007/s11263-013-0662-8
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song S., 2016, CORR
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang X., 2017, PROC CVPR IEEE, P2097, DOI [DOI 10.1109/CVPR.2017.369, 10.1109/CVPR.2017.369]
   Wang XL, 2016, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2016.291
   Wang ZK, 2018, IEEE IMAGE PROC, P3458, DOI 10.1109/ICIP.2018.8451483
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan Shiyang, 2017, SIGNAL PROCESS-IMAGE, V61, P73
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 39
TC 15
Z9 16
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2019
VL 15
IS 3
AR 74
DI 10.1145/3321511
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ6YR
UT WOS:000494301200006
DA 2024-07-18
ER

PT J
AU Zhao, L
   Chen, ZK
   Yang, LT
   Deen, MJ
   Wang, ZJ
AF Zhao, Liang
   Chen, Zhikui
   Yang, Laurence T.
   Deen, M. Jamal
   Wang, Z. Jane
TI Deep Semantic Mapping for Heterogeneous Multimedia Transfer Learning
   Using Co-Occurrence Data
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep semantic mapping; heterogeneous multimedia; transfer learning; deep
   neural networks; canonical correlation analysis
ID DOMAIN; CLASSIFICATION
AB Transfer learning, which focuses on finding a favorable representation for instances of different domains based on auxiliary data, can mitigate the divergence between domains through knowledge transfer. Recently, increasing efforts on transfer learning have employed deep neural networks (DNN) to learn more robust and higher level feature representations to better tackle cross-media disparities. However, only a few articles consider the correction and semantic matching between multi-layer heterogeneous domain networks. In this article, we propose a deep semantic mapping model for heterogeneous multimedia transfer learning (DHTL) using co-occurrence data. More specifically, we integrate the DNN with canonical correlation analysis (CCA) to derive a deep correlation subspace as the joint semantic representation for associating data across different domains. In the proposed DHTL, a multi-layer correlation matching network across domains is constructed, in which the CCA is combined to bridge each pair of domain-specific hidden layers. To train the network, a joint objective function is defined and the optimization processes are presented. When the deep semantic representation is achieved, the shared features of the source domain are transferred for task learning in the target domain. Extensive experiments for three multimedia recognition applications demonstrate that the proposed DHTL can effectively find deep semantic representations for heterogeneous domains, and it is superior to the several existing state-of-the-art methods for deep transfer learning.
C1 [Zhao, Liang; Chen, Zhikui] Dalian Univ Technol, Sch Software Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116024, Liaoning, Peoples R China.
   [Yang, Laurence T.] St Francis Xavier Univ, Dept Comp Sci, Antigonish, NS B2G 2W5, Canada.
   [Deen, M. Jamal] McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
   [Wang, Z. Jane] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
C3 Dalian University of Technology; Saint Francis Xavier University -
   Canada; McMaster University; University of British Columbia
RP Zhao, L (corresponding author), Dalian Univ Technol, Sch Software Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116024, Liaoning, Peoples R China.
EM liangzhao@dlut.edu.cn; zkchen@dlut.edu.cn; ltyang@gmail.com;
   jamal@mcmaster.ca; zjanew@ece.ubc.ca
RI Zhao, Liang/T-9147-2019; Deen, M. Jamal/A-7567-2008; Wang,
   Qian/GRF-2696-2022; Laurence T. Yang, FCAE/AAA-1898-2019
OI Laurence T. Yang, FCAE/0000-0002-7986-4244; Deen,
   Jamal/0000-0002-6390-0933
FU National Natural Science Foundation of China [61672123]; Key Science and
   Technology Planning Project of Guangdong Province, China
   [2015B010110006]; Fundamental Research Funds for the Central
   Universities [DUT18RC(3)025]; Canada Research Chair (CRC) program
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants No. 61672123, in part by Key Science
   and Technology Planning Project of Guangdong Province, China, under
   Grant No. 2015B010110006 and in part by the Fundamental Research Funds
   for the Central Universities under Grant No. DUT18RC(3)025. It is also
   supported by the Canada Research Chair (CRC) program.
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2012, ARXIV12064660
   [Anonymous], 2017, P IEEE C COMPUTER VI
   [Anonymous], 2011, P 25 AAAI C ART INT
   Cheng M., 2012, 2012 8th International Symposium on Communication Systems, Networks Digital Signal Processing (CSNDSP), P1
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Ding ZM, 2016, INT CONF ACOUST SPEE, P2414, DOI 10.1109/ICASSP.2016.7472110
   Donahue J, 2013, PROC CVPR IEEE, P668, DOI 10.1109/CVPR.2013.92
   Feng FX, 2015, NEUROCOMPUTING, V154, P50, DOI 10.1016/j.neucom.2014.12.020
   Glorot X., 2011, P 28 INT C INT C MAC, P513
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hoffman J., 2016, FCNS WILD PIXELLEVEL
   Huang KQ, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2449081
   Jing LP, 2012, IEEE T IMAGE PROCESS, V21, P4508, DOI 10.1109/TIP.2012.2206040
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167
   Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183
   Ngiam J., 2012, INT C MACH LEARN, P689
   Ni J, 2013, PROC CVPR IEEE, P692, DOI 10.1109/CVPR.2013.95
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pan WK, 2013, ARTIF INTELL, V197, P39, DOI 10.1016/j.artint.2013.01.003
   Raina R., 2007, P 24 INT C MACH LEAR, P759
   Sagha H, 2016, INT CONF ACOUST SPEE, P5800, DOI 10.1109/ICASSP.2016.7472789
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935, DOI DOI 10.1007/978-3-319-46478-7
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Yang L, 2016, IEEE T NEUR NET LEAR, V27, P2187, DOI 10.1109/TNNLS.2015.2472457
   Yang L, 2015, IEEE T IMAGE PROCESS, V24, P4701, DOI 10.1109/TIP.2015.2465157
   Yang Q., 2009, ACL IJCNLP 2009 JT C, P1, DOI DOI 10.3115/1687878.1687880
   Yang Xianrui, 2015, ScientificWorldJournal, V2015, P718180, DOI 10.1155/2015/718180
   Yeh YR, 2014, IEEE T IMAGE PROCESS, V23, P2009, DOI 10.1109/TIP.2014.2310992
   Zhang QB, 2016, GASTROENT RES PRACT, V2016, DOI 10.1155/2016/8962321
   Zhang  X., 2015, COMPUTER VISION PATT
   Zhao L, 2018, IEEE SYST J, V12, P1610, DOI 10.1109/JSYST.2016.2576026
   Zhao L, 2018, IEEE SIGNAL PROC LET, V25, P60, DOI 10.1109/LSP.2017.2769086
   Zhou JT, 2014, AAAI CONF ARTIF INTE, P2213
   Zhou JTY, 2014, JMLR WORKSH CONF PRO, V33, P1095
   Zhuang FZ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4119
NR 39
TC 30
Z9 30
U1 2
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 9
DI 10.1145/3241055
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100009
DA 2024-07-18
ER

PT J
AU Bhat, D
   Rizk, A
   Zink, M
   Steinmetz, R
AF Bhat, Divyashri
   Rizk, Amr
   Zink, Michael
   Steinmetz, Ralf
TI SABR: Network-Assisted Content Distribution for QoE-Driven ABR Video
   Streamingd
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE SDN; OpenFlow; QoE; ABR streaming; DASH; network-assisted streaming;
   video quality metrics; caching
AB State-of-the-art software-defined wide area networks (SD-WANs) provide the foundation for flexible and highly resilient networking. In this work, we design, implement, and evaluate a novel architecture (denoted as SABR) that leverages the benefits of software-defined networking (SDN) to provide network-assisted adaptive bitrate streaming. With clients retaining full control of their streaming algorithms, we clearly show that by this network assistance, both the clients and the content providers benefit significantly in terms of quality of experience (QoE) and content origin offloading. SABR utilizes information on available bandwidths per link and network cache contents to guide video streaming clients with the goal of improving the viewer's QoE. In addition, SABR uses SDN capabilities to dynamically program flows to optimize the utilization of content delivery network caches.
   Backed by our study of SDN-assisted streaming, we discuss the change in the requirements for network-to-player APIs that enables flexible video streaming. We illustrate the difficulty of the problem and the impact of SDN-assisted streaming on QoE metrics using various well-established player algorithms. We evaluate SABR together with state-of-the-art dynamic adaptive streaming over HTTP (DASH) quality adaptation algorithms through a series of experiments performed on a real-world, SDN-enabled testbed network with minimal modifications to an existing DASH client. In addition, we compare the performance of different caching strategies in combination with SABR. Our trace-based measurements show the substantial improvement in cache hit rates and QoE metrics in conjunction with SABR indicating a rich design space for jointly optimized SDN-assisted caching architectures for adaptive bitrate video streaming applications.
C1 [Bhat, Divyashri; Zink, Michael] Univ Massachusetts, Dept Elect & Comp Engn, 151 Holdsworth Way, Amherst, MA 01003 USA.
   [Rizk, Amr; Steinmetz, Ralf] Tech Univ Darmstadt, Rundeturmstr 10, D-64293 Darmstadt, Germany.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   Technical University of Darmstadt
RP Bhat, D (corresponding author), Univ Massachusetts, Dept Elect & Comp Engn, 151 Holdsworth Way, Amherst, MA 01003 USA.
RI Steinmetz, Patrick R. H./AAD-4093-2022
OI Steinmetz, Ralf/0000-0002-6839-9359; Bhat,
   Divyashri/0000-0001-5768-291X; Zink, Michael/0000-0002-0309-9240
FU NSF [1419199]; DFG [1053 MAKI]; IREP Program; Direct For Computer & Info
   Scie & Enginr; Division Of Computer and Network Systems [1419199]
   Funding Source: National Science Foundation
FX This work was funded in part by NSF grant 1419199, by the DFG as part of
   the Collaborative Research Center 1053 MAKI (TP B4, C3), and the IREP
   Program.
CR Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   Agarwal S, 2013, IEEE INFOCOM SER, P2211
   [Anonymous], 2013 OPT FIB COMM C
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], 2014, LOGIN
   [Anonymous], 2015, 12 USENIX S NETW SYS
   [Anonymous], P ACM INT C SCAL INF
   [Anonymous], P IEEE ICC QOE FI WO
   [Anonymous], SMOOTH STREAM
   [Anonymous], ISP PARTN OPT
   [Anonymous], GLOB INT PHEN LAT AM
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Berde P., 2014, Proceedings of the Third Workshop on Hot Topics in Software Defined Networking, P1, DOI DOI 10.1109/NOMS.2014.6838228
   Berger DS, 2014, PERFORM EVALUATION, V79, P2, DOI 10.1016/j.peva.2014.07.001
   Berman M, 2014, COMPUT NETW, V61, P5, DOI 10.1016/j.bjp.2013.12.037
   Bhat D, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P62, DOI 10.1145/3083187.3083196
   Bouten N, 2016, INT CONF NETW SER, P82, DOI 10.1109/CNSM.2016.7818403
   Bouten N, 2014, IEEE T MULTIMEDIA, V16, P2281, DOI 10.1109/TMM.2014.2362856
   BURNHAM K.P., 2002, MODEL SELECTION MULT, P352
   Che H, 2002, IEEE J SEL AREA COMM, V20, P1305, DOI 10.1109/JSAC.2002.801752
   Cofano G., 2014, P 2014 WORKSHOP DESI, P7
   Ferragut A, 2016, SIGMETRICS/PERFORMANCE 2016: PROCEEDINGS OF THE SIGMETRICS/PERFORMANCE JOINT INTERNATIONAL CONFERENCE ON MEASUREMENT AND MODELING OF COMPUTER SCIENCE, P101, DOI [10.1145/2896377.2901459, 10.1145/2964791.2901459]
   Fofack NC, 2014, COMPUT NETW, V65, P212, DOI 10.1016/j.comnet.2014.03.006
   Fricker C., 2012, P 24 INT TELETRAFFIC, P1
   Georgopoulos P, 2015, COMPUT COMMUN, V69, P79, DOI 10.1016/j.comcom.2015.06.015
   Hemmati M, 2015, 2015 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P1182, DOI 10.1109/SSCI.2015.170
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Jain S, 2013, ACM SIGCOMM COMP COM, V43, P3, DOI 10.1145/2534169.2486019
   Kleinrouweler JW, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P36, DOI 10.1145/2910017.2910599
   Kleinrouweler JW, 2015, 2015 27TH INTERNATIONAL TELETRAFFIC CONGRESS ITC 27, P177, DOI 10.1109/ITC.2015.28
   Krishnappa D.K., 2015, Proceedings of the 6th ACM Multimedia Systems Conference. ACM, P37, DOI DOI 10.1145/2713168.2713175
   Lederer S., 2012, P 3 MULT SYST C, P89
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liu X, 2012, ACM SIGCOMM COMP COM, V42, P359, DOI 10.1145/2377677.2377752
   Lübben R, 2014, IEEE ACM T NETWORK, V22, P484, DOI 10.1109/TNET.2013.2261914
   Maggs BM, 2015, ACM SIGCOMM COMP COM, V45, P52, DOI 10.1145/2805789.2805800
   Martina V, 2014, IEEE INFOCOM SER, P2040, DOI 10.1109/INFOCOM.2014.6848145
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Mukerjee MK, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P311, DOI 10.1145/2785956.2787475
   Nam H, 2014, IEEE GLOB COMM CONF, P1317, DOI 10.1109/GLOCOM.2014.7036990
   Podlipnig S, 2003, ACM COMPUT SURV, V35, P374, DOI 10.1145/954339.954341
   Pournelle G. H., 1953, Journal of Mammalogy, V34, P133, DOI 10.1890/0012-9658(2002)083[1421:SDEOLC]2.0.CO;2
   Raghavan B, 2012, PROCEEDINGS OF THE 11TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS-XI), P43
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Thomas E., 2015, ENHANCING MPEG DASH
   Wang C, 2016, PR INT CONGR SOUND V
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zink M, 2005, IEEE T MULTIMEDIA, V7, P75, DOI 10.1109/TMM.2004.840595
NR 49
TC 19
Z9 19
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 32
DI 10.1145/3183516
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TD
UT WOS:000434634700005
DA 2024-07-18
ER

PT J
AU Tiwari, A
   Von Der Weth, C
   Kankanhalli, MS
AF Tiwari, Akanksha
   Von Der Weth, Christian
   Kankanhalli, Mohan S.
TI Multimodal Multiplatform Social Media Event Summarization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Socialmedia summarization; multimedia; topic modeling; Markov random
   fields; evaluation; user study
AB Social media platforms are turning into important news sources since they provide real-time information from different perspectives. However, high volume, dynamism, noise, and redundancy exhibited by social media data make it difficult to comprehend the entire content. Recent works emphasize on summarizing the content of either a single social media platform or of a single modality (either textual or visual). However, each platform has its own unique characteristics and user base, which brings to light different aspects of real-world events. This makes it critical as well as challenging to combine textual and visual data from different platforms. In this article, we propose summarization of real-world events with data stemming from different platforms and multiple modalities. We present the use of a Markov Random Fields based similarity measure to link content across multiple platforms. This measure also enables the linking of content across time, which is useful for tracking the evolution of long-running events. For the final content selection, summarization is modeled as a subset selection problem. To handle the complexity of the optimal subset selection, we propose the use of submodular objectives. Facets such as coverage, novelty, and significance are modeled as submodular objectives in a multimodal social media setting. We conduct a series of quantitative and qualitative experiments to illustrate the effectiveness of our approach compared to alternative methods.
C1 [Tiwari, Akanksha; Von Der Weth, Christian; Kankanhalli, Mohan S.] Natl Univ Singapore, SSI, I3 Bldg,SSI HQ 02-02,21 Heng Mui Keng Terrace, Singapore 119613, Singapore.
C3 National University of Singapore
RP Tiwari, A (corresponding author), Natl Univ Singapore, SSI, I3 Bldg,SSI HQ 02-02,21 Heng Mui Keng Terrace, Singapore 119613, Singapore.
EM akanksha@comp.nus.edu.sg; vonderweth@nus.edu.sg; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
FU National Research Foundation; Prime Minister's Office, Singapore under
   its IRC@SG Funding Initiative
FX This research is supported by the National Research Foundation, Prime
   Minister's Office, Singapore under its IRC@SG Funding Initiative and
   administered by the Interactive and Digital Media Programme Office
   (IDMPO).
CR AlRubaian M, 2015, PROCEEDINGS OF THE 2015 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2015), P1434, DOI 10.1145/2808797.2810065
   [Anonymous], P 4 INT C ADV MULT M
   [Anonymous], 1980, Markov Random Fields and Their Applications, volume 1 of Contemporary Mathematics
   [Anonymous], 2014, ADV NEURAL INFORM PR
   Baum MA, 2015, J PEACE RES, V52, P384, DOI 10.1177/0022343314554791
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Bian JW, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1807, DOI 10.1145/2505515.2505652
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Chen ZY, 2014, PR MACH LEARN RES, V32, P703
   Chu LY, 2016, IEEE T CIRC SYST VID, V26, P556, DOI 10.1109/TCSVT.2014.2347551
   Clarke C. L., 2008, P 31 ANN INT ACM SIG, P659, DOI [DOI 10.1145/1390334.1390446, 10.1145/1390334.1390446]
   Cui Bin., 2010, ACM SIGMOD International Conference on Management of Data, P435
   Cui P, 2016, IEEE MULTIMEDIA, V23, P92, DOI 10.1109/MMUL.2016.8
   FLEISS JL, 1971, PSYCHOL BULL, V76, P378, DOI 10.1037/h0031619
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Haghighi Aria, 2009, P HUMAN LANGUAGE TEC, P362, DOI DOI 10.3115/1620754.1620807
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Kim G, 2015, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2015.7298927
   Kirchhoff Katrin, 2014, P 2014 C EMPIRICAL M, P131
   Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420
   Lim BH, 2015, PROCEEDINGS OF THE 2015 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2015), P113, DOI 10.1145/2808797.2808820
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Lin H., 2011, P 49 ANN M ASS COMP, P510
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   McParlane P.J., 2014, P 23 ACM INT C INF K, P1459
   Metzler D., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P472, DOI 10.1145/1076034.1076115
   Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528
   Mirzasoleiman Baharan, 2013, ADV NEURAL INFORM PR, P2049, DOI DOI 10.1007/S10898-019-00840-8
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Nie L., 2016, Synthesis Lectures on Information Concepts Retrieval & Services, V8, P1, DOI 10.2200/S00714ED1V01Y201603ICR048
   Nie LQ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P591, DOI 10.1145/2733373.2806217
   OConnor B., 2010, P INT AAAI C WEBLOGS, P1
   Qian SS, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P99, DOI 10.1145/2733373.2806234
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Qian SS, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2659521
   Qian Shengsheng, 2016, P 24 ACM INT C MULTI
   Schinas M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P203, DOI 10.1145/2671188.2749407
   Shah RR, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P185, DOI 10.1145/2733373.2809932
   Sharifi B., 2010, HUMAN LANGUAGE TECHN, P685
   Sinha Pinaki, 2011, P ACM INT C MULT RET
   Sipos Ruben, 2012, P 21 ACM INT C INF K, P754, DOI DOI 10.1145/2396761.2396857
   Song XM, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P213, DOI 10.1145/2766462.2767726
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Wang Z., 2012, Proceedings of the 20th ACM International Conference on Multimedia, MM '12, P1359
   Wang ZJ, 2014, J CHEM-NY, V2014, DOI 10.1155/2014/475389
   Wei Kai, 2013, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, P721
   Yan M, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2957755
   Zaharieva M, 2015, IEEE MULTIMEDIA, V22, P14, DOI 10.1109/MMUL.2015.31
   Zhang TZ, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2602633
NR 50
TC 8
Z9 9
U1 4
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 38
DI 10.1145/3115433
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TD
UT WOS:000434634700011
DA 2024-07-18
ER

PT J
AU Yao, YQ
   Huang, D
   Yang, XD
   Wang, YH
   Chen, LM
AF Yao, Yongqiang
   Huang, Di
   Yang, Xudong
   Wang, Yunhong
   Chen, Liming
TI Texture and Geometry Scattering Representation-Based Facial Expression
   Recognition in 2D+3D Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Facial expression recognition; the scattering descriptor; 2D and 3D
   videos; multi-modal fusion
ID FACE; MODEL; CLASSIFICATION; HISTOGRAMS
AB Facial Expression Recognition (FER) is one of the most important topics in the domain of computer vision and pattern recognition, and it has attracted increasing attention for its scientific challenges and application potentials. In this article, we propose a novel and effective approach to FER using multi-model two-dimensional (2D) and 3D videos, which encodes both static and dynamic clues by scattering convolution network. First, a shape-based detection method is introduced to locate the start and the end of an expression in videos; segment its onset, apex, and offset states; and sample the important frames for emotion analysis. Second, the frames in Apex of 2D videos are represented by scattering, conveying static texture details. Those of 3D videos are processed in a similar way, but to highlight static shape details, several geometric maps in terms of multiple order differential quantities, i.e., Normal Maps and Shape Index Maps, are generated as the input of scattering, instead of original smooth facial surfaces. Third, the average of neighboring samples centred at each key texture frame or shape map in Onset is computed, and the scattering features extracted from all the average samples of 2D and 3D videos are then concatenated to capture dynamic texture and shape cues, respectively. Finally, Multiple Kernel Learning is adopted to combine the features in the 2D and 3D modalities and compute similarities to predict the expression label. Thanks to the scattering descriptor, the proposed approach not only encodes distinct local texture and shape variations of different expressions as by several milestone operators, such as SIFT, HOG, and so on, but also captures subtle information hidden in high frequencies in both channels, which is quite crucial to better distinguish expressions that are easily confused. The validation is conducted on the BU-4DFE and BP-4D databa ses, and the accuracies reached are very competitive, indicating its competency for this issue.
C1 [Yao, Yongqiang; Huang, Di] Beihang Univ, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
   [Yang, Xudong; Wang, Yunhong] Beihang Univ, Sch Comp Sci & Engn, IRIP Lab, Beijing 100191, Peoples R China.
   [Chen, Liming] Ecole Cent Lyon, CNRS UMR 5205, LIRIS, Dept Math Informat, 36 Ave Guy de Collongue, F-69134 Ecully, France.
C3 Beihang University; Beihang University; Ecole Centrale de Lyon; Institut
   National des Sciences Appliquees de Lyon - INSA Lyon
RP Huang, D (corresponding author), Beihang Univ, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
EM dirkyao@buaa.edu.cn; dhuang@buaa.edu.cn; xdyang@buaa.edu.cn;
   yhwang@buaa.edu.cn; liming.chen@ec-lyon.fr
RI Huang, Di/JBJ-3541-2023
OI Huang, Di/0000-0001-7877-7301
FU National Key Research and Development Plan [2016YFC0801002]; National
   Natural Science Foundation of China [61673033]; State Key Laboratory of
   Software Development Environment [SKLSDE-2017ZX-07]; Microsoft Research
   Asia Collaborative Program [FY17-RES-THEME-033]; French Research Agency,
   l'Agence Nationale de Recherche (ANR) through the Jemime project
   [ANR-13-CORD-0004-02]; PUF 4D Vision project - Partner University
   Foundation
FX This work is partly supported by the National Key Research and
   Development Plan (No. 2016YFC0801002); the National Natural Science
   Foundation of China (No. 61673033); the Research Program of State Key
   Laboratory of Software Development Environment (SKLSDE-2017ZX-07);
   Microsoft Research Asia Collaborative Program (FY17-RES-THEME-033); the
   French Research Agency, l'Agence Nationale de Recherche (ANR), through
   the Jemime project (ANR-13-CORD-0004-02); and the PUF 4D Vision project
   funded by the Partner University Foundation.
CR Abboud B, 2004, SIGNAL PROCESS-IMAGE, V19, P723, DOI 10.1016/j.image.2004.05.009
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], IEEE T AFFECTIVE COM
   [Anonymous], 2013, BIOMETRIC RECOGNITIO
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE INT C AUT FAC
   [Anonymous], P IEEE INT C AUT FAC
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], 2013, 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)
   Ben Amor B, 2014, IEEE T CYBERNETICS, V44, P2443, DOI 10.1109/TCYB.2014.2308091
   Berretti S., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4125, DOI 10.1109/ICPR.2010.1002
   Berretti S, 2013, VISUAL COMPUT, V29, P1333, DOI 10.1007/s00371-013-0869-2
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelakis A, 2015, MULTIMED TOOLS APPL, V74, P5577, DOI 10.1007/s11042-014-1869-6
   Dapogny A, 2015, IEEE I CONF COMP VIS, P3783, DOI 10.1109/ICCV.2015.431
   Drira H, 2012, INT C PATT RECOG, P1104
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Fang T., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P603, DOI 10.1109/FG.2011.5771466
   Fang TH, 2012, IMAGE VISION COMPUT, V30, P738, DOI 10.1016/j.imavis.2012.02.004
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Gong B., 2009, Proceedings of the 17th ACM International Conference on Multimedia, P569
   Huang D, 2012, IEEE T INF FOREN SEC, V7, P1551, DOI 10.1109/TIFS.2012.2206807
   Huang D, 2011, LECT NOTES COMPUT SC, V6523, P206
   Huang D, 2011, IEEE T SYST MAN CY C, V41, P765, DOI 10.1109/TSMCC.2011.2118750
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   KOENDERINK JJ, 1992, IMAGE VISION COMPUT, V10, P557, DOI 10.1016/0262-8856(92)90076-F
   Lanitis A, 1997, IEEE T PATTERN ANAL, V19, P743, DOI 10.1109/34.598231
   Li HB, 2015, COMPUT VIS IMAGE UND, V140, P83, DOI 10.1016/j.cviu.2015.07.005
   Li HB, 2012, INT C PATT RECOG, P2577
   Li HB, 2011, LECT NOTES COMPUT SC, V6915, P483, DOI 10.1007/978-3-642-23687-7_44
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Liu P, 2014, PROC CVPR IEEE, P1805, DOI 10.1109/CVPR.2014.233
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Mollaeian Aida, 2016, 2016 IEEE Conference on Electromagnetic Field Computation (CEFC), DOI 10.1109/CEFC.2016.7816397
   Mpiperis I, 2008, IEEE T INF FOREN SEC, V3, P498, DOI 10.1109/TIFS.2008.924598
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Qingkai Zhen, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015, January 5-7, 2015, Proceedings: LNCS 8935, P522, DOI 10.1007/978-3-319-14445-0_45
   Reale M, 2013, IEEE INT CONF AUTOMA
   RINN WE, 1984, PSYCHOL BULL, V95, P52, DOI 10.1037/0033-2909.95.1.52
   Sandbach G., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P406, DOI 10.1109/FG.2011.5771434
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P762, DOI 10.1016/j.imavis.2012.01.006
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Sun Y, 2008, LECT NOTES COMPUT SC, V5303, P58, DOI 10.1007/978-3-540-88688-4_5
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Tsalakanidou F, 2010, PATTERN RECOGN, V43, P1763, DOI 10.1016/j.patcog.2009.12.009
   Xi Zhao, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3724, DOI 10.1109/ICPR.2010.907
   Xue ML, 2015, IEEE WINT CONF APPL, P199, DOI 10.1109/WACV.2015.34
   Yang SF, 2012, IEEE T SYST MAN CY B, V42, P980, DOI 10.1109/TSMCB.2012.2192269
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Yin Lijun, 2008, P IEEE INT C AUTOMAT, P1
   Zalewski L, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P493, DOI 10.1109/AFGR.2004.1301581
   Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
NR 57
TC 26
Z9 27
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2018
VL 14
IS 1
SU S
AR 18
DI 10.1145/3131345
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZK
UT WOS:000433517100004
DA 2024-07-18
ER

PT J
AU Dutta, T
   Gupta, HP
AF Dutta, Tanima
   Gupta, Hari Prabhat
TI An Efficient Framework for Compressed Domain Watermarking in P Frames of
   High-Efficiency Video Coding (HEVC)-Encoded Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Compressed Domain; High-Efficiency Video Coding (HEVC); Video
   Watermarking
ID ROBUST WATERMARKING
AB Digital watermarking has received much attention in recent years as a promising solution to copyright protection. Video watermarking in compressed domain has gained importance since videos are stored and transmitted in a compressed format. This decreases the overhead to fully decode and re-encode the video for embedding and extraction of the watermark. High Efficiency Video Coding (HEVC/H.265) is the latest and most efficient video compression standard and a successor to H.264 Advanced Video Coding. In this article, we propose a robust watermarking framework for HEVC-encoded video using informed detector. A readable watermark is embedded invisibly in P frames for better perceptual quality. Our framework imposes security and robustness by selecting appropriate blocks using a random key and the spatio-temporal characteristics of the compressed video. A detail analysis of the strengths of different compressed domain features is performed for implementing the watermarking framework. We experimentally demonstrate the utility of the proposed work. The results show that the proposed work effectively limits the increase in video bitrate and degradation in perceptual quality. The proposed framework is robust against re-encoding and image processing attacks.
C1 [Dutta, Tanima; Gupta, Hari Prabhat] Indian Inst Technol BHU Varanasi, Dept Comp Sci & Engn, Varanasi 221005, Uttar Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology BHU Varanasi (IIT BHU Varanasi)
RP Dutta, T (corresponding author), Indian Inst Technol BHU Varanasi, Dept Comp Sci & Engn, Varanasi 221005, Uttar Pradesh, India.
EM dutta.tanima@gmail.com; hprabhatgupta@gmail.com
RI GUPTA, HARI PRABHAT/AAC-1727-2021; Dutta, Tanima/N-7222-2015
OI GUPTA, HARI PRABHAT/0000-0003-3207-1340; 
CR [Anonymous], 1996, AM NAT STAND TEL DIG
   [Anonymous], 2012, CHIN J INORG ANAL CH
   [Anonymous], P IEEE INT S BROADB
   Boho A, 2013, IEEE SIGNAL PROC MAG, V30, P97, DOI 10.1109/MSP.2012.2230220
   Chen W, 2014, MULTIMEDIA SYST, V20, P179, DOI 10.1007/s00530-013-0329-x
   Cock J. D., 2014, MULTIMED TOOLS APPL, V2014, P1
   Cox IJ, 2008, MKS MULTIMED INFORM, P425, DOI 10.1016/B978-012372585-1.50015-2
   Dutta T., 2014, THESIS
   Dutta T, 2016, J VIS COMMUN IMAGE R, V38, P29, DOI 10.1016/j.jvcir.2015.12.007
   Dutta T, 2013, NATL CONF COMMUN
   Esen E, 2011, IEEE T CIRC SYST VID, V21, P1130, DOI 10.1109/TCSVT.2011.2134770
   Gui Feng, 2011, Proceedings of the 2011 International Conference on Anti-Counterfeiting, Security and Identification (2011 ASID), P73, DOI 10.1109/ASID.2011.5967419
   Kuo TY, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P853, DOI 10.1109/IIH-MSP.2008.230
   Mansouri A, 2010, IEEE T INF FOREN SEC, V5, P649, DOI 10.1109/TIFS.2010.2076280
   Nguyen CV, 2006, 2006 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, P81
   Noorkami M, 2005, IEEE IMAGE PROC, P1229
   Noorkami M, 2008, IEEE T INF FOREN SEC, V3, P441, DOI 10.1109/TIFS.2008.923825
   Noorkami M, 2007, IEEE T INF FOREN SEC, V2, P14, DOI 10.1109/TIFS.2006.890306
   Ogawa K, 2015, I SYMP CONSUM ELECTR, P102, DOI 10.1109/ICCE.2015.7066337
   Qiu G, 2004, INT C PATT RECOG, P865
   Salomon D., 2010, ENG GUIDE AUTOMATED
   Stütz T, 2014, IEEE T MULTIMEDIA, V16, P1337, DOI 10.1109/TMM.2014.2310595
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Swaraja K., 2011, 2011 Annual IEEE India Conference, P1
   Swati S, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0105613
   WATSON AB, 1993, P SOC PHOTO-OPT INS, V1913, P202, DOI 10.1117/12.152694
   Xu D.Y., 2011, EC PERSPECTIVE, V6, P6
   Zhang J, 2007, IEEE T CIRCUITS-II, V54, P205, DOI 10.1109/TCSII.2006.886247
   Zhang J, 2006, ICICIC 2006: FIRST INTERNATIONAL CONFERENCE ON INNOVATIVE COMPUTING, INFORMATION AND CONTROL, VOL 3, PROCEEDINGS, P46
   Zou D., 2009, P MED FOR SEC, P725
   Zou DK, 2010, IEEE INT CON MULTI, P117, DOI 10.1109/ICME.2010.5583550
NR 31
TC 30
Z9 32
U1 0
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2017
VL 13
IS 1
AR 12
DI 10.1145/3002178
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM7WJ
UT WOS:000395522700012
DA 2024-07-18
ER

PT J
AU Ademoye, OA
   Murray, N
   Muntean, GM
   Ghinea, G
AF Ademoye, Oluwakemi A.
   Murray, Niall
   Muntean, Gabriel-Miro
   Ghinea, Gheorghita
TI Audio Masking Effect on Inter-Component Skews in Olfaction-Enhanced
   Multimedia Presentations
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Experimentation; Human Factors; Audio; olfaction; masking
   effect; quality of experience; synchronization
ID VIRTUAL ENVIRONMENTS; SYNCHRONIZATION; EXPERIENCE; PERCEPTION;
   EDUCATION; QUALITY; MEMORY; SENSE
AB Media-rich content plays a vital role in consumer applications today, as these applications try to find new and interesting ways to engage their users. Video, audio, and the more traditional forms of media content continue to dominate with respect to the use of media content to enhance the user experience. Tactile interactivity has also now become widely popular in modern computing applications, while our olfactory and gustatory senses continue to have a limited role. However, in recent times, there have been significant advancements regarding the use of olfactory media content (i.e., smell), and there are a variety of devices now available to enable its computer-controlled emission. This paper explores the impact of the audio stream on user perception of olfactory-enhanced video content in the presence of skews between the olfactory and video media. This research uses the results from two experimental studies of user-perceived quality of olfactory-enhanced multimedia, where audio was present and absent, respectively. Specifically, the paper shows that the user Quality of Experience (QoE) is generally higher in the absence of audio for nearly perfect synchronized olfactory-enhanced multimedia presentations (i.e., an olfactory media skew of between {-10,+ 10s}); however, for greater olfactory media skews (ranging between {-30s;-10s} and {+ 10s, +30s}) user QoE is higher when the audio stream is present. It can be concluded that the presence of the audio has the ability to mask larger synchronization skews between the other media components in olfaction-enhanced
C1 [Ademoye, Oluwakemi A.] Univ Wales Trinity St David, Sch Appl Comp, Swansea SA1 6ED, W Glam, Wales.
   [Murray, Niall] Athlone Inst Technol, Dept Elect, Comp Software Engn, Dublin Rd, Athlone, Ireland.
   [Muntean, Gabriel-Miro] Dublin City Univ, Sch Elect Engn, Dublin 9, Ireland.
   [Ghinea, Gheorghita] Brunel Univ, Dept Comp Sci, Kingston Lane, Uxbridge UB8 3PH, Middx, England.
C3 University of Wales Trinity St David; Technological University of the
   Shannon: Midlands Midwest; Dublin City University; Brunel University
RP Ademoye, OA (corresponding author), Univ Wales Trinity St David, Sch Appl Comp, Swansea SA1 6ED, W Glam, Wales.
EM kemi.ademoye@uwtsd.ac.uk; nmurray@research.ait.ie;
   gabriel.muntean@dcu.ie; george.ghinea@brunel.ac.uk
RI Muntean, Gabriel-Miro/U-6783-2019; Ghinea, Gheorghita/AAG-6770-2020
OI Muntean, Gabriel-Miro/0000-0002-9332-4770; Ghinea,
   Gheorghita/0000-0003-2578-5580
CR Ademoye O. A., 2014, IEEE T SYST MAN CYB, V40, P657
   Ademoye OA, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487270
   Ademoye OA, 2009, IEEE T MULTIMEDIA, V11, P561, DOI 10.1109/TMM.2009.2012927
   [Anonymous], 2014, RP913 ITUT
   [Anonymous], P 25 SPRING C COMP G
   Blakowski G, 1996, IEEE J SEL AREA COMM, V14, P5, DOI 10.1109/49.481691
   Cha J., 2009, MM '09: Proceedings of the seventeen ACM international conference on Multimedia, P1135, DOI [10.1145/1631272.1631535, DOI 10.1145/1631272.1631535]
   Concolato C., 2012, P ACM MULT SYST C NE, P227
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Fernström M, 1997, MUSEUMS AND THE WEB 97: SELECTED PAPERS, P191
   Ghinea G., 2011, MULTIPLE SENSORIAL M
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   Ghinea G, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2617994
   Ghinea G, 2011, MULTIMED TOOLS APPL, V55, P601, DOI 10.1007/s11042-010-0581-4
   Hall T, 2006, J COMPUT ASSIST LEAR, V22, P231, DOI 10.1111/j.1365-2729.2006.00177.x
   Huang ZX, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490821
   Hughes CE, 2005, IEEE COMPUT GRAPH, V25, P24, DOI 10.1109/MCG.2005.139
   Kaye J., 2001, Symbolic olfactory display
   Kiger P. J., 2006, LOS ANGELES TIMES
   Kovács PT, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON INTERACTIVE MOBILE COMMUNICATION TECHNOLOGIES AND LEARNING (IMCL), P283, DOI 10.1109/IMCTL.2015.7359604
   LAING DG, 1983, PERCEPTION, V12, P99, DOI 10.1068/p120099
   LUDVIGSON HW, 1989, CHEM SENSES, V14, P525, DOI 10.1093/chemse/14.4.525
   Ma HD, 2004, IEEE T MULTIMEDIA, V6, P565, DOI 10.1109/tmm.2004.830807
   Mekuria Rufael., 2012, Proc. of the 10th European conference on Interactive tv and video-EuroiTV '12, P71, DOI DOI 10.1145/2325616.2325632
   Mikropoulos TA, 2011, COMPUT EDUC, V56, P769, DOI 10.1016/j.compedu.2010.10.020
   Montagud M., 2013, P 21 ACM INT C MULT, P323
   Murray N, 2017, IEEE T SYST MAN CY-S, V47, P2503, DOI 10.1109/TSMC.2016.2531654
   Murray N, 2016, ACM COMPUT SURV, V48, DOI 10.1145/2816454
   Murray N, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540994
   Murray Niall., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys'13, P162, DOI [https://doi.org/10.1145/2483977.2483999, DOI 10.1145/2483977.2483999]
   Nagle HT, 1998, IEEE SPECTRUM, V35, P22, DOI 10.1109/6.715180
   Narumi T, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P93
   Pan S, 2009, J TRAVEL TOUR MARK, V26, P625, DOI 10.1080/10548400903276897
   Richard E., 2006, Virtual Reality, V10, P207, DOI DOI 10.1007/S10055-006-0040-8
   Richard E, 2006, LECT NOTES COMPUT SC, V3942, P1274, DOI 10.1007/11736639_158
   Rowe LA, 2005, ACM T MULTIM COMPUT, V1, P3, DOI 10.1145/1047936.1047938
   Schauble L, 2002, LEARNING CONVERSATIONS IN MUSEUMS, P425
   Schiavone G, 2013, IEEE INT C MICROELEC, P13, DOI 10.1109/ICMTS.2013.6528138
   Steinmetz R, 1996, IEEE J SEL AREA COMM, V14, P61, DOI 10.1109/49.481694
   Tijou A, 2006, LECT NOTES COMPUT SC, V3942, P1223, DOI 10.1007/11736639_152
   Timmerer C, 2014, T-LAB SER TELECOMMUN, P351, DOI 10.1007/978-3-319-02681-7_24
   Washburn D. A., 2003, MODELING SIMULATION, V2, P19
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
NR 43
TC 22
Z9 23
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2016
VL 12
IS 4
AR 51
DI 10.1145/2957753
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DV4EH
UT WOS:000382877500005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Schaber, P
   Kopf, S
   Wetzel, S
   Ballast, T
   Wesch, C
   Effelsberg, W
AF Schaber, Philipp
   Kopf, Stephan
   Wetzel, Sina
   Ballast, Tyler
   Wesch, Christoph
   Effelsberg, Wolfgang
TI CamMark: Analyzing, Modeling, and Simulating Artifacts in Camcorder
   Copies
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Security; Digital watermarking; camcorder copy; video
   watermarking benchmark; perspective distortion; lens distortion; spatial
   re-sampling; temporal re-sampling; frame blending; rolling shutter;
   display synchronization; automatic gain control; automatic white
   balance; bayer CFA; moire
ID BENCHMARK
AB To support the development of any system that includes the generation and evaluation of camcorder copies, as well as to provide a common benchmark for robustness against camcorder copies, we present a tool to simulate digital video re-acquisition using a digital video camera. By resampling each video frame, we simulate the typical artifacts occurring in a camcorder copy: geometric modifications (aspect ratio changes, cropping, perspective and lens distortion), temporal sampling artifacts (due to different frame rates, shutter speeds, rolling shutters, or playback), spatial and color subsampling (rescaling, filtering, Bayer color filter array), and processing steps (automatic gain control, automatic white balance). We also support the simulation of camera movement (e.g., a hand-held camera) and background insertion. Furthermore, we allow for an easy setup and calibration of all the simulated artifacts, using sample/reference pairs of images and videos. Specifically temporal subsampling effects are analyzed in detail to create realistic frame blending artifacts in the simulated copies. We carefully evaluated our entire camcorder simulation system and found that the models we developed describe and match the real artifacts quite well.
C1 [Schaber, Philipp; Kopf, Stephan; Wetzel, Sina; Ballast, Tyler; Wesch, Christoph; Effelsberg, Wolfgang] Univ Mannheim, Dept Comp Sci 4, D-68131 Mannheim, Germany.
C3 University of Mannheim
RP Schaber, P (corresponding author), Univ Mannheim, Dept Comp Sci 4, D-68131 Mannheim, Germany.
EM schaber@informatik.uni-mannheim.de
RI Wetzel, Sina/IWV-2499-2023
OI Kopf, Stephan/0000-0002-1140-6685
CR Bayer B. E., 1976, U.S. patent, Patent No. 3,971,065
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Davies A., 2001, Digital Imaging for Photographers, V4th
   Farrell J., 2012, APPL OPTICS, V15, P4
   Farrell JE, 2004, P SOC PHOTO-OPT INS, V5294, P124
   Guitart O, 2006, PROC SPIE, V6072, DOI 10.1117/12.648161
   Hernandez-Avalos PA, 2010, MIDWEST SYMP CIRCUIT, P628, DOI 10.1109/MWSCAS.2010.5548906
   Kolb C., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P317, DOI 10.1145/218380.218463
   Kopf S., 2010, PROC ACM SIGMM C MUL, P23
   Kutter M, 1999, PROC SPIE, V3657, P226, DOI 10.1117/12.344672
   Maeda PY, 2005, P SOC PHOTO-OPT INS, V5678, P48, DOI 10.1117/12.588153
   Pereira S., 2001, INFORM HIDING, P340, DOI [10.1007/3-540-45496-925, DOI 10.1007/3-540-45496-925]
   Petitcolas FAP, 1998, LECT NOTES COMPUT SC, V1525, P218
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   Ramanath R, 2002, J ELECTRON IMAGING, V11, P306, DOI 10.1117/1.1484495
   Riemersma T., 2000, Windows Developers Journal, V11, P8
   Schaber Philipp, 2014, P 5 ACM MULT SYST C, P91, DOI DOI 10.1145/2557642.2557644
   Solachidis V, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P1023, DOI 10.1109/ICIP.2001.958300
   Weng CC, 2005, IEEE INT SYMP CIRC S, P3801
   Zid Cherif Ben, 2013, P SPIE MEDIA WATERMA, V8665
NR 20
TC 13
Z9 14
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2015
VL 11
IS 2
SU S
SI SI
AR 42
DI 10.1145/2700295
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CC7RV
UT WOS:000350567000002
DA 2024-07-18
ER

PT J
AU Chang, SF
AF Chang, Shih-Fu
TI How Far We've Come: Impact of 20 Years of Multimedia Information
   Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Content-based image retrieval; video retrieval; music
   retrieval; multimedia information retrieval
ID IMAGE RETRIEVAL
AB This article reviews the major research trends that emerged in the last two decades within the broad area of multimedia information retrieval, with a focus on the ACM Multimedia community. Trends are defined (nonscientifically) to be topics that appeared in ACM multimedia publications and have had a significant number of citations. The article also assesses the impacts of these trends on real-world applications. The views expressed are subjective and likely biased but hopefully useful for understanding the heritage of the community and stimulating new research direction.
C1 Columbia Univ, New York, NY 10027 USA.
C3 Columbia University
RP Chang, SF (corresponding author), Columbia Univ, 500 W 120th St,SW Mudd Bldg Rm 1312, New York, NY 10027 USA.
EM shih.fu.chang@columbia.edu
CR Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
NR 2
TC 4
Z9 4
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2013
VL 9
IS 1
SU S
AR 42
DI 10.1145/2491844
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 241SJ
UT WOS:000326187700012
DA 2024-07-18
ER

PT J
AU Dong, J
   Cheng, B
   Chen, XY
   Chua, TS
   Yan, SC
   Zhou, X
AF Dong, Jian
   Cheng, Bin
   Chen, Xiangyu
   Chua, Tat-Seng
   Yan, Shuicheng
   Zhou, Xi
TI Robust Image Annotation via Simultaneous Feature and Sample Outlier
   Pursuit
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Low-Rank Representation;
   Sample and feature outlier removal
AB Graph-based semi-supervised image annotation has achieved great success in a variety of studies, yet it essentially and intuitively suffers from both the irrelevant/noisy features (referred to as feature outliers) and the unusual/corrupted samples (referred to as sample outliers). In this work, we investigate how to derive robust sample affinity matrix via simultaneous feature and sample outlier pursuit. This task is formulated as a Dual-outlier and Prior-driven Low-Rank Representation (DP-LRR) problem, which possesses convexity in objective function. In DP-LRR, the clean data are assumed to be self-reconstructible with low-rank coefficient matrix as in LRR; while the error matrix is decomposed as the sum of a row-wise sparse matrix and a column-wise sparse matrix, the l(2,1)-norm minimization of which encourages the pursuit of feature and sample outliers respectively. The DP-LRR is further regularized by the priors from side information, that is, the inhomogeneous data pairs. An efficient iterative procedure based on linearized alternating direction method is presented to solve the DP-LRR problem, with closed-form solutions within each iteration. The derived low-rank reconstruction coefficient matrix is then fed into any graph based semi-supervised label propagation algorithm for image annotation, and as a by-product, the cleaned data from DP-LRR can also be utilized as a better image representation to generally boost image annotation performance. Extensive experiments on MIRFlickr, Corel30K, NUS-WIDE-LITE and NUS-WIDE databases well demonstrate the effectiveness of the proposed formulation for robust image annotation.
C1 [Dong, Jian] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
   [Cheng, Bin; Chen, Xiangyu; Chua, Tat-Seng; Yan, Shuicheng] Natl Univ Singapore, Singapore 117576, Singapore.
   [Zhou, Xi] Chinese Acad Sci, Beijing 100864, Peoples R China.
C3 National University of Singapore; National University of Singapore;
   Chinese Academy of Sciences
RP Zhou, X (corresponding author), Hanguo Ctr Bldg,85 Jinyu Ave, Chongqing 401122, Peoples R China.
EM a0068947@nus.edu.sg; zhouxi@cigit.ac.cn
RI Yan, Shuicheng/HCI-1431-2022; Dong, Jian/AAR-8670-2021
FU Microsoft Research Asia [R-263-000-628-597]
FX This work is supported by the grant from Microsoft Research Asia under
   R-263-000-628-597.
CR [Anonymous], 2006, P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], SIAM J OPTIM
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P C ADV NEUR INF PRO
   Belkin M., 2006, J MACH LEARN RES
   Bertsekas D.P., 2014, Constrained Optimization and Lagrange Multiplier Methods, DOI DOI 10.1016/B978-0-12-093480-5.50005-2
   BLUM A., 1997, ARTIF INTELL
   CAI D., 2010, P ACM SIGKDD INT C K
   CANDES E. J., 2011, J ACM
   CANDES E. J., 2009, FDN COMPUT MATH
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CHEN X., 2011, J MACH LEARN RES
   CHEN X., 2010, P IEEE INT C MULT
   CHEN X., 2011, P IEEE INT C COMP VI
   Cheng B., 2010, IEEE Transactions on Image Processing
   Chua Tat-Seng, 2009, CIVR
   Collobert R., 2006, Journal of Machine Learning Research
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   GAO Y., 2006, P INT C MULT
   HE B., 2011, SIAM J OPTIM
   HSIEH L.-C., 2010, P INT C AC SPEECH SI
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Kohavi R., 1997, ARTIF INTELL
   LANG C., 2012, T IMAGE PROCESS
   LI X., 2006, P INT C MULT
   Lin, 2010, ARXIV10095055
   LIU G., 2010, IEEE T PATTERN ANAL
   Liu H., 1998, FEATURE SELECTION KN, DOI [DOI 10.1007/978-1-4615-5689-3, 10.1007/978-1-4615-5689-3]
   Liu J., 2013, SLEP: Sparse Learning with Efficient Projections
   MA Z., 2011, P INT C MULT
   NI B., 2008, P IEEE INT C DAT MIN
   NIE F., 2010, P C ADV NEUR INF PRO
   PENG Y., 2011, IEEE T PATTERN ANAL
   SUBRAMANYA A., 2009, P C ADV NEUR INF PRO
   TANG J., 2007, KNOWLEDGE INF SYST
   TANG J., 2009, P INT C MULT
   TAO M., 2011, SIAM J OPTIM
   WANG C., 2009, P IEEE C COMP VIS PA
   WANG F., 2008, IEEE T KNOWL DATA EN
   WANG J., 2009, IEEE T PATTERN ANAL
   Wang X., 2008, IEEE Transactions on Pattern Analysis and Machine Intelligence
   XU H., 2010, P C ADV NEUR INF PRO
   YANG J., 2011, MATH COMPUT
   YANG Y., 2012, T IMAGE PROCESS
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   ZHAO Z., 2010, P NAT C ART INT
   Zhou D., 2003, P C ADV NEUR INF PRO
   Zhu G., 2010, P INT C MULT
   Zhu X., 2003, P 20 INT C MACH LEAR, V3, P58, DOI DOI 10.1109/18.850663
   Zhu X., 2002, CMUCALD02107, V3175, P237, DOI DOI 10.1007/978-3-540-28649-3_29
NR 51
TC 5
Z9 5
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2013
VL 9
IS 4
AR 24
DI 10.1145/2501643.2501646
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 206EW
UT WOS:000323501800002
DA 2024-07-18
ER

PT J
AU Doherty, J
   Curran, K
   Mckevitt, P
AF Doherty, Jonathan
   Curran, Kevin
   Mckevitt, Paul
TI A Self-Similarity Approach to Repairing Large Dropouts of Streamed Music
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Streaming audio; streaming media; forward error correction;
   audio repair; data compaction and compression
ID MPEG-7
AB Enjoyment of audio has now become about flexibility and personal freedom. Digital audio content can be acquired from many sources and wireless networking allows digital media devices and associated peripherals to be unencumbered by wires. However, despite recent improvements in capacity and quality of service, wireless networks are inherently unreliable communications channels for the streaming of audio, being susceptible to the effects of range, interference, and occlusion. This time-varying reliability of wireless audio transfer introduces data corruption and loss, with unpleasant audible effects that can be profound and prolonged in duration. Traditional communications techniques for error mitigation perform poorly and in a bandwidth inefficient manner in the presence of such large-scale defects in a digital audio stream. A novel solution that can complement existing techniques takes account of the semantics and natural repetition of music. Through the use of self-similarity metadata, missing or damaged audio segments can be seamlessly replaced with similar undamaged segments that have already been successfully received. We propose a technology to generate relevant self-similarity metadata for arbitrary audio material and to utilize this metadata within a wireless audio receiver to provide sophisticated and real-time correction of large-scale errors. The primary objectives are to match the current section of a song being received with previous sections while identifying incomplete sections and determining replacements based on previously received portions of the song. This article outlines our approach to Forward Error Correction (FEC) technology that is used to "repair" a bursty dropout when listening to time-dependent media on a wireless network. Using self-similarity analysis on a music file, we can "automatically" repair the dropout with a similar portion of the music already received thereby minimizing a listener's discomfort.
C1 [Doherty, Jonathan; Curran, Kevin; Mckevitt, Paul] Univ Ulster, Sch Comp & Intelligent Syst, Derry, North Ireland.
C3 Ulster University
RP Curran, K (corresponding author), Univ Ulster, Sch Comp & Intelligent Syst, Derry, North Ireland.
EM kj.curran@ulster.ac.uk
RI Curran, Kevin/AAC-4865-2019
OI Curran, Kevin/0000-0001-5237-5355
CR [Anonymous], 1987, Consciousness and the computational mind
   [Anonymous], 2004, P 8 INT C MUS PERC C
   Bartsch MA, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P15, DOI 10.1109/ASPAA.2001.969531
   Berenzweig A, 2004, COMPUT MUSIC J, V28, P63, DOI 10.1162/014892604323112257
   Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P91
   Casey M., 2002, ORGAN SOUND, V6, P153
   Chai W, 2003, ACM-IEEE J CONF DIG, P27
   CHIARIGLIONE L., 2010, DESCRIPTION MPEG 7 A
   CHINRUNGRUENG C, 1995, IEEE T NEURAL NETWOR, V6, P157, DOI 10.1109/72.363440
   Crysandt H, 2004, P SOC PHOTO-OPT INS, V5307, P117
   Dannenberg RB, 2003, J NEW MUSIC RES, V32, P153, DOI 10.1076/jnmr.32.2.153.16738
   Doraisamy S., 2004, P INT C MUSIC INFORM, P204
   Downie JS, 2004, COMPUT MUSIC J, V28, P12, DOI 10.1162/014892604323112211
   ESSID S., 2004, P 25 INT C AUD ENG S
   Foote JT, 2003, PROC SPIE, V5021, P167, DOI 10.1117/12.476302
   Gómez E, 2003, J NEW MUSIC RES, V32, P23, DOI 10.1076/jnmr.32.1.23.16799
   Jackson I., 2008, SONG FORMS TERMS QUI
   KAMATA M., 2007, P INT C ADV COMP ENT, P196
   Kim HG, 2004, IEEE T CIRC SYST VID, V14, P716, DOI 10.1109/TCSVT.2004.826766
   KRIEGEL H. ., 2005, P 9 PAC AS C ADV KNO
   LEMAN M., 2002, FORUM ACUSTICUM SEVI, V33, P1
   Lukasiak J, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P273
   Martínez JM, 2002, IEEE MULTIMEDIA, V9, P78, DOI 10.1109/93.998074
   Meredith D., 2001, P 5 WORLD MULT SYST, P22
   Nafaa A, 2008, IEEE COMMUN MAG, V46, P72, DOI 10.1109/MCOM.2008.4427233
   Olson HarryFerdinand., 1967, Music, Physics and Engineering
   PAN D, 1995, IEEE MULTIMEDIA, V2, P60, DOI 10.1109/93.388209
   PEETERS G, 2002, P INT C MUS INF RETR, P98
   PEETERS G., 2002, INFORM RETRIEVAL, P98
   Perkins C, 1998, IEEE NETWORK, V12, P40, DOI 10.1109/65.730750
   Seo JS, 2005, INT CONF ACOUST SPEE, P213
   STEINBACH M., 2000, P KDD WORKSH TEXT MI, P34
   Tao D.-C., 2004, P 12 ANN ACM INT C M, P464
   YIN J., AD HOC NETW, V4, P651
   Zha HY, 2002, ADV NEUR IN, V14, P1057
NR 35
TC 1
Z9 1
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2013
VL 9
IS 3
AR 20
DI 10.1145/2487268.2487273
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 175GL
UT WOS:000321218800005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, J
   Liu, HM
   Huang, JW
   Shi, YQ
AF Li, Jian
   Liu, Hongmei
   Huang, Jiwu
   Shi, Yun Q.
TI Reference Index-Based H.264 Video Watermarking Scheme
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Management; Security; Watermarking; video; H.264; reference
   index
ID DIGITAL VIDEO
AB Video watermarking has received much attention over the past years as a promising solution to copy protection. Watermark robustness is still a key issue of research, especially when a watermark is embedded in the compressed video domain. In this article, a robust watermarking scheme for H.264 video is proposed. During video encoding, the watermark is embedded in the index of the reference frame, referred to as reference index, a bitstream syntax element newly proposed in the H.264 standard. Furthermore, the video content (current coded blocks) is modified based on an optimization model, aiming at improving watermark robustness without unacceptably degrading the video's visual quality or increasing the video's bit rate. Compared with the existing schemes, our method has the following three advantages: (1) The bit rate of the watermarked video is adjustable; (2) the robustness against common video operations can be achieved; (3) the watermark embedding and extraction are simple. Extensive experiments have verified the good performance of the proposed watermarking scheme.
C1 [Li, Jian; Liu, Hongmei; Huang, Jiwu] Sun Yat Sen Univ, Guangzhou 510006, Guangdong, Peoples R China.
   [Shi, Yun Q.] New Jersey Inst Technol, Newark, NJ 07102 USA.
C3 Sun Yat Sen University; New Jersey Institute of Technology
RP Huang, JW (corresponding author), State Key Lab Informat Secur, Beijing 100190, Peoples R China.
EM isshjw@gmail.sysu.edu.cn
RI Shi, Yun/JWP-3360-2024; huang, jw/KVY-9917-2024
FU 973 Program in China [2011CB302204]; NSFC [U1135001, 61272498]
FX The work was supported in part by 973 Program (2011CB302204) in China
   and NSFC (U1135001, 61272498).
CR Alattar AM, 2003, IEEE T CIRC SYST VID, V13, P787, DOI 10.1109/TCSVT.2003.815958
   [Anonymous], 2003, H.264 and MPEG-4 video compression: video coding for next generation multimedia
   Barni M, 2005, IEEE T MULTIMEDIA, V7, P23, DOI 10.1109/TMM.2004.840594
   Bodo Y, 2004, EURASIP J APPL SIG P, V2004, P2224, DOI 10.1155/S1110865704401061
   Boyd S., 2004, CONVEX OPTIMIZATION
   Golikeri A, 2007, J ELECTRON IMAGING, V16, DOI 10.1117/1.2816054
   Hartung F, 1998, SIGNAL PROCESS, V66, P283, DOI 10.1016/S0165-1684(98)00011-5
   JORDAN F, 1997, MPEG97M2281 ISOIEC J
   KERRIS N., 2007, YOUTUBE LIVE APPLE T
   Koz A, 2008, IEEE T CIRC SYST VID, V18, P326, DOI 10.1109/TCSVT.2008.918446
   Langelaar GC, 2001, IEEE T IMAGE PROCESS, V10, P148, DOI 10.1109/83.892451
   Liu Z, 2004, 2004 7TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS 1-3, P2358
   Mansouri A, 2010, IEEE T INF FOREN SEC, V5, P649, DOI 10.1109/TIFS.2010.2076280
   Mohaghegh N, 2008, 2008 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING, VOLS 1 AND 2, PROCEEDINGS, P1384, DOI 10.1109/ICALIP.2008.4590217
   Nguyen CV, 2006, 2006 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, P81
   Noorkami M, 2008, IEEE T INF FOREN SEC, V3, P441, DOI 10.1109/TIFS.2008.923825
   Noorkami M, 2007, IEEE T INF FOREN SEC, V2, P14, DOI 10.1109/TIFS.2006.890306
   Qiu G, 2004, INT C PATT RECOG, P865
   SHRING K., 2007, H 264 AVC SOFTWARE C
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   VideoLAN, 2010, X264 FREE H264 AVC E
   Wang Y., 2001, VIDEO PROCESSING COM
   WATSON AB, 1993, P SOC PHOTO-OPT INS, V1913, P202, DOI 10.1117/12.152694
   WEISSTEIN E. W., 2010, VORONOI DIAGRAM
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xingguang Song, 2008, 2008 11th IEEE International Conference on Communication Technology (ICCT 2008), P738, DOI 10.1109/ICCT.2008.4716228
   Zhang J, 2001, XIV BRAZILIAN SYMPOSIUM ON COMPUTER GRAPHICS AND IMAGE PROCESSING, PROCEEDINGS, P179, DOI 10.1109/SIBGRAPI.2001.963053
   Zhang J, 2007, IEEE T CIRCUITS-II, V54, P205, DOI 10.1109/TCSII.2006.886247
   Zou DK, 2008, INT CONF ACOUST SPEE, P1749, DOI 10.1109/ICASSP.2008.4517968
NR 29
TC 4
Z9 4
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2012
VL 8
IS 2
SU S
AR 33
DI 10.1145/2344436.2344439
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 011KE
UT WOS:000309162800003
DA 2024-07-18
ER

PT J
AU Ghinea, G
   Ademoye, O
AF Ghinea, Georghita
   Ademoye, Oluwakemi
TI The Sweet Smell of Success: Enhancing Multimedia Applications with
   Olfaction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Experimentation; Human Factors; Olfaction; Human-computer
   interaction; multimedia quality; quality of perception
ID DISPLAY
AB Olfaction, or smell, is one of the last challenges which multimedia applications have to conquer. As far as computerized smell is concerned, there are several difficulties to overcome, particularly those associated with the ambient nature of smell. In this article, we present results from an empirical study exploring users' perception of olfaction-enhanced multimedia displays. Findings show that olfaction significantly adds to the user multimedia experience. Moreover, use of olfaction leads to an increased sense of reality and relevance. Our results also show that users are tolerant of the interference and distortion effects caused by olfactory effect in multimedia.
C1 [Ghinea, Georghita; Ademoye, Oluwakemi] Brunel Univ, Uxbridge UB8 3PH, Middx, England.
C3 Brunel University
RP Ghinea, G (corresponding author), Brunel Univ, Uxbridge UB8 3PH, Middx, England.
EM george.ghinea@brunei.ac.uk
RI Ghinea, Gheorghita/AAG-6770-2020
OI Ghinea, Gheorghita/0000-0003-2578-5580
CR [Anonymous], P 8 INT C HUM COMP I
   [Anonymous], P CHI 2006
   ATER J. P., 1992, PRESENCE, V1, P493
   Bodnar A, 2004, P 6 INT C MULTIMODAL, P183, DOI [10.1145/1027933.1027965, DOI 10.1145/1027933.1027965]
   Boyd-Davis S., 2006, P HUM FACT ERG SOC 2, P25
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   DROLET E, 2009, P IEEE VIRTL REAL C, P271
   Ghinea G., 1998, Proceedings ACM Multimedia 98, P49, DOI 10.1145/290747.290754
   GULAS CS, 1995, J BUS PSYCHOL, V10, P87, DOI 10.1007/BF02249272
   Gulliver SR, 2006, ACM T MULTIM COMPUT, V2, P241, DOI 10.1145/1201730.1201731
   HASTRETTE M., 2002, OLFACTION TASTE COGN, P100
   Herz RS, 2002, OLFACTION, TASTE, AND COGNITION, P160, DOI 10.1017/CBO9780511546389.016
   Jones L, 2004, HUMAN PERFORMANCE, SITUATION AWARENESS AND AUTOMATION: CURRENT RESEARCH AND TRENDS, VOL 2, P282
   Jumisko-Pyykko S., 2007, P SPIE, V6507
   Kato Y, 2006, PROCEEDINGS OF THE IASTED INTERNATIONAL CONFERENCE ON INTERNET AND MULTIMEDIA SYSTEMS AND APPLICATIONS, P203
   Kaye J."J."., 2004, INTERACTIONS, V11, P48, DOI DOI 10.1145/962342.964333
   Kaye J.N., 2001, THESIS MIT
   Köster EP, 2002, OLFACTION, TASTE, AND COGNITION, P27, DOI 10.1017/CBO9780511546389.007
   Nakamoto T, 2008, IEEE COMPUT GRAPH, V28, P75, DOI 10.1109/MCG.2008.3
   Nakamoto T, 2006, IEICE T FUND ELECTR, VE89A, P3327, DOI 10.1093/ietfec/e89-a.l 1.3327
   Richard E., 2006, Virtual Reality, V10, P207, DOI DOI 10.1007/S10055-006-0040-8
   Saito S, 2006, CHEM SENSES, V31, P379, DOI 10.1093/chemse/bjj042
   Serif T, 2004, P ACM S APPL COMP, P1580
   TIJOU A., 2006, P 1 INT C TECHN E LE, P1223
   Washburn D A., 2003, Model. Simul. Mag, V2
   Washburn DA, 2004, COMPUT SCI ENG, V6, P80, DOI 10.1109/MCSE.2004.66
   ZYBURA M., 1999, 543 U WASH IND ENG
NR 27
TC 59
Z9 66
U1 0
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2012
VL 8
IS 1
AR 2
DI 10.1145/2071396.2071398
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 894AV
UT WOS:000300400200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hefeeda, M
   Hsu, CH
AF Hefeeda, Mohamed
   Hsu, Cheng-Hsin
TI Design and Evaluation of a Testbed for Mobile TV Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Mobile TV; DVB-H; testbed; energy saving; mobile multimedia;
   wireless streaming; channel switching delay; broadcast networks
ID IN TIME REDUCTION; DVB-H; BROADCAST SERVICES; SYSTEM
AB This article presents the design of a complete, open-source, testbed for broadcast networks that offer mobile TV services. Although basic architectures and protocols have been developed for such networks, detailed performance tuning and analysis are still needed, especially when these networks scale to serve many diverse TV channels to numerous subscribers. The detailed performance analysis could also motivate designing new protocols and algorithms for enhancing future mobile TV networks. Currently, many researchers evaluate the performance of mobile TV networks using simulation and/or theoretical modeling methods. These methods, while useful for early assessment, typically abstract away many necessary details of actual, fairly complex, networks. Therefore, an open-source platform for evaluating new ideas in a real mobile TV network is needed. This platform is currently not possible with commercial products, because they are sold as black boxes without the source code. In this article, we summarize our experiences in designing and implementing a testbed for mobile TV networks. We integrate off-the-shelf hardware components with carefully designed software modules to realize a scalable testbed that covers almost all aspects of real networks. We use our testbed to empirically analyze various performance aspects of mobile TV networks and validate/refute several claims made in the literature as well as discover/quantify multiple important performance tradeoffs.
C1 [Hefeeda, Mohamed; Hsu, Cheng-Hsin] Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
   [Hsu, Cheng-Hsin] Natl Tsing Hua Univ, Hsinchu, Taiwan.
C3 Simon Fraser University; National Tsing Hua University
RP Hefeeda, M (corresponding author), Simon Fraser Univ, Sch Comp Sci, 250-13450 102nd Ave, Surrey, BC V3T 0A3, Canada.
EM mhefeeda@cs.sfu.ca
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council (BCIC); Nokia Canada
FX This work is partially supported by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada, the British Columbia Innovation
   Council (BCIC), and Nokia Canada.
CR [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 102377 ETSI EN
   [Anonymous], SPECTR LP49 DTV IND
   [Anonymous], 1996, WIRELESS COMMUNICATI
   [Anonymous], DVBSAM DVB H SOL AN
   [Anonymous], X264 FREE H264 AVC E
   [Anonymous], 2004, 302304 ETSI EN
   [Anonymous], WINTV NOVA T USB STI
   [Anonymous], P ACM MULT 08 DEM SE
   [Anonymous], 2004, 300744V151 ETSI EN
   [Anonymous], P IEEE WIR COMM NETW
   [Anonymous], 102468 ETSI EN
   [Anonymous], P JOINT I WORKSH MOB
   [Anonymous], 300401 ETSI EN
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], DEKT DTA 110T PCI MO
   [Anonymous], P IEEE INT PACK VID
   [Anonymous], DIV CATCH RF T H TRA
   [Anonymous], MOB TV EUR COMM END
   [Anonymous], 2009, FLO TECHNOLOGY OVERV
   [Anonymous], EN 1 WATT RF POW AMP
   Buchinger S., 2009, P EUROITV 09, P179, DOI http://doi.acm.org/10.1145/1542084.1542121
   Cho S, 2007, IEEE T BROADCAST, V53, P171, DOI 10.1109/TBC.2007.891712
   Faria G, 2006, P IEEE, V94, P194, DOI 10.1109/JPROC.2005.861011
   Furht B, 2008, INTERNET COMMUN, P1, DOI 10.1201/9781420053890
   Hartung F, 2007, IEEE T BROADCAST, V53, P188, DOI 10.1109/TBC.2007.891711
   Hefeeda M, 2010, IEEE ACM T NETWORK, V18, P610, DOI 10.1109/TNET.2009.2030326
   KORNFELD M., 2007, P IEEE INT S CONSUME, P1
   Kornfeld M, 2007, IEEE T BROADCAST, V53, P161, DOI 10.1109/TBC.2006.889210
   Rezaei M, 2006, IEEE IMAGE PROC, P3041, DOI 10.1109/ICIP.2006.313081
   Rezaei M, 2007, IEEE T BROADCAST, V53, P320, DOI 10.1109/TBC.2006.889682
   Rezaei M, 2008, IEEE T MULTIMEDIA, V10, P1455, DOI 10.1109/TMM.2008.2007315
   Takada M, 2006, P IEEE, V94, P251, DOI 10.1109/JPROC.2005.859692
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
NR 34
TC 1
Z9 1
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2012
VL 8
IS 1
AR 3
DI 10.1145/2071396.2071399
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 894AV
UT WOS:000300400200003
OA Bronze
DA 2024-07-18
ER

PT J
AU Mei, T
   Li, LS
   Hua, XS
   Li, SP
AF Mei, Tao
   Li, Lusong
   Hua, Xian-Sheng
   Li, Shipeng
TI ImageSense: Towards Contextual Image Advertising
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Human Factors
AB The daunting volumes of community-contributed media contents on the Internet have become one of the primary sources for online advertising. However, conventional advertising treats image and video advertising as general text advertising by displaying relevant ads based on the contents of the Web page, without considering the inherent characteristics of visual contents. This article presents a contextual advertising system driven by images, which automatically associates relevant ads with an image rather than the entire text in a Web page and seamlessly inserts the ads in the nonintrusive areas within each individual image. The proposed system, called ImageSense, supports scalable advertising of, from root to node, Web sites, pages, and images. In ImageSense, the ads are selected based on not only textual relevance but also visual similarity, so that the ads yield contextual relevance to both the text in the Web page and the image content. The ad insertion positions are detected based on image salience, as well as face and text detection, to minimize intrusiveness to the user. We evaluate ImageSense on a large-scale real-world images and Web pages, and demonstrate the effectiveness of ImageSense for online image advertising.
C1 [Mei, Tao; Hua, Xian-Sheng; Li, Shipeng] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Li, Lusong] Beihang Univ, Beijing, Peoples R China.
C3 Microsoft Research Asia; Microsoft; Beihang University
RP Mei, T (corresponding author), Microsoft Res Asia, Bldg 2,5 Dan Ling St, Beijing 100080, Peoples R China.
EM tmei@microsoft.com; xshua@microsoft.com; spli@microsoft.com
RI Mei, Tao/GQZ-0596-2022; Li, Shipeng/AAA-3374-2020
OI Mei, Tao/0000-0002-5990-7307; Li, Shipeng/0000-0001-5368-4256
CR ADAMS B., 2006, P ACM MULT
   AHLERS D, 2008, P INT C MULT MOD
   [Anonymous], 1976, PHOTOGRAPHIC COMPOSI
   [Anonymous], 2007, P 15 ACM INT C MULT
   [Anonymous], The New York Times
   [Anonymous], Adwords
   [Anonymous], P INT WORLD WID WEB
   Aroyo L, 2007, 2007 INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P298
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boyd S., 2004, CONVEX OPTIMIZATION
   BRODER A., 2007, P ANN ACM SIGIR C RE
   CAI D, 2003, MSRTR200379 MICR
   Chatterjee P, 2003, MARKET SCI, V22, P520, DOI 10.1287/mksc.22.4.520.24906
   Chen XR, 2001, LECT NOTES COMPUT SC, V2195, P222
   CHEN Z, 2007, P INT WORKSH DAT MIN
   GERRIG RJ, 2001, PSYCHOL LIFE
   GOODMAN J, 2006, P INT WORLD WID WEB
   He XF, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1230812.1230816
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   JOSHI A, 2006, P WORKSH IEEE INT C
   KASTIDOU G, 2006, P EUR C INT TEL
   Kennedy L., 2007, P ACM MULT
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   LACERDA A., 2006, P ANN ACM SIGIR C RE
   Li SZ, 2002, LECT NOTES COMPUT SC, V2353, P67
   Liao Wei-Shing., 2008, P 31 ANN INT ACM SIG, P767, DOI [10.1145/1390334.1390494, DOI 10.1145/1390334.1390494]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma, 2008, P ACM MULT, P1051
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   McCoy S, 2007, COMMUN ACM, V50, P84, DOI 10.1145/1226736.1226740
   Mehta A, 2007, J ACM, V54, DOI 10.1145/1284320.1284321
   Mei T., 2008, P 16 ACM INT C MULT, P439, DOI [https://doi.org/10.1145/1459359.1459418, DOI 10.1145/1459359.1459418]
   Mei T., 2007, TREC VID RETR EV ONL
   Mei T, 2009, IEEE T CIRC SYST VID, V19, P1866, DOI 10.1109/TCSVT.2009.2026949
   Mei Tao., 2007, Proceedings of the 15th International Conference on Multimedia, P1075
   Murdock V., 2007, P INT WORKSH DAT MIN
   RIBEIRO-NETO B., 2005, P ANN ACM SIGIR C RE
   RICHARDSON M., 2007, P INT WORLD WID WEB
   SHEN D., 2006, P ANN ACM SIGIR C RE
   WHITLEY D, 1994, STAT COMPUT, V4, P65, DOI 10.1007/BF00175354
   ZHAO L., 2006, P INT WORLD WID WEB
   ADSENSE
NR 43
TC 24
Z9 27
U1 1
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2012
VL 8
IS 1
AR 6
DI 10.1145/2071396.2071402
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 894AV
UT WOS:000300400200006
DA 2024-07-18
ER

PT J
AU Sarhan, NJ
   Alsmirat, MA
   Al-Hadrusi, M
AF Sarhan, Nabil J.
   Alsmirat, Mohammad A.
   Al-Hadrusi, Musab
TI Waiting-Time Prediction in Scalable On-Demand Video Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Performance; Scheduling; stream merging; time-of-service
   guarantees; video streaming; waiting-time prediction
AB Providing video streaming users with expected waiting times enhances their perceived quality-of-service (QoS) and encourages them to wait. In the absence of any waiting-time feedback, users are more likely to defect because of the uncertainty as to when their services will start. We analyze waiting-time predictability in scalable video streaming. We propose two prediction schemes and study their effectiveness when applied with various stream merging techniques and scheduling policies. The results demonstrate that the waiting time can be predicted accurately, especially when enhanced cost-based scheduling is applied. The combination of waiting-time prediction and cost-based scheduling leads to outstanding performance benefits.
C1 [Sarhan, Nabil J.; Alsmirat, Mohammad A.; Al-Hadrusi, Musab] Wayne State Univ, Dept Elect & Comp Engn, Media Res Lab, Detroit, MI 48202 USA.
C3 Wayne State University
RP Sarhan, NJ (corresponding author), Wayne State Univ, Dept Elect & Comp Engn, Media Res Lab, Detroit, MI 48202 USA.
EM nabil@wayne.edu; msmirat@wayne.edu; hadrusi@wayne.edu
OI Alsmirat, Mohammad/0000-0002-1071-7713; Sarhan,
   Nabil/0000-0002-0527-5666
FU NSF [CNS-0626861, CNS-0834537]
FX This work was supported in part by NSF grants CNS-0626861 and
   CNS-0834537.
CR Aggarwal CC, 2001, IEEE T COMPUT, V50, P97, DOI 10.1109/12.908987
   ALHADRUSI M, 2008, P SPIE ACM MULT COMP
   ALSMIRAT M, 2007, P ACM MULT SEP, P791
   [Anonymous], P ACM WWW 04
   Bar-Noy A, 2004, MULTIMEDIA SYST, V9, P411, DOI 10.1007/s00530-003-0114-3
   Cai Y, 2003, MULTIMED TOOLS APPL, V21, P125, DOI 10.1023/A:1025516608573
   Cai Y, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P211
   Cai Y., 2003, P 9 INT C DISTR MULT, P72
   Carter SW, 1997, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATIONS AND NETWORKS, PROCEEDINGS, P200, DOI 10.1109/ICCCN.1997.623313
   Dan Asit, 1994, P ACM MULT, P391
   Eager D, 2001, IEEE T KNOWL DATA EN, V13, P742, DOI 10.1109/69.956098
   Eager D, 2000, PROC SPIE, V3969, P206
   Eager D, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P199, DOI 10.1145/319463.319601
   Hua K., 1997, PROC SIGCOMM, P89
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   Huang C., 2004, MULTIMEDIA '04, P152
   Juhn LS, 1997, IEEE T BROADCAST, V43, P268, DOI 10.1109/11.632927
   Ma HD, 2005, MULTIMED TOOLS APPL, V26, P101, DOI 10.1007/s11042-005-6851-x
   Paris JF, 1998, SIXTH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P127, DOI 10.1109/MASCOT.1998.693685
   QUDAH B, 2006, P ACM MULT OCT, P347
   ROCHA M, 2005, P ACM MULT NOV, P966
   SARHAN NJ, 2007, P MULT COMP NETW C M
   SARHAN NJ, 2004, P 7 IFIP IEEE INT C, P127
   SHI L, 2006, P ACM MULT OCT, P337
   TSIOLIS AK, 1997, P ACM SIGMETRICS C M, P285
NR 25
TC 4
Z9 4
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR 10
PY 2010
VL 6
IS 2
AR 11
DI 10.1145/1671962.1671967
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 579PA
UT WOS:000276382700005
DA 2024-07-18
ER

PT J
AU Sivaram, GSVS
   Kankanhalli, MS
   Ramakrishnan, KR
AF Sivaram, G. S. V. S.
   Kankanhalli, Mohan S.
   Ramakrishnan, K. R.
TI Design of Multimedia Surveillance Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Security; Performance vector; sensor selection and placement
ID INTEGRATION; FUSION
AB This article addresses the problem of how to select the optimal combination of sensors and how to determine their optimal placement in a surveillance region in order to meet the given performance requirements at a minimal cost for a multimedia surveillance system. We propose to solve this problem by obtaining a performance vector, with its elements representing the performances of subtasks, for a given input combination of sensors and their placement. Then we show that the optimal sensor selection problem can be converted into the form of Integer Linear Programming problem (ILP) by using a linear model for computing the optimal performance vector corresponding to a sensor combination. Optimal performance vector corresponding to a sensor combination refers to the performance vector corresponding to the optimal placement of a sensor combination. To demonstrate the utility of our technique, we design and build a surveillance system consisting of PTZ (Pan-Tilt-Zoom) cameras and active motion sensors for capturing faces. Finally, we show experimentally that optimal placement of sensors based on the design maximizes the system performance.
C1 [Sivaram, G. S. V. S.] Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA.
   [Kankanhalli, Mohan S.] Natl Univ Singapore, Singapore 117548, Singapore.
   [Ramakrishnan, K. R.] Indian Inst Sci, Bangalore 560012, Karnataka, India.
C3 Johns Hopkins University; National University of Singapore; Indian
   Institute of Science (IISC) - Bangalore
RP Sivaram, GSVS (corresponding author), Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA.
EM sivaram@jhu.edu
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
CR Bodor R, 2007, J INTELL ROBOT SYST, V50, P257, DOI 10.1007/s10846-007-9164-7
   CHEN X, 2000, CSTR200007 STANF U D
   Chen XJ, 2004, J DRUG ISSUES, V34, P1, DOI 10.1177/002204260403400101
   Dhillon SS, 2003, IEEE WCNC, P1609
   Erdem U.M., 2004, Proceedings of the International Workshop on Omnidirectional Vision, Camera Networks and Non-classical Cameras (OMNIVIS), P30
   Hörster E, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1257, DOI 10.1109/ICME.2006.262766
   Howard A, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P434, DOI 10.1109/IRDS.2002.1041428
   Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420
   LUO RC, 1989, IEEE T SYST MAN CYB, V19, P901, DOI 10.1109/21.44007
   Luo RC, 2002, IEEE SENS J, V2, P107, DOI 10.1109/JSEN.2002.1000251
   Mittal A, 2004, LECT NOTES COMPUT SC, V3021, P175
   Pahalawatta PV, 2004, IEEE IMAGE PROC, P3073
   Schrijver A., 1998, THEORY LINEAR INTEGE
   Scott WR, 2003, ACM COMPUT SURV, V35, P64, DOI 10.1145/641865.641868
   SIVARAM G, 2006, P ACM INT WORKSH VID, P149
   TARABANIS KA, 1995, IEEE T ROBOTIC AUTOM, V11, P86, DOI 10.1109/70.345940
   Wren C.R., 2005, P INT WORKSHOP VIDEO, P113
   ZIERHUT H, Patent No. 4529874
NR 18
TC 11
Z9 12
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2009
VL 5
IS 3
AR 23
DI 10.1145/1556134.1556140
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 504DV
UT WOS:000270595600006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sakr, Z
   Georganas, ND
AF Sakr, Ziad
   Georganas, Nicolas D.
TI Robust content-based MPEG-4 XMT scene structure authentication and
   multimedia content location
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; design; experimentation; security; verification; multimedia;
   MPEG-4; polynomial; pseudorandom sequences; steganography; VRML;
   watermarking; XML; XMT
AB For the past decade, there have been numerous research works focusing on the protection of digital images, audio, video, 3D virtual scenes, and software data from unauthorized use and distribution. With the emerging technology of the MPEG-4 standard, MPEG-4 scenes that may include images, video, audio, and 3D objects can easily be built using the text-based MPEG-4 XMT standard. XMT allows content authors to exchange their content with other authors, tools, or service providers and facilitates interoperability with MPEG-4, X3D, and SMIL. In order for owners and designers to protect and/or authenticate their work, some form of security needs to be applied into the MPEG-4 XMT structure and its media content. Unlike images or videos, watermarking an XMT structure is not an easy task, since the structure contains no noise components to embed the watermark. This article is the first one proposing a novel robust algorithm for the authentication of a given MPEG-4 XMT structured scene and the location of its multimedia content.
C1 Univ Ottawa, Dept Elect & Comp Engn, Ottawa, ON K1N 6N5, Canada.
C3 University of Ottawa
RP Sakr, Z (corresponding author), Univ Ottawa, Dept Elect & Comp Engn, Ottawa, ON K1N 6N5, Canada.
EM ziad.sakr@utt.edu.tt; georganas@discover.uottawa.ca
CR [Anonymous], 1999, 144961 ISO IEC
   [Anonymous], P S GRAPHS COMBINATO
   [Anonymous], J MATH RES EXPOSITIO
   [Anonymous], P NAT WORKSH GRAPH T
   [Anonymous], 1991, GRAPH COMBINATORICS
   Babai Laszlo, 1983, P 15 ANN ACM S THEOR, P171, DOI [10.1145/800061.808746, DOI 10.1145/800061.808746]
   BALL JR, 1975, MATH COMPUT, V11, P107
   BEARD JTB, 1974, MATH COMPUT, V28, P1159, DOI 10.2307/2005374
   Berlekamp ER., 1968, Algebraic coding theory mcgraw-hill
   CARTER DE, 1974, IEEE T AERO ELEC SYS, VAE10, P898, DOI 10.1109/TAES.1974.307905
   CUMMING IG, 1967, P I ELECTR ENG, V114, P1360, DOI 10.1049/piee.1967.0264
   DARLAGIANNIS V, 2000, P 4 WORLD MULT CIRC
   FREDRICSSON SA, 1975, IEEE T INFORM THEORY, V21, P115, DOI 10.1109/TIT.1975.1055310
   Golomb S.W., 1972, GRAPHTHEORYANDCOMPUT, P23
   HARVEY JT, 1974, ELECTRON LETT, V10, P480, DOI 10.1049/el:19740382
   HOSSEINI M, 2001, IEEE INT WORKSH EN T
   Inoue S., 2002, P 2002 S CRYPT INF S, P301
   INOUE S, 2001, 1 WORKSH NLP XML NOV
   *ISO IEC, 1997, 147721 ISO IEC
   KIM M, 2000, P ACM WORKSH MULT, P71
   Lang A, 2003, PROC SPIE, V5020, P452, DOI 10.1117/12.476850
   Petriu EM, 2000, IEEE IMTC P, P1237, DOI 10.1109/IMTC.2000.848675
   ROBERTS PD, 1966, P I ELECTR ENG, V113, P190, DOI 10.1049/piee.1966.0025
   Sakr Z, 2004, 14TH INTERNATIONAL WORKSHOP ON RESEARCH ISSUES ON DATA ENGINEERING: WEB SERVICES FOR E-COMMERCE AND E-GOVERNMENT APPLICATIONS, PROCEEDINGS, P48, DOI 10.1109/RIDE.2004.1281702
   SANZ Z, 2004, P CAN C EL COMP ENG
   SION R, 2001, 200154 CERIAS TR
   Walsh A. E., 2002, MPEG 4 JUMP START
   2001, XML EXTENSIBLE MARKU
   1998, SMIL SYNCHRONIZED MU
   1998, VRML MPEG4 WG MAIN D
NR 30
TC 5
Z9 5
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 3
AR 18
DI 10.1145/1236471.1236477
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230JB
UT WOS:000250871700006
DA 2024-07-18
ER

PT J
AU Mayer-Patel, K
   Smith, BC
   Rowe, LA
AF Mayer-Patel, Ketan
   Smith, Brian C.
   Rowe, Lawrence A.
TI The Berkeley Software MPEG-1 Video Decoder
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Performance; Video compression; MPEG
AB This article reprises the description of the Berkeley software-only MPEG-1 video decoder originally published in the proceedings of the 1st International ACM Conference on Multimedia in 1993. The software subsequently became widely used in a variety of research systems and commercial products. Its main impact was to provide a platform for experimenting with streaming compressed video and to expose the strengths and weaknesses of software-only video decoding using general purpose computing architectures. This article compares the original performance results with experiments run on a modern processor to demonstrate the gains of processing power in the past ten years relative to this specific application and discusses the history of MPEG-1 video software decoding and the Berkeley MPEG research group.
C1 [Mayer-Patel, Ketan] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA.
   [Smith, Brian C.] TippingPoint Technol Inc, Austin, TX 78731 USA.
   [Rowe, Lawrence A.] Univ Calif Berkeley, Comp Sci Div EECS, Berkeley, CA 94720 USA.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   University of California System; University of California Berkeley
RP Mayer-Patel, K (corresponding author), Univ N Carolina, Dept Comp Sci, Campus Box 3175,Siterson Hall, Chapel Hill, NC 27599 USA.
EM kmp@email.unc.edu; bsmith@tippingpoint.com; rowe@cs.berkeley.edu
CR Bahl P., 1995, Digital Technical Journal, V7, P52
   BHASKARAN V, 1995, IEEE T CIRC SYST VID, V5, P380, DOI 10.1109/76.473548
   ECKERT S, 1995, P SPIE C DIG VID COM, V2419, P446
   Foley J., 1993, INTRO COMPUTER GRAPH, V1st
   GONG K, 1994, P INT PICT COD S PCS
   *ISO IEC, 1993, 11721993 ISOIEC
   LANE T, 1992, JPEG SOFTWARE
   LEE R, 1995, P COMPC, P186
   MayerPatel K, 1997, P SOC PHOTO-OPT INS, V3020, P194, DOI 10.1117/12.264292
   MCMILLAN L, 1992, P DAT COMPR C
   Patel K., 1993, Proceedings ACM Multimedia 93, P75, DOI 10.1145/166266.166274
   PENNEBAKER WB, 1993, JPEG SILL IMAGE DATA
   ROWE JM, 1995, ADV BLOOD DISORD, V1, P1
   ROWE L, 1993, LECT NOTES COMPUTER, V712, P376
   ROWE LA, 1994, P SOC PHOTO-OPT INS, V2188, P134, DOI 10.1117/12.171693
   Schank P. K., 1993, Journal of Educational Multimedia and Hypermedia, V2, P299
   SHEN K, 1995, P SOC PHOTO-OPT INS, V2419, P407, DOI 10.1117/12.206377
   Soderquist P, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P291, DOI 10.1145/266180.266380
   Ulichney R., 1987, DIGITAL HALFTONING
NR 19
TC 5
Z9 5
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2005
VL 1
IS 1
BP 110
EP 125
DI 10.1145/1047936.1047944
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V74DU
UT WOS:000205012200008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xie, J
   Pang, YW
   Pan, J
   Nie, J
   Cao, JL
   Han, JG
AF Xie, Jin
   Pang, Yanwei
   Pan, Jing
   Nie, Jing
   Cao, Jiale
   Han, Jungong
TI Complementary Feature Pyramid Network for Object Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Object detection; Feature pyramid networks; Multi-scale object detection
AB The way of constructing a robust feature pyramid is crucial for object detection. However, existing feature pyramid methods, which aggregate multi-level features by using element-wise sum or concatenation, are inefficient to construct a robust feature pyramid. The reason is that these methods cannot be effective in discriminating the relevant semantics of objects. In this article, we propose a Complementary Feature Pyramid Network (CFPN) to aggregate multi-level features selectively and efficiently by exploring complementary information betweenmulti-level features. Specifically, a Spatial Complementary Module (SCM) and a Channel Complementary Module (CCM) are designed and embedded in CFPN to enhance useful information and suppress irrelevant information during feature fusions along spatial and channel dimensions, respectively. CFPN is a generic feature extractor, as evidenced by its seamless integration into single-stage, two-stage, and end-to-end object detectors. Experiments conducted on the COCO and Pascal VOC datasets demonstrate that integrating our CFPN into RetinaNet, Faster RCNN, Cascade RCNN, and Sparse RCNN obtains consistent performance improvements with negligible overheads. Code and models are available at: https://github.com/ VIPLab- CQU/CFPN.
C1 [Xie, Jin; Pang, Yanwei; Cao, Jiale] Tianjin Univ, 92 Weijin Rd, Tianjin 300072, Peoples R China.
   [Xie, Jin; Nie, Jing] Chongqing Univ, 174 Shazhengjie, Chongqing 400044, Peoples R China.
   [Pan, Jing] Tianjin Univ Technol & Educ, 1310 Dagu South Rd, Tianjin 300222, Peoples R China.
   [Han, Jungong] Univ Sheffield, Western Bank, Sheffield S1 4DP, England.
C3 Tianjin University; Chongqing University; Tianjin University of
   Technology & Education; University of Sheffield
RP Xie, J; Pang, YW (corresponding author), Tianjin Univ, 92 Weijin Rd, Tianjin 300072, Peoples R China.; Xie, J (corresponding author), Chongqing Univ, 174 Shazhengjie, Chongqing 400044, Peoples R China.
EM xiejin@cqu.edu.cn; pyw@tju.edu.cn; jingpan23@gmail.com;
   jingnie@tju.edu.cn; connor@tju.edu.cn; jungonghan77@gmail.com
RI Xie, Jindou/ABB-6044-2020
OI Nie, Jing/0000-0001-9872-9286; Xie, Jin/0000-0001-6978-8834
FU National Key Research and Development Program of China [2022ZD0160400];
   National Natural Science Foundation of China [62206031, 62271346]; China
   Postdoctoral Science Foundation [2021M700613, 2022M720581]; Tianjin
   Science and Technology Program [19ZXZNGX00050]
FX This work is supported in part by the National Key Research and
   Development Program of China (Grant No. 2022ZD0160400), in part by
   National Natural Science Foundation of China (Grant No. 62206031 and
   62271346), in part by China Postdoctoral Science Foundation (Grant No.
   2021M700613 and 2022M720581), and in part by Tianjin Science and
   Technology Program (Grant No. 19ZXZNGX00050).
CR Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Dai JF, 2016, ADV NEUR IN, V29
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Jiale Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11482, DOI 10.1109/CVPR42600.2020.01150
   Kim SW, 2018, LECT NOTES COMPUT SC, V11209, P239, DOI 10.1007/978-3-030-01228-1_15
   Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11
   Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li X, 2021, PROC CVPR IEEE, P11627, DOI 10.1109/CVPR46437.2021.01146
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo XF, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3381086
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Pang YW, 2019, PROC CVPR IEEE, P7328, DOI 10.1109/CVPR.2019.00751
   Paszke A., 2017, NIPS W
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang JF, 2021, PROC CVPR IEEE, P15844, DOI 10.1109/CVPR46437.2021.01559
   Wang ZT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3462219
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xinjiang Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13356, DOI 10.1109/CVPR42600.2020.01337
   Xu H, 2019, IEEE I CONF COMP VIS, P6648, DOI 10.1109/ICCV.2019.00675
   Yang DB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472393
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhao GM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2743, DOI 10.1109/ICCV48922.2021.00276
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
   Zhu Xizhou, 2021, INT C LEARN REPRESEN
NR 49
TC 2
Z9 2
U1 13
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
DI 10.1145/3584362
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200001
DA 2024-07-18
ER

PT J
AU Hou, GJ
   Li, YX
   Yang, H
   Li, KQ
   Pan, ZK
AF Hou, Guojia
   Li, Yuxuan
   Yang, Huan
   Li, Kunqian
   Pan, Zhenkuan
TI UID2021: An Underwater Image Dataset for Evaluation of No-Reference
   Quality Assessment Metrics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Underwater image; image quality assessment; benchmark dataset; image
   enhancement and restoration; mean opinion score
ID BLUR ASSESSMENT; ENHANCEMENT; DATABASE
AB Achieving subjective and objective quality assessment of underwater images is of high significance in underwater visual perception and image/video processing. However, the development of underwater image quality assessment (UIQA) is limited for the lack of publicly available underwater image datasets with human subjective scores and reliable objective UIQA metrics. To address this issue, we establish a large-scale underwater image dataset, dubbed UID2021, for evaluating no-reference (NR) UIQA metrics. The constructed dataset contains 60 multiply degraded underwater images collected from various sources, covering six common underwater scenes (i.e., bluish scene, blue-green scene, greenish scene, hazy scene, low-light scene, and turbid scene), and their corresponding 900 quality improved versions are generated by employing 15 state-of-the-art underwater image enhancement and restoration algorithms. Mean opinion scores with 52 observers for each image of UID2021 are also obtained by using the pairwise comparison sorting method. Both in-air and underwater-specific NR IQA algorithms are tested on our constructed dataset to fairly compare their performance and analyze their strengths and weaknesses. Our proposed UID2021 dataset enables ones to evaluate NR UIQA algorithms comprehensively and paves the way for further research on UIQA. The dataset is available at https://github.com/Hou-Guojia/UID2021.
C1 [Hou, Guojia; Li, Yuxuan; Yang, Huan; Pan, Zhenkuan] Qingdao Univ, Coll Comp Sci & Technol, 308 Ningxia Rd, Qingdao 266071, Peoples R China.
   [Li, Kunqian] Ocean Univ China, Coll Engn, 238 Songling Rd, Qingdao 266100, Peoples R China.
C3 Qingdao University; Ocean University of China
RP Li, KQ (corresponding author), Ocean Univ China, Coll Engn, 238 Songling Rd, Qingdao 266100, Peoples R China.
EM hgj2015@qdu.du.cn; lyx776239423@gmail.com; cathy_huanyang@hotmail.com;
   likunqian@ouc.edu.cn; zkpan@126.com
RI Wang, Jinlong/KHC-3829-2024; zhang, yingying/KGM-8162-2024; Li,
   Zexi/KFA-6939-2024
OI zhang, yingying/0000-0001-7479-3398; Li, Kunqian/0000-0001-9831-6457;
   Hou, Guojia/0000-0001-6509-6259
FU National Natural Science Foundation of China [61901240, 61906177];
   Natural Science Foundation of Shandong Province, China [ZR2019BF042,
   ZR2019BF034]; China Scholarship Council, China [201908370002]; China
   Postdoctoral Science Foundation [2017M612204]
FX The researchwork is partially supported by the National Natural Science
   Foundation of China (No. 61901240, 61906177), the Natural Science
   Foundation of Shandong Province, China (No. ZR2019BF042, ZR2019BF034),
   China Scholarship Council, China (No. 201908370002), and the China
   Postdoctoral Science Foundation (No. 2017M612204).
CR Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Anwar S, 2020, SIGNAL PROCESS-IMAGE, V89, DOI 10.1016/j.image.2020.115978
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Codevilla Felipe, 2015, INT J COMPUT VISION, V60, P91
   Corchs S, 2017, LECT NOTES COMPUT SC, V10213, P95, DOI 10.1007/978-3-319-56010-6_8
   Duarte A., 2016, OCEANS-IEEE, P1
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Fu XY, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115892
   Fu XY, 2017, I S INTELL SIG PROC, P789, DOI 10.1109/ISPACS.2017.8266583
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Fu Z., 2022, IMAGE COMMUN, V102
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Guo CL, 2022, Arxiv, DOI arXiv:2208.06857
   Guo PF, 2022, IEEE T MULTIMEDIA, V24, P1980, DOI 10.1109/TMM.2021.3074825
   Han M, 2020, IEEE T SYST MAN CY-S, V50, P1820, DOI 10.1109/TSMC.2017.2788902
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Hope Nicholas., BUBBLE VISION UNDERW
   Horita Y., 2023, MICT IMAGE QUALITY E
   Hou GJ, 2020, IEEE ACCESS, V8, P122078, DOI 10.1109/ACCESS.2020.3006359
   Hou GJ, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102732
   Hou GJ, 2019, NEUROCOMPUTING, V369, P106, DOI 10.1016/j.neucom.2019.08.041
   Hou GJ, 2018, IET IMAGE PROCESS, V12, P292, DOI 10.1049/iet-ipr.2017.0359
   Iqbal Kashif, 2007, IAENG International Journal of Computer Science, V34, P239
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li K., 2022, IEEE T CIRCUITS SYST
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Li N, 2022, DIGIT SIGNAL PROCESS, V129, DOI 10.1016/j.dsp.2022.103660
   Li XJ, 2022, ENG APPL ARTIF INTEL, V111, DOI 10.1016/j.engappai.2022.104759
   Li XJ, 2020, IEEE ACCESS, V8, P197448, DOI 10.1109/ACCESS.2020.3034275
   Lin HH, 2018, Arxiv, DOI arXiv:1803.08489
   Liu LX, 2019, IEEE T MULTIMEDIA, V21, P2305, DOI 10.1109/TMM.2019.2900941
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Ma YP, 2018, INT CONF IMAG PROC, P199
   Marques TP, 2020, IEEE COMPUT SOC CONF, P2286, DOI 10.1109/CVPRW50498.2020.00277
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Ninassi A, 2006, PROC SPIE, V6057, DOI 10.1117/12.650780
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Qi Q, 2022, IEEE T IMAGE PROCESS, V31, P6816, DOI 10.1109/TIP.2022.3216208
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sánchez-Ferreira C, 2019, SIGNAL PROCESS-IMAGE, V77, P49, DOI 10.1016/j.image.2019.05.015
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun W, 2017, PATTERN RECOGN, V61, P153, DOI 10.1016/j.patcog.2016.07.033
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wang J, 2022, IEEE COMPUT SOC CONF, P1268, DOI 10.1109/CVPRW56347.2022.00133
   Wang Y, 2018, COMPUT ELECTR ENG, V70, P904, DOI 10.1016/j.compeleceng.2017.12.006
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3447530
   Winkler S, 2012, IEEE J-STSP, V6, P616, DOI 10.1109/JSTSP.2012.2215007
   Wu D, 2020, SCI PROGRAMMING-NETH, V2020, DOI 10.1155/2020/8856640
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Xie J, 2022, IEEE T CIRC SYST VID, V32, P3514, DOI 10.1109/TCSVT.2021.3115791
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   Yang M, 2019, IEEE ACCESS, V7, P123638, DOI 10.1109/ACCESS.2019.2932611
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yang N, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116218
   Yuan JY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3110575
   Zhang WD, 2022, IEEE T IMAGE PROCESS, V31, P3997, DOI 10.1109/TIP.2022.3177129
   Zhang WX, 2023, IEEE T PATTERN ANAL, V45, P2864, DOI 10.1109/TPAMI.2022.3178874
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zheng YN, 2022, IEEE T IMAGE PROCESS, V31, P5456, DOI 10.1109/TIP.2022.3196815
   Zhou JC, 2022, ENG APPL ARTIF INTEL, V111, DOI 10.1016/j.engappai.2022.104785
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
   Zhuang PX, 2021, ENG APPL ARTIF INTEL, V101, DOI 10.1016/j.engappai.2021.104171
NR 84
TC 23
Z9 23
U1 23
U2 45
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2023
VL 19
IS 4
AR 151
DI 10.1145/3578584
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FX8
UT WOS:001011937600013
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Liu, H
   Yan, ZY
   Liu, B
   Zhao, JQ
   Zhou, Y
   El Saddik, A
AF Liu, Hao
   Yan, Zhaoyu
   Liu, Bing
   Zhao, Jiaqi
   Zhou, Yong
   El Saddik, Abdulmotaleb
TI Distilled Meta-learning for Multi-Class Incremental Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Incremental learning; meta-learning; knowledge distillation;
   catastrophic forgetting; stability-plasticity dilemma
AB Meta-learning approaches have recently achieved promising performance inmulti-class incremental learning. However, meta-learners still suffer from catastrophic forgetting, i.e., they tend to forget the learned knowledge from the old tasks when they focus on rapidly adapting to the new classes of the current task. To solve this problem, we propose a novel distilled meta-learning (DML) framework for multi-class incremental learning that integrates seamlessly meta-learning with knowledge distillation in each incremental stage. Specifically, during inner-loop training, knowledge distillation is incorporated into the DML to overcome catastrophic forgetting. During outer-loop training, a meta-update rule is designed for the meta-learner to learn across tasks and quickly adapt to new tasks. By virtue of the bilevel optimization, our model is encouraged to reach a balance between the retention of old knowledge and the learning of new knowledge. Experimental results on four benchmark datasets demonstrate the effectiveness of our proposal and show that our method significantly outperforms other state-of-the-art incremental learning methods.
C1 [Liu, Hao; Yan, Zhaoyu; Liu, Bing; Zhao, Jiaqi; Zhou, Yong] China Univ Min & Technol, 1 Daxue Rd, Xuzhou 221116, Jiangsu, Peoples R China.
   [El Saddik, Abdulmotaleb] Univ Ottawa, Sch Elect Engn & Comp Sci, 800 King Edward, Ottawa, ON K1N 6N5, Canada.
C3 China University of Mining & Technology; University of Ottawa
RP Zhou, Y (corresponding author), China Univ Min & Technol, 1 Daxue Rd, Xuzhou 221116, Jiangsu, Peoples R China.
EM tb20170007b4@cumt.edu.cn; 06192204@cumt.edu.cn; liubing@cumt.edu.cn;
   jiaqizhao@cumt.edu.cn; yzhou@cumt.edu.cn; elsaddik@uottawa.ca
RI /D-4159-2009
OI /0000-0002-7690-8547; Hao, Liu/0000-0001-6728-1773; Liu,
   Bing/0000-0002-2365-6606
FU National Natural Science Foundation of China [62272461, 62276266,
   62172417, 62106268]; Natural Science Foundation of Jiangsu Province
   [BK20180639, BK20201346]
FX This work was supported by the National Natural Science Foundation of
   China (No. 62272461, 62276266, 62172417, 62106268), and the Natural
   Science Foundation of Jiangsu Province (No. BK20180639, BK20201346).
CR Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9
   Bengio Y., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), DOI 10.1109/IJCNN.1991.155621
   Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15
   Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33
   Cheng M, 2022, IEEE T CIRC SYST VID, V32, P2158, DOI 10.1109/TCSVT.2021.3088545
   Chi Z., 2022, P IEEECVF C COMPUTER, P14166
   Cichon J, 2015, NATURE, V520, P180, DOI 10.1038/nature14251
   Dhar P, 2019, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR.2019.00528
   Ditzler G, 2015, IEEE COMPUT INTELL M, V10, P12, DOI 10.1109/MCI.2015.2471196
   Douillard A, 2021, PROC CVPR IEEE, P4039, DOI 10.1109/CVPR46437.2021.00403
   Hinton G., 2015, COMPUT SCI, V2
   Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092
   Javed K, 2019, AVIAN PATHOL, V48, P557, DOI 10.1080/03079457.2019.1643450
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Kj Joseph, 2021, IEEE T PATTERN ANAL, V2021, P1
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Lange S, 2002, THEOR COMPUT SCI, V288, P277, DOI 10.1016/S0304-3975(01)00404-2
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Liu LY, 2021, Arxiv, DOI arXiv:1908.03265
   Liu XL, 2018, INT C PATT RECOG, P2262, DOI 10.1109/ICPR.2018.8545895
   Liu YY, 2021, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR46437.2021.00257
   Liu YR, 2021, C IND ELECT APPL, P18, DOI 10.1109/ICIEA51954.2021.9516081
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   McCloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI [10.1016/S0079-7421(08)60536-8, DOI 10.1016/S0079-7421(08)60536-8]
   Mermillod M, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00504
   Muhlbaier MD, 2009, IEEE T NEURAL NETWOR, V20, P152, DOI 10.1109/TNN.2008.2008326
   Netzer Yuval, 2011, ADV NEUR INF PROC SY
   Nichol A, 2018, Arxiv, DOI [arXiv:1803.02999, DOI 10.48550/ARXIV.1803.02999]
   Rajasegaran Jathushan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13585, DOI 10.1109/CVPR42600.2020.01360
   Rajasegaran J, 2019, PROC CVPR IEEE, P10717, DOI 10.1109/CVPR.2019.01098
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Rüping S, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P641, DOI 10.1109/ICDM.2001.989589
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   Shin H, 2017, ADV NEUR IN, V30
   Tan Z, 2022, WSDM'22: PROCEEDINGS OF THE FIFTEENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P987, DOI 10.1145/3488560.3498455
   Wang K, 2022, IEEE COMPUT SOC CONF, P3728, DOI 10.1109/CVPRW56347.2022.00417
   Wu Y, 2019, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2019.00046
   Yang DB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472393
   Yaoyao Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12242, DOI 10.1109/CVPR42600.2020.01226
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zhang JT, 2020, IEEE WINT CONF APPL, P1120, DOI [10.1109/WACV45572.2020.9093365, 10.1109/wacv45572.2020.9093365]
   Zhou DW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1645, DOI 10.1145/3474085.3475306
NR 44
TC 0
Z9 0
U1 13
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2023
VL 19
IS 4
AR 149
DI 10.1145/3576045
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FX8
UT WOS:001011937600011
DA 2024-07-18
ER

PT J
AU Liu, H
   Li, SS
   Zhu, JC
   Deng, K
   Liu, M
   Nie, LQ
AF Liu, Hui
   Li, Shanshan
   Zhu, Jicheng
   Deng, Kai
   Liu, Meng
   Nie, Liqiang
TI DDIFN: A Dual-discriminator Multi-modal Medical Image Fusion Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-modal fusion; medical image fusion; generative adversarial
   networks; dual discriminator; U-Net
ID PERFORMANCE; FRAMEWORK
AB Multi-modal medical image fusion is a long-standing important research topic that can obtain informative medical images and assist doctors diagnose and treat diseases more efficiently. However, most fusion methods extract and fuse features by subjectively defining constraints, which easily distorts the unique information of source images. In this work, we present a novel end-to-end unsupervised network to fuse multi-modal medical images. It is composed of a generator and two symmetrical discriminators. The former aims to generate a "real-like" fused image based on a specifically designed content and structure loss, while the latter are devoted to distinguishing the differences between the fused image and the source ones. They are trained alternately until discriminators cannot distinguish the fused image from the source ones. In addition, the symmetrical discriminator scheme is conducive to maintaining the feature consistency among different modalities. More importantly, to enhance the retention degree of texture details, U-Net is adopted as the generator heuristically, where the up-sampling method is modified to bilinear interpolation for avoiding checkerboard artifacts. As for the optimization, we define the content loss function, which preserves the gradient information and pixel activity of source images. Both visual analysis and quantitative evaluation of experimental results show the superiority of our method as compared to the cutting-edge baselines.
C1 [Liu, Hui; Li, Shanshan; Zhu, Jicheng] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Shandong Key Lab Digital Media Technol, Jinan 250014, Shandong, Peoples R China.
   [Deng, Kai] Shandong First Med Univ, Affiliated Hosp 1, Jinan 250013, Shandong, Peoples R China.
   [Liu, Meng] Shandong Jianzhu Univ, Sch Comp Sci & Technol, Jinan 250102, Shandong, Peoples R China.
   [Nie, Liqiang] Harbin Inst Technol Shenzhen, Shandong Key Lab Digital Media Technol, Sch Comp Sci & Technol, Shenzhen 518055, Guangdong, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong First Medical
   University & Shandong Academy of Medical Sciences; Shandong Jianzhu
   University; Harbin Institute of Technology
RP Liu, H (corresponding author), Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Shandong Key Lab Digital Media Technol, Jinan 250014, Shandong, Peoples R China.; Nie, LQ (corresponding author), Harbin Inst Technol Shenzhen, Shandong Key Lab Digital Media Technol, Sch Comp Sci & Technol, Shenzhen 518055, Guangdong, Peoples R China.
EM liuh_lh@sdufe.edu.cn; 202115001@mail.sdufe.edu.cn; sdufezjc@outlook.com;
   calab2a@126.com; mengliu.sdu@gmail.com; nieliqiang@gmail.com
OI Liu, Meng/0000-0002-1582-5764; Deng, Kai/0000-0003-4753-8564
FU National Natural Science Foundation of China [62072274]; Shandong
   Provincial Transfer and Transformation Project of Scientific and
   Technological Achievements [2021LYXZ011]; Shandong-Chongqing Science and
   Technology Cooperation Project [cstc2021-lyjsAX0003]
FX This work was supported by National Natural Science Foundation of China
   (62072274), Shandong Provincial Transfer and Transformation Project of
   Scientific and Technological Achievements (2021LYXZ011).
   Shandong-Chongqing Science and Technology Cooperation Project
   (cstc2021-lyjsAX0003).
CR Arjovsky M, 2017, Arxiv, DOI [arXiv:1701.07875, DOI 10.48550/ARXIV.1701.07875]
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   [陈佛计 Chen Foji], 2021, [计算机学报, Chinese Journal of Computers], V44, P347
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Fan FD, 2019, Arxiv, DOI arXiv:1906.00225
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   Hermessi H, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.108036
   Karakaya D, 2022, INT CONF ACOUST SPEE, P2345, DOI 10.1109/ICASSP43922.2022.9746779
   Kim M, 2016, INFORM FUSION, V27, P198, DOI 10.1016/j.inffus.2015.03.003
   Kumar A, 2017, PROCEEDINGS OF THE ADVANCES IN ROBOTICS (AIR'17), DOI 10.1145/3132446.3134899
   Lahoud F, 2019, 2019 22ND INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2019), DOI 10.23919/fusion43075.2019.9011178
   Li XX, 2020, IEEE T INSTRUM MEAS, V69, P6880, DOI 10.1109/TIM.2020.2975405
   Li Y., 2021, Int J Cogn Comput Eng, V2, P21, DOI DOI 10.1016/J.IJCCE.2020.12.004
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Perra C., 2005, IEEE International Conference on Image Processing, V1, pI, DOI DOI 10.1109/ICIP.2005.1529769
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Rao YJ, 1997, MEAS SCI TECHNOL, V8, P355, DOI 10.1088/0957-0233/8/4/002
   Ratliff LJ, 2013, ANN ALLERTON CONF, P917, DOI 10.1109/Allerton.2013.6736623
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Wang XH, 2020, AAAI CONF ARTIF INTE, V34, P12249
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3408317
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yin M, 2019, IEEE T INSTRUM MEAS, V68, P49, DOI 10.1109/TIM.2018.2838778
   Yu X, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108772
   Yu X, 2021, INFORM SCIENCES, V568, P350, DOI 10.1016/j.ins.2021.03.059
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhou ZQ, 2016, INFORM FUSION, V30, P15, DOI 10.1016/j.inffus.2015.11.003
NR 38
TC 2
Z9 2
U1 8
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2023
VL 19
IS 4
AR 145
DI 10.1145/3574136
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FX8
UT WOS:001011937600007
DA 2024-07-18
ER

PT J
AU Yang, S
   Li, Q
   Li, WH
   Li, XY
   Jin, R
   Lv, B
   Wang, R
   Liu, AA
AF Yang, Song
   Li, Qiang
   Li, Wenhui
   Li, Xuan-Ya
   Jin, Ran
   Lv, Bo
   Wang, Rui
   Liu, Anan
TI Semantic Completion and Filtration for Image-Text Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image-text retrieval; multimodal; semantic completion; semantic
   filtration
AB Image-text retrieval is a vital task in computer vision and has received growing attention, since it connects cross-modality data. It comes with the critical challenges of learning unified representations and eliminating the large gap between visual and textual domains. Over the past few decades, although many works have made significant progress in image-text retrieval, they are still confronted with the challenge of incomplete text descriptions of images, i.e., how to fully learn the correlations between relevant region-word pairs with semantic diversity. In this article, we propose a novel semantic completion and filtration (SCAF) method to alleviate the above issue. Specifically, the text semantic completion module is presented to generate a complete semantic description of an image using multi-view text descriptions, guiding the model to explore the correlations of relevant region-word pairs fully. Meanwhile, the adaptive structural semantic matching module is presented to filter irrelevant region-word pairs by considering the relevance score of each region-word pair, which facilitates the model to focus on learning the relevance of matching pairs. Extensive experiments show that our SCAF outperforms the existing methods on Flickr30K and MSCOCO datasets, which demonstrates the superiority of our proposed method.
C1 [Yang, Song; Li, Qiang; Li, Wenhui; Liu, Anan] Tianjin Univ, Tianjin, Peoples R China.
   [Yang, Song; Liu, Anan] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China.
   [Li, Xuan-Ya] Baidu Inc, Beijing, Peoples R China.
   [Jin, Ran] ZhejiangWanli Univ, Ningbo, Peoples R China.
   [Lv, Bo; Wang, Rui] China Elect Technol Grp Corp, Res Inst 30, Chengdu, Peoples R China.
C3 Tianjin University; Baidu; China Electronics Technology Group
RP Li, XY (corresponding author), Baidu Inc, Beijing, Peoples R China.
EM lixuanya@baidu.com
RI Zeng, Yun/JFK-6190-2023; LU, lpp pp/JFJ-9011-2023; LI,
   Wenhui/JCD-9947-2023; Lu, Wang/JVO-0416-2024
FU National Natural Science Foundation of China [U21B2024, 62202327]; China
   Postdoctoral Science Foundation [2022M712369]; Baidu Program
FX This work was supported in part by the National Natural Science
   Foundation of China (U21B2024, 62202327), the China Postdoctoral Science
   Foundation (2022M712369), and the Baidu Program.
CR [Anonymous], 2008, COLING 2008 P WORKSH
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Barzilay Regina, 2003, HLTNAACL 2003 HUM
   Chung Junyoung, 2014, ARXIV14123555
   Cong GX, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4496, DOI 10.1145/3503161.3548206
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Fan ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6514
   Filippova Katja, 2010, Proceedings of the 23rd International Conference on Computational Linguistics, P322
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Biten AF, 2019, PROC CVPR IEEE, P12458, DOI 10.1109/CVPR.2019.01275
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang FR, 2019, IEEE T IMAGE PROCESS, V28, P2008, DOI 10.1109/TIP.2018.2882225
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Lebanoff L, 2019, Arxiv, DOI arXiv:1910.00203
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li L, 2022, IEEE T IMAGE PROCESS, V31, P2726, DOI 10.1109/TIP.2022.3158546
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2020, P IEEECVF C COMPUTER, p10 918
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Liu JH, 2021, IEEE T CIRC SYST VID, V31, P3242, DOI 10.1109/TCSVT.2020.3037661
   Liu Xuejing, 2022, ARXIV
   Luo MN, 2017, COMPUT VIS IMAGE UND, V163, P67, DOI 10.1016/j.cviu.2017.07.001
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Ma Y., 2019, FRONT DATA COMPUT, V1, P105, DOI [DOI 10.11871/JFDC, DOI 10.11871/JFDC.ISSN.2096.742X.2019.01.011]
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Nie XS, 2021, IEEE T CIRC SYST VID, V31, P401, DOI 10.1109/TCSVT.2020.2974877
   Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208
   Paddlepaddle, 2019, PADDLEPADDLE EAS TO
   Plummer BA, 2017, INT J COMPUT VISION, V123, P74, DOI 10.1007/s11263-016-0965-7
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Wang H, 2021, PROC CVPR IEEE, P7022, DOI 10.1109/CVPR46437.2021.00695
   Wang H, 2019, PROC CVPR IEEE, P3522, DOI 10.1109/CVPR.2019.00364
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wen X, 2021, IEEE T CIRC SYST VID, V31, P2427, DOI 10.1109/TCSVT.2020.3017344
   Wu LX, 2021, IEEE T CIRC SYST VID, V31, P3118, DOI 10.1109/TCSVT.2020.3036860
   Wu YL, 2021, IEEE T MULTIMEDIA, V23, P559, DOI 10.1109/TMM.2020.2985540
   Xu X, 2019, WORLD WIDE WEB, V22, P657, DOI 10.1007/s11280-018-0541-x
   Yongzhi Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12783, DOI 10.1109/CVPR42600.2020.01280
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang CY, 2021, IEEE T CIRC SYST VID, V31, P4334, DOI 10.1109/TCSVT.2020.3047095
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zhu B, 2019, PROC CVPR IEEE, P11469, DOI 10.1109/CVPR.2019.01174
NR 54
TC 5
Z9 5
U1 9
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2023
VL 19
IS 4
AR 140
DI 10.1145/3572844
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FX8
UT WOS:001011937600002
DA 2024-07-18
ER

PT J
AU Xie, C
   Zhuang, ZK
   Zhao, SJ
   Liang, S
AF Xie, Chi
   Zhuang, Zikun
   Zhao, Shengjie
   Liang, Shuang
TI Temporal Dropout for Weakly Supervised Action Localization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Weakly supervised; temporal action localization; adversarial erasing;
   adaptive dropout
ID NETWORK
AB Weakly supervised action localization is a challenging problem in video understanding and action recognition. Existing models usually formulate the training process as direct classification using video-level supervision. They tend to only locate the most discriminative parts of action instances and produce temporally incomplete detection results. A natural solution for this problem, the adversarial erasing strategy, is to remove such parts from training so that models can attend to complementary parts. Previous works do it in an offline and heuristic way. They adopt a multi-stage pipeline, where discriminative regions are determined and erased under the guidance of detection results from last stage. Such a pipeline can be both ineffective and inefficient, possibly hindering the overall performance. On the contrary, we combine adversarial erasing with dropout mechanism and propose a Temporal Dropout Module that learns where to remove in a data-driven and online manner. This plug-and-play module is trained without iterative stages, which not only simplifies the pipeline but also makes the regularization during training easier and more adaptive. Experiments show that the proposed method outperforms previous erasing-based methods by a large margin. More importantly, it achieves universal improvement when plugged into various direct classification methods and obtains state-of-the-art performance.
C1 [Xie, Chi; Zhuang, Zikun; Zhao, Shengjie; Liang, Shuang] Tongji Univ, 4800 Caoan Rd, Shanghai, Peoples R China.
C3 Tongji University
RP Liang, S (corresponding author), Tongji Univ, 4800 Caoan Rd, Shanghai, Peoples R China.
EM chixie@tongji.edu.cn; 2131488@tongji.edu.cn; shengjiezhao@tongji.edu.cn;
   shuangliang@tongji.edu.cn
RI Huang, Yong/KFA-1191-2024; wang, Xiaoming/KBB-8854-2024; Xie,
   Chi/B-5612-2014
OI Zhao, Shengjie/0000-0002-4301-394X; Xie, Chi/0000-0002-5808-1742;
   Zhuang, Zikun/0000-0002-2242-4918
FU National Natural Science Foundation of China [62076183, 61936014,
   61976159]; Natural Science Foundation of Shanghai [20ZR1473500,
   19ZR1461200]; Shanghai Innovation Action Project of Science and
   Technology [20511100700]; National Key Research and Development Project
   [2019YFB2102300]; Shanghai Municipal Science and Technology Major
   Project [2021SHZDZX0100]; Fundamental Research Funds for the Central
   Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076183, 61936014 and 61976159, in part
   by the Natural Science Foundation of Shanghai under Grant 20ZR1473500,
   19ZR1461200, in part by the Shanghai Innovation Action Project of
   Science and Technology under Grant 20511100700, in part by the National
   Key Research and Development Project under Grant 2019YFB2102300, in part
   by the Shanghai Municipal Science and Technology Major Project under
   Grant 2021SHZDZX0100, and in part by the Fundamental Research Funds for
   the Central Universities.
CR Arnab Anurag, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P751, DOI 10.1007/978-3-030-58607-2_44
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Ghiasi G, 2018, ADV NEUR IN, V31
   Huang LJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7982, DOI 10.1109/ICCV48922.2021.00790
   Huang LJ, 2020, AAAI CONF ARTIF INTE, V34, P11053
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Jang E., 2017, P ICLR, P1
   Kingma Diederik, 2015, P 3 INT C LEARN REPR
   Kingma DP, 2015, ADV NEUR IN, V28
   Lee P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13628, DOI 10.1109/ICCV48922.2021.01339
   Lee P, 2021, AAAI CONF ARTIF INTE, V35, P1854
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu ZY, 2021, AAAI CONF ARTIF INTE, V35, P2233
   Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Luo W, 2021, PROC CVPR IEEE, P9964, DOI 10.1109/CVPR46437.2021.00984
   Ma JW, 2021, PROC CVPR IEEE, P7583, DOI 10.1109/CVPR46437.2021.00750
   Moltisanti D, 2019, PROC CVPR IEEE, P9907, DOI 10.1109/CVPR.2019.01015
   Narayan S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13588, DOI 10.1109/ICCV48922.2021.01335
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Pardo A, 2021, IEEE WINT CONF APPL, P3318, DOI 10.1109/WACV48630.2021.00336
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Peisen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P539, DOI 10.1007/978-3-030-58598-3_32
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Qu SQ, 2021, Arxiv, DOI arXiv:2104.02967
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shi QHY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485665
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su H., 2018, P AS C COMP VIS, P558
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Yang ZC, 2022, AAAI CONF ARTIF INTE, P3090
   Yu T, 2019, IEEE I CONF COMP VIS, P5521, DOI 10.1109/ICCV.2019.00562
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zeng RH, 2022, IEEE T PATTERN ANAL, V44, P6209, DOI 10.1109/TPAMI.2021.3090167
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zeng RH, 2019, IEEE T IMAGE PROCESS, V28, P5797, DOI 10.1109/TIP.2019.2922108
   Zhai YH, 2022, IEEE T MULTIMEDIA, V24, P1857, DOI 10.1109/TMM.2021.3073235
   Zhang C, 2021, PROC CVPR IEEE, P16005, DOI 10.1109/CVPR46437.2021.01575
   Zhang SW, 2020, IEEE T MULTIMEDIA, V22, P2610, DOI 10.1109/TMM.2019.2959425
   Zhekun Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P729, DOI 10.1007/978-3-030-58526-6_43
   Zhong JX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P35, DOI 10.1145/3240508.3240511
   Zhongzheng Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10595, DOI 10.1109/CVPR42600.2020.01061
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu SG, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3361845
NR 62
TC 4
Z9 4
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2023
VL 19
IS 3
AR 102
DI 10.1145/3567827
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FQ5
UT WOS:001011930300002
DA 2024-07-18
ER

PT J
AU Puig, JMM
   Rifà-Pous, H
   Oukemeni, S
AF Marques Puig, Joan Manuel
   Rifa-Pous, Helena
   Oukemeni, Samia
TI From False-Free to Privacy-Oriented Communitarian Microblogging Social
   Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Microblogging services; online social networks; privacy;
   decentralization; censorship-resistance
ID RESOURCE-ALLOCATION
AB Online Social Networks (OSNs) have gained enormous popularity in recent years. They provide a dynamic platform for sharing content (text messages or multimedia) and for facilitating communication between friends and acquaintances. Microblogging services are a popular form of OSNs. They allow sending small messages in a one-to-many messaging model so that users can communicate with their favorite celebrity, brand, politician, or other regular users without the obligation of a pre-existing social relationship. A chain of privacy-related scandals linked to questionable data handling practices in microblogging services has arisen in the last past few years. Most current microblogging service providers offer centralized services and their business model is based on monitoring, analyzing, and selling users' activity and patterns. In the end, the personal information shared by the users to benefit from the free-of-charge services is used for the underlying payment in such systems. In this paper, we present Garlanet, a privacy-aware censorship-resistant microblogging social network that does not rely on a centralized service provider as all data is hosted in computers voluntarily contributed by the users of the system. Garlanet provides microblogging functionalities while protecting privacy and preserving the confidentiality and integrity of users and data. It ensures that users' identities and their social graphs are hidden from the system and adversaries and it provides availability and scalability of the services. We also evaluate the privacy level of Garlanet and we compare it with the privacy level of eight other microblogging systems.
C1 [Marques Puig, Joan Manuel; Rifa-Pous, Helena; Oukemeni, Samia] Univ Oberta De Catalunya, Rambla Poblenou, Rambla Del Poblenou 156, Barcelona 08018, Catalunya, Spain.
C3 UOC Universitat Oberta de Catalunya
RP Oukemeni, S (corresponding author), Univ Oberta De Catalunya, Rambla Poblenou, Rambla Del Poblenou 156, Barcelona 08018, Catalunya, Spain.
EM jmarquesp@uoc.edu; hrifa@uoc.edu; soukemeni@uoc.edu
RI Rifà-Pous, Helena/ABA-3227-2020
OI Rifà-Pous, Helena/0000-0003-0923-0235
FU Spanish Ministry of Science, Innovation and Universities
   [PGC2018-097599-B-I00, RTI2018-095094-B-C22, PID2021-125962OB-C31]
FX This work has been partially supported by the Spanish Ministry of
   Science, Innovation and Universities (PGC2018-097599-B-I00,
   RTI2018-095094-B-C22 "CONSENT" and PID2021-125962OB-C31 "SECURING").
CR Aichner T, 2015, INT J MARKET RES, V57, P257, DOI 10.2501/IJMR-2015-018
   Al-Qurishi M, 2017, IEEE ACCESS, V5, P1200, DOI 10.1109/ACCESS.2017.2656635
   [Anonymous], 2012, IEEE DATA ENG B
   [Anonymous], 2020, Most popular social networks worldwide as of April 2020, ranked by number of active users
   [Anonymous], 2007, P 9 WEBKDD 1 SNA KDD, DOI [10.1145/1348549.1348556, DOI 10.1145/1348549.1348556]
   Boyd DM, 2007, J COMPUT-MEDIAT COMM, V13, P210, DOI 10.1111/j.1083-6101.2007.00393.x
   Buchegger Sonja., 2009, P 2 AC M EUROSYS WOR, P46, DOI DOI 10.1145/1578002.1578010
   Cutillo LA, 2011, WOWMOM, P1, DOI [10.1109/WoWMOM.2011.5986118, DOI 10.1109/WOWMOM.2011.5986118]
   Cutillo LA, 2010, HANDBOOK OF SOCIAL NETWORK TECHNOLOGIES AND APPLICATIONS, P497, DOI 10.1007/978-1-4419-7142-5_23
   Cutillo LA, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON A WORLD OF WIRELESS, MOBILE AND MULTIMEDIA NETWORKS & WORKSHOPS, P401
   Cutillo LA, 2009, IEEE COMMUN MAG, V47, P94, DOI 10.1109/MCOM.2009.5350374
   Daubert J, 2014, I C COMP SYST APPLIC, P817, DOI 10.1109/AICCSA.2014.7073285
   Daubert Jorg, 2016, THESIS TU DARMSTADT
   De Cristofaro E, 2012, P IEEE S SECUR PRIV, P285, DOI 10.1109/SP.2012.26
   Demers Alan, 1987, P 6 ANN ACM S PRINCI, P1, DOI [DOI 10.1145/41840.41841, 10.1145/41840.41841]
   Devmane MA, 2014, 2014 RECENT ADVANCES AND INNOVATIONS IN ENGINEERING (ICRAIE)
   Freitas M, 2013, Arxiv, DOI arXiv:1312.7152
   Freitas M, 2016, INT J PARALLEL EMERG, V31, P20, DOI 10.1080/17445760.2015.1053808
   Gab, 2018, US
   Gadkari Pia, 2013, DOES TWITTER MAKE MO
   GnuSocial, 2018, US
   Hallinan D, 2012, COMPUT LAW SECUR REV, V28, P263, DOI 10.1016/j.clsr.2012.03.005
   Han B.-C., 2017, PSYCHOPOLITICS NEOLI
   IC3, INT CRIM SCHEM
   joindiaspora, 2018, GORD MOR
   JoinDiaspora, 2018, US
   Kwak H., WWW'10, DOI DOI 10.1145/1772690.1772751
   Oukemeni S, 2019, IEEE ACCESS, V7, DOI 10.1109/ACCESS.2019.2932899
   Oukemeni S, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3321481
   Panadero J, 2018, FUTURE GENER COMP SY, V82, P29, DOI 10.1016/j.future.2017.11.039
   Perfitt Timothy, 2010, Proceedings of the Fifth International Conference on Internet and Web Applications and Services (ICIW 2010), P469, DOI 10.1109/ICIW.2010.77
   pump.io, 2018, US
   Robertson Adi, 2020, BIGGEST DECENTRALIZE
   Sayce D., 2020, The Number of tweets per day in 2020-David Sayce
   Serra X, 2016, WINT SIMUL C PROC, P3167
   Singh I, 2013, IEEE SECUR PRIV, V11, P46, DOI 10.1109/MSP.2013.3
   TheGuardian, 2019, THEGUARDIAN
   Troia Vinny, 2019, 1 2 BILLION PEOPLE E
   Twister, 2018, US
   Twitter, 2019, US
   Vogels W, 2009, COMMUN ACM, V52, P40, DOI 10.1145/1435417.1435432
   wikipedia, 2020, MAST
   Zheleva E., 2012, Privacy in Social Networks
NR 43
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 83
DI 10.1145/3555354
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300008
DA 2024-07-18
ER

PT J
AU Neto, JBC
   Ferrari, C
   Marana, AN
   Berretti, S
   Del Bimbo, A
AF Cardia Neto, Joao Baptista
   Ferrari, Claudio
   Marana, Aparecido Nilceu
   Berretti, Stefano
   Del Bimbo, Alberto
TI Learning Streamed Attention Network from Descriptor Images for
   Cross-Resolution 3D Face Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D face recognition; convolutional neural networks; feature descriptors;
   self; and cross-attention
ID DATABASE; DENSE; DEEP
AB In this article, we propose a hybrid framework for cross-resolution 3D face recognition which utilizes a Streamed Attention Network (SAN) that combines handcrafted features with Convolutional Neural Networks (CNNs). It consists of two main stages: first, we process the depth images to extract low-level surface descriptors and derive the corresponding Descriptor Images (DIs), represented as four-channel images. To build the DIs, we propose a variation of the 3D Local Binary Pattern (3DLBP) operator that encodes depth differences using a sigmoid function. Then, we design a CNN that learns from these DIs. The peculiarity of our solution consists in processing each channel of the input image separately, and fusing the contribution of each channel by means of both self- and cross-attention mechanisms. This strategy showed two main advantages over the direct application of Deep-CNN to depth images of the face; on the one hand, the DIs can reduce the diversity between high- and low-resolution data by encoding surface properties that are robust to resolution differences. On the other, it allows a better exploitation of the richer information provided by low-level features, resulting in improved recognition. We evaluated the proposed architecture in a challenging cross-dataset, cross-resolution scenario. To this aim, we first train the network on scanner-resolution 3D data. Next, we utilize the pre-trained network as feature extractor on low-resolution data, where the output of the last fully connected layer is used as face descriptor. Other than standard benchmarks, we also perform experiments on a newly collected dataset of paired high- and low-resolution 3D faces. We use the high-resolution data as gallery, while low-resolution faces are used as probe, allowing us to assess the real gap existing between these two types of data. Extensive experiments on low-resolution 3D face benchmarks show promising results with respect to state-of-the-art methods.
C1 [Cardia Neto, Joao Baptista] Sao Paulo State Technol Coll FATEC, Rua Maranhao 898, BR-15800020 Catanduva, SP, Brazil.
   [Ferrari, Claudio] Univ Parma, Dept Architecture & Engn, Parco Area Sci 181-A, I-43124 Parma Pr, Italy.
   [Marana, Aparecido Nilceu] Sao Paulo State Univ UNESP, RECOGNA Lab, Av Engn Luis Edmundo Carrijo Coube 14-01, Bauru, SP, Brazil.
   [Berretti, Stefano; Del Bimbo, Alberto] Univ Florence, MICC, Viale Giovanni Battista Morgagni 65, I-50134 Firenze Fi, Italy.
C3 University of Parma; Universidade Estadual Paulista; University of
   Florence
RP Neto, JBC (corresponding author), Sao Paulo State Technol Coll FATEC, Rua Maranhao 898, BR-15800020 Catanduva, SP, Brazil.
EM joao.cardia@fatec.sp.gov.br; nilceu.marana@unesp.br
RI Berretti, Stefano/U-9004-2019; Ferrari, Claudio/AEQ-4611-2022; Marana,
   Aparecido Nilceu/A-6334-2008; Cardia, João/GYU-7785-2022
OI Berretti, Stefano/0000-0003-1219-4386; Marana, Aparecido
   Nilceu/0000-0003-4861-7061; 
FU CAPES - Brazil [88881.188744/2018-01]; Petrobras/Fundunesp [2662/2017]
FX The authors acknowledge support from CAPES - Brazil (PDSE Grant no.
   88881.188744/2018-01) and Petrobras/Fundunesp (Process 2662/2017).
CR Abbass MY, 2021, VISUAL COMPUT, V37, P831, DOI 10.1007/s00371-020-01833-5
   Al-Obaydy WNI, 2020, NEURAL COMPUT APPL, V32, P1405, DOI 10.1007/s00521-018-3649-0
   [Anonymous], 2006, BRIT MACHINE VISION, DOI DOI 10.5244/C.20.90
   Berretti S, 2010, IEEE T PATTERN ANAL, V32, P2162, DOI 10.1109/TPAMI.2010.43
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bondi E, 2016, IEEE T INF FOREN SEC, V11, P2843, DOI 10.1109/TIFS.2016.2601059
   Cardia Neto J. B., 2019, EUROGRAPHICS WORKSHO
   Cardia Neto J. B., 2014, THESIS UNESP BAURU
   Cardia Neto J. B., 2018, PROGR PATTERN RECOGN, P135
   Drira H, 2013, IEEE T PATTERN ANAL, V35, P2270, DOI 10.1109/TPAMI.2013.48
   Drosou A., 2013, 2013 International Conference of the Biometrics Special Interest Group (BIOSIG), P1
   Faltemier TC, 2008, IEEE T INF FOREN SEC, V3, P62, DOI 10.1109/TIFS.2007.916287
   Ferrari C, 2018, IEEE T IMAGE PROCESS, V27, P5638, DOI 10.1109/TIP.2018.2861359
   Ferrari Claudio, 2021, IEEE T PATTERN ANAL, P2
   Ferrari Claudio, 2022, SENSORS-BASEL
   Galteri L, 2019, COMPUT VIS IMAGE UND, V185, P31, DOI 10.1016/j.cviu.2019.05.002
   Gilani SZ, 2018, PROC CVPR IEEE, P1896, DOI 10.1109/CVPR.2018.00203
   Gilani SZ, 2017, PATTERN RECOGN, V69, P238, DOI 10.1016/j.patcog.2017.04.013
   Goswami G., 2013, 2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems BTAS, P1
   He KM, 2015, Arxiv, DOI [arXiv:1512.03385, 10.48550/arxiv.1512.03385]
   He R, 2020, IEEE T PATTERN ANAL, V42, P1025, DOI 10.1109/TPAMI.2019.2961900
   HengWang Du Tran, 2020, P IEEECVF C COMPUTER
   Hernandez M, 2012, EUR SIGNAL PR CONF, P1995
   Howard A, 2019, Arxiv, DOI [arXiv:1905.02244, DOI 10.48550/ARXIV.1905.02244]
   Hu Zhenguo, 2019, IEEECVF C COMPUTER V, P0
   Jiang L, 2020, IEEE T PATTERN ANAL, V42, P2552, DOI 10.1109/TPAMI.2019.2919284
   Kim D, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P133, DOI 10.1109/BTAS.2017.8272691
   Li BYL, 2013, IEEE WORK APP COMP, P186, DOI 10.1109/WACV.2013.6475017
   Liao JS, 2020, IEEE IMAGE PROC, P748, DOI 10.1109/ICIP40778.2020.9190677
   Mantecón T, 2016, IEEE SIGNAL PROC LET, V23, P771, DOI 10.1109/LSP.2016.2553784
   Min R, 2014, IEEE T SYST MAN CY-S, V44, P1534, DOI 10.1109/TSMC.2014.2331215
   Min R, 2012, INT C PATT RECOG, P1739
   Mu GD, 2019, PROC CVPR IEEE, P5766, DOI 10.1109/CVPR.2019.00592
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Parchomiuk Marcin., 2017, 2017 PROGR APPL ELEC, P1, DOI DOI 10.1109/AVSS.2017.8078554
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Singh M, 2018, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2018.00089
   Spreeuwers L, 2011, INT J COMPUT VISION, V93, P389, DOI 10.1007/s11263-011-0426-2
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q, 2021, PROC CVPR IEEE, P3875, DOI 10.1109/CVPR46437.2021.00387
   Wang Qiangchang, 2020, P IEEE CVF C COMPUTE
   Xiong Xingwang, 2019, INT S BENCHMARKING M, P141
   Yin X, 2018, IEEE T IMAGE PROCESS, V27, P964, DOI 10.1109/TIP.2017.2765830
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 48
TC 2
Z9 2
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
DI 10.1145/3527158
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800005
DA 2024-07-18
ER

PT J
AU Zhu, GY
   Zhou, Y
   Yao, R
   Zhu, HC
   Zhao, JQ
AF Zhu, Guanyu
   Zhou, Yong
   Yao, Rui
   Zhu, Hancheng
   Zhao, Jiaqi
TI Cyclic Self-attention for Point Cloud Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Point cloud; self-attention; cyclic pairing; adaptive fuse
ID NETWORKS
AB Point clouds provide a flexible geometric representation for computer vision research. However, the harsh demands for the number of input points and computer hardware are still significant challenges, which hinder their deployment in real applications. To address these challenges, we design a simple and effective module named cyclic self-attention module (CSAM). Specifically, three attention maps of the same input are obtained by cyclically pairing the feature maps, thus exploring the features sufficiently of the attention space of the original input. CSAM can adequately explore the correlation between points to obtain sufficient feature information despite the multiplicative decrease in inputs. Meanwhile, it can direct the computational power to the more essential features, relieving the burden on the computer hardware. We build a point cloud classification network by simply stacking CSAM called cyclic self-attention network (CSAN). We also propose a novel framework for point cloud semantic segmentation called full cyclic self-attention network (FCSAN). By adaptively fusing the original mapping features and the CSAM extracted features, it can better capture the context information of point clouds. Extensive experiments on several benchmark datasets show that our methods can achieve competitive performance in classification and segmentation tasks.
C1 [Zhu, Guanyu; Zhou, Yong; Yao, Rui; Zhu, Hancheng] China Univ Min & Technol, Engn Res Ctr Mine Digitizat, Sch Comp Sci & Technol, Minist Educ Peoples Republ China, 1 Daxue Rd, Xuzhou, Jiangsu, Peoples R China.
   [Zhao, Jiaqi] China Univ Min & Technol, Innovat Res Ctr Disaster Intelligent Prevent & Em, Minist Educ Peoples Republ China, Sch Comp Sci & Technol Engn,Res Ctr Mine Digitiza, 1 Daxue Rd, Xuzhou, Jiangsu, Peoples R China.
C3 China University of Mining & Technology; China University of Mining &
   Technology
RP Zhou, Y (corresponding author), China Univ Min & Technol, Engn Res Ctr Mine Digitizat, Sch Comp Sci & Technol, Minist Educ Peoples Republ China, 1 Daxue Rd, Xuzhou, Jiangsu, Peoples R China.
EM tb19170009b2@cumt.edu.cn; yzhou@cumt.edu.cn; ruiyao@cumt.edu.cn;
   zhuhancheng@cumt.edu.cn; jiaqizhao@cumt.edu.cn
RI Wang, zhenhua/KFA-8731-2024; chen, huan/KEC-2019-2024; TIAN,
   YI/KHU-9704-2024; Li, Kunpeng/KFS-6306-2024; Wang, Zejun/KBB-8454-2024;
   Yin, Jing/KDO-6274-2024
OI Yao, Rui/0000-0003-2734-915X; Zhu, Guanyu/0000-0002-4867-7235
FU National Natural Science Foundation of China [62172417, 62101555,
   61806206, 61772530]; Natural Science Foundation of Jiangsu Province
   [BK20180639, BK20201346, BK20210488]; Six Talent Peaks Project in
   Jiangsu Province [2015-DZXX-010, 2018-XYDXX-044]
FX This work was supported by the National Natural Science Foundation of
   China (Nos. 62172417, 62101555, 61806206, 61772530), the Natural Science
   Foundation of Jiangsu Province (Nos. BK20180639, BK20201346,
   BK20210488), and the Six Talent Peaks Project in Jiangsu Province (Nos.
   2015-DZXX-010, 2018-XYDXX-044).
CR Ainam JP, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377352
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Ben-Shabat Y, 2018, IEEE ROBOT AUTOM LET, V3, P3145, DOI 10.1109/LRA.2018.2850061
   Boulch A, 2020, COMPUT GRAPH-UK, V88, P24, DOI 10.1016/j.cag.2020.02.005
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen C, 2019, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR.2019.00513
   Chen XZ, 2017, IEEE INT C INT ROBOT, P783, DOI 10.1109/IROS.2017.8202239
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Engelmann Francis, 2018, P EUROPEAN C COMPUTE, P0
   Feng MT, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107446
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Gao Yongbin, 2022, IEEE T INTELL TRANSP, V2022
   Goyal A, 2021, PR MACH LEARN RES, V139
   Guo MH, 2021, Arxiv, DOI arXiv:2012.09688
   Han WK, 2020, AAAI CONF ARTIF INTE, V34, P10925
   Huang FR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3388861
   Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278
   Huiyu Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P108, DOI 10.1007/978-3-030-58548-8_7
   Kaul C, 2021, INT C PATT RECOG, P7211, DOI 10.1109/ICPR48806.2021.9412731
   Khan S., 2021, ARXIV210101169
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Li YZ, 2018, ADV NEUR IN, V31
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Liu JX, 2019, IEEE I CONF COMP VIS, P7545, DOI 10.1109/ICCV.2019.00764
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Liu Z, 2020, AAAI CONF ARTIF INTE, V34, P11677
   Liu ZJ, 2019, ADV NEUR IN, V32
   Loshchilov Ilya, 2016, arXiv
   Mao JG, 2019, IEEE I CONF COMP VIS, P1578, DOI 10.1109/ICCV.2019.00166
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Misra Ishan, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision, P2906
   Nie Weizhi, 2020, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), V16, P1
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qiu S, 2021, IEEE WINT CONF APPL, P3812, DOI 10.1109/WACV48630.2021.00386
   Rao YM, 2019, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2019.00054
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xie ZY, 2020, NEUROCOMPUTING, V402, P245, DOI 10.1016/j.neucom.2020.03.086
   Xu CF, 2021, IEEE INT C INT ROBOT, P4589, DOI 10.1109/IROS51168.2021.9636858
   Xu QG, 2020, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR42600.2020.00570
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang Z, 2019, IEEE I CONF COMP VIS, P7504, DOI 10.1109/ICCV.2019.00760
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zeng W, 2019, LECT NOTES COMPUT SC, V11131, P314, DOI 10.1007/978-3-030-11015-4_24
   Zhang M, 2021, Arxiv, DOI arXiv:2109.11835
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169
   Zhang ZY, 2019, INT CONF 3D VISION, P204, DOI 10.1109/3DV.2019.00031
   Zhao C, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108626
   Zhao H., 2020, P IEEE CVF C COMP VI, V2020, P10076, DOI 10.1109/CVPR42600.2020.01009
   Zhou Wei, 2021, ARXIV
NR 65
TC 3
Z9 3
U1 1
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
AR 49
DI 10.1145/3538648
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800023
DA 2024-07-18
ER

PT J
AU Ignat, O
   Castro, S
   Zhou, YH
   Bao, JJ
   Shan, DD
   Mihalcea, R
AF Ignat, Oana
   Castro, Santiago
   Zhou, Yuhang
   Bao, Jiajun
   Shan, Dandan
   Mihalcea, Rada
TI When Did It Happen? Duration-informed Temporal Localization of Narrated
   Actions in Vlogs
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Action temporal localization; action duration; vlogs; natural language
   processing; video processing; multimodal processing
AB We consider the task of temporal human action localization in lifestyle vlogs. We introduce a novel dataset consisting of manual annotations of temporal localization for 13,000 narrated actions in 1,200 video clips. We present an extensive analysis of this data, which allows us to better understand how the language and visual modalities interact throughout the videos. We propose a simple yet effective method to localize the narrated actions based on their expected duration. Through several experiments and analyses, we show that our method brings complementary information with respect to previousmethods, and leads to improvements over previous work for the task of temporal action localization.
C1 [Ignat, Oana; Castro, Santiago; Zhou, Yuhang; Bao, Jiajun; Shan, Dandan; Mihalcea, Rada] Univ Michigan, 500 S State St, Ann Arbor, MI 48109 USA.
C3 University of Michigan System; University of Michigan
RP Ignat, O (corresponding author), Univ Michigan, 500 S State St, Ann Arbor, MI 48109 USA.
EM oignat@umich.edu; sacastro@umich.edu; tonyzhou@umich.edu;
   jiajunb@umich.edu; dandans@umich.edu; mihalcea@umich.edu
FU Automotive Research Center (ARC) at the University of Michigan
   [W56HZV-19-2-0001]
FX This research was partially supported by a grant from the Automotive
   Research Center (ARC) at the University of Michigan in accordance with
   Cooperative Agreement W56HZV-19-2-0001.
CR Hudson DA, 2019, Arxiv, DOI arXiv:1902.09506
   Sigurdsson GA, 2018, Arxiv, DOI arXiv:1804.09626
   Abu-El-Haija Sami, 2016, arXiv, DOI [DOI 10.48550/ARXIV.1609.08675, DOI 10.48550/-ARXIV.1609.08675]
   Alayrac JB, 2016, PROC CVPR IEEE, P4575, DOI 10.1109/CVPR.2016.495
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Hendricks LA, 2018, Arxiv, DOI arXiv:1809.01337
   [Anonymous], 2017, Proceedings of the IEEE conference on computer vision and pattern recognition
   [Anonymous], ACM INT C IM VID
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen D., 2014, P 2014 C EMPIRICAL M, P740, DOI DOI 10.3115/V1/D14-1082
   Devlin J., 2018, BERT PRE TRAINING DE
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Escorcia V, 2022, Arxiv, DOI arXiv:1907.12763
   FLEISS JL, 1973, EDUC PSYCHOL MEAS, V33, P613, DOI 10.1177/001316447303300309
   Fouhey DF, 2018, PROC CVPR IEEE, P4991, DOI 10.1109/CVPR.2018.00524
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Ghosh S, 2019, Arxiv, DOI arXiv:1904.02755
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Graves A., 2008, Studies in Computational Intelligence
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Ignat Oana, 2019, P ACL
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   KRIPPEND.K, 1970, EDUC PSYCHOL MEAS, V30, P61, DOI 10.1177/001316447003000105
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Lei J, 2020, Arxiv, DOI arXiv:1904.11574
   Levy O, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P302, DOI 10.3115/v1/p14-2050
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Lu JS, 2019, ADV NEUR IN, V32
   Miech A., 2019, arXiv, DOI DOI 10.48550/ARXIV.1906.03327
   Miech A., 2019, arXiv
   Mikolov T, 2013, Arxiv, DOI [arXiv:1301.3781, 10.48550/arXiv.1301.3781]
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Monfort M, 2020, IEEE T PATTERN ANAL, V42, P502, DOI 10.1109/TPAMI.2019.2901464
   Motwani TS, 2012, FRONT ARTIF INTEL AP, V242, P600, DOI 10.3233/978-1-61499-098-7-600
   Palaskar S, 2019, Arxiv, DOI arXiv:1906.07901
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801
   Shi BT, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6382
   Sigurdsson GA, 2017, IEEE I CONF COMP VIS, P2156, DOI 10.1109/ICCV.2017.235
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Suhr A, 2019, Arxiv, DOI arXiv:1811.00491
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Tan H, 2019, Arxiv, DOI [arXiv:1908.07490, 10.48550/arXiv.1908.07490]
   Tang YS, 2019, PROC CVPR IEEE, P1207, DOI 10.1109/CVPR.2019.00130
   Tran D, 2015, Arxiv, DOI [arXiv:1412.0767, 10.48550/ARXIV.1412.0767]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang MZ, 2016, LECT NOTES COMPUT SC, V9912, P696, DOI 10.1007/978-3-319-46484-8_42
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yuan YT, 2018, Arxiv, DOI arXiv:1804.07014
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhukov D, 2019, PROC CVPR IEEE, P3532, DOI 10.1109/CVPR.2019.00365
NR 61
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2022
VL 18
IS 3
SU S
SI SI
AR 142
DI 10.1145/3495211
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0AE2
UT WOS:000951829800012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Buckchash, H
   Raman, B
AF Buckchash, Himanshu
   Raman, Balasubramanian
TI GraSP: Local Grassmannian Spatio-Temporal Patterns for Unsupervised Pose
   Sequence Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Spatio-temporal patterns; unsupervised pose sequence recognition;
   grassmann manifolds; action recognition
AB Many applications of action recognition, especially broad domains like surveillance or anomaly-detection, favor unsupervised methods considering that exhaustive labeling of actions is not possible. However, very limited work has happened in this domain. Moreover, the existing self-supervised approaches suffer from their dependency upon labeled data for finetuning. To this end, this paper puts forward a manifold based unsupervised pose-sequence recognition approach that leverages only the natural biases present in the data. It works by clustering the projections of temporal derivatives of the fragmented data on the Grassmann manifold. Temporal derivatives are formed by the inter-frame gradients with local and global metrics. To commensurate with this, a dynamic view-invariant pose representation is proposed. Additionally, a variable aggregation step is introduced for better feature vector quantization. Extensive empirical evaluation and ablations on several challenging datasets under three categories confirm the superiority of the proposed approach in contrast to current methods.
C1 [Buckchash, Himanshu; Raman, Balasubramanian] Indian Inst Technol Roorkee, Roorkee, Uttarakhand, India.
   [Buckchash, Himanshu; Raman, Balasubramanian] Indian Inst Technol, Dept Comp Sci & Engn, Machine Vis Lab S211, Roorkee 247667, Uttarakhand, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Roorkee
RP Buckchash, H (corresponding author), Indian Inst Technol Roorkee, Roorkee, Uttarakhand, India.; Buckchash, H (corresponding author), Indian Inst Technol, Dept Comp Sci & Engn, Machine Vis Lab S211, Roorkee 247667, Uttarakhand, India.
EM hbuckchash@cs.iitr.ac.in; bala@cs.iitr.ac.in
FU Ministry of Electronics and Information Technology [MIT1100CSE]
FX This work was supported by the Ministry of Electronics and Information
   Technology, Govt. of India, under grant no. MIT1100CSE.
CR Amelio A, 2015, PROCEEDINGS OF THE 2015 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2015), P1584, DOI 10.1145/2808797.2809344
   [Anonymous], 2014, INT J SIGNAL PROCESS, DOI DOI 10.14257/IJSIP.2014.7.3.11
   [Anonymous], 2018, Advances in Neural Information Processing Systems
   Begelfor Evgeni, 2006, P IEEE C COMP VIS PA, V2, P2087, DOI DOI 10.1109/CVPR.2006.50
   Ben Tanfous A, 2018, PROC CVPR IEEE, P2840, DOI 10.1109/CVPR.2018.00300
   BoyueWang Yongli Hu, 2016, AAAI C ART INT, V30
   Buckchash H, 2020, IEEE IMAGE PROC, P2406, DOI [10.1109/icip40778.2020.9190765, 10.1109/ICIP40778.2020.9190765]
   Chen C, 2016, J REAL-TIME IMAGE PR, V12, P155, DOI 10.1007/s11554-013-0370-1
   Cheng Y.B., 2021, IEEE INT C MULTIMEDI, P1, DOI [DOI 10.1109/ICME51207.2021.9428459, 10.1109/ICME51207.2021.9428459]
   CMU, 2007, CARN MELL MOT CAPT D
   Derkach D, 2019, INT J COMPUT VISION, V127, P1565, DOI 10.1007/s11263-019-01208-x
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   FOWLKES EB, 1983, J AM STAT ASSOC, V78, P553, DOI 10.2307/2288117
   Ghahramani Z, 2004, LECT NOTES ARTIF INT, V3176, P72
   Gharaee Z, 2020, COGN SYST RES, V63, P11, DOI 10.1016/j.cogsys.2020.05.002
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gowayyed M.A., 2013, Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, P1351
   Holden D., 2015, P SIGGRAPH AS TECH B, DOI DOI 10.1145/2820903.2820918
   Huang HE, 2020, J VIS COMMUN IMAGE R, V73, DOI 10.1016/j.jvcir.2020.102925
   Huang ZW, 2017, PROC CVPR IEEE, P1243, DOI 10.1109/CVPR.2017.137
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Pham HH, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19081932
   Kaufmann M, 2020, INT CONF 3D VISION, P918, DOI 10.1109/3DV50981.2020.00102
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu R, 2020, IISE T OCCUP ERG HUM, V8, P175, DOI 10.1080/24725838.2021.1875519
   Ludl D, 2019, IEEE INT C INTELL TR, P581, DOI 10.1109/ITSC.2019.8917128
   Lui YM, 2010, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2010.5540131
   Ma Yi, 1998, C MATH THEOR NETW SY
   Morais R, 2019, PROC CVPR IEEE, P11988, DOI 10.1109/CVPR.2019.01227
   Qiang Nie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P102, DOI 10.1007/978-3-030-58529-7_7
   Rao Haocong, 2020, ARXIV200800188
   Rosenberg A, 2007, P 2007 JOINT C EMP M, P410, DOI [10 . 7916 / D80V8N84, DOI 10.7916/D80V8N84]
   Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Slama R, 2015, PATTERN RECOGN, V48, P556, DOI 10.1016/j.patcog.2014.08.011
   Sloane N. J. A., 1996, EXPT MATH, V5, P139
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sundararajan Kalaivani, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P50, DOI 10.1109/CVPRW.2015.7301354
   Thien HT, 2020, INFORM SCIENCES, V513, P112, DOI 10.1016/j.ins.2019.10.047
   Turaga P., 2008, 2008 IEEE C COMPUTER, P1, DOI [10.1109/CVPR.2008.4587733, DOI 10.1109/CVPR.2008.4587733]
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang HS, 2018, PATTERN RECOGN, V81, P23, DOI 10.1016/j.patcog.2018.03.030
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang MJ, 2017, PROC CVPR IEEE, P6053, DOI 10.1109/CVPR.2017.641
   Wang P, 2016, LECT NOTES COMPUT SC, V9911, P370, DOI 10.1007/978-3-319-46478-7_23
   Yu JH, 2020, IEEE ACCESS, V8, P43243, DOI 10.1109/ACCESS.2020.2977856
   Yui Man Lui, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P97, DOI 10.1109/FG.2011.5771378
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang Z, 2021, INT J COMPUT VISION, V129, P703, DOI 10.1007/s11263-020-01398-9
   Zhao R, 2019, IEEE I CONF COMP VIS, P6881, DOI 10.1109/ICCV.2019.00698
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
   Zhou YR, 2018, AAAI CONF ARTIF INTE, P4596
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
NR 60
TC 1
Z9 1
U1 3
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 79
DI 10.1145/3491227
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600013
DA 2024-07-18
ER

PT J
AU Xu, HT
   Jin, XB
   Wang, QF
   Hussain, A
   Huang, KZ
AF Xu, Haotian
   Jin, Xiaobo
   Wang, Qiufeng
   Hussain, Amir
   Huang, Kaizhu
TI Exploiting Attention-Consistency Loss For Spatial-Temporal Stream Action
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Action recognition; attention consistency; multi-level attention;
   two-stream structure
ID FORM
AB Currently, many action recognition methods mostly consider the information from spatial streams. We propose a new perspective inspired by the human visual system to combine both spatial and temporal streams to measure their attention consistency. Specifically, a branch-independent convolutional neural network (CNN) based algorithm is developed with a novel attention-consistency loss metric, enabling the temporal stream to concentrate on consistent discriminative regions with the spatial stream in the same period. The consistency loss is further combined with the cross-entropy loss to enhance the visual attention consistency. We evaluate the proposed method for action recognition on two benchmark datasets: Kinetics400 and UCF101. Despite its apparent simplicity, our proposed framework with the attention consistency achieves better performance than most of the two-stream networks, i.e., 75.7% top-1 accuracy on Kinetics400 and 95.7% on UCF101, while reducing 7.1% computational cost compared with our baseline. Particularly, our proposed method can attain remarkable improvements on complex action classes, showing that our proposed network can act as a potential benchmark to handle complicated scenarios in industry 4.0 applications.
C1 [Xu, Haotian; Jin, Xiaobo; Wang, Qiufeng] Xian Jiaotong Liverpool Univ, 111 Renal Rd, Suzhou 215000, Jiangsu, Peoples R China.
   [Hussain, Amir] Edinburgh Napier Univ, Edinburgh EH11 4BN, Midlothian, Scotland.
   [Huang, Kaizhu] Duke Kunshan Univ, 8 Duke Ave, Kunshan 215316, Jiangsu, Peoples R China.
C3 Xi'an Jiaotong-Liverpool University; Edinburgh Napier University; Duke
   Kunshan University
RP Jin, XB (corresponding author), Xian Jiaotong Liverpool Univ, 111 Renal Rd, Suzhou 215000, Jiangsu, Peoples R China.; Huang, KZ (corresponding author), Duke Kunshan Univ, 8 Duke Ave, Kunshan 215316, Jiangsu, Peoples R China.
EM haotian.xu18@student.xjtlu.edu.cn; xiaobo.jin@xjtlu.edu.cn;
   qiufeng.wang@xjtlu.edu.cn; A.Hussain@napier.ac.uk;
   kaizhu.huang@dukekunshan.edu.cn
RI Hussain, Amir/AAG-6299-2020; Wang, Qiufeng/ACA-9839-2022; Huang,
   Kaizhu/O-4721-2014
OI Hussain, Amir/0000-0002-8080-082X; Huang, Kaizhu/0000-0002-3034-9639
CR Arnab A., 2021, arXiv, DOI DOI 10.48550/ARXIV.2103.15691
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bertasius G, 2021, Arxiv, DOI arXiv:2102.05095
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen YP, 2018, LECT NOTES COMPUT SC, V11205, P364, DOI 10.1007/978-3-030-01246-5_22
   DERRINGTON AM, 1984, J PHYSIOL-LONDON, V357, P219, DOI 10.1113/jphysiol.1984.sp015498
   Diba A, 2017, Arxiv, DOI arXiv:1711.08200
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan HQ, 2021, Arxiv, DOI arXiv:2104.11227
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gavrilyuk K, 2018, PROC CVPR IEEE, P5958, DOI 10.1109/CVPR.2018.00624
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang K, 2019, Deep learning: fundamentals, theory and applications, V2, DOI DOI 10.1007/978-3-030-06073-2
   HUBEL DH, 1965, J NEUROPHYSIOL, V28, P229, DOI 10.1152/jn.1965.28.2.229
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Li C, 2022, IEEE T NEUR NET LEAR, V33, P4800, DOI 10.1109/TNNLS.2021.3061115
   Li YG, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3378026
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   LIVINGSTONE M, 1988, SCIENCE, V240, P740, DOI 10.1126/science.3283936
   Sevilla-Lara Laura, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P281, DOI 10.1007/978-3-030-12939-2_20
   Sharir G., 2021, arXiv
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Stroud JC, 2020, IEEE WINT CONF APPL, P614, DOI 10.1109/wacv45572.2020.9093274
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   VANESSEN DC, 1994, NEURON, V13, P1, DOI 10.1016/0896-6273(94)90455-3
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu Haotian, 2020, INT C NEUR INF PROC, P815
   Zhang JX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321511
   Zhao Y, 2018, ADV NEUR IN, V31
   Zhenzhi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P34, DOI 10.1007/978-3-030-58595-2_3
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 51
TC 8
Z9 8
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 119
DI 10.1145/3538749
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000006
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Hsu, CF
   Hung, TH
   Hsu, CH
AF Hsu, Chih-Fan
   Hung, Tse-Hou
   Hsu, Cheng-Hsin
TI Optimizing Immersive Video Coding Configurations Using Deep Learning: A
   Case Study on TMIV
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Virtual reality; augmented reality; extended reality; 3DoF+; 6DoF;
   head-mounted displays; view synthesis; streaming; optimization
AB Immersive video streaming technologies improve Virtual Reality (VR) user experience by providing users more intuitive ways to move in simulated worlds, e.g., with 6 Degree-of-Freedom (6DoF) interaction mode. A naive method to achieve 6DoF is deploying cameras at numerous different positions and orientations that may be required based on users' movement, which unfortunately is expensive, tedious, and inefficient. A better solution for realizing 6DoF interactions is to synthesize target views on-the-fly from a limited number of source views. While such view synthesis is enabled by the recent Test Model for Immersive Video (TMIV) codec, TMIV dictates manually-composed configurations, which cannot exercise the tradeoff among video quality, decoding time, and bandwidth consumption. In this article, we study the limitation of TMIV and solve its configuration optimization problem by searching for the optimal configuration in a huge configuration space. We first identify the critical parameters in the TMIV configurations. Then, we introduce two Neural Network (NN)-based algorithms from two heterogeneous aspects: (i) a Convolutional Neural Network (CNN) algorithm solving a regression problem and (ii) a Deep Reinforcement Learning (DRL) algorithm solving a decision making problem, respectively. We conduct both objective and subjective experiments to evaluate the CNN and DRL algorithms on two diverse datasets: an equirectangular and a perspective projection dataset. The objective evaluations reveal that both algorithms significantly outperform the default configurations. In particular, with the equirectangular (perspective) projection dataset, the proposed algorithms only require 95% (23%) decoding time, stream 79% (23%) views, and improve the utility by 6% (73%) on average. The subjective evaluations confirm the proposed algorithms consume fewer resources while achieving comparable Quality of Experience (QoE) than the default and the optimal TMIV configurations.
C1 [Hsu, Chih-Fan; Hung, Tse-Hou; Hsu, Cheng-Hsin] Natl Tsing Hua Univ, 101 Sec 2 Kuang Fu Rd, Hsinchu 300, Taiwan.
   [Hsu, Chih-Fan] Natl Yang Ming Chiao Tung Univ, 1001 Univ Rd, Hsinchu 300, Taiwan.
C3 National Tsing Hua University; National Yang Ming Chiao Tung University
RP Hsu, CF (corresponding author), Natl Tsing Hua Univ, 101 Sec 2 Kuang Fu Rd, Hsinchu 300, Taiwan.; Hsu, CF (corresponding author), Natl Yang Ming Chiao Tung Univ, 1001 Univ Rd, Hsinchu 300, Taiwan.
EM hsuchihfan@gmail.com; tsehou.nthu@gmail.com; chsu@cs.nthu.edu.tw
OI Hsu, Cheng-Hsin/0000-0002-8116-2591; Hsu, Chih-Fan/0000-0002-4180-8255
FU Ministry of Science and Technology of Taiwan [107-2221-E-007-091-MY3,
   107-2221-E-009-028-MY3, 110-2218-E-A49-011-MBK, 109-2917-I-564-041]
FX This work was partially supported by the Ministry of Science and
   Technology of Taiwan under the grants: #107-2221-E-007-091-MY3,
   #107-2221-E-009-028-MY3, #110-2218-E-A49-011-MBK, and
   #109-2917-I-564-041.
CR Altamimi S, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3397227
   Avidan S, 1997, PROC CVPR IEEE, P1034, DOI 10.1109/CVPR.1997.609457
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Chan Y, 2011, IEEE T NANOTECHNOL, V10, P947, DOI 10.1109/TNANO.2010.2090170
   Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153
   Cheng B, 2015, IEEE T AUTOM SCI ENG, V12, P1104, DOI 10.1109/TASE.2014.2387212
   Chiariotti F, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P77, DOI 10.1145/2910017.2910603
   Chung CH, 2017, I S INTELL SIG PROC, P570, DOI 10.1109/ISPACS.2017.8266543
   Claeys M, 2014, IEEE COMMUN LETT, V18, P716, DOI 10.1109/LCOMM.2014.020414.132649
   Corbillon X, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P237, DOI 10.1145/3204949.3204968
   Costero L, 2019, DES AUT TEST EUROPE, P558, DOI [10.23919/date.2019.8715256, 10.23919/DATE.2019.8715256]
   Dore R., 2018, MPEG2018N17617 ISOIE
   Dziembowski A, 2018, INT CONF SIGNALS ELE, P83, DOI 10.1109/ICSES.2018.8507338
   Fan CL, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329119
   Fleureau J., 2020, 2020 IEEE INT C MULT, P1
   Fu J, 2019, IEEE INT CON MULTI, P290, DOI 10.1109/ICME.2019.00058
   Gadaleta M, 2017, IEEE T COGN COMMUN, V3, P703, DOI 10.1109/TCCN.2017.2755007
   Gescheider G. A., 2013, Psychophysics: the fundamentals
   Ghosh A., 2017, ARXIV PREPRINT ARXIV
   Hosseini M, 2017, MULTIMEDIA SYST, V23, P421, DOI 10.1007/s00530-016-0511-z
   Hu JH, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351575
   Huang JW, 2017, P IEEE VIRT REAL ANN, P37, DOI 10.1109/VR.2017.7892229
   Huang TC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1208, DOI 10.1145/3240508.3240545
   Jeong Jong-Beom, 2020, P 28 ACM INT C MULT, P3687
   Jiang XL, 2018, C LOCAL COMPUT NETW, P393, DOI 10.1109/LCN.2018.8638092
   Jung J., 2019, MPEGN18563 ISOIEC JT
   Kapov L., 2018, ACM T MULTIM COMPUT, V14
   Kingma D.P., 2014, ARXIV14126980
   MPEG, 2021, MPEGW20003 ISOIEC JT
   MPEG, 2019, HM1616
   MPEG, 2020, MPEGW19579 ISOIEC JT
   Müller K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/438148
   Pang HT, 2019, IEEE INFOCOM SER, P991, DOI 10.1109/INFOCOM.2019.8737395
   PLACKETT RL, 1975, ROY STAT SOC C-APP, V24, P193
   Salahieh B., 2019, MPEGN1857 ISOIEC JTC
   Salahieh B., 2019, US Patent, Patent No. [2019/0320164 A1, 20190320164]
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun YL, 2017, IEEE SIGNAL PROC LET, V24, P1408, DOI 10.1109/LSP.2017.2720693
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Tian D, 2009, PROC SPIE, V7443, DOI 10.1117/12.829372
   van der Hooft J, 2015, PROCEEDINGS OF THE 2015 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM), P131, DOI 10.1109/INM.2015.7140285
   Yaqoob A, 2020, IEEE COMMUN SURV TUT, V22, P2801, DOI 10.1109/COMST.2020.3006999
   Zhang YX, 2019, IEEE INFOCOM SER, P1252, DOI [10.1109/INFOCOM.2019.8737361, 10.1109/infocom.2019.8737361]
   ZION Market Research, 2018, VIRT REAL VR MARK HA
NR 44
TC 1
Z9 2
U1 3
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 19
DI 10.1145/3471191
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900019
DA 2024-07-18
ER

PT J
AU Yao, P
   Feng, JQ
AF Yao, Peng
   Feng, Jieqing
TI Sparse LIDAR Measurement Fusion with Joint Updating Cost for Fast Stereo
   Matching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Stereo matching; LIDAR-Stereo fusion; Tree Filtering
ID AGGREGATION; PREDICTION; ACCURATE
AB The complementary virtues of active and passive depth sensors inspire the LIDAR-Stereo fusion for enhancing the accuracy of stereo matching. However, most of the fusion based stereo matching algorithms have exploited dense LIDAR priors with single fusion methodology. In this paper, we intend to break these fetters, utilizing sparse LIDAR priors with multi-step fusion strategy for obtaining accurate disparity estimation more efficiently. At first, random sparse sampling LIDAR depth measurements are provided in Naive Fusion for updating the matching cost of Semi-Global Matching (SGM). Then Neighborhood Based Fusion is performed based on the former step for further updating the cost. Subsequently, Diffusion Based Fusion is utilized to update both the cost and disparities. At last, Tree Filtering is applied for removing speckle outliers and smoothing disparities. Performance evaluations on various stereo data sets demonstrate that the proposed algorithm outperforms other most challenging stereo matching algorithms significantly with approximately real-time implementation efficiency. Furthermore, it is worth pointing out that our proposal surprisingly possesses one of the top ten performances on Middlebury v.3 online evaluation system even if it has not been adopted any learning-based techniques.
C1 [Yao, Peng; Feng, Jieqing] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Feng, JQ (corresponding author), Zhejiang Univ, Sch Comp Sci & Technol, State Key Lab CAD&CG, 866 Yuhang Tang Rd, Hangzhou 310030, Zhejiang, Peoples R China.
EM yp19880120@sina.com; jqfeng@cad.zju.edu.cn
FU National Natural Science Foundation of China [61732015, 61932018,
   61472349]
FX This research was jointly supported by the National Natural Science
   Foundation of China under Grants Nos. 61732015, 61932018, and 61472349.
CR [Anonymous], 2007, IEEE Conference on Computer Vision and Pattern Recognition
   Badino H., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P405, DOI 10.1109/3DIMPVT.2011.58
   Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191
   Batsos K, 2018, PROC CVPR IEEE, P2060, DOI 10.1109/CVPR.2018.00220
   Batsos K, 2018, INT CONF 3D VISION, P238, DOI 10.1109/3DV.2018.00036
   Besse F, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.132
   Bleyer M, 2013, ADV COMPUT VIS PATT, P143, DOI 10.1007/978-1-4471-5520-1_6
   Chen SX, 2020, IEEE J-STARS, V13, P2081, DOI 10.1109/JSTARS.2020.2992298
   Cheng XL, 2019, PROC CVPR IEEE, P6332, DOI 10.1109/CVPR.2019.00650
   Cui TT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17010070
   Drory A, 2014, LECT NOTES COMPUT SC, V8753, P43, DOI 10.1007/978-3-319-11752-2_4
   Facciolo G., 2015, Procedings of the British Machine Vision Conference 2015 (Swansea: British Machine Vision Association), p90.1, DOI DOI 10.5244/C.29.90
   Fischer Jan, 2011, IEEE International Conference on Robotics and Automation, P3548
   Gandhi V, 2012, IEEE INT CONF ROBOT, P4742, DOI 10.1109/ICRA.2012.6224771
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Geiger A, 2011, LECT NOTES COMPUT SC, V6492, P25, DOI 10.1007/978-3-642-19315-6_3
   Gidaris S, 2017, PROC CVPR IEEE, P7187, DOI 10.1109/CVPR.2017.760
   Gong ML, 2007, INT J COMPUT VISION, V75, P283, DOI 10.1007/s11263-006-0032-x
   Gong Z, 2020, ISPRS J PHOTOGRAMM, V159, P90, DOI 10.1016/j.isprsjprs.2019.10.015
   Haeusler R, 2013, PROC CVPR IEEE, P305, DOI 10.1109/CVPR.2013.46
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297
   Maddern W, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P2181, DOI 10.1109/IROS.2016.7759342
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Menze M, 2015, ISPRS ANN PHOTO REM, VII-3, P427, DOI 10.5194/isprsannals-II-3-W5-427-2015
   Min DB, 2013, IEEE T PATTERN ANAL, V35, P2539, DOI 10.1109/TPAMI.2013.15
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Park MG, 2015, PROC CVPR IEEE, P101, DOI 10.1109/CVPR.2015.7298605
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Poggi M, 2019, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2019.00107
   Poggi M, 2016, INT CONF 3D VISION, P509, DOI 10.1109/3DV.2016.61
   Premebida C, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P2469
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Seki A., 2016, BMVC, V2, P4
   Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703
   Shivakumar SS, 2019, IEEE INT CONF ROBOT, P6482, DOI [10.1109/icra.2019.8794023, 10.1109/ICRA.2019.8794023]
   Spyropoulos A, 2016, INT J COMPUT VISION, V118, P300, DOI 10.1007/s11263-015-0877-y
   Spyropoulos A, 2014, PROC CVPR IEEE, P1621, DOI 10.1109/CVPR.2014.210
   Taniai T, 2018, IEEE T PATTERN ANAL, V40, P2725, DOI 10.1109/TPAMI.2017.2766072
   Taniai T, 2014, PROC CVPR IEEE, P1613, DOI 10.1109/CVPR.2014.209
   Tombari F., 2008, Computer Vision and Pattern Recognition, P1
   Wang Z., 2020, INT ARCH PHOTOGRAMM, V43, P375, DOI [10.5194/isprs-archives-XLIII-B2-2020-375-2020, DOI 10.5194/ISPRS-ARCHIVES-XLIII-B2-2020-375-2020]
   Yang QQ, 2015, IEEE SIGNAL PROC LET, V22, P1429, DOI 10.1109/LSP.2015.2409203
   Yang QX, 2015, IEEE T PATTERN ANAL, V37, P834, DOI 10.1109/TPAMI.2014.2353642
   Yang QX, 2012, PROC CVPR IEEE, P1402, DOI 10.1109/CVPR.2012.6247827
   Yao P, 2021, MACH VISION APPL, V32, DOI 10.1007/s00138-021-01211-8
   Yao P, 2019, IET IMAGE PROCESS, V13, P98, DOI 10.1049/iet-ipr.2018.5801
   Yao P, 2018, IET COMPUT VIS, V12, P908, DOI 10.1049/iet-cvi.2017.0599
   Yao P, 2018, LECT NOTES COMPUT SC, V10704, P67, DOI 10.1007/978-3-319-73603-7_6
   Yin JH, 2017, PATTERN RECOGN, V71, P278, DOI 10.1016/j.patcog.2017.06.015
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767
   Zhang C, 2014, LECT NOTES COMPUT SC, V8690, P112, DOI 10.1007/978-3-319-10605-2_8
   Zhang SM, 2016, INT ARCH PHOTOGRAMM, V41, P741, DOI 10.5194/isprsarchives-XLI-B5-741-2016
   Zhu JJ, 2008, PROC CVPR IEEE, P3262
NR 60
TC 0
Z9 0
U1 3
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 1
DI 10.1145/3471870
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900001
DA 2024-07-18
ER

PT J
AU Zhang, JH
   Feng, ZY
   Su, Y
   Xing, M
AF Zhang, Jianhai
   Feng, Zhiyong
   Su, Yong
   Xing, Meng
TI Bayesian Covariance Representation with Global Informative Prior for 3D
   Action Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Covariance matrix; 3D action recognition; riemannian manifold; Bayesian
   regularization
ID REGION COVARIANCE; DESCRIPTOR; MATRICES; ENSEMBLE; POSE
AB For the merits of high-order statistics and Riemannian geometry, covariance matrix has become a generic feature representation for action recognition. An independent action can be represented by an empirical statistics over all of its pose samples. Two major problems of covariance include the following: (1) it is prone to be singular so that actions fail to be represented properly, and (2) it is short of global action/pose-aware information so that expressive and discriminative power is limited. In this article, we propose a novel Bayesian covariance representation by a prior regularization method to solve the preceding problems. Specifically, covariance is viewed as a parametric maximum likelihood estimate of Gaussian distribution over local poses from an independent action. Then, a Global Informative Prior (GIP) is generated over global poses with sufficient statistics to regularize covariance. In this way, (1) singularity is greatly relieved due to sufficient statistics, (2) global pose information of GIP makes Bayesian covariance theoretically equivalent to a saliency weighting covariance over global action poses so that discriminative characteristics of actions can be represented more clearly. Experimental results show that our Bayesian covariance with GIP efficiently improves the performance of action recognition. In some databases, it outperforms the state-of-the-art variant methods that are based on kernels, temporal-order structures, and saliency weighting attentions, among others.
C1 [Zhang, Jianhai; Feng, Zhiyong; Xing, Meng] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
   [Su, Yong] Tianjin Normal Univ, Tianjin Key Lab Wireless Mobile Commun & Power Tr, Tianjin, Peoples R China.
C3 Tianjin University; Tianjin Normal University
RP Xing, M (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.; Su, Y (corresponding author), Tianjin Normal Univ, Tianjin Key Lab Wireless Mobile Commun & Power Tr, Tianjin, Peoples R China.
EM zhangjianhai@tju.edu.cn; zyfeng@tju.edu.cn; suyong@tju.edu.cn;
   xingmeng@tju.edu.cn
RI Feng, Zhi-Yong/I-7541-2016; su, yong/JEO-5411-2023
OI su, yong/0000-0002-6851-4142; Zhang, Jianhai/0000-0002-0330-6908
CR [Anonymous], 2016, 2016 IEEE Transportation Electrification Conference and Expo (ITEC)
   Arsigny V, 2007, SIAM J MATRIX ANAL A, V29, P328, DOI 10.1137/050637996
   Bonnabel S, 2009, SIAM J MATRIX ANAL A, V31, P1055, DOI 10.1137/080731347
   Carter KM, 2009, IEEE T PATTERN ANAL, V31, P2093, DOI 10.1109/TPAMI.2009.67
   Cavazza J, 2016, INT C PATT RECOG, P408, DOI 10.1109/ICPR.2016.7899668
   Chen C, 2016, J REAL-TIME IMAGE PR, V12, P155, DOI 10.1007/s11554-013-0370-1
   Chen G, 2015, SIGNAL PROCESS, V110, P67, DOI 10.1016/j.sigpro.2014.08.024
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Faraki M, 2015, INT CONF ACOUST SPEE, P1364, DOI 10.1109/ICASSP.2015.7178193
   Garcia-Hernando G, 2017, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.2017.51
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Gowayyed M.A., 2013, Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, P1351
   Harandi M, 2014, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2014.132
   Harandi MT, 2014, LECT NOTES COMPUT SC, V8690, P17, DOI 10.1007/978-3-319-10605-2_2
   Harandi MT, 2012, LECT NOTES COMPUT SC, V7573, P216, DOI 10.1007/978-3-642-33709-3_16
   Huang M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177757
   Huang ZW, 2017, AAAI CONF ARTIF INTE, P2036
   Hussein, 2013, INT JOINT C ART INT
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Koniusz P, 2016, PROC CVPR IEEE, P5395, DOI 10.1109/CVPR.2016.582
   Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3
   Lancewicki T., 2017, ARXIV170706156
   Lancewicki T, 2014, IEEE T SIGNAL PROCES, V62, P6380, DOI 10.1109/TSP.2014.2364784
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Li XB, 2008, CAN J EDUC ADM POLIC, P1
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Minh H.Q., 2017, SYNTH LECT COMPUT VI, V7, P1
   Minh HQ, 2016, PROC CVPR IEEE, P5195, DOI 10.1109/CVPR.2016.561
   Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1
   Nguyen XS, 2019, MACH VISION APPL, V30, P321, DOI 10.1007/s00138-018-0989-9
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Pang YW, 2008, IEEE T CIRC SYST VID, V18, P989, DOI 10.1109/TCSVT.2008.924108
   Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z
   Qiao RZ, 2017, PATTERN RECOGN, V66, P202, DOI 10.1016/j.patcog.2017.01.015
   Rahimi Ali, 2007, ADV NEURAL INFORM PR
   Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4
   S. James Press, 2005, APPL MULTIVARIATE AN
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   Shimodaira H, 2002, ADV NEUR IN, V14, P921
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Tuzel O, 2007, PROC CVPR IEEE, P1736
   Vemulapalli R, 2016, PROC CVPR IEEE, P4471, DOI 10.1109/CVPR.2016.484
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wand M. P., 1994, KERNEL SMOOTHING, DOI DOI 10.1201/B14876
   Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang L, 2015, IEEE I CONF COMP VIS, P4570, DOI 10.1109/ICCV.2015.519
   Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965
   Weng JW, 2017, PROC CVPR IEEE, P445, DOI 10.1109/CVPR.2017.55
   Xiang M, 2018, SIGNAL PROCESS, V148, P193, DOI 10.1016/j.sigpro.2018.02.018
   Yang JQ, 2008, ITESS: 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES, PT 1, P1, DOI 10.1109/CVPR.2008.4587647
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Ye J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3038917
   Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342
   Zhang JH, 2020, SIGNAL PROCESS, V171, DOI 10.1016/j.sigpro.2020.107499
   Zhang JH, 2019, INT CONF ACOUST SPEE, P2132, DOI [10.1109/icassp.2019.8682645, 10.1109/ICASSP.2019.8682645]
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
   Zhu Y, 2013, IEEE COMPUT SOC CONF, P486, DOI 10.1109/CVPRW.2013.78
NR 60
TC 0
Z9 0
U1 2
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 135
DI 10.1145/3460235
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800019
DA 2024-07-18
ER

PT J
AU Alaya, B
AF Alaya, Bechir
TI Payoff-based Dynamic Segment Replication and Graph Classification Method
   with Attribute Vectors Adapted to Urban VANET
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Urban VANET; optimization; video streaming; load balancing; feedback
   control; P-DSR; replication; inter-class load balancing
AB Due to the number of constraints and the dynamic nature of vehicular ad hoc networks (VANET), effective video broadcasting always remains a difficult task. In this work, we proposed a quality of video visualization guarantee model based on a feedback loop and an efficient algorithm for segmenting and replicating video segments using the Payoff-based Dynamic Segment Replication Policy (P-DSR). In the urban VANET environment, P-DSR is defined by taking into account the position of the vehicles, the speed, the direction, the number of neighboring vehicles, and the reputation of each node to stabilize the urban VANET topology. However, the management of various load control parameters between the different components of the urban VANET network remains a problem to be studied. This work uses a multi-objective problem that takes the parameters of our algorithm based on the Graph Classification Method with Attribute Vectors (GCMAV) as input. This algorithm aims to provide an improved class lifetime, an improved video segment delivery rate, a reduced inter-class overload, and an optimization of a global criterion. A scalable algorithm is used to optimize the parameters of the GCMAV. The simulations were carried out using the NetSim simulator and Multi-Objective Evolutionary Algorithms framework to optimize parameters. Experiments were carried out with realistic maps of Open Street Maps and its results were compared with other algorithms such as Seamless and Authorized Multimedia Streaming and P-DSR. The survey suggests that the proposed methodology works well concerning the average lifetime of the inter-classes and the delivery rate of video segments.
C1 [Alaya, Bechir] Qassim Univ, Coll Business & Econ, Dept Management Informat Syst & Prod Management, Buraydah, Saudi Arabia.
   [Alaya, Bechir] Univ Gabes, IResCoMath Lab, Gabes, Tunisia.
   [Alaya, Bechir] Qassim Univ, Coll Business & Econ, Buraydah 52571, Saudi Arabia.
C3 Qassim University; Universite de Gabes; Qassim University
RP Alaya, B (corresponding author), Qassim Univ, Coll Business & Econ, Dept Management Informat Syst & Prod Management, Buraydah, Saudi Arabia.; Alaya, B (corresponding author), Univ Gabes, IResCoMath Lab, Gabes, Tunisia.; Alaya, B (corresponding author), Qassim Univ, Coll Business & Econ, Buraydah 52571, Saudi Arabia.
EM alaya.bechir@gmail.com
RI Alaya, Bechir/HCI-1812-2022
FU Qassim University
FX This work is supported by Qassim University.
CR Al-Madani B, 2014, ADV MULTIMED, V2014, DOI 10.1155/2014/164940
   Alaya Bechir., 2009, P 18 INT C SOFTW ENG, P238
   Alaya Bechir., 2015, INT J COMPUT SCI, P57
   Araki D, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19122688
   Bezerra P, 2019, INT J DISTRIB SENS N, V15, DOI 10.1177/1550147719832839
   Bradai A, 2014, VEH COMMUN, V1, P105, DOI 10.1016/j.vehcom.2014.05.002
   DAN A, 1995, MULTIMEDIA SYST, V3, P93, DOI 10.1007/BF01542861
   De Meo P., 2011, Proceedings of the 2011 11th International Conference on Intelligent Systems Design and Applications (ISDA), P88, DOI 10.1109/ISDA.2011.6121636
   Garg Atul, 2019, INT J ENG ADV TECHNO, V8
   Hadka D, 2017, BEGINNERS GUIDE MOEA
   Han H, 2018, PROCEEDINGS OF THE 28TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'18), P7, DOI 10.1145/3210445.3210452
   Hsieh YL, 2012, COMPUT NETW, V56, P3609, DOI 10.1016/j.comnet.2012.07.011
   Hu M, 2017, IEEE ACCESS, V5, P4140, DOI 10.1109/ACCESS.2017.2683640
   Huang C. F., 2020, ADV TECHNOL INNOV, V5, P56, DOI DOI 10.46604/AITI.2020.4080
   Huang CJ, 2013, APPL SOFT COMPUT, V13, P4508, DOI 10.1016/j.asoc.2013.07.025
   Jan MA, 2019, IEEE INTERNET THINGS, V6, P1576, DOI 10.1109/JIOT.2018.2848284
   Jiang SR, 2016, IEEE T INTELL TRANSP, V17, P2193, DOI 10.1109/TITS.2016.2517603
   Khan H, 2020, IEEE T VEH TECHNOL, V69, P3513, DOI 10.1109/TVT.2020.2975068
   Khan S, 2018, COMPUT ELECTR ENG, V68, P447, DOI 10.1016/j.compeleceng.2018.04.017
   Knorr F, 2012, IEEE T VEH TECHNOL, V61, P3490, DOI 10.1109/TVT.2012.2209690
   Kong PY, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P71
   Lyamin N, 2018, IEEE NETWORK, V32, P15, DOI 10.1109/MNET.2018.1800074
   Mejri MN, 2014, VEH COMMUN, V1, P53, DOI 10.1016/j.vehcom.2014.05.001
   Mohammad SA, 2011, LECT NOTES COMPUT SC, V6596, P95, DOI 10.1007/978-3-642-19786-4_9
   Monteiro R, 2017, WIREL NETW, V23, P2145, DOI 10.1007/s11276-016-1275-2
   Pablo Pedro., 2018, SENSORS BASEL, V18, P9
   Qadri N, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON WIRELESS AND MOBILE COMPUTING, NETWORKING AND COMMUNICATIONS, P429, DOI 10.1109/WiMob.2009.79
   Qadri NN, 2010, IET COMMUN, V4, P1300, DOI 10.1049/iet-com.2009.0458
   Qadri N.N, 2009, P 5 INT ICST MOB MUL
   Rezende C, 2015, COMPUT NETW, V81, P43, DOI 10.1016/j.comnet.2014.12.010
   Senouci O, 2020, INT J COMMUN SYST, V33, DOI 10.1002/dac.4402
   Sheikh MS, 2020, WIREL COMMUN MOB COM, V2020, DOI 10.1155/2020/5129620
   Silvestre G, 2013, INT C PAR DISTRIB SY, P412, DOI 10.1109/ICPADS.2013.64
   Tetcos.com, 2019, NETSIM NETW SIM EM H
   Vemireddy S, 2020, VEH COMMUN, V25, DOI 10.1016/j.vehcom.2020.100251
   Xu CQ, 2015, IEEE T VEH TECHNOL, V64, P1201, DOI 10.1109/TVT.2014.2329696
   Xu CQ, 2013, IEEE T VEH TECHNOL, V62, P2273, DOI 10.1109/TVT.2012.2228682
   Yang X, 2004, PROCEEDINGS OF MOBIQUITOUS 2004, P114
   Yang ZY, 2012, IEEE T WIREL COMMUN, V11, P3006, DOI 10.1109/TWC.2012.052512.120049
   Zhao H, 2017, IEEE T MULTIMEDIA, V19, P149, DOI 10.1109/TMM.2016.2612123
NR 40
TC 2
Z9 2
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2021
VL 17
IS 3
AR 85
DI 10.1145/3440018
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC5SO
UT WOS:000686585400009
DA 2024-07-18
ER

PT J
AU Huang, CY
   Cheng, YC
   Huang, GZ
   Fan, CL
   Hsu, CH
AF Huang, Chun-Ying
   Cheng, Yun-Chen
   Huang, Guan-Zhang
   Fan, Ching-Ling
   Hsu, Cheng-Hsin
TI On the Performance Comparisons of Native and Clientless Real-Time
   Screen-Sharing Technologies
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Live video streaming; real-time encoding; measurements; performance
   evaluations; performance optimization
AB Real-time screen-sharing provides users with ubiquitous access to remote applications, such as computer games, movie players, and desktop applications (apps), anywhere and anytime. In this article, we study the performance of different screen-sharing technologies, which can be classified into native and clientless ones. The native ones dictate that users install special-purpose software, while the clientless ones directly run in web browsers. In particular, we conduct extensive experiments in three steps. First, we identify a suite of the most representative native and clientless screen-sharing technologies. Second, we propose a systematic measurement methodology for comparing screen-sharing technologies under diverse and dynamic network conditions using different performance metrics. Last, we conduct extensive experiments and perform indepth analysis to quantify the performance gap between clientless and native screen-sharing technologies. We found that our WebRTC-based implementation achieves the best overall performance. More precisely, it consumes a maximum of 3 Mbps bandwidth while reaching a high decoding ratio and delivering good video quality. Moreover, it leads to a steadily high decoding ratio and video quality under dynamic network conditions. By presenting the very first rigorous comparisons of the native and clientless screen-sharing technologies, this article will stimulate more exciting studies on the emerging clientless screen-sharing technologies.
C1 [Huang, Chun-Ying; Cheng, Yun-Chen; Huang, Guan-Zhang] Natl Chiao Tung Univ, 1001 Univ Rd, Hsinchu 30010, Taiwan.
   [Huang, Chun-Ying; Cheng, Yun-Chen; Huang, Guan-Zhang] Natl Chiao Tung Univ, Dept Comp Sci, 1001 Univ Rd, Hsinchu 30010, Taiwan.
   [Fan, Ching-Ling; Hsu, Cheng-Hsin] Natl Tsing Hua Univ, Dept Comp Sci, 101 Sec 2 Kuang Fu Rd, Hsinchu 300, Taiwan.
C3 National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung
   University; National Tsing Hua University
RP Huang, CY (corresponding author), Natl Chiao Tung Univ, 1001 Univ Rd, Hsinchu 30010, Taiwan.; Huang, CY (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, 1001 Univ Rd, Hsinchu 30010, Taiwan.
EM chuang@cs.nctu.edu.tw; emmas191@gmail.com; johnhuan00834@gmail.com;
   ch.ling.fan@gmail.com; chsu@cs.nthu.edu.tw
OI Huang, Chun-Ying/0000-0001-5503-9541
FU Ministry of Science and Technology of Taiwan [107-2221-E-009-028-MY3,
   107-2221-E-007-091-MY3]; NOVATEK Fellowship
FX This work was partially supported by the Ministry of Science and
   Technology of Taiwan (#107-2221-E-009-028-MY3 and
   #107-2221-E-007-091-MY3) and by a NOVATEK Fellowship.
CR Abdallah M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3212804
   Ammar D, 2016, 2016 IEEE SIXTH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND ELECTRONICS (ICCE), P429, DOI 10.1109/CCE.2016.7562675
   [Anonymous], 2006, 160222006E ISOIEC
   [Anonymous], ACM T MULTIMEDIA COM, V17
   Beer Daniel, 2019, QR DECODER LIB
   Channappayya SS, 2008, INT CONF ACOUST SPEE, P765, DOI 10.1109/ICASSP.2008.4517722
   Chen H, 2019, IEEE T PARALL DISTR, V30, P2849, DOI 10.1109/TPDS.2019.2922205
   Claypool M., 2014, Network and Systems Support for Games (NetGames), 2014 13th Annual Workshop on, P1, DOI DOI 10.1109/NETGAMES.2014.7008964
   Claypool M., 2015, P 25 ACM WORKSH NETW, P67
   Claypool Mark, 2010, Proceedings of the 1st Annual ACM SIGMM Conference on Multimedia Systems, P215, DOI [10.1145/1730836.1730863, DOI 10.1145/1730836.1730863]
   Denso Wave Incorporated, 2019, DENS WAV
   FFmpeg team, 2019, FFMPEG
   Ganji RR., 2016, ELECT IMAGING, P1
   Garcia B., 2016, 9 EAI INT C MOB MULT, P40
   GlavSoft LLC, 2019, TIGHTVNC VNC COMP FR
   Google, 2019, STAD
   Hsu CF, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2886778
   Hsu CF, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P813, DOI 10.1145/2647868.2654991
   Hsu Chih-Fan, 2015, P ACM SIGMM C MULT S, P177
   Huang CY, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2537855
   Janzen BenjaminF., 2014, CHI 14 EXTENDED ABST, P1477
   Li Yan, 2011, 2011 IEEE 3rd International Conference on Communication Software and Networks (ICCSN 2011), P326, DOI 10.1109/ICCSN.2011.6013725
   Lin YM, 2015, COMPUT COMMUN, V72, P17, DOI 10.1016/j.comcom.2015.05.006
   López L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1187, DOI 10.1145/2964284.2973798
   Martin Joel, 2019, NOVNC
   Microsoft Corp, 2019, US REM ASS LET SOM F
   Microsoft Corp, 2018, REM DESKT PROT
   MinGW.org, 2019, MINGW MIN GNU WIND
   Miniwatts Marketing Group, 2019, WORLD INT US STAT 20
   Mochida Yasuhiro, 2016, P INT C SUPP GROUP W, P437
   Mulfari D, 2014, SYMP NETW CLOUD, P125, DOI 10.1109/NCCA.2014.28
   Nam H, 2016, IEEE INFOCOM SER
   NVIDIA, 2019, GAM AN YOUR MAC WIND
   Richardson T, 1998, IEEE INTERNET COMPUT, V2, P33, DOI 10.1109/4236.656066
   Sharp R, 2012, BELL LABS TECH J, V17, P67, DOI 10.1002/bltj.21545
   Sheldon N., 2003, P 2 WORKSHOP NETWORK, P3, DOI DOI 10.1145/963900.963901
   Shi S, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2719921
   Shi Shu., 2011, Proceedings of the 19th ACM International Conference on Multimedia, P103, DOI [10.1145/2072298.2072313, DOI 10.1145/2072298.2072313]
   Sony Interactive Entertainment, 2019, PS4 REM PLAY WIND PC
   Ueberheide M, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1031, DOI 10.1145/2733373.2806394
   Valve Corporation, 2019, STEAM IN HOM STREAM
   WebRTC, 2019, WEBRTC HOM
   Zhang YH, 2016, IEEE T PARALL DISTR, V27, P1239, DOI 10.1109/TPDS.2015.2433916
NR 43
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 54
DI 10.1145/3437881
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000016
DA 2024-07-18
ER

PT J
AU Lan, XY
   Yang, ZF
   Zhang, W
   Yuen, PC
AF Lan, Xiangyuan
   Yang, Zifei
   Zhang, Wei
   Yuen, Pong C.
TI Spatial-temporal Regularized Multi-modality Correlation Filters for
   Tracking with Re-detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-modality fusion; tracking; behavior understanding
ID MULTIPLE; IMAGES
AB The development of multi-spectrum image sensing technology has brought great interest in exploiting the information of multiple modalities (e.g., RGB and infrared modalities) for solving computer vision problems. In this article, we investigate how to exploit information from RGB and infrared modalities to address two important issues in visual tracking: robustness and object re-detection. Although various algorithms that attempt to exploit multi-modality information in appearance modeling have been developed, they still face challenges that mainly come from the following aspects: (1) the lack of robustness to deal with large appearance changes and dynamic background, (2) failure in re-capturing the object when tracking loss happens, and (3) difficulty in determining the reliability of different modalities. To address these issues and perform effective integration of multiple modalities, we propose a new tracking-by-detection algorithm called Adaptive Spatial-temporal Regulated Multi-Modality Correlation Filter. Particularly, an adaptive spatial-temporal regularization is imposed into the correlation filter framework in which the spatial regularization can help to suppress effect from the cluttered background while the temporal regularization enables the adaptive incorporation of historical appearance cues to deal with appearance changes. In addition, a dynamic modality weight learning algorithm is integrated into the correlation filter training, which ensures that more reliable modalities gain more importance in target tracking. Experimental results demonstrate the effectiveness of the proposed method.
C1 [Lan, Xiangyuan; Yuen, Pong C.] Hong Kong Baptist Univ, Dept Comp Sci, 34 Renfrew Rd, Hong Kong, Peoples R China.
   [Yang, Zifei; Zhang, Wei] Shandong Univ, Sch Control Sci & Engn, 73 Jingshi Rd, Jinan 250061, Peoples R China.
C3 Hong Kong Baptist University; Shandong University
RP Yuen, PC (corresponding author), Hong Kong Baptist Univ, Dept Comp Sci, 34 Renfrew Rd, Hong Kong, Peoples R China.
EM xiangyuanlan@life.hkbu.edu.hk; yang_zifei@foxmail.com;
   davidzhang@sdu.edu.cn; pcyuen@comp.hkbu.edu.hk
RI Lan, Xiangyuan/F-8034-2018
FU National Natural Science Foundation of China [61991411]; Hong Kong
   Research Grant Council GRF project [RGC/HKBU12254316]; Hong Kong Baptist
   University Tier 1 Grant
FX This project is supported by the National Natural Science Foundation of
   China under Grant 61991411, the Hong Kong Research Grant Council GRF
   project RGC/HKBU12254316, and Hong Kong Baptist University Tier 1 Grant.
   Authors' addresses: X. Lan and P. C. Yuen (corresponding author),
   Department of Computer Science, Hong Kong Baptist University, 34 Renfrew
   Road, Kowloon Tong, Hong Kong, China; emails:
   xiangyuanlan@life.hkbu.edu.hk,pcyuen@comp.hkbu.edu.hk;Z.Yang and W.
   Zhang, School of Control Science and Engineering, Shandong University,
   73 Jingshi Road Jinan, China 250061; emails:
   yang_zifei@foxmail.com,davidzhang@sdu.edu.cn.
CR [Anonymous], 2016, P 25 INT JOINT C ART
   [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Bunyak F, 2007, P WACV
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Galoogahi HK, 2013, IEEE I CONF COMP VIS, P3072, DOI 10.1109/ICCV.2013.381
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Han JG, 2012, IEEE T CONSUM ELECTR, V58, P255, DOI 10.1109/TCE.2012.6227420
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Hua Y, 2014, LECT NOTES COMPUT SC, V8694, P172, DOI 10.1007/978-3-319-10599-4_12
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Lan X., 2019, IEEE T IND INFORM, DOI [10.1109/TII.2019.2947293, DOI 10.1109/TII.2019.2947293]
   Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618
   Lan XY, 2020, PATTERN RECOGN LETT, V130, P12, DOI 10.1016/j.patrec.2018.10.002
   Lan XY, 2018, AAAI CONF ARTIF INTE, P7008
   Lan XY, 2018, IEEE T IMAGE PROCESS, V27, P2022, DOI 10.1109/TIP.2017.2777183
   Lan XY, 2017, AAAI CONF ARTIF INTE, P4118
   Lan XY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2481325
   Lan XY, 2014, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2014.156
   Lebeda K, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P153, DOI 10.1109/ICCVW.2013.26
   Leykin A, 2010, MACH VISION APPL, V21, P587, DOI 10.1007/s00138-008-0176-5
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liu HP, 2012, SCI CHINA INFORM SCI, V55, P590, DOI 10.1007/s11432-011-4536-9
   Liu S, 2016, PROC CVPR IEEE, P4312, DOI 10.1109/CVPR.2016.467
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Nie F., 2016, IJCAI, P1881
   Qi YK, 2019, IEEE T PATTERN ANAL, V41, P1116, DOI 10.1109/TPAMI.2018.2828817
   Salti S, 2012, IEEE T IMAGE PROCESS, V21, P4334, DOI 10.1109/TIP.2012.2206035
   Shao R, 2019, IEEE T INF FOREN SEC, V14, P923, DOI 10.1109/TIFS.2018.2868230
   Song YB, 2017, COMPUT VIS IMAGE UND, V162, P135, DOI 10.1016/j.cviu.2017.08.009
   Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308
   Tang M, 2015, IEEE I CONF COMP VIS, P3038, DOI 10.1109/ICCV.2015.348
   Wu YF, 2011, PROCEEDINGS OF THE ASME INTERNATIONAL MANUFACTURING SCIENCE AND ENGINEERING CONFERENCE 2011, VOL 1, P1
   Ye M., 2018, IJCAI, P1092
   Ye M, 2020, IEEE T IMAGE PROCESS, V29, P9387, DOI 10.1109/TIP.2020.2998275
   Ye M, 2020, IEEE T IND INFORM, V16, P615, DOI 10.1109/TII.2019.2946030
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang SP, 2017, IEEE T NEUR NET LEAR, V28, P2357, DOI 10.1109/TNNLS.2016.2586194
   Zhang SP, 2017, IEEE T CIRC SYST VID, V27, P421, DOI 10.1109/TCSVT.2016.2539860
   Zhang SP, 2013, PATTERN RECOGN, V46, P1772, DOI 10.1016/j.patcog.2012.10.006
   Zhou QQ, 2020, IEEE T IMAGE PROCESS, V29, P7578, DOI 10.1109/TIP.2020.3004267
NR 61
TC 7
Z9 7
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 57
DI 10.1145/3430257
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000011
DA 2024-07-18
ER

PT J
AU Sun, L
   Al Osman, H
   Lang, J
AF Sun, Lu
   Al Osman, Hussein
   Lang, Jochen
TI An Augmented Reality Online Assistance Platform for Repair Tasks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Augmented reality; remote assistance; performance evaluation;
   collaborative tools
ID INDUSTRY 4.0; MAINTENANCE
AB Our augmented reality online assistance platform enables an expert to specify 6DoF movements of a component and apply the geometrical and physical constraints in real-time. We track the real components on the expert's side to monitor the operations of an expert. We leverage a remote rendering technique that we proposed previously to relieve the rendering burden of the augmented reality end devices. By conducting a user study, we show that the proposed method outperforms conventional instructional videos and sketches. The answers to the questionnaires show that the proposed method receives higher recommendation than sketching, and, compared to conventional instructional videos, is outstanding in terms of instruction clarity, preference, recommendation, and confidence of task completion. Moreover, as to the overall user experience, the proposed method has an advantage over the video method.
C1 [Sun, Lu; Al Osman, Hussein; Lang, Jochen] Univ Ottawa, Sch Elect Engn & Comp Sci, 800 King Edward Ave, Ottawa, ON K1N 6N5, Canada.
C3 University of Ottawa
RP Sun, L (corresponding author), Univ Ottawa, Sch Elect Engn & Comp Sci, 800 King Edward Ave, Ottawa, ON K1N 6N5, Canada.
EM lsun100@uottawa.ca; halosman@uottawa.ca; jlang@uottawa.ca
OI Sun, Lu/0000-0002-0110-4165
CR Adcock M, 2014, SUI 2014 P 2 ACM S S, P113, DOI [10.1145/2659766.2659768, DOI 10.1145/2659766.2659768]
   [Anonymous], ACM T MULTIMEDIA COM, V17
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bhattacharya B, 2015, PROC SPIE, V9392, DOI 10.1117/12.2081214
   Blanco-Novoa O, 2018, IEEE ACCESS, V6, P8201, DOI 10.1109/ACCESS.2018.2802699
   Blanco-Pons S, 2019, MULTIMED TOOLS APPL, V78, P10265, DOI 10.1007/s11042-018-6609-x
   Boulanger P, 2004, 1ST CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P320, DOI 10.1109/CCCRV.2004.1301462
   De Crescenzio F, 2011, IEEE COMPUT GRAPH, V31, P96, DOI 10.1109/MCG.2011.4
   Gattullo M, 2019, ROBOT CIM-INT MANUF, V56, P276, DOI 10.1016/j.rcim.2018.10.001
   Gavish N, 2015, INTERACT LEARN ENVIR, V23, P778, DOI 10.1080/10494820.2013.815221
   Google LLC, 2020, DOWNL ANDR STUD SDK
   Google LLC, 2020, BUILD NEW AUGM REAL
   Gurevich P., 2012, P SIGCHI C HUM FACT, P619
   Haringer M, 2002, INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P237, DOI 10.1109/ISMAR.2002.1115093
   Harrabin R., 2019, BBC News
   Henderson S. J., 2011, 2011 IEEE International Symposium on Mixed and Augmented Reality, P191, DOI 10.1109/ISMAR.2011.6092386
   Henderson SJ, 2009, INT SYM MIX AUGMENT, P135, DOI 10.1109/ISMAR.2009.5336486
   Hou X., 2017, Computer Communication and Networks (ICCCN), 2017 26th International Conference on, P1
   jMonkeyEngine, 2020, JMONKEYENGINE
   Lamb Philip., 2020, ARTOOLKIT HOME PAGE
   Lee J, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10041547
   MAXST Co. Ltd, 2019, MAXST TECHN CO SPEC
   Mitrovic A., 2009, International Journal of Artificial Intelligence in Education, V19, P155
   Mizell D.W., 1992, P 25 HAW INT C SYST, VVolume 2, P659, DOI [DOI 10.1109/HICSS.1992.183317, 10.1109/HICSS.1992.183317]
   Oda O, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P405, DOI 10.1145/2807442.2807497
   Palmarini R, 2018, ROBOT CIM-INT MANUF, V49, P215, DOI 10.1016/j.rcim.2017.06.002
   PTC Inc., 2020, VUF ENG
   Ranatunga D, 2013, INT SYM MIX AUGMENT
   RE'FLEKT GmbH, 2019, AUGM REAL PLATF MAIN
   Speicher M, 2015, PROC INFORMATIK 2015, V246, P1561
   Sun HL, 2018, PROCEEDINGS OF CHINESE CHI 2018: SIXTH INTERNATIONAL SYMPOSIUM OF CHINESE CHI (CHINESE CHI 2018), P64, DOI 10.1145/3202667.3202676
   Sun Lu., 2019, MULTIMED TOOLS APPL, V79, P1
   Sutherland IE., 1968, Assoc. Comput. Machinery, V68, P757, DOI [DOI 10.1145/1476589.1476686, 10.1145/1476589.1476686, 10.1145/1476589.1476686.2.2.1]
   Syberfeldt A, 2017, IEEE ACCESS, V5, P9118, DOI 10.1109/ACCESS.2017.2703952
   TUKEY JW, 1949, BIOMETRICS, V5, P99, DOI 10.2307/3001913
   Wang MJ, 2010, IFIP ADV INF COMM TE, V332, P285
   Wang X, 2016, ADV MANUF, V4, P1, DOI 10.1007/s40436-015-0131-4
   Webel S, 2013, ROBOT AUTON SYST, V61, P398, DOI 10.1016/j.robot.2012.09.013
NR 38
TC 6
Z9 6
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 50
DI 10.1145/3429285
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000007
DA 2024-07-18
ER

PT J
AU Tanveer, M
   Gupta, T
   Shah, M
AF Tanveer, M.
   Gupta, Tarun
   Shah, Miten
CA Alzhelmers Dis Neuroimaging Initia
TI Pinball Loss Twin Support Vector Clustering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Support vector machine; twin support vector machine; clustering; twin
   support vector clustering; pinball loss; quantile distance; noise
   insensitivity; optimization; convex programming
ID CLASSIFICATION; MACHINE; ROBUST; MRI
AB Twin Support Vector Clustering (TWSVC) is a clustering algorithm inspired by the principles of Twin Support Vector Machine (TWSVM). TWSVC has already outperformed other traditional plane based clustering algorithms. However, TWSVC uses hinge loss, which maximizes shortest distance between clusters and hence suffers from noise-sensitivity and low re-sampling stability. In this article, we propose Pinball loss Twin Support Vector Clustering (pinTSVC) as a clustering algorithm. The proposed pinTSVC model incorporates the pinball loss function in the plane clustering formulation. Pinball loss function introduces favorable properties such as noise-insensitivity and re-sampling stability. The time complexity of the proposed pinTSVC remains equivalent to that of TWSVC. Extensive numerical experiments on noise-corrupted benchmark UCI and artificial datasets have been provided. Results of the proposed pinTSVC model are compared with TWSVC, Twin Bounded Support Vector Clustering (TBSVC) and Fuzzy c-means clustering (FCM). Detailed and exhaustive comparisons demonstrate the better performance and generalization of the proposed pinTSVC for noise-corrupted datasets. Further experiments and analysis on the performance of the above-mentioned clustering algorithms on structural MRI (sMRI) images taken from the ADNI database, face clustering, and facial expression clustering have been done to demonstrate the effectiveness and feasibility of the proposed pinTSVC model.
C1 [Tanveer, M.] Indian Inst Technol Indore, Dept Math, Indore 453552, Madhya Pradesh, India.
   [Gupta, Tarun; Shah, Miten] Indian Inst Technol Indore, Dept Comp Sci & Gineering, Indore 953552, Madhya Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Indore; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Indore
RP Tanveer, M (corresponding author), Indian Inst Technol Indore, Dept Math, Indore 453552, Madhya Pradesh, India.
EM mtanveer@iiti.ac.in; tarungupta360@gmail.com; mitenshah16@gmail.com
RI Tanveer, Mohammad/I-4585-2013
OI Tanveer, Mohammad/0000-0002-5727-3697
FU Science & Engineering Research Board (SERB) Government of INDIA
   [SB/S2/RJN-001/2016, ECR/2017/000053]; Council of Scientific &
   Industrial Research (CSIR), New Delhi, INDIA under Extra Mural Research
   (EMR) Scheme [22(0751)/17/EMR-II]; Alzheimer's Disease Neuroimaging
   Initiative (ADNI) (National Institutes of Health) [U01 AG024904]; DOD
   ADNI (Department of Defense) [W81XWH-12-2-0012]; National Institute on
   Aging; National Institute of Biomedical Imaging and Bioengineering;
   AbbVie; Alzheimer's Association; Alzheimer's Drug Discovery Foundation;
   Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company;
   CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli
   Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd; Genentech, Inc.;
   Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy
   Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research
   & Development LLC.; Lumosity; Lundbeck; Merck Co., Inc.; Meso Scale
   Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis
   Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier;
   Takeda Pharmaceutical Company; Transition Therapeutics; Canadian
   Institutes of Health Research
FX This work was supported by Science & Engineering Research Board (SERB)
   Government of INDIA under Ramanujan fellowship grant no.
   SB/S2/RJN-001/2016 and Early Career Research Award (ECRA) grant no.
   ECR/2017/000053. It is also supported by Council of Scientific &
   Industrial Research (CSIR), New Delhi, INDIA under Extra Mural Research
   (EMR) Scheme grant no. 22(0751)/17/EMR-II. We gratefully acknowledge the
   Indian Institute of Technology Indore for providing facilities and
   support. The process of collection of Data and sharing for this project
   has been funded by the Alzheimer's Disease Neuroimaging Initiative
   (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI
   (Department of Defense award number W81XWH-12-2-0012). The funding is
   provided by the following: the National Institute on Aging, the National
   Institute of Biomedical Imaging and Bioengineering, and through generous
   and ample contributions from: AbbVie, Alzheimer's Association;
   Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica,
   Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate;
   Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company;
   EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company
   Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer
   Immunotherapy Research & Development, LLC.; Johnson & Johnson
   Pharmaceutical Research & Development LLC.; Lumosity; Lundbeck; Merck &
   Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack
   Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal
   Imaging; Servier; Takeda Pharmaceutical Company; and Transition
   Therapeutics. The funding to support ADNI clinical sites in Canada is
   provided by the Canadian Institutes of Health Research. Contributions
   from private sector are enabled by the Foundation for the National
   Institutes of Health (www.fnih.org).The grantee organization is the
   Northern California Institute for Research and Education, and the study
   is coordinated by the Alzheimer's Therapeutic Research Institute at the
   University of Southern California. ADNI data are disseminated by the
   Laboratory for Neuro Imaging at the University of Southern California.
CR [Anonymous], 2000, PRACTICAL METHODS OP, DOI DOI 10.1002/9781118723203
   [Anonymous], 2002, PRESS SERIES
   [Anonymous], 2018, ALZHEIMERS DEMENT, DOI DOI 10.1016/J.JALZ.2018.02.001
   Bai L., 2020, ARXIV PREPRINT ARXIV
   Bai L, 2019, KNOWL-BASED SYST, V163, P227, DOI 10.1016/j.knosys.2018.08.034
   BEZDEK JC, 1984, COMPUT GEOSCI, V10, P191, DOI 10.1016/0098-3004(84)90020-7
   Bradley PS, 2000, J GLOBAL OPTIM, V16, P23, DOI 10.1023/A:1008324625522
   Burns A, 2009, BMJ-BRIT MED J, V338, DOI 10.1136/bmj.b158
   Cherkassky V, 2004, NEURAL NETWORKS, V17, P113, DOI 10.1016/S0893-6080(03)00169-2
   Christmann A., 2008, ADV NEURAL INF PROCE, P305
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Davatzikos C, 2008, NEUROIMAGE, V41, P1220, DOI 10.1016/j.neuroimage.2008.03.050
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Frisoni GB, 2010, NAT REV NEUROL, V6, P67, DOI 10.1038/nrneurol.2009.215
   Gao XB, 2000, 2000 5TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS I-III, P1457, DOI 10.1109/ICOSP.2000.893376
   Goel N, 2005, PROC SPIE, V5779, P426, DOI 10.1117/12.605553
   Guo Guodong, 1970, P 4 IEEE INT C AUT F
   Har-Peled S., 2004, P 36 ANN ACM S THEOR, P291
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Hassoun MH., 1995, FUNDAMENTALS ARTIFIC
   Huang XL, 2014, IEEE T PATTERN ANAL, V36, P984, DOI 10.1109/TPAMI.2013.178
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Jayadeva, 2007, IEEE T PATTERN ANAL, V29, P905, DOI 10.1109/TPAMI.2007.1068
   Kamachi M., 1997, The japanese female facial expression (jaffe) database
   Khan RU, 2021, EXPERT SYST, V38, DOI 10.1111/exsy.12566
   Kim SK, 2010, PATTERN RECOGN, V43, P2871, DOI 10.1016/j.patcog.2010.03.008
   Kumar MA, 2009, EXPERT SYST APPL, V36, P7535, DOI 10.1016/j.eswa.2008.09.066
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Mitra V, 2007, APPL SOFT COMPUT, V7, P908, DOI 10.1016/j.asoc.2006.04.002
   Newman D., 1998, UCI REPOSITORY MACHI
   Rahman MM, 2011, IEEE T INF TECHNOL B, V15, P640, DOI 10.1109/TITB.2011.2151258
   Richhariya B, 2020, INFORM SCIENCES, V533, P1, DOI 10.1016/j.ins.2020.05.001
   Richhariya B, 2020, BIOMED SIGNAL PROCES, V59, DOI 10.1016/j.bspc.2020.101903
   Richhariya B, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107150
   Richhariya B, 2018, EXPERT SYST APPL, V106, P169, DOI 10.1016/j.eswa.2018.03.053
   Saunders C., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P515
   Shao YH, 2013, PROCEDIA COMPUT SCI, V17, P41, DOI 10.1016/j.procs.2013.05.007
   Shao YH, 2011, IEEE T NEURAL NETWOR, V22, P962, DOI 10.1109/TNN.2011.2130540
   Sharma S, 2021, IEEE T SYST MAN CY-S, V51, P987, DOI 10.1109/TSMC.2019.2896642
   Sun A., 2002, P 4 INT WORKSHOP WEB, P96, DOI 10.1109/SSD.2013.6564011
   Tanveer M, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3344998
   Tanveer M, 2019, IEEE SYS MAN CYBERN, P3287, DOI 10.1109/SMC.2019.8914642
   Tanveer M, 2019, APPL SOFT COMPUT, V83, DOI 10.1016/j.asoc.2019.105617
   Tanveer M, 2019, INFORM SCIENCES, V494, P311, DOI 10.1016/j.ins.2019.04.032
   Tanveer M, 2019, APPL SOFT COMPUT, V78, P164, DOI 10.1016/j.asoc.2019.02.022
   Tanveer M, 2015, KNOWL INF SYST, V45, P191, DOI 10.1007/s10115-014-0786-3
   Tanveer M, 2015, COGN COMPUT, V7, P137, DOI 10.1007/s12559-014-9278-8
   Tanveer M, 2016, APPL INTELL, V45, P174, DOI 10.1007/s10489-015-0751-1
   Wang Z, 2020, NEURAL COMPUT APPL, V32, P9885, DOI 10.1007/s00521-019-04511-3
   Wang Z, 2015, IEEE T NEUR NET LEAR, V26, P2583, DOI 10.1109/TNNLS.2014.2379930
   Westman E, 2012, NEUROIMAGE, V62, P229, DOI 10.1016/j.neuroimage.2012.04.056
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Wu YC, 2007, J AM STAT ASSOC, V102, P974, DOI 10.1198/016214507000000617
   Yu H., 2012, Handbook of Natural computing, V1, P479, DOI [DOI 10.1007/978-3-540-92910-915, DOI 10.1007/978-3-540-92910-9_15]
   Yuille AL, 2002, ADV NEUR IN, V14, P1033
   ZhenWang Yuan-Hai Shao, 2019, ARXIV PREPRINT ARXIV
NR 56
TC 16
Z9 17
U1 3
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 63
DI 10.1145/3409264
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100006
DA 2024-07-18
ER

PT J
AU Wei, HY
   Li, ZX
   Huang, FC
   Zhang, CL
   Ma, HF
   Shi, ZZ
AF Wei, Haiyang
   Li, Zhixin
   Huang, Feicheng
   Zhang, Canlong
   Ma, Huifang
   Shi, Zhongzhi
TI Integrating Scene Semantic Knowledge into Image Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image captioning; attention mechanism; scene semantics; encoder-decoder
   framework
AB Most existing image captioning methods use only the visual information of the image to guide the generation of captions, lack the guidance of effective scene semantic information, and the current visual attention mechanism cannot adjust the focus intensity on the image. In this article, we first propose an improved visual attention model. At each timestep, we calculated the focus intensity coefficient of the attention mechanism through the context information of themodel, then automatically adjusted the focus intensity of the attention mechanism through the coefficient to extract more accurate visual information. In addition, we represented the scene semantic knowledge of the image through topic words related to the image scene, then added them to the language model. We used the attention mechanism to determine the visual information and scene semantic information that the model pays attention to at each timestep and combined them to enable the model to generate more accurate and scene-specific captions. Finally, we evaluated our model on Microsoft COCO (MSCOCO) and Flickr30k standard datasets. The experimental results show that our approach generates more accurate captions and outperforms many recent advanced models in various evaluation metrics.
C1 [Wei, Haiyang; Li, Zhixin; Huang, Feicheng; Zhang, Canlong] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, 15 Yucai Rd, Guilin 541004, Guangxi, Peoples R China.
   [Ma, Huifang] Northwest Normal Univ, Coll Comp Sci & Engn, 967 Anning East Rd, Lanzhou 730070, Gansu, Peoples R China.
   [Shi, Zhongzhi] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.
C3 Guangxi Normal University; Northwest Normal University - China; Chinese
   Academy of Sciences; Institute of Computing Technology, CAS
RP Li, ZX (corresponding author), Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, 15 Yucai Rd, Guilin 541004, Guangxi, Peoples R China.
EM whyang1102@163.com; lizx@gxnu.edu.cn; hfcheng01@163.com;
   clzhang@gxnu.edu.cn; mahuifang@yeah.net; shizz@ict.ac.cn
RI yang, qing/JBR-8440-2023; Li, Zhixin/ABI-9264-2022; zhang,
   lin/IZQ-4870-2023; L, J/JEF-9564-2023; zhang, cl/JDW-6549-2023; Ma,
   Huifang/JTV-4982-2023; LU, LU/JEZ-4760-2023
OI Li, Zhixin/0000-0002-5313-6134; Ma, Huifang/0000-0002-5104-8982; 
FU National Natural Science Foundation of China [61966004, 61663004,
   61866004, 61762078]; Guangxi Natural Science Foundation
   [2019GXNSFDA245018, 2018GXNSFDA281009]; Guangxi "Bagui Scholar" Teams
   for Innovation and Research Project; Guangxi Talent Highland Project of
   Big Data Intelligence and Application; Guangxi Collaborative Innovation
   Center of Multi-Source Information Integration and Intelligent
   Processing
FX This work is supported by the National Natural Science Foundation of
   China (Nos. 61966004, 61663004, 61866004, 61762078), the Guangxi Natural
   Science Foundation (Nos. 2019GXNSFDA245018, 2018GXNSFDA281009), the
   Guangxi "Bagui Scholar" Teams for Innovation and Research Project, the
   Guangxi Talent Highland Project of Big Data Intelligence and
   Application, and Guangxi Collaborative Innovation Center of Multi-Source
   Information Integration and Intelligent Processing.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cho K. etal, 2014, arXiv, DOI DOI 10.3115/V1/D14-1179
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Dai JF, 2016, ADV NEUR IN, V29
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, AAAI CONF ARTIF INTE, P6837
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin Junyang, 2018, ARXIV PREPRINT ARXIV
   Liu Daqing, 2018, ARXIV PREPRINT ARXIV
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Liu XH, 2018, LECT NOTES COMPUT SC, V11219, P353, DOI 10.1007/978-3-030-01267-0_21
   Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Ranzato MarcAurelio, 2015, CoRR
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Sun DJ, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATIONS AND COMPUTING (ICSPCC)
   Sutskever I, 2014, ADV NEUR IN, V27
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang X, 2019, IEEE I CONF COMP VIS, P4249, DOI 10.1109/ICCV.2019.00435
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yuille A., 2014, ARXIV PREPRINT ARXIV
NR 40
TC 27
Z9 27
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
AR 52
DI 10.1145/3439734
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR4UU
UT WOS:000661037000017
DA 2024-07-18
ER

PT J
AU Feng, SM
   Hu, HF
AF Feng, Shenming
   Hu, Haifeng
TI Learning Joint Structure for Human Pose Estimation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Learning joint structure; structural consistency; heatmap and offset
   estimation; single person pose estimation
AB Recently, tremendous progress has been achieved on human pose estimation with the development of convolutional neural networks (CNNs). However, current methods still suffer from severe occlusion, back view, and large pose variation due to the lack of consideration of the spatial relationship between different joints, which can provide strong cues for localizing the hidden keypoints. In this work, we design a Structural Pose Network (SPN) to take full advantage of joint structure for human pose estimation under unconstrained environment. Specifically, the proposed model is composed of two subnets: Structure Residual Network (SRN) and Structure Improving Network (SIN). Given an input image, SRN first captures rich joint structure as priors through a multi-branch feature extraction module, following a hourglass network with pyramid residual units to enlarge the receptive field and further obtain structural feature representations. SIN, based on coordinate regression, can optimize the spatial relationship of different joints via the attention mechanism, thus refining the initial prediction from SRN. In addition, we propose a novel structure-consistency constraint, which can maintain the structural consistency between the joints and body parts via estimating whether the joints are located in their corresponding parts. At the same time, an online hard regions mining (OHRM) strategy is introduced to drive the network to pay corresponding attention to different body parts. The experimental results on three challenging datasets show that our method outperforms other state-of-the-art algorithms.
C1 [Feng, Shenming; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM fengshm3@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn
OI Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; National Key R&D Program of China [2018YFB1601101]; Natural
   Science Foundation of Guangdong Province [2017A030311029]; Science and
   Technology Program of Guangzhou of China [201704020180]; Fundamental
   Research Funds for the Central Universities of China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61673402, Grant 61273270, and Grant
   60802069, in part by the National Key R&D Program of China under Grant
   2018YFB1601101, in part by the Natural Science Foundation of Guangdong
   Province under Grant 2017A030311029, in part by the Science and
   Technology Program of Guangzhou of China under Grant 201704020180, and
   in part by the Fundamental Research Funds for the Central Universities
   of China.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   [Anonymous], CoRR abs/1511.07122
   Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400
   Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen X., 2014, Advances in neural information processing systems, P1736, DOI DOI 10.1109/CVPR.2018.00742
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chou CJ, 2018, ASIAPAC SIGN INFO PR, P17, DOI 10.23919/APSIPA.2018.8659538
   Chu X., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1831, DOI DOI 10.1109/CVPR.2017.601
   Chu X, 2016, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR.2016.510
   Everingham M., 2010, BMVC, V2, P5
   Fan HQ, 2016, IMAGE VISION COMPUT, V47, P27, DOI 10.1016/j.imavis.2015.11.004
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Huang Z, 2015, ARXIV PREPRINT ARXIV
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Jain A, 2015, LECT NOTES COMPUT SC, V9004, P302, DOI 10.1007/978-3-319-16808-1_21
   Ke LP, 2018, LECT NOTES COMPUT SC, V11206, P731, DOI 10.1007/978-3-030-01216-8_44
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P494, DOI 10.1109/TPAMI.2019.2894422
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie BX, 2017, IEEE I CONF COMP VIS, P3467, DOI 10.1109/ICCV.2017.373
   Nie XC, 2018, PROC CVPR IEEE, P2100, DOI 10.1109/CVPR.2018.00224
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471
   Sun K, 2017, IEEE I CONF COMP VIS, P5600, DOI 10.1109/ICCV.2017.597
   Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Tompson Jonathan, 2014, ARXIV14062984, DOI DOI 10.5555/2968826.2969027
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Xiao SL, 2017, TEXT BIOENG INFORM S, P1212
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xu R, 2012, LECT NOTES COMPUT SC, V7378, P114, DOI 10.1007/978-3-642-31567-1_11
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   Zhang Hong, 2019, ARXIV190101760
NR 41
TC 0
Z9 1
U1 2
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2020
VL 16
IS 3
AR 85
DI 10.1145/3392302
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NO3HD
UT WOS:000569375200011
DA 2024-07-18
ER

PT J
AU Wang, SH
   Zhang, YD
AF Wang, Shui-Hua
   Zhang, Yu-Dong
TI DenseNet-201-Based Deep Neural Network with Composite Learning Factor
   and Precomputation for Multiple Sclerosis Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multiple sclerosis; DenseNet; deep neural network; transfer learning;
   deep learning; precomputation; composite learning factor; simple
   learning factor
ID IDENTIFICATION; SEGMENTATION; ENHANCEMENT
AB (Aim) Multiple sclerosis is a neurological condition that may cause neurologic disability. Convolutional neural network can achieve good results, but tuning hyperparameters of CNN needs expert knowledge and are difficult and time-consuming. To identify multiple sclerosis more accurately, this article proposed a new transfer-learning-based approach. (Method) DenseNet-121, DenseNet-169, and DenseNet-201 neural networks were compared. In addition, we proposed the use of a composite learning factor (CLF) that assigns different learning factor to three types of layers: early frozen layers, middle layers, and late replaced layers. [low to allocate layers into those three layers remains a problem. Hence, four transfer learning settings (viz., Settings A, B, C, and D) were tested and compared. A precomputation method was utilized to reduce the storage burden and accelerate the program. (Results) We observed that DenseNet-201-D (the layers from CP to T3 are frozen, the layers of D4 are updated with learning factor of 1, and the final new layers of FCL are randomly initialized with learning factor of 10) can achieve the best performance. The sensitivity, specificity, and accuracy of DenseNet-201-D was 98.27 +/- 0.58, 98.35 +/- 0.69, and 98.31 +/- 0.53, respectively. (Conclusion) Our method gives better performances than state-of-the-art approaches. Furthermore, this composite learning rate gives superior results to traditional simple learning factor (SLF) strategy.
C1 [Wang, Shui-Hua] Loughborough Univ, Sch Architecture Bldg & Civil Engn, Loughborough LE11 3TU, Leics, England.
   [Zhang, Yu-Dong] Univ Leicester, Dept Informat, Leicester LE1 7RH, Leics, England.
   [Wang, Shui-Hua; Zhang, Yu-Dong] Henan Polytech Univ, Sch Comp Sci & Technol, Jiaozuo 454000, Henan, Peoples R China.
C3 Loughborough University; University of Leicester; Henan Polytechnic
   University
RP Zhang, YD (corresponding author), Univ Leicester, Dept Informat, Leicester LE1 7RH, Leics, England.
EM shuihuawang@ieee.org; yudongzhang@ieee.org
RI Wang, Shuihua/G-7326-2016; Zhang, Yudong/I-7633-2013
OI Zhang, Yudong/0000-0002-4870-1493
FU Henan Key Research and Development Project [182102310629]; Natural
   Science Foundation of China [61602250, 11502090, U1711263, U1811264];
   National Key Research and Development Plan [2017YFB1103202]; Guangxi Key
   Laboratory of Trusted Software [kx201901]; Natural Science Foundation of
   Zhejiang Province [Y18F010018]; Fundamental Research Funds for the
   Central Universities [CDLS-2020-03]; Key Laboratory of Child Development
   and Learning Science (Southeast University), Ministry of Education
FX Thanks for the financial support by Henan Key Research and Development
   Project (182102310629), Natural Science Foundation of China (61602250,
   11502090, U1711263, U1811264), National Key Research and Development
   Plan (2017YFB1103202), Guangxi Key Laboratory of Trusted Software
   (kx201901), Natural Science Foundation of Zhejiang Province
   (Y18F010018), Fundamental Research Funds for the Central Universities
   (CDLS-2020-03), and Key Laboratory of Child Development and Learning
   Science (Southeast University), Ministry of Education.
CR [Anonymous], 2017, AQUADEMIA WATER ENV
   Benito-León J, 2019, INFECTION, V47, P135, DOI 10.1007/s15010-018-1196-3
   Browne P, 2014, NEUROLOGY, V83, P1022, DOI 10.1212/WNL.0000000000000768
   Cheng H., 2018, ENTROPY, V20, P4
   Didonna A, 2016, CURR MED CHEM, V23, P1442, DOI 10.2174/0929867323666160406121218
   Fujino S, 2019, NEUROCOMPUTING, V338, P393, DOI 10.1016/j.neucom.2018.05.124
   Gromisch E. S., 2019, INT J MS CARE
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jiang Y., 2015, STUDIES EARLY CHILDH, V2015, P45
   Jiang Y, 2019, LINGUIST VANGUARD, V5, DOI 10.1515/lingvan-2019-0009
   Jiang YZ, 2015, IEEE T CYBERNETICS, V45, P688, DOI 10.1109/TCYB.2014.2334595
   Jin XB, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8030379
   Jin Y, 2018, J MED IMAG HEALTH IN, V8, P609, DOI 10.1166/jmihi.2018.2321
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Matlab, 2019, TRAIN DEEP LEARN NET
   Murray V, 2010, IEEE T IMAGE PROCESS, V19, P1138, DOI 10.1109/TIP.2010.2040446
   Porten L., 2017, KAI TIAKI NURS N, V23, P16
   Qian PJ, 2018, IEEE ACCESS, V6, P28594, DOI 10.1109/ACCESS.2018.2825352
   Qian PJ, 2018, INFORM SCIENCES, V422, P51, DOI 10.1016/j.ins.2017.08.093
   Qian PJ, 2017, KNOWL-BASED SYST, V130, P33, DOI 10.1016/j.knosys.2017.05.018
   Qian PJ, 2017, IEEE T NEUR NET LEAR, V28, P1123, DOI 10.1109/TNNLS.2015.2511179
   Qian PJ, 2016, IEEE T CYBERNETICS, V46, P181, DOI 10.1109/TCYB.2015.2399351
   Qian PJ, 2016, PATTERN RECOGN, V50, P155, DOI 10.1016/j.patcog.2015.08.009
   Sellami A, 2019, EXPERT SYST APPL, V122, P75, DOI 10.1016/j.eswa.2018.12.037
   Simonyan K, 2015, IEEE INT C ICLR
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   University of Cyprus, 2018, MRI LES SEGM MULT SC
   Veluchamy M, 2019, OPTIK, V183, P329, DOI 10.1016/j.ijleo.2019.02.054
   Vural G, 2019, NEUROL SCI, V40, P385, DOI 10.1007/s10072-018-3660-3
   Wang F. B., 2016, P 3 INT C IND ENG AP
   Wang FB, 2018, MICROW OPT TECHN LET, V60, P854, DOI 10.1002/mop.31062
   Wang FB, 2018, INT J ADV MANUF TECH, V94, P2605, DOI 10.1007/s00170-017-1007-5
   Weiguo Zhu, 2012, 2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet), P2858, DOI 10.1109/CECNet.2012.6201802
   Xia K. J., 2019, J MED SYST, V43, P12
   Xia K. J., 2019, J MED SYST, V43, P8
   Xia K.-J., 2018, CLUSTER COMPUTING
   Xia KJ, 2017, J MED IMAG HEALTH IN, V7, P1171, DOI 10.1166/jmihi.2017.2250
   Xue QK, 2016, PROC IEEE MICR ELECT, P1, DOI 10.1109/MEMSYS.2016.7421541
   Yahia S, 2018, MULTIMED TOOLS APPL, V77, P30769, DOI 10.1007/s11042-018-6160-9
   Zhang YD, 2018, J COMPUT SCI-NETH, V28, P1, DOI 10.1016/j.jocs.2018.07.003
   Zhang YD, 2018, J COMPUT SCI-NETH, V27, P57, DOI 10.1016/j.jocs.2018.05.005
   Zhou QH, 2018, INT CONF DIGIT SIG, DOI 10.1109/INTMAG.2018.8508587
   Zhu WG, 2013, APPL MECH MATER, V397-400, P775, DOI 10.4028/www.scientific.net/AMM.397-400.775
   Zhu WG, 2012, ADV MATER RES-SWITZ, V345, P66, DOI 10.4028/www.scientific.net/AMR.345.66
NR 44
TC 125
Z9 127
U1 6
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 60
DI 10.1145/3341095
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FH
UT WOS:000583712600003
DA 2024-07-18
ER

PT J
AU Zhang, ZX
   Guo, CY
   Meng, FZ
   Xu, TZ
   Huang, JK
AF Zhang, Zhaoxin
   Guo, Changyong
   Meng, Fanzhi
   Xu, Taizhong
   Huang, Junkai
TI CovLets: A Second-Order Descriptor for Modeling Multiple Features
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Action recognition; covariance descriptor; Riemannian manifold;
   reproducing kernel Hilbert space
ID ACTION RECOGNITION
AB State-of-the-art techniques for image and video classification take a bottom-up approach where local features are aggregated into a global final representation. Existing frameworks (i.e., bag of words or Fisher vectors) are specifically designed to aggregate vector-valued features such as SIFT descriptors. In this article, we propose a technique to aggregate local descriptors in the form of covariance descriptors (CovDs) into a rich descriptor, which in essence benefit from the second-order statistics along the coding pipeline. The difficulty in aggregating CovDs arises from the fact that CovDs lie on the Riemannian manifold of symmetric positive definite (SPD) matrices. Therefore, the aggregating scheme must take advantage of metrics and the geometry of the SPD manifolds. In our proposal, we make use of the Stein divergence and Nystrom method to embed the SPD manifold into a Hilbert space. We compare our proposal, dubbed CovLets, against state-of-the-art methods on several image and video classification problems including facial expression recognition and action recognition.
C1 [Zhang, Zhaoxin; Guo, Changyong; Huang, Junkai] Harbin Inst Technol, 2 Wenhuaxi Rd, Weihai 264209, Peoples R China.
   [Meng, Fanzhi] China Acad Engn Phys, Inst Comp Applicat, 2 Wenhuaxi Rd, Mianyang 621900, Sichuan, Peoples R China.
   [Xu, Taizhong] Natl Comp Network Emergency Response Tech Team, Coordinat Ctr, Beijing, Peoples R China.
C3 Harbin Institute of Technology; Chinese Academy of Engineering Physics
RP Xu, TZ (corresponding author), Natl Comp Network Emergency Response Tech Team, Coordinat Ctr, Beijing, Peoples R China.
EM heart@hit.edu.cn; hit_gcy@163.com; mengfz@caep.cn; xtz@cert.org.cn;
   heyuangunia@gmail.com
RI wang, yan/GSE-6489-2022; wang, yatong/KDN-3824-2024; Meng,
   Fanzhi/HHZ-1686-2022
CR [Anonymous], 2015, NEURAL NETWORKS LEAR
   [Anonymous], 2004, Int. J. Comput. Vis., DOI [DOI 10.1023/B:VISI.0000029664.99615.94, 10.1023/B:VISI.0000029664.99615.94]
   [Anonymous], 2000, INT C MACH LEARN
   Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965
   Cherian A, 2014, LECT NOTES COMPUT SC, V8691, P299, DOI 10.1007/978-3-319-10578-9_20
   Danelljan Martin, 2018, ARXIV181107628
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fuller S, 2008, J VISION, V8, DOI 10.1167/8.1.16
   Griffin G., 2007, CALTECH 256 OBJECT C
   Guo K, 2013, IEEE T IMAGE PROCESS, V22, P2479, DOI 10.1109/TIP.2013.2252622
   He ZY, 2017, IEEE T CYBERNETICS, V47, P354, DOI 10.1109/TCYB.2016.2514714
   He ZY, 2016, IEEE T IMAGE PROCESS, V25, P3698, DOI 10.1109/TIP.2016.2570553
   Hu WM, 2012, IEEE T PATTERN ANAL, V34, P2420, DOI 10.1109/TPAMI.2012.42
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Jayasumana S, 2013, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2013.17
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jiang F, 2015, J MACH LEARN RES, V16, P227
   Jiang ZH, 2019, IEEE T IMAGE PROCESS, V28, P1133, DOI 10.1109/TIP.2018.2875335
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Lan XY, 2014, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2014.156
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Le Quoc V, 2011, Advances in neural information processing systems, P1017
   Li F.-F., 2004, P COMP VIS PATT REC
   Li Peihua, 2013, P 2013 IEEE INT C CO
   Jingen Liu, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1996, DOI [10.1109/ICINIS.2009.13, 10.1109/CVPRW.2009.5206744]
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Marzalek M., 2009, P INT C PATT REC, P2929
   Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Qi YW, 2020, IEEE T SYST MAN CY-S, V50, P1442, DOI 10.1109/TSMC.2018.2801284
   Qi YK, 2018, IEEE T IMAGE PROCESS, V27, P3857, DOI 10.1109/TIP.2018.2797482
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sra S., 2012, ADV NEURAL INFORM PR, V1, P144
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Valstar M.F., 2010, P INT C LANG RES EV
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang LM, 2013, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2013.345
   Wilf P, 2016, P NATL ACAD SCI USA, V113, P3305, DOI 10.1073/pnas.1524473113
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Yao YJ, 2018, LECT NOTES COMPUT SC, V11213, P560, DOI 10.1007/978-3-030-01240-3_34
   Yi SY, 2019, IEEE T MULTIMEDIA, V21, P1399, DOI 10.1109/TMM.2018.2877888
   Yi SY, 2017, PATTERN RECOGN, V61, P524, DOI 10.1016/j.patcog.2016.08.025
   Zhang L, 2015, PATTERN RECOGN LETT, V62, P17, DOI 10.1016/j.patrec.2015.04.010
   Zhang L, 2018, NANO-MICRO LETT, V10, DOI 10.1007/s40820-017-0178-9
   Zhang Shao-Yan, 2013, ISRN Radiol, V2013, P874570, DOI 10.5402/2013/874570
   Zhang SP, 2018, IEEE T INTELL TRANSP, V19, P187, DOI 10.1109/TITS.2017.2766093
   Zhang SP, 2017, IEEE T NEUR NET LEAR, V28, P2357, DOI 10.1109/TNNLS.2016.2586194
   Zhang SP, 2017, IEEE T CIRC SYST VID, V27, P421, DOI 10.1109/TCSVT.2016.2539860
   Zhang SP, 2015, AAAI CONF ARTIF INTE, P3165
   Zhang SP, 2015, IEEE T CIRC SYST VID, V25, P1749, DOI 10.1109/TCSVT.2015.2406194
   Zhang SP, 2015, SIGNAL PROCESS, V110, P132, DOI 10.1016/j.sigpro.2014.08.027
   Zhang SP, 2014, INFORM SCIENCES, V281, P635, DOI 10.1016/j.ins.2013.12.052
   Zhang SP, 2013, PATTERN RECOGN, V46, P1772, DOI 10.1016/j.patcog.2012.10.006
   Zhang SP, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168757
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974
   Zhu HY, 2017, MULTIMED TOOLS APPL, V76, P4599, DOI 10.1007/s11042-016-3538-4
NR 59
TC 0
Z9 0
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2020
VL 16
IS 1
SU S
SI SI
AR 21
DI 10.1145/3357525
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EL
UT WOS:000583710300003
DA 2024-07-18
ER

PT J
AU Ding, YH
   Fan, HH
   Xu, ML
   Yang, Y
AF Ding, Yuhang
   Fan, Hehe
   Xu, Mingliang
   Yang, Yi
TI Adaptive Exploration for Unsupervised Person Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; unsupervised learning; domain adaptation; deep
   learning
ID DOMAIN ADAPTATION
AB Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called "target-only re-ID", in which only the unlabeled target data is used for training. The second one is called "domain adaptive re-ID", in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.
C1 [Ding, Yuhang] Southern Univ Sci & Technol, SUSTech UTS Joint Ctr CIS, Shenzhen, Peoples R China.
   [Fan, Hehe; Yang, Yi] Univ Technol Sydney, Ctr Artificial Intelligence, 15 Broadway, Sydney, NSW 2007, Australia.
   [Xu, Mingliang] Zhengzhou Univ, Sch Informat Engn, Zhengzhou, Henan, Peoples R China.
C3 Southern University of Science & Technology; University of Technology
   Sydney; Zhengzhou University
RP Ding, YH (corresponding author), Southern Univ Sci & Technol, SUSTech UTS Joint Ctr CIS, Shenzhen, Peoples R China.
EM dyh.ustc.uts@gmail.com; hehe.fan@student.uts.edu.au;
   iexumingliang@zzu.edu.cn; yee.i.yang@gmail.com
RI yang, yang/GVT-5210-2022; yang, yang/GWB-9426-2022; yang,
   yang/HGT-7999-2022; Lang, Ming/HIK-0758-2022; wang,
   Xiaoming/KBB-8854-2024; Yang, Yi/B-9273-2017
OI Yang, Yi/0000-0002-0512-880X
CR [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247939
   Chen SZ, 2016, IEEE T IMAGE PROCESS, V25, P2353, DOI 10.1109/TIP.2016.2545929
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Cho YJ, 2016, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2016.151
   Deng C, 2019, IEEE T GEOSCI REMOTE, V57, P1741, DOI 10.1109/TGRS.2018.2868851
   Deng C, 2018, PATTERN RECOGN, V77, P306, DOI 10.1016/j.patcog.2017.10.007
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fan HH, 2017, IEEE I CONF COMP VIS, P736, DOI 10.1109/ICCV.2017.86
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hermans Alexander, 2017, ARXIV170307737
   Karanam S, 2015, IEEE I CONF COMP VIS, P4516, DOI 10.1109/ICCV.2015.513
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YJ, 2018, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2018.00054
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin Shan, 2018, 2018 IEEE 36 VLSI TE
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Ma F, 2017, PR MACH LEARN RES, V70
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Pawan Kumar M., 2010, NIPS
   Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   Tian MQ, 2018, PROC CVPR IEEE, P5794, DOI 10.1109/CVPR.2018.00607
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Varior Rahul Rama, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang J, 2018, PATTERN RECOGN, V74, P38, DOI 10.1016/j.patcog.2017.09.014
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Wu ZR, 2018, LECT NOTES COMPUT SC, V11211, P712, DOI 10.1007/978-3-030-01234-2_42
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Wu ZY, 2015, IEEE T PATTERN ANAL, V37, P1095, DOI 10.1109/TPAMI.2014.2360373
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang Qize, 2019, CVPR
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zhedong Zheng, 2017, ACM Transactions on Multimedia Computing, Communications and Applications, V14, DOI 10.1145/3159171
   Zheng K, 2017, IEEE I CONF COMP VIS, P2877, DOI 10.1109/ICCV.2017.311
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2017, P IEEE C COMP VIS PA, P3346, DOI DOI 10.1109/CVPR.2017.357
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 67
TC 102
Z9 106
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 3
DI 10.1145/3369393
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Pan, ZQ
   Yi, XK
   Zhang, Y
   Yuan, H
   Wang, FL
   Kwong, S
AF Pan, Zhaoqing
   Yi, Xiaokai
   Zhang, Yun
   Yuan, Hui
   Wang, Fu Lee
   Kwong, Sam
TI Frame-level Bit Allocation Optimization Based on Video Content
   Characteristics for HEVC
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE HEVC; rate control; bit allocation; video content characteristics
ID ALGORITHM; TEXTURE
AB Rate control plays an important role in high efficiency video coding (HEVC), and bit allocation is the foundation of rate control The video content characteristics are significant for bit allocation, and modeling an accurate relationship between video content characteristics and bit allocation is essential for bit allocation optimization. Therefore, in this article, a video content characteristics-based frame-level optimal bit allocation algorithm is proposed for improving the rate distortion (RD) performance of HEVC. First, the number of search points of motion estimation is used to evaluate the motion activity of video content, and the relationship between the search points and bit allocation is modeled as the search-points model. Second, the grey level co-occurrence matrix and temporal perceptual information are used to evaluate the spatial and temporal texture complexity, and the relationship between the video content texture complexity and bit allocation is modeled as the texture-complexity model. Then, the search-points model and texture-complexity model are jointly employed to allocate the coding bits for the second and third layers of the HEVC hierarchical coding structure. Finally, the remaining coding bits of a group-of-pictures (GOP) are allocated to the first layer of HEVC coding structure. To evaluate the performance of the proposed algorithm, the RD performance and bitrate accuracy are used as evaluation criteria, and the experimental results show that when compared with the popularly used R-A model-based bit allocation algorithm, the proposed algorithm achieves an average of -3.43% BDBR reduction and 0.13 dB BDPSNR gains with only 0.02% loss of bitrate accuracy.
C1 [Pan, Zhaoqing] Nanjing Univ Informat Sci & Technol, Jiangsu Engn Ctr Network Monitoring, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
   [Yi, Xiaokai] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
   [Zhang, Yun] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Guangdong, Peoples R China.
   [Yuan, Hui] Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Shandong, Peoples R China.
   [Wang, Fu Lee] Caritas Inst Higher Educ, Sch Comp & Informat Sci, Hong Kong, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS; Shandong
   University; Saint Francis University Hong Kong; City University of Hong
   Kong
RP Pan, ZQ (corresponding author), Nanjing Univ Informat Sci & Technol, Jiangsu Engn Ctr Network Monitoring, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.; Pan, ZQ (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
EM zqpan3-c@my.cityu.edu.hk; xyi@nuist.edu.cn; yun.zhang@siat.ac.cn;
   huiyuan@sdu.edu.cn; pwang@cihe.edu.hk; cssamk@cityu.edu.hk
RI Zhang, Yun/V-7261-2019; Kwong, Sam/C-9319-2012; Yuan, Hui/HDO-3699-2022;
   Wang, Fu Lee/AAD-9782-2021
OI Zhang, Yun/0000-0001-9457-7801; Kwong, Sam/0000-0001-7484-7261; Yuan,
   Hui/0000-0001-5212-3393; Wang, Fu Lee/0000-0002-3976-0053
FU National Natural Science Foundation of China [61971232, 61501246]; Six
   Talent Peaks Project of Jiangsu Province [XYDXXJS-041]; Research Grants
   Council of the Hong Kong Special Administrative Region of China
   [UGC/FDS/E04/16]; Priority Academic Program Development of Jiangsu
   Higher Education Institutions; Collaborative Innovation Center of
   Atmospheric Environment and Equipment Technology
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61971232 and 61501246, in part by the
   Six Talent Peaks Project of Jiangsu Province under Grant XYDXXJS-041, in
   part by the Research Grants Council of the Hong Kong Special
   Administrative Region of China under Grant UGC/FDS/E04/16, in part by
   the Project through the Priority Academic Program Development of Jiangsu
   Higher Education Institutions, in part by the Collaborative Innovation
   Center of Atmospheric Environment and Equipment Technology.
CR [Anonymous], 2001, ITU T VCEG M
   [Anonymous], 2003, IEEE T CIRC SYST VID, DOI DOI 10.1109/TCSVT.2003
   [Anonymous], 2007, BT1788
   Bharati MH, 2004, CHEMOMETR INTELL LAB, V72, P57, DOI 10.1016/j.chemolab.2004.02.005
   Bossen Frank, 2012, JCTVCJ1100 ITUT SG 1
   Bossen Frank, 2011, JCTVCE700 ITUT SG 16
   Chen YQ, 2015, INT CONF MACH LEARN, P295, DOI 10.1109/ICMLC.2015.7340938
   Choi H, 2013, IEEE J-STSP, V7, P1112, DOI 10.1109/JSTSP.2013.2272241
   Corbera J., 1997, Q15A20 VID COD EXP G
   Fan R, 2017, IEEE T MULTIMEDIA, V19, P893, DOI 10.1109/TMM.2016.2642786
   Gao W, 2016, IEEE T MULTIMEDIA, V18, P988, DOI 10.1109/TMM.2016.2535254
   Gao W, 2016, IEEE T CIRC SYST VID, V26, P139, DOI 10.1109/TCSVT.2015.2444671
   He J, 2017, IET IMAGE PROCESS, V11, P245, DOI 10.1049/iet-ipr.2016.0166
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P928, DOI 10.1109/76.937431
   Lee B, 2014, IEEE T CIRC SYST VID, V24, P465, DOI 10.1109/TCSVT.2013.2276880
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Li Bin, 2012, JCTVCK0103 ITUT SG 1
   Li Bin, 2013, JCTVCM0036 ITUT SG 1
   Li L, 2018, IEEE T CIRC SYST VID, V28, P130, DOI 10.1109/TCSVT.2016.2598672
   LI ZG, 2003, JVTG012 ISOIEC MPEG
   Lin HW, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.4.043008
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   Naccari M., 2016, JCTVCY1002 ITUT SG 1
   Pan ZQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159170
   Pan ZQ, 2016, J VIS COMMUN IMAGE R, V40, P516, DOI 10.1016/j.jvcir.2016.07.018
   Pan ZQ, 2016, IEEE T BROADCAST, V62, P675, DOI 10.1109/TBC.2016.2580920
   Pourazad MT, 2012, IEEE CONSUM ELECTR M, V1, P36, DOI 10.1109/MCE.2012.2192754
   Shen LQ, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700298
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tu YK, 2007, IEEE T CIRC SYST VID, V17, P530, DOI 10.1109/TCSVT.2007.894041
   Wang MH, 2015, IEEE SIGNAL PROC LET, V22, P896, DOI 10.1109/LSP.2014.2377032
   Wang SS, 2013, IEEE J-STSP, V7, P1101, DOI 10.1109/JSTSP.2013.2272240
   Zhou ML, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107616
NR 33
TC 26
Z9 26
U1 2
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 15
DI 10.1145/3380827
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100014
DA 2024-07-18
ER

PT J
AU Kua, J
   Armitage, G
   Branch, P
   But, J
AF Kua, Jonathan
   Armitage, Grenville
   Branch, Philip
   But, Jason
TI Adaptive Chunklets and AQM for Higher-Performance Content Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE DASH; HTTP; TCP; FIFO; AQM; PIE; CoDel; FQ-CoDel; QoE; content
   streaming; chunklets; adaptive bitrate algorithm; queue management;
   latency; packet loss
ID IMPACT
AB Commercial streaming services such as Netflix and YouTube use proprietary HTTP-based adaptive streaming (HAS) techniques to deliver content to consumers worldwide. MPEG recently developed Dynamic Adaptive Streaming over HTTP (DASH) as a unifying standard for HAS-based streaming. In DASH systems, streaming clients employ adaptive bitrate (ABR) algorithms to maximise user Quality of Experience (QoE) under variable network conditions. In a typical Internet-enabled home, video streams have to compete with diverse application flows for the last-mile Internet Service Provider (ISP) bottleneck capacity. Under such circumstances, ABR algorithms will only act upon the fraction of the network capacity that is available, leading to possible QoE degradation. We have previously explored chunklets as an approach orthogonal to ABR algorithms, which uses parallel connections for intra-video chunk retrieval. Chunklets effectively make more bandwidth available for ABR algorithms in the presence of cross-traffic, especially in environments where Active Queue Management (AQM) schemes such as Proportional Integral controller Enhanced (PIE) and FlowQueue-Controlled Delay (FQ-CoDel) are deployed. However, chunklets consume valuable server/middlebox resources which typically handle hundreds of thousands of requests/connections per second. In this article, we propose 'adaptive chunklets' - a novel chunklet enhancement that dynamically tunes the number of concurrent connections. We demonstrate that the combination of adaptive chunklets and FQ-CoDel is the most effective strategy. Our experiments show that adaptive chunklets can reduce the number of connections by almost 30% and consume almost 8% less bandwidth than fixed chunklets while providing the same QoE.
C1 [Kua, Jonathan; Armitage, Grenville; Branch, Philip; But, Jason] Swinburne Univ Technol, John St, Hawthorn, Vic 3122, Australia.
   [Armitage, Grenville] Netflix Inc, 100 Winchester Cir, Los Gatos, CA 95032 USA.
C3 Swinburne University of Technology; Netflix, Inc.
RP Kua, J (corresponding author), Swinburne Univ Technol, John St, Hawthorn, Vic 3122, Australia.
EM jtkua@swin.edu.au; garmitage@swin.edu.au; pbranch@swin.edu.au;
   jbut@swin.edu.au
OI Kua, Jonathan/0000-0001-9699-9418
FU Netflix, Inc; Australian Government Research Training Program
   Scholarship scheme
FX This work was enabled in part by PhD stipend support from Netflix, Inc
   and an Australian Government Research Training Program Scholarship
   scheme.
CR Al-Saadi R., 2015, 150911A CAIA
   Al-Saadi R., 2016, 160418A CAIA
   Allman Mark., 2009, TCP CONGESTION CONTR, DOI [10.17487/RFC5681, DOI 10.17487/RFC5681]
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], 2011, China-Japan Joint Microwave Conf. Proceedings (CJMW), DOI DOI 10.1093/CID/CIQ146
   Ansari M, 2016, I S MOD ANAL SIM COM, P337, DOI 10.1109/MASCOTS.2016.63
   Armitage G., 2017, 170113A CAIA SWIN BU
   Armitage G., 2017, 26 INT C COMP COMM N, P1, DOI [10. 1109/ICCCN.2017.8038400, DOI 10.1109/ICCCN.2017.8038400]
   Bampis C. G., 2018, ARXIV180803898
   Bentaleb A, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P13, DOI 10.1145/3204949.3204961
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Brunnstrom K., 2013, Qualinet White Paper on Definitions of Quality of Experience
   Crowcroft J., 1998, Computer Communication Review, V28, P53, DOI 10.1145/293927.293930
   Gettys J, 2011, IEEE INTERNET COMPUT, V15, P95, DOI 10.1109/MIC.2011.56
   Hoiland-Jorgensen T, 2018, IETF RFC 8290, DOI [10.17487/RFC8290, DOI 10.17487/RFC8290]
   Hoiland-Jorgensen T, 2018, IEEE COMMUN LETT, V22, P2266, DOI 10.1109/LCOMM.2018.2871457
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Huang TY, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P86, DOI 10.1145/3304109.3306219
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   ISO/IEC, 2012, 230912012 ISOIEC
   Jiang J, 2012, 2012 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE AND INTELLIGENT AGENT TECHNOLOGY (WI-IAT 2012), VOL 2, P90, DOI 10.1109/WI-IAT.2012.40
   Juluri P, 2015, IEEE INT CONF COMM, P1765, DOI 10.1109/ICCW.2015.7247436
   Krishnan R. K., 2012, P INT MEAS C, P211
   Kua J., 2017, P 2017 26 INT C COMP, P1, DOI [10.1109/ICCCN.2017.8038403, DOI 10.1109/ICCCN.2017.8038403]
   Kua J, 2017, IEEE INTERNET THINGS, V4, P1399, DOI 10.1109/JIOT.2017.2722683
   Kua J, 2017, IEEE COMMUN SURV TUT, V19, P1842, DOI 10.1109/COMST.2017.2685630
   Kua J, 2016, C LOCAL COMPUT NETW, P121, DOI 10.1109/LCN.2016.24
   Kuschnig R, 2010, CONSUM COMM NETWORK, P200
   Lederer S., 2012, P 3 MULT SYST C, P89
   Mäki T, 2015, 2015 11TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P591, DOI 10.1109/SITIS.2015.41
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mok R.K., 2011, PROC ACM SIGCOMM WOR, P31, DOI DOI 10.1145/2018602.2018611
   Nichols Kathleen, 2018, RFC 8289, DOI DOI 10.17487/RFC8289
   Pan Rong, 2017, RFC 8033, DOI [10.17487/RFC8033, DOI 10.17487/RFC8033]
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Spiteri K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P123, DOI 10.1145/3204949.3204953
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Stewart L, 2009, LECT NOTES COMPUT SC, V5550, P392, DOI 10.1007/978-3-642-01399-7_31
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   White G., 2017, 8034 RFC, DOI [10.17487/RFC8034, DOI 10.17487/RFC8034]
   Yin XQ, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P325, DOI 10.1145/2785956.2787486
   Zander S., 2015, 150529A CAIA SWINB U
   Zander S, 2013, C LOCAL COMPUT NETW, P264, DOI 10.1109/LCN.2013.6761245
NR 43
TC 8
Z9 8
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2019
VL 15
IS 4
AR 115
DI 10.1145/3344381
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KJ8DH
UT WOS:000512285800015
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Mou, WX
   Gunes, H
   Patras, I
AF Mou, Wenxuan
   Gunes, Hatice
   Patras, Ioannis
TI Alone versus In-a-group: A Multi-modal Framework for Automatic Affect
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Affect analysis; multimodal interaction; group settings; non-verbal
   behaviours; context analysis
ID FACIAL-EXPRESSION; FACE
AB Recognition and analysis of human affect has been researched extensively within the field of computer science in the past two decades. However, most of the past research in automatic analysis of human affect has focused on the recognition of affect displayed by people in individual settings and little attention has been paid to the analysis of the affect expressed in group settings. In this article, we first analyze the affect expressed by each individual in terms of arousal and valence dimensions in both individual and group videos and then propose methods to recognize the contextual information, i.e., whether a person is alone or in-agroup by analyzing their face and body behavioral cues. For affect analysis, we first devise affect recognition models separately in individual and group videos and then introduce a cross-condition affect recognition model that is trained by combining the two different types of data. We conduct a set of experiments on two datasets that contain both individual and group videos. Our experiments show that (1) the proposed Volume Quantized Local Zernike Moments Fisher Vector outperforms other unimodal features in affect analysis; (2) the temporal learning model, Long-Short Term Memory Networks, works better than the static learning model, Support Vector Machine; (3) decision fusion helps to improve affect recognition, indicating that body behaviors carry emotional information that is complementary rather than redundant to the emotion content in facial behaviors; and (4) it is possible to predict the context, i.e., whether a person is alone or in-a-group, using their non-verbal behavioral cues.
C1 [Mou, Wenxuan; Patras, Ioannis] Queen Mary Univ London, London, England.
   [Gunes, Hatice] Univ Cambridge, Cambridge, England.
C3 University of London; Queen Mary University London; University of
   Cambridge
RP Mou, WX (corresponding author), Queen Mary Univ London, London, England.
EM w.mou@qmul.ac.uk; Hatice.Gunes@cl.cam.ac.uk; i.patras@qmul.ac.uk
RI ARSLAN, Okan/AAA-3232-2020
FU CSC/Queen Mary joint PhD scholarship; EPSRC under its IDEAS Factory
   Sandpits call on Digital Personhood [EP/L00416X/1]; Nvidia corporation
FX The work of Wenxuan Mou is supported by CSC/Queen Mary joint PhD
   scholarship. The work of Hatice Gunes and Wenxuan Mou is partially
   funded by the EPSRC under its IDEAS Factory Sandpits call on Digital
   Personhood (Grant No. EP/L00416X/1). This work is also supported by
   Nvidia corporation with the donation of a TitanX GPU.
CR Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606
   [Anonymous], 2017, P 19 ACM INT C MULT
   [Anonymous], 2014, OXFORD HDB AFFECTIVE
   [Anonymous], 2016, ARXIV161003640
   Bänziger T, 2012, EMOTION, V12, P1161, DOI 10.1037/a0025827
   Barsade SG, 2012, CURR DIR PSYCHOL SCI, V21, P119, DOI 10.1177/0963721412438352
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Çeliktutan O, 2017, IEEE T AFFECT COMPUT, V8, P29, DOI 10.1109/TAFFC.2015.2513401
   Celiktutan Oya, 2014, P C IM PROC ICIP 14
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen S., 2017, P 7 ANN WORKSH AUD V
   CRONBACH LJ, 1951, PSYCHOMETRIKA, V16, P297, DOI [10.1007/BF02310555, DOI 10.1007/BF02310555]
   Dautenhahn K, 2007, PHILOS T R SOC B, V362, P679, DOI 10.1098/rstb.2006.2004
   Dhall A., 2016, P ACM INT C MULT INT
   Dhall A, 2015, IEEE INT CONF AUTOMA
   Dhall A, 2015, IEEE T AFFECT COMPUT, V6, P13, DOI 10.1109/TAFFC.2015.2397456
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Dhall Abhinav, 2015, P INT C AFF COMP INT
   Dhall Abhinav, 2012, FINDING HAPPIEST MOM
   Fan Y., 2016, P ACM INT C MULT INT
   Gallagher Andrew, 2009, P IEEE INT C COMP VI
   Goodfellow Ian J., 2013, NEURAL NETW
   Gunes Hatice, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P827, DOI 10.1109/FG.2011.5771357
   Gunes H., 2010, Automatic, dimensional and continuous emotion recognition
   Gunes Hatice, 2015, EMOTION RECOGN PATTE
   Hernandez J., 2012, MOOD METER COUNTING
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang Xiaohua, 2018, IEEE T MULTIMEDIA
   Huang Xiaohua, 2015, P BRIT MACH VIS C BM
   Jain V., 2014, P INT WORKSH AUD VIS
   Kaltwang Sebastian, 2012, P INT S VIS COMP
   Karg M, 2013, IEEE T AFFECT COMPUT, V4, P341, DOI 10.1109/T-AFFC.2013.29
   Kleinsmith A, 2013, IEEE T AFFECT COMPUT, V4, P15, DOI 10.1109/T-AFFC.2012.16
   Koelstra S, 2013, IMAGE VISION COMPUT, V31, P164, DOI 10.1016/j.imavis.2012.10.002
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Li J., 2016, P ACM INT C MULT INT
   Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413
   Meeren HKM, 2005, P NATL ACAD SCI USA, V102, P16518, DOI 10.1073/pnas.0507650102
   Miranda-Correa J. A., 2017, ARXIV170202510
   Morency Louis-Philippe, 2013, SOC EMOT NATURE ARTI
   Mou W., 2016, P ACM INT C MULT
   Mou WX, 2015, IEEE INT CONF AUTOMA
   Mou Wenxuan, 2016, P IEEE INT C COMP VI
   Palasek Petar, 2016, P INT WORKSH MULT AN
   Pantic M, 2006, IEEE T SYST MAN CY B, V36, P433, DOI 10.1109/TSMCB.2005.859075
   Ringeval F, 2013, IEEE INT CONF AUTOMA
   Ringeval Fabien, 2017, P 7 ANN WORKSHOP AUD, P3, DOI DOI 10.1145/3133944.3133953
   Ringeval Fabien, 2015, P INT WORKSH AUD VIS
   Sanchez J., 2013, INT J COMPUT VIS
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Sariyanidi E, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.108
   Sikka Karan, 2013, P ACM INT C MULT INT
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Tan Lianzhi, 2017, P 19 ACM INT C MULT
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Valstar Michel, 2016, P ACM INT C MULT WOR
   Van den Stock J, 2007, EMOTION, V7, P487, DOI 10.1037/1528-3542.7.3.487
   Vlachostergiou A., 2014, P ACM WORKSH UND MOD
   Wang H., 2013, INT J COMPUT VIS, P1
   Wang Heng, P IEEE INT C COMP VI
   Wang H, 2011, PROC CVPR IEEE, P793, DOI 10.1109/CVPR.2011.5995379
   Xiong X., 2013, P IEEE INT C COMP VI
   Yang Heng, 2015, P BRIT MACH VIS C BM
   Yi Zhu, 2014, P 19 C HUM VIS EL IM
NR 64
TC 12
Z9 12
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2019
VL 15
IS 2
AR 47
DI 10.1145/3321509
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IM3ZV
UT WOS:000477935400018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, JW
   Zha, ZJ
   Chen, XJ
   Wang, ZL
   Zhang, YD
AF Liu, Jiawei
   Zha, Zheng-Jun
   Chen, Xuejin
   Wang, Zilei
   Zhang, Yongdong
TI Dense 3D-Convolutional Neural Network for Person Re-Identification in
   Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; deep learning; network structure
AB Person re-identification aims at identifying a certain pedestrian across non-overlapping multi-camera networks in different time and places. Existing person re-identification approaches mainly focus on matching pedestrians on images; however, little attention has been paid to re-identify pedestrians in videos. Compared to images, video clips contain motion patterns of pedestrians, which is crucial to person re-identification. Moreover, consecutive video frames present pedestrian appearance with different body poses and from different viewpoints, providing valuable information toward addressing the challenge of pose variation, occlusion, and viewpoint change, and so on. In this article, we propose a Dense 3D-Convolutional Network (D3DNet) to jointly learn spatio-temporal and appearance representation for person re-identification in videos. The D3DNet consists of multiple three-dimensional (3D) dense blocks and transition layers. The 3D dense blocks enlarge the receptive fields of visual neurons in both spatial and temporal dimensions, leading to discriminative appearance representation as well as short-term and long-term motion patterns of pedestrians without the requirement of an additional motion estimation module. Moreover, we formulate a loss function consisting of an identification loss and a center loss to minimize intra-class variance and maximize inter-class variance simultaneously, toward addressing the challenge of large intra-class variance and small inter-class variance. Extensive experiments on two real-world video datasets of person identification, i.e., MARS and iLIDS-VID, have shown the effectiveness of the proposed approach.
C1 [Liu, Jiawei; Zha, Zheng-Jun; Chen, Xuejin; Wang, Zilei; Zhang, Yongdong] Univ Sci & Technol China, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zha, ZJ (corresponding author), Univ Sci & Technol China, Hefei 230027, Anhui, Peoples R China.
EM ljw368@mail.ustc.edu.cn; zhazj@ustc.edu.cn; xjchen99@ustc.edu.cn;
   zlwang@ustc.edu.cn; zhyd73@ustc.edu.cn
RI Zha, Zheng-Jun/AAE-8408-2020; Zha, Zheng-Jun/AAF-8667-2020
OI Zha, Zheng-Jun/0000-0003-2510-8993; 
FU National Key R&D Program of China [2017YFB1300201]; National Natural
   Science Foundation of China (NSFC) [61622211, 61472392, 61620106009,
   61525206]
FX This work was supported by National Key R&D Program of China under Grant
   2017YFB1300201 and National Natural Science Foundation of China (NSFC)
   under Grants 61622211, 61472392, 61620106009, and 61525206.
CR [Anonymous], 2017, ARXIV170606196
   [Anonymous], 2004, Int. J. Comput. Vis., DOI [DOI 10.1023/B:VISI.0000029664.99615.94, 10.1023/B:VISI.0000029664.99615.94]
   [Anonymous], 2006, IEEE Transactions on Pattern Analysis and Machine Intelligence, DOI DOI 10.1109/TPAMI.2006.38
   [Anonymous], ELECT J DIFFEREN EQU
   Chen DP, 2018, PROC CVPR IEEE, P8649, DOI 10.1109/CVPR.2018.00902
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Chen DP, 2015, PROC CVPR IEEE, P1565, DOI 10.1109/CVPR.2015.7298764
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Davis J. V., 2007, ICML, P209
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiao YF, 2018, IEEE T MULTIMEDIA, V20, P2693, DOI 10.1109/TMM.2018.2815998
   Karanam S, 2015, IEEE I CONF COMP VIS, P4516, DOI 10.1109/ICCV.2015.513
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Li ZT, 2018, IEEE T IMAGE PROCESS, V27, P4478, DOI 10.1109/TIP.2018.2839916
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lisanti G, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3038916
   Liu H, 2018, IEEE T CIRC SYST VID, V28, P2788, DOI 10.1109/TCSVT.2017.2715499
   Liu Jiawei, 2016, P 24 ACM INT C MULT, P192
   Liu K, 2015, IEEE I CONF COMP VIS, P3810, DOI 10.1109/ICCV.2015.434
   Liu Y, 2017, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2017.499
   Ma XL, 2017, PATTERN RECOGN, V65, P197, DOI 10.1016/j.patcog.2016.11.018
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Pala F, 2016, IEEE T CIRC SYST VID, V26, P788, DOI 10.1109/TCSVT.2015.2424056
   Shi ZY, 2015, PROC CVPR IEEE, P4184, DOI 10.1109/CVPR.2015.7299046
   Su C, 2018, IEEE T PATTERN ANAL, V40, P1167, DOI 10.1109/TPAMI.2017.2679002
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Varior RR, 2016, IEEE T IMAGE PROCESS, V25, P3395, DOI 10.1109/TIP.2016.2531280
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang TQ, 2016, IEEE T PATTERN ANAL, V38, P2501, DOI 10.1109/TPAMI.2016.2522418
   Wang YC, 2018, PROC CVPR IEEE, P1470, DOI 10.1109/CVPR.2018.00159
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1
   Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507
   Yan YC, 2016, LECT NOTES COMPUT SC, V9910, P701, DOI 10.1007/978-3-319-46466-4_42
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   You JJ, 2016, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2016.150
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang  Wei, 2017, ARXIV170206294
   Zhao  Liming, 2017, P IEEE INT C COMP VI, V8
   Zhao R, 2017, IEEE T PATTERN ANAL, V39, P356, DOI 10.1109/TPAMI.2016.2544310
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng Liang, 2016, ARXIV161002984
   Zheng WS, 2016, IEEE T PATTERN ANAL, V38, P591, DOI 10.1109/TPAMI.2015.2453984
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhou SP, 2017, PROC CVPR IEEE, P5028, DOI 10.1109/CVPR.2017.534
   Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 60
TC 48
Z9 60
U1 3
U2 28
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2019
VL 15
IS 1
SU S
AR 8
DI 10.1145/3231741
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HM9IU
UT WOS:000459798100008
DA 2024-07-18
ER

PT J
AU Ma, M
   Zhang, L
   Liu, JC
   Wang, Z
   Pang, HT
   Sun, LF
   Li, WH
   Hou, GL
   Chu, KY
AF Ma, Ming
   Zhang, Lei
   Liu, Jiangchuan
   Wang, Zhi
   Pang, Haitian
   Sun, Lifeng
   Li, Weihua
   Hou, Guangling
   Chu, Kaiyan
TI Characterizing User Behaviors in Mobile Personal Livecast: Towards an
   Edge Computing-assisted Paradigm
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Mobile personal livecast; User behavior analysis; Edge computing
   paradigm
ID TIME
AB Mobile personal livecast (MPL) services are emerging and have received great attention recently. In MPL, numerous and geo-distributed ordinary people broadcast their video contents to worldwide viewers. Different from conventional social networking services like Twitter and Facebook, which have a tolerance for interaction delay, the interactions (e.g., chat messages) in a personal livecast must be in real-time with low feedback latency. These unique characteristics inspire us to: (1) investigate how the relationships (e.g., social links and geo-locations) between viewers and broadcasters influence the user behaviors, which has yet to be explored in depth; and (2) explore insights to benefit the improvement of system performance. In this article, we carry out extensive measurements of a representative MPL system, with a large-scale dataset containing 11M users. In the current costly and limited cloud-based MPL system, which is faced with scalability problem, we find: (1) the long content uploading distances between broadcasters and cloud ingesting servers result in an impaired system QoS, including a high broadcast latency and a frequently buffering events; and (2) most of the broadcasters in MPL are geographically locally popular (the majority of the views come from the same region of the broadcaster), which consume vast computation and bandwidth resources of the clouds and Content Delivery Networks. Fortunately, the emergence of edge computing, which provides cloud-computing capabilities at the edge of the mobile network, naturally sheds new light on the MPL system; i.e., localized ingesting, transcoding, and delivering locally popular live content is possible. Based on these critical observations, we propose an edge-assisted MPL system that collaboratively utilizes the core-cloud and abundant edge computing resources to improve the system efficiency and scalability. In our framework, we consider a dynamic broadcaster assignment to minimize the broadcast latency while keeping the resource lease cost low. We formulate the broadcaster scheduling as a stable matching with migration problem to solve it effectively. Compared with the current pure cloud-based system, our edge-assisted delivery approach reduces the broadcast latency by about 35%.
C1 [Ma, Ming; Pang, Haitian; Sun, Lifeng] Tsinghua Univ, Beijing, Peoples R China.
   [Zhang, Lei; Liu, Jiangchuan] Simon Fraser Univ, Sch Comp Sci, 8888 Univ Dr, Burnaby, BC V5A 1S6, Canada.
   [Li, Weihua] Beijing PowerInfo Co Ltd, Beijing, Peoples R China.
   [Hou, Guangling] Meelive Network Technol Co Ltd, Beijing, Peoples R China.
   [Chu, Kaiyan] Alibaba Co Ltd, Hangzhou, Zhejiang, Peoples R China.
C3 Tsinghua University; Simon Fraser University; Alibaba Group
RP Ma, M (corresponding author), Tsinghua Univ, Beijing, Peoples R China.
EM mm13@mails.tsinghua.edu.cn; lza70@cs.sfu.ca; jcliu@cs.sfu.ca;
   pht14@mails.tsinghua.edu.cn; sunlf@tsinghua.edu.cn;
   weihua_li@sjdd.com.cn; guangling.hou@meelive.cn;
   chuxu.cky@alibaba-inc.com
RI Wang, Zhi/GZB-2713-2022
OI Wang, Zhi/0000-0001-6952-8848
FU National Natural Science Foundation of China [61472204, 61521002,
   61531006, U1301253]; Beijing Key Laboratory of Networked Multimedia
   [Z161100005016051]; National Key Research and Development Program of
   China [2018YFB1003703]; Tsinghua-Alibaba Cooperation Project
FX This work is part-funded by National Natural Science Foundation of China
   under Grant No. 61472204, 61521002, 61531006, and U1301253, Beijing Key
   Laboratory of Networked Multimedia under Grant No. Z161100005016051,
   National Key Research and Development Program of China under Grant No.
   2018YFB1003703, and the Tsinghua-Alibaba Cooperation Project.
CR Ahmed ANR, 2016, IEEE IMTC P, P1327
   [Anonymous], P INT MEAS C IMC 12
   [Anonymous], 2014, TWITCH IS 4 PEAK US
   [Anonymous], P IEEE INT C NETW PR
   [Anonymous], IDT HIGHL COLL IBM 5
   [Anonymous], P C WORLD WID WEB WE
   [Anonymous], 2016, SOCIALMEDIATODAY
   [Anonymous], STEAM HARDW HIGHL SO
   [Anonymous], ACM SIGOPS OPERATING
   [Anonymous], P WORKSH NETW OP SYS
   [Anonymous], 2016, DMR
   [Anonymous], 2015, P 6 ACM MULTIMEDIA S, DOI DOI 10.1145/2713168.2713195
   [Anonymous], 2 ACM IEEE S EDG COM
   [Anonymous], CISC VIS NETW IND GL
   [Anonymous], P WORKSH NETW OP SYS
   [Anonymous], MOB EDG COMP
   [Anonymous], P C COMP COMM INFOCO
   [Anonymous], CISC VIS NETW IND GL
   Aparicio-Pardo Ramon., 2015, Proceedings of the 6th ACM Multimedia Systems Conference, MMSys'15, P49
   Babaoglu O., 2012, P 27 ANN ACM S APPL, P412, DOI DOI 10.1145/2245276.2245357
   Chan Connie., 2016, Andreessen Horowitz
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Gale D, 2013, AM MATH MON, V120, P386, DOI 10.4169/amer.math.monthly.120.05.386
   Guo J, 2006, IEEE T PARALL DISTR, V17, P1321, DOI 10.1109/TPDS.2006.159
   He QY, 2017, IEEE T MULTIMEDIA, V19, P1365, DOI 10.1109/TMM.2017.2652061
   He QY, 2016, IEEE T MULTIMEDIA, V18, P916, DOI 10.1109/TMM.2016.2544698
   Hu W, 2015, IEEE ICC, P5884, DOI 10.1109/ICC.2015.7249260
   Huang ZX, 2011, IEEE INFOCOM SER, P201, DOI 10.1109/INFCOM.2011.5935009
   Jia M., 2016, CHINESE MED J-PEKING, P1, DOI DOI 10.1109/INFOCOM.2016.7524411
   Kamecke U., 1992, Two sided matching: A study in gametheoretic modeling and analysis
   Krajsa O, 2011, 2011 34TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P231, DOI 10.1109/TSP.2011.6043737
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Li Y., 2011, Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference, P209, DOI [10.1145/2068816, DOI 10.1145/2068816]
   Lin S, 2013, IEEE INT SYMP CIRC S, P2864, DOI 10.1109/ISCAS.2013.6572476
   Mach P, 2017, IEEE COMMUN SURV TUT, V19, P1628, DOI 10.1109/COMST.2017.2682318
   Roth AE, 2008, INT J GAME THEORY, V36, P537, DOI 10.1007/s00182-008-0117-6
   Shi WS, 2016, IEEE INTERNET THINGS, V3, P637, DOI 10.1109/JIOT.2016.2579198
   SHMOYS DB, 1993, MATH PROGRAM, V62, P461, DOI 10.1007/BF01585178
   Wang Bolun., 2016, Proceedings of the 2016 ACM on Internet Measurement Conference, P485
   Wang F, 2016, IEEE ACM T NETWORK, V24, P272, DOI 10.1109/TNET.2014.2362541
   Wang SX, 2013, IEEE T MULTIMEDIA, V15, P870, DOI 10.1109/TMM.2013.2240674
   Wang Tao, 2016, P IEEE C COMPUTER CO, P1
   Xi ZY, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTERS, COMMUNICATIONS, AND SYSTEMS (ICCCS), P252, DOI 10.1109/CCOMS.2015.7562910
   Xu H, 2011, IEEE INFOCOM SER, P586, DOI 10.1109/INFCOM.2011.5935232
   Zhang C., 2015, ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, P55, DOI DOI 10.1145/2736084.2736091
   Zhang C, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3095755
   Zhang GP, 2003, NEUROCOMPUTING, V50, P159, DOI 10.1016/S0925-2312(01)00702-0
   Zhang Lei., 2014, P NETWORK OPERATING, P85, DOI DOI 10.1145/2597176.2578278
   Zhi Zhou, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P2650, DOI 10.1109/INFOCOM.2015.7218656
   Zhu YF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1372, DOI 10.1145/3123266.3123384
NR 51
TC 9
Z9 9
U1 0
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
SU S
AR 66
DI 10.1145/3219751
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG9GD
UT WOS:000455314500014
DA 2024-07-18
ER

PT J
AU Muszynski, M
   Kostoulas, T
   Lombardo, P
   Pun, T
   Chanel, G
AF Muszynski, Michal
   Kostoulas, Theodoros
   Lombardo, Patrizia
   Pun, Thierry
   Chanel, Guillaume
TI Aesthetic Highlight Detection in Movies Based on Synchronization of
   Spectators' Reactions
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Synchronization; dynamical systems; physiological signals; behavioral
   signals; aesthetic experience; aesthetic highlight detection; video
   summarization; affective computing
ID TIME-SERIES; EEG; EMOTIONS
AB Detection of aesthetic highlights is a challenge for understanding the affective processes taking place during movie watching. In this article, we study spectators' responses to movie aesthetic stimuli in a social context. Moreover, we look for uncovering the emotional component of aesthetic highlights in movies. Our assumption is that synchronized spectators' physiological and behavioral reactions occur during these highlights because: (i) aesthetic choices of filmmakers are made to elicit specific emotional reactions (e.g., special effects, empathy, and compassion toward a character) and (ii) watching a movie together causes spectators' affective reactions to be synchronized through emotional contagion. We compare different approaches to estimation of synchronization among multiple spectators' signals, such as pairwise, group, and overall synchronization measures to detect aesthetic highlights in movies. The results show that the unsupervised architecture relying on synchronization measures is able to capture different properties of spectators' synchronization and detect aesthetic highlights based on both spectators' electrodermal and acceleration signals. We discover that pairwise synchronization measures perform the most accurately independently of the category of the highlights and movie genres. Moreover, we observe that electrodermal signals have more discriminative power than acceleration signals for highlight detection.
C1 [Muszynski, Michal; Kostoulas, Theodoros; Lombardo, Patrizia; Pun, Thierry; Chanel, Guillaume] Univ Geneva, 24 Rue Gen Dufour, CH-1211 Geneva, Switzerland.
   [Kostoulas, Theodoros] Bournemouth Univ, Poole House,Talbot Campus, Bournemouth BH12 5BB, Dorset, England.
C3 University of Geneva; Bournemouth University
RP Muszynski, M (corresponding author), Univ Geneva, 24 Rue Gen Dufour, CH-1211 Geneva, Switzerland.
EM michal.muszynski@unige.ch; tkostoulas@bournemouth.ac.uk;
   patrizia.lombardo@unige.ch; thierry.punn@unige.ch;
   guillaume.chanel@unige.ch
RI Chanel, Guillaume/AAB-4289-2020
OI Chanel, Guillaume/0000-0002-6184-8924; Kostoulas,
   Theodoros/0000-0003-2201-6938
FU Swiss Center for Affective Sciences; Swiss National Science Foundation
FX This work is supported by grants from the Swiss Center for Affective
   Sciences and the Swiss National Science Foundation.
CR Ancona N, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.056221
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2005, Electric Fields of the Brain: The Neurophysics of Eeg
   [Anonymous], NIPS
   [Anonymous], 2013, PSYCHOL BEING
   [Anonymous], 2000, BOREDOM ANXIETY
   [Anonymous], 2007, Information retrieval for music and motion
   [Anonymous], 2015, P 1 INT WORKSH AFF S
   [Anonymous], 2013, Social Media Retrieval, DOI [10.1007/978-1-4471-4555-4_10, DOI 10.1007/978-1-4471-4555-4_10]
   [Anonymous], FRONTIERS ICT
   [Anonymous], **NON-TRADITIONAL**
   [Anonymous], PATTERN RECOGNITION
   [Anonymous], P 2015 7 INT WORKSH
   [Anonymous], P MEDIAEVAL 2011 WOR
   Aviyente S, 2005, INT CONF ACOUST SPEE, P481
   Baveye Y, 2015, INT CONF AFFECT, P77, DOI 10.1109/ACII.2015.7344554
   Bazin Andre., 2004, What is Cinema
   Berndt D.J., 1994, AAAI 94 WORKSH KNOWL, V10, P359, DOI [10.5555/3000850.3000887, DOI 10.5555/3000850.3000887]
   Blinowska KJ, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.050902
   Bordwell D., 1997, FILM ART INTRO
   Borenstein M., 2009, INT STAT REV
   Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2
   Carmeli C, 2005, NEUROIMAGE, V25, P339, DOI 10.1016/j.neuroimage.2004.11.049
   Cavell Stanley., 1979, The World Viewed: Reflections on the Ontology of Film
   Chen L, 2011, OPER THEORY ADV APPL, V211, P119, DOI 10.1007/978-3-0348-0024-2_3
   Chen YH, 2004, PHYS LETT A, V324, P26, DOI 10.1016/j.physleta.2004.02.032
   Chepushtanova S, 2015, INT GEOSCI REMOTE SE, P449, DOI 10.1109/IGARSS.2015.7325797
   Cohen J., 1988, STAT POWER ANAL BEHA
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Csikszentmihalyi M., 2014, PSYCHOL OPTIMAL EXPE
   Cui D, 2010, NEURAL NETWORKS, V23, P698, DOI 10.1016/j.neunet.2010.04.003
   Cupchik GC, 2009, BRAIN COGNITION, V70, P84, DOI 10.1016/j.bandc.2009.01.003
   Dauwels J, 2010, NEUROIMAGE, V49, P668, DOI 10.1016/j.neuroimage.2009.06.056
   David BORDWELL., 1994, Film History: An Introduction"
   Deleuze G., 1989, Cinema 2:The Time-Image
   Deleuze G., 1986, THE MOVEMENT IMAGE
   Eyben F, 2013, INT CONF ACOUST SPEE, P483, DOI 10.1109/ICASSP.2013.6637694
   Fleureau J, 2013, INT CONF AFFECT, P73, DOI 10.1109/ACII.2013.19
   Ghaemmaghami P, 2015, LECT NOTES COMPUT SC, V9279, P683, DOI 10.1007/978-3-319-23231-7_61
   Ghrist R, 2008, B AM MATH SOC, V45, P61, DOI 10.1090/s0273-0979-07-01191-3
   Golland Y, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0125804
   GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791
   Gunduz A, 2009, SIGNAL PROCESS, V89, P14, DOI 10.1016/j.sigpro.2008.07.005
   Hamm Jihun, 2008, P 25 INT C MACH LEAR, P376, DOI DOI 10.1145/1390156.1390204
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hatfield Elaine., 1992, EMOTION SOCIAL BEHAV
   Jalili M, 2014, IEEE T NEUR SYS REH, V22, P212, DOI 10.1109/TNSRE.2013.2289899
   Jelles B, 2008, CLIN NEUROPHYSIOL, V119, P837, DOI 10.1016/j.clinph.2007.12.002
   Joho H, 2011, MULTIMED TOOLS APPL, V51, P505, DOI 10.1007/s11042-010-0632-x
   Juslin PN, 2013, PHYS LIFE REV, V10, P235, DOI 10.1016/j.plrev.2013.05.008
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   Kipp M., 2014, The Oxford handbook of Corpus phonology (chapter 21), P420
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Kostoulas Theodoros, 2015, P 1 WORKSHOP MODELIN, P35
   Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138
   Kroupi E, 2013, INT CONF AFFECT, P865, DOI 10.1109/ACII.2013.162
   Lachaux JP, 1999, HUM BRAIN MAPP, V8, P194, DOI 10.1002/(SICI)1097-0193(1999)8:4<194::AID-HBM4>3.0.CO;2-C
   Liu W, 2015, PROC CVPR IEEE, P3707, DOI 10.1109/CVPR.2015.7298994
   Markovic S, 2012, I-PERCEPTION, V3, P1, DOI 10.1068/i0450aap
   Muszynski M, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P292, DOI 10.1145/2964284.2967229
   Muszynski M, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P235, DOI 10.1145/2818346.2820773
   Penet C, 2015, MULTIMED TOOLS APPL, V74, P1143, DOI 10.1007/s11042-014-2038-7
   Perea JA, 2015, FOUND COMPUT MATH, V15, P799, DOI 10.1007/s10208-014-9206-z
   Perea JA, 2015, BMC BIOINFORMATICS, V16, DOI 10.1186/s12859-015-0645-6
   Pereda E, 2005, PROG NEUROBIOL, V77, P1, DOI 10.1016/j.pneurobio.2005.10.003
   Quiroga RQ, 2000, PHYS REV E, V61, P5142, DOI 10.1103/PhysRevE.61.5142
   RULKOV NF, 1995, PHYS REV E, V51, P980, DOI 10.1103/PhysRevE.51.980
   Saito N, 1998, BIOL PSYCHIAT, V43, P794, DOI 10.1016/S0006-3223(97)00547-7
   Scherer KR, 2005, SOC SCI INFORM, V44, P695, DOI 10.1177/0539018405058216
   Sigari M.-H., 2015, Int. J. Comput. Graph., V6, P13
   Soleymani M., 2008, P 2 ACM WORKSHOP MUL, P32, DOI DOI 10.1145/1460676.1460684
   Soleymani M, 2014, IEEE INT CON MULTI
   Takens F, 1981, Lecture Notes in Mathematics, V898, P366, DOI [10.1007/BFb0091924, DOI 10.1007/BFB0091924]
   Tarvainen J, 2014, IEEE T MULTIMEDIA, V16, P2085, DOI 10.1109/TMM.2014.2357688
   TELLEGEN A, 1974, J ABNORM PSYCHOL, V83, P268, DOI 10.1037/h0036681
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Yang H, 2015, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2015.526
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Zhou F, 2014, INTERACT COMPUT, V26, P285, DOI 10.1093/iwc/iwt039
NR 80
TC 12
Z9 12
U1 2
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
AR 68
DI 10.1145/3175497
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GT7CI
UT WOS:000444677600001
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Cornia, M
   Baraldi, L
   Serra, G
   Cucchiara, R
AF Cornia, Marcella
   Baraldi, Lorenzo
   Serra, Giuseppe
   Cucchiara, Rita
TI Paying More Attention to Saliency: Image Captioning with Saliency and
   Context Attention
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Saliency; visual saliency prediction; image captioning; deep learning
AB Image captioning has been recently gaining a lot of attention thanks to the impressive achievements shown by deep captioning architectures, which combine Convolutional Neural Networks to extract image representations and Recurrent Neural Networks to generate the corresponding captions. At the same time, a significant research effort has been dedicated to the development of saliency prediction models, which can predict human eye fixations. Even though saliency information could be useful to condition an image captioning architecture, by providing an indication of what is salient and what is not, research is still struggling to incorporate these two techniques. In this work, we propose an image captioning approach in which a generative recurrent neural network can focus on different parts of the input image during the generation of the caption, by exploiting the conditioning given by a saliency prediction model on which parts of the image are salient and which are contextual. We show, through extensive quantitative and qualitative experiments on large-scale datasets, that our model achieves superior performance with respect to captioning baselines with and without saliency and to different state-of-the-art approaches combining saliency and captioning.
C1 [Cornia, Marcella; Baraldi, Lorenzo; Cucchiara, Rita] Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, Via P Vivarelli 10, I-41125 Modena, Italy.
   [Serra, Giuseppe] Univ Udine, Dept Comp Sci Math & Phys, Via Sci 206, I-33100 Udine, Italy.
C3 Universita di Modena e Reggio Emilia; University of Udine
RP Cornia, M (corresponding author), Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, Via P Vivarelli 10, I-41125 Modena, Italy.
EM marcella.cornia@unimore.it; lorenzo.baraldi@unimore.it;
   giuseppe.serra@uniud.it; rita.cucchiara@unimore.it
RI Cucchiara, Rita/L-3006-2015; Cornia, Marcella/Y-9903-2019; Serra,
   Giuseppe/M-3572-2015
OI Cornia, Marcella/0000-0001-9640-9385; Baraldi,
   Lorenzo/0000-0001-5125-4957
FU "Citta educante" of the National Technological Cluster on Smart
   Communities (Italian Ministry of Education, University and Research -
   MIUR) [CTN01_00034_393801]; project "JUMP - Una piattaforma sensoristica
   avanzata per rinnovare la pratica e la fruizione dello sport, del
   benessere, della riabilitazione e del gioco educativo" - Emilia-Romagna
   region within the POR-FESR program; CINECA award under the ISCRA
   initiative; NVIDIA Corporation
FX This work is partially supported by "Citta educante"
   (CTN01_00034_393801) of the National Technological Cluster on Smart
   Communities (cofunded by the Italian Ministry of Education, University
   and Research - MIUR) and by the project "JUMP - Una piattaforma
   sensoristica avanzata per rinnovare la pratica e la fruizione dello
   sport, del benessere, della riabilitazione e del gioco educativo,"
   funded by the Emilia-Romagna region within the POR-FESR 2014-2020
   program. We acknowledge the CINECA award under the ISCRA initiative for
   the availability of high-performance computing resources and support. We
   also gratefully acknowledge the support of NVIDIA Corporation with the
   donation of the GPUs used for this research.
CR [Anonymous], 2015, 2015 IEEE INT C COMP
   [Anonymous], EUR C COMP VIS
   [Anonymous], ARXIV161001708
   [Anonymous], 2017, MIT SALIENCY BENCHMA
   [Anonymous], ACL WORKSH TEXT SUMM
   [Anonymous], ARXIV170310476
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], NAACL HLT WORKSH
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], 2016, ARXIV161001563
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], ARXIV161109571
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], 2016, EUR C COMP VIS
   [Anonymous], 2005, P ACL WORKSH INTR EX
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2015, ICLR
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], 2002, 40 ANN M ASS COMP LI
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], C EMP METH NAT LANG
   [Anonymous], 2017, NEUROCOMPUTING, DOI DOI 10.1016/j.neucom.2017.03.018
   [Anonymous], 2013, INT C MACHINE LEARNI
   [Anonymous], 2015, ICML
   [Anonymous], 2013, IEEE INT C COMP VIS
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], 2015, IEEE INT C COMP VIS
   [Anonymous], PROC
   [Anonymous], IEEE INT C COMP VIS
   [Anonymous], 2010, EUR C COMP VIS
   Bahdanau D., 2015, 3 INT C LEARN REPR I
   Borji A., 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247706
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Griffin ZM, 2000, PSYCHOL SCI, V11, P274, DOI 10.1111/1467-9280.00255
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   JIANG M, 2015, PROC CVPR IEEE, P1072, DOI DOI 10.1109/CVPR.2015.7298710
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kingma D.P., 2014, ARXIV14126980
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kruthiventi S. S. S., 2016, IEEE C COMP VIS PATT
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, Very Deep Convolutional Networks for Large-Scale Image Recognition
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   SUGANO Y, 2016, ARXIV160805203
   Tavakoli HR, 2017, IEEE I CONF COMP VIS, P2506, DOI 10.1109/ICCV.2017.272
   Vedantam R, 2015, IEEE I CONF COMP VIS, P2542, DOI 10.1109/ICCV.2015.292
   Yao BZ, 2010, P IEEE, V98, P1485, DOI 10.1109/JPROC.2010.2050411
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
NR 61
TC 73
Z9 78
U1 0
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
AR 48
DI 10.1145/3177745
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GG9BS
UT WOS:000432996000005
DA 2024-07-18
ER

PT J
AU Siekkinen, M
   Kämäräinen, T
   Favario, L
   Masala, E
AF Siekkinen, Matti
   Kamarainen, Teemu
   Favario, Leonardo
   Masala, Enrico
TI Can You See What I See? Quality-of-Experience Measurements of Mobile
   Live Video Broadcasting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE QoE; Facebook Live; Periscope; mobile video streaming; adaptive
   streaming; DASH; live video; video quality; latency
ID STREAMS
AB Broadcasting live video directly from mobile devices is rapidly gaining popularity with applications like Periscope and Facebook Live. The quality of experience (QoE) provided by these services comprises many factors, such as quality of transmitted video, video playback stalling, end-to-end latency, and impact on battery life, and they are not yet well understood. In this article, we examine mainly the Periscope service through a comprehensive measurement study and compare it in some aspects to Facebook Live. We shed light on the usage of Periscope through analysis of crawled data and then investigate the aforementioned QoE factors through statistical analyses as well as controlled small-scale measurements using a couple of different smartphones and both versions, Android and iOS, of the two applications. We report a number of findings including the discrepancy in latency between the two most commonly used protocols, RTMP and HLS, surprising surges in bandwidth demand caused by the Periscope app's chat feature, substantial variations in video quality, poor adaptation of video bitrate to available upstream bandwidth at the video broadcaster side, and significant power consumption caused by the applications.
C1 [Siekkinen, Matti; Kamarainen, Teemu] Aalto Univ, Dept Comp Sci, Sch Sci, Helsinki, Finland.
   [Favario, Leonardo; Masala, Enrico] Politecn Torino, Control & Comp Engn Dept, Turin, Italy.
C3 Aalto University; Polytechnic University of Turin
RP Siekkinen, M (corresponding author), Aalto Univ, Dept Comp Sci, Sch Sci, Helsinki, Finland.
EM matti.siekkinen@aalto.fi; teemu.kamarainen@aalto.fi;
   leonardo.favario@polito.it; enrico.masala@polito.it
RI Favario, Leonardo/HJP-0433-2023; Masala, Enrico/B-6973-2008; Kämäräinen,
   Teemu/GLU-3738-2022; Siekkinen, Matti/H-2447-2018
OI Masala, Enrico/0000-0001-8906-354X; Kamarainen,
   Teemu/0000-0003-4685-6763; Siekkinen, Matti/0000-0003-0423-1060
FU Academy of Finland [278207, 297892]; Tekes - the Finnish Funding Agency
   for Innovation; Academy of Finland (AKA) [297892] Funding Source:
   Academy of Finland (AKA)
FX The work is supported by the Academy of Finland under Grant No.: 278207
   and 297892 and Tekes - the Finnish Funding Agency for Innovation.
CR Adhikari VK, 2015, IEEE ACM T NETWORK, V23, P1984, DOI 10.1109/TNET.2014.2354262
   [Anonymous], 2003, ITU T RECOMMENDATION
   [Anonymous], 2012, P 3 MULT SYST C MMS
   [Anonymous], 2017, 23009520952017 ISOIE
   [Anonymous], 2010, P 1 ANN ACM C MULT S
   [Anonymous], 2007, 1381812007 ISOIEC
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Brandao T., 2008, 16 EUR SIGN PROC C E, P1
   Chen ZZ, 2007, SIGNAL PROCESS-IMAGE, V22, P19, DOI 10.1016/j.image.2006.11.002
   Deng J, 2017, LECT NOTES COMPUT SC, V10176, P60, DOI 10.1007/978-3-319-54328-4_5
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Favario L., 2016, Multimedia Signal Processing, P1
   Haimson OL, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P48, DOI 10.1145/3025453.3025642
   He QY, 2016, IEEE T MULTIMEDIA, V18, P916, DOI 10.1109/TMM.2016.2544698
   Hsu CH, 2011, IEEE T MOBILE COMPUT, V10, P406, DOI 10.1109/TMC.2010.173
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   ISO/IEC, 2005, 1449632005 ISOIEC
   Kang K, 2007, IEEE T VEH TECHNOL, V56, P2655, DOI 10.1109/TVT.2007.899943
   Krishnan R. K., 2012, P INT MEAS C, P211
   Larumbe F., 2015, Under the Hood: Broadcasting Live Video to Millions. goo.gl/qpBAJM
   Li ZG, 2012, PLANT SCI, V185, P185, DOI 10.1016/j.plantsci.2011.10.006
   Pires K., 2015, P 6 ACM MULT SYST C, P225, DOI DOI 10.1145/2713168.2713195
   Rajaraman SV, 2014, 2014 IEEE 25TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATION (PIMRC), P2013, DOI 10.1109/PIMRC.2014.7136502
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Shafiq M. Zubair, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P367, DOI 10.1145/2591971.2591975
   Sharrab YousefO., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys '13, P60
   Siekkinen M., 2016, Proceedings of the 2016 Internet Measurement Conference, P477
   Siekkinen M, 2017, IEEE T MOBILE COMPUT, V16, P1059, DOI 10.1109/TMC.2016.2585138
   Stohr D, 2015, 2015 IEEE 40TH LOCAL COMPUTER NETWORKS CONFERENCE WORKSHOPS (LCN WORKSHOPS), P673, DOI 10.1109/LCNW.2015.7365913
   Stokke KR, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P145, DOI 10.1145/2910017.2910591
   Sun L., 2013, GUIDE VOICE VIDEO IP, V151
   Tang JC, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P4770, DOI 10.1145/2858036.2858374
   Tarkoma S, 2014, SMARTPHONE ENERGY CONSUMPTION: MODELING AND OPTIMIZATION, P1, DOI 10.1017/CBO9781107326279
   Wang Bolun., 2016, Proceedings of the 2016 ACM on Internet Measurement Conference, P485
   Wei Sheng., 2014, NETWORK OPERATING SY, P37
   Wilk S, 2015, IEEE INT SYM MULTIM, P403, DOI 10.1109/ISM.2015.110
   Wu DP, 2001, P IEEE, V89, P6, DOI 10.1109/5.904503
   Zhang C., 2015, ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, P55, DOI DOI 10.1145/2736084.2736091
   Zheng Y., 2016, IEEE T CIRCUITS SYST, V99, P1
   Zhou L, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2886776
NR 40
TC 10
Z9 11
U1 1
U2 32
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 34
DI 10.1145/3165279
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TD
UT WOS:000434634700007
DA 2024-07-18
ER

PT J
AU de Boer, MHT
   Lu, YJ
   Zhang, H
   Schutte, K
   Ngo, CW
   Kraaij, W
AF de Boer, Maaike H. T.
   Lu, Yi-Jie
   Zhang, Hao
   Schutte, Klamer
   Ngo, Chong-Wah
   Kraaij, Wessel
TI Semantic Reasoning in Zero Example Video Event Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Content-based visual information retrieval; multimedia event detection;
   zero shot; semantics
AB Searching in digital video data for high-level events, such as a parade or a car accident, is challenging when the query is textual and lacks visual example images or videos. Current research in deep neural networks is highly beneficial for the retrieval of high-level events using visual examples, but without examples it is still hard to (1) determine which concepts are useful to pre-train (Vocabulary challenge) and (2) which pre-trained concept detectors are relevant for a certain unseen high-level event (Concept Selection challenge). In our article, we present our Semantic Event Retrieval Systemwhich (1) shows the importance of high-level concepts in a vocabulary for the retrieval of complex and generic high-level events and (2) uses a novel concept selection method (i-w2v) based on semantic embeddings. Our experiments on the international TRECVID Multimedia Event Detection benchmark show that a diverse vocabulary including high-level concepts improves performance on the retrieval of high-level events in videos and that our novel method outperforms a knowledge-based concept selection method.
C1 [de Boer, Maaike H. T.] TNO, Intelligent Imaging Dept, Oude Waalsdorperweg 63, NL-2597 AK The Hague, Netherlands.
   [de Boer, Maaike H. T.] Radboud Univ Nijmegen, Data Sci Dept, Fac Sci, Toernooiveld 212, NL-6525 EC Nijmegen, Netherlands.
   [Lu, Yi-Jie; Zhang, Hao; Ngo, Chong-Wah] City Univ Hong Kong, 83 Tat Chee Ave, Kowloon, Hong Kong, Peoples R China.
   [Schutte, Klamer] TNO, Oude Waalsdorperweg 63, NL-2597 AK The Hague, Netherlands.
   [Kraaij, Wessel] TNO, Anna Van Buerenpl 1 & Niels Bohrweg 1, NL-2595 DA The Hague, Netherlands.
   [Kraaij, Wessel] Leiden Univ, NL-2333 CA Leiden, Netherlands.
C3 Netherlands Organization Applied Science Research; Radboud University
   Nijmegen; City University of Hong Kong; Netherlands Organization Applied
   Science Research; Netherlands Organization Applied Science Research;
   Leiden University - Excl LUMC; Leiden University
RP de Boer, MHT (corresponding author), TNO, Intelligent Imaging Dept, Oude Waalsdorperweg 63, NL-2597 AK The Hague, Netherlands.; de Boer, MHT (corresponding author), Radboud Univ Nijmegen, Data Sci Dept, Fac Sci, Toernooiveld 212, NL-6525 EC Nijmegen, Netherlands.
EM maaike.deboer@tno.nl; Klamer.schutte@tno.nl; cscwngo@cityu.edu.hk;
   w.kraaij@liacs.leidenuniv.nl
RI Zhang, Hao/ABF-5434-2021; Kraaij, Wessel/S-2071-2016
OI Kraaij, Wessel/0000-0001-7797-619X
FU technology program Making Sense of Big Data (MSoBD); Research Grants
   Council of the Hong Kong Special Administrative Region, China [CityU
   120213]
FX We thank the technology program Making Sense of Big Data (MSoBD) for
   their financial support. The work described in this article was
   supported in part by a grant from the Research Grants Council of the
   Hong Kong Special Administrative Region, China (CityU 120213).
CR Aly R, 2012, MULTIMED TOOLS APPL, V60, P203, DOI 10.1007/s11042-011-0818-x
   [Anonymous], P ANN TREC VID RETR
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P INT C MULT RETR
   [Anonymous], P 3 INT C MULT RETR
   [Anonymous], 2017, IEEE Transactions on Pattern Analysis and Machine Intelligence
   [Anonymous], 2015, MULTIMEDIA TOOLS APP
   [Anonymous], 2015, ARXIV150301817
   [Anonymous], J APPL COMPUTER SCI
   [Anonymous], P 29 AAAI C ART INT
   [Anonymous], P ANN TREC RETR EV T
   Ballan L, 2011, MULTIMED TOOLS APPL, V51, P279, DOI 10.1007/s11042-010-0643-7
   Carpineto C, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2071389.2071390
   Chang XJ, 2016, AAAI CONF ARTIF INTE, P3464
   Chang XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2234
   Chen Jinhu, 2014, Int J Endocrinol, V2014, P203930, DOI 10.1155/2014/203930
   Dalton J, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1857, DOI 10.1145/2505515.2507880
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Habibian A, 2013, P 3 ACM C INT C MULT, P89, DOI DOI 10.1145/2461466.2461482
   Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Hauptmann Alexander., 2007, CIVR 07, P627
   Huurnink B., 2008, P ACM INT C MULTIMED, P459
   Jain M, 2015, IEEE I CONF COMP VIS, P4588, DOI 10.1109/ICCV.2015.521
   Jiang L, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P547, DOI 10.1145/2647868.2654918
   Jiang L, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P27, DOI 10.1145/2671188.2749399
   Jiang Yu-Gang., 2012, IJMIR, P1
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kennedy L., 2006, "LSCOM Lexicon Definitions and Annotations (Version 1.0)"
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Levy O., 2015, Transactions of the Association for Computational Linguistics, V3, P211
   Levy O, 2014, ADV NEUR IN, V27
   Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045
   Lu YJ, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P127, DOI 10.1145/2911996.2912015
   Mensink T, 2014, PROC CVPR IEEE, P2441, DOI 10.1109/CVPR.2014.313
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Milne D, 2013, ARTIF INTELL, V194, P222, DOI 10.1016/j.artint.2012.06.007
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   Neo SY, 2006, LECT NOTES COMPUT SC, V4071, P143
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Spagnola S., 2011, Proc. 9th Int. Conf. Computational Semantics, P385
   Tzelepis C, 2016, IMAGE VISION COMPUT, V53, P35, DOI 10.1016/j.imavis.2015.09.005
   Wu S, 2014, PROC CVPR IEEE, P2665, DOI 10.1109/CVPR.2014.341
   Xu SC, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P675, DOI 10.1145/2671188.2749413
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
   Yu SI, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P825, DOI 10.1145/2647868.2654997
   Zhou BL, 2014, ADV NEUR IN, V27
NR 49
TC 12
Z9 12
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 60
DI 10.1145/3131288
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300014
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Gokhale, V
   Nair, J
   Chaudhuri, S
AF Gokhale, Vineet
   Nair, Jayakrishnan
   Chaudhuri, Subhasis
TI Congestion Control for Network-Aware Telehaptic Communication
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Telehaptic communication; congestion control; transport layer; dynamic
   rate adaptation; QoS; multimedia
AB Telehaptic applications involve delay-sensitive multimedia communication between remote locations with distinct Quality of Service (QoS) requirements for different media components. These QoS constraints pose a variety of challenges, especially when the communication occurs over a shared network, with unknown and time-varying cross-traffic. In this work, we propose a transport layer congestion control protocol for telehaptic applications operating over shared networks, termed as Dynamic Packetization Module (DPM). DPM is a lossless, network-aware protocol that tunes the telehaptic packetization rate based on the level of congestion in the network. To monitor the network congestion, we devise a novel network feedback module, which communicates the end-to-end delays encountered by the telehaptic packets to the respective transmitters with negligible overhead. Via extensive simulations, we show that DPM meets the QoS requirements of telehaptic applications over a wide range of network cross-traffic conditions. We also report qualitative results of a real-time telepottery experiment with several human subjects, which reveal that DPM preserves the quality of telehaptic activity even under heavily congested network scenarios. Finally, we compare the performance of DPM with several previously proposed telehaptic communication protocols and demonstrate that DPM outperforms these protocols.
C1 [Gokhale, Vineet; Nair, Jayakrishnan; Chaudhuri, Subhasis] Indian Inst Technol, Dept Elect Engn, Bombay 400076, Maharashtra, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Bombay
RP Gokhale, V (corresponding author), Indian Inst Technol, Dept Elect Engn, Bombay 400076, Maharashtra, India.
EM vineet@ee.iitb.ac.in; jayakrishnan.nair@ee.iitb.ac.in; sc@ee.iitb.ac.in
CR Al Osman Hussein, 2007, P INT WORKSH HAPT AU
   ANDERSON RJ, 1989, IEEE T AUTOMAT CONTR, V34, P494, DOI 10.1109/9.24201
   Chaudhury Subhajit, 2014, P HAPT S
   CHIU DM, 1989, COMPUT NETWORKS ISDN, V17, P1, DOI 10.1016/0169-7552(89)90019-6
   Cizmeci B, 2014, LECT NOTES COMPUT SC, V8619, P131, DOI 10.1007/978-3-662-44196-1_17
   Clarke Stella, 2006, P INT WORKSH HAPT AU
   Dabeer O, 2011, IEEE T SIGNAL PROCES, V59, P1868, DOI 10.1109/TSP.2010.2101071
   Eid M, 2011, IEEE T INSTRUM MEAS, V60, P21, DOI 10.1109/TIM.2010.2065530
   FERRELL WR, 1965, IEEE TRANS HUM FACT, VHFE6, P24, DOI 10.1109/THFE.1965.6591253
   Fujimoto Masaki, 2005, P 4 ACM SIGCOMM WORK
   Fujimoto Takeshi, 2008, P S HAPT INT VIRT EN
   Gokhale Vineet, 2013, P INT S HAPT AUD VIS
   Gokhale Vineet, 2015, P 21 NAT C COMM NCC
   Gokhale Vineet, 2016, P HAPT S
   Hinterseer P, 2008, IEEE T SIGNAL PROCES, V56, P588, DOI 10.1109/TSP.2007.906746
   Hinterseer Peter, 2005, P INT C AC SPEECH SI
   Hoshino Sosuke, 2011, P INT WORKSH TECHN C
   Isomura Eiichi, 2013, P CONS COMM NETW C C
   Jay C, 2007, ACM T COMPUT-HUM INT, V14, DOI 10.1145/1275511.1275514
   Kokkonis G, 2015, GENETIC EVOLUTIONARY, V388, P93
   LAWRENCE DA, 1993, IEEE T ROBOTIC AUTOM, V9, P624, DOI 10.1109/70.258054
   Lee Seokhee, 2007, P GLOB TEL C
   Marshall A, 2008, ADV MULTIMED, V2008, DOI 10.1155/2008/841590
   MILLS DL, 1991, IEEE T COMMUN, V39, P1482, DOI 10.1109/26.103043
   Miras Dimitrios., 2002, SURVEY NETWORK QOS N
   Montgomery DC, 2007, INTRO STAT QUALITY C
   ns3, 2011, NETWORK SIMULATOR
   Postel J., 1980, TECHNICAL REPORT
   Rizzo Luigi., 1997, ACM COMPUTER COMMUNI, V27, P31, DOI DOI 10.1145/251007.251012
   Sakr N, 2011, IEEE T INSTRUM MEAS, V60, P3534, DOI 10.1109/TIM.2011.2161144
   Sangtae Ha, 2008, Operating Systems Review, V42, P64, DOI 10.1145/1400097.1400105
   Schulzrinne H., 2003, RTP TRANSPORT PROTOC
   Steinbach E, 2012, P IEEE, V100, P937, DOI 10.1109/JPROC.2011.2182100
   Suzuki Nobuhiro, 2013, P INT C MECH ICM
   Szigeti T., 2004, QUALITY SERVICE DESI
   Tos Uras, 2011, P COMP SOFTW APPL C
   Wirz R, 2008, LECT NOTES COMPUT SC, V5024, P3, DOI 10.1007/978-3-540-69057-3_1
NR 37
TC 16
Z9 16
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2017
VL 13
IS 2
AR 17
DI 10.1145/3052821
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EV1VK
UT WOS:000401537300005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hsu, CF
   Fan, CL
   Tsai, TH
   Huang, CY
   Hsu, CH
   Chen, KT
AF Hsu, Chih-Fan
   Fan, Ching-Ling
   Tsai, Tsung-Han
   Huang, Chun-Ying
   Hsu, Cheng-Hsin
   Chen, Kuan-Ta
TI Toward an Adaptive Screencast Platform: Measurement and Optimization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 6th ACM International Conference on Multimedia Systems (MMSys)
   Co-Located with 25th ACM Workshop on Network and Operating Systems
   Support for Digital Audio and Video (NOSSDAV)
CY MAR 18-20, 2015
CL Portland, OR
SP ACM
DE Design; Measurement; Live video streaming; real-time encoding;
   performance evaluation; performance optimization
AB The binding between computing devices and displays is becoming dynamic and adaptive, and screencast technologies enable such binding over wireless networks. In this article, we design and conduct the first detailed measurement study on the performance of the state-of-the-art screencast technologies. Several commercial and one open-source screencast technologies are considered in our detailed analysis, which leads to several insights: (1) there is no single winning screencast technology, indicating room to further enhance the screencast technologies; (2) hardware video encoders significantly reduce the CPU usage at the expense of slightly higher GPU usage and end-to-end delay, and should be adopted in future screencast technologies; (3) comprehensive error resilience tools are needed as wireless communication is vulnerable to packet loss; (4) emerging video codecs designed for screen contents lead to a better Quality of Experience (QoE) of screencast; and (5) rate adaptation mechanisms are critical to avoiding degraded QoE due to network dynamics. As a case study, we propose a nonintrusive yet accurate available bandwidth estimation mechanism. Real experiments demonstrate the practicality and efficiency of our proposed solution. Our measurement methodology, open-source screencast platform, and case study allow researchers and developers to quantitatively evaluate other design considerations, which will lead to optimized screencast technologies.
C1 [Hsu, Chih-Fan; Tsai, Tsung-Han; Chen, Kuan-Ta] Acad Sinica, 128 Acad Rd,Sect 2, Taipei 11574, Taiwan.
   [Fan, Ching-Ling; Hsu, Cheng-Hsin] Natl Tsing Hua Univ, 101,Sect 2,Kuang Fu Rd, Hsinchu 30013, Taiwan.
   [Huang, Chun-Ying] Natl Chiao Tung Univ, 1001 Univ Rd, Hsinchu 30010, Taiwan.
C3 Academia Sinica - Taiwan; National Tsing Hua University; National Yang
   Ming Chiao Tung University
RP Hsu, CF (corresponding author), Acad Sinica, 128 Acad Rd,Sect 2, Taipei 11574, Taiwan.
EM hsuchihfan@gmail.com; yyytr7180@hotmail.com; zark912@iis.sinica.edu.tw;
   chuang@cs.nctu.edu.tw; chsu@cs.nthu.edu.tw; swc@iis.sinica.edu.tw
OI Hsu, Chih-Fan/0000-0002-4180-8255; Huang, Chun-Ying/0000-0001-5503-9541
FU Ministry of Science and Technology of Taiwan [103-2221-E-001-023-MY2,
   102-2221-E-007-062-MY3, 103-2221-E-019-033-MY2]
FX This work was supported in part by the Ministry of Science and
   Technology of Taiwan under grants 103-2221-E-001-023-MY2,
   102-2221-E-007-062-MY3, and 103-2221-E-019-033-MY2.
CR AirPlay, 2014, AIRPLAY PLAY CONT IO
   [Anonymous], 2003, IEEE T CIRC SYST VID, DOI DOI 10.1109/TCSVT.2003
   [Anonymous], IEEE C VIS COMM IM P
   Baratto RicardoA., 2005, Proceedings of the 20th ACM Symposium on Operating Systems Principles, SOSP '05, P277, DOI DOI 10.1145/1095810.1095837
   Calagari K, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2656203
   Chandra S., 2012, P 20 ACM INT C MULT, P389
   Chandra S, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2534328
   Chang Yu-Chun., 2011, IEEE INT WORKSHOP TE, P1, DOI [10.1109/CQR.2011.5996092, DOI 10.1109/ICME.2011.6012177]
   Chen KT, 2014, IEEE T MULTIMEDIA, V16, P480, DOI 10.1109/TMM.2013.2291532
   Claypool M., 2012, P ACM WORKSH NETW SY, P22
   Cumberland B C., 1999, Microsoft Windows NT Server f.0
   Farshad A, 2014, IEEE INT CONF SENS, P108, DOI 10.1109/SAHCN.2014.6990333
   GamingAnywhere Web Page, 2013, GAMINGANYWHERE OP SO
   He Y., 2014, P IM MULT AN WEB MOB, P9027
   HEVC Test Model, 2014, HEVC TEST MOD HM DOC
   Hong HJ, 2015, IEEE T CIRC SYST VID, V25, P2078, DOI 10.1109/TCSVT.2015.2450173
   Hsu Chih-Fan, 2015, P ACM SIGMM C MULT S, P177
   Huang CY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P351, DOI 10.1145/2733373.2806261
   Huang CY, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2537855
   Intel Web Page, 2015, INT MED CLIENT SOL W
   Javadtalab A, 2015, IEEE T INSTRUM MEAS, V64, P190, DOI 10.1109/TIM.2014.2331423
   Kapoor R, 2004, ACM SIGCOMM COMP COM, V34, P67, DOI 10.1145/1030194.1015476
   KLEINROCK L, 1992, IEEE COMMUN MAG, V30, P36, DOI 10.1109/35.135787
   Lagar-Cavilla HA, 2007, LECT NOTES COMPUT SC, V4834, P143
   Lai AM, 2006, ACM T COMPUT SYST, V24, P175, DOI 10.1145/1132026.1132029
   Li M, 2008, PROCEEDINGS OF THE 2008 INTERNATIONAL WORKSHOP ON METAMATERIALS, P374, DOI 10.1109/META.2008.4723618
   Markets, 2012, GLOB MARK WI FI WLAN
   Markets, 2014, FLEX DISPL MARK WORT
   Miracast, 2014, WI FI CERT MIR
   open-airplay, 2014, OP AIRPL COLL LIB CO
   Ribeiro V. J., 2003, PROC PASSIVE ACTIVE, P1
   Richardson T, 1998, IEEE INTERNET COMPUT, V2, P33, DOI 10.1109/4236.656066
   Roskind J., 2012, TECHNICAL REPORT
   Schmidt BK, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P32, DOI 10.1145/319344.319154
   Tolia N, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.101
   Tursunova S, 2010, IEEE IFIP NETW OPER, P448, DOI 10.1109/NOMS.2010.5488496
   Wang Y., 2001, VIDEO PROCESSING COM
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu CC, 2013, IEEE T MULTIMEDIA, V15, P1121, DOI 10.1109/TMM.2013.2241043
   Xu KX, 2003, IEEE MILIT COMMUN C, P1018
   Yang SJ, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P131
   Zhang Chi., 2015, Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services, P405
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
NR 43
TC 4
Z9 4
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2016
VL 12
IS 5
SU S
AR 79
DI 10.1145/2886778
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EJ0VT
UT WOS:000392929700010
DA 2024-07-18
ER

PT J
AU Papapanagiotou, V
   Diou, C
   Delopoulos, A
AF Papapanagiotou, Vasileios
   Diou, Christos
   Delopoulos, Anastasios
TI Improving Concept-Based Image Retrieval with Training Weights Computed
   from Tags
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Concept-based image retrieval; fuzzy support vector
   machine; weighted training sample
ID SUPPORT VECTOR MACHINES; CLASSIFICATION
AB This article presents a novel approach to training classifiers for concept detection using tags and a variant of Support Vector Machine that enables the usage of training weights per sample. Combined with an appropriate tag weighting mechanism, more relevant samples play a more important role in the calibration of the final concept-detector model. We propose a complete, automated framework that (i) calculates relevance scores for each image-concept pair based on image tags, (ii) transforms the scores into relevance probabilities and automatically annotates each image according to this probability, (iii) transforms either the relevance scores or the probabilities into appropriate training weights and finally, (iv) incorporates the training weights and the visual features into a Fuzzy Support Vector Machine classifier to build the concept-detector model. The framework can be applied to online public collections, by gathering a large pool of diverse images, and using the calculated probability to select a training set and the associated training weights. To evaluate our argument, we experiment on two large annotated datasets. Experiments highlight the retrieval effectiveness of the proposed approach. Furthermore, experiments with various levels of annotation error show that using weights derived from tags significantly increases the robustness of the resulting concept detectors.
C1 [Papapanagiotou, Vasileios; Diou, Christos; Delopoulos, Anastasios] Aristotle Univ Thessaloniki, Elect & Comp Engn Dept, GR-54124 Thessaloniki, Greece.
C3 Aristotle University of Thessaloniki
RP Papapanagiotou, V; Diou, C; Delopoulos, A (corresponding author), Aristotle Univ Thessaloniki, Elect & Comp Engn Dept, GR-54124 Thessaloniki, Greece.
EM vassilis@mug.ee.auth.gr; diou@mug.ee.auth.gr; adelo@eng.auth.gr
RI Delopoulos, Anastasios/ABB-6127-2021; Delopoulos,
   Anastasios/B-2140-2013; Papapanagiotou, Vasileios/AAG-7218-2019; Diou,
   Christos/AAM-5533-2021
OI Papapanagiotou, Vasileios/0000-0001-6834-5548; Diou,
   Christos/0000-0002-2461-1928
CR [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], 2010, PROC ACM SIGMM INT C
   [Anonymous], 2007, CIVR '07
   Arampatzis A., 2001, SIGIR Forum, P285
   Arampatzis A, 2011, INFORM RETRIEVAL, V14, P26, DOI 10.1007/s10791-010-9145-5
   Arampatzis Avi., 2009, Proceedings of the 18th ACM conference on Information and knowledge management, CIKM '09, P797
   Bouma G., 2009, P GSCL POTSD GERM, P31
   Bovolo F, 2010, IEEE T IMAGE PROCESS, V19, P2983, DOI 10.1109/TIP.2010.2051632
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Datta R, 2005, P 7 ACM SIGMM INT WO, P153, DOI [DOI 10.1145/1101826.1101866, 10.1145/1101826.1101866]
   Diou C, 2010, IEEE T CIRC SYST VID, V20, P1808, DOI 10.1109/TCSVT.2010.2087814
   Ewerth R, 2012, IEEE T MULTIMEDIA, V14, P1008, DOI 10.1109/TMM.2012.2186956
   Habibian A, 2014, COMPUT VIS IMAGE UND, V124, P110, DOI 10.1016/j.cviu.2014.02.003
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Leng XM, 2008, PROCEEDINGS OF 2008 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P1260, DOI 10.1109/ICMLC.2008.4620598
   Li HX, 2013, INFORM SCIENCES, V221, P60, DOI 10.1016/j.ins.2012.09.041
   Lin CF, 2004, PATTERN RECOGN LETT, V25, P1647, DOI 10.1016/j.patrec.2004.06.009
   Lin CF, 2002, IEEE T NEURAL NETWOR, V13, P464, DOI 10.1109/72.991432
   Liu Y, 2007, IEEE T SIGNAL PROCES, V55, P3272, DOI 10.1109/TSP.2007.894403
   Mandel M., 2011, ACM T MULTIM COMPUT, V7S, P1
   Manmatha R., 2001, SIGIR Forum, P267
   Markov I, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1161, DOI 10.1145/2348283.2348519
   Min R, 2009, PATTERN RECOGN, V42, P147, DOI 10.1016/j.patcog.2008.07.001
   Nottelmann H, 2003, LECT NOTES COMPUT SC, V2633, P235
   Nowak S., 2010, P INT C MULT INF RET, P557, DOI [10.1145/1743384.1743478, DOI 10.1145/1743384.1743478]
   PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814
   Rafiee G., 2010, 2010 7th International Symposium on Communication Systems, Networks & Digital Signal Processing (CSNDSP 2010), P775
   Rao Y, 2006, LECT NOTES COMPUT SC, V4071, P350
   Redi M, 2012, LECT NOTES COMPUT SC, V7131, P300
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P963, DOI 10.1109/TMM.2011.2181344
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Sohail AM, 2011, LECT NOTES COMPUT SC, V6669, P176
   Tang JH, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2501643.2501651
   Tsikrika T, 2011, MULTIMED TOOLS APPL, V55, P27, DOI 10.1007/s11042-010-0584-1
   Tsikrika Theodora, 2009, P 8 ACM INT C IM VID
   Tsirelis T., 2011, 12 INT WORKSH IM AN
   ULGES A, 2009, P IEEE INT C DAT MIN, P190
   van de Sande K.E. A., 2008, CIVR, P141, DOI DOI 10.1145/1386352.1386376
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wang S, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1236471.1236473
   Wang YQ, 2005, IEEE T FUZZY SYST, V13, P820, DOI 10.1109/TFUZZ.2005.859320
   Wu K, 2008, STUD COMPUT INTELL, V96, P271
   Xian GM, 2010, EXPERT SYST APPL, V37, P6737, DOI 10.1016/j.eswa.2010.02.067
   Yang LJ, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240141
   Zhang J, 2009, IEEE T IMAGE PROCESS, V18, P2370, DOI 10.1109/TIP.2009.2026669
   Zhang L, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490823
   Zheng Sun, 2009, Proceedings of the 2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2009), P402, DOI 10.1109/FSKD.2009.35
   Zhu SA, 2012, IEEE T MULTIMEDIA, V14, P1068, DOI 10.1109/TMM.2012.2190387
NR 50
TC 4
Z9 5
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2016
VL 12
IS 2
AR 32
DI 10.1145/2790230
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PK
UT WOS:000373906200006
DA 2024-07-18
ER

PT J
AU Krishnappa, DK
   Zink, M
   Griwodz, C
   Halvorsen, P
AF Krishnappa, Dilip Kumar
   Zink, Michael
   Griwodz, Carsten
   Halvorsen, Pal
TI Cache-Centric Video Recommendation: An Approach to Improve the
   Efficiency of YouTube Caches
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Measurement; Performance; Caching; YouTube; recommendation
AB In this article, we take advantage of the user behavior of requesting videos from the top of the related list provided by YouTube to improve the performance of YouTube caches. We recommend that local caches reorder the related lists associated with YouTube videos, presenting the cached content above noncached content. We argue that the likelihood that viewers select content from the top of the related list is higher than selection from the bottom, and pushing contents already in the cache to the top of the related list would increase the likelihood of choosing cached content. To verify that the position on the list really is the selection criterion more dominant than the content itself, we conduct a user study with 40 YouTube-using volunteers who were presented with random related lists in their everyday YouTube use. After confirming our assumption, we analyze the benefits of our approach by an investigation that is based on two traces collected from a university campus. Our analysis shows that the proposed reordering approach for related lists would lead to a 2 to 5 times increase in cache hit rate compared to an approach without reordering the related list. This increase in hit rate would lead to reduction in server load and backend bandwidth usage, which in turn reduces the latency in streaming the video requested by the viewer and has the potential to improve the overall performance of YouTube's content distribution system. An analysis of YouTube's recommendation system reveals that related lists are created from a small pool of videos, which increases the potential for caching content from related lists and reordering based on the content in the cache.
C1 [Krishnappa, Dilip Kumar; Zink, Michael] Univ Massachusetts, Amherst, MA 01003 USA.
   [Griwodz, Carsten; Halvorsen, Pal] Univ Oslo, Simula Lab, N-0316 Oslo, Norway.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   University of Oslo
RP Krishnappa, DK (corresponding author), Univ Massachusetts, Amherst, MA 01003 USA.
EM dilip.manu@gmail.com
OI Halvorsen, Pal/0000-0003-2073-7029; Zink, Michael/0000-0002-0309-9240
CR Adhikari V.K., 2011, 2011 Proceedings of 20th International Conference on in Computer Communications and Networks (ICCCN), P1
   Adhikari VK, 2012, IEEE INFOCOM SER, P2521, DOI 10.1109/INFCOM.2012.6195644
   [Anonymous], 2013, P 7 ACM C REC SYST H
   [Anonymous], 2013, PROC ACM WORKSHOP NE
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chakareski J., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2429, DOI 10.1109/ICIP.2011.6116134
   Cheng X, 2009, IEEE INFOCOM SER, P1152, DOI 10.1109/INFCOM.2009.5062028
   Cheng XH, 2009, PROCEEDINGS OF THE THIRD INTERNATIONAL WORKSHOP ON MATRIX ANALYSIS AND APPPLICATIONS, VOL 1, P49, DOI 10.1145/1631144.1631156
   Conover WilliamJay., 1999, PRACTICAL NONPARAMET, VThird, P388
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   Donald E. K., 1998, The art of computer programming, sorting and searching, VIII
   FilterProxy, 2001, HTTP PROX
   Google, 2012, GOOGLETRANSPARENCYRE
   KendallCoefficient, 2014, KEND TAU RANK CORR C
   Khemmarat S., 2011, P 2 ANN ACM C MULT S, P187
   Krishnappa D.K., 2013, Proc. of ACM Multimedia Systems Conference. ACM, P261
   Muffin, 2012, WORLD WID WEB FILT S
   NetworkMonitor, 2014, END DAG NETW MON INT
   Opera, 2012, MOB BROWS
   Papadakis Andreas, 2013, ADV ELECT TELECOMMUN, V3, P5
   PlanetLab, 2007, PLANETLAB PORT
   Podlipnig S, 2003, ACM COMPUT SURV, V35, P374, DOI 10.1145/954339.954341
   TcpDump, 2010, NETW PACK AN
   WebCleaner, 2010, FILT HTTP PROX
   Yoo B, 2012, ELECTRON COMMER R A, V11, P180, DOI 10.1016/j.elerap.2011.12.007
   YouTube, 2007, YOUTUBEAPI
   YouTube, 2012, YOUTUBE KEYN MMSYS 2
   YouTube, 2014, YOUTUBE VID LOGG CHR
   YouTube, 2014, VIM VS YOUTUBE
   YouTube, 2012, YOUTUBE PRESS STAT
   Zhou R., 2011, Databases and Social Networks, P13, DOI [10.1145/1996413.1996416, DOI 10.1145/1996413.1996416]
   Zhou Renjie, 2010, P 10 ACM SIGCOMM C I, P404, DOI DOI 10.1145/1879141.1879193
   Zink M, 2009, COMPUT NETW, V53, P501, DOI 10.1016/j.comnet.2008.09.022
NR 33
TC 56
Z9 57
U1 1
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2015
VL 11
IS 4
AR 48
DI 10.1145/2716310
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CK0ZF
UT WOS:000355933700001
DA 2024-07-18
ER

PT J
AU Lv, Z
   Halawani, A
   Feng, SZ
   Li, HB
   Réhman, SUR
AF Lv, Zhihan
   Halawani, Alaa
   Feng, Shengzhong
   Li, Haibo
   Rehman, Shafiq U. R.
TI Multimodal Hand and Foot Gesture Interaction for Handheld Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Multimodal interaction; smartphone games; gesture
   estimation; HCI; mobile; vibrotactile
ID POSE ESTIMATION; MOBILE
AB We present a hand-and-foot-based multimodal interaction approach for handheld devices. Our method combines input modalities (i.e., hand and foot) and provides a coordinated output to both modalities along with audio and video. Human foot gesture is detected and tracked using contour-based template detection (CTD) and Tracking-Learning-Detection (TLD) algorithm. 3D foot pose is estimated from passive homography matrix of the camera. 3D stereoscopic and vibrotactile are used to enhance the immersive feeling. We developed a multimodal football game based on the multimodal approach as a proof-of-concept. We confirm our systems user satisfaction through a user study.
C1 [Lv, Zhihan; Feng, Shengzhong] Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing 100864, Peoples R China.
   [Halawani, Alaa; Rehman, Shafiq U. R.] Umea Univ, Inst Tillampad Fys Elek, S-90187 Umea, Sweden.
   [Li, Haibo] Royal Inst Technol KTH, Sch Comp Sci & Commun, S-10044 Stockholm, Sweden.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Umea University; Royal Institute of Technology
RP Lv, Z (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing 100864, Peoples R China.
EM lvzhihan@gmail.com; shafiq.urrehman@umu.se
RI Lyu, Zhihan/I-3187-2014; Lv, Zhihan/GLR-6000-2022
OI Lyu, Zhihan/0000-0003-2525-3074; Lv, Zhihan/0000-0003-2525-3074
CR Alam KM, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2501643.2501649
   Alexander J., 2012, CHI 12 P SIGCHI C HU, P1229, DOI DOI 10.1145/2207676.2208575
   Amft O, 2009, IEEE PERVAS COMPUT, V8, P8, DOI 10.1109/MPRV.2009.44
   Amma C., 2010, Proceedings of the 1st Augmented Human International Conference, page, P10
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], 2013, PROC INT C INDOOR PO
   Augsten Thomas, 2010, P 23 ANN ACM S US IN
   Bailly Gilles, 2012, P SIGCHI C HUMAN FAC, P1239
   Butler A, 2008, UIST 2008: PROCEEDINGS OF THE 21ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P201, DOI 10.1145/1449715.1449746
   Cohen Michael, 2008, P 7 ACM SIGGRAPH INT
   Crossan A., 2010, P 24 BCS INTERACTION, P418, DOI DOI 10.14236/EWIC/HCI2010
   Felzenszwalb PF, 2011, IEEE T PATTERN ANAL, V33, P721, DOI 10.1109/TPAMI.2010.135
   Grauman K, 2001, PROC CVPR IEEE, P1010
   Guan W, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2348816.2348826
   Guan W, 2009, INT SYM MIX AUGMENT, P191, DOI 10.1109/ISMAR.2009.5336470
   Hagbi N, 2011, IEEE T VIS COMPUT GR, V17, P1369, DOI 10.1109/TVCG.2010.241
   Halawani A, 2012, INTEL SERV ROBOT, V5, P89, DOI 10.1007/s11370-011-0098-3
   Halawani Alaa, 2013, P AUSTR COMP HUM INT
   Han T., 2011, P 13 INT C HUMAN COM, P29, DOI [DOI 10.1145/2037373, 10.1145/2037373.2037379, DOI 10.1145/2037373.2037379]
   HARALICK RM, 1989, IEEE T SYST MAN CYB, V19, P1426, DOI 10.1109/21.44063
   Harrison C, 2009, UIST 2009: PROCEEDINGS OF THE 22ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P121
   Jaimes A, 2007, COMPUT VIS IMAGE UND, V108, P116, DOI 10.1016/j.cviu.2006.10.019
   Ji Q, 2000, ISPRS J PHOTOGRAMM, V55, P75, DOI 10.1016/S0924-2716(00)00009-5
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Klein George, 2007, P1
   Kondapalli Ravi, 2011, P 7 INT WORKSH NETW
   Lu Z., 2013, Proceedings of the 21st ACM international conference on Multimedia, P621, DOI DOI 10.1145/2502081.2502163
   Lu Zhihan, 2013, P SIGGRAPH AS POST S
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Luo ZH, 2013, 2013 INTERNATIONAL CONFERENCE ON VIRTUAL REALITY AND VISUALIZATION (ICVRV 2013), P305, DOI 10.1109/ICVRV.2013.59
   Lv Z., 2014, CHI'14 Extended Abstracts on Human Factors in Computing Systems, P293, DOI DOI 10.1145/2559206.2580096
   Lv Z., 2013, Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia, P16
   Lv Zhihan, 2013, P IEEE INT C COMP VI
   Lv Zhihan, 2013, P SIGGRAPH AS S MOB
   Mistry P., 2009, SIGGRAPH ASIA Art Gallery Emerging Technologies, page, P85, DOI [10.1145/1667146.1667160, DOI 10.1145/1667146.1667160]
   Mistry P., 2009, P CHI 09 EXTENDED AB, P4111, DOI [DOI 10.1145/1520340.1520626, 10.1145/1520340.1520626]
   Paelke V., 2004, ACM SIGCHI INT C ADV, P321
   Ramalingam S, 2006, COMPUT VIS IMAGE UND, V103, P218, DOI 10.1016/j.cviu.2006.06.006
   Rehman S., 2012, P ACM INT WORKSH MOB
   Sangsuriyachot Nuttapol, 2012, P 10 AS PAC C COMP H
   Scott J., 2010, P 23 ANN ACM S USER, P199, DOI [DOI 10.1145/1866029, DOI 10.1145/1866029.1866063]
   Simpson T, 2010, NEUROREHAB NEURAL RE, V24, P188, DOI 10.1177/1545968309341647
   Tran KN, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P25
   Ufkes A, 2013, 2013 INTERNATIONAL CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P226, DOI 10.1109/CRV.2013.51
   ur Rehman Shafiq, 2007, 2007 Third International IEEE Conference on Signal-Image Technologies and Internet-Based System (SITIS), P768, DOI 10.1109/SITIS.2007.102
   Ur Réhman S, 2008, IEEE T MULTIMEDIA, V10, P1022, DOI 10.1109/TMM.2008.2001352
   Valkov D., 2010, P INT C VIRT REAL
   Wagner D, 2010, IEEE T VIS COMPUT GR, V16, P355, DOI 10.1109/TVCG.2009.99
   Wagner D, 2008, INT SYM MIX AUGMENT, P125, DOI 10.1109/ISMAR.2008.4637338
   Wang JJ, 2006, J INSECT SCI, V6, DOI 10.1673/2006_06_19.1
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.1093/jee/39.2.269
   Yousefi S, 2013, PATTERN RECOGN LETT, V34, P912, DOI 10.1016/j.patrec.2013.02.004
   Yousefi S, 2011, LECT NOTES COMPUT SC, V6855, P555, DOI 10.1007/978-3-642-23678-5_66
NR 53
TC 169
Z9 171
U1 1
U2 55
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2014
VL 11
IS 1
SU S
SI SI
AR 10
DI 10.1145/2645860
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AS0RG
UT WOS:000343984800002
DA 2024-07-18
ER

PT J
AU Liu, QZ
   Sung, AH
   Qiao, MY
AF Liu, Qingzhong
   Sung, Andrew H.
   Qiao, Mengyu
TI Derivative-Based Audio Steganalysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Security; Audio; steganography; steganalysis;
   derivative; Mel-cepstrum; Markov; signal complexity; SVM
ID STEGANOGRAPHY
AB This article presents a second-order derivative-based audio steganalysis. First, Mel-cepstrum coefficients and Markov transition features from the second-order derivative of the audio signal are extracted; a support vector machine is then applied to the features for discovering the existence of hidden data in digital audio streams. Also, the relation between audio signal complexity and steganography detection accuracy, which is an issue relevant to audio steganalysis performance evaluation but so far has not been explored, is analyzed experimentally. Results demonstrate that, in comparison with a recently proposed signal stream-based Mel-cepstrum method, the second-order derivative-based audio steganalysis method gains a considerable advantage under all categories of signal complexity-especially for audio streams with high signal complexity, which are generally the most challenging for steganalysis-and thereby significantly improves the state of the art in audio steganalysis.
C1 [Liu, Qingzhong] Sam Houston State Univ, Dept Comp Sci, Huntsville, TX 77341 USA.
   [Sung, Andrew H.] New Mexico Inst Min & Technol, Dept Comp Sci, Socorro, NM 87801 USA.
   [Sung, Andrew H.] New Mexico Inst Min & Technol, Inst Complex Addit Syst Anal, Socorro, NM 87801 USA.
   [Qiao, Mengyu] S Dakota Sch Mines & Technol, Dept Math & Comp Sci, Rapid City, SD 57701 USA.
C3 Texas State University System; Sam Houston State University; New Mexico
   Institute of Mining Technology; New Mexico Institute of Mining
   Technology; South Dakota School Mines & Technology
RP Liu, QZ (corresponding author), Sam Houston State Univ, Dept Comp Sci, Huntsville, TX 77341 USA.
EM qxl005@shsu.edu; sung@cs.nmt.edu; mengyu.qiao@sdsmt.edu
RI Qiao, Mengyu/N-4466-2016; Sung, Andrew H/ABD-7870-2022
OI Liu, Qingzhong/0000-0002-2006-5413; Sung, Andrew/0009-0005-0815-3102
FU Institute for Complex Additive Systems Analysis, a research division of
   New Mexico Tech.
FX This research was supported by the Institute for Complex Additive
   Systems Analysis, a research division of New Mexico Tech.
CR A Reynolds D., 1992, GAUSSIAN MIXTURE MOD
   [Anonymous], P 20 IJCAI
   [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], PROC
   [Anonymous], DIGITAL IMAGE PROCES
   Avcibas I, 2006, IEEE SIGNAL PROC LET, V13, P92, DOI 10.1109/LSP.2005.862152
   Farid H, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P905
   Fridrich J, 2004, LECT NOTES COMPUT SC, V3200, P67
   HARMSEN J, 2003, THESIS RENSSELAER PO
   Harmsen JJ, 2003, PROC SPIE, V5020, P131, DOI 10.1117/12.476813
   Hetzl S, 2005, LECT NOTES COMPUT SC, V3677, P119
   Hill T., 2005, STAT METHODS APPL
   Holotyak T, 2005, LECT NOTES COMPUT SC, V3677, P273
   Johnson MK, 2005, PROC SPIE, V5681, P664, DOI 10.1117/12.586941
   Kirovski D, 2003, IEEE T SIGNAL PROCES, V51, P1020, DOI 10.1109/TSP.2003.809384
   KRAETZER C, 2007, P SPIE, V6505
   Liu Q., 2008, P IEEE VEHICLE POWER, P1, DOI DOI 10.1109/VPPC.2008.4677444
   LIU Q, 2008, P 21 INT JOINT C NEU, P3351
   Liu QZ, 2008, INFORM SCIENCES, V178, P21, DOI 10.1016/j.ins.2007.08.007
   Liu QZ, 2008, PATTERN RECOGN, V41, P56, DOI 10.1016/j.patcog.2007.06.005
   Liu QZ, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899420
   Liu QZ, 2010, INFORM SCIENCES, V180, P1643, DOI 10.1016/j.ins.2010.01.001
   Liu QZ, 2009, IEEE T INF FOREN SEC, V4, P359, DOI 10.1109/TIFS.2009.2024718
   Liu Qingzhong., 2009, Proceedings of the 17th ACM international conference on Multimedia, MM '09, P873
   Liu YL, 2008, LECT NOTES COMPUT SC, V5222, P487, DOI 10.1007/978-3-540-85886-7_33
   Lyu SW, 2006, IEEE T INF FOREN SEC, V1, P111, DOI 10.1109/TIFS.2005.863485
   MCEACHERN RH, 1994, DSP APPL
   Mengyu Qiao, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P2566, DOI 10.1109/IJCNN.2009.5178971
   Özer H, 2006, DIGIT SIGNAL PROCESS, V16, P389, DOI 10.1016/j.dsp.2005.12.001
   PEVNY T, 2007, P SPIE ELECT IMAG, V6505
   Sharp Toby, 2001, Information Hiding, V2137, P13, DOI 10.1007/3-540-45496-9_2
   Shi YQ, 2007, LECT NOTES COMPUT SC, V4437, P249
   Zeng W., 2008, P INT C INF AUT, P1667
   Zeng W, 2007, ALPIT 2007: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON ADVANCED LANGUAGE PROCESSING AND WEB INFORMATION TECHNOLOGY, P261, DOI 10.1109/ALPIT.2007.41
NR 34
TC 34
Z9 36
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2011
VL 7
IS 3
AR 18
DI 10.1145/2000486.2000492
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 814DB
UT WOS:000294425100006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gopinathan, A
   Li, ZP
AF Gopinathan, Ajay
   Li, Zongpeng
TI Optimal Layered Multicast
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Theory; Multicast routing; network coding; optimization
ID OPTIMIZATION; BRANCH
AB Recent advances in network coding research dramatically changed the underlying structure of optimal multicast routing algorithms and made them efficiently computable. While most such algorithm design assumes a single file/layer being multicast, layered coding introduces new challenges into the paradigm due to its cumulative decoding nature. Layered coding is designed to handle heterogeneity in receiver capacities, and a node may decode layer k only if it successfully receives all layers in 1..k. We show that recently proposed optimization models for layered multicast do not correctly address this challenge. We argue that in order to achieve the absolute maximum throughput (or minimum cost), it is necessary to decouple the application-layer throughput from network-layer throughput. In particular, a node should be able to receive a nonconsecutive layer or a partial layer even if it cannot decode and utilize it (e.g., for playback in media streaming applications). The rationale is that nodes at critical network locations need to receive data just for helping other peers. We present a mathematical programming model that addresses these challenges and achieves absolute optimal performance. Simulation results show considerable throughput gain (cost reduction) compared with previous models, in a broad range of network scenarios. We then provide a formal proof that the layered multicast problem is NP-complete. We design a randomized rounding algorithm to approximate the optimal layered multicast, and show the efficacy of our technique using simulations. We then proceed to further generalize our model by studying the optimal progression of layer sizes. We show that such optimization is nonconvex, and apply a simulated annealing algorithm to solve it, with flexible trade-off between solution quality and running time. We verify the effectiveness of the new model and the simulated annealing algorithm through extensive simulations, and point out insights on the connection between optimal layer size progression and node capacity distribution.
C1 [Gopinathan, Ajay; Li, Zongpeng] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Gopinathan, A (corresponding author), Univ Calgary, Dept Comp Sci, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.
EM ajay.gopinathan@ucalgary.ca; zongpeng@ucalgary.ca
FU NSERC; Alberta Ingenuity Fund
FX This research was supported in part by NSERC and the Alberta Ingenuity
   Fund.
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   [Anonymous], ACM SIGCOMM
   [Anonymous], P 10 ANN ACM SIAM S
   [Anonymous], 1995, 138182 ISOIEC
   [Anonymous], BRITE BOSTON U REPRE
   [Anonymous], INFORM THEORY IEEE T
   [Anonymous], GLPK GNU LINEAR PROG
   [Anonymous], H263 ITUT
   Chen SW, 2000, IEEE ACM T NETWORK, V8, P311, DOI 10.1109/90.851977
   FLETCHER R, 1994, MATH PROGRAM, V66, P327, DOI 10.1007/BF01581153
   Garey M.R., 1979, COMPUTERS INTRACTABI
   Geoffrion A. M., 1972, Journal of Optimization Theory and Applications, V10, P237, DOI 10.1007/BF00934810
   GUPTA OK, 1985, MANAGE SCI, V31, P1533, DOI 10.1287/mnsc.31.12.1533
   HAJEK B, 1988, MATH OPER RES, V13, P311, DOI 10.1287/moor.13.2.311
   Ho T, 2006, IEEE T INFORM THEORY, V52, P4413, DOI 10.1109/TIT.2006.881746
   Karp R. M, 1972, COMPLEXITY COMPUTER
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Koetter R, 2003, IEEE ACM T NETWORK, V11, P782, DOI 10.1109/TNET.2003.818197
   Li B, 2003, IEEE NETWORK, V17, P24
   Li ZP, 2006, IEEE T INFORM THEORY, V52, P2467, DOI 10.1109/TIT.2006.874515
   Papadimitriou C.H., 1998, COMBINATORIAL OPTIMI
   QUESADA I, 1992, COMPUT CHEM ENG, V16, P937, DOI 10.1016/0098-1354(92)80028-8
   Thimm M, 2001, LECT NOTES COMPUT SC, V2136, P678
   Wu Y, 2005, IEEE J SEL AREA COMM, V23, P136, DOI 10.1109/JSAC.2004.837362
   Yuan J, 2006, IEEE J SEL AREA COMM, V24, P2092, DOI 10.1109/JSAC.2006.881617
   Zhao J, 2006, IEEE T MULTIMEDIA, V8, P1021, DOI 10.1109/TMM.2006.879847
   P ANN JOINT C IEEE C
   P ACM SIGCOMM DAT CO
   P ANN JOINT C IEEE C
   P ANN JOINT C IEEE C
   P 11 INT COMP COMB C
   P IEEE PROF COMM C
   P 6 INT WORKSH NETW
   P 11 EUR S ALG ESA
   P ANN JOINT C IEEE C
   P ANN JOINT C IEEE C
   P ANN JOINT C IEEE C
   P 43 ANN ALL C COMM
NR 39
TC 5
Z9 8
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2011
VL 7
IS 2
AR 7
DI 10.1145/1925101.1925102
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 729AT
UT WOS:000287919200001
DA 2024-07-18
ER

PT J
AU De Oliveira, R
   Cherubini, M
   Oliver, N
AF De Oliveira, Rodrigo
   Cherubini, Mauro
   Oliver, Nuria
TI Looking at Near-Duplicate Videos from a Human-Centric Perspective
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Human Factors; Measurement; Verification;
   Psychophysical experiment; similarity; user study; YouTube NDVC;
   near-duplicate; video sharing
AB Popular content in video sharing websites (e.g., YouTube) is usually replicated via identical copies or near-duplicates. These duplicates are usually studied because they pose a threat to site owners in terms of wasted disk space, or privacy infringements. Furthermore, this content might potentially hinder the users' experience in these websites. The research presented in this article focuses around the central argument that there is no agreement on the technical definition of what these near-duplicates are, and, more importantly, there is no strong evidence that users of video sharing websites would like this content to be removed. Most scholars define near-duplicate video clips (NDVC) by means of non-semantic features (e.g., different image/audio quality), while a few also include semantic features (i.e., different videos of similar content). However, it is unclear what features contribute to the human perception of near-duplicate videos. The findings of four large scale online surveys that were carried out in the context of our research confirm the relevance of both types of features. Some of our findings confirm the adopted definitions of NDVC whereas other findings are surprising: Near-duplicate videos with different image quality, audio quality, or with/without overlays were perceived as NDVC. However, the same could not be verified when videos differed by more than one of these features at the same time. With respect to semantics, it is yet unclear the exact role that it plays in relation to the features that make videos alike. From a user's perspective, participants preferred in most cases to see only one of the NDVC in the search results of a video search query and they were more tolerant to changes in the audio than in the video tracks. Based on all these findings, we propose a new user-centric NDVC definition and present implications for how duplicate content should be dealt with by video sharing Web sites.
C1 [De Oliveira, Rodrigo; Cherubini, Mauro; Oliver, Nuria] Telefon Res, Barcelona, Spain.
C3 Telefonica SA
RP De Oliveira, R (corresponding author), Via Augusta 177, Barcelona 09021, Spain.
EM oliveira@tid.es
RI Oliveira, Rodrigo Rafael Souza de/KFB-1888-2024; Oliveira,
   Rodrigo/E-7563-2010; Oliver, Nuria/AAK-6995-2021; Cherubini,
   Mauro/K-4721-2015
OI Oliveira, Rodrigo Rafael Souza de/0000-0002-4342-9355; Oliver,
   Nuria/0000-0001-5985-691X; Cherubini, Mauro/0000-0002-1860-6110
FU European Social Fund
FX Telefonica Research participates in the Torres Quevedo subprogram
   (MICINN), cofinanced by the European Social Fund, for researchers
   recruitment.
CR [Anonymous], P INT C MULT INF RET
   Basharat A, 2008, COMPUT VIS IMAGE UND, V110, P360, DOI 10.1016/j.cviu.2007.09.016
   Benevenuto F., 2008, 16 ACM INT C MULTIME, P761
   BRUCE B, 1996, VISUAL PERCEPTION
   CELEBI ME, 2005, P 18 INT FLOR ART IN, P245
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Cheng R., 2009, P 17 ACM INT C MULT, P1001
   GILL P, 2008, P SPIE ACM C MULT CO
   GUYADER N, 2002, P IEEE INT WORKSH NE, P385
   Halvey M.J., 2007, WWW 07, P1273, DOI DOI 10.1145/1242572.1242804
   Hsu W. H., 2006, MULTIMEDIA '06, P35
   Kim HS, 2010, LECT NOTES COMPUT SC, V5993, P229
   Kruitbosch G., 2008, Proceedings of the 3rd ACM Workshop on Human-Centered Computing, P7, DOI [10.1145/1462027.1462029, DOI 10.1145/1462027.1462029]
   Maia M., 2008, P 1 WORKSHOP SOCIAL, P1, DOI [10.1145/1435497.1435498, DOI 10.1145/1435497.1435498]
   MIN HS, 2009, P INT S MULT, P65
   Payne JS, 2001, PROCEEDINGS OF 2001 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P154, DOI 10.1109/ISIMP.2001.925355
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Shao J, 2008, PROC VLDB ENDOW, V1, P1598, DOI 10.14778/1454159.1454232
   Shen H.T., 2007, VLDB, P1374
   TVERSKY A, 1977, PSYCHOL REV, V84, P327, DOI 10.1037/h0026750
   Wu A. G., 2007, P ACM MM, P218
   Yang X., 2009, P 1 ACM WORKSH LARG, P73, DOI DOI 10.1145/1631058.1631073
   ZHOU X, 2009, TRANS MULTIMEDIA, V11, P879
NR 23
TC 4
Z9 4
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2010
VL 6
IS 3
SI SI
AR 15
DI 10.1145/1823746.1823749
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 649UL
UT WOS:000281799300004
DA 2024-07-18
ER

PT J
AU Zha, ZJ
   Yang, LJ
   Mei, T
   Wang, M
   Wang, ZF
   Chua, TS
   Hua, XS
AF Zha, Zheng-Jun
   Yang, Linjun
   Mei, Tao
   Wang, Meng
   Wang, Zengfu
   Chua, Tat-Seng
   Hua, Xian-Sheng
TI Visual Query Suggestion: Towards Capturing User Intent in Internet Image
   Search
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Human Factors; Intention gap; query
   suggestion; image search
ID INFORMATION
AB Query suggestion is an effective approach to bridge the Intention Gap between the users' search intents and queries. Most existing search engines are able to automatically suggest a list of textual query terms based on users' current query input, which can be called Textual Query Suggestion. This article proposes a new query suggestion scheme named Visual Query Suggestion (VQS) which is dedicated to image search. VQS provides a more effective query interface to help users to precisely express their search intents by joint text and image suggestions. When a user submits a textual query, VQS first provides a list of suggestions, each containing a keyword and a collection of representative images in a dropdown menu. Once the user selects one of the suggestions, the corresponding keyword will be added to complement the initial query as the new textual query, while the image collection will be used as the visual query to further represent the search intent. VQS then performs image search based on the new textual query using text search techniques, as well as content-based visual retrieval to refine the search results by using the corresponding images as query examples. We compare VQS against three popular image search engines, and show that VQS outperforms these engines in terms of both the quality of query suggestion and the search performance.
C1 [Zha, Zheng-Jun; Wang, Zengfu] Univ Sci & Technol China, Hefei 230027, Anhui, Peoples R China.
   [Zha, Zheng-Jun; Chua, Tat-Seng] Natl Univ Singapore, Singapore 117417, Singapore.
   [Yang, Linjun; Mei, Tao; Wang, Meng; Hua, Xian-Sheng] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; National University of Singapore; Microsoft; Microsoft
   Research Asia
RP Zha, ZJ (corresponding author), Univ Sci & Technol China, 96 Jinzhai Rd, Hefei 230027, Anhui, Peoples R China.
EM zhazj@comp.nus.edu.sg; linjuny@microsoft.com; tmei@microsoft.com;
   mengwang@microsoft.com; zfwang@ustc.edu.cn; chuats@comp.nus.edu.sg;
   xshua@microsoft.com
RI Zha, Zheng-Jun/AAF-8667-2020; Zha, Zheng-Jun/AAE-8408-2020; Mei,
   Tao/GQZ-0596-2022
OI Zha, Zheng-Jun/0000-0003-2510-8993; Mei, Tao/0000-0002-5990-7307
CR [Anonymous], 2009, P 17 ACM INT C MULTI
   [Anonymous], P IEEE INFOCOM APR
   [Anonymous], 1996, P 19 ANN INT ACM SIG, DOI DOI 10.1145/243199.243202
   [Anonymous], P ACM MULT
   Beeferman D., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P407, DOI 10.1145/347090.347176
   Ben-Hur A., 2002, Journal of Machine Learning Research, V2, P125, DOI 10.1162/15324430260185565
   Boyd S., 2004, CONVEX OPTIMIZATION
   Carpineto C, 2001, ACM T INFORM SYST, V19, P1, DOI 10.1145/366836.366860
   CUI J, 2008, P ACM INT C MULT VAN, P729
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Fisher R.A., 1970, STAT METHODS RES WOR
   Fonseca Bruno M., 2005, P 14 ACM INT C INF K, P696
   Frey B., 2007, MULTI DATABASE RETRI, Vvol. 315, ppp, DOI [DOI 10.1126/SCIENCE.1136800, 10.1126/science.1136800]
   GERRIG RJ, 2001, PSYCHOL LIFE
   HEESCH D, 2004, P EUR C INF RETR
   Huang CK, 2003, J AM SOC INF SCI TEC, V54, P638, DOI 10.1002/asi.10256
   Jarvelin Kalervo, 2000, P 23 ANN INT ACM SIG, P41, DOI DOI 10.1145/345508.345545
   JIA Y, 2008, P ACM MULT
   Jones Rosie, 2006, Proceedings of the 15th International Conference on World Wide Web (WWW '06), P387, DOI [DOI 10.1145/1135777.1135835, 10.1145/1135777.1135835]
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   LAMADESINA AM, 2001, P 24 ANN INT ACM SIG, P1, DOI DOI 10.1145/383952.383953
   Lew M. S., 2006, ACM T MULTIMED COMPU
   LIU Y, 2009, P ACM C RES DEV INF
   RICARDO BY, 2004, P INT C EXT DAT TECH
   Sigurbjornsson B., 2008, P 17 INT C WORLD WID
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tian X., 2008, ACM INT C MULTIMEDIA, P131, DOI DOI 10.1145/1459359.1459378.ISBN
   Weinberger K, 2008, P ACM C MULT
   Yan R, 2003, LECT NOTES COMPUT SC, V2728, P238
   Yu Shipeng., 2003, P 12 INT C ONWORLDWI, P11, DOI DOI 10.1145/775152.775155
NR 30
TC 74
Z9 83
U1 0
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2010
VL 6
IS 3
SI SI
AR 13
DI 10.1145/1823746.1823747
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 649UL
UT WOS:000281799300002
DA 2024-07-18
ER

PT J
AU Mondet, S
   Cheng, W
   Morin, G
   Grigoras, R
   Boudon, F
   Ooi, WT
AF Mondet, Sebastien
   Cheng, Wei
   Morin, Geraldine
   Grigoras, Romulus
   Boudon, Frederic
   Ooi, Wei Tsang
TI Compact and Progressive Plant Models for Streaming in Networked Virtual
   Environments
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Performance; Experimentation; Streaming; plant models;
   multiresolution; progressive coding; progressive transmission; networked
   virtual environment
AB Just as in the real world, plants are important objects in virtual worlds for creating pleasant and realistic environments, especially those involving natural scenes. As such, much effort has been made in realistic modeling of plants. As the trend moves towards networked and distributed virtual environments, however, the current models are inadequate as they are not designed for progressive transmissions. In this article, we fill in this gap by proposing a progressive representation for plants based on generalized cylinders. We model the shape and thickness of branches in a plant as Bezier curves, group the curves according to the similarity, and differentially code the curves to represent the plant in a compact and progressive manner. To facilitate the transmission of the plants, we quantify the visual contribution of each branch and use this weight in packet scheduling. We show the efficiency of our representations and the effectiveness of our packet scheduler through experiments over a wide area network.
C1 [Mondet, Sebastien; Morin, Geraldine; Grigoras, Romulus] Univ Toulouse, Toulouse, France.
   [Cheng, Wei; Ooi, Wei Tsang] Natl Univ Singapore, Singapore, Singapore.
   [Boudon, Frederic] CIRAD, Montpellier, France.
C3 Universite de Toulouse; National University of Singapore; CIRAD
RP Morin, G (corresponding author), Univ Toulouse, Toulouse, France.
EM morin@n7.fr
RI Ooi, Wei Tsang/AAE-7810-2019; Boudon, Frédéric/E-6812-2015; Ooi, Wei
   Tsang/HLW-5142-2023
OI Ooi, Wei Tsang/0000-0001-8994-1736; Ooi, Wei Tsang/0000-0001-8994-1736;
   Boudon, Frederic/0000-0001-9636-3102
FU French National Research Agency [05-MMSA-0004-01]; National University
   of Singapore Academic Research Fund [R-252-000-306-112]
FX This work has been partly funded by the NatSim ANR project (French
   National Research Agency 05-MMSA-0004-01) and National University of
   Singapore Academic Research Fund R-252-000-306-112.
CR ALREGIB G, 2003, P IEEE INT C MULT EX, V1, P421
   AlRegib G., 2002, Proceedings of the IEEE International Conference, V2, P2041
   [Anonymous], ACM SIGGRAPH 1996 C
   Behrendt S, 2005, COMPUT GRAPH FORUM, V24, P507, DOI 10.1111/j.1467-8659.2005.00876.x
   Bloomenthal J., 1985, Computer Graphics, V19, P305, DOI 10.1145/325165.325249
   BOGACKI P, 1995, COMPUT AIDED DESIGN, V27, P651, DOI 10.1016/0010-4485(94)00011-2
   BOUDON F, 2006, 5205 LIRIS UMR CNRS
   Chen ZH, 2005, MULTIMEDIA SYST, V10, P230, DOI 10.1007/s00530-004-0154-3
   Cheng I, 2007, ADV MULTIMED, V2007, DOI 10.1155/2007/95218
   CHENG W, 2008, P INT WORKSH NETW OP
   Costes E, 2003, ANN BOT-LONDON, V91, P91, DOI 10.1093/aob/mcg010
   DECAUDIN P, 2004, P 2004 EUR WORKSH RE, P93
   DELARE A, 2009, P INT C COMP SCI, P801
   Deussen O, 2005, Digital Design of Nature: Computer Generated Plants and Organics
   DEUSSEN O, 2002, P IEEE VIS C
   Farin G., 2001, Curves and Surfaces for CAGD: A Practical Guide, Vfifth
   Galbraith C, 2004, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P78, DOI 10.1109/CGI.2004.1309195
   Guo YH, 2007, PROCEEDINGS OF INTERNATIONAL CONFERENCE ON HEALTH MONITORING OF STRUCTURE, MATERIALS AND ENVIRONMENT, VOLS 1 AND 2, P738
   Harris A F., 2002, Proc. NOSSDAV, P43
   JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588
   Kohler E., 2006, DATAGRAM CONGESTION
   Li H, 2006, ACM T MULTIM COMPUT, V2, P282, DOI 10.1145/1201730.1201733
   MENG F, 2003, INT C 3D DIG IM MOD
   Meyer A, 2001, SPRING EUROGRAP, P183
   Mokhtarian F., 1996, BRIT MACHINE VISION, P53
   MONDET S, 2008, P ACM MULT C, P1
   Neubert B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239539
   Osserman R., 1986, Dover Books on Mathematics, V2
   Park SB, 2006, IEEE T MULTIMEDIA, V8, P885, DOI 10.1109/TMM.2006.879914
   Prusinkiewicz P, 2001, COMP GRAPH, P289, DOI 10.1145/383259.383291
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   PRUSINKIEWICZ P, 1986, VISION INTERFACE 86, P247
   Ramanathan P, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P277
   REMOLAR I, 2002, EUROGRAPHICS SHORT P, P397
   RUSINKIEWICZ S, 2001, P 2001 S INT 3D GRAP, P63
   Sinoquet Herve, 1997, Silva Fennica, V31, P265
   TARI B, 2005, P 13 EUR SIGN PROC C
   TIAN D, 2004, P ACM MULT C
   Weber J., 1995, Proceedings of the 22Nd Annual Conference on Computer Graphics and Interactive Techniques, P119, DOI DOI 10.1145/218380.218427
   Yan HP, 2004, ANN BOT-LONDON, V93, P591, DOI 10.1093/aob/mch078
   Yan ZD, 2001, IEEE T CIRC SYST VID, V11, P860, DOI 10.1109/76.931112
   YANG S, 2004, P ACM MULT C
   Zhang X., 2006, P ACM SIGGRAPH INT C, P331
NR 43
TC 3
Z9 3
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2009
VL 5
IS 3
AR 21
DI 10.1145/1556134.1556138
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 504DV
UT WOS:000270595600004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, ML
   Leng, HY
   Fang, B
   Xiang, T
   Wei, XK
   Jia, WJ
AF Zhou, Mingliang
   Leng, Hongyue
   Fang, Bin
   Xiang, Tao
   Wei, Xuekai
   Jia, Weijia
TI Low-light Image Enhancement via a Frequency-based Model with Structure
   and Texture Decomposition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE low-light image enhancement; denosing; Retinex theory
ID RETINEX; ILLUMINATION
AB This article proposes a frequency-based structure and texture decomposition model in a Retinex-based framework for low-light image enhancement and noise suppression. First, we utilize the total variation-based noise estimation to decompose the observed image into low-frequency and high-frequency components. Second, we use a Gaussian kernel for noise suppression in the high-frequency layer. Third, we propose a frequency-based structure and texture decomposition method to achieve low-light enhancement. We extract texture and structure priors by using the high-frequency layer and a low-frequency layer, respectively. We present an optimization problem and solve it with the augmented Lagrange multiplier to generate a balance between structure and texture in the reflectance map. Our experimental results reveal that the proposed method can achieve superior performance in naturalness preservation and detail retention compared with state-of-the-art algorithms for low-light image enhancement. Our code is available on the following website.
C1 [Zhou, Mingliang; Leng, Hongyue; Fang, Bin; Xiang, Tao] Chongqing Univ, Sch Comp Sci, Chongqing 40044, Peoples R China.
   [Wei, Xuekai] Univ Macau, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
   [Wei, Xuekai] Univ Macau, Dept Elect & Comp Engn, Macau 999078, Peoples R China.
   [Jia, Weijia] Beijing Normal Univ Zhuhai, 2000 Jintong St, Zhuhai 519087, Peoples R China.
   [Jia, Weijia] BNU HKBU United Int Coll, Guangdong Key Lab Multi Modal Data Proc, 2000 Jintong St, Zhuhai 519087, Peoples R China.
C3 Chongqing University; University of Macau; University of Macau; Beijing
   Normal University; Beijing Normal University Zhuhai; Beijing Normal
   University - Hong Kong Baptist University United International College
RP Zhou, ML; Fang, B (corresponding author), Chongqing Univ, Sch Comp Sci, Chongqing 40044, Peoples R China.
EM mingliangzhou@cqu.edu.cn; lhycqucs@163.com; fb@cqu.edu.cn;
   txiang@cqu.edu.cn; 11132021609@bnu.edu.cn; jiawj@uic.edu.cn
RI Zhou, Mingliang/HPC-0298-2023; Xiang, Tao/N-3706-2016; ZHOU,
   MING/JVP-2920-2024
OI Xiang, Tao/0000-0002-9439-4623; WEI, Xuekai/0000-0002-3761-1759; Zhou,
   Mingliang/0000-0002-1874-3641
FU National Natural Science Foundation of China [62176027]; General Program
   of National Natural Science Foundation of Chongqing [cstc2020jcyj
   msxmX0790]; Fundamental Research Funds for the Central Universities
   [2021CDJJMRH-014]; Guangxi Key Laboratory of Cryptography and
   Information Security [GCIS201905]; Human Resources and Social Security
   Bureau project of Chongqing [cx2020073]; Youth Project of Guizhou
   Education Department [Qian Jiao he KY [2017] 359]; University-Level
   Scientific Research Projects [qnsy2018028]; Guizhou Provincial Education
   Department Young Science and Technology Talents Development Project
   [Qian Jiao he KY [2020] 210]
FX This work was supported by National Natural Science Foundation of China
   (No. 62176027), the General Program of National Natural Science
   Foundation of Chongqing (No. cstc2020jcyj msxmX0790), the Fundamental
   Research Funds for the Central Universities (No. 2021CDJJMRH-014), the
   Guangxi Key Laboratory of Cryptography and Information Security (No.
   GCIS201905), the Human Resources and Social Security Bureau project of
   Chongqing (No. cx2020073), the Youth Project of Guizhou Education
   Department (No. Qian Jiao he KY [2017] 359), the University-Level
   Scientific Research Projects (No. qnsy2018028), the Guizhou Provincial
   Education Department Young Science and Technology Talents Development
   Project (No. Qian Jiao he KY [2020] 210).
CR Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Chen BH, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2726947
   Chen XY, 2021, IEEE T CIRC SYST VID, V31, P715, DOI 10.1109/TCSVT.2020.2987465
   Cui ZT, 2022, Arxiv, DOI arXiv:2205.14871
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Immerkaer J, 1996, COMPUT VIS IMAGE UND, V64, P300, DOI 10.1006/cviu.1996.0060
   Jin KH, 2015, IEEE T IMAGE PROCESS, V24, P3498, DOI 10.1109/TIP.2015.2446943
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kim JY, 2001, IEEE T CIRC SYST VID, V11, P475, DOI 10.1109/76.915354
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lecca M, 2021, IEEE T IMAGE PROCESS, V30, P3543, DOI 10.1109/TIP.2021.3062724
   Lecca M, 2018, IEEE T IMAGE PROCESS, V27, P5802, DOI 10.1109/TIP.2018.2858541
   Lecca M, 2017, IEEE T IMAGE PROCESS, V26, P2767, DOI 10.1109/TIP.2017.2686652
   Lecca M, 2016, J OPT SOC AM A, V33, P31, DOI 10.1364/JOSAA.33.000031
   Leng HY, 2023, INT J PATTERN RECOGN, V37, DOI 10.1142/S0218001423540034
   Li L, 2015, IEEE IMAGE PROC, P3730, DOI 10.1109/ICIP.2015.7351501
   Li MD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3341728
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li XL, 2017, PROCEEDINGS OF 2017 IEEE 2ND INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC), P837, DOI 10.1109/ITNEC.2017.8284852
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu HF, 2014, IEEE INT CON MULTI
   Liu YF, 2017, IEEE T CIRC SYST VID, V27, P1171, DOI 10.1109/TCSVT.2016.2527338
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moniruzzaman M., 2014, PROC 2013 INT C ELEC, P1, DOI DOI 10.1109/EICT.2014.6777872
   Parihar AS, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON INVENTIVE SYSTEMS AND CONTROL (ICISC 2018), P619, DOI 10.1109/ICISC.2018.8398874
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Rizzi A, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.3.031207
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Saini Akshita, 2019, 2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI). Proceedings, P628, DOI 10.1109/ICOEI.2019.8862794
   Sun J, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC), P157, DOI 10.1109/ICIVC.2018.8492727
   Sun ZH, 2021, IEEE T CIRC SYST VID, V31, P1819, DOI 10.1109/TCSVT.2020.3009717
   Todorovic S, 2008, IEEE T PATTERN ANAL, V30, P2158, DOI 10.1109/TPAMI.2008.24
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang YF, 2021, Arxiv, DOI [arXiv:2109.05923, DOI 10.48550/ARXIV.2109.05923]
   Wang YF, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2922106
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C., 2018, BRIT MACHINE VISION
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Yang KF, 2020, IEEE T IMAGE PROCESS, V29, P1493, DOI 10.1109/TIP.2019.2938310
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
NR 55
TC 3
Z9 3
U1 12
U2 28
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
AR 187
DI 10.1145/3590965
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200010
DA 2024-07-18
ER

PT J
AU Yin, HB
   Wang, HK
   Yu, L
   Liang, JH
   Zhai, GT
AF Yin, Haibing
   Wang, Hongkui
   Yu, Li
   Liang, Junhui
   Zhai, Guangtao
TI Feedforward and Feedback Modulations Based Foveated JND Estimation for
   Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE JND estimation model; visual attention; foveated masking; feedforward
   and feedback modulatory effects
ID FREE-ENERGY PRINCIPLE; NOTICEABLE DIFFERENCE; PROFILE; CORTEX; BRAIN;
   MODEL
AB The just noticeable difference (JND) reveals the key characteristic of visual perception, which has been widely used in many perception-based image and video applications. Nevertheless, the modulatory mechanism of the human visual system (HVS) has not been fully exploited in JND threshold estimation, which results in the existing JND models not being accurate enough. In this article, by analyzing the feedforward and feedback modulatory behaviors in the HVS, an enhanced foveated JND (FJND) estimation model is proposed considering modulatory effects and masking effects in visual perception. The contributions of this article are mainly twofold. On the one hand, by analyzing the modulatory behaviors in the HVS, the modulatory mechanism is incorporated into JND estimation and a hierarchical modulation-based JND estimation framework is proposed for the first time. On the other hand, according to the response characteristics of visual neurons, modulatory effects on visual sensitivity are formulated as several modulatory factors to modulate the estimated JND threshold properly. Compared with existing models, the proposed model is developed in view of not only the masking effects but also the modulatory effects, which makes our model more consistent with the HVS. For different complex input images, experimental results show that the proposed FJND model tolerates more distortion at the same perceptual quality in comparison with other existing models.
C1 [Yin, Haibing; Wang, Hongkui] Hangzhou Dianzi Univ, Sch Commun Engn, 1158,2 St,Baiyang St, Hangzhou 310018, Zhejiang, Peoples R China.
   [Yu, Li; Liang, Junhui] Huazhong Univ Sci & Technol, Coll Elect Informat Engn, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Zhai, Guangtao] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
C3 Hangzhou Dianzi University; Huazhong University of Science & Technology;
   Shanghai Jiao Tong University
RP Wang, HK (corresponding author), Hangzhou Dianzi Univ, Sch Commun Engn, 1158,2 St,Baiyang St, Hangzhou 310018, Zhejiang, Peoples R China.
EM haibingyin@163.com; wanghk@hdu.edu.cn; hustlyu@hust.edu.cn;
   hustljh@hust.edu.cn; zhaiguangtao@sjtu.edu.cn
RI liang, junhui/N-6821-2019; Zhai, Guangtao/X-5949-2019
OI liang, junhui/0000-0002-0508-8297; Zhai, Guangtao/0000-0001-8165-9322
FU "Pioneer" and "Leading Goose" R&D Program of Zhejiang Province
   [2022C01068]; NSFC [62202134, 61871437, 61972123, 62031009]; Natural
   Science Foundation of Hubei Province of China [2019CFA022]
FX This work was supported in part by the "Pioneer" and "Leading Goose" R&D
   Program of Zhejiang Province under grant 2022C01068; in part by the NSFC
   under grants 62202134, 61871437, 61972123, and 62031009; and in part by
   the Natural Science Foundation of Hubei Province of China under grant
   2019CFA022.
CR AHUMADA AJ, 1992, P SOC PHOTO-OPT INS, V1666, P365
   Bae SH, 2017, IEEE T CIRC SYST VID, V27, P1196, DOI 10.1109/TCSVT.2016.2539862
   Barlow HB., 1961, SENS COMMUN, V1, P217, DOI 10.7551/mitpress/9780262518420.003.0013
   Chen ZZ, 2010, IEEE T CIRC SYST VID, V20, P806, DOI 10.1109/TCSVT.2010.2045912
   Feldman H, 2010, FRONT HUM NEUROSCI, V4, DOI 10.3389/fnhum.2010.00215
   Feng HC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2014.2384273
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Friston KJ, 2009, TRENDS COGN SCI, V13, P293, DOI 10.1016/j.tics.2009.04.005
   Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hochstein S, 2002, NEURON, V36, P791, DOI 10.1016/S0896-6273(02)01091-7
   Huber D, 2008, NATURE, V451, P61, DOI 10.1038/nature06445
   IRCCyN/IVC, 2013, SUBJ QUAL ASS IVC DA
   Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Ki S, 2018, IEEE T IMAGE PROCESS, V27, P3178, DOI 10.1109/TIP.2018.2818439
   Knill DC, 2004, TRENDS NEUROSCI, V27, P712, DOI 10.1016/j.tins.2004.10.007
   Li JL, 2022, IET IMAGE PROCESS, V16, P1724, DOI 10.1049/ipr2.12443
   Liu AM, 2010, IEEE T CIRC SYST VID, V20, P1648, DOI 10.1109/TCSVT.2010.2087432
   Liu HH, 2020, IEEE T IMAGE PROCESS, V29, P641, DOI 10.1109/TIP.2019.2933743
   Macknik SL, 1998, NAT NEUROSCI, V1, P144, DOI 10.1038/393
   Mannion DJ, 2015, EUR J NEUROSCI, V42, P2895, DOI 10.1111/ejn.13082
   Oosuga T, 2012, 2012 PROCEEDINGS OF SICE ANNUAL CONFERENCE (SICE), P481
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   RECOMMENDATION ITU-R BT, 2002, METH SUBJ ASS QUAL T
   Rouis K, 2018, IEEE ACCESS, V6, P33589, DOI 10.1109/ACCESS.2018.2843384
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sano Masaki, 2015, P APS M
   Tian T, 2020, IEEE T BROADCAST, V66, P690, DOI 10.1109/TBC.2020.2977542
   Wallisch P, 2008, NEURON, V60, P195, DOI 10.1016/j.neuron.2008.10.008
   Wang HK, 2021, IEEE T IMAGE PROCESS, V30, P487, DOI 10.1109/TIP.2020.3037525
   Wang HK, 2020, J VIS COMMUN IMAGE R, V71, DOI 10.1016/j.jvcir.2020.102850
   Wang HK, 2020, IEEE SIGNAL PROC LET, V27, P181, DOI 10.1109/LSP.2019.2957647
   Wang HK, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Wang RB, 2007, COGN NEURODYNAMICS, V1, P203, DOI 10.1007/s11571-007-9015-z
   Wang RB, 2009, NEUROCOMPUTING, V73, P139, DOI 10.1016/j.neucom.2009.02.022
   Wang SQ, 2016, IEEE T IMAGE PROCESS, V25, P3838, DOI 10.1109/TIP.2016.2573597
   Wang Z, 2003, IEEE T IMAGE PROCESS, V12, P243, DOI 10.1109/TIP.2003.809015
   Wei ZY, 2009, IEEE T CIRC SYST VID, V19, P337, DOI 10.1109/TCSVT.2009.2013518
   Williams Raymond, 1977, MARXISM LIT, P133
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1705, DOI 10.1109/TMM.2013.2268053
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P4892, DOI 10.1109/TIP.2013.2279934
   Yang KF, 2014, IEEE T IMAGE PROCESS, V23, P5020, DOI 10.1109/TIP.2014.2361210
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Zeng ZP, 2019, IEEE ACCESS, V7, P132111, DOI 10.1109/ACCESS.2019.2939569
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang XF, 2017, IEEE SIGNAL PROC LET, V24, P96, DOI 10.1109/LSP.2016.2641456
   Zhang Y, 2022, IEEE T CIRC SYST VID, V32, P1197, DOI 10.1109/TCSVT.2021.3076224
   Zhou ML, 2019, IEEE T MULTIMEDIA, V21, P1921, DOI 10.1109/TMM.2019.2895281
NR 51
TC 1
Z9 1
U1 3
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD SEP
PY 2023
VL 19
IS 5
AR 154
DI 10.1145/3579094
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K7WP6
UT WOS:001018509300003
OA Bronze
DA 2024-07-18
ER

PT J
AU Menon, VV
   Amirpour, H
   Ghanbari, M
   Timmerer, C
AF Menon, Vignesh V.
   Amirpour, Hadi
   Ghanbari, Mohammad
   Timmerer, Christian
TI EMES: Efficient Multi-encoding Schemes for HEVC-based Adaptive Bitrate
   Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE HTTP Adaptive Streaming; HEVC; multi-rate encoding; multi-encoding
AB In HTTP Adaptive Streaming (HAS), videos are encoded at multiple bitrates and spatial resolutions (i.e., representations) to adapt to the heterogeneity of network conditions, device attributes, and end-user preferences. Encoding the same video segment at multiple representations increases costs for content providers. State-of-the-art multi-encoding schemes improve the encoding process by utilizing encoder analysis information from already encoded representation(s) to reduce the encoding time of the remaining representations. These schemes typically use the highest bitrate representation as the reference to accelerate the encoding of the remaining representations. Nowadays, most streaming services utilize cloud-based encoding techniques, enabling a fully parallel encoding process to reduce the overall encoding time. The highest bitrate representation has a higher encoding time than the other representations. Thus, utilizing it as the reference encoding is unfavorable in a parallel encoding setup as the overall encoding time is bound by its encoding time. This article provides a comprehensive study of various multi-rate and multi-encoding schemes in both serial and parallel encoding scenarios. Furthermore, it introduces novel heuristics to limit the Rate Distortion Optimization (RDO) process across various representations. Based on these heuristics, three multi-encoding schemes are proposed, which rely on encoder analysis sharing across different representations: (i) optimized for the highest compression efficiency, (ii) optimized for the best compression efficiency-encoding time savings tradeoff, and (iii) optimized for the best encoding time savings. Experimental results demonstrate that the proposed multi-encoding schemes (i), (ii), and (iii) reduce the overall serial encoding time by 34.71%, 45.27%, and 68.76% with a 2.3%, 3.1%, and 4.5% bitrate increase to maintain the same VMAF, respectively, compared to stand-alone encodings. The overall parallel encoding time is reduced by 22.03%, 20.72%, and 76.82% compared to stand-alone encodings for schemes (i), (ii), and (iii), respectively.
C1 [Menon, Vignesh V.; Amirpour, Hadi; Timmerer, Christian] Alpen Adria Univ Klagenfurt, Christian Doppler Lab ATHENA, Klagenfurt, Austria.
   [Ghanbari, Mohammad] Univ Essex, Sch Comp Sci & Elect Engn, Colchester, Essex, England.
C3 University of Klagenfurt; University of Essex
RP Menon, VV (corresponding author), Alpen Adria Univ Klagenfurt, Christian Doppler Lab ATHENA, Klagenfurt, Austria.
EM vignesh.menon@aau.at; hadi.amirpour@aau.at; ghan@essex.ac.uk;
   christian.timmerer@aau.at
RI Menon, Vignesh V/JGE-1033-2023
OI , VIGNESH V MENON/0000-0003-1454-6146
FU Austrian Federal Ministry for Digital and Economic Affairs; National
   Foundation for Research, Technology and Development; Christian Doppler
   Research Association
FX The financial support of the Austrian Federal Ministry for Digital and
   Economic Affairs, the National Foundation for Research, Technology and
   Development, and the Christian Doppler Research Association is
   gratefully acknowledged. Christian Doppler Laboratory ATHENA:
   https://athena.itec.aau.at/.
CR Amirpour Hadi, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P469, DOI 10.1007/978-3-030-67832-6_38
   Amirpour H, 2020, IEEE DATA COMPR CONF, P358, DOI 10.1109/DCC47342.2020.00080
   Bjontegaard G., 2001, CALCULATION AVERAGE
   Boyce J., 2018, JVET-J1010: JVET common test conditions and software reference configurations
   Cheon M, 2018, IEEE T CIRC SYST VID, V28, P1467, DOI 10.1109/TCSVT.2017.2683504
   De Cock J, 2016, IEEE IMAGE PROC, P1484, DOI 10.1109/ICIP.2016.7532605
   De Praeter J, 2015, IEEE INT WORKSH MULT
   Goswami K, 2018, IEEE IMAGE PROC, P1008, DOI 10.1109/ICIP.2018.8451485
   Grellert M, 2021, J REAL-TIME IMAGE PR, V18, P1881, DOI 10.1007/s11554-020-01063-x
   Gu JW, 2018, IEEE IMAGE PROC, P988, DOI 10.1109/ICIP.2018.8451251
   Katsenou AV, 2021, IEEE OPEN J SIGNAL P, V2, P496, DOI 10.1109/OJSP.2021.3086691
   Lindino M, 2021, EUR SIGNAL PR CONF, P550, DOI 10.23919/Eusipco47968.2020.9287789
   Matheswaran A, 2020, PROC SPIE, V11510, DOI 10.1117/12.2567877
   Menon VV, 2022, INT CONF ACOUST SPEE, P1865, DOI 10.1109/ICASSP43922.2022.9746745
   Menon VV, 2021, IEEE IMAGE PROC, P2174, DOI 10.1109/ICIP42928.2021.9506092
   Menon VV, 2022, IEEE DATA COMPR CONF, P475, DOI 10.1109/DCC52660.2022.00086
   Menon VV, 2021, IEEE INT WORKSH MULT, DOI 10.1109/MMSP53017.2021.9733517
   MENON VV, 2021, EFFICIENT MULTIENCOD, P1, DOI DOI 10.1109/PCS50896.2021.9477499
   Radicke S, 2014, 2014 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND MULTIMEDIA APPLICATIONS (SIGMAP), P90
   Reznik YA, 2019, IEEE INT CONF MULTI, P348, DOI 10.1109/ICMEW.2019.00066
   Ringis DJ, 2021, PROC SPIE, V11842, DOI 10.1117/12.2593238
   Schroeder D, 2018, IEEE T CIRC SYST VID, V28, P143, DOI 10.1109/TCSVT.2016.2599028
   Schroeder D, 2015, IEEE IMAGE PROC, P3972, DOI 10.1109/ICIP.2015.7351551
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Song L, 2013, INT WORK QUAL MULTIM, P34, DOI 10.1109/QoMEX.2013.6603201
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yang S., 2019, 2019 IEEE GLOBAL COM, P1, DOI DOI 10.13031/AIM.201901018
NR 28
TC 0
Z9 0
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2023
VL 19
IS 3
SU S
AR 129
DI 10.1145/3575659
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FU9
UT WOS:001011934700004
OA Bronze
DA 2024-07-18
ER

PT J
AU Zheng, Q
   Dong, JF
   Qu, XY
   Yang, X
   Wang, YB
   Zhou, P
   Liu, BL
   Wang, X
AF Zheng, Qi
   Dong, Jianfeng
   Qu, Xiaoye
   Yang, Xun
   Wang, Yabing
   Zhou, Pan
   Liu, Baolong
   Wang, Xun
TI Progressive Localization Networks for Language-Based Moment Localization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Moment localization; progressive learning; coarse-to-fine manner;
   multi-stage model
ID AWARE NETWORK; VIDEO; RETRIEVAL; IMAGE
AB This article targets the task of language-based video moment localization. The language-based setting of this task allows for an open set of target activities, resulting in a large variation of the temporal lengths of video moments. Most existing methods prefer to first sample sufficient candidate moments with various temporal lengths, then match them with the given query to determine the target moment. However, candidate moments generated with a fixed temporal granularity may be suboptimal to handle the large variation in moment lengths. To this end, we propose a novel multi-stage Progressive Localization Network (PLN) that progressively localizes the target moment in a coarse-to-fine manner. Specifically, each stage of PLN has a localization branch and focuses on candidate moments that are generated with a specific temporal granularity. The temporal granularities of candidate moments are different across the stages. Moreover, we devise a conditional feature manipulation module and an upsampling connection to bridge the multiple localization branches. In this fashion, the later stages are able to absorb the previously learned information, thus facilitating the more fine-grained localization. Extensive experiments on three public datasets demonstrate the effectiveness of our proposed PLN for language-based moment localization, especially for localizing short moments in long videos.
C1 [Zheng, Qi; Dong, Jianfeng; Wang, Yabing; Liu, Baolong] Zhejiang Gongshang Univ, 18 Xuezheng St, Hangzhou 314423, Zhejiang, Peoples R China.
   [Qu, Xiaoye; Zhou, Pan] Huazhong Univ Sci & Technol, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Yang, Xun] Univ Sci & Technol China, 96 JinZhai Rd, Hefei 230026, Anhui, Peoples R China.
C3 Zhejiang Gongshang University; Huazhong University of Science &
   Technology; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Liu, BL (corresponding author), Zhejiang Gongshang Univ, 18 Xuezheng St, Hangzhou 314423, Zhejiang, Peoples R China.
EM zhengqi1225@outlook.com; dongjf24@gmail.com; xiaoye@hust.edu.cn;
   xyang21@ustc.edu.cn; wyb7wyb7@163.com; panzhou@hust.edu.cn;
   liubaolongx@gmail.com; wx@zjgsu.edu.cn
OI Qu, Xiaoye/0000-0002-4907-3978; Dong, Jianfeng/0000-0001-5244-3274;
   Wang, Yabing/0000-0001-7231-1260; YANG, Xun/0000-0003-0201-1638
FU National Key R&D Program of China [2018YFB1404102]; NSFC [61972448,
   61902347, 61976188, 62002323]; PublicWelfare Technology Research Project
   of Zhejiang Province [LGF21F020010]; Fundamental Research Funds for the
   Provincial Universities of Zhejiang; Open Projects Program of the
   National Laboratory of Pattern Recognition
FX This work was supported by the National Key R&D Program of China
   (2018YFB1404102), NSFC (61972448, 61902347, 61976188, 62002323), the
   PublicWelfare Technology Research Project of Zhejiang Province
   (LGF21F020010), the Fundamental Research Funds for the Provincial
   Universities of Zhejiang, and the Open Projects Program of the National
   Laboratory of Pattern Recognition.
CR Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P898, DOI 10.1145/3394171.3413841
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Dong J, 2022, IEEE Transactions on Circuits and Systems for Video Technology
   Dong JF, 2022, IEEE T PATTERN ANAL, V44, P4065, DOI 10.1109/TPAMI.2021.3059295
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P2371, DOI 10.1109/TMM.2018.2796248
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao J., 2021, P IEEECVF INT C COMP, P1523
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao JY, 2022, IEEE T CIRC SYST VID, V32, P1646, DOI 10.1109/TCSVT.2021.3075470
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Hahn Meera, 2020, P BRIT MACHINE VISIO
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hu YP, 2021, IEEE T IMAGE PROCESS, V30, P4667, DOI 10.1109/TIP.2021.3073867
   Jiang B, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P217, DOI 10.1145/3323873.3325019
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lin TW, 2020, IEEE T CIRC SYST VID, V30, P4899, DOI 10.1109/TCSVT.2019.2962063
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Lin ZJ, 2020, IEEE T IMAGE PROCESS, V29, P3750, DOI 10.1109/TIP.2020.2965987
   Liu DZ, 2021, PROC CVPR IEEE, P11230, DOI 10.1109/CVPR46437.2021.01108
   Liu Daizong, 2020, P 28 INT C COMP LING, P1841
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu QY, 2020, AAAI CONF ARTIF INTE, V34, P11612
   Liu W., 2019, Neural Inf. Process. Syst., P536
   Liu XF, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441577
   Liu Y, 2022, IEEE T CIRC SYST VID, V32, P5, DOI 10.1109/TCSVT.2021.3075607
   Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144
   Mei T, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487269
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Ning K, 2021, IEEE T IMAGE PROCESS, V30, P2538, DOI 10.1109/TIP.2021.3052086
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Qu XY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4280, DOI 10.1145/3394171.3414053
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rodriguez Cristian, 2020, WACV, P2464
   Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P601, DOI 10.1007/978-3-030-58565-5_36
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun XY, 2021, IEEE T IMAGE PROCESS, V30, P5589, DOI 10.1109/TIP.2021.3086591
   Tan RB, 2021, IEEE WINT CONF APPL, P2082, DOI 10.1109/WACV48630.2021.00213
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wu J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1283, DOI 10.1145/3394171.3413862
   Wu J, 2020, AAAI CONF ARTIF INTE, V34, P12386
   Xiao SN, 2021, AAAI CONF ARTIF INTE, V35, P2986
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Yang XS, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2962719
   Yang X, 2022, IEEE T IMAGE PROCESS, V31, P1204, DOI 10.1109/TIP.2022.3140611
   Yuan Y., 2020, IEEETrans. Pattern Anal. Mach. Intell., V44, P2725
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, CVPR
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang D, 2020, PROC CVPR IEEE, P3881, DOI 10.1109/CVPR42600.2020.00394
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang Hao, 2020, P 58 ANN M ASS COMPU, P6543
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang SY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1230, DOI 10.1145/3343031.3350879
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhang ZJ, 2021, IEEE T MULTIMEDIA, V23, P3306, DOI 10.1109/TMM.2020.3023339
NR 80
TC 7
Z9 7
U1 3
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 55
DI 10.1145/3543857
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shao, X
   Shen, Y
   Zhang, L
   Zhao, SJ
   Zhu, DD
   Zhou, YC
AF Shao, Xuan
   Shen, Ying
   Zhang, Lin
   Zhao, Shengjie
   Zhu, Dandan
   Zhou, Yicong
TI SLAM for Indoor Parking: A Comprehensive Benchmark Dataset and a Tightly
   Coupled Semantic Framework
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Autonomous indoor parking; benchmark dataset; groundtruth trajectory
   acquisition; Electronic Total Station; semantic SLAM
ID URBAN DATASET; O(N) SOLUTION; ROBUST; TRANSFORMATION; ODOMETRY
AB For the task of autonomous indoor parking, various Visual-Inertial Simultaneous Localization And Mapping (SLAM) systems are expected to achieve comparable results with the benefit of complementary effects of visual cameras and the Inertial Measurement Units. To compare these competing SLAM systems, it is necessary to have publicly available datasets, offering an objective way to demonstrate the pros/cons of each SLAM system. However, the availability of such high-quality datasets is surprisingly limited due to the profound challenge of the groundtruth trajectory acquisition in the Global Positioning Satellite denied indoor parking environments. In this article, we establish BeVIS, a large-scale Benchmark dataset with Visual (front-view), I nertial and Surround-view sensors for evaluating the performance of SLAM systems developed for autonomous indoor parking, which is the first of its kind where both the raw data and the groundtruth trajectories are available. In BeVIS, the groundtruth trajectories are obtained by tracking artificial landmarks scattered in the indoor parking environments, whose coordinates are recorded in a surveying manner with a high-precision Electronic Total Station. Moreover, the groundtruth trajectories are comprehensively evaluated in terms of two respects, the reprojection error and the pose volatility, respectively. Apart from BeVIS, we propose a novel tightly coupled semantic SLAM framework, namely VISSLAM-2, leveraging Visual (frontview), I nertial, and Surround-view sensor modalities, specially for the task of autonomous indoor parking. It is the first work attempting to provide a general form to model various semantic objects on the ground. Experiments on BeVIS demonstrate the effectiveness of the proposed VISSLAM-2. Our benchmark dataset BeVIS is publicly available at https://shaoxuan92.github.io/BeVIS.
C1 [Shao, Xuan; Shen, Ying; Zhang, Lin; Zhao, Shengjie] Tongji Univ, Sch Software Engn, 4800 Cao An Highway, Shanghai 201804, Peoples R China.
   [Zhu, Dandan] Shanghai Jiao Tong Univ, Artificial Intelligence Inst, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Taipa Univ Rd, Macau, Peoples R China.
C3 Tongji University; Shanghai Jiao Tong University; University of Macau
RP Shen, Y; Zhang, L (corresponding author), Tongji Univ, Sch Software Engn, 4800 Cao An Highway, Shanghai 201804, Peoples R China.
EM 1810553@tongji.edu.cn; yingshen@tongji.edu.cn; cslinzhang@tongji.edu.cn;
   shengjiezhao@tongji.edu.cn; ddz@sjtu.edu.cn; yicongzhou@um.edu.mo
RI Zhou, Yicong/A-8017-2009
OI Zhou, Yicong/0000-0002-4487-6384
FU National Natural Science Foundation of China [61973235, 61972285,
   61936014]; Natural Science Foundation of Shanghai [19ZR1461300];
   Shanghai Science and Technology Innovation Plan [20510760400]; Dawn
   Program of Shanghai Municipal Education Commission [21SG23]; Shanghai
   Municipal Science and Technology Major Project [2021SHZDZX0100];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61973235, 61972285, and 61936014, in
   part by the Natural Science Foundation of Shanghai under Grant
   19ZR1461300, in part by the Shanghai Science and Technology Innovation
   Plan under Grant 20510760400, in part by the Dawn Program of Shanghai
   Municipal Education Commission under Grant 21SG23, in part by the
   Shanghai Municipal Science and Technology Major Project under Grant
   2021SHZDZX0100, and in part by the Fundamental Research Funds for the
   Central Universities.
CR Abdel-Aziz YI, 2015, PHOTOGRAMM ENG REM S, V81, P103, DOI 10.14358/PERS.81.2.103
   [Anonymous], 2017, INTRO INERTIAL NAVIG
   Awange JL, 2010, ALGEBRAIC GEODESY AND GEOINFORMATICS, SECOND EDITION, P249, DOI 10.1007/978-3-642-12124-1_14
   Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039
   Blanco-Claraco JL, 2014, INT J ROBOT RES, V33, P207, DOI 10.1177/0278364913507326
   Bloesch M, 2015, IEEE INT C INT ROBOT, P298, DOI 10.1109/IROS.2015.7353389
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Civera J, 2011, IEEE INT C INT ROBOT, P1277, DOI 10.1109/IROS.2011.6048293
   Dame A, 2013, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2013.170
   Engel J, 2016, Arxiv, DOI arXiv:1607.02555
   Frost D, 2018, IEEE T ROBOT, V34, P736, DOI 10.1109/TRO.2018.2820722
   Furgale P, 2013, IEEE INT C INT ROBOT, P1280, DOI 10.1109/IROS.2013.6696514
   Gao XS, 2003, IEEE T PATTERN ANAL, V25, P930, DOI 10.1109/TPAMI.2003.1217599
   Gaspar AR, 2018, ROBOT AUTON SYST, V109, P59, DOI 10.1016/j.robot.2018.08.004
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Jeong J, 2019, INT J ROBOT RES, V38, P642, DOI 10.1177/0278364919843996
   Judd KM, 2019, IEEE ROBOT AUTOM LET, V4, P800, DOI 10.1109/LRA.2019.2892656
   Kaneko M, 2018, IEEE COMPUT SOC CONF, P371, DOI 10.1109/CVPRW.2018.00063
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813
   Moreno-Noguer F, 2007, IEEE I CONF COMP VIS, P2252
   Munguía R, 2016, INT J ADV ROBOT SYST, V13, DOI 10.5772/64011
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2017, IEEE ROBOT AUTOM LET, V2, P796, DOI 10.1109/LRA.2017.2653359
   Nicholson L, 2019, IEEE ROBOT AUTOM LET, V4, P1, DOI 10.1109/LRA.2018.2866205
   Olson E, 2011, IEEE INT CONF ROBOT
   Penate-Sanchez A, 2013, IEEE T PATTERN ANAL, V35, P2387, DOI 10.1109/TPAMI.2013.36
   Qin T, 2019, Arxiv, DOI arXiv:1901.03642
   Qin T, 2018, IEEE INT C INT ROBOT, P3662, DOI 10.1109/IROS.2018.8593603
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Richardson A, 2013, IEEE INT C INT ROBOT, P1814, DOI 10.1109/IROS.2013.6696595
   Salas-Moreno RF, 2013, PROC CVPR IEEE, P1352, DOI 10.1109/CVPR.2013.178
   Schubert D, 2018, IEEE INT C INT ROBOT, P1680, DOI 10.1109/IROS.2018.8593419
   Shao X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2691, DOI 10.1145/3394171.3413867
   Shao X, 2019, IEEE INT CON MULTI, P1486, DOI 10.1109/ICME.2019.00257
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sünderhauf N, 2017, IEEE INT C INT ROBOT, P5079, DOI 10.1109/IROS.2017.8206392
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   Wang CX, 2014, ADV MECH ENG, DOI 10.1155/2014/847406
   Wang J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4193, DOI 10.1109/IROS.2016.7759617
   Weiss S, 2011, IEEE INT CONF ROBOT
   Yang SC, 2019, IEEE T ROBOT, V35, P925, DOI 10.1109/TRO.2019.2909168
   Yang SC, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1222, DOI 10.1109/IROS.2016.7759204
   Zhang L, 2018, IEEE T IMAGE PROCESS, V27, P5350, DOI 10.1109/TIP.2018.2857407
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao JQ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19010161
NR 53
TC 0
Z9 0
U1 5
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 1
DI 10.1145/3510856
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400001
DA 2024-07-18
ER

PT J
AU Wu, DY
   Dai, Q
   Li, B
   Wang, WP
AF Wu, Dayan
   Dai, Qi
   Li, Bo
   Wang, Weiping
TI Deep Uncoupled Discrete Hashing via Similarity Matrix Decomposition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep hashing; similarity-transfer; large-scale image retrieval
AB Hashing has been drawing increasing attention in the task of large-scale image retrieval owing to its storage and computation efficiency, especially the recent asymmetric deep hashing methods. These approaches treat the query and database in an asymmetric way and can take full advantage of the whole training data. Though it has achieved state-of-the-art performance, asymmetric deep hashing methods still suffer from the large quantization error and efficiency problem on large-scale datasets due to the tight coupling between the query and database. In this article, we propose a novel asymmetric hashing method, called Deep Uncoupled Discrete Hashing (DUDH), for large-scale approximate nearest neighbor search. Instead of directly preserving the similarity between the query and database, DUDH first exploits a small similarity-transfer image set to transfer the underlying semantic structures from the database to the query and implicitly keep the desired similarity. As a result, the large similarity matrix is decomposed into two relatively small ones and the query is decoupled from the database. Then both database codes and similarity-transfer codes are directly learned during optimization. The quantization error of DUDH only exists in the process of preserving similarity between the query and similarity-transfer set. By uncoupling the query from the database, the training cost of optimizing the CNN model for the query is no longer related to the size of the database. Besides, to further accelerate the training process, we propose to optimize the similarity-transfer codes with a constant-approximation solution. In doing so, the training cost of optimizing similarity-transfer codes can be almost ignored. Extensive experiments on four widely used image retrieval benchmarks demonstrate that DUDH can achieve state-of-the-art retrieval performance with remarkable training cost reduction (30%-50% relative).
C1 [Wu, Dayan; Li, Bo; Wang, Weiping] Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China.
   [Dai, Qi] Microsoft Res Asia, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Microsoft; Microsoft Research Asia
RP Li, B (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China.
EM wudayan@iie.ac.cn; qid@micorsoft.com; libo@iie.ac.cn;
   wangweiping@iie.ac.cn
RI Dai, Qi/JXM-6895-2024
OI Dai, Qi/0000-0002-4693-2968; Li, Bo/0000-0001-6709-0942
FU National Natural Science Foundation of China [62106258, 62006242]
FX This work was supported by the National Natural Science Foundation of
   China (No. 62106258 and No. 62006242).
CR [Anonymous], 2009, NEURIPS
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen YD, 2019, IEEE I CONF COMP VIS, P9795, DOI 10.1109/ICCV.2019.00989
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Deng C, 2020, IEEE T NEUR NET LEAR, V31, P2189, DOI 10.1109/TNNLS.2019.2929068
   Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Dizaji KG, 2018, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR.2018.00386
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Gu GH, 2020, NEUROCOMPUTING, V385, P348, DOI 10.1016/j.neucom.2019.12.096
   He T, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2477
   He XY, 2019, PROC CVPR IEEE, P2834, DOI 10.1109/CVPR.2019.00295
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Jiang QY, 2019, IEEE T IMAGE PROCESS, V28, P3490, DOI 10.1109/TIP.2019.2897944
   Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jin S, 2020, IEEE T IMAGE PROCESS, V29, P5336, DOI 10.1109/TIP.2020.2971105
   Kang WC, 2016, AAAI CONF ARTIF INTE, P1230
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Li CX, 2019, IEEE T MULTIMEDIA, V21, P2863, DOI 10.1109/TMM.2019.2912714
   Li Q, 2017, ADV NEUR IN, V30
   Li SY, 2019, IEEE I CONF COMP VIS, P8211, DOI 10.1109/ICCV.2019.00830
   Li WJ, 2016, IJCAI, P1711
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Luo X, 2019, IEEE T IMAGE PROCESS, V28, P2962, DOI 10.1109/TIP.2019.2892703
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Nie XS, 2020, IEEE T KNOWL DATA EN, V32, P1951, DOI 10.1109/TKDE.2019.2913383
   Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P595, DOI 10.1145/3077136.3080767
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Tang JH, 2018, IEEE T NEUR NET LEAR, V29, P6154, DOI 10.1109/TNNLS.2018.2816743
   Do TT, 2016, LECT NOTES COMPUT SC, V9909, P219, DOI 10.1007/978-3-319-46454-1_14
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Wu DY, 2019, PROC CVPR IEEE, P9061, DOI 10.1109/CVPR.2019.00928
   Wu L, 2018, COMPUT VIS IMAGE UND, V167, P63, DOI 10.1016/j.cviu.2017.11.009
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Yan XY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3238
   Yang EK, 2019, PROC CVPR IEEE, P2941, DOI 10.1109/CVPR.2019.00306
   Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zhang HF, 2018, IEEE T IMAGE PROCESS, V27, P1626, DOI 10.1109/TIP.2017.2781422
   Zhang J, 2019, IEEE T CIRC SYST VID, V29, P212, DOI 10.1109/TCSVT.2017.2771332
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhou X, 2020, IEEE T CYBERNETICS, V50, P1460, DOI 10.1109/TCYB.2018.2883970
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
   Zhuang BH, 2016, PROC CVPR IEEE, P5955, DOI 10.1109/CVPR.2016.641
NR 61
TC 6
Z9 6
U1 2
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2023
VL 19
IS 1
AR 22
DI 10.1145/3524021
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4GM
UT WOS:000927167400022
OA Bronze
DA 2024-07-18
ER

PT J
AU Li, YR
   Wang, ZF
   Yu, J
AF Li, Yongrui
   Wang, Zengfu
   Yu, Jun
TI Densely Enhanced Semantic Network for Conversation System in Social
   Media
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Social media; question answering; multimedia application; deep learning
AB The human-computer conversation system is a significant application in the field of multimedia. To select an appropriate response, retrieval-based systems model the matching between the dialogue history and response candidates. However, most of the existing methods cannot fully capture and utilize varied matching patterns, which may degrade the performance of the systems. To address the issue, a densely enhanced semantic network (DESN) is proposed in our work. Given a multi-turn dialogue history and a response candidate, DESN first constructs the semantic representations of sentences from the word perspective, the sentence perspective, and the dialogue perspective. In particular, the dialogue perspective is a novel one introduced in our work. The dependencies between a single sentence and the whole dialogue are modeled from the dialogue perspective. Then, the response candidate and each utterance in the dialogue history are made to interact with each other. The varied matching patterns are captured for each utterance-response pair by using a dense matching module. The matching patterns of all the utterance-response pairs are accumulated in chronological order to calculate the matching degree between the dialogue history and the response. The responses in the candidate pool are ranked with the matching degree, thereby returning the most appropriate candidate. Our model is evaluated on the benchmark datasets. The experimental results prove that our model achieves significant and consistent improvement when compared with other baselines.
C1 [Li, Yongrui; Wang, Zengfu; Yu, Jun] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Peoples R China.
   [Wang, Zengfu] Chinese Acad Sci, Inst Intelligent Machines, Beijing, Peoples R China.
   [Li, Yongrui; Wang, Zengfu; Yu, Jun] Univ Sci & Technol China, 96 JinZhai Rd, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Wang, ZF; Yu, J (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Peoples R China.; Wang, ZF (corresponding author), Chinese Acad Sci, Inst Intelligent Machines, Beijing, Peoples R China.; Wang, ZF; Yu, J (corresponding author), Univ Sci & Technol China, 96 JinZhai Rd, Hefei 230026, Anhui, Peoples R China.
EM lyr123@mail.ustc.edu.cn; zfwang@ustc.edu.cn; harryjun@ustc.edu.cn
RI Wang, Zengfu/C-8618-2017
OI Wang, Zengfu/0000-0003-0424-3010
FU Strategic Priority Research Program of the Chinese Academy of Sciences
   [XDC08020400]
FX This work was supported by the Strategic Priority Research Program of
   the Chinese Academy of Sciences (XDC08020400).
CR [Anonymous], CoRR abs/1511.07122
   [Anonymous], 2015, ARXIV151208849
   [Anonymous], 2016, ARXIV161201627
   [Anonymous], 2018, ARXIV180604441
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2016, PRUNING CONVOLUTIONA
   [Anonymous], 2016, P 25 INT JOINT C ART
   [Anonymous], 2016, P 2016 C EMP METH NA, DOI [DOI 10.18653/V1/D16-1036, 10.18653/v1/D16-1036]
   Chaudhary C, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3375786
   Chen HL, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P110, DOI 10.1145/3219819.3219928
   Chen WQ, 2018, CHINESE J CANCER RES, V30, P1, DOI 10.21147/j.issn.1000-9604.2018.01.01
   Cui C, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P445, DOI 10.1145/3331184.3331226
   Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121
   Dang-Nguyen DT, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3103613
   Gu JC, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P2321, DOI 10.1145/3357384.3358140
   Hongshen Chen, 2017, ACM SIGKDD Explorations Newsletter, V19, P25, DOI 10.1145/3166054.3166058
   Hu J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P456, DOI 10.1145/3240508.3240626
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ji Zongcheng, 2014, ARXIV14086988
   Jin XS, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1403, DOI 10.1145/3269206.3271683
   Kadlec R., 2015, ARXIV151003753
   Kang GC, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2024
   King DB, 2015, ACS SYM SER, V1214, P1
   Li H, 2019, J VIS COMMUN IMAGE R, V64, DOI 10.1016/j.jvcir.2019.102611
   Li JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P678
   Li Jiwei, 2017, P 2017 C EMP METH NA, P2157, DOI DOI 10.18653/V1/D17-1230
   Li K, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152126
   Li L, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2020.102415
   Li Q, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3300938
   Li YR, 2019, IEEE DATA MINING, P1186, DOI 10.1109/ICDM.2019.00145
   Liao LZ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P801, DOI 10.1145/3240508.3240605
   Lin J., 2018, ARXIV180808561
   Lin M., 2013, ARXIV13124400
   Liu YH, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2769
   Liu ZG, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3187011
   Lowe Ryan, 2015, P 16 ANN M SPECIAL I, V3, P285
   Lu Junyu, 2020, P 43 INT ACM SIGIR C, P1805
   Luo L., 2018, ARXIV180808795
   Ma W., 2019, ARXIV190910666
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Nie LQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1098, DOI 10.1145/3343031.3350923
   Pang L, 2016, AAAI CONF ARTIF INTE, P2793
   Peng DL, 2020, KNOWL-BASED SYST, V192, DOI 10.1016/j.knosys.2019.105319
   Saha A, 2018, AAAI CONF ARTIF INTE, P696
   Seo M., 2016, Bidirectional attention flow for machine comprehension
   ShuoWang Dan Guo, 2019, ACM T MULTIM COMPUT, V15, P1
   Shuster Kurt, 2018, ARXIV181100945
   Song Yiping, 2016, Two are better than one: An ensemble of retrievaland generation-based dialog systems
   Tan Ming, 2015, Lstm-based deep learning models for non-factoid answer selection
   Tao CY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1
   Tao CY, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P267, DOI 10.1145/3289600.3290985
   Tay Y, 2018, AAAI CONF ARTIF INTE, P5512
   Tay Y, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2299, DOI 10.1145/3219819.3220048
   Tiwari A, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115433
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan SX, 2016, AAAI CONF ARTIF INTE, P2835
   Wang H, 2018, IEEE DATA MINING, P557, DOI 10.1109/ICDM.2018.00071
   Wang S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300258
   Wu Q, 2018, PROC CVPR IEEE, P6106, DOI 10.1109/CVPR.2018.00639
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Wu Y, 2018, AAAI CONF ARTIF INTE, P5594
   Yan R, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4525
   Yan R, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P55, DOI 10.1145/2911451.2911542
   Yu DF, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3316767
   Zhang W., 2018, P 27 INT C COMP LING, P2437
   Zhang Zhuosheng, 2018, P 27 INT C COMP LING, P3740
   Zhao Hai, 2006, P 5 SIGHAN WORKSH CH, P162
   Zho XY, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1118
NR 69
TC 0
Z9 0
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 101
DI 10.1145/3501799
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600012
DA 2024-07-18
ER

PT J
AU Singh, KN
   Singh, AK
AF Singh, Kedar Nath
   Singh, Amit Kumar
TI Towards Integrating Image Encryption with Compression: A Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Encryption; digital images; compression; joint encryption compression
ID IDENTITY AUTHENTICATION; ALGORITHM; SCHEME; SYSTEM; ROBUST; CHAOS;
   WATERMARKING
AB As digital images are consistently generated and transmitted online, the unauthorized utilization of these images is an increasing concern that has a significant impact on both security and privacy issues; additionally, the representation of digital images requires a large amount of data. In recent years, an image compression scheme has been widely considered; such a scheme saves on hardware storage space and lowers both the transmission time and bandwidth demand for various potential applications. In this article, we review the various approaches taken to consider joint encryption and compression, assessing both their merits and their limitations. In addition to the survey, we also briefly introduce the most interesting and most often utilized applications of image encryption and evaluation metrics, providing an overview of the various kinds of image encryption schemes available. The contribution made by these approaches is then summarized and compared, offering a consideration of the different technical perspectives. Lastly, we highlight the recent challenges and some potential research directions that could fill the gaps in these domains for both researchers and developers.
C1 [Singh, Kedar Nath] Noida Inst Engn & Technol, Dept CSE, Greater Noida, UP, India.
   [Singh, Kedar Nath] Noida Inst Engn & Thchnol, Dept Comp Sci & Engn, Greater Noida 201306, Uttar Pradesh, India.
   [Singh, Kedar Nath] Noida Inst Engn & Thchnol, Dept CSE, Greater Noida 201306, Uttar Pradesh, India.
   [Singh, Amit Kumar] Natl Inst Technol Patna, Dept Comp Sci & Engn, Patna 800005, Bihar, India.
C3 Noida Institute of Engineering & Technology; Noida Institute of
   Engineering & Technology; Noida Institute of Engineering & Technology;
   National Institute of Technology (NIT System); National Institute of
   Technology Patna
RP Singh, AK (corresponding author), Natl Inst Technol Patna, Dept Comp Sci & Engn, Patna 800005, Bihar, India.
EM knsinghait@gmail.com; amit.singh@nitp.ac.in
RI Singh, Kedar Nath/HKV-0830-2023; Singh, Amit Kumar/D-1300-2015
OI Singh, Kedar Nath/0000-0001-7857-5945; Singh, Amit
   Kumar/0000-0001-7359-2068
CR Abbas NA, 2016, EGYPT INFORM J, V17, P139, DOI 10.1016/j.eij.2015.10.001
   Alqaralleh Bassam A. Y., 2024, Personal and Ubiquitous Computing, V28, P17, DOI 10.1007/s00779-021-01543-2
   Anand A, 2020, IEEE MULTIMEDIA, V27, P133, DOI 10.1109/MMUL.2020.2993269
   Anwar S, 2019, MULTIMED TOOLS APPL, V78, P27569, DOI 10.1007/s11042-019-07852-2
   Asgari-Chenaghlu M, 2021, INFORM SCIENCES, V542, P212, DOI 10.1016/j.ins.2020.07.007
   Cambareri V, 2015, IEEE T INF FOREN SEC, V10, P2182, DOI 10.1109/TIFS.2015.2450676
   Chai XL, 2020, SIGNAL PROCESS, V176, DOI 10.1016/j.sigpro.2020.107684
   Chen H, 2013, OPT LASER ENG, V51, P768, DOI 10.1016/j.optlaseng.2013.01.016
   Chuman T, 2019, IEEE T INF FOREN SEC, V14, P1515, DOI 10.1109/TIFS.2018.2881677
   Dang PP, 2000, IEEE T CONSUM ELECTR, V46, P395, DOI 10.1109/30.883383
   Duseja T, 2019, MULTIMED TOOLS APPL, V78, P16727, DOI 10.1007/s11042-018-7023-0
   Gan ZH, 2019, NEURAL COMPUT APPL, V31, P7111, DOI 10.1007/s00521-018-3541-y
   Gan ZH, 2021, NEURAL COMPUT APPL, V33, P12845, DOI 10.1007/s00521-021-05937-4
   Ghaffari A, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-79747-4
   Guan MM, 2019, IET IMAGE PROCESS, V13, P1535, DOI 10.1049/iet-ipr.2019.0051
   Gupta BB, 2018, MULTIMED TOOLS APPL, V77, P9203, DOI 10.1007/s11042-017-5301-x
   Haddad S, 2020, IEEE T INF FOREN SEC, V15, P2556, DOI 10.1109/TIFS.2020.2972159
   Hu HY, 2021, IEEE ACCESS, V9, P22141, DOI 10.1109/ACCESS.2021.3054842
   Hu JK, 2009, J NETW COMPUT APPL, V32, P788, DOI 10.1016/j.jnca.2009.02.009
   Huang W, 2021, IEEE ACCESS, V9, P41704, DOI 10.1109/ACCESS.2021.3065453
   Huang XL, 2015, SECUR COMMUN NETW, V8, P3659, DOI 10.1002/sec.1289
   Huo DM, 2020, APPL PHYS B-LASERS O, V126, DOI 10.1007/s00340-020-7397-3
   Hussain AJ, 2018, NEUROCOMPUTING, V300, P44, DOI 10.1016/j.neucom.2018.02.094
   Jayasankar U, 2021, J KING SAUD UNIV-COM, V33, P119, DOI 10.1016/j.jksuci.2018.05.006
   Jiang DH, 2021, IET IMAGE PROCESS, V15, P3698, DOI 10.1049/ipr2.12237
   Kumar M, 2017, DIGIT SIGNAL PROCESS, V60, P81, DOI 10.1016/j.dsp.2016.08.011
   Kumar S, 2019, MED BIOL ENG COMPUT, V57, P2517, DOI 10.1007/s11517-019-02037-3
   Kurihara K, 2015, IEICE T FUND ELECTR, VE98A, P2238, DOI 10.1587/transfun.E98.A.2238
   Li PY, 2018, IEEE T MULTIMEDIA, V20, P1960, DOI 10.1109/TMM.2017.2786860
   Li PY, 2017, J VIS COMMUN IMAGE R, V44, P61, DOI 10.1016/j.jvcir.2017.01.021
   Li XY, 2020, IEEE ACCESS, V8, P211676, DOI 10.1109/ACCESS.2020.3039643
   Li YH, 2021, CONCURR COMP-PRACT E, V33, DOI 10.1002/cpe.5182
   Li YH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2903717
   Lidong L, 2020, IEEE ACCESS, V8, P210382, DOI 10.1109/ACCESS.2020.3039891
   Lima JB, 2015, SIGNAL PROCESS-IMAGE, V35, P1, DOI 10.1016/j.image.2015.03.005
   Liu H, 2019, IEEE ACCESS, V7, P65450, DOI 10.1109/ACCESS.2019.2917498
   Mahato S, 2019, J INF SECUR APPL, V47, P275, DOI 10.1016/j.jisa.2019.05.013
   Mahendiran N, 2021, SN Comput Sci, V2, P1
   del Rey AM, 2015, LOG J IGPL, V23, P485, DOI 10.1093/jigpal/jzv013
   Mehra I, 2015, OPT COMMUN, V354, P344, DOI 10.1016/j.optcom.2015.06.015
   Minemura K, 2017, MULTIMED TOOLS APPL, V76, P6709, DOI 10.1007/s11042-016-3338-x
   Mohamed FK, 2014, ENG SCI TECHNOL, V17, P85, DOI 10.1016/j.jestch.2014.04.001
   Muhammad K, 2018, IEEE T IND INFORM, V14, P3679, DOI 10.1109/TII.2018.2791944
   Niu Y, 2020, IEEE ACCESS, V8, P22082, DOI 10.1109/ACCESS.2020.2970103
   Osman M, 2021, WILD INTERESTING FAC
   Ponuma R, 2018, MULTIMED TOOLS APPL, V77, P19209, DOI 10.1007/s11042-017-5378-2
   Preishuber M, 2018, IEEE T INF FOREN SEC, V13, P2137, DOI 10.1109/TIFS.2018.2812080
   Qin C, 2019, IEEE T CIRC SYST VID, V29, P3341, DOI 10.1109/TCSVT.2018.2878026
   Quicksprout, 2019, INCR TWITT ENG 324
   Schneier B., 2015, APPL CRYPTOGRAPHY, VSecond
   Shafique A, 2020, WIRELESS PERS COMMUN, V115, P2243, DOI 10.1007/s11277-020-07680-w
   Singh A., 2019, ArXiv preprint arXiv:1904.07854
   Singh AK, 2021, ACM T INTERNET TECHN, V21, DOI 10.1145/3414474
   Singh Amit Kumar, 2020, IT Professional, V22, P45, DOI 10.1109/MITP.2019.2961898
   Singh H, 2016, 3D RES, V7, DOI 10.1007/s13319-016-0110-y
   Smith C, 2021, FLICKR STAT USER COU
   Socek D, 2007, MULTIMEDIA SYST, V13, P191, DOI 10.1007/s00530-007-0083-z
   Song Jaehun, 2021, OPT COMMUN, V485
   Srivastava G, 2020, J INTELL FUZZY SYST, V38, P2561, DOI 10.3233/JIFS-179543
   Suguna T, 2021, WIRELESS PERS COMMUN, V116, P2239, DOI 10.1007/s11277-020-07789-y
   Sun WW, 2021, IEEE T CIRC SYST VID, V31, P1208, DOI 10.1109/TCSVT.2020.2998476
   Sun WQ, 2018, J EUR OPT SOC-RAPID, V14, DOI 10.1186/s41476-018-0096-6
   Sun YY, 2015, IET IMAGE PROCESS, V9, P173, DOI 10.1049/iet-ipr.2014.0224
   Swaminathan A, 2006, IEEE T INF FOREN SEC, V1, P215, DOI 10.1109/TIFS.2006.873601
   Systrom K, 2021, INSTAGRAM NUMBERS ST
   Thoms GRW, 2019, IEEE ACCESS, V7, P158697, DOI 10.1109/ACCESS.2019.2950007
   Tong XJ, 2016, NONLINEAR DYNAM, V84, P2333, DOI 10.1007/s11071-016-2648-x
   Tong XJ, 2013, NONLINEAR DYNAM, V72, P229, DOI 10.1007/s11071-012-0707-5
   Wang CT, 2020, IEEE ACCESS, V8, P11328, DOI 10.1109/ACCESS.2019.2963170
   Wang CT, 2015, SIGNAL PROCESS-IMAGE, V39, P141, DOI 10.1016/j.image.2015.09.009
   Wang QZ, 2018, MULTIMED TOOLS APPL, V77, P1715, DOI 10.1007/s11042-017-4349-y
   Wang QZ, 2016, BIOMED ENG ONLINE, V15, DOI 10.1186/s12938-016-0239-1
   Wu Y, 2011, Cyber J Multidiscip J Sci Technol J Sel Areas Telecommun (JSAT), V1, P31
   Xu YY, 2014, J VIS COMMUN IMAGE R, V25, P805, DOI 10.1016/j.jvcir.2014.01.005
   Yang FF, 2020, MULTIMED TOOLS APPL, V79, P19963, DOI 10.1007/s11042-020-08821-w
   Yang FF, 2019, IEEE ACCESS, V7, P58751, DOI 10.1109/ACCESS.2019.2914722
   Yao LL, 2017, OPT LASER ENG, V89, P72, DOI 10.1016/j.optlaseng.2016.06.006
   Yu CZ, 2019, IET IMAGE PROCESS, V13, P2224, DOI 10.1049/iet-ipr.2018.5912
   Zhang B, 2021, IEEE T MULTIMEDIA, V23, P2656, DOI 10.1109/TMM.2020.3014489
   Zhang M, 2020, IEEE ACCESS, V8, P40838, DOI 10.1109/ACCESS.2020.2976798
   Zhang XQ, 2012, OPT COMMUN, V285, P1736, DOI 10.1016/j.optcom.2011.12.023
   Zhang XP, 2014, IEEE T MULTIMEDIA, V16, P1327, DOI 10.1109/TMM.2014.2315974
   Zhang Y, 2017, OPT COMMUN, V392, P223, DOI 10.1016/j.optcom.2017.01.061
   Zhang YS, 2016, IEEE ACCESS, V4, P2507, DOI 10.1109/ACCESS.2016.2569421
   Zhou JT, 2014, IEEE T INF FOREN SEC, V9, P39, DOI 10.1109/TIFS.2013.2291625
   Zhou NR, 2016, OPT LASER TECHNOL, V82, P121, DOI 10.1016/j.optlastec.2016.02.018
   Zhou XQ, 2001, IEEE T MED IMAGING, V20, P784, DOI 10.1109/42.938246
   Zhu HG, 2013, SIGNAL PROCESS-IMAGE, V28, P670, DOI 10.1016/j.image.2013.02.004
   Zhu SQ, 2018, IEEE ACCESS, V6, P67095, DOI 10.1109/ACCESS.2018.2874336
NR 89
TC 42
Z9 42
U1 8
U2 65
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 89
DI 10.1145/3498342
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600023
DA 2024-07-18
ER

PT J
AU Francis, J
   Baburaj, M
   George, SN
AF Francis, Jobin
   Baburaj, M.
   George, Sudhish N.
TI An <i>l</i><sub>1/2</sub> and Graph Regularized Subspace Clustering
   Method for Robust Image Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image segmentation; subspace clustering; sparsity; constrained
   optimization
ID L-1/2 REGULARIZATION; ALGORITHM
AB Segmenting meaningful visual structures from an image is a fundamental and most-addressed problem in image analysis algorithms. However, among factors such as diverse visual patterns, noise, complex backgrounds, and similar textures present in foreground and background, image segmentation still stands as a challenging research problem. In this article, the proposed method employs an unsupervised method that addresses image segmentation as subspace clustering of image feature vectors. Initially, an image is partitioned into a set of homogeneous regions called superpixels, from which Local Spectral Histogram features are computed. Subsequently, a feature data matrix is created whereupon subspace clustering methodology is applied. A single-stage optimization model is formulated with enhanced segmentation capabilities by the combined action of l(1/2) and l(2) norm minimization. Robustness of l(1/2) regularization toward both the noise and overestimation of sparsity provides simultaneous noise robustness and better subspace selection, respectively. While l(2) norm facilitates grouping effect. Hence, the designed optimization model ensures an improved sparse solution and a sparse representation matrix with an accurate block diagonal structure, which thereby favours getting properly segmented images. Then, experimental results of the proposed method are compared with the state-of-art algorithms. Results demonstrate the improved performance of our method over the state-of-art algorithms.
C1 [Francis, Jobin; George, Sudhish N.] Natl Inst Technol, Dept Elect & Commun Engn, Calicut 673601, Kerala, India.
   [Baburaj, M.] Govt Engn Coll Kozhikode, Dept Elect & Instrumentat Engn, Calicut 673005, Kerala, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Calicut; Government Engineering College Kozhikode
RP Francis, J (corresponding author), Natl Inst Technol, Dept Elect & Commun Engn, Calicut 673601, Kerala, India.
EM jobinkapyarumalayil@gmail.com; baburajmadathil@gmail.com;
   sudhish@nitc.ac.in
RI Madathil, Baburaj/T-2001-2019
OI Madathil, Baburaj/0000-0003-3151-9270; FRANCIS,
   JOBIN/0000-0003-2666-3737
CR Belkin M, 2002, ADV NEUR IN, V14, P585
   Bouwmans T, 2018, P IEEE, V106, P1427, DOI 10.1109/JPROC.2018.2853589
   Cheng B, 2011, IEEE I CONF COMP VIS, P2439, DOI 10.1109/ICCV.2011.6126528
   Choy SK, 2020, SIGNAL PROCESS, V171, DOI 10.1016/j.sigpro.2020.107483
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Francis J, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3399806
   Ghosh S, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329784
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Grave G. R., 2011, Adv. Neural Inf. Process.Syst., V24, P2187
   Ji Q, 2019, IEEE ACCESS, V7, P74122, DOI 10.1109/ACCESS.2019.2920592
   Jiao X, 2020, NEUROCOMPUTING, V409, P83, DOI 10.1016/j.neucom.2020.05.073
   Kim W, 2020, IEEE T IMAGE PROCESS, V29, P8055, DOI 10.1109/TIP.2020.3011269
   Lei T, 2019, IEEE T FUZZY SYST, V27, P1753, DOI 10.1109/TFUZZ.2018.2889018
   Li T, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2856058
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Li ZG, 2012, PROC CVPR IEEE, P789, DOI [10.1109/CVPR.2012.6247750, 10.1109/ISRA.2012.6219309]
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu JM, 2014, IEEE T IMAGE PROCESS, V23, P4022, DOI 10.1109/TIP.2014.2343458
   Lu CY, 2012, LECT NOTES COMPUT SC, V7578, P347, DOI 10.1007/978-3-642-33786-4_26
   Lu CY, 2013, IEEE I CONF COMP VIS, P1345, DOI 10.1109/ICCV.2013.170
   Lu XQ, 2013, IEEE T GEOSCI REMOTE, V51, P4009, DOI 10.1109/TGRS.2012.2226730
   Madathil B, 2021, SIGNAL IMAGE VIDEO P, V15, P341, DOI 10.1007/s11760-020-01752-x
   Pedroche F, 2016, CZECH MATH J, V66, P603, DOI 10.1007/s10587-016-0281-y
   Qian YT, 2011, IEEE T GEOSCI REMOTE, V49, P4282, DOI 10.1109/TGRS.2011.2144605
   Saxena A, 2017, NEUROCOMPUTING, V267, P664, DOI 10.1016/j.neucom.2017.06.053
   Seo J, 2019, IEEE INT CONF COMP V, P633, DOI 10.1109/ICCVW.2019.00077
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tang ZY, 2018, IEEE T MED IMAGING, V37, P2224, DOI 10.1109/TMI.2018.2824243
   Tom AJ, 2021, IEEE T CYBERNETICS, V51, P1004, DOI 10.1109/TCYB.2019.2921827
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Wang WW, 2018, IEEE T CIRC SYST VID, V28, P2612, DOI 10.1109/TCSVT.2017.2714861
   Wang WW, 2017, NEUROCOMPUTING, V267, P426, DOI 10.1016/j.neucom.2017.06.046
   Wang WH, 2015, IEEE J-STARS, V8, P2618, DOI 10.1109/JSTARS.2015.2401603
   Wang ZB, 2020, ARTIF INTELL REV, V53, P5637, DOI 10.1007/s10462-020-09830-9
   Wu T, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107145
   Xie Y, 2020, IEEE T CYBERNETICS, V50, P572, DOI 10.1109/TCYB.2018.2869789
   Xu C, 2013, SCI CHINA INFORM SCI, V56, DOI 10.1007/s11432-012-4679-3
   Xu JH, 2021, IEEE GEOSCI REMOTE S, V18, P871, DOI 10.1109/LGRS.2020.2985981
   Xu ZB, 2012, IEEE T NEUR NET LEAR, V23, P1013, DOI 10.1109/TNNLS.2012.2197412
   Yang CY, 2015, PR MACH LEARN RES, V37, P2463
   Yang SM, 2021, IEEE T CYBERNETICS, V51, P1981, DOI 10.1109/TCYB.2019.2895497
   Yin M, 2016, IEEE T PATTERN ANAL, V38, P504, DOI 10.1109/TPAMI.2015.2462360
   Zeng JS, 2014, IEEE T SIGNAL PROCES, V62, P2317, DOI 10.1109/TSP.2014.2309076
   Zhai H, 2019, IEEE T GEOSCI REMOTE, V57, P1723, DOI 10.1109/TGRS.2018.2868796
   Zhang HY, 2016, IEEE T GEOSCI REMOTE, V54, P3672, DOI 10.1109/TGRS.2016.2524557
   Zhang YJ, 1996, PATTERN RECOGN, V29, P1335, DOI 10.1016/0031-3203(95)00169-7
   Zhao F, 2020, IEEE T FUZZY SYST, V28, P1023, DOI 10.1109/TFUZZ.2020.2973121
   Zheng M, 2011, IEEE T IMAGE PROCESS, V20, P1327, DOI 10.1109/TIP.2010.2090535
   Zhu PF, 2017, PATTERN RECOGN, V66, P364, DOI 10.1016/j.patcog.2017.01.016
   Zhu XF, 2020, ACM T KNOWL DISCOV D, V14, DOI 10.1145/3396238
   Zohrizadeh F, 2018, IEEE WINT CONF APPL, P1470, DOI 10.1109/WACV.2018.00165
NR 51
TC 7
Z9 7
U1 1
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 53
DI 10.1145/3476514
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400010
DA 2024-07-18
ER

PT J
AU Holloman, AK
   Crawford, CS
AF Holloman, Amanda K.
   Crawford, Chris S.
TI Defining Scents: A Systematic Literature Review of Olfactory-based
   Computing Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Review
DE Olfaction; smell; human-computer interaction; multisensory multimedia;
   olfactory displays
ID ODOR; MEMORY; SENSE
AB The human sense of smell is a primal ability that has the potential to reveal unexplored relationships between user behaviors and technology. Humans use millions of olfactory receptor cells to observe the environment around them. Olfaction studies are gaining popularity with the progression of scent delivering (commercial and prototype) devices. This influx of research features various software and hardware designs. Additionally, previous studies have explored numerous target audiences and evaluation methodologies. This article presents a systematic review of pertinent literature that investigates olfactory-based computing (OBC) systems in the field of Human-Computer Interaction. Last, this article highlights state-of-the-art study/system designs, evaluation methods, and offers insights on ways to address current challenges/contributions relevant to OBC technologies.
C1 [Holloman, Amanda K.; Crawford, Chris S.] Univ Alabama, 3043 HM Corner,245 7th Ave, Tuscaloosa, AL 35487 USA.
C3 University of Alabama System; University of Alabama Tuscaloosa
RP Holloman, AK (corresponding author), Univ Alabama, 3043 HM Corner,245 7th Ave, Tuscaloosa, AL 35487 USA.
EM akholloman@crimson.ua.edu; crawford@cs.ua.edu
OI Holloman, Amanda/0000-0003-0114-3321; Crawford,
   Chris/0000-0003-3127-308X
FU HTIL
FX Thank you to HTIL for the support.
CR Ademoye OA, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2957753
   Ademoye OA, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487270
   Amores J, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P28, DOI 10.1145/3025453.3026004
   Arzi A, 2012, NAT NEUROSCI, V15, P1460, DOI 10.1038/nn.3193
   Barker S, 2003, PERCEPT MOTOR SKILL, V97, P1007, DOI 10.2466/PMS.97.7.1007-1010
   Brereton P, 2007, J SYST SOFTWARE, V80, P571, DOI 10.1016/j.jss.2006.07.009
   Chaudhury D, 2010, BEHAV NEUROSCI, V124, P490, DOI 10.1037/a0020293
   Choi Y, 2011, PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTER ENTERTAINMENT TECHNOLOGY (ACE 2011)
   Cornelius C, 2014, MOBISYS'14: PROCEEDINGS OF THE 12TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P55, DOI 10.1145/2594368.2594369
   Crowther Bosley, 1960, SCREEN OLFACTORY DEB
   Diego MA, 1998, INT J NEUROSCI, V96, P217, DOI 10.3109/00207459808986469
   Diekelmann S, 2010, NAT REV NEUROSCI, V11, P114, DOI 10.1038/nrn2762
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Dmitrenko D, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3313001
   Dmitrenko D, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P234, DOI 10.1145/3242969.3243015
   Dmitrenko D, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE SURFACES AND SPACES (ACM ISS 2017), P171, DOI 10.1145/3132272.3134121
   Dobbelstein D, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS (ISWC 17), P130, DOI 10.1145/3123021.3123035
   Ferdenzi C, 2016, PSYCHONEUROENDOCRINO, V66, P166, DOI 10.1016/j.psyneuen.2016.01.016
   Firestein S, 2001, NATURE, V413, P211, DOI 10.1038/35093026
   Frederica Goncalves, 2018, P 36 EUR C COGN ERG
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2379790.2379794
   Giordano Robert., Wordlist maker - list unique words, count total words
   Herr N., 2014, PROGNOSTICS HLTH MAN, P1, DOI [10.1145/2660579.2660584, DOI 10.1145/2660579.2660584]
   Huang Yiyuan, 2015, P 2015 VIRTUAL REALI, P1, DOI [10.1145/2806173.2806179, DOI 10.1145/2806173.2806179]
   Itou S, 2018, COMPANION OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES (IUI'18), DOI 10.1145/3180308.3180331
   Kay LM, 2011, CURR BIOL, V21, pR928, DOI 10.1016/j.cub.2011.10.008
   Kaye J."J."., 2004, INTERACTIONS, V11, P48, DOI DOI 10.1145/962342.964333
   Kim S.-J., 2017, P 11 INT C UB INF MA, P1
   Kroupi E, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2637287
   Kwan J, 2016, PROCEEDINGS OF THE TENTH ANNIVERSARY CONFERENCE ON TANGIBLE EMBEDDED AND EMBODIED INTERACTION (TEI16), P482, DOI 10.1145/2839462.2856537
   Lai MK, 2015, SIGDOC2015: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL CONFERENCE ON THE DESIGN OF COMMUNICATION, DOI 10.1145/2775441.2775483
   Larsson M, 2000, J GERONTOL B-PSYCHOL, V55, pP304, DOI 10.1093/geronb/55.5.P304
   Lötsch J, 2019, CHEM SENSES, V44, P11, DOI 10.1093/chemse/bjy067
   Maggioni E, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P45, DOI 10.1145/3242969.3242975
   Martzke JS, 1997, BIOL PSYCHIAT, V42, P721, DOI 10.1016/S0006-3223(96)00442-8
   Metatla O, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300608
   Mizuno K, 2004, ACTA PAEDIATR, V93, P1640, DOI 10.1080/08035250410023115
   Murray N, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3108243
   Murray N, 2016, ACM COMPUT SURV, V48, DOI 10.1145/2816454
   Murray N, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2637293
   Murray N, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540994
   Murray Niall., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys'13, P162, DOI [https://doi.org/10.1145/2483977.2483999, DOI 10.1145/2483977.2483999]
   Nakamoto T., 2013, Human Olfactory Displays and Interfaces: Odor Sensing and Presentation, Information Science reference
   Narumi T, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P93
   Noguchi D, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P83
   PCMAG, 2020, CAN SMELL O VIS SAV
   Ranasinghe N, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174151
   REDD WH, 1994, JMRI-J MAGN RESON IM, V4, P623, DOI 10.1002/jmri.1880040419
   Richard E., 2006, Virtual Reality, V10, P207, DOI DOI 10.1007/S10055-006-0040-8
   Saleme EB, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3319853
   Salminen K, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P73, DOI 10.1145/3242969.3242999
   Schablitzky S, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00045
   SCHLEIDT M, 1988, CHEM SENSES, V13, P279, DOI 10.1093/chemse/13.2.279
   Shaw E, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300856
   Sugimoto Sayumi., 2010, Proceedings of the 18th ACM International Conference on Multimedia, MM'10, P301, DOI DOI 10.1145/1873951.1873994
   Sullivan RM, 2015, FRONT BEHAV NEUROSCI, V9, DOI 10.3389/fnbeh.2015.00036
   Truong K., 2017, GetMobile: Mobile Comp. and Comm, V20, P8, DOI DOI 10.1145/3081016.3081020
   Warnock D., 2013, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, New York, NY, USA, P1091, DOI DOI 10.1145/2470654.2466139
   WEDEKIND C, 1995, P ROY SOC B-BIOL SCI, V260, P245, DOI 10.1098/rspb.1995.0087
   Weiser Mark., 1997, BEYOND CALCULATION, P75
   Wintersberger P, 2019, PROCEEDINGS OF IUI 2019, P538, DOI 10.1145/3301275.3302332
   worditout, 2020, MAKE WORD CLOUD
   Yamada T, 2006, P IEEE VIRT REAL ANN, P199, DOI 10.1109/VR.2006.147
   Yanagida Y, 2012, IEEE SENSOR, P1013
NR 65
TC 2
Z9 2
U1 2
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 15
DI 10.1145/3470975
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900015
DA 2024-07-18
ER

PT J
AU Nandanwar, L
   Shivakumara, P
   Krishnani, D
   Ramachandra, R
   Lu, T
   Pal, U
   Kankanhalli, M
AF Nandanwar, Lokesh
   Shivakumara, Palaiahnakote
   Krishnani, Divya
   Ramachandra, Raghavendra
   Lu, Tong
   Pal, Umapada
   Kankanhalli, Mohan
TI A New Foreground-Background based Method for Behavior-Oriented Social
   Media Image Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Social media; person traits; person behavior; hanman transform; person
   identification; behavior oriented social media image classification
ID VIDEO SURVEILLANCE; RECOGNITION; EVENTS; FUSION
AB Due to various applications, research on personal traits using information on social media has become an important area. In this paper, a new method for the classification of behavior-oriented social images uploaded on various social media platforms is presented. The proposed method introduces a multimodality concept using skin of different parts of human body and background information, such as indoor and outdoor environments. For each image, the proposed method detects skin candidate components based on R, G, B color spaces and entropy features. The iterative mutual nearest neighbor approach is proposed to detect accurate skin candidate components, which result in foreground components. Next, the proposed method detects the remaining part (other than skin components) as background components based on structure tensor of R, G, B color spaces, and Maximally Stable Extremal Regions (MSER) concept in the wavelet domain. We then explore Hanman Transform for extracting context features from foreground and background components through clustering and fusion operation. These features are then fed to an SVM classifier for the classification of behavior-oriented images. Comprehensive experiments on 10-class datasets of Normal Behavior-Oriented Social media Image (NBSI) and Abnormal Behavior-Oriented Social media Image (ABSI) show that the proposed method is effective and outperforms the existing methods in terms of average classification rate. Also, the results on the benchmark dataset of five classes of personality traits and two classes of emotions of different facial expressions (FERPlus dataset) demonstrated the robustness of the proposed method over the existing methods.
C1 [Nandanwar, Lokesh; Shivakumara, Palaiahnakote] Univ Malaya, Dept Comp Syst & Informat Technol, Kuala Lumpur 50603, Wilayah Perseku, Malaysia.
   [Krishnani, Divya] Int Inst Informat Technol IIIT, Sect 24, Naya Raipur 493661, Chhattisgarh, India.
   [Ramachandra, Raghavendra] Fac Informat Technol & Elect Engn, Mail Box 191, NO-2815 Gjovik, Norway.
   [Lu, Tong] Nanjing Univ, Natl Key Lab Novel Software Technol, 163 Xianlin Ave, Nanjing 210023, Peoples R China.
   [Pal, Umapada] Indian Stat Inst, Barrackpore Trunk Rd, Kolkata 700108, W Bengal, India.
   [Kankanhalli, Mohan] Natl Univ Singapore, Dept Comp Sci, 13 Comp Dr, Singapore 117417, Singapore.
C3 Universiti Malaya; Nanjing University; Indian Statistical Institute;
   Indian Statistical Institute Kolkata; National University of Singapore
RP Nandanwar, L (corresponding author), Univ Malaya, Dept Comp Syst & Informat Technol, Kuala Lumpur 50603, Wilayah Perseku, Malaysia.
EM lokeshnandanwar150@gmail.com; shiva@um.edu.my; divya16100@iiitnr.edu.in;
   raghavendra.ramachandra@ntnu.no; lutong@nju.edu.cn;
   umapada@isical.ac.in; mohan@comp.nus.edu.sg
RI Palaiahnakote, Shivakumara/ITU-6488-2023; Palaiahnakote,
   Shivakumara/B-6261-2013; Kankanhalli, Mohan/Q-9284-2019; Pal,
   Umapada/AAC-4930-2022
OI Kankanhalli, Mohan/0000-0002-4846-2015; 
FU Natural Science Foundation of China [61672273]; University of Malaya,
   Malaysia [GPF014D-2019]
FX This work received the Natural Science Foundation of China under Grant
   61672273. Palaiahnakote Shivakumara received partial support for this
   work from the Faculty Grant: GPF014D-2019, University of Malaya,
   Malaysia.
CR Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Beeck K. V, 2017, P AVSS 2017
   Ben Mabrouk A, 2018, EXPERT SYST APPL, V91, P480, DOI 10.1016/j.eswa.2017.09.029
   Benezeth Y, 2009, PROC CVPR IEEE, P2450
   Chen ZN, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231742
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Cosar S, 2017, IEEE T CIRC SYST VID, V27, P683, DOI 10.1109/TCSVT.2016.2589859
   Dwivedi R., 2018, PATTERN RECOGNIT LET
   Farzindar A., 2015, Natural language processing for social media, V8, P1, DOI [DOI 10.2200/S00659ED1V01Y201508HLT030, 10.2200/s00659ed1v01y201508hlt030]
   Grover J, 2018, ENG APPL ARTIF INTEL, V67, P111, DOI 10.1016/j.engappai.2017.08.016
   Han H, 2015, IEEE T PATTERN ANAL, V37, P1148, DOI 10.1109/TPAMI.2014.2362759
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hsu SC, 2018, PROC INT WORKSH ADV
   Hu Y., 2014, P INT AAAI C WEB SOC, P595
   Huang X., 2017, IEEE INFOCOM SER, V8, P1
   Jiang SQ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231738
   Krishnani D., 2019, P ACPR, P594
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumano S, 2017, IEEE T MULTIMEDIA, V19, P107, DOI 10.1109/TMM.2016.2608002
   Lee H, 2014, IEEE IMAGE PROC, P4427, DOI 10.1109/ICIP.2014.7025898
   Liu CW, 2017, IEEE T CIRC SYST VID, V27, P409, DOI 10.1109/TCSVT.2017.2649019
   Liu LN, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P3
   Mungra D, 2020, MULTIMED TOOLS APPL, V79, P2285, DOI 10.1007/s11042-019-08397-0
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Raghunandan K. S, 2018, IEEE T CSVT, P1
   Rougier C, 2011, IEEE T CIRC SYST VID, V21, P611, DOI 10.1109/TCSVT.2011.2129370
   Sarkar A, 2017, IEEE WINT CONF APPL, P20, DOI 10.1109/WACV.2017.10
   Sharma M, 2019, MULTIMED TOOLS APPL, V78, P16195, DOI 10.1007/s11042-018-7030-1
   Shen C, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3309881
   Simonyan K., 2014, 14091556 ARXIV
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tiwari A, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115433
   Tiwari C, 2015, IEEE APP IMG PAT
   Wang AR, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115932
   Wang DY, 2017, IEEE T PATTERN ANAL, V39, P1122, DOI 10.1109/TPAMI.2016.2582166
   Wang T., 2017, PROC ICDAMW, P1584
   Wang T, 2014, IEEE T INF FOREN SEC, V9, P988, DOI 10.1109/TIFS.2014.2315971
   Xu XJ, 2016, BIOMED SIGNAL PROCES, V27, P103, DOI 10.1016/j.bspc.2016.02.008
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang YH, 2015, IEEE T CIRC SYST VID, V25, P1231, DOI 10.1109/TCSVT.2014.2355711
NR 41
TC 1
Z9 1
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 132
DI 10.1145/3458051
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800016
DA 2024-07-18
ER

PT J
AU Wu, HJ
   Dwivedi, AD
   Srivastava, G
AF Wu, Hongjiao
   Dwivedi, Ashutosh Dhar
   Srivastava, Gautam
TI Security and Privacy of Patient Information in Medical Systems Based on
   Blockchain Technology
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Blockchain; medical system; privacy information; anti-theft; cloud
   database; asymmetric encryption
ID HEALTH-CARE-SYSTEM
AB The essence of "blockchain" is a shared database in which information stored is un-falsifiable, traceable, open, and transparent. Therefore, to improve the security of private information in medical systems, this article uses blockchain technology to design a method to protect private information in medical systems and effectively realize anti-theft control of private information. First, the Patient-oriented Privacy Preserving Access Control model is introduced into the access control process of private information in medical systems. Next, a private information storage platform is built by using blockchain technology, and information transmission is realized using standard cryptographic algorithms. In this process, file authorization contracts are also used to guarantee the security of private information and further prevent theft of medical private information. Our simulation results show that the storage response time of this method is kept below 1,000 ms, and the maximum information throughput rate reaches 550 kbit/s, which indicates that this method has strong performance in information storage and transmission efficiency. Moreover, the reliability and bandwidth utilization of data transmission across domains is higher,so the method has higher information security control performance and superior overall performance.
C1 [Wu, Hongjiao] Henan Vocat Coll Tuina, 10 Xuefu St, Luoyang 471000, Peoples R China.
   [Dwivedi, Ashutosh Dhar] Tech Univ Denmark, DTU Compute, DK-2800 Lyngby, Denmark.
   [Srivastava, Gautam] Brandon Univ, Dept Math & Comp Sci, Brandon, MB R7A 6A9, Canada.
   [Srivastava, Gautam] China Med Univ, Res Ctr Interneural Comp, Taichung 40402, Taiwan.
C3 Technical University of Denmark; Brandon University; China Medical
   University Taiwan
RP Srivastava, G (corresponding author), Brandon Univ, Dept Math & Comp Sci, Brandon, MB R7A 6A9, Canada.; Srivastava, G (corresponding author), China Med Univ, Res Ctr Interneural Comp, Taichung 40402, Taiwan.
EM whjrpf@163.com; adhdw@dtu.dk; srivastavag@brandonu.ca
RI Dwivedi, Ashutosh Dhar/T-8727-2019; Srivastava, Gautam/N-5668-2019
OI Dwivedi, Ashutosh Dhar/0000-0001-8010-6275; Srivastava,
   Gautam/0000-0001-9851-4103
FU Independent Research Fund Denmark for Technology and Production
   [8022-00348A]; Natural Sciences and Engineering Council of Canada
   (NSERC) [RGPIN-2020-05363]
FX The work of Ashutosh Dhar Dwivedi is funded by a grant from the
   Independent Research Fund Denmark for Technology and Production, Grant
   No. 8022-00348A and the work of Gautam Srivastava is funded by Natural
   Sciences and Engineering Council of Canada (NSERC) Discovery Grant held
   by Gautam Srivastava (Grant No. RGPIN-2020-05363). Authors' addresses:
   H. Wu, Henan Vocational College of Tuina No.10, Xuefu Street, Luoyang,
   China 471000; email: whjrpf@163.com;A.D.Dwivedi, DTU Compute, Technical
   University of Denmark, 2800 Kgs. Lyngby, Denmark; email:
   adhdw@dtu.dk;G.Srivastava (corresponding author), Department of
   Mathematics and Computer Science, Brandon University, Brandon, MB R7A
   6A9, Canada Research Centre for Interneural Computing, China Medical
   University, Taichung 40402, Taiwan; email:
   srivastavag@brandonu.ca.Permission to make digital or hard copies of all
   or part of this work for personal or classroom use is granted without
   fee provided that copies are not made or distributed for profit or
   commercial advantage and that copies bear this notice and the full
   citation on the first page. Copyrights for components of this work owned
   by others than ACM must be honored. Abstracting with credit is
   permitted. To copy otherwise, or republish, to post on servers or to
   redistribute to lists, requires prior specific permission and/or a fee.
   Request permissions from permissions@acm.org.
CR Alghazo JM, 2019, CURR MED IMAGING REV, V15, P386, DOI 10.2174/1573405615666181228121535
   Amini J, 2017, IEEE T POWER ELECTR, V32, P2, DOI 10.1109/TPEL.2016.2522939
   Baynham-Herd Z, 2017, NATURE, V548, P523, DOI 10.1038/548523c
   Baza Mohamed, 2019, IEEE T NETW SCI ENG, V2019
   Chrisler JC, 2016, J SOC ISSUES, V72, P86, DOI 10.1111/josi.12157
   Daemen J, 2005, ENCY CRYPTOGRAPHY SE, P520
   Dong ZY, 2018, J MOD POWER SYST CLE, V6, P958, DOI 10.1007/s40565-018-0418-0
   Dwivedi AD, 2019, 2019 42ND INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P135, DOI [10.1109/tsp.2019.8769060, 10.1109/TSP.2019.8769060]
   Dwivedi AD, 2019, IEEE ACCESS, V7, P16476, DOI 10.1109/ACCESS.2019.2894337
   Dwivedi AD, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020326
   Dwivedi AD, 2018, IEEE ACCESS, V6, P79105, DOI 10.1109/ACCESS.2018.2881130
   Dwivedi AD, 2018, INT J ELECTRON TELEC, V64, P147, DOI 10.24425/119362
   Fairley P, 2017, IEEE SPECTRUM, V54, P36, DOI 10.1109/MSPEC.2017.8048837
   Fenton JJ, 2016, JAMA INTERN MED, V176, P391, DOI 10.1001/jamainternmed.2015.6020
   Huang H, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-018-9781-0
   Krah E, 2018, J COMMUN HEALTH, V43, P157, DOI 10.1007/s10900-017-0398-4
   Liu S, 2020, MECH SYST SIGNAL PR, V138, DOI 10.1016/j.ymssp.2019.106537
   Liu S, 2017, FRACTALS, V25, DOI 10.1142/S0218348X17400047
   Liu SA, 2017, MULTIMED TOOLS APPL, V76, P5787, DOI 10.1007/s11042-014-2408-1
   Ma ZF, 2018, CHINESE J ELECTRON, V27, P1025, DOI 10.1049/cje.2018.07.003
   Malliaros Stefanos, 2019, Acta Inform Med, V27, P333, DOI 10.5455/aim.2019.27.333-340
   Peck ME, 2017, IEEE SPECTRUM, V54, P24, DOI 10.1109/MSPEC.2017.8048835
   Qi YN, 2019, SCI CHINA TECHNOL SC, V62, P698, DOI 10.1007/s11431-017-9258-0
   Singh R, 2020, CLUSTER COMPUT, V23, P2035, DOI 10.1007/s10586-020-03046-w
   Sperber NR, 2019, QUAL MANAG HEALTH CA, V28, P147, DOI 10.1097/QMH.0000000000000221
   Srivastava G., 2018, AUTOMATED REMOTE PAT
   Srivastava G., 2019, Communications in Computer and Information Science, P334
   Treleaven P, 2017, COMPUTER, V50, P14, DOI 10.1109/MC.2017.3571047
   Underwood S, 2016, COMMUN ACM, V59, P15, DOI 10.1145/2994581
   Wyse JJ, 2018, SUBST ABUS, V39, P139, DOI 10.1080/08897077.2018.1452327
   Yazdinejad A, 2020, IEEE J BIOMED HEALTH, V24, P2146, DOI 10.1109/JBHI.2020.2969648
   Zhang XM, 2018, IEEE T CYBERNETICS, V48, P2569, DOI 10.1109/TCYB.2017.2743161
   Zhao B, 2017, TSINGHUA SCI TECHNOL, V22, P218
NR 33
TC 27
Z9 28
U1 1
U2 31
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 60
DI 10.1145/3408321
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zareapoor, M
   Yang, J
AF Zareapoor, Masoumeh
   Yang, Jie
TI Equivariant Adversarial Network for Image-to-image Translation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Stylistic image generation; image-to-image translation; generative
   model; domain adaptation
AB Image-to-Image translation aims to learn an image from a source domain to a target domain. However, there are three main challenges, such as lack of paired datasets, multimodality, and diversity, that are associated with these problems and need to be dealt with. Convolutional neural networks (CNNs), despite of having great performance in many computer vision tasks, they fail to detect the hierarchy of spatial relationships between different parts of an object and thus do not form the ideal representative model we look for. This article presents a new variation of generative models that aims to remedy this problem. We use a trainable transformer, which explicitly allows the spatial manipulation of data within training. This differentiable module can be augmented into the convolutional layers in the generative model, and it allows to freely alter the generated distributions for image-to-image translation. To reap the benefits of proposed module into generative model, our architecture incorporates a new loss function to facilitate an effective end-to-end generative learning for image-to-image translation. The proposed model is evaluated through comprehensive experiments on image synthesizing and image-to-image translation, along with comparisons with several state-of-the-art algorithms.
C1 [Zareapoor, Masoumeh; Yang, Jie] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai, Peoples R China.
   [Zareapoor, Masoumeh; Yang, Jie] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, 800 DongChuan Rd, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Yang, J (corresponding author), Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai, Peoples R China.; Yang, J (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, 800 DongChuan Rd, Shanghai 200240, Peoples R China.
EM mzarea222@gmail.com; jieyang@sjtu.edu.cn
RI Zareapoor, Dr. Masoumeh/AAE-6067-2019; Yang, Jie/JCD-9867-2023
OI Zareapoor, Dr. Masoumeh/0000-0002-3991-0584; Zareapoor,
   Masoumeh/0000-0002-7569-9018
FU National Key R&D Program of China [2019YFB1311503]; NSFC, China
   [61876107, U1803261]; Committee of Science and Technology, Shanghai,
   China [19510711200]
FX This research is partly supported by National Key R&D Program of China
   (No. 2019YFB1311503); NSFC, China (No: 61876107, U1803261); Committee of
   Science and Technology, Shanghai, China (No. 19510711200).
CR Amodio M, 2019, PROC CVPR IEEE, P8975, DOI 10.1109/CVPR.2019.00919
   [Anonymous], 2017, Advances in neural information processing systems
   Balaji Yogesh, 2018, ARXIV PREPRINT ARXIV
   Bass C, 2019, PR MACH LEARN RES, V102, P39
   Ben-Yosef Matan, 2018, ARXIV PREPRINT ARXIV
   Bunne Charlotte, 2019, ARXIV PREPRINT ARXIV
   Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012
   Fedus William, 2017, ARXIV PREPRINT ARXIV
   Genevay Aude, 2017, ARXIV PREPRINT ARXIV
   Gonzalez-Garcia Abel, 2018, P INT C ADV NEUR INF, P1287
   Goodfellow I., 2016, arXiv
   Gulrajani Ishaan, 2017, P ADV NEUR INF PROC, P5767
   Hwang Uiwon, 2019, ARXIV PREPRINT ARXIV
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jaderberg M, 2015, ADV NEUR IN, V28
   Karras Tero, 2017, P INT C LEARN REPR
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luc P., 2016, NIPS WORKSHOP ADVERS
   Mao XD, 2019, IEEE T PATTERN ANAL, V41, P2947, DOI 10.1109/TPAMI.2018.2872043
   Mejjati Y.A., 2018, NIPS, P3693
   Netzer Y., 2011, READING DIGITS NATUR
   Odena A, 2017, PR MACH LEARN RES, V70
   Peyré G, 2016, PR MACH LEARN RES, V48
   Radford A., 2015, ARXIV
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Salimans T., 2018, ARXIV PREPRINT ARXIV
   Shamsolmoali P, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3355612
   Shamsolmoali P, 2019, NEUROCOMPUTING, V366, P140, DOI 10.1016/j.neucom.2019.07.094
   Shamsolmoali Pourya, 2021, INF FUS
   Shamsolmoali Pourya, 2020, NEUROCOMPUTING
   Wang Z., 2019, ARXIV190601529
   Wei J., 2019, P 2019 C EMP METH NA, P6383
   Yang K., 2018, ARXIV PREPRINT ARXIV
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zareapoor M, 2021, MECH SYST SIGNAL PR, V149, DOI 10.1016/j.ymssp.2020.107175
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 44
TC 4
Z9 4
U1 2
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 73
DI 10.1145/3458280
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100015
DA 2024-07-18
ER

PT J
AU Rahman, MA
   Hossain, MS
   Alrajeh, NA
   Gupta, BB
AF Rahman, Md Abdur
   Hossain, M. Shamim
   Alrajeh, Nabil A.
   Gupta, B. B.
TI A Multimodal, Multimedia Point-of-Care Deep Learning Framework for
   COVID-19 Diagnosis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep Learning; COVID-19 pandemic; point-of-care
ID AI
AB In this article, we share our experiences in designing and developing a suite of deep neural network-(DNN) based COVID-19 case detection and recognition framework. Existing pathological tests such as RT-PCR-based pathogen RNA detection from nasal swabbing seem to display low detection rates during the early stages of virus contraction. Moreover, the reliance on a few overburdened laboratories based around an epicenter capable of supplying large numbers of RT-PCR tests makes this testing method non-scalable when the rate of infections is high. Similarly, finding an effective drug or vaccine with which to combat COVID-19 requires a long time and many clinical trials. The development of pathological COVID-19 tests is hindered by shortages in the supply chain of chemical reagents necessary for testing on a large scale. This diminishes the speed of diagnosis and the ability to filter out COVID-19 positive patients from uninfected patients on a national level. Existing research has shown that DNN has been successful in identifying COVID-19 from radiological media such as CT scans and X-ray images, audio media such as cough sounds, optical coherence tomography to identify conjunctivitis and pink eye symptoms on the ocular surface, body temperature measurement using smartphone fingerprint sensors or thermal cameras, the use of live facial detection to identify safe social distancing practices from camera images, and face mask detection from camera images. We also investigate the utility of federated learning in diagnosis cases where private data can be trained via edge learning. These point-of-care modalities can be integrated with DNN-based RT-PCR laboratory test results to assimilate multiple modalities of COVID-19 detection and thereby provide more dimensions of diagnosis. Finally, we will present our initial test results, which are encouraging.
C1 [Rahman, Md Abdur] Univ Prince Mugrin, Dept Sci Res & Postgrad Studies, Madinah, Saudi Arabia.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, Res Chair Pervas & Mobile Comp, Riyadh, Saudi Arabia.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh, Saudi Arabia.
   [Alrajeh, Nabil A.] King Saud Univ, Coll Appl Med Sci, Biomed Technol Dept, Riyadh, Saudi Arabia.
   [Gupta, B. B.] Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra 136119, Haryana, India.
   [Gupta, B. B.] Asia Univ, Dept Comp Sci & Informat Engn, Taichung, Taiwan.
C3 University of Prince Mugrin; King Saud University; King Saud University;
   King Saud University; National Institute of Technology (NIT System);
   National Institute of Technology Kurukshetra; Asia University Taiwan
RP Hossain, MS (corresponding author), King Saud Univ, Coll Comp & Informat Sci, Res Chair Pervas & Mobile Comp, Riyadh, Saudi Arabia.; Hossain, MS (corresponding author), King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh, Saudi Arabia.
EM m.arahman@upm.edu.sa; mshossain@ksu.edu.sa; nabil@ksmedu.sa;
   bbgupta@nitkkr.ac.in
RI Rahman, Abdur/AAG-9302-2019; Hossain, M. Shamim/K-1362-2014; Guizani,
   Mohsen/AAX-4534-2021; Gupta, Brij B/E-9813-2011; Alrajeh,
   Nabil/T-2446-2017
OI Rahman, Abdur/0000-0002-4105-0368; Hossain, M.
   Shamim/0000-0001-5906-9422; Guizani, Mohsen/0000-0002-8972-8094; Gupta,
   Brij B/0000-0003-4929-4698; Alrajeh, Nabil/0000-0002-1861-0582
FU KACST, Saudi Arabia [5-20-01-127-0001]
FX This work was supported by the KACST, Saudi Arabia, Research Fund under
   Grant #5-20-01-127-0001.
CR Abbas Asmaa, 2021, Applied Intelligence. The International Journal of Research on Intelligent Systems for Real Life Complex Problems, V51, P854, DOI 10.1007/s10489-020-01829-7
   Abdulsalam Y, 2022, IEEE T NETW SCI ENG, V9, P309, DOI 10.1109/TNSE.2020.3026637
   Afshar P, 2020, PATTERN RECOGN LETT, V138, P638, DOI 10.1016/j.patrec.2020.09.010
   Al-Tawfiq JA, 2020, J HOSP INFECT, V105, P154, DOI 10.1016/j.jhin.2020.03.001
   Al-Tawfiq JA, 2020, TRAVEL MED INFECT DI, V34, DOI 10.1016/j.tmaid.2020.101615
   Alimadadi A, 2020, PHYSIOL GENOMICS, V52, P200, DOI 10.1152/physiolgenomics.00029.2020
   Biscayart C, 2020, TRAVEL MED INFECT DI, V33, DOI 10.1016/j.tmaid.2020.101567
   Bullock Alexandra Luccioni J., 2020, ARXIV200311336
   Chaganti S, 2020, ARXIV200401279
   Chen JL, 2020, MICROBES INFECT, V22, P69, DOI 10.1016/j.micinf.2020.01.004
   Cordes AK, 2020, J CLIN VIROL, V125, DOI 10.1016/j.jcv.2020.104305
   Pham C, 2016, LECT NOTES ARTIF INT, V9621, P300, DOI 10.1007/978-3-662-49381-6_29
   Farooq M., 2020, ARXIV PREPRINT ARXIV
   Gao Y, 2020, SCIENCE, V368, P779, DOI 10.1126/science.abb7498
   Ghassemi, 2020, ARXIV200611988
   Gozes O., ARXIV PREPRINT ARXIV
   Guo F, 2018, IEEE ACCESS, V6, P77414, DOI 10.1109/ACCESS.2018.2882946
   Guo QL, 2020, IEEE ACCESS, V8, P63368, DOI 10.1109/ACCESS.2020.2985231
   Hall Lawrence O., 2020, ARXIV200402060
   Hande Roshan S., 2017, INT J INNOV SCI RES, V2, P283
   Hoffmann M, 2020, CELL, V181, P271, DOI 10.1016/j.cell.2020.02.052
   Hossain MS, 2020, IEEE NETWORK, V34, P120, DOI 10.1109/MNET.011.2000064
   Hossain MS, 2020, IEEE NETWORK, V34, P126, DOI 10.1109/MNET.011.2000458
   Hossain MS, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3241056
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Hou H., 2020, RSNA RADIOL
   kaggle, Covid-19 Image Dataset
   Kaissis GA, 2020, NAT MACH INTELL, V2, P305, DOI 10.1038/s42256-020-0186-1
   Khalifa NEM, ARXIV200401184
   Killeen B. D., 2020, COUNTY LEVEL DATASET
   Ko WC, 2020, INT J ANTIMICROB AG, V55, DOI 10.1016/j.ijantimicag.2020.105933
   Kooraki S., 2020, J AM COLL RADIOL, P1, DOI [10.1016/j.jacr.2020.02.008, DOI 10.1016/J.JACR.2020.02.008]
   Lane ND, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P283, DOI 10.1145/2750858.2804262
   Li XW, 2020, J PHARM ANAL, V10, P102, DOI 10.1016/j.jpha.2020.03.001
   Liang YX, 2020, IDENTIFYING RADIOLOG
   Lin C, 2020, CLIN IMAG, V63, P7, DOI 10.1016/j.clinimag.2020.02.008
   Lin QY, 2020, INT J INFECT DIS, V93, P211, DOI 10.1016/j.ijid.2020.02.058
   Liu X, 2020, J GENET GENOMICS, V47, P119, DOI 10.1016/j.jgg.2020.02.001
   Loey M, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12040651
   Lu RJ, 2020, LANCET, V395, P565, DOI 10.1016/S0140-6736(20)30251-8
   Maghded HS, 2020, 2020 IEEE 21ST INTERNATIONAL CONFERENCE ON INFORMATION REUSE AND INTEGRATION FOR DATA SCIENCE (IRI 2020), P180, DOI 10.1109/IRI49571.2020.00033
   Muhammad G, 2021, IEEE J SEL AREA COMM, V39, P603, DOI 10.1109/JSAC.2020.3020654
   Narin A, 2021, PATTERN ANAL APPL, V24, P1207, DOI 10.1007/s10044-021-00984-y
   Nishiura H, 2020, INT J INFECT DIS, V93, P284, DOI 10.1016/j.ijid.2020.02.060
   Porter P, 2019, RESP RES, V20, DOI 10.1186/s12931-019-1046-6
   Qian YF, 2020, IEEE NETWORK, V34, P46, DOI 10.1109/MNET.001.1900161
   Rahman A, 2018, IEEE ACCESS, V6, P72469, DOI 10.1109/ACCESS.2018.2881246
   Rahman MA, 2021, IEEE INTERNET THINGS, V8, P15847, DOI 10.1109/JIOT.2021.3051080
   Rahman MA, 2021, IEEE INTERNET THINGS, V8, P9603, DOI 10.1109/JIOT.2020.3013710
   Rahman MA, 2020, IEEE NETWORK, V34, P98, DOI 10.1109/MNET.011.2000353
   Rahman MA, 2019, IEEE ACCESS, V7, P34874, DOI 10.1109/ACCESS.2019.2903024
   Rahman MA, 2019, IEEE ACCESS, V7, P18611, DOI 10.1109/ACCESS.2019.2896065
   Rahman MA, 2020, IEEE ACCESS, V8, P205071, DOI 10.1109/ACCESS.2020.3037474
   Randhawa GS, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0232391
   Rothan HA, 2020, J AUTOIMMUN, V109, DOI 10.1016/j.jaut.2020.102433
   Santosh KC, 2020, J MED SYST, V44, DOI 10.1007/s10916-020-01562-1
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sethy PK, 2020, INT J MATH ENG MANAG, V5, P643, DOI 10.33889/IJMEMS.2020.5.4.052
   Sharif M, 2019, ACM T PRIV SECUR, V22, DOI 10.1145/3317611
   Shen MZ, 2020, J PHARM ANAL, V10, P97, DOI 10.1016/j.jpha.2020.02.010
   Shi HS, 2020, LANCET INFECT DIS, V20, P425, DOI 10.1016/S1473-3099(20)30086-4
   Shorfuzzaman M, 2021, SUSTAIN CITIES SOC, V64, DOI 10.1016/j.scs.2020.102582
   Shorfuzzaman M, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107700
   Song FX, 2020, RADIOLOGY, V295, P210, DOI [10.1148/radiol.2020200274, 10.1148/radiol.2020209021]
   Song Y, 2020, GUT, V69, P1143, DOI 10.1136/gutjnl-2020-320891
   Song YZ, 2022, IEEE T INTELL TRANSP, V23, P12287, DOI 10.1109/TITS.2021.3112458
   Sundararajan S. K., 2020, P 3 INT C INT THINGS, P714, DOI [10.1109/i-smac47947.2019.9032705, DOI 10.1109/I-SMAC47947.2019.9032705]
   Taori Rohan, 2019, 2019 IEEE Security and Privacy Workshops (SPW). Proceedings, P15, DOI 10.1109/SPW.2019.00016
   Tonda A., 2020, ACCURATE IDENTIFICAT
   Tucker A., 2020, DETECTION, P1
   Phan T, 2020, INFECT GENET EVOL, V79, DOI 10.1016/j.meegid.2020.104211
   Wang LD, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-76550-z
   Wang S, 2021, EUR RADIOL, V31, P6096, DOI [10.1080/1064119X.2021.1966557, 10.1079/9781789246070.0001, 10.1007/s00330-021-07715-1]
   Wilder-Smith A, 2020, LANCET INFECT DIS, V20, pE102, DOI 10.1016/S1473-3099(20)30129-8
   Xu YH, 2020, J INFECTION, V80, P394, DOI 10.1016/j.jinf.2020.02.017
   Ye Y, 2020, ARXIV200312232
   Zhang HW, 2020, ACAD RADIOL, V27, P463, DOI 10.1016/j.acra.2020.02.003
   Zhang WJ, 2019, IEEE ACCESS, V7, P151103, DOI 10.1109/ACCESS.2019.2946461
   Zhang X, 2020, OCUL SURF, V18, P360, DOI 10.1016/j.jtos.2020.03.010
NR 79
TC 37
Z9 37
U1 2
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 18
DI 10.1145/3421725
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900018
DA 2024-07-18
ER

PT J
AU Xu, ML
   Li, QF
   Niu, JW
   Su, H
   Liu, XT
   Xu, WW
   Lv, P
   Zhou, B
   Yang, Y
AF Xu, Mingliang
   Li, Qingfeng
   Niu, Jianwei
   Su, Hao
   Liu, Xiting
   Xu, Weiwei
   Lv, Pei
   Zhou, Bing
   Yang, Yi
TI ART-UP: A Novel Method for Generating Scanning-Robust Aesthetic QR Codes
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Aesthetic QR codes; error analysis; visualization optimization; scanning
   robustness; scanning probability calculation
ID IMAGE QUALITY ASSESSMENT
AB Quick response (QR) codes are usually scanned in different environments, so they must be robust to variations in illumination, scale, coverage, and camera angles. Aesthetic QR codes improve the visual quality, but subtle changes in their appearance may cause scanning failure. In this article, a new method to generate scanning-robust aesthetic QR codes is proposed, which is based on a module-based scanning probability estimation model that can effectively balance the tradeoff between visual quality and scanning robustness. Our method locally adjusts the luminance of each module by estimating the probability of successful sampling. The approach adopts the hierarchical, coarse-to-fine strategy to enhance the visual quality of aesthetic QR codes, which sequentially generate the following three codes: a binary aesthetic QR code. a grayscale aesthetic QR code, and the final color aesthetic QR code. Our approach also can be used to create QR codes with different visual styles by adjusting some initialization parameters. User surveys and decoding experiments were adopted for evaluating our method compared with state-of-the-art algorithms, which indicates that the proposed approach has excellent performance in terms of both visual quality and scanning robustness.
C1 [Xu, Mingliang; Li, Qingfeng; Lv, Pei; Zhou, Bing] Zhengzhou Univ, Zhengzhou, Peoples R China.
   [Niu, Jianwei; Su, Hao; Liu, Xiting] Beihang Univ, Beijing, Peoples R China.
   [Xu, Weiwei] Zhejiang Univ, Hangzhou, Peoples R China.
   [Yang, Yi] Univ Technol Sydney, Sydney, NSW, Australia.
C3 Zhengzhou University; Beihang University; Zhejiang University;
   University of Technology Sydney
RP Niu, JW (corresponding author), Beihang Univ, Beijing, Peoples R China.
EM iexumingliang@zzu.edu.cn; liqingfengzzu@163.com; niujianwei@buaa.edu.cn;
   bhsuhao@buaa.edu.cn; liuxiting@buaa.edu.cn; xww@cad.zju.edu.cn;
   ielvpei@zzu.edu.cn; iebzhou@zzu.edu.cn; Yi.Yang@uts.edu.au
RI Han, Liang/KFR-6745-2024; Li, Qingfeng/GYU-7238-2022; yang,
   yang/GVT-5210-2022; zhang, xueying/JMB-7808-2023; Su, Hao/JFJ-8901-2023;
   yang, yang/GWB-9426-2022; yang, yang/HGT-7999-2022; Lang,
   Ming/HIK-0758-2022; Yang, Yi/B-9273-2017
OI Li, Qingfeng/0000-0002-3603-7580; Yang, Yi/0000-0002-0512-880X
FU National Natural Science Foundation of China [61822701, 61772060]; Henan
   Provincial Prevention and Control Emergency Project of COVID-19
   [201100312000]
FX This work was supported by the National Natural Science Foundation of
   China (grant numbers 61822701, 61772060), and the Henan Provincial
   Prevention and Control Emergency Project of COVID-19 (grant number
   201100312000).
CR Alexis Laporte, 2011, UNITAG
   Alva N., 2012, VISUALEAD
   Baharav Z, 2013, IEEE INT CON MULTI
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Chengfang Fang, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P278, DOI 10.1007/978-3-319-04114-8_24
   Chu HK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508408
   Gao ZP, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1047, DOI 10.1145/2733373.2806398
   Garateguy GJ, 2014, IEEE T IMAGE PROCESS, V23, P2842, DOI 10.1109/TIP.2014.2321501
   ISO, 2000, 180042000E ISOIEC
   Jiang Bo., 2014, SIGGRAPH ASIA 2014 P, P31
   Li L, 2016, J SYST SOFTWARE, V116, P85, DOI 10.1016/j.jss.2015.07.009
   Lin SS, 2015, IEEE T MULTIMEDIA, V17, P1515, DOI 10.1109/TMM.2015.2437711
   Lin YS, 2013, COMPUT GRAPH FORUM, V32, P137, DOI 10.1111/cgf.12221
   Lin YH, 2013, IEEE T MULTIMEDIA, V15, P2198, DOI 10.1109/TMM.2013.2271745
   Liu Y, 2008, 2008 CHINESE CONTROL AND DECISION CONFERENCE, VOLS 1-11, P203, DOI 10.1109/CCDC.2008.4597299
   Liu Y, 2006, ISDA 2006: SIXTH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS DESIGN AND APPLICATIONS, VOL 2, P783
   MacWilliams F. J., 1978, The Theory of Error-Correcting Codes
   Ohbuchi E, 2004, 2004 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P260, DOI 10.1109/CW.2004.23
   Owen S., 2013, Zxing
   Ramya S, 2015, STRUCTURE, V17, p4V
   Russ Cox, 2012, QART CODES
   Samretwit D., 2011, Proceedings of the 2011 Third International Conference on Intelligent Networking and Collaborative Systems (INCoS 2011), P552, DOI 10.1109/INCoS.2011.117
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Xu ML, 2019, IEEE T MULTIMEDIA, V21, P1960, DOI 10.1109/TMM.2019.2891420
   Yang Z, 2016, UBICOMP'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P904, DOI 10.1145/2971648.2971733
   Yongtai Zhang, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P183, DOI 10.1007/978-3-319-14442-9_16
NR 29
TC 7
Z9 7
U1 2
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
AR 25
DI 10.1145/3418214
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO6SX
UT WOS:000641174200005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, X
   Wang, SQ
   Wang, Z
   Zhang, XL
   Hu, RM
AF Xu, Xin
   Wang, Shiqin
   Wang, Zheng
   Zhang, Xiaolong
   Hu, Ruimin
TI Exploring Image Enhancement for Salient Object Detection in Low Light
   Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Low light images; salient object detection; images enhancement; physical
   lighting model; non-local-block layer
ID RETINEX
AB Low light images captured in a non-uniform illumination environment usually are degraded with the scene depth and the corresponding environment lights. This degradation results in severe object information loss in the degraded image modality, which makes the salient object detection more challenging due to low contrast property and artificial light influence. However, existing salient object detection models are developed based on the assumption that the images are captured under a sufficient brightness environment, which is impractical in real-world scenarios. In this work, we propose an image enhancement approach to facilitate the salient object detection in low light images. The proposed model directly embeds the physical lighting model into the deep neural network to describe the degradation of low light images, in which the environment light is treated as a point-wise variate and changes with local content. Moreover, a Non-Local-Block Layer is utilized to capture the difference of local content of an object against its local neighborhood favoring regions. To quantitative evaluation, we construct a low light Images dataset with pixel-level human-labeled ground-truth annotations and report promising results on four public datasets and our benchmark dataset.
C1 [Xu, Xin; Wang, Shiqin; Zhang, Xiaolong] Wuhan Univ Sci & Technol, Wuhan, Peoples R China.
   [Wang, Zheng] Natl Inst Informat, Tokyo, Japan.
   [Hu, Ruimin] Wuhan Univ, Wuhan, Peoples R China.
C3 Wuhan University of Science & Technology; Research Organization of
   Information & Systems (ROIS); National Institute of Informatics (NII) -
   Japan; Wuhan University
RP Xu, X (corresponding author), Wuhan Univ Sci & Technol, Wuhan, Peoples R China.
EM xuxin@wust.edu.cn; wust_wangshiqin@163.com; wangz@nii.ac.jp;
   xiaolong.zhang@wust.edu.cn; hrm1964@163.com
RI Wang, Zheng/ABC-6029-2020; Xu, Xin/JRW-5800-2023; 汪, 诗/HHC-5822-2022;
   ZHANG, XIAOLONG/IZQ-4553-2023
OI Wang, Zheng/0000-0003-3846-9157; Xu, Xin/0000-0003-0748-3669
FU Natural Science Foundation of China [U1803262, 61602349, 61440016]
FX This work was supported by the Natural Science Foundation of China
   (U1803262, 61602349, and 61440016).
CR Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Deng G, 2011, IEEE T IMAGE PROCESS, V20, P1249, DOI 10.1109/TIP.2010.2092441
   Deng Zijun, 2018, P 27 INT JOINT C ART, P640
   Dong X, 2011, IEEE INT CON MULTI
   Farid H, 2001, IEEE T IMAGE PROCESS, V10, P1428, DOI 10.1109/83.951529
   Feng W, 2019, IEEE T IMAGE PROCESS, V28, P3232, DOI 10.1109/TIP.2019.2895411
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang J, 2019, LECT NOTES COMPUT SC, V11133, P230, DOI 10.1007/978-3-030-11021-5_15
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Kansal K, 2020, IEEE T CIRC SYST VID, V30, P3422, DOI 10.1109/TCSVT.2019.2963721
   LAND EH, 1986, P NATL ACAD SCI USA, V83, P3078, DOI 10.1073/pnas.83.10.3078
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li B, 2018, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2018.00682
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li L, 2015, IEEE IMAGE PROC, P3730, DOI 10.1109/ICIP.2015.7351501
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Lv F., 2019, ARXIV190800682
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Preetham AJ, 1999, COMP GRAPH, P91, DOI 10.1145/311535.311545
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren YR, 2019, IEEE T CIRC SYST VID, V29, P968, DOI 10.1109/TCSVT.2018.2828141
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen Xiaohui, 2019, IEEE T IMAGE PROCESS, DOI DOI 10.1109/TIP.2021.3051462
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang LZ, 2019, IEEE T PATTERN ANAL, V41, P1734, DOI 10.1109/TPAMI.2018.2846598
   Wang M, 2015, IEEE T KNOWL DATA EN, V27, P2564, DOI 10.1109/TKDE.2015.2415497
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P858, DOI 10.1109/TMM.2012.2187181
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang Y, 2017, IEEE T IMAGE PROCESS, V26, P1393, DOI 10.1109/TIP.2017.2655449
   Wang Y, 2020, ARCH DERMATOL RES, V312, P581, DOI 10.1007/s00403-020-02044-7
   Wei Chen, 2016, ARXIV PREPRINT ARXIV
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Wei SK, 2019, IEEE T IMAGE PROCESS, V28, P4580, DOI 10.1109/TIP.2019.2913513
   Wu L, 2019, IEEE T NEUR NET LEAR, V30, P3347, DOI 10.1109/TNNLS.2019.2891244
   Wu L, 2020, IEEE T IMAGE PROCESS, V29, P1233, DOI 10.1109/TIP.2019.2940684
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu L, 2019, IEEE T IMAGE PROCESS, V28, P1602, DOI 10.1109/TIP.2018.2878970
   Wu L, 2019, IEEE T CYBERNETICS, V49, P1791, DOI 10.1109/TCYB.2018.2813971
   Xu Xin, 2018, P EUR C COMP VIS ECC, P580
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Zeng Y, 2018, PROC CVPR IEEE, P1644, DOI 10.1109/CVPR.2018.00177
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhan FN, 2018, LECT NOTES COMPUT SC, V11212, P257, DOI 10.1007/978-3-030-01237-3_16
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P1755, DOI 10.1109/TPAMI.2019.2900649
   Zhang H, 2018, IEEE COMPUT SOC CONF, P1015, DOI 10.1109/CVPRW.2018.00135
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang J, 2016, IEEE T NEUR NET LEAR, V27, P1177, DOI 10.1109/TNNLS.2015.2464316
NR 67
TC 37
Z9 39
U1 8
U2 58
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 8
DI 10.1145/3414839
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, Z
   Ren, JH
   Zhang, HJ
   Zhang, Z
   Liu, GC
   Yan, SC
AF Zhang, Zhao
   Ren, Jiahuan
   Zhang, Haijun
   Zhang, Zheng
   Liu, Guangcan
   Yan, Shuicheng
TI DLRF-Net: A Progressive Deep Latent Low-Rank Fusion Network for
   Hierarchical Subspace Discovery
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Hierarchical subspace discovery; deep latent low-rank fusion network;
   image representation; clustering
ID FACE RECOGNITION; SPARSE-REPRESENTATION; DIMENSIONALITY REDUCTION;
   REGRESSION; RECOVERY; ILLUMINATION; DICTIONARY; ALGORITHM
AB Low-rank coding-based representation learning is powerful for discovering and recovering the subspace structures in data, which has obtained an impressive performance; however, it still cannot obtain deep hidden information due to the essence of single-layer structures. In this article, we investigate the deep low-rank representation of images in a progressive way by presenting a novel strategy that can extend existing single-layer latent low-rank models into multiple layers. Technically, we propose a new progressive Deep Latent Low-Rank Fusion Network (DLRF-Net) to uncover deep features and the clustering structures embedded in latent subspaces. The basic idea of DLRF-Net is to progressively refine the principal and salient features in each layer from previous layers by fusing the clustering and projective subspaces, respectively, which can potentially learn more accurate features and subspaces. To obtain deep hidden information, DLRF-Net inputs shallow features from the last layer into subsequent layers. Then, it aims at recovering the hierarchical information and deeper features by respectively congregating the subspaces in each layer of the network. As such, one can also ensure the representation learning of deeper layers to remove the noise and discover the underlying clean subspaces, which will be verified by simulations. It is noteworthy that the framework of our DLRF-Net is general and is applicable to most existing latent low-rank representation models, i.e., existing latent low-rank models can be easily extended to the multilayer scenario using DLRF-Net. Extensive results on real databases show that our framework can deliver enhanced performance over other related techniques.
C1 [Zhang, Zhao; Ren, Jiahuan] Soochow Univ, Sch Comp Sci & Technol, 1 Shizi St, Suzhou 215006, Peoples R China.
   [Zhang, Haijun; Zhang, Zheng] Harbin Inst Technol, Dept Comp Sci, Taoyuan St, Shenzhen 518055, Peoples R China.
   [Liu, Guangcan] Nanjing Univ Informat Sci & Technol, Sch Informat & Control, 219 Ningliu Rd, Nanjing, Peoples R China.
   [Yan, Shuicheng] YITU Technol, 523 Loushanguan Rd, Shanghai, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, 21 Lower Kent Ridge Rd, Singapore 117583, Singapore.
C3 Soochow University - China; Harbin Institute of Technology; Nanjing
   University of Information Science & Technology; National University of
   Singapore
RP Zhang, Z (corresponding author), Soochow Univ, Sch Comp Sci & Technol, 1 Shizi St, Suzhou 215006, Peoples R China.; Zhang, HJ (corresponding author), Harbin Inst Technol, Dept Comp Sci, Taoyuan St, Shenzhen 518055, Peoples R China.
EM cszzhang@gmail.com; hmzry10086@outlook.com; hjzhang@hit.edu.cn;
   darrenzz219@gmail.com; gcliu@nuist.edu.cn; shuicheng.yan@yitu-inc.com
RI Zhang, Zhang/JAX-2097-2023; zhang, zhang/GQZ-6804-2022; Zhang,
   Zheng/M-6325-2014; Liu, Guangcan/J-1391-2014; Yan,
   Shuicheng/HCI-1431-2022; Zhang, Haijun/N-8470-2015; Zhang,
   Zhao/B-5136-2010
OI Zhang, Zheng/0000-0003-1470-6998; Liu, Guangcan/0000-0002-9428-4387;
   Zhang, Zhao/0000-0002-5703-7969
FU National Key R&D Program of China [2018YFB1003800, 2018YFB1003805];
   National Natural Science Foundation of China [61672365, 61972112,
   61832004]
FX This work is partially supported by the National Key R&D Program of
   China (2018YFB1003800 and 2018YFB1003805), and National Natural Science
   Foundation of China (61672365, 61972112, 61832004).
CR Acharya Anish, 2019, AAAI CONF ARTIF INTE, V33, P6196
   Bao BK, 2012, IEEE T IMAGE PROCESS, V21, P3794, DOI 10.1109/TIP.2012.2192742
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chen J, 2014, J VIS COMMUN IMAGE R, V25, P763, DOI 10.1016/j.jvcir.2014.01.015
   Chen M., 2009, UILUENG092215, DOI [10.1016/j.jsb, DOI 10.1016/J.JSB]
   Chen X, 2019, IEEE ACCESS, V7, P112142, DOI 10.1109/ACCESS.2019.2934482
   Chen YY, 2020, IEEE T IMAGE PROCESS, V29, P1426, DOI 10.1109/TIP.2019.2941319
   Cunningham JP, 2015, J MACH LEARN RES, V16, P2859
   Ding ZM, 2019, IEEE T NEUR NET LEAR, V30, P1768, DOI 10.1109/TNNLS.2018.2874567
   Fomin FV, 2020, ACM T ALGORITHMS, V16, DOI 10.1145/3365653
   Fomin FV, 2020, DATA MIN KNOWL DISC, V34, P478, DOI 10.1007/s10618-019-00669-5
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Giampouras PV, 2019, IEEE T SIGNAL PROCES, V67, P490, DOI 10.1109/TSP.2018.2883921
   Graham Daniel B, 1998, CHARACTERISING VIRTU, P446
   He P, 2020, INFORM SCIENCES, V514, P131, DOI 10.1016/j.ins.2019.12.004
   Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Kang Z, 2020, KNOWL-BASED SYST, V189, DOI 10.1016/j.knosys.2019.105102
   Kang Z, 2020, NEURAL NETWORKS, V122, P279, DOI 10.1016/j.neunet.2019.10.010
   Kang Z, 2020, IEEE T CYBERNETICS, V50, P1833, DOI 10.1109/TCYB.2018.2887094
   Kim B, 2019, PROC CVPR IEEE, P9004, DOI 10.1109/CVPR.2019.00922
   Kim JH, 2015, IEEE T IMAGE PROCESS, V24, P2658, DOI 10.1109/TIP.2015.2428933
   Lan XY, 2018, IEEE T IMAGE PROCESS, V27, P2022, DOI 10.1109/TIP.2017.2777183
   Li H, 2020, MACH LEARN, V109, P103, DOI 10.1007/s10994-019-05819-w
   Li J., 2017, IEEE T IMAGE PROCESS, V27, P464, DOI 10.1109/tip.2017
   Li J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2145
   Li LY, 2014, IMAGE VISION COMPUT, V32, P814, DOI 10.1016/j.imavis.2014.02.007
   Li ZC, 2017, AAAI CONF ARTIF INTE, P4154
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Lu CY, 2019, PROC CVPR IEEE, P5989, DOI 10.1109/CVPR.2019.00615
   Lu YW, 2020, NEURAL NETWORKS, V125, P245, DOI 10.1016/j.neunet.2020.02.007
   Ma L, 2012, PROC CVPR IEEE, P2586, DOI 10.1109/CVPR.2012.6247977
   Nene S. A., 1996, COLUMBIA OBJECT IMAG
   Ren JH, 2020, IEEE T IMAGE PROCESS, V29, P3941, DOI 10.1109/TIP.2020.2965289
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Song Y, 2018, NEUROCOMPUTING, V275, P2479, DOI 10.1016/j.neucom.2017.11.021
   Song ZJ, 2020, INT J MACH LEARN CYB, V11, P71, DOI 10.1007/s13042-019-00941-6
   Su F, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/7410701
   Wang F, 2015, DATA MIN KNOWL DISC, V29, P534, DOI 10.1007/s10618-014-0356-z
   Wang L, 2019, NEURAL NETWORKS, V117, P201, DOI 10.1016/j.neunet.2019.05.007
   Wang m, 2019, P 19 IEEE INT C DAT
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Weyrauch Benjamin, 2004, IEEE COMPUTER VISION, P85, DOI DOI 10.1109/CVPR.2004.315
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Xiao H., 2017, arXiv
   Xie JC, 2017, IEEE T IMAGE PROCESS, V26, P2286, DOI 10.1109/TIP.2017.2662213
   Xue Z, 2019, INFORM SCIENCES, V482, P210, DOI 10.1016/j.ins.2019.01.018
   Zhang H, 2017, IEEE WINT CONF APPL, P1259, DOI 10.1109/WACV.2017.145
   Zhang HY, 2014, NEUROCOMPUTING, V145, P369, DOI 10.1016/j.neucom.2014.05.022
   Zhang YMZ, 2013, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2013.93
   Zhang Z, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2762
   Zhang Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1569, DOI 10.1145/3343031.3351023
   Zhang Z, 2017, NEURAL NETWORKS, V96, P55, DOI 10.1016/j.neunet.2017.08.001
   Zhang Z, 2016, IEEE T IMAGE PROCESS, V25, P2429, DOI 10.1109/TIP.2016.2547180
   Zhang Z, 2014, NEURAL NETWORKS, V53, P81, DOI 10.1016/j.neunet.2014.01.001
   Zhao JW, 2017, NEURAL NETWORKS, V94, P115, DOI 10.1016/j.neunet.2017.06.013
NR 61
TC 2
Z9 2
U1 0
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2021
VL 17
IS 1
SU S
SI SI
AR 5
DI 10.1145/3402030
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW2ZT
UT WOS:000646396900005
DA 2024-07-18
ER

PT J
AU Shao, HR
   Li, J
   Zhang, J
   Yu, H
   Sun, JD
AF Shao, Huiru
   Li, Jing
   Zhang, Jia
   Yu, Hui
   Sun, Jiande
TI Eye-based Recognition for User Identification on Mobile Devices
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Identity recognition; deep neural network; mobile device; eye
   recognition; eye movement
ID DYNAMICS; IRIS
AB User identification is becoming more and more important for Apps on mobile devices. However, the identity recognition based on eyes, e.g., iris recognition, is rarely used on mobile devices comparing with those based on face and fingerprint due to its extra cost in hardware and complicated operations during recognition. In this article, an eye-based recognition method is designed for identity recognition on mobile devices, which can be implemented just like face recognition. In the proposed method, the eye feature is composed of the static and dynamic features, where the periocular feature extracted by deep neural network from the eye image is used as the static feature, and the motion feature of saccadic velocity is selected as the dynamic feature. The eye images can be captured by the normal camera on mobile devices just like faces, and dynamic features can provide living information to increase the difficulty of forgery. The GazeCapture dataset is used to test the proposed method, because the eye images in this dataset are captured by mobile devices during daily use. The recognition accuracy of the proposed method on the GazeCapture dataset can reach 96.87% only based on the periocular feature and can be enhanced to 97.99% when it is fused with the saccadic feature. The experiment results show that the performance of the proposed method can be comparative to that of iris recognition methods. It demonstrates that the proposed method is a practical reference for the eye-based identity recognition, and the proposed method provides one more biometric choice for mobile devices.
C1 [Shao, Huiru; Zhang, Jia; Sun, Jiande] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan, Peoples R China.
   [Li, Jing] Shandong Management Univ, Sch Mech & Elect Engn, Jinan, Peoples R China.
   [Yu, Hui] Univ Portsmouth, Sch Creat Technol, Portsmouth, Hants, England.
C3 Shandong Normal University; Shandong Management University; University
   of Portsmouth
RP Sun, JD (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan, Peoples R China.
EM huirushao@hotmail.com; lijingjdsun@hotrnail.com; zhangjia@sdnu.edu.cn;
   hui.yu@port.ac.uk; jiandesun@hotmail.com
RI Yu, Hui/G-1115-2018; Zhang, Yuchen/GYI-8858-2022
OI Yu, Hui/0000-0002-7655-9228; 
FU Natural Science Foundation for Distinguished Young Scholars of Shandong
   Province [JQ201718]; Natural Science Foundation of China [U1736122]
FX This work is supported by Natural Science Foundation for Distinguished
   Young Scholars of Shandong Province (JQ201718) and the Natural Science
   Foundation of China (U1736122).
CR Bhardwaj I, 2019, IETE TECH REV, V36, P178, DOI 10.1080/02564602.2018.1444515
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   Joshi A., 2012, ADV INTELLIGENT SOFT, V166, P1023
   Komogortsev O, 2016, IEEE T INF FOREN SEC, V11, P621, DOI 10.1109/TIFS.2015.2503263
   Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Li CC, 2015, VISUAL COMPUT, V31, P1419, DOI 10.1007/s00371-014-1023-5
   Li Y, 2018, NEUROCOMPUTING, V297, P50, DOI 10.1016/j.neucom.2018.02.037
   Liu NAF, 2016, PATTERN RECOGN LETT, V82, P154, DOI 10.1016/j.patrec.2015.09.016
   Lohr DJ, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P315, DOI 10.1145/2857491.2884058
   Mukhopadhyay S, 2018, MACH LEARN, V107, P313, DOI 10.1007/s10994-017-5649-1
   Park U, 2011, IEEE T INF FOREN SEC, V6, P96, DOI 10.1109/TIFS.2010.2096810
   Proença H, 2018, IEEE T INF FOREN SEC, V13, P888, DOI 10.1109/TIFS.2017.2771230
   Rigas I, 2014, IEEE T INF FOREN SEC, V9, P1743, DOI 10.1109/TIFS.2014.2350960
   Roy Kaushik, 2007, 2007 10th International Conference on Computer and Information Technology (ICCIT 2007), P1, DOI 10.1109/ICCITECHN.2007.4579426
   Shao Huiru, 2020, MULTIMEDIA TOOLS APP, V79, P1
   Sun JD, 2018, MULTIMED TOOLS APPL, V77, P24909, DOI 10.1007/s11042-018-5722-1
   Sun ZN, 2009, IEEE T PATTERN ANAL, V31, P2211, DOI 10.1109/TPAMI.2008.240
   Tiong L.C.O., 2019, INT CONF BIOMETR, P1, DOI DOI 10.1109/icb45273.2019.8987278
   Wang W, 2016, IEEE IMAGE PROC, P3151, DOI 10.1109/ICIP.2016.7532940
   Wang Z, 2018, IEEE ACCESS, V6, P17905, DOI 10.1109/ACCESS.2018.2812208
   Zhang J, 2019, SIGNAL PROCESS, V160, P164, DOI 10.1016/j.sigpro.2019.02.025
   Zhang Q, 2018, IEEE T INF FOREN SEC, V13, P2897, DOI 10.1109/TIFS.2018.2833033
   Zhao Z, 2017, IEEE I CONF COMP VIS, P3829, DOI 10.1109/ICCV.2017.411
   Zhao ZJ, 2017, IEEE T INF FOREN SEC, V12, P1017, DOI 10.1109/TIFS.2016.2636093
NR 26
TC 3
Z9 4
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2021
VL 16
IS 4
AR 117
DI 10.1145/3399659
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4CR
UT WOS:000614088800001
DA 2024-07-18
ER

PT J
AU Jiang, YZ
   Gu, XQ
   Ji, DC
   Qian, PJ
   Xue, J
   Zhang, YP
   Zhu, JQ
   Xia, KJ
   Wang, ST
AF Jiang, Yizhang
   Gu, Xiaoqing
   Ji, Dingcheng
   Qian, Pengjiang
   Xue, Jing
   Zhang, Yuanpeng
   Zhu, Jiaqi
   Xia, Kaijian
   Wang, Shitong
TI Smart Diagnosis: A Multiple-Source Transfer TSK Fuzzy System for EEG
   Seizure Identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Epileptic identification; EEG signals; multiple source transfer
   learning; fuzzy system; manifold regularization learning
ID STATISTICAL COMPARISONS; DOMAIN ADAPTATION; REGULARIZATION;
   CLASSIFICATION; CLASSIFIERS
AB To effectively identify electroencephalogram (EEG) signals in multiple-source domains, a multiple-source transfer learning-based Takagi-Sugeno Kang (TSK) fuzzy system (FS), called MST-TSK, is proposed, which combines multiple-source transfer learning and manifold regularization (MR) learning mechanisms together into the TSK-FS framework. Specifically, the advantages of MST-TSK include the following: (1) by evaluating the significance of each source domain (SD), a flexible domain entropy-weighting index is presented; (2) using the theory of sample transfer learning, a reweighting strategy is presented to weigh the prediction of unknown samples in the target domain (I'D) and the output of the source prediction functions; (3) by taking into account the MR term, the manifold structure of the TD is effectively maintained in the proposed system; and (4) by inheriting the interpretability of TSK-FS, MST-TSK displays good interpretability in identifying EEG signals that are understandable by humans (domain experts). The effectiveness of the proposed FS is demonstrated in several EEG multiple-source transfer learning tasks.
C1 [Jiang, Yizhang; Ji, Dingcheng; Qian, Pengjiang; Zhu, Jiaqi; Wang, Shitong] Jiangnan Univ, Sch Digital Media, 1800 Lihu Ave, Wuxi 214122, Jiangsu, Peoples R China.
   [Gu, Xiaoqing] Changzhou Univ, Sch Informat Sci & Engn, Changzhou 213164, Peoples R China.
   [Xue, Jing] Nanjing Med Univ, Affiliated Wuxi Peoples Hosp, Dept Nephrol, 299 Qingyang Rd, Wuxi 214023, Jiangsu, Peoples R China.
   [Zhang, Yuanpeng] Nantong Univ, Dept Med Informat, 9 Qixiu Rd, Nantong 226001, Jiangsu, Peoples R China.
   [Xia, Kaijian] Changshu 1 Peoples Hosp, Changshu 215500, Jiangsu, Peoples R China.
C3 Jiangnan University; Changzhou University; Nanjing Medical University;
   Nantong University
RP Xia, KJ (corresponding author), Changshu 1 Peoples Hosp, Changshu 215500, Jiangsu, Peoples R China.
EM yzjiang@jiangnan.edu.cn; guxq@cczu.edu.cn; know4thyself@gmail.com;
   qianpjiang@jiangnan.edu.cn; xuejing19880609@sina.com;
   maxbirdzhang@ntu.edu.cn; jiachess@163.com; xiakaijian@163.com;
   wxwangst@aliyun.com
RI li, tong/JYO-7530-2024; Gu, Xiaoqing/GPP-6913-2022; Liu,
   Yixuan/JFJ-2820-2023; Jiang, Yizhang/V-2171-2019
OI Jiang, Yizhang/0000-0002-4558-9803; Zhang, Yuanpeng/0000-0003-1736-3425;
   Qian, Pengjiang/0000-0002-5596-3694
FU National Natural Science Foundation of China [61702225, 61806026];
   Natural Science Foundation of Jiangsu Province [BK20160187, BK20180956];
   Six Talent Peaks Project of Jiangsu Province [XYDXX-127]; Science and
   Technology demonstration project of social development of Wuxi
   [WX18IVJN002]; Youth Foundation of the Commission of Health and Family
   Planning of Wuxi [Q201654]; Jiangsu Committee of Health [H2018071]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61702225 and 61806026, by the Natural Science
   Foundation of Jiangsu Province under Grants BK20160187 and BK20180956,
   by the 2018 Six Talent Peaks Project of Jiangsu Province under Grant
   XYDXX-127, by the Science and Technology demonstration project of social
   development of Wuxi under Grant WX18IVJN002, by the Youth Foundation of
   the Commission of Health and Family Planning of Wuxi under Grant
   Q201654, and by the Jiangsu Committee of Health under Grant H2018071.
CR Aarabi A, 2009, CLIN NEUROPHYSIOL, V120, P1648, DOI 10.1016/j.clinph.2009.07.002
   Abid F., 2017, BIOCH BIOINFORMATICS, V7, P141, DOI [10.17706/ijbbb.2017.7.3, DOI 10.17706/IJBBB.2017.7.3.143-152]
   Abonyi J., 2003, FUZZY MODEL IDENTIFI, P87
   Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907
   [Anonymous], 2009, ADV NEURAL INFORM PR
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Deng ZH, 2014, IEEE T CYBERNETICS, V44, P2585, DOI 10.1109/TCYB.2014.2311014
   Deng ZH, 2013, IEEE T FUZZY SYST, V21, P597, DOI 10.1109/TFUZZ.2012.2212444
   Deng ZH, 2011, IEEE T FUZZY SYST, V19, P210, DOI 10.1109/TFUZZ.2010.2091961
   Duan LX, 2012, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2012.6247819
   Duan Lixin, 2012, IEEE Trans Neural Netw Learn Syst, V23, P504, DOI 10.1109/TNNLS.2011.2178556
   García S, 2008, J MACH LEARN RES, V9, P2677
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   GRIFFIN DW, 1984, IEEE T ACOUST SPEECH, V32, P236, DOI 10.1109/TASSP.1984.1164317
   Güler I, 2007, IEEE T INF TECHNOL B, V11, P117, DOI 10.1109/TITB.2006.879600
   Harpale VK, 2016, 2016 INT C MICR COMP, DOI [10.1109/MicroCom.2016.7522581, DOI 10.1109/MICROCOM.2016.7522581]
   Iscan Z, 2011, EXPERT SYST APPL, V38, P10499, DOI 10.1016/j.eswa.2011.02.110
   Jayaram V, 2016, IEEE COMPUT INTELL M, V11, P20, DOI 10.1109/MCI.2015.2501545
   Jiang YZ, 2017, IEEE T NEUR SYS REH, V25, P2270, DOI 10.1109/TNSRE.2017.2748388
   Jiang YZ, 2017, IEEE T FUZZY SYST, V25, P3, DOI 10.1109/TFUZZ.2016.2637405
   Kabir Enamul, 2016, Brain Inform, V3, P93, DOI 10.1007/s40708-015-0030-2
   KARAYIANNIS NB, 1994, PROCEEDINGS OF THE THIRD IEEE CONFERENCE ON FUZZY SYSTEMS - IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE, VOLS I-III, P630, DOI 10.1109/FUZZY.1994.343658
   Liu YX, 2012, IEEE T NEUR SYS REH, V20, P749, DOI 10.1109/TNSRE.2012.2206054
   Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Lughofer E, 2011, STUD FUZZ SOFT COMP, V266, P1, DOI 10.1007/978-3-642-18087-3
   MAMDANI EH, 1977, IEEE T COMPUT, V26, P1182, DOI 10.1109/TC.1977.1674779
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pedrycz W, 2007, FUZZY SYSTEMS ENGINEERING: TOWARD HUMAN-CENTRIC COMPUTING, P1, DOI 10.1002/9780470168967
   Pratama M, 2014, IEEE T FUZZY SYST, V22, P547, DOI 10.1109/TFUZZ.2013.2264938
   Qi FF, 2015, IEEE T NEUR NET LEAR, V26, P3070, DOI 10.1109/TNNLS.2015.2402694
   Rabbi AF, 2012, COMPUT INTEL NEUROSC, V2012, DOI 10.1155/2012/705140
   Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583, DOI 10.1007/BFb0020217
   Srinivasan V, 2005, J Med Syst, V29, P647, DOI 10.1007/s10916-005-6133-1
   Sun SL, 2015, INFORM FUSION, V24, P84, DOI 10.1016/j.inffus.2014.12.003
   TAKAGI T, 1985, IEEE T SYST MAN CYB, V15, P116, DOI 10.1109/TSMC.1985.6313399
   Tzallas AT, 2009, IEEE T INF TECHNOL B, V13, P703, DOI 10.1109/TITB.2009.2017939
   Wang D, 2013, IEEE T INF FOREN SEC, V8, P520, DOI 10.1109/TIFS.2013.2244884
   Wu DR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0056624
   Wu T, 2008, MEASUREMENT, V41, P618, DOI 10.1016/j.measurement.2007.07.007
   Xie LX, 2019, IEEE T CYBERNETICS, V49, P2200, DOI 10.1109/TCYB.2018.2821764
   Xu ZL, 2010, IEEE T NEURAL NETWOR, V21, P1033, DOI 10.1109/TNN.2010.2047114
   Yang CJ, 2014, ARTIF INTELL MED, V62, P165, DOI 10.1016/j.artmed.2014.10.002
   Yang J., 2007, P 15 ACM INT C MULT, P188
NR 45
TC 11
Z9 11
U1 2
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 59
DI 10.1145/3340240
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FH
UT WOS:000583712600002
DA 2024-07-18
ER

PT J
AU Gelli, F
   Uricchio, T
   He, XN
   Del Bimbo, A
   Chua, TS
AF Gelli, Francesco
   Uricchio, Tiberio
   He, Xiangnan
   Del Bimbo, Alberto
   Chua, Tat-Seng
TI Learning Visual Elements of Images for Discovery of Brand Posts
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Content discovery; image ranking; computational marketing
AB Online Social Network Sites have become a primary platform for brands and organizations to engage their audience by sharing image and video posts on their timelines. Different from traditional advertising, these posts are not restricted to the products or logo but include visual elements that express more in general the values and attributes of the brand, called brand associations. Since marketers are increasingly spending time in discovering and re-posting user generated posts that reflect the brand attributes, there is an increasing demand for such discovery systems. The goal of these systems is to assist brand experts in filtering through online collections of new user media to discover actionable posts, which match the brand value and have the potential to engage the consumers. Driven by this real-life application, we define and formulate a new task of content discovery for brands and propose a framework that learns to rank posts for brands from their historical timeline. We design a Personalized Content Discovery (PCD) framework to address the three challenges of high inter-brand similarity, sparsity of brand-post interactions, and diversification of timeline. To learn fine-grained brand representation and to generate explanations for the ranking, we automatically learn visual elements of posts from the timeline of brands and from a set of brand attributes in the domain of marketing. To test our framework we use two large-scale Instagram datasets that contain a total of more than 1.5 million image and video posts from the historical timeline of hundreds of brands from multiple verticals such as food and fashion. Extensive experiments indicate that our model can effectively learn fine-grained brand representations and outperform the closest state-of-the-art solutions.
C1 [Gelli, Francesco; Chua, Tat-Seng] Natl Univ Singapore, Singapore, Singapore.
   [Uricchio, Tiberio; Del Bimbo, Alberto] Univ Firenze, Florence, Italy.
   [He, Xiangnan] Univ Sci & Technol China, Hefei, Peoples R China.
C3 National University of Singapore; University of Florence; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Uricchio, T (corresponding author), Univ Firenze, Florence, Italy.; He, XN (corresponding author), Univ Sci & Technol China, Hefei, Peoples R China.
EM francesco.gelli@u.nus.edu; tiberio.uricchio@unifi.it;
   xiangnanhe@gmail.com; alberto.delbimbo@unifi.it; chuats@comp.nus.edu.sg
OI URICCHIO, TIBERIO/0000-0003-1025-4541
FU National Research Foundation, Prime Minister's Office, Singapore under
   its IRC@SG Funding Initiative
FX NExT++ research is supported by the National Research Foundation, Prime
   Minister's Office, Singapore under its IRC@SG Funding Initiative.
CR Aaker JL, 1997, J MARKETING RES, V34, P347, DOI 10.2307/3151897
   Alboqami Hassan, 2015, INT J INTERNET MARKE, V4, P338
   [Anonymous], 2016, ACM MULTIMEDIA
   [Anonymous], 2014, P INT C MULT RETR
   [Anonymous], 2014, THESIS
   [Anonymous], 2012, ABS12125701 CORR
   [Anonymous], 2017, P 2017 ACM INT C MUL, DOI DOI 10.1145/3078971.3078998
   [Anonymous], 1997, Information theory and statistics
   Ashley C, 2015, PSYCHOL MARKET, V32, P15, DOI 10.1002/mar.20761
   Barlow J., 2004, Branded customer service: The new competitive edge
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Cui P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P597, DOI 10.1145/2647868.2654946
   Cvijikj IP, 2013, SOC NETW ANAL MIN, V3, P843, DOI 10.1007/s13278-013-0098-8
   de Vries L, 2012, J INTERACT MARK, V26, P83, DOI 10.1016/j.intmar.2012.01.003
   Farseev Aleksandr, 2018, P 26 ACM INT C MULT
   Gelli F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2263, DOI 10.1145/3343031.3350574
   Gelli F, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P465, DOI 10.1145/3240508.3240689
   Gelli F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1828, DOI 10.1145/3123266.3127909
   Geng X, 2015, IEEE I CONF COMP VIS, P4274, DOI 10.1109/ICCV.2015.486
   Gensler S, 2013, J INTERACT MARK, V27, P242, DOI 10.1016/j.intmar.2013.09.004
   Goh KY, 2013, INFORM SYST RES, V24, P88, DOI 10.1287/isre.1120.0469
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He XH, 2015, BMC CANCER, V15, DOI 10.1186/s12885-015-1027-1
   He XN, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P355, DOI 10.1145/3077136.3080777
   Hidasi B, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P241, DOI 10.1145/2959100.2959167
   Jayasingh S., 2015, ASIAN SOCIAL SCI, V11, DOI DOI 10.5539/ASS.V11N26P19
   Kang WC, 2019, PROC CVPR IEEE, P10524, DOI 10.1109/CVPR.2019.01078
   Kang WC, 2017, IEEE DATA MINING, P207, DOI 10.1109/ICDM.2017.30
   Kim G, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P623, DOI 10.1145/2556195.2556212
   Kim G, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P404, DOI 10.1109/ICCVW.2013.60
   Kim J, 2012, IEEE I C EMBED SOFTW, P855, DOI 10.1109/HPCC.2012.121
   Lei CY, 2016, PROC CVPR IEEE, P2545, DOI 10.1109/CVPR.2016.279
   Litsa Tereza, 2016, RISE USER GENERATED
   Lovett M, 2014, MARKET SCI, V33, P609, DOI 10.1287/mksc.2014.0861
   Ma Shuang, 2018, ABS180206451 CORR
   Mazloom M., 2016, P ACM MULT, P197, DOI [10.1145/2964284.2967210, 10]
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Niu W, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P423, DOI 10.1145/3159652.3159728
   Olenski Steve, 2016, CAN YOUR CONTENT SEL
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Saveski M, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P89, DOI 10.1145/2645710.2645751
   Sprague RH, 2014, P ANN HICSS, P1785, DOI 10.1109/HICSS.2014.226
   Wang SS, 2018, ELECTRON COMMER R A, V28, P73, DOI 10.1016/j.elerap.2017.12.010
   Wu Y, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P153, DOI 10.1145/2835776.2835837
   Yu WH, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P649, DOI 10.1145/3178876.3186146
   Zhang FZ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P353, DOI 10.1145/2939672.2939673
   Zhang H, 2013, ADV BIOM ENG, V14, P33
   Zhang Y, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1583, DOI 10.1145/3357384.3358042
NR 49
TC 3
Z9 3
U1 2
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 56
DI 10.1145/3385413
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA OJ1EO
UT WOS:000583710600017
DA 2024-07-18
ER

PT J
AU Liu, JY
   Song, SJ
   Liu, CH
   Li, YH
   Hu, YY
AF Liu, Jiaying
   Song, Sijie
   Liu, Chunhui
   Li, Yanghao
   Hu, Yueyu
TI A Benchmark Dataset and Comparison Study for Multi-modal Human Action
   Analytics
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Benchmark; multi-modal; action detection; action recognition
ID ACTION RECOGNITION; EVENT
AB Large-scale benchmarks provide a solid foundation for the development of action analytics. Most of the previous activity benchmarks focus on analyzing actions in RGB videos. There is a lack of large-scale and high-quality benchmarks for multi-modal action analytics. In this article, we introduce PKU Multi-Modal Dataset (PKU-MMD), a new large-scale benchmark for multi-modal human action analytics. It consists of about 28,000 action instances and 6.2 million frames in total and provides high-quality multi-modal data sources, including RGB, depth, infrared radiation (IR), and skeletons. To make PKU-MMD more practical, our dataset comprises two subsets under different settings for action understanding, namely Part I and Part II. Part I contains 1,076 untrimmed video sequences with 51 action classes performed by 66 subjects, while Part II contains 1,009 untrimmed video sequences with 41 action classes performed by 13 subjects. Compared to Part I, Part II is more challenging due to short action intervals, concurrent actions and heavy occlusion. PKU-MMD can be leveraged in two scenarios: action recognition with trimmed video clips and action detection with untrimmed video sequences. For each scenario, we provide benchmark performance on both subsets by conducting different methods with different modalities under two evaluation protocols, respectively. Experimental results show that PKU-MMD is a significant challenge to many state-of-the-art methods. We further illustrate that the features learned on PKU-MMD can be well transferred to other datasets. We believe this large-scale dataset will boost the research in the field of action analytics for the community.
C1 [Liu, Jiaying; Song, Sijie; Liu, Chunhui; Li, Yanghao; Hu, Yueyu] Peking Univ, Inst Comp Sci & Technol, Zhongguancun North St 128, Beijing, Peoples R China.
C3 Peking University
RP Liu, JY (corresponding author), Peking Univ, Inst Comp Sci & Technol, Zhongguancun North St 128, Beijing, Peoples R China.
EM liujiaying@pku.edu.cn; ssj940920@pku.edu.cn; liuchunhui@pku.edu.cn;
   lyttonhao@pku.edu.cn; huyy@pku.edu.cn
RI wang, yitian/JFA-6804-2023; Liu, JY/GYJ-0138-2022
OI Liu, Jiaying/0000-0002-0468-9576
FU National Natural Science Foundation of China [61772043]; Beijing Natural
   Science Foundation [4192025]; Microsoft Research Asia
   [FY19-Research-Sponsorship-115]; Peking University Tencent Rhino Bird
   Innovation Fund
FX This work was supported by National Natural Science Foundation of China
   under contract No. 61772043, Beijing Natural Science Foundation under
   contract No. 4192025, Microsoft Research Asia
   (FY19-Research-Sponsorship-115) and Peking University Tencent Rhino Bird
   Innovation Fund.
CR Amiri S. M., 2013, 2013 IEEE 15 INT C E, P606
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2013, P IEEE C COMP VIS PA
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2012, CoRR
   Bloom V., 2012, 2012 IEEE COMP SOC C, P7, DOI [DOI 10.1109/CVPRW.2012.6239175, 10.1109/CVPRW.2012.6239175]
   Blumenstock A, 2007, IEEE DATA MINING, P53, DOI 10.1109/ICDM.2007.29
   Bo Li, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P613, DOI 10.1109/ICMEW.2017.8026283
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cai ZY, 2017, MULTIMED TOOLS APPL, V76, P4313, DOI 10.1007/s11042-016-3374-6
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Cheng ZW, 2012, LECT NOTES COMPUT SC, V7584, P52, DOI 10.1007/978-3-642-33868-7_6
   Chenxia Wu, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P4362, DOI 10.1109/CVPR.2015.7299065
   CMU, 2003, Graphics lab motion capture database
   De Geest R, 2016, LECT NOTES COMPUT SC, V9909, P269, DOI 10.1007/978-3-319-46454-1_17
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Du Y, 2016, IEEE T IMAGE PROCESS, V25, P3010, DOI 10.1109/TIP.2016.2552404
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fothergill S., 2012, P SIGCHI C HUM FACT, P1737, DOI DOI 10.1145/2207676.2208303
   He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502
   Hongsong Wang, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P591, DOI 10.1109/ICMEW.2017.8026278
   Hu JF, 2017, IEEE T PATTERN ANAL, V39, P2186, DOI 10.1109/TPAMI.2016.2640292
   Hu Yueyu, 2017, P BRIT MACHINE VISIO, P1
   Huang M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177757
   Jain A, 2013, PROC CVPR IEEE, P2571, DOI 10.1109/CVPR.2013.332
   Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100
   Jiang F, 2015, J MACH LEARN RES, V16, P227
   Jiang YG, 2015, IEEE T IMAGE PROCESS, V24, P3781, DOI 10.1109/TIP.2015.2456412
   Jiang Zhuolin, 2017, P IEEE C COMPUTER VI, P115
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Li YH, 2016, LECT NOTES COMPUT SC, V9911, P203, DOI 10.1007/978-3-319-46478-7_13
   Lillo I, 2014, PROC CVPR IEEE, P812, DOI 10.1109/CVPR.2014.109
   Liu AA, 2015, SIGNAL PROCESS, V112, P74, DOI 10.1016/j.sigpro.2014.08.038
   Liu CY, 2017, AER ADV ENG RES, V120, P1, DOI 10.1109/glocomw.2017.8269175
   Liu JJ, 2017, CHIN CONTR CONF, P2432, DOI 10.23919/ChiCC.2017.8027723
   Luo Y, 2018, INTERNATIONAL SYMPOSIUM 2018 - SOCIAL SCIENCE MANAGEMENT AND INNOVATION, P174
   Mahasseni B, 2016, PROC CVPR IEEE, P3054, DOI 10.1109/CVPR.2016.333
   Miao J, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2490551
   Hoai M, 2014, INT J COMPUT VISION, V107, P191, DOI 10.1007/s11263-013-0683-3
   Ofli F, 2013, IEEE WORK APP COMP, P53, DOI 10.1109/WACV.2013.6474999
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389
   Rahmani H, 2014, LECT NOTES COMPUT SC, V8690, P742, DOI 10.1007/978-3-319-10605-2_48
   Raptis M, 2012, PROC CVPR IEEE, P1242, DOI 10.1109/CVPR.2012.6247807
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sharaf A, 2015, IEEE WINT CONF APPL, P998, DOI 10.1109/WACV.2015.138
   Shi ZY, 2017, PROC CVPR IEEE, P4684, DOI 10.1109/CVPR.2017.498
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Sung J., 2011, PROC AAAI C ARTIF IN, P47
   Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591
   Tian YC, 2013, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2013.341
   Vemulapalli R, 2016, PROC CVPR IEEE, P4471, DOI 10.1109/CVPR.2016.484
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang KZ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P97, DOI 10.1145/2647868.2654912
   Wang L, 2015, MEDIATORS INFLAMM, V2015, P2015, DOI DOI 10.1155/2015/108417
   Wang L., 2016, P ECCV
   Wang LM, 2014, LECT NOTES COMPUT SC, V8693, P565, DOI 10.1007/978-3-319-10602-1_37
   Wang LM, 2013, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2013.345
   Wang Limin, 2014, THUMOS
   Wei P, 2013, IEEE I CONF COMP VIS, P3272, DOI 10.1109/ICCV.2013.406
   Wei P, 2013, IEEE I CONF COMP VIS, P3136, DOI 10.1109/ICCV.2013.389
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Ye J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3038917
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342
   Zhang JX, 2016, OXID MED CELL LONGEV, V2016, DOI 10.1155/2016/4350965
   Zhang L, 2018, NANO-MICRO LETT, V10, DOI 10.1007/s40820-017-0178-9
   Zhang Y, 2017, IEEE I CONF COMP VIS, P2116, DOI 10.1109/ICCV.2017.231
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhu J, 2013, IEEE I CONF COMP VIS, P3559, DOI 10.1109/ICCV.2013.442
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
   Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316
NR 86
TC 27
Z9 27
U1 2
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 41
DI 10.1145/3365212
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600002
DA 2024-07-18
ER

PT J
AU Chen, B
   Ruan, LY
   Lam, ML
AF Chen, Bin
   Ruan, Lingyan
   Lam, Miu-Ling
TI LFGAN: 4D Light Field Synthesis from a Single RGB Image
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Light field synthesis; generative adversarial network; single image
   superresolution
AB We present a deep neural network called the light field generative adversarial network (LFGAN) that synthesizes a 4D light field from a single 2D RGB image. We generate light fields using a single image superresolution (SISR) technique based on two important observations. First, the small baseline gives rise to the high similarity between the full light field image and each sub-aperture view. Second, the occlusion edge at any spatial coordinate of a sub-aperture view has the same orientation as the occlusion edge at the corresponding angular patch, implying that the occlusion information in the angular domain can be inferred from the sub-aperture local information. We employ the Wasserstein GAN with gradient penalty (WGANGP) to learn the color and geometry information from the light field datasets. The network can generate a plausible 4D light field comprising 8 x 8 angular views from a single sub-aperture 2D image. We propose new loss terms, namely epipolar plane image (EPI) and brightness regularization (BRI) losses, as well as a novel multi-stage training framework to feed the loss terms at different time to generate superior light fields. The EPI loss can reinforce the network to learn the geometric features of the light fields, and the BRI loss can preserve the brightness consistency across different sub-aperture views. Two datasets have been used to evaluate our method: in addition to an existing light field dataset capturing scenes of flowers and plants, we have built a large dataset of toy animals consisting of 2,100 light fields captured with a plenoptic camera. We have performed comprehensive ablation studies to evaluate the effects of individual loss terms and the multi-stage training strategy, and have compared LFGAN to other state-of-the-art techniques. Qualitative and quantitative evaluation demonstrates that LFGAN can effectively estimate complex occlusions and geometry in challenging scenes, and outperform other existing techniques.
C1 [Chen, Bin; Ruan, Lingyan; Lam, Miu-Ling] City Univ Hong Kong, Sch Creat Media, Kowloon, 18 Tat Hong Ave, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Lam, ML (corresponding author), City Univ Hong Kong, Sch Creat Media, Kowloon, 18 Tat Hong Ave, Hong Kong, Peoples R China.
EM binorchen@cityu.edu.hk; lyruanruan@cityu.edu.hk; miu.lam@cityu.edu.hk
RI Wang, Zejun/KBB-8454-2024; xu, chen/JNE-5010-2023
OI RUAN, Lingyan/0000-0001-7799-9148
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 11206216]; ACIM-SCM
FX This work was partially supported by grants from the Research Grants
   Council of the Hong Kong Special Administrative Region, China (CityU
   11206216) and ACIM-SCM.
CR [Anonymous], 2015, ADV NEURAL INFORM PR
   [Anonymous], 2017, ARXIV170404886
   [Anonymous], 2009, ACM TOG
   [Anonymous], 2015, CVPRW
   [Anonymous], 2017, CVPR
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2009, INT C COMP PHOT
   [Anonymous], 2014, AUTOENCODING VARIATI
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   [Anonymous], 2017, ARXIV170400028CSLG
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Arjovsky Martin, 2017, P INT C LEARN REPR
   Bishop TE, 2012, IEEE T PATTERN ANAL, V34, P972, DOI 10.1109/TPAMI.2011.168
   BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Denton Emily, 2015, Advances in Neural Information Processing Systems
   Didyk P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508376
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Finley DR, 2006, HSP Color Model-Alternative to HSV (HSB) and HSL
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Funkhouser T.A., 2015, P COMP VIS PATT REC
   Georgiev T., 2009, SUPERRESOLUTION PLEN
   Georgiev T. G., 2006, P 17 EUROGRAPHICS C, V2006
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Haack CI, 2014, CURR SURG REP, V2, DOI 10.1007/s40137-014-0071-0
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Karras T., 2018, INT CONFLEARN REPRES
   Kauvar I, 2015, ACM T GRAPHIC, V34, DOI [10.1145/2682631, 10.1145/2816795.2818070]
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Levin A, 2008, LECT NOTES COMPUT SC, V5305, P88, DOI 10.1007/978-3-540-88693-8_7
   Levin A, 2010, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR.2010.5539854
   Levin A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531403
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mateos J, 2009, IEEE IMAGE PROC, P129, DOI 10.1109/ICIP.2009.5414169
   Metz Luke, 2016, P INT C LEARN REPR
   Mitra K., 2012, P IEEE COMP SOC C CO, P22
   Radford Alec, 2015, P INT C LEARN REPR
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Sangkloy Patsorn, 2017, P IEEE C COMP VIS PA, V2
   Srinivasan Pratul P, 2017, International Conference on Computer Vision (ICCV), V2, P6
   Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20
   Vagharshakyan S, 2018, IEEE T PATTERN ANAL, V40, P133, DOI 10.1109/TPAMI.2017.2653101
   Vagharshakyan S, 2017, IEEE J-STSP, V11, P1082, DOI 10.1109/JSTSP.2017.2738617
   Wang TC, 2015, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2015.398
   Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147
   Wanner S, 2012, LECT NOTES COMPUT SC, V7576, P608, DOI 10.1007/978-3-642-33715-4_44
   Wu GC, 2017, PROC CVPR IEEE, P1638, DOI 10.1109/CVPR.2017.178
   Xian Wenqi, 2018, P IEEE C COMP VIS PA
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
NR 56
TC 10
Z9 10
U1 2
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 2
DI 10.1145/3366371
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100001
OA Bronze
DA 2024-07-18
ER

PT J
AU Li, MP
   Zhou, ZM
   Liu, XG
AF Li, Miaopeng
   Zhou, Zimeng
   Liu, Xinguo
TI Cross Refinement Techniques for Markerless Human Motion Capture
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human pose estimation; convolutional neural network; camera calibration;
   epipolar geometry
ID PICTORIAL STRUCTURES; POSE ESTIMATION
AB This article presents a global 3D human pose estimation method for markerless motion capture. Given two calibrated images of a person, it first obtains the 2D joint locations in the images using a pre-trained 2D Pose CNN, then constructs the 3D pose based on stereo triangulation. To improve the accuracy and the stability of the system, we propose two efficient optimization techniques for the joints. The first one, called cross-view refinement, optimizes the joints based on epipolar geometry. The second one, called cross-joint refinement, optimizes the joints using bone-length constraints. Our method automatically detects and corrects the unreliable joint, and consequently is robust against heavy occlusion, symmetry ambiguity, motion blur, and highly distorted poses. We evaluate our method on a number of benchmark datasets covering indoors and outdoors, which showed that our method is better than or on par with the state-of-the-art methods. As an application, we create a 3D human pose dataset using the proposed motion capture system, which contains about 480K images of both indoor and outdoor scenes, and demonstrate the usefulness of the dataset for human pose estimation.
C1 [Li, Miaopeng; Zhou, Zimeng; Liu, Xinguo] Zhejiang Univ, Key Lab CAD&CG, Hangzhou, Peoples R China.
C3 Zhejiang University
RP Li, MP (corresponding author), Zhejiang Univ, Key Lab CAD&CG, Hangzhou, Peoples R China.
EM li_miaopeng@zju.edu.cn; zmzhou@zju.edu.cn; xgliu@cad.zju.edu.cn
FU NSFC [61872317]; FaceUnity Technology
FX This work was partially supported by NSFC (No. 61872317) and FaceUnity
   Technology.
CR Akhter Ijaz, 2015, P IEEE CVPR
   Amin Sikandar, 2013, P BMVC
   Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   [Anonymous], 2016, P BMVC
   [Anonymous], 2017, Computer Vision and Pattern Recognition, 2017 IEEE Computer Society Conference on
   [Anonymous], 2014, P ADV NEUR INF PROC
   Balazia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152124
   Belagiannis V, 2016, IEEE T PATTERN ANAL, V38, P1929, DOI 10.1109/TPAMI.2015.2509986
   Belagiannis V, 2014, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR.2014.216
   Bergtholdt Martin, 2010, International Journal of Computer Vision, V87, P93, DOI 10.1007/s11263-009-0209-1
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Burenius M, 2013, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2013.464
   Cao Zhe, 2017, P IEEE CVPR
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen XW, 2014, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MANAGEMENT AND ENGINEERING (CME 2014), P1737
   Chen Xipeng, 2019, P IEEE CVPR
   Chen Yen-Lin, 2009, P ACCV
   Elhayek A, 2017, IEEE T PATTERN ANAL, V39, P501, DOI 10.1109/TPAMI.2016.2557779
   Elhayek A, 2015, PROC CVPR IEEE, P3810, DOI 10.1109/CVPR.2015.7299005
   Fang Haoshu, 2017, 171006513 ARXIV
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Ho ESL, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487274
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Kazemi Vahid, 2013, P BMVC
   Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Li Miaopeng, 2018, P IEEE ICPR
   Li SJ, 2015, IEEE I CONF COMP VIS, P2848, DOI 10.1109/ICCV.2015.326
   Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23
   Lin QX, 2017, COMMUN MATH BIOL NEU
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Marcos-Ramiro A, 2015, IEEE T MULTIMEDIA, V17, P1721, DOI 10.1109/TMM.2015.2464152
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta A, 2017, OBES SCI PRACT, V3, P3, DOI 10.1002/osp4.84
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138
   Pavlakos Georgios, 2017, 170404793 ARXIV
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Ramakrishna V., 2012, ECCV
   Sanzari M, 2016, LECT NOTES COMPUT SC, V9912, P566, DOI 10.1007/978-3-319-46484-8_34
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4
   Sigal L, 2010, INT J COMPUT VISION, V87, P1, DOI 10.1007/s11263-009-0293-2
   Su Y, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3180420
   Sun Xiao, 2017, P IEEE ICCV
   Taylor GW, 2010, PROC CVPR IEEE, P631, DOI 10.1109/CVPR.2010.5540157
   Tekin B, 2016, PROC CVPR IEEE, pCP8, DOI 10.1109/CVPR.2016.113
   Tekin Bugra, 2017, P IEEE ICCV
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wandt B, 2016, IEEE T PATTERN ANAL, V38, P1505, DOI 10.1109/TPAMI.2016.2553028
   Wandt Bastian, 2018, P ECCV
   Wang Chunyu, 2014, P IEEE CVPR
   Wei Shih-En., 2016, P IEEE CVPR
   Wu Jiahong, 2017, 171106475 ARXIV
   Yang Wei, 2018, 180309722 ARXIV
   Yao A., 2011, P ADV NEUR INF PROC, P1359
   Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535
   Zell Petrissa, 2017, P IEEE CVPRW
   Zhou F, 2014, LECT NOTES COMPUT SC, V8694, P62, DOI 10.1007/978-3-319-10599-4_5
   Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537
   Zhou XW, 2015, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2015.7299074
   Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17
NR 66
TC 0
Z9 0
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2020
VL 16
IS 1
AR 6
DI 10.1145/3372207
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FD
UT WOS:000583712100005
DA 2024-07-18
ER

PT J
AU Feng, ZL
   Yu, ZY
   Jing, YC
   Wu, S
   Song, ML
   Yang, YZ
   Jiang, JX
AF Feng, Zunlei
   Yu, Zhenyun
   Jing, Yongcheng
   Wu, Sai
   Song, Mingli
   Yang, Yezhou
   Jiang, Junxiao
TI Interpretable Partitioned Embedding for Intelligent Multi-item Fashion
   Outfit Composition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Outfit composition; interpretable; adversarial; embedding
AB Intelligent fashion outfit composition has become more popular in recent years. Some deep-learning-based approaches reveal competitive composition. However, the uninterpretable characteristic makes such a deeplearning-based approach fail to meet the businesses', designers', and consumers' urges to comprehend the importance of different attributes in an outfit composition. To realize interpretable and intelligent multi-item fashion outfit compositions, we propose a partitioned embedding network to learn interpretable embeddings from clothing items. The network contains two vital components: attribute partition module and partition adversarial module. In the attribute partition module, multiple attribute labels are adopted to ensure that different parts of the overall embedding correspond to different attributes. In the partition adversarial module, adversarial operations are adopted to achieve the independence of different parts. With the interpretable and partitioned embedding, we then construct an outfit-composition graph and an attribute matching map. Extensive experiments demonstrate that (1) the partitioned embedding have unmingled parts that correspond to different attributes and (2) outfits recommended by our model are more desirable in comparison with the existing methods.
C1 [Feng, Zunlei; Yu, Zhenyun; Jing, Yongcheng; Wu, Sai; Song, Mingli] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Yang, Yezhou] Arizona State Univ, Tempe, AZ 85287 USA.
   [Jiang, Junxiao] Alibaba Grp, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University; Arizona State University; Arizona State
   University-Tempe; Alibaba Group
RP Feng, ZL; Yu, ZY; Jing, YC; Wu, S; Song, ML (corresponding author), Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
EM zunleifeng@zju.edu.cn; zhenyunyu@zju.edu.cn; ycjing@zju.edu.cn;
   wusai@zju.edu.cn; brooksong@zju.edu.cn; yz.yang@asu.edu;
   junxiao.jjx@alibaba-inc.com
RI Jing, Yongcheng/HHN-4906-2022
FU National Key Research and Development Program [2016YFB1200203]; National
   Natural Science Foundation of China [61572428, U1509206, 61872315,
   61661146001]; Program of International Science and Technology
   Cooperation [2013DFG12840]; Key Research and Development Program of
   Zhejiang Province [2018-C01004]
FX This work is supported by the National Key Research and Development
   Program (2016YFB1200203), National Natural Science Foundation of China
   (61572428, U1509206, 61872315, 61661146001), Program of International
   Science and Technology Cooperation under Grant (2013DFG12840), and Key
   Research and Development Program of Zhejiang Province (2018-C01004).
CR [Anonymous], 2017, 2 STEP DISENTANGLEME
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], STREETSTYLE EXPLORIN
   [Anonymous], 2017, Multi-level variational autoencoder: Learning disentangled representations from grouped observations
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], P 11 AS C COMP VIS
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], P 20 ACM SIGKDD INT
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2016, P INT C LEARN REPR
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2012, AS C COMP VIS
   [Anonymous], NEURAL INFORM PROCES
   [Anonymous], DEV UNIVERSAL GAMULT
   [Anonymous], 2016, P 24 ACM INT C MULTI, DOI DOI 10.1145/2964284.2973834
   [Anonymous], ARXIV160400036
   [Anonymous], 2014, AUTOENCODING VARIATI
   [Anonymous], CHANGING FASHION CUL
   [Anonymous], 2016, NIPS
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen Q, 2015, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR.2015.7299169
   Feng ZL, 2018, NEUROCOMPUTING, V273, P395, DOI 10.1016/j.neucom.2017.07.043
   Feng Zunlei, 2018, Advances in Neural Information Processing Systems, P5898
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   He RN, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P507, DOI 10.1145/2872427.2883037
   Hu QY, 2018, PROC CVPR IEEE, P3399, DOI 10.1109/CVPR.2018.00358
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Iwata Tomoharu, 2011, P 22 INT JOINT C ON, V3, P2262
   Jiang SH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152114
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kulkarni TD, 2015, ADV NEUR IN, V28
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liu L, 2013, PROTEOME SCI, V11, DOI 10.1186/1477-5956-11-1
   Liu Q, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P841, DOI 10.1145/3077136.3080658
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Perarnau G., 2016, NIPS WORKSHOP ADVERS
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   Simo-Serra E, 2016, PROC CVPR IEEE, P298, DOI 10.1109/CVPR.2016.39
   Simo-Serra E, 2015, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2015.7298688
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Vittayakorn S, 2015, IEEE WINT CONF APPL, P951, DOI 10.1109/WACV.2015.131
   Wang CY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2901
   Wang H, 2011, 2011 INTERNATIONAL CONFERENCE ON EDUCATION SCIENCE AND MANAGEMENT ENGINEERING (ESME 2011), VOLS 1-5, P1353
   Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20
   Wei-Lin Hsiao, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P7161, DOI 10.1109/CVPR.2018.00748
   Wen JQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152463
   Wu Q, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2890104
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yamaguchi T, 2015, 2015 CONFERENCE ON TECHNOLOGIES AND APPLICATIONS OF ARTIFICIAL INTELLIGENCE (TAAI), P51, DOI 10.1109/TAAI.2015.7407118
   Zhang QN, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457454
NR 56
TC 7
Z9 7
U1 1
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 61
DI 10.1145/3326332
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900014
DA 2024-07-18
ER

PT J
AU Zhu, Y
   Guntuku, SC
   Lin, WS
   Ghinea, G
   Redi, JA
AF Zhu, Yi
   Guntuku, Sharath Chandra
   Lin, Weisi
   Ghinea, Gheorghita
   Redi, Judith A.
TI Measuring Individual Video QoE: A Survey, and Proposal for Future
   Directions Using Social Media
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Individual Quality of Experience; user factors; Facebook
ID PERCEIVED QUALITY; COGNITIVE-STYLE; FACEBOOK; PERSONALITY; EXPERIENCE;
   PERCEPTION; INTERNET; CULTURE; SELF
AB The next generation of multimedia services have to be optimized in a personalized way, taking user factors into account for the evaluation of individual experience. Previous works have investigated the influence of user factors mostly in a controlled laboratory environment which often includes a limited number of users and fails to reflect real-life environment. Social media, especially Facebook, provide an interesting alternative for Internet-based subjective evaluation. In this article, we develop (and open-source) a Facebook application, named YouQ1, as an experimental platform for studying individual experience for videos. Our results show that subjective experiments based on YouQ can produce reliable results as compared to a controlled laboratory experiment. Additionally, YouQ has the ability to collect user information automatically from Facebook, which can be used for modeling individual experience.
C1 [Zhu, Yi; Redi, Judith A.] Delft Univ Technol, POB 5031, NL-2600 GA Delft, Netherlands.
   [Guntuku, Sharath Chandra] Univ Penn, Philadelphia, PA 19104 USA.
   [Lin, Weisi] Nanyang Technol Univ, 50 Nanyang Ave, Singapore 639798, Singapore.
   [Ghinea, Gheorghita] Brunel Univ, Uxbridge UB8 3PH, Middx, England.
C3 Delft University of Technology; University of Pennsylvania; Nanyang
   Technological University; Brunel University
RP Zhu, Y (corresponding author), Delft Univ Technol, POB 5031, NL-2600 GA Delft, Netherlands.
EM Y.Zhu-1@tudelft.nl; sharathg@sas.upenn.edu; wslin@ntu.edu.sg;
   george.ghinea@brunel.ac.uk; J.A.Redi@tudelft.nl
RI Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011; Guntuku, Sharath
   Chandra/U-6314-2019; Ghinea, Gheorghita/AAG-6770-2020
OI Lin, Weisi/0000-0001-9866-1947; Guntuku, Sharath
   Chandra/0000-0002-2929-0035; Ghinea, Gheorghita/0000-0003-2578-5580
FU China Scholarship Council (CSC) [201206090028]; NWO Veni Grant
   [639.021.230]; Singapore MoE Tier 1 Project [M4011379, RG141/14]
FX This work is supported by China Scholarship Council (CSC) Grant No.
   201206090028. This work is also partially supported by the NWO Veni
   Grant 639.021.230 and by the Singapore MoE Tier 1 Project M4011379,
   RG141/14.
CR [Anonymous], 2010, INT WORKSH SEARCH MI
   [Anonymous], 2015, PROC 7 INT WORKSHOP
   [Anonymous], 2011, INT J ASIAN LANG PRO
   [Anonymous], 2000, Psychometric scaling, a toolkit for imaging systems development
   [Anonymous], 2013, CISCO VISUAL NETWORK
   [Anonymous], P PAR INF ITS INT SP
   [Anonymous], P SIGCHI C HUM FACT
   Bachrach Y, 2012, P 4 ANN ACM WEB SCI
   Back MD, 2010, PSYCHOL SCI, V21, P372, DOI 10.1177/0956797609360756
   Balachandran A., 2012, PROCEEDINGS OF THE 1
   Balcetis E., 2010, Social psychology of visual perception
   Brooks P, 2010, IEEE NETWORK, V24, P8, DOI 10.1109/MNET.2010.5430138
   Burger John D, 2011, EMPIRICAL METHODS NA
   Casler K, 2013, COMPUT HUM BEHAV, V29, P2156, DOI 10.1016/j.chb.2013.05.009
   Conviva, 2015, EXP REP
   Delogu F., 2015, MOBILE MM COMMUNICAT
   Doan A, 2011, COMMUN ACM, V54, P86, DOI 10.1145/1924421.1924442
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Eftekhar A, 2014, COMPUT HUM BEHAV, V37, P162, DOI 10.1016/j.chb.2014.04.048
   El-Nasr M. S., 2006, WORKSH CST
   Garcia D, 2014, PERS INDIV DIFFER, V67, P92, DOI 10.1016/j.paid.2013.10.001
   Gardlo B., 2012, QUALITY MULTIMEDIA E
   Ghinea G, 2006, MULTIMEDIA SYST, V11, P271, DOI 10.1007/s00530-005-0007-8
   Ghinea G, 2005, IEEE T MULTIMEDIA, V7, P786, DOI 10.1109/TMM.2005.850960
   Gosling SD, 2015, ANNU REV PSYCHOL, V66, P877, DOI 10.1146/annurev-psych-010814-015321
   Gosling Samuel D, 2003, J RES PERSONALITY, V37, P6
   Gou L., 2014, ACM CHI
   Graham LT., 2011, The Wiley-Blackwell handbook of individual differences, P773
   Greenwald AG, 1998, J PERS SOC PSYCHOL, V74, P1464, DOI 10.1037/0022-3514.74.6.1464
   Gulliver SR, 2006, ACM T MULTIM COMPUT, V2, P241, DOI 10.1145/1201730.1201731
   Gulliver SR, 2010, ONLINE INFORM REV, V34, P39, DOI 10.1108/14684521011024119
   Gunter S., 2016, PROC 18 EUR C POWER, P1
   Guntuku S. C., 2016, IEEE T AFFECTIVE COM
   Guntuku S.C., 2015, Proc. 1st Int. Workshop on Affect Sentiment in Multimedia, P21, DOI 10.1145/2813524.2813528
   Guntuku S.C., 2016, ARXIV PREPRINT ARXIV
   Guntuku SC, 2017, PROCEEDINGS OF THE 2017 ACM WEB SCIENCE CONFERENCE (WEBSCI '17), P223, DOI 10.1145/3091478.3091522
   Guntuku SC, 2019, J ATTEN DISORD, V23, P1475, DOI 10.1177/1087054717738083
   Guntuku SC, 2017, CURR OPIN BEHAV SCI, V18, P43, DOI 10.1016/j.cobeha.2017.07.005
   Guntuku SC, 2015, INT CONF AFFECT, P236, DOI 10.1109/ACII.2015.7344577
   Hanjalic A., 2005, TMM
   Hofeld T., 2014, CROWDSOURCING
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Hu R., 2011, ACM RECSYS
   Huynh-Thu Q, 2008, IEEE T BROADCAST, V54, P641, DOI 10.1109/TBC.2008.2001246
   Ickin S, 2012, IEEE COMMUN MAG, V50, P48, DOI 10.1109/MCOM.2012.6178833
   International Telecommunication Union, 2002, BT50011 ITUR, P2
   Kara P. A., 2015, QUALITY MULTIMEDIA E
   Keimel C., 2012, QUALITY MULTIMEDIA E
   Kortum P, 2010, HUM FACTORS, V52, P105, DOI 10.1177/0018720810366020
   Kosinski M, 2015, AM PSYCHOL, V70, P543, DOI 10.1037/a0039210
   Kosinski M, 2013, P NATL ACAD SCI USA, V110, P5802, DOI 10.1073/pnas.1218772110
   Le Callet P., 2012, ENQEMSS COST ACTION
   Li H., 2012, P INT WORKSH NETW OP
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Moller S, 2014, T-LAB SER TELECOMMUN, P1, DOI 10.1007/978-3-319-02681-7
   Naumann AB, 2010, INTERACT COMPUT, V22, P465, DOI 10.1016/j.intcom.2010.08.005
   Nisbett RE, 2005, TRENDS COGN SCI, V9, P467, DOI 10.1016/j.tics.2005.08.004
   O'Brien HL, 2008, J AM SOC INF SCI TEC, V59, P938, DOI 10.1002/asi.20801
   Orero P., 2007, RODOPI
   Owsley C., 1983, VISION RES
   Palhais J., 2011, INT C MOB NETW MAN B, P261
   Park M., 2016, ARXIV160308308
   Preotiuc-Pietro D, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0138717
   Redi J., 2015, CROWDMM
   Redi J. A., 2015, PASSIVE IMAGE VIEWER
   Robitza W., 2015, QUALITY MULTIMEDIA E
   Sackl A., 2014, QUALITY MULTIMEDIA E
   Sackl A., 2017, Quality and User Experience, V2, P3, DOI [10.1007/s41233-016-0004-z, DOI 10.1007/S41233-016-0004-Z]
   Schwartz HA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073791
   Scott M. J., 2015, ACM MULTIMEDIA
   Scott MJ, 2016, IEEE T MULTIMEDIA, V18, P1796, DOI 10.1109/TMM.2016.2574623
   Seitz N, 2003, IEEE COMMUN MAG, V41, P82, DOI 10.1109/MCOM.2003.1204752
   Silvia PJ, 2008, CURR DIR PSYCHOL SCI, V17, P57, DOI 10.1111/j.1467-8721.2008.00548.x
   Skitka LJ, 2006, ANNU REV PSYCHOL, V57, P529, DOI 10.1146/annurev.psych.57.102904.190048
   Staelens N, 2010, IEEE T BROADCAST, V56, P458, DOI 10.1109/TBC.2010.2067710
   Stillwell David J, 2004, American Psychologist, V59, P93
   Swan M, 2012, J PERS MED, V2, P93, DOI 10.3390/jpm2030093
   Weisbuch M, 2009, J EXP SOC PSYCHOL, V45, P573, DOI 10.1016/j.jesp.2008.12.009
   WESTERINK JHDM, 1989, SMPTE J, V98, P113, DOI 10.5594/J02825
   Wilson RE, 2012, PERSPECT PSYCHOL SCI, V7, P203, DOI 10.1177/1745691612442904
   Wu W., 2013, ACM HYPERTEXT
   Yamori K., 2004, COMMUNICATIONS
   Yi X., 2014, P 8 ACM C REC SYST
   Zhu Y., 2016, QOE PREDICTION ENRIC
   Zhu Y, 2015, COMPUT HUM BEHAV, V49, P412, DOI 10.1016/j.chb.2015.02.054
   Zwarun L, 2014, COMPUT HUM BEHAV, V41, P236, DOI 10.1016/j.chb.2014.09.041
NR 86
TC 19
Z9 20
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2018
VL 14
IS 2
SU S
AR 30
DI 10.1145/3183512
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GI6TD
UT WOS:000434634700003
DA 2024-07-18
ER

PT J
AU Sharrab, YO
   Sarhan, NJ
AF Sharrab, Yousef O.
   Sarhan, Nabil J.
TI Modeling and Analysis of Power Consumption in Live Video Streaming
   Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Live video streaming; power consumption modeling; video bitrate
   modeling; video surveillance systems
ID WIRELESS; TRANSFORM; LIFETIME; IMPACT
AB This article develops an aggregate power consumption model for live video streaming systems, including many-to-many systems. In many-to-one streaming systems, multiple video sources (i.e., cameras and/or sensors) stream videos to a monitoring station. We model the power consumed by the video sources in the capturing, encoding, and transmission phases and then provide an overall model in terms of the main capturing and encoding parameters, including resolution, frame rate, number of reference frames, motion estimation range, and quantization. We also analyze the power consumed by the monitoring station due to receiving, decoding, and upscaling the received video streams. In addition to modeling the power consumption, we model the achieved bitrate of video encoding. We validate the developed models through extensive experiments using two types of systems and different video contents. Furthermore, we analyze many-to-one systems in terms of bitrate, video quality, and the power consumed by the sources, as well as that by the monitoring station, considering the impacts of multiple parameters simultaneously.
C1 [Sharrab, Yousef O.] Wayne State Univ, Dept Elect & Comp Engn, Detroit, MI 48202 USA.
   Wayne State Univ, Multimedia Comp & Res Lab, Detroit, MI 48202 USA.
C3 Wayne State University; Wayne State University
RP Sharrab, YO (corresponding author), Wayne State Univ, Dept Elect & Comp Engn, Detroit, MI 48202 USA.
EM yousef.sharrab@wayne.edu; nabil@wayne.edu
OI Berretti, Stefano/0000-0003-1219-4386; Sarhan, Nabil/0000-0002-0527-5666
FU National Science Foundation [CNS-0834537]
FX This work was supported by the National Science Foundation under grant
   CNS-0834537.
CR ACM Trans, 2017, MULT COMP COMM APPL, V13
   Alsmirat M, 2016, IEEE INT SYM MULTIM, P243, DOI [10.1109/ISM.2016.67, 10.1109/ISM.2016.0055]
   [Anonymous], 2010, The H.264 Advanced Video Compression Standard
   Bhardwaj M, 2002, IEEE INFOCOM SER, P1587, DOI 10.1109/INFCOM.2002.1019410
   Burd TD, 1996, J VLSI SIG PROC SYST, V13, P203, DOI 10.1007/BF01130406
   Cheng RH, 2013, INT CONF UBIQ FUTUR, P77, DOI 10.1109/ICUFN.2013.6614782
   Chi Feng W., 2005, ACM Transactions on Multimedia Computing, Communications and Applications, V1, P151, DOI DOI 10.1145/1062253.1062256
   Cotuk H, 2014, IEEE T COMPUT, V63, P2866, DOI 10.1109/TC.2013.151
   Elouardi A, 2007, IEEE T INSTRUM MEAS, V56, P1675, DOI 10.1109/TIM.2007.895671
   He ZH, 2005, IEEE T CIRC SYST VID, V15, P645, DOI 10.1109/TCSVT.2005.846433
   He ZH, 2006, IEEE T CIRC SYST VID, V16, P590, DOI 10.1109/TCSVT.2006.873154
   Hoque MA, 2015, PERVASIVE MOB COMPUT, V16, P96, DOI 10.1016/j.pmcj.2014.05.004
   Kannangara CS, 2008, IEEE T CIRC SYST VID, V18, P1191, DOI 10.1109/TCSVT.2008.928881
   Kim C, 2007, IEEE T CIRC SYST VID, V17, P441, DOI 10.1109/TCSVT.2006.888829
   Kim JW, 2011, EVID-BASED COMPL ALT, V2011, DOI 10.1155/2011/109164
   Kim J, 2006, LECT NOTES COMPUT SC, V4179, P454
   LiKamWa Robert, 2013, P 11 ANN INT C MOB S, P69
   Lin WY, 2011, IEEE T CIRC SYST VID, V21, P237, DOI 10.1109/TCSVT.2011.2106290
   Lu X, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 2, PROCEEDINGS, P77
   Pu W, 2006, IEEE ICC, P441
   Rajaraman SV, 2014, 2014 IEEE 25TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATION (PIMRC), P2013, DOI 10.1109/PIMRC.2014.7136502
   Sarhan Nabil J., 2017, SUPPLEMENTARY INFORM
   Sarif Bambang A. B., 2015, INT J DISTRIB SENS N, V11
   Shafique M, 2010, DES AUT TEST EUROPE, P1713
   Sharrab Y. O., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P410, DOI 10.1109/ICME.2012.77
   Sharrab YousefO., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys '13, P60
   Su L, 2009, IEEE T CIRC SYST VID, V19, P477, DOI 10.1109/TCSVT.2009.2014017
   Sun MT, 1998, J VIS COMMUN IMAGE R, V9, P163, DOI 10.1006/jvci.1998.0381
   Tan YH, 2010, IEEE T CIRC SYST VID, V20, P1271, DOI 10.1109/TCSVT.2010.2058480
   Tourapis AM, 2001, P SOC PHOTO-OPT INS, V4310, P883
   Wang YK, 2004, IEEE T CONSUM ELECTR, V50, P923, DOI 10.1109/TCE.2004.1341701
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu XZ, 2008, IEEE T CIRC SYST VID, V18, P285, DOI 10.1109/TCSVT.2008.918122
   Zhu C, 2001, INT CONF ACOUST SPEE, P1593, DOI 10.1109/ICASSP.2001.941239
NR 34
TC 7
Z9 7
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2017
VL 13
IS 4
AR 54
DI 10.1145/3115505
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6JZ
UT WOS:000414352300008
DA 2024-07-18
ER

PT J
AU Chu, WT
   Chiu, CH
AF Chu, Wei-Ta
   Chiu, Chih-Hao
TI Predicting Occupation from Images by Combining Face and Body Context
   Information
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Occupation prediction; discriminant multi-channel SVM; adaptive
   weighting; body context; spatial pyramid; locality-constrained linear
   coding; convolutional neural network
ID AGE STRUCTURES; IDENTIFICATION; RECOGNITION; INFERENCES; RETRIEVAL;
   PHOTO
AB Facial images embed age, gender, and other rich information that is implicitly related to occupation. In this work, we advocate that occupation prediction from a single facial image is a doable computer vision problem. We extract multilevel hand-crafted features associated with locality-constrained linear coding and convolutional neural network features as image occupation descriptors. To avoid the curse of dimensionality and over-fitting, a boost strategy called multichannel SVM is used to integrate features from face and body. Intra- and interclass visual variations are Jointly considered in the boosting framework to further improve performance. In the evaluation, we verify the effectiveness of predicting occupation from face and demonstrate promising performance obtained by combining face and body information. More importantly, our work further integrates deep features into the multichannel SVM framework and shows significantly better performance over the state of the art.
C1 [Chu, Wei-Ta; Chiu, Chih-Hao] Natl Chung Cheng Univ, Minxiong Township, Chiayi, Taiwan.
   [Chu, Wei-Ta; Chiu, Chih-Hao] CSIE, 168 Univ Rd, Min Hsiung 621, Chiayi, Taiwan.
C3 National Chung Cheng University
RP Chu, WT; Chiu, CH (corresponding author), Natl Chung Cheng Univ, Minxiong Township, Chiayi, Taiwan.; Chu, WT; Chiu, CH (corresponding author), CSIE, 168 Univ Rd, Min Hsiung 621, Chiayi, Taiwan.
EM wtchu@ccu.edu.tw; kennex2conan14@gmail.com
RI Chu, Wei-Ta/AAE-8471-2022
OI Chu, Wei-Ta/0000-0001-5722-7239
FU Ministry of Science and Technology of Taiwan
   [MOST103-2221-E-194-027-MY3, MOST104-2221-E-194-014,
   MOST105-2628-E-194-001-MY2]
FX The work was partially supported by the Ministry of Science and
   Technology of Taiwan wider the grant MOST103-2221-E-194-027-MY3,
   MOST104-2221-E-194-014, and MOST105-2628-E-194-001-MY2.
CR [Anonymous], 2015, P 23 ACM INT C MULT
   [Anonymous], P CVPR WORKSH DEEP V
   Antonakis J, 2009, SCIENCE, V323, P1183, DOI 10.1126/science.1167748
   Browne KR, 2006, J ORGAN BEHAV, V27, P143, DOI 10.1002/job.349
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen BC, 2013, IEEE T MULTIMEDIA, V15, P1163, DOI 10.1109/TMM.2013.2242460
   Chen HZ, 2013, PROC CVPR IEEE, P3366, DOI 10.1109/CVPR.2013.432
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1388, DOI 10.1109/TMM.2013.2250492
   Chu WT, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P9, DOI 10.1109/ISM.2014.13
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dibeklioglu H, 2015, IEEE T MULTIMEDIA, V17, P279, DOI 10.1109/TMM.2015.2394777
   Fan HQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P933, DOI 10.1145/2647868.2654960
   Gabriel PE, 2007, MON LABOR REV, V130, P19
   KAUFMAN RL, 1982, AM J SOCIOL, V87, P827, DOI 10.1086/227523
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwak IS, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.14
   Nguyen LS, 2014, IEEE T MULTIMEDIA, V16, P1018, DOI 10.1109/TMM.2014.2307169
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P854, DOI 10.1109/TMM.2015.2419452
   Ramanathan V, 2013, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2013.320
   Rule NO, 2008, PSYCHOL SCI, V19, P109, DOI 10.1111/j.1467-9280.2008.02054.x
   Shao M, 2013, IEEE I CONF COMP VIS, P3631, DOI 10.1109/ICCV.2013.451
   SMITH JM, 1973, J GERONTOL, V28, P484, DOI 10.1093/geronj/28.4.484
   Song Z, 2011, IEEE I CONF COMP VIS, P1084, DOI 10.1109/ICCV.2011.6126355
   Todorov A, 2005, SCIENCE, V308, P1623, DOI 10.1126/science.1110589
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Xiong C, 2014, IEEE T MULTIMEDIA, V16, P1473, DOI 10.1109/TMM.2014.2316475
   Zhang X, 2012, IEEE T MULTIMEDIA, V14, P995, DOI 10.1109/TMM.2012.2186121
NR 36
TC 2
Z9 3
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2017
VL 13
IS 1
AR 7
DI 10.1145/3009911
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA EM7WJ
UT WOS:000395522700007
DA 2024-07-18
ER

PT J
AU Wang, W
   Chen, G
   Chen, HB
   Dinh, TTA
   Gao, JY
   Ooi, BC
   Tan, KL
   Wang, S
   Zhang, MH
AF Wang, Wei
   Chen, Gang
   Chen, Haibo
   Tien Tuan Anh Dinh
   Gao, Jinyang
   Ooi, Beng Chin
   Tan, Kian-Lee
   Wang, Sheng
   Zhang, Meihui
TI Deep Learning at Scale and at Ease
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimedia; deep learning; distributed training
AB Recently, deep learning techniques have enjoyed success in various multimedia applications, such as image classification and multimodal data analysis. Large deep learning models are developed for learning rich representations of complex data. There are two challenges to overcome before deep learning can be widely adopted in multimedia and other applications. One is usability, namely the implementation of different models and training algorithms must be done by nonexperts without much effort, especially when the model is large and complex. The other is scalability, namely the deep learning system must be able to provision for a huge demand of computing resources for training large models with massive datasets. To address these two challenges, in this article we design a distributed deep learning platform called SINGA, which has an intuitive programming model based on the common layer abstraction of deep learning models. Good scalability is achieved through flexible distributed training architecture and specific optimization techniques. SINGA runs on both GPUs and CPUs, and we show that it outperforms many other state-of-the-art deep learning systems. Our experience with developing and training deep learning models for real-life multimedia applications in SINGA shows that the platform is both usable and scalable.
C1 [Wang, Wei; Tien Tuan Anh Dinh; Gao, Jinyang; Ooi, Beng Chin; Tan, Kian-Lee; Wang, Sheng] Natl Univ Singapore, COM1, 13 Comp Dr, Singapore 117417, Singapore.
   [Chen, Gang] Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China.
   [Chen, Haibo] NetEase Inc, 599 Wang Shang Rd, Hangzhou 310052, Zhejiang, Peoples R China.
   [Zhang, Meihui] Singapore Univ Technol & Design, 8 Somapah Rd, Singapore 487372, Singapore.
C3 National University of Singapore; Zhejiang University; Singapore
   University of Technology & Design
RP Wang, W (corresponding author), Natl Univ Singapore, COM1, 13 Comp Dr, Singapore 117417, Singapore.
EM wangwei@comp.nus.edu.sg; cg@cs.zju.edu.cn; seaokcs@163.com;
   dinhtta@comp.nus.edu.sg; jinyang.gao@comp.nus.edu.sg;
   ooibc@comp.nus.edu.sg; tankl@comp.nus.edu.sg; wangsh@comp.nus.edu.sg;
   meihui_zhang@sutd.edu.sg
RI Dinh, Tien Tuan Anh/AAB-1363-2022; Wang, Sheng/CAG-2392-2022
OI Dinh, Tien Tuan Anh/0000-0002-8158-3636; Wang,
   Sheng/0000-0001-7622-4842; Tan, Kian-Lee/0000-0001-9315-4057; Wang,
   Wei/0000-0001-5367-7056
FU National Research Foundation (NRF); Prime Minister's Office, Singapore
   under its Competitive Research Programme (CRP) [NRF-CRP8-2011-08];
   A*STAR project [1321202073]; National Natural Science Foundation of
   China [61472348]; Energy Innovation Research Programme (EIRP)
   [NRF2014EWTEIRP002-026]
FX This work was supported in part by the National Research Foundation
   (NRF), Prime Minister's Office, Singapore under its Competitive Research
   Programme (CRP, award NRF-CRP8-2011-08) and A*STAR project 1321202073.
   G. Chen's work was supported by National Natural Science Foundation of
   China grant 61472348. M. Zhang's work was funded under the Energy
   Innovation Research Programme (EIRP, award NRF2014EWTEIRP002-026), which
   is administered by the Energy Market Authority. The EIRP is a
   competitive grant call initiative driven by the Energy Innovation
   Programme Office and funded by the NRF.
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   [Anonymous], P DEEP LEARN WORKSH
   [Anonymous], 2016, ARXIV160406174
   [Anonymous], 2012, P 29 INT COFERENCE I
   [Anonymous], 2013, ARXIV13127651
   [Anonymous], 2015, Deep image: Scaling up image recognition
   [Anonymous], 2010, ARXIV10030358
   Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3
   Chenoweth JM, 2016, FLA MUS NAT HIST-RIP, P1
   Chilimbi T., 2014, 11 USENIX S OPERATIN, P571
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Coates A, 2013, PROC INT C MACH LEAR, P1337
   Collobert R., 2011, P BIGLEARN WORKSH NI
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Heng Tao Shen, 2000, Proceedings ACM Multimedia 2000, P39, DOI 10.1145/354384.376098
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Jia Yangqing, 2014, ARXIV14085093, DOI [10.1145/2647868.2654889, DOI 10.1145/2647868.2654889]
   Jiang DW, 2014, PROC VLDB ENDOW, V7, P541, DOI 10.14778/2732286.2732291
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2014, ARXIV14045997
   LeCun Y., 1996, Neural Networks: Tricks of the Trade, this book is an outgrowth of a 1996 NIPS workshop, volume 1524 of Lecture Notes in Computer Science, ppp 9
   Li Mu, 2014, USENIX OSDI, P583
   Mikolov T, 2011, INT CONF ACOUST SPEE, P5528
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Ooi Beng Chin, 2015, P ACM MULT C
   Paine T., 2013, Gpu asynchronous stochastic gradient descent to speed up neural network training
   Recht Benjamin, 2011, 25 ANN C NEUR INF PR, DOI 10.48550/arXiv.1106.5730
   Seide F, 2014, INTERSPEECH, P1058
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan KL, 2015, SIGMOD REC, V44, P35
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948
   Wang W, 2014, PROC VLDB ENDOW, V7, P649, DOI 10.14778/2732296.2732301
   Wang Wei, 2015, P ACM MULT C
   Wang WD, 2015, AD HOC NETW, V25, P1, DOI 10.1016/j.adhoc.2014.08.011
   Wang XX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P627, DOI 10.1145/2647868.2654940
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Yadan O., 2013, ARXIV13125853
   You QZ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1071, DOI 10.1145/2733373.2806284
   Yu D., 2014, INTRO COMPUTATIONAL
   Zhang C, 2014, PROC VLDB ENDOW, V7, P1283, DOI 10.14778/2732977.2733001
   Zhang HW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P187, DOI 10.1145/2647868.2654915
NR 45
TC 17
Z9 22
U1 2
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2016
VL 12
IS 4
SU S
AR 69
DI 10.1145/2996464
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8DS
UT WOS:000392736100013
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Petrangeli, S
   Famaey, J
   Claeys, M
   Latré, S
   De Turck, F
AF Petrangeli, Stefano
   Famaey, Jeroen
   Claeys, Maxim
   Latre, Steven
   De Turck, Filip
TI QoE-Driven Rate Adaptation Heuristic for Fair Adaptive Video Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Management; Performance; Experimental evaluation;
   fairness; HTTP Adaptive Streaming; Microsoft Smooth Streaming; mobile;
   quality of experience; rate adaptation
AB HTTP Adaptive Streaming (HAS) is quickly becoming the de facto standard for video streaming services. In HAS, each video is temporally segmented and stored in different quality levels. Rate adaptation heuristics, deployed at the video player, allow the most appropriate level to be dynamically requested, based on the current network conditions. It has been shown that today's heuristics underperform when multiple clients consume video at the same time, due to fairness issues among clients. Concretely, this means that different clients negatively influence each other as they compete for shared network resources. In this article, we propose a novel rate adaptation algorithm called FINEAS (Fair In-Network Enhanced Adaptive Streaming), capable of increasing clients' Quality of Experience (QoE) and achieving fairness in a multiclient setting. A key element of this approach is an in-network system of coordination proxies in charge of facilitating fair resource sharing among clients. The strength of this approach is threefold. First, fairness is achieved without explicit communication among clients and thus no significant overhead is introduced into the network. Second, the system of coordination proxies is transparent to the clients, that is, the clients do not need to be aware of its presence. Third, the HAS principle is maintained, as the in-network components only provide the clients with new information and suggestions, while the rate adaptation decision remains the sole responsibility of the clients themselves. We evaluate this novel approach through simulations, under highly variable bandwidth conditions and in several multiclient scenarios. We show how the proposed approach can improve fairness up to 80% compared to state-of-the-art HAS heuristics in a scenario with three networks, each containing 30 clients streaming video at the same time.
C1 [Petrangeli, Stefano; Claeys, Maxim; De Turck, Filip] Univ Ghent, iMinds, Dept Informat Technol INTEC, B-9050 Ghent, Belgium.
   [Famaey, Jeroen; Latre, Steven] Univ Antwerp, iMinds, Dept Math & Comp Sci, Middelheimlaan 1, B-2020 Antwerp, Belgium.
C3 Ghent University; IMEC; University of Antwerp; IMEC
RP Petrangeli, S; Claeys, M; De Turck, F (corresponding author), Univ Ghent, iMinds, Dept Informat Technol INTEC, B-9050 Ghent, Belgium.; Famaey, J; Latré, S (corresponding author), Univ Antwerp, iMinds, Dept Math & Comp Sci, Middelheimlaan 1, B-2020 Antwerp, Belgium.
EM stefano.petrangeli@intec.ugent.be; jeroen.famaey@uantwerpen.be;
   maxim.claeys@intec.ugent.be; steven.latre@uantwerpen.be;
   filip.deturck@intec.ugent.be
RI Latre, Steven/N-8689-2016; Famaey, Jeroen/AAB-6171-2022; Famaey,
   Jeroen/L-3709-2018
OI Famaey, Jeroen/0000-0002-3587-1354; 
FU iMinds V-FORCE project [130655]; FLAMINGO, a Network of Excellence
   project - European Commission under its Seventh Framework Programme
   [ICT-318488]; Agency for Innovation, Science and Technology in Flanders
   (IWT)
FX The research was performed partially within the iMinds V-FORCE project
   (under IWT grant agreement no. 130655). This work was partly funded by
   FLAMINGO, a Network of Excellence project (ICT-318488) supported by the
   European Commission under its Seventh Framework Programme. Maxim Claeys
   is funded by a grant of the Agency for Innovation by Science and
   Technology in Flanders (IWT).
CR Akhshabi S, 2012, SIGNAL PROCESS-IMAGE, V27, P271, DOI 10.1016/j.image.2011.10.003
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], P SPIE APPL DIGITAL
   [Anonymous], 2013, P IEEE 20 INT PACK V
   Bouten N., 2012, 2012 8th International Conference on Network and Service Management (CNSM 2012), P336
   Claeys M., 2014, CONNECT SCI, V26, P27
   Claus C, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P746
   De Cicco L., 2011, P 2 ANN ACM C MULTIM, P145
   De Vriendt J, 2013, 2013 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2013), P1288
   El Essaili A., 2013, 2013 IEEE International Conference on Communications (ICC), P2480, DOI 10.1109/ICC.2013.6654905
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   GPP, 2013, 23139 GPP TS, P1
   Houdaille R., 2012, P 3 MULTIMEDIA SYSTE, P1
   Jarnikov D, 2010, IEEE INT CON MULTI, P1499, DOI 10.1109/ICME.2010.5583268
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Kun Ma, 2011, 2011 7th International Conference on Next Generation Web Services Practices, P1, DOI 10.1109/NWeSP.2011.6088144
   Kuschnig R., 2010, MMSYS, P157
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liu CH, 2012, SIGNAL PROCESS-IMAGE, V27, P288, DOI 10.1016/j.image.2011.10.001
   Miller K., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P173, DOI 10.1109/PV.2012.6229732
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   Mueller C., 2012, VISUAL COMMUN-US, P1
   Muller C., 2012, 4th ACM Workshop on Mobile Video (MoVID), P37, DOI DOI 10.1145/2151677.2151686
   Petrangeli Stefano., 2014, 2014 IEEE Network Operations and Management Symposium (NOMS), P1
   Riiser H, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240137
   Schmidt RD, 2013, INT CONF NETW SER, P152, DOI 10.1109/CNSM.2013.6727827
   Tian G., 2012, P 8 INT C EM NETW EX, P109
   Villa Bjorn J., 2012, Information and Communication Technologies. Proceedings 18th EUNICE/IFIP WG 6.2, 6.6. International Conference, EUNICE 2012, P183, DOI 10.1007/978-3-642-32808-4_17
   Xiang S., 2012, ACM MMSys '12, P167
   Zhou C., 2012, 2012 Visual Communications and Image Processing, P1
NR 30
TC 66
Z9 68
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2016
VL 12
IS 2
AR 28
DI 10.1145/2818361
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PK
UT WOS:000373906200002
OA Green Accepted, Green Published
DA 2024-07-18
ER

PT J
AU Liu, XL
   Mu, YD
   Lang, B
   Chang, SF
AF Liu, Xianglong
   Mu, Yadong
   Lang, Bo
   Chang, Shih-Fu
TI Mixed Image-Keyword Query Adaptive Hashing over Multilabel Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
ID GRAPH
AB This article defines a new hashing task motivated by real-world applications in content-based image retrieval, that is, effective data indexing and retrieval given mixed query (query image together with user-provided keywords). Our work is distinguished from state-of-the-art hashing research by two unique features: (1) Unlike conventional image retrieval systems, the input query is a combination of an exemplar image and several descriptive keywords, and (2) the input image data are often associated with multiple labels. It is an assumption that is more consistent with the realistic scenarios. The mixed image-keyword query significantly extends traditional image-based query and better explicates the user intention. Meanwhile it complicates semantics-based indexing on the multilabel data. Though several existing hashing methods can be adapted to solve the indexing task, unfortunately they all prove to suffer from low effectiveness. To enhance the hashing efficiency, we propose a novel scheme "boosted shared hashing". Unlike prior works that learn the hashing functions on either all image labels or a single label, we observe that the hashing function can be more effective if it is designed to index over an optimal label subset. In other words, the association between labels and hash bits are moderately sparse. The sparsity of the bit-label association indicates greatly reduced computation and storage complexities for indexing a new sample, since only limited number of hashing functions will become active for the specific sample. We develop a Boosting style algorithm for simultaneously optimizing both the optimal label subsets and hashing functions in a unified formulation, and further propose a query-adaptive retrieval mechanism based on hash bit selection for mixed queries, no matter whether or not the query words exist in the training data. Moreover, we show that the proposed method can be easily extended to the case where the data similarity is gauged by nonlinear kernel functions. Extensive experiments are conducted on standard image benchmarks like CIFAR-10, NUS-WIDE and a-TRECVID. The results validate both the sparsity of the bit-label association and the convergence of the proposed algorithm, and demonstrate that the proposed hashing scheme achieves substantially superior performances over state-of-the-art methods under the same hash bit budget.
C1 [Liu, Xianglong; Lang, Bo] Beihang Univ, Beijing 100191, Peoples R China.
   [Mu, Yadong] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
   [Chang, Shih-Fu] Columbia Univ, New York, NY 10027 USA.
C3 Beihang University; Columbia University; Columbia University
RP Liu, XL (corresponding author), Beihang Univ, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
EM xlliu@nlsde.buaa.edu.cn; muyadong@gmail.com
RI Lang, Bo/AAA-7966-2022
OI Liu, Xianglong/0000-0002-7618-3275
CR [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2012, IEEE T PATTERN ANAL
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2009, ADV NEURAL INFORM PR
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P INT C MACH LEARN
   [Anonymous], P ACM S THEOR COMP
   [Anonymous], 2008, Advances in Neural Information Processing Systems
   [Anonymous], 2009, P IEEE INT C COMP VI
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   [Anonymous], 2004, P S COMP GEOM
   [Anonymous], 2014, ACM T MULTIMEDIA COM, V10
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Broder A., 1998, P S THEOR COMP
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Friedman J., 1998, ANN STAT
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   He J., 2012, P INT C MACH LEARN
   He J., 2010, P ACM SIGKDD INT C K
   INDYK P, 2004, HDB DISCRETE COMPUTA, pCH39
   Ji SW, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754431
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Liu W., 2012, P IEEE INT C COMP VI
   Liu X., 2012, P ACM INT C MULT RET
   Liu X., 2012, P ACM INT C MULT
   Mu Y., 2012, INT J MULTIMEDIA INF, P1
   Mu Y., 2010, P IEEE INT C COMP VI
   Mu Y., 2011, P ACM INT C MULT RET
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Song J., 2011, P ACM INT C MULT
   TORRALBA A, 2004, P IEEE INT C COMP VI
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   YAN R, 2007, P ACM SIGKDD INT C K
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yu F., 2012, P IEEE INT C COMP VI
   Yuille A. L., 2002, ADV NEURAL INFORM PR
   Zhang D., 2011, P ACM SIGIR C RES DE
NR 45
TC 16
Z9 16
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2014
VL 10
IS 2
AR 22
DI 10.1145/2540990
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB6DY
UT WOS:000331879000007
DA 2024-07-18
ER

PT J
AU Yuan, J
   Zhao, YL
   Luan, HB
   Wang, M
   Chua, TS
AF Yuan, Jin
   Zhao, Yi-Liang
   Luan, Huanbo
   Wang, Meng
   Chua, Tat-Seng
TI Memory Recall Based Video Search: Finding Videos You Have Seen Before
   Based on Your Memory
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
ID RETRIEVAL
AB We often remember images and videos that we have seen or recorded before but cannot quite recall the exact venues or details of the contents. We typically have vague memories of the contents, which can often be expressed as a textual description and/or rough visual descriptions of the scenes. Using these vague memories, we then want to search for the corresponding videos of interest. We call this "Memory Recall based Video Search" (MRVS). To tackle this problem, we propose a video search system that permits a user to input his/her vague and incomplete query as a combination of text query, a sequence of visual queries, and/or concept queries. Here, a visual query is often in the form of a visual sketch depicting the outline of scenes within the desired video, while each corresponding concept query depicts a list of visual concepts that appears in that scene. As the query specified by users is generally approximate or incomplete, we need to develop techniques to handle this inexact and incomplete specification by also leveraging on user feedback to refine the specification. We utilize several innovative approaches to enhance the automatic search. First, we employ a visual query suggestion model to automatically suggest potential visual features to users as better queries. Second, we utilize a color similarity matrix to help compensate for inexact color specification in visual queries. Third, we leverage on the ordering of visual queries and/or concept queries to rerank the results by using a greedy algorithm. Moreover, as the query is inexact and there is likely to be only one or few possible answers, we incorporate an interactive feedback loop to permit the users to label related samples which are visually similar or semantically close to the relevant sample. Based on the labeled samples, we then propose optimization algorithms to update visual queries and concept weights to refine the search results. We conduct experiments on two large-scale video datasets: TRECVID 2010 and YouTube. The experimental results demonstrate that our proposed system is effective for MRVS tasks.
C1 [Yuan, Jin; Zhao, Yi-Liang; Luan, Huanbo; Wang, Meng; Chua, Tat-Seng] Natl Univ Singapore, Singapore 117548, Singapore.
C3 National University of Singapore
RP Yuan, J (corresponding author), Natl Univ Singapore, Singapore 117548, Singapore.
EM jinjinyuan1981@gmail.com
RI Wang, Meng/ITR-8699-2023
CR Amir A., 2005, P TRECVID WORKSH
   [Anonymous], 2004, P INT C MACH LEARN
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], 2009, INTRO INFORM RETRIEV
   [Anonymous], 2014, ACM T MULTIMEDIA COM, V10
   [Anonymous], P TRECVID WORKSH
   [Anonymous], 2010, TRECVID2010
   Chaisorn L., 2010, P TRECVID WORKSH
   Chen X. Y., 2010, P TRECVID WORKSH
   Fairchild M.D., 2005, Color Appearance Models, V2nd
   Hestenes M. R., 1969, Journal of Optimization Theory and Applications, V4, P303, DOI 10.1007/BF00927673
   Hsu WH, 2007, IEEE MULTIMEDIA, V14, P14, DOI 10.1109/MMUL.2007.61
   Hu WM, 2007, IEEE T IMAGE PROCESS, V16, P1168, DOI 10.1109/TIP.2006.891352
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Kennedy L., 2005, P ACM INT C MULT
   Kennedy L., 2010, SEMANTIC COMPUTING, P155
   Liu Y, 2009, IEEE T CIRC SYST VID, V19, P1841, DOI 10.1109/TCSVT.2009.2026951
   Liu YA, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P500, DOI 10.1145/1571941.1572027
   Ma YF, 2002, INT C PATT RECOG, P548, DOI 10.1109/ICPR.2002.1048361
   Neo S.-Y., 2006, P ACM INT C IM VID R
   Nie LQ, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P695
   Sivic J., 2005, P INT C IM VID RETR
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   SNOEK CGM, 2008, P TRECVID WORKSH
   Wang Dong., 2007, the 15th ACM International Conference on Multimedia, P285
   Yan R, 2007, INFORM RETRIEVAL, V10, P445, DOI 10.1007/s10791-007-9031-y
   YANAGAWA A, 2007, 22220068 ADVENT
   Yang J., 2006, P INT C MULT INF RET
   Yao T, 2013, IEEE T IMAGE PROCESS, V22, P1642, DOI 10.1109/TIP.2012.2236341
   Yuan J., 2011, P ACM INT C MULT
   Yuan J, 2011, IEEE T MULTIMEDIA, V13, P1343, DOI 10.1109/TMM.2011.2168813
   Zavesky E., 2008, P INT C MULT INF RET
   Zha Z. J., 2009, P INT C MULT
NR 34
TC 8
Z9 8
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2014
VL 10
IS 2
AR 21
DI 10.1145/2534409
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB6DY
UT WOS:000331879000006
DA 2024-07-18
ER

PT J
AU Riiser, H
   Endestad, T
   Vigmostad, P
   Griwodz, C
   Halvorsen, P
AF Riiser, Haakon
   Endestad, Tore
   Vigmostad, Paul
   Griwodz, Carsten
   Halvorsen, Pal
TI Video Streaming Using a Location-Based Bandwidth-Lookup Service for
   Bitrate Planning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Measurement; Performance; Adaptive streaming; bandwidth
   prediction; bitrate planning; mobile internet; wireless; fluctuating
   bandwidth; GPS
AB A lot of people around the world commute using public transportation and would like to spend this time viewing streamed video content such as news or sports updates. However, mobile wireless networks typically suffer from severe bandwidth fluctuations, and the networks are often completely unresponsive for several seconds, sometimes minutes. Today, there are several ways of adapting the video bitrate and thus the video quality to such fluctuations, for example, using scalable video codecs or segmented adaptive HTTP streaming that switches between nonscalable video streams encoded in different bitrates. Still, for a better long-term video playout experience that avoids disruptions and frequent quality changes while using existing video adaptation technology, it is desirable to perform bandwidth prediction and planned quality adaptation.
   This article describes a video streaming system for receivers equipped with a GPS. A receiver's download rate is constantly monitored, and periodically reported back to a central database along with associated GPS positional data. Thus, based on the current location, a streaming device can use a GPS-based bandwidth-lookup service in order to better predict the near-future bandwidth availability and create a schedule for the video playout that takes likely future availability into account. To create a prototype and perform initial tests, we conducted several field trials while commuting using public transportation. We show how our database has been used to predict bandwidth fluctuations and network outages, and how this information helps maintain uninterrupted playback with less compromise on video quality than possible without prediction.
C1 [Riiser, Haakon; Endestad, Tore; Vigmostad, Paul] Netview Technol AS, As, Norway.
   [Griwodz, Carsten; Halvorsen, Pal] Univ Oslo, Dept Informat, N-0316 Oslo, Norway.
C3 University of Oslo
RP Riiser, H (corresponding author), Netview AS, Postal Box 8749 Youngstorget, N-0028 Oslo, Norway.
EM Haakon.riiser@netview.no
OI Halvorsen, Pal/0000-0003-2073-7029
FU HyStream project [176847]; iAD centre for Research-based Innovation
   [174867]; Norwegian Research Council
FX This work has been performed in the context of the HyStream project
   (project number 176847) and the iAD centre for Research-based Innovation
   (project number 174867), both funded by Norwegian Research Council.
CR ADOBE, 2010, HTTP DYN STREAM AD F
   AKAMAI, 2010, AK HD IPHONE ENC BES
   [Anonymous], P ACM SIGMM C MULT S
   [Anonymous], 2009, SMOOTH STREAMING TEC
   [Anonymous], 2010, P 1 ANN ACM C MULT S
   BRANDT J., 2008, P NOSSDAV 2008 MAY, P113
   Curcio I.D., 2010, Proc. ACM Workshop on Mobile Video (MoVid), P3
   Diaz A, 2007, IEEE VTS VEH TECHNOL, P624, DOI 10.1109/VETECS.2007.139
   Evensen K, 2010, NOSSDAV 2010: PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P21
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Guo M, 2005, PERVASIVE MOB COMPUT, V1, P404, DOI 10.1016/j.pmcj.2005.08.001
   Horsmanheimo S, 2004, WIRELESS PERS COMMUN, V30, P207, DOI 10.1023/B:WIRE.0000049400.25243.fd
   Jie H, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P342, DOI 10.1109/AVSS.2003.1217941
   Johansen D., 2009, P 17 ACM INT C MULTI, P989
   Kaspar D, 2010, CONSUM COMM NETWORK, P47
   Krasic C., 2003, Proceedings of the 13th International Workshop on Network and Operating Systems Support for Digital Audio and Video, P112
   Lee KC, 2010, VEH TECHNOL CONFE
   Liva G, 2008, IEEE VTS VEH TECHNOL, P2996, DOI 10.1109/VETECS.2008.319
   MAHONEN P., 2006, P IEEE INFOCOM
   Mai CH, 2010, VEH TECHNOL CONFE
   MOVE NETWORKS, 2008, INT TEL CHALL OPP
   Ni PP, 2009, NOSSDAV 09: 18TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P103
   Pantos R., 2010, HTTP LIVE STREAMING
   Rejaie R., 2003, Proceedings of ACM International Workshop on Network and Operating Systems Support for Digital Audio and Video (NOSSDAV), P153
   Riiser H, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P489, DOI 10.1109/ICME.2008.4607478
   Schierl T, 2011, MULTIMED TOOLS APPL, V55, P227, DOI 10.1007/s11042-010-0572-5
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sun JZ, 2005, CONTEL 2005: PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS, VOLS 1 AND 2, P303, DOI 10.1109/CONTEL.2005.185880
   Tamai M, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P189, DOI 10.1109/ICME.2004.1394157
   Wac K, 2006, LECT NOTES COMPUT SC, V4278, P1924
   Zink M, 2003, LECT NOTES COMPUT SC, V2707, P137
NR 31
TC 68
Z9 87
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2012
VL 8
IS 3
AR 24
DI 10.1145/2240136.2240137
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 986AM
UT WOS:000307311700001
DA 2024-07-18
ER

PT J
AU Ji, RR
   Gao, Y
   Zhong, BN
   Yao, HX
   Tian, Q
AF Ji, Rongrong
   Gao, Yue
   Zhong, Bineng
   Yao, Hongxun
   Tian, Qi
TI Mining Flickr Landmarks by Modeling Reconstruction Sparsity
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Documentation; Multimedia systems; knowledge representation;
   internet; image analysis
AB In recent years, there have been ever-growing geographical tagged photos on the community Web sites such as Flickr. Discovering touristic landmarks from these photos can help us to make better sense of our visual world. In this article, we report our work on mining landmarks from geotagged Flickr photos for city scene summarization and touristic recommendations. We begin by exploring the geographical and visual statistics of the Web users' photographing manner, based on which we conduct landmark mining in two steps: First, we propose to partition each city into geographical regions based on spectral clustering over the geotags of Flickr photos. Second, in each landmark region, we present a representative photo mining scheme based on sparse representation. Our main idea is to regard the landmark mining problem as a process to find photos whose visual signatures can be reconstructed using other photos of this landmark region with a minimal coding length. This sparse reconstruction scheme offers a general perspective to mine the representative photos. Indeed, by simplifying the data correlation constraints in our scheme, several previous works in representative photo discovery and landmark mining can be derived. Finally, we introduce a Hyperlink-Induced Topic Search model to refine our landmark ranking, which incorporates the community knowledge to simulate the landmark ranking problem as a dynamic page ranking problem. We have deployed our proposed landmark mining framework on a city scene summarization and navigation system, which works on one million geotagged Flickr photos coming from twenty worldwide metropolises. We have also quantitatively compared our scheme with several state-of-the-art works.
C1 [Ji, Rongrong; Yao, Hongxun] Harbin Inst Technol, Dept Comp Sci, Harbin 150001, Peoples R China.
   [Zhong, Bineng] Huaqiao Univ, Dept Comp Sci & Technol, Xiamen 361021, Peoples R China.
   [Gao, Yue] Tsinghua Univ, Dept Automat, Beijing 100086, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Harbin Institute of Technology; Huaqiao University; Tsinghua University;
   University of Texas System; University of Texas at San Antonio (UTSA)
RP Yao, HX (corresponding author), Harbin Inst Technol, Dept Comp Sci, Harbin 150001, Peoples R China.
EM nji.bnzhong@vilab.hit.edu.cn; gaoyue08@mails.tsinghua.edu.cn;
   bnzhong@gmail.com; yhx@vilab.hit.edu.cn; qitian@cs.utsa.edu
RI Gao, Yue/B-3376-2012
FU National Key Basic Research Program of China [2009CB320906]; Natural
   Science Foundation of China [60775024]
FX This research is supported by the National Key Basic Research Program of
   China (under Grant Number: 2009CB320906) and the Natural Science
   Foundation of China (under Grant Number: 60775024).
CR ABBASI R, 2009, P EUR C INF RETR
   AHERN S, 2007, P JOINT C DIG LIB
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], 2008, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2008.4587784
   [Anonymous], P INT C COMP VIS
   [Anonymous], 2006, P IEEE COMPUTER SOC
   [Anonymous], P INT WORLD WID WEB
   BRIN S, 1998, P INT WORLD WID WEB
   Chen SS., 2001, SIAM review
   DONOHO D, 2006, COMMUNICATIONS PURE
   DONOHO D, 2006, FAST SOLUTION I NORM
   GAO Y, 2010, P ACM C MULT
   Ji R, 2009, P ACM MULT
   JING F, 2006, P ACM MULT
   Jing Y., 2008, P INT WORLD WID WEB
   JOSHI D, 2010, P C MULT TOOLS APPL
   KEIJI Y, 2010, HDB SOCIAL NETWORK T
   Kennedy L., 2007, P ACM MULT
   Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140
   KRETZSCHMAR H, 2008, P IEEE C INT ROB SYS
   LAZEM SY, 2005, P INT C INF VIS
   LI X, 2008, P EUR C COMP VIS
   Li Y., 2009, P INT C COMP VIS
   MA Y, 2007, IEEE T PATT ANAL MAC
   MAIER M, 2008, P C ADV NEUR INF PRO
   NG AY, 2001, P C ADV NEUR INF PRO
   PAPADOPOULOS S, 2011, IEEE MULTIMEDIA
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Sivic J., 2003, P IEEE C COMP VIS PA
   SNAVELY N, 2006, P ACM SIGGRAPH INT C
   Tibshirani R., 1997, J ROYAL STAT SOC
   Torniai C., 2007, SHARING DISCOVERING
   WRIGHT J, 2009, IEEE T PATT ANAL MAC
   Wu J., 2009, P INT C COMP VIS
NR 35
TC 38
Z9 38
U1 2
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 31
DI 10.1145/2037676.2037688
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 857NR
UT WOS:000297725800012
DA 2024-07-18
ER

PT J
AU Lin, PY
   Lee, JS
   Chang, CC
AF Lin, Pei-Yu
   Lee, Jung-San
   Chang, Chin-Chen
TI Protecting the Content Integrity of Digital Imagery with Fidelity
   Preservation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Verification; Authentication; fragile watermarking; integrity
   protection; key-lock-pair system; weighted-sum function
ID FRAGILE WATERMARKING SCHEME; AUTHENTICATION; SECRET; STEGANOGRAPHY;
   OWNERSHIP
AB Fragile watermarking is applied to protect the content integrity of digital images. The main concerns related to watermarking include retaining the quality of the watermarked image and retaining the ability to detect whether any manipulation has occurred. Because recent watermarking techniques seriously distort the quality of the protected image after embedding the authentication code into the image content, attention has been drawn to how to satisfy both the need for image fidelity and detection ability. To account for the influence from both essentials, a novel algorithm is proposed in this article. The new scheme utilizes a weighted-sum function to embed (n+1) authentication bits into a block with 2(n) pixels by modifying only one original pixel with (+/- 1). With fewer authentication codes, the new process can protect the content of the image. The experimental results demonstrate that the approach can guarantee the fidelity of the watermarked image while retaining tamper-proof functionality.
C1 [Lin, Pei-Yu] Yuan Ze Univ, Dept Informat Commun, Chungli 32003, Taiwan.
   [Lee, Jung-San; Chang, Chin-Chen] Feng Chia Univ, Dept Comp Sci & Informat Engn, Taichung 40724, Taiwan.
C3 Yuan Ze University; Feng Chia University
RP Lin, PY (corresponding author), Yuan Ze Univ, Dept Informat Commun, 135 Yuan Tung Rd, Chungli 32003, Taiwan.
EM pagelin3@gmail.com; leejs@fcu.edu.tw; ccc@cs.ccu.edu.tw
RI Chang, Ching-Chun/JAN-6210-2023
OI Lin, Pei-Yu/0000-0001-8809-1063
CR CHAN CS, 2007, Patent No. 402681
   Chang CC, 2007, IMAGING SCI J, V55, P140, DOI 10.1179/174313107X165227
   Chang CC, 2006, PATTERN RECOGN LETT, V27, P439, DOI 10.1016/j.patrec.2005.09.006
   CHANG CK, 1989, IEEE T COMPUT, V38, P1462, DOI 10.1109/12.35842
   Cheddad A, 2009, SIGNAL PROCESS, V89, P2324, DOI 10.1016/j.sigpro.2009.02.001
   Fei CH, 2006, IEEE T INF FOREN SEC, V1, P43, DOI 10.1109/TIFS.2005.863505
   Fridrich A, 2000, P SOC PHOTO-OPT INS, V3971, P428, DOI 10.1117/12.384997
   Giakoumaki A, 2006, IEEE T INF TECHNOL B, V10, P722, DOI 10.1109/TITB.2006.875655
   Ho ATS, 2008, IEEE T INF FOREN SEC, V3, P567, DOI 10.1109/TIFS.2008.926994
   Holliman M, 2000, IEEE T IMAGE PROCESS, V9, P432, DOI 10.1109/83.826780
   Hsu CT, 1998, IEEE T CIRCUITS-II, V45, P1097, DOI 10.1109/82.718818
   Kundur D, 1999, P IEEE, V87, P1167, DOI 10.1109/5.771070
   Lin CC, 2004, J SYST SOFTWARE, V73, P405, DOI 10.1016/S0164-1212(03)00239-5
   Lu CS, 2003, IEEE T MULTIMEDIA, V5, P161, DOI 10.1109/TMM.2003.811621
   Miyahara M, 1998, IEEE T COMMUN, V46, P1215, DOI 10.1109/26.718563
   Phan RCW, 2008, PATTERN RECOGN, V41, P3493, DOI 10.1016/j.patcog.2008.05.009
   Rey C, 2002, EURASIP J APPL SIG P, V2002, P613, DOI 10.1155/S1110865702204047
   Stallings W., 2006, Cryptography and Network Security, V4th
   Suthaharan S, 2004, PATTERN RECOGN LETT, V25, P1893, DOI 10.1016/j.patrec.2004.08.017
   Thien CC, 2003, PATTERN RECOGN, V36, P2875, DOI 10.1016/S0031-3203(03)00221-8
   Tsai P, 2003, SIGNAL PROCESS-IMAGE, V18, P813, DOI 10.1016/j.image.2003.06.001
   Wang SJ, 2005, APPL MATH COMPUT, V164, P99, DOI 10.1016/j.amc.2004.04.059
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wong PW, 2001, IEEE T IMAGE PROCESS, V10, P1593, DOI 10.1109/83.951543
   WU ML, 1984, IEEE T SOFTWARE ENG, V10, P185, DOI 10.1109/TSE.1984.5010221
   Yang HJ, 2007, IEEE T MULTIMEDIA, V9, P475, DOI 10.1109/TMM.2006.887990
   Yeung MM, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P680, DOI 10.1109/ICIP.1997.638587
   Zhang WM, 2007, IEEE SIGNAL PROC LET, V14, P848, DOI 10.1109/LSP.2007.903255
   Zhang X, 2007, IEEE SIGNAL PROC LET, V14, P727, DOI 10.1109/LSP.2007.896436
   Zhang XP, 2009, SIGNAL PROCESS, V89, P675, DOI 10.1016/j.sigpro.2008.10.001
NR 30
TC 45
Z9 46
U1 0
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2011
VL 7
IS 3
AR 15
DI 10.1145/2000486.2000489
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 814DB
UT WOS:000294425100003
DA 2024-07-18
ER

PT J
AU Zheng, QF
   Gao, W
AF Zheng, Qing-Fang
   Gao, Wen
TI Constructing Visual Phrases for Effective and Efficient Object-Based
   Image Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT ACM Multimedia Conference 2007
CY SEP 24-29, 2007
CL Augsburg, GERMANY
SP ACM
DE Algorithms; Experimentation; Performance; Content-based image retrieval;
   inverted index; local image descriptor; SIFT; object-based image
   retrieval; visual phrase
ID WEB
AB The explosion of multimedia data necessitates effective and efficient ways for us to get access to our desired ones. In this article, we draw an analogy between image retrieval and text retrieval and propose a visual phrase-based approach to retrieve images containing desired objects (object-based image retrieval). The visual phrase is defined as a pair of frequently co-occurred adjacent local image patches and is constructed using data mining. We design methods on how to construct visual phrase and how to index/search images based on visual phrase. We demonstrate experiments to show our visual phrase-based approach can be very efficient and more effective than current visual word-based approach.
C1 [Zheng, Qing-Fang; Gao, Wen] Chinese Acad Sci, Inst Comp Technol, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Zheng, QF (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100080, Peoples R China.
EM qfzheng@jdl.ac.cn
CR Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108
   Agrawal R., 1994, P INT VLDB C VLDB 94, P487, DOI DOI 10.5555/645920.672836
   [Anonymous], P IEEE C CVPR
   [Anonymous], PATCH WORKSH CONJ CV
   Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800
   Ceglar A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1132956/1132958
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   DATTA R, 2006, 06009 CSE PENN STAT
   Enser P, 2003, LECT NOTES COMPUT SC, V2728, P291
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   Flickner M., 1995, IEEE COMPUT, V28, P23, DOI DOI 10.1109/2.410146
   GARCYAPEREZ D, 2006, P 18 INT C PATT REC, V4, P750
   Goethals B., 2003, Survey on frequent pattern mining
   Grabner M, 2006, LECT NOTES COMPUT SC, V3851, P918
   Hammouda KM, 2004, IEEE T KNOWL DATA EN, V16, P1279, DOI 10.1109/TKDE.2004.58
   HOIEM R, 2004, P IEEE C COMP VIS PA, V2, P490
   Jansen BJ, 2000, INFORM PROCESS MANAG, V36, P207, DOI 10.1016/S0306-4573(99)00056-4
   Jin R., 2004, Proceedings of the 12th annual ACM international conference on Multimedia, P892, DOI DOI 10.1145/1027527.1027732
   Jing F, 2004, IEEE T IMAGE PROCESS, V13, P699, DOI 10.1109/TIP.2004.826125
   Jurie F, 2005, IEEE I CONF COMP VIS, P604
   Ke Y., 2004, ACM MULTIMEDIA 04, P869
   Kherfi ML, 2004, ACM COMPUT SURV, V36, P35, DOI 10.1145/1013208.1013210
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo J., 2003, P 1 ACM INT WORKSHOP, P63
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Papageorgiou CP, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P555, DOI 10.1109/ICCV.1998.710772
   Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Sebe N., 1999, P 7 ACM INT C MULTIM, P79, DOI DOI 10.1145/319878.319901
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith J., 1996, INTELL MULTIMED INFO, P23
   Smyth W.F., 2003, P 7 DIGITAL IMAGE CO, P909
   Squire D.M., 2000, PATTERN RECOGN LETT, V21, P13
   Squire DM, 2000, PATTERN RECOGN LETT, V21, P1193, DOI 10.1016/S0167-8655(00)00081-7
   Sun X, 1998, ELECTROCHEM SOLID ST, V1, P239, DOI 10.1149/1.1390698
   Veltkamp Remco C., 2000, UUCS200034
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang G., 2006, CVPR, P1597
   Williams HE, 2004, ACM T INFORM SYST, V22, P573, DOI 10.1145/1028099.1028102
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   WITTER IH, 1999, MANAGING GIGABYTES C
   Yan XF, 2005, ACM T DATABASE SYST, V30, P960, DOI 10.1145/1114244.1114248
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   YEH T, 2004, CVPR, V2, P76
   ZHENG QF, 2006, P ACM INT C MULT SAN, P77
NR 48
TC 13
Z9 17
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2008
VL 5
IS 1
AR 7
DI 10.1145/1404880.1404887
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 376AS
UT WOS:000261155700007
DA 2024-07-18
ER

PT J
AU Li, CJ
   Zheng, SQ
   Prabhakaran, B
AF Li, Chuanjun
   Zheng, S. Q.
   Prabhakaran, B.
TI Segmentation and recognition of motion streams by similarity search
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE algorithms; motion capture; gesture recognition; pattern analysis;
   segmentation; similarity measures; principal component analysis;
   singular value decomposition
AB Fast and accurate recognition of motion data streams from gesture sensing and motion capture devices has many applications and is the focus of this article. Based on the analysis of the geometric structures revealed by singular value decompositions (SVD) of motion data, a similarity measure is proposed for simultaneously segmenting and recognizing motion streams. A direction identification approach is explored to further differentiate motions with similar data geometric structures. Experiments show that the proposed similarity measure can segment and recognize motion streams of variable lengths with high accuracy, without knowing beforehand the number of motions in a stream.
C1 Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
C3 University of Texas System; University of Texas Dallas
RP Li, CJ (corresponding author), Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
EM chuanjun@utdallas.edu; sizheng@utdallas.edu; praba@utdallas.edu
CR [Anonymous], 1994, P AAAI 94 WORKSH KNO
   [Anonymous], 2003, KDD
   Chen L., 2005, P 2005 ACM SIGMOD IN, P491
   Dyaberi Vidyarani M., 2004, P 12 ANN ACM INT C M, P332, DOI [10.1145/1027527.1027604, DOI 10.1145/1027527.1027604]
   Faloutsos Christos., 1994, ACM SIGMOD Record, V23, P419
   Golub GH., 1986, MATRIX COMPUTATIONS, V2nd
   Ikemoto Leslie., 2004, SCA 2004: Proceedings of the 2004 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P99
   Kahol K, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P105
   KRZANOWSKI WJ, 1979, J AM STAT ASSOC, V74, P703, DOI 10.2307/2286995
   LI C, 2004, P 2 ACM INT WORKSH M, P75
   Li C., 2004, P ACM MULTIMEDIA C 2, P836
   Li C.J., 2004, Proceedings of the ASME International Mechanical Engineering Congress, P1
   Liang RH, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P558, DOI 10.1109/AFGR.1998.671007
   PULLEN K, 2002, P 29 ANN C COMP GRAP, P501
   QIAN G, 2004, P IEEE INT C MULT EX
   Schöllhorn WI, 2002, GAIT POSTURE, V15, P180, DOI 10.1016/S0966-6362(01)00193-X
   SHAHABI C, 2001, P 9 INT C HUM COMP I, P441
   Shahabi C., 2003, P 9 INT C MULT MOD, P93
   Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811
   STEWART GW, 1973, SIAM REV, V15, P727, DOI 10.1137/1015095
   Taylor CJ, 2000, COMPUT VIS IMAGE UND, V80, P349, DOI 10.1006/cviu.2000.0878
   Watelain E, 2000, ARCH PHYS MED REHAB, V81, P579, DOI 10.1016/S0003-9993(00)90038-8
   Yang K., 2004, ACM INT WORKSHOP MUL, P65, DOI DOI 10.1145/1032604.1032616
   2007, VICON ONLINE PRODUCT
NR 24
TC 23
Z9 29
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PY 2007
VL 3
IS 3
AR 16
DI 10.1145/1236471.1236475
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230JB
UT WOS:000250871700004
DA 2024-07-18
ER

PT J
AU Ferrara, A
   Ludovico, LA
   Montanelli, S
   Castano, S
   Haus, G
AF Ferrara, Alfio
   Ludovico, Luca A.
   Montanelli, Stefano
   Castano, Silvana
   Haus, Goffredo
TI A semantic web ontology for context-based classification and retrieval
   of music resources
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article; Proceedings Paper
CT 11th International Workshop on Multimedia Information Systems
CY SEP 19-21, 2005
CL Sorrento, ITALY
DE design; documentation; standardization; languages; music classification;
   music retrieval; ontology-based music representation
AB In this article, we describe the MX-Onto ontology for providing a Semantic Web compatible representation of music resources based on their context. The context representation is realized by means of an OWL ontology that describes music information and that defines rules and classes for a flexible genre classification. By flexible classification we mean that the proposed approach enables capturing the subjective interpretation of music genres by defining multiple membership relations between a music resource and the corresponding music genres, thus supporting context-based and proximity-based search of music resources.
C1 Univ Stud Milan, DICo, I-20135 Milan, Italy.
RP Ferrara, A (corresponding author), Univ Stud Milan, DICo, Via Comelico 39, I-20135 Milan, Italy.
EM ferrara@dico.unimi.it; ludovico@dico.unimi.it; montanelli@dico.unimi.it;
   castano@dico.unimi.it; haus@dico.unimi.it
RI Ludovico, Luca Andrea/H-1913-2015; Ferrara, Alfio/AAG-3038-2020;
   MONTANELLI, STEFANO/M-4876-2017; Haus, Goffredo/H-1839-2015
OI Ludovico, Luca Andrea/0000-0002-8251-2231; Ferrara,
   Alfio/0000-0002-4991-4984; Haus, Goffredo/0000-0002-3477-4042; CASTANO,
   SILVANA/0000-0002-3826-2407
CR ALVARO J, 2005, P 10 BRAZ S MUS COMP
   [Anonymous], 2004, W3C MEMB SUBMISS
   [Anonymous], WEB SEMANTICS ONTOLO
   Aucouturier JJ, 2003, J NEW MUSIC RES, V32, P83, DOI 10.1076/jnmr.32.1.83.16801
   Bohlman P., 2001, RETHINKING MUSIC
   CAMBOUROPOULOS E, 1998, P 12 C MUSICIAL INFO
   CASTANO S, 2006, J DATA SEMANT JODS, pR5
   CELMA O, 2004, P 1 WORKSH FRIEND FR
   DANNENBERG R, 2001, P INT COMP MUS C HAB
   Deshpande H., 2001, P COST G 6 C DIG AUD
   Diday E., 1981, Digital Image Processing. Proceedings of the NATO Advanced Study Institute, P19
   Ding Zhongli, 2004, P 37 ANN HAW INT C S
   FRENCH J, 2001, P INT C C WEB DEL MU
   GRUBER TR, 1993, KNOWL ACQUIS, V5, P199, DOI 10.1006/knac.1993.1008
   HAUS G, 2001, IEEE SA, V1599
   Huron D, 2002, COMPUT MUSIC J, V26, P11, DOI 10.1162/014892602760137158
   Huron D., 1995, HUMDRUM TOOLKIT REFE
   LAMBROU T, 1998, P IEEE INT C ACOUST
   NOY N, 2004, W3C
   PACHET F, 2001, P INT C WEB DEL MUS
   Pachet F., 2000, P CONTENT BASED MULT
   PESTONI F, 2001, P INT C WEB DEL MUS
   RANWEZ S, 2002, MUSIC ONTOLOGY
   Schutze H., 1992, Proceedings. Supercomputing '92. (Cat. No.92CH3216-9), P787, DOI 10.1109/SUPERC.1992.236684
   SELFRIDGEFIELD E, 1997, MIDI HDB MUSICAL COD
   SHARDANAND U, 1995, P ACM C HUM RACT COM
   Smith M. K., 2004, W3C RECOMMENDATION
   SOLTAU H, 1998, P INT C ACOUST SPEEC
   Stoilos G., 2005, INT WORKSH OWL EXP D
   Straccia U, 2001, J ARTIF INTELL RES, V14, P137, DOI 10.1613/jair.813
   Tzanetakis G., 2000, P INT S MUS INF RETR
   TZANETAKIS G, 2001, P INT S MUS INF RETR
NR 32
TC 10
Z9 13
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2006
VL 2
IS 3
BP 177
EP 198
DI 10.1145/1152149.1152151
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 238IL
UT WOS:000251440700002
DA 2024-07-18
ER

PT J
AU Lew, MS
   Sebe, N
   Djeraba, C
   Jain, R
AF Lew, Michael S.
   Sebe, Nicu
   Djeraba, Chabane
   Jain, Ramesh
TI Content-based multimedia information retrieval: State of the art and
   challenges
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Review
DE design; experimentation; human factors; performance; multimedia
   information retrieval; image search; video retrieval; audio retrieval;
   image databases; multimedia indexing; human-computer interaction
ID IMAGE RETRIEVAL; CLASSIFICATION; SHAPES; SCALE; FACES
AB Extending beyond the boundaries of science, art, and culture, content-based multimedia information retrieval provides new paradigms and methods for searching through the myriad variety of media all over the world. This survey reviews 100+ recent articles on content-based multimedia information retrieval and discusses their role in current research directions which include browsing and search paradigms, user studies, affective computing, learning, semantic queries, new features and media types, high performance indexing, and evaluation techniques. Based on the current state of the art, we discuss the major challenges for the future.
C1 Leiden Univ, NL-2300 RA Leiden, Netherlands.
   Univ Amsterdam, Fac Sci, NL-1098 SJ Amsterdam, Netherlands.
   Univ Calif Irvine, Irvine, CA USA.
C3 Leiden University - Excl LUMC; Leiden University; University of
   Amsterdam; University of California System; University of California
   Irvine
RP Sebe, N (corresponding author), Leiden Univ, NL-2300 RA Leiden, Netherlands.
RI Tavares, António JV/A-7115-2008; Djeraba, Chaabane/ABI-8490-2020;
   Djeraba, Chaabane/AAC-6311-2020; Sebe, Niculae/KEC-2000-2024
OI Djeraba, Chaabane/0000-0003-4579-9592; Sebe, Niculae/0000-0002-6597-7248
CR Amir A, 2004, COMPUT VIS IMAGE UND, V96, P216, DOI 10.1016/j.cviu.2004.02.006
   [Anonymous], 2002, CSCW '02, DOI DOI 10.1145/587078.587102
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], P 6 ACM SIGMM INT WO
   [Anonymous], 2001, International Journal of Image and Graphics (IJIG), Vol, DOI DOI 10.1142/S021946780100027X
   [Anonymous], 1989, Decision Estimation and Classification
   Assfalg J., 2004, Proceedings of the ACM SIGMM International Workshop on Multimedia Information Retrieval, P77
   Bach JR, 1996, P SOC PHOTO-OPT INS, V2670, P76, DOI 10.1117/12.234785
   BAKKER EM, 2002, P 1 INT C IM VID RET, P262
   Ballard D.H., 1982, Computer Vision
   Bartolini I, 2005, IEEE T PATTERN ANAL, V27, P142, DOI 10.1109/TPAMI.2005.21
   Battelle John, 2005, The search: How Google and Its Rivals Rewrote the Rules of Business and Transformed Our Culture
   Baumberg A, 2000, PROC CVPR IEEE, P774, DOI 10.1109/CVPR.2000.855899
   BELL G, 2004, P 12 ANN ACM INT C M
   BENITEZ AB, 2002, P IEEE INT C MULT
   Berretti S, 2001, IEEE T PATTERN ANAL, V23, P1089, DOI 10.1109/34.954600
   BERTHOUZE NB, 1998, ADV DATABASE SYSTEMS, P227
   Bliujute R, 1999, PROC INT CONF DATA, P314, DOI 10.1109/ICDE.1999.754947
   BOSSON A, 2002, P INT C IM VID RETR, P50
   Cappelli R, 2001, IEEE T PATTERN ANAL, V23, P977, DOI 10.1109/34.955111
   Chang SF, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P531, DOI 10.1109/ICIP.1998.727321
   Chen Y., 2002, Proceedings of the Eleventh International Conference on Information and Knowledge Management. CIKM 2002, P227, DOI 10.1145/584792.584832
   Chen Y., 2001, P IEEE INT C IM PROC, P815
   CHIU P., 2005, P 13 ANN ACM INT C M, P213
   Chua TS, 2002, VISUAL COMPUT, V18, P121, DOI 10.1007/s003710100137
   Cooper Matthew., 2005, ACM T MULTIM COMPUT, V1, P269, DOI [DOI 10.1145/1083314.1083317, 10.1145/1083314.1083317]
   Dimitrova N, 2002, IEEE MULTIMEDIA, V9, P42, DOI 10.1109/MMUL.2002.1022858
   DIMITROVA N, 2003, P 2 INT C IM VID RET, P9
   DIMITROVA N, 2000, EUROPEAN SIGNAL PROC
   Djeraba C, 2003, IEEE T KNOWL DATA EN, V15, P118, DOI 10.1109/TKDE.2003.1161586
   Djeraba C, 2002, IEEE MULTIMEDIA, V9, P18, DOI 10.1109/MMUL.2002.998047
   Downie J.Stephen., 2003, P 4 INT C MUSIC INFO, P25
   Dufournaud Y, 2000, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2000.855876
   Dy JG, 2003, IEEE T PATTERN ANAL, V25, P373, DOI 10.1109/TPAMI.2003.1182100
   Eakins JP, 2003, LECT NOTES COMPUT SC, V2728, P28
   ECAS R, 1999, P INT C VIS INF SYST, P533
   EITER T, 2005, DATABASE THEORY
   El-Kwae EA, 2000, ACM T INFORM SYST, V18, P171, DOI 10.1145/348751.348762
   Enser P, 2003, LECT NOTES COMPUT SC, V2728, P291
   ENSER PGB, 2005, P 4 INT C IM VID RET, P497
   Fan Jianping., 2004, ACM International Conference on Multimedia, P540, DOI [DOI 10.1145/1027527, DOI 10.1145/1027527.1027660]
   Flickner M., 1995, IEEE COMPUT, V28, P23, DOI DOI 10.1109/2.410146
   Foote J, 1999, MULTIMEDIA SYST, V7, P2, DOI 10.1007/s005300050106
   Foote J, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P452, DOI 10.1109/ICME.2000.869637
   Forsyth DA, 1999, INT J COMPUT VISION, V32, P63, DOI 10.1023/A:1008145029462
   Frankel C., 1996, 9614 U CHIC
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Gevers T, 2001, ADV PTRN RECOGNIT, P11
   GONG B, 2004, P 6 ACM SIGMM INT WO, P7
   Graham A., 2002, Proceedings of the second ACM/IEEE-CS joint conference on Digital libraries, P326
   Greenspan H, 2004, IEEE T PATTERN ANAL, V26, P384, DOI 10.1109/TPAMI.2004.1262334
   GUO G, 2001, P IEEE C MULT EXP AU
   Haas M., 1997, IMAGE DATABASES MULT, P191
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   HANJALIC A, 1997, IMAGE DATABASES MULT, P97
   Haralick R.M., 1993, COMPUTER ROBOT VISIO
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hastings SK, 1999, LIBR TRENDS, V48, P438
   He X., 2002, PROC ACM MULTIMEDIA, P343
   Heng Tao Shen, 2000, Proceedings ACM Multimedia 2000, P39, DOI 10.1145/354384.376098
   HOWE N, 2003, P 2 INT C IM VID RET, P61
   Huijsmans DP, 2005, IEEE T PATTERN ANAL, V27, P245, DOI 10.1109/TPAMI.2005.30
   Jacobs DW, 2000, IEEE T PATTERN ANAL, V22, P583, DOI 10.1109/34.862197
   Jafari-Khouzani K, 2005, IEEE T PATTERN ANAL, V27, P1004, DOI 10.1109/TPAMI.2005.126
   JAIMES A, 2006, IN PRESS COMPUT VISI
   Jain R, 2003, COMMUN ACM, V46, P48, DOI 10.1145/792704.792729
   JAIN R., 2003, ACM SIGMM WORKSHOP E, P1
   Jolion JM, 2001, ADV PTRN RECOGNIT, P121
   Krishnapuram R, 2004, IEEE T KNOWL DATA EN, V16, P1185, DOI 10.1109/TKDE.2004.53
   Levine MartinD., 1985, VISION MAN MACHINE
   Lew M. S., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P601, DOI 10.1109/ICPR.1996.547017
   Lew M.S., 2001, PRINCIPLES VISUAL IN
   Lew MS, 2001, IMAGE VISION COMPUT, V19, P561, DOI 10.1016/S0262-8856(00)00102-5
   LEW MS, 2000, IEEE COMPUTER    NOV, P46
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Lim JH, 2003, IEEE MULTIMEDIA, V10, P28, DOI 10.1109/MMUL.2003.1237548
   Lindeberg T, 1997, IMAGE VISION COMPUT, V15, P415, DOI 10.1016/S0262-8856(97)01144-X
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   LIU X, 2003, P 3 INT C IM VID RET, P50
   Liu YM, 2004, J OPT A-PURE APPL OP, V6, P84, DOI 10.1088/1464-4258/6/1/015
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Markkula M., 2000, Information Retrieval, V1, P259, DOI 10.1023/A:1009995816485
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   MONGY S, 2005, P ACM MDM KDD WORKSH
   Müller H, 2000, INT C PATT RECOG, P1043, DOI 10.1109/ICPR.2000.905650
   Muller H., 2002, PROC INT C IMAGE VID, P38
   MULLER W, 2003, P 5 ACM SIGMM INT WO, P79
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Pereira Fernando., 2001, International Journal of Image and Graphics, V1, P527, DOI DOI 10.1142/S021946780100030X
   Picard R.W., 2000, Affective Computing
   Pickering MJ, 2003, COMPUT VIS IMAGE UND, V92, P217, DOI 10.1016/j.cviu.2003.06.002
   RAUTIAINEN M, 2003, P INT C IM VID RETR, P260
   ROCCHIO, 1971, SMART RETRIEVAL SYST
   Rodden K., 2003, P SIGCHI C HUMAN FAC, P409, DOI DOI 10.1145/642611.642682
   RODDEN K, 2001, P SIGCHI C HUM FACT, P190, DOI DOI 10.1145/365024.365097
   Rowe LA, 2005, ACM T MULTIM COMPUT, V1, P3, DOI 10.1145/1047936.1047938
   Rowley HA, 1996, ADV NEUR IN, V8, P875
   RUBIN R, 2004, FDN LIB INF SCI
   Rui Y, 2001, ADV PTRN RECOGNIT, P219
   Salway A., 2003, P 11 ACM INT C MULTI, P299
   Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446
   Schneiderman H, 2004, INT J COMPUT VISION, V56, P151, DOI 10.1023/B:VISI.0000011202.85607.00
   Sclaroff S, 2001, ADV PTRN RECOGNIT, P259
   SCORR GJ, 2003, P 2 INT C IM VID RET, P467
   Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924
   Sebe N, 2000, IEEE T PATTERN ANAL, V22, P1132, DOI 10.1109/34.879793
   Sebe N, 2003, IMAGE VISION COMPUT, V21, P1087, DOI 10.1016/j.imavis.2003.08.012
   Sebe N, 2002, LECT NOTES COMPUT SC, V2383, P17
   Sebe N, 2001, PATTERN RECOGN LETT, V22, P223, DOI 10.1016/S0167-8655(00)00092-1
   Sebe N, 2003, P 2 INT C IM VID RET
   Shao H, 2003, LECT NOTES COMPUT SC, V2728, P71
   SILVA GC, 2005, P 13 ACM INT C MULT
   SMEATON AF, 2003, P 2 INT C IM VID RET, P10
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith JR, 1997, IEEE MULTIMEDIA, V4, P12, DOI 10.1109/93.621578
   SNOEK CGM, 2005, P 13 ACM INT C MULT, P225
   SPIERENBURG JA, 1997, P 8 BRIT MACH VIS C
   Srivastava A, 2005, IEEE T PATTERN ANAL, V27, P590, DOI 10.1109/TPAMI.2005.86
   Sundaram Hari., 2002, COMUNICA O APRESENTA, DOI [DOI 10.1145/641007.641042, 10.1145/641007.641042]
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   Tian Q, 2001, J ELECTRON IMAGING, V10, P835, DOI 10.1117/1.1406945
   TIAN Q, 2002, P 1 INT C IM VID RET, P7
   Tieu K, 2004, INT J COMPUT VISION, V56, P17, DOI 10.1023/B:VISI.0000004830.93820.78
   Tuytelaars T., 2000, BMV2000. Proceedings of the 11th British Machine Vision Conference, P412
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Vailaya A, 1998, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES - PROCEEDINGS, P3, DOI 10.1109/IVL.1998.694464
   Veltkamp RC, 2001, ADV PTRN RECOGNIT, P87
   Wang WN, 2004, IEEE SYS MAN CYBERN, P6407
   Winston P.H., 1992, Artificial Intelligence
   Worring M, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P297, DOI 10.1109/ICME.2004.1394184
   Worring M., 2001, International Journal of Image and Graphics, V1, P387
   WU P, 2001, INT J IMAGE GRAPH, V1, P547
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   YE H, 2003, P 2 INT C IM VID RET, P477
   Yin PY, 2005, IEEE T PATTERN ANAL, V27, P1536, DOI 10.1109/TPAMI.2005.201
   Zhou X.S., 2001, P ACM INT C MULTIMED, P137
NR 136
TC 849
Z9 977
U1 5
U2 252
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2006
VL 2
IS 1
BP 1
EP 19
DI 10.1145/1126004.1126005
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 230IW
UT WOS:000250871200001
DA 2024-07-18
ER

PT J
AU Chan, PPK
   Hu, XM
   Song, HR
   Peng, P
   Chen, KK
   Yeung, DS
AF Chan, Patrick P. K.
   Hu, Xiaoman
   Song, Haorui
   Peng, Peng
   Chen, Keke
   Yeung, Daniel S.
TI Learning Disentangled Features for Person Re-identification under
   Clothes Changing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; clothes changing; feature disentanglement
ID NEURAL-NETWORKS
AB Clothes changing is one of the challenges in person re-identification (ReID), since clothes provide remarkable and reliable information for decision, especially when the resolution of an image is low. Variation of clothes significantly downgrades standard ReID models, since the clothes information dominates the decisions. The performance of the existing methods considering clothes changing is still not satisfying, since they fail to extract sufficient identity information that excludes clothes information. This study aims to disentangle identity, clothes, and unrelated features with a Generative Adversarial Network (GAN). A GAN model with three encoders, one generator, and three discriminators, and its training procedure are proposed to learn these kinds of features separately and exclusively. Experimental results indicate that our model generally achieves the best performance among state-of-the-art methods in both ReID tasks with and without clothes changing, which confirms that the identity, clothes, and unrelated features are extracted by our model more precisely and effectively.
C1 [Chan, Patrick P. K.] South China Univ Technol, Shien Ming Wu Sch Intelligent Engn, Guangzhou 510006, Peoples R China.
   [Hu, Xiaoman; Song, Haorui; Peng, Peng; Chen, Keke; Yeung, Daniel S.] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
C3 South China University of Technology; South China University of
   Technology
RP Hu, XM (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM patrickchan@ieee.org; jacquelinehxm@outlook.com; haorui.song@qq.com;
   pengpeng-scut@qq.com; kkechen@qq.com; danyeung@ieee.org
RI Xiaoman, Hu/KFR-4589-2024
OI Xiaoman, Hu/0000-0002-7222-0673; Peng, Peng/0000-0003-1204-9234
FU Natural Science Foundation of Guangdong Province, China
   [2018A030313203]; Fundamental Research Funds for the Central
   Universities [2018ZD32]
FX This article is supported by the Natural Science Foundation of Guangdong
   Province, China (Grant No. 2018A030313203) and the Fundamental Research
   Funds for the Central Universities (Grant No. 2018ZD32).
CR Bak S, 2018, LECT NOTES COMPUT SC, V11217, P193, DOI 10.1007/978-3-030-01261-8_12
   Barbosa IB, 2012, LECT NOTES COMPUT SC, V7583, P433, DOI 10.1007/978-3-642-33863-2_43
   Chan PPK, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533763
   Chan PPK, 2021, LECT NOTES COMPUT SC, V12891, P42, DOI 10.1007/978-3-030-86362-3_4
   Chen JX, 2021, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR46437.2021.00805
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eom Chanho, 2019, P 33 INT C NEURAL IN
   Fan X, 2019, LECT NOTES COMPUT SC, V11362, P19, DOI 10.1007/978-3-030-20890-5_2
   Fangbin Wan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P3620, DOI 10.1109/CVPRW50498.2020.00423
   Ge YX, 2018, ADV NEUR IN, V31
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   Hong PX, 2021, PROC CVPR IEEE, P10508, DOI 10.1109/CVPR46437.2021.01037
   Huang Y, 2019, INT JOINT C NEURAL N
   Huang Y, 2020, IEEE T CIRC SYST VID, V30, P3459, DOI 10.1109/TCSVT.2019.2948093
   Ioffe Sergey, 2015, P IEEE INT C MACHINE
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Li YY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P163, DOI 10.1145/3240508.3240573
   Li YJ, 2021, IEEE WINT CONF APPL, P2431, DOI 10.1109/WACV48630.2021.00248
   Li YJ, 2019, IEEE I CONF COMP VIS, P8089, DOI 10.1109/ICCV.2019.00818
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Maas Andrew L., 2013, P IEEE INT C MACHINE
   Qian Xuelin, 2017, P EUROPEAN C COMPUTE
   Sabour S, 2017, ADV NEUR IN, V30
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Xu Wanlu, 2021, IJCAI, P1201
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Yu SJ, 2020, Arxiv, DOI [arXiv:2005.07862, 10.48550/arXiv.2005.07862, DOI 10.48550/ARXIV.2005.07862]
   Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017
   Zhang CZ, 2021, IEEE T IMAGE PROCESS, V30, P1291, DOI 10.1109/TIP.2020.3042083
   Zheng JH, 2022, INFORM SCIENCES, V615, P758, DOI 10.1016/j.ins.2022.09.060
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
NR 47
TC 0
Z9 0
U1 3
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2023
VL 19
IS 6
DI 10.1145/3584359
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3AP4
UT WOS:001035785200003
DA 2024-07-18
ER

PT J
AU Xiang, SC
   Qian, DH
   Guan, MY
   Yan, BJ
   Liu, T
   Fu, YZ
   You, GJ
AF Xiang, Suncheng
   Qian, Dahong
   Guan, Mengyuan
   Yan, Binjie
   Liu, Ting
   Fu, Yuzhuo
   You, Guanjie
TI Less Is More: Learning from Synthetic Data with Fine-Grained Attributes
   for Person Re-Identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; synthetic data; efficient training
AB Person re-identification (ReID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data [9], which benefits from the popularity of the synthetic data engine, has attracted great attention from the public. However, existing datasets are limited in quantity, diversity, and realisticity, and cannot be efficiently used for the ReID problem. To address this challenge, we manually construct a large-scale person dataset named FineGPR with fine-grained attribute annotations. Moreover, aiming to fully exploit the potential of FineGPR and promote the efficient training from millions of synthetic data, we propose an attribute analysis pipeline called AOST based on the traditional machine learning algorithm, which dynamically learns attribute distribution in a real domain, then eliminates the gap between synthetic and real-world data and thus is freely deployed to new scenarios. Experiments conducted on benchmarks demonstrate that FineGPR with AOST outperforms (or is on par with) existing real and synthetic datasets, which suggests its feasibility for the ReID task and proves the proverbial less-is-more principle. Our synthetic FineGPR dataset is publicly available at https://github.com/JeremyXSC/FineGPR.
C1 [Xiang, Suncheng; Qian, Dahong] Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai, Peoples R China.
   [Guan, Mengyuan; Yan, Binjie; Liu, Ting; Fu, Yuzhuo] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai, Peoples R China.
   [You, Guanjie] Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha, Hunan, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; National
   University of Defense Technology - China
RP Xiang, SC (corresponding author), Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai, Peoples R China.
EM xiangsuncheng17@sjtu.edu.cn; dahong.qian@sjtu.edu.cn;
   gemini.my@sjtu.edu.cn; yanbinjie@sjtu.edu.cn; louisa_liu@sjtu.edu.cn;
   yzfu@sjtu.edu.cn; ygjssxz@163.com
OI Liu, Ting/0000-0003-3489-4578; Xiang, Suncheng/0000-0002-9141-6460
FU National Natural Science Foundation of China [61977045, 81974276]
FX This work was partially supported by the National Natural Science
   Foundation of China under grants 61977045 and 81974276.
CR Bak S, 2018, LECT NOTES COMPUT SC, V11217, P193, DOI 10.1007/978-3-030-01261-8_12
   Barbosa IB, 2018, COMPUT VIS IMAGE UND, V167, P50, DOI 10.1016/j.cviu.2017.12.002
   Buolamwini J., 2018, FACCT, P77
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Das A, 2014, LECT NOTES COMPUT SC, V8690, P330, DOI 10.1007/978-3-319-10605-2_22
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Fabbri M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10829, DOI 10.1109/ICCV48922.2021.01067
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goddard M, 2017, INT J MARKET RES, V59, P703, DOI 10.2501/IJMR-2017-050
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gray D., 2007, P IEEE INT WORKSH PE, V3, P41
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Kingma D. P., 2014, arXiv
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li ZJ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3362988
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2020, PROC CVPR IEEE, P6886, DOI 10.1109/CVPR42600.2020.00692
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Pepik B, 2012, PROC CVPR IEEE, P3362, DOI 10.1109/CVPR.2012.6248075
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Ruiz N, 2019, Arxiv, DOI arXiv:1810.02513
   Schwartz WR, 2009, SIBGRAPI, P322, DOI 10.1109/SIBGRAPI.2009.42
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang YN, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3422, DOI 10.1145/3394171.3413815
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xiang SC, 2022, Arxiv, DOI arXiv:2110.05074
   Xiang SC, 2022, IEEE COMPUT SOC CONF, P4730, DOI 10.1109/CVPRW56347.2022.00519
   Xiang SC, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102822
   Xiang SC, 2023, MACH LEARN, V112, P1923, DOI 10.1007/s10994-022-06184-x
   Xiang SC, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P3765, DOI 10.1109/ICASSP39728.2021.9413757
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Yao Y., 2020, EUR C COMP VIS, P775, DOI DOI 10.1007/978-3-030
   Zhang TY, 2021, PROC CVPR IEEE, P11501, DOI 10.1109/CVPR46437.2021.01134
   Zhang Z., 2018, P 32 INT C NEURAL IN, VVolume 31
   Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26
   Zhao ZW, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491225
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 52
TC 10
Z9 10
U1 4
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2023
VL 19
IS 5
SU S
AR 173
DI 10.1145/3588441
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L0SK0
UT WOS:001020437100009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhu, LW
   Zhang, Y
   Li, N
   Jiang, GY
   Kwong, S
AF Zhu, Linwei
   Zhang, Yun
   Li, Na
   Jiang, Gangyi
   Kwong, Sam
TI Deep Learning-Based Intra Mode Derivation for Versatile Video Coding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Versatile video coding; intra mode derivation; most probable mode; deep
   learning; multi-class classification
ID PREDICTION
AB In intra coding, Rate Distortion Optimization (RDO) is performed to achieve the optimal intra mode from a pre-defined candidate list. The optimal intra mode is also required to be encoded and transmitted to the decoder side besides the residual signal, where lots of coding bits are consumed. To further improve the performance of intra coding in Versatile Video Coding (VVC), an intelligent intra mode derivation method is proposed in this paper, termed as Deep Learning based Intra Mode Derivation (DLIMD). In specific, the process of intra mode derivation is formulated as a multi-class classification task, which aims to skip the module of intra mode signaling for coding bits reduction. The architecture of DLIMD is developed to adapt to different quantization parameter settings and variable coding blocks including non-square ones, where only one single trained model is required. Different from the existing deep learning based classification problems, the hand-crafted features are also fed into intra mode derivation network besides the learned features from feature learning network. To compete with traditional methods, one additional binary flag is utilized in the video codec to indicate the selected scheme with RDO. Extensive experimental results reveal that the proposed method can achieve 2.28%, 1.74%, and 2.18% bit rate reduction on average for Y, U, and V components on the platform of VVC test model, which outperforms the state-of-the-art works.
C1 [Zhu, Linwei; Zhang, Yun; Li, Na] Chinese Acad Sci, Shenzhen Inst Adv Technol, 1068 Xueyuan Ave,Univ Town, Shenzhen 518055, Peoples R China.
   [Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, 818 Fenghua Rd, Ningbo 315211, Zhejiang, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Kowloon, Tat Chee Ave, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Ningbo University; City University of Hong Kong
RP Zhang, Y (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, 1068 Xueyuan Ave,Univ Town, Shenzhen 518055, Peoples R China.
EM lw.zhu@siat.ac.cn; yun.zhang@siat.ac.cn; na.li1@siat.ac.cn;
   jianggangyi@nbu.edu.cn; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012; jiang, gang/KII-8233-2024
OI Kwong, Sam/0000-0001-7484-7261; , Na/0000-0003-3888-7893
FU National Natural Science Foundation of China [62172400, 61901459,
   61902389, 62271276]; Shenzhen Science and Technology Program
   [JCYJ20200109110410133]; Guangdong Basic and Applied Basic Research
   Foundation [2022A1515011351]; Membership of Youth Innovation Promotion
   Association, Chinese Academy of Sciences (CAS) [2018392]; China
   Postdoctoral Science Foundation [2021T140696]; CAS Present's
   International Fellowship Initiative (PIFI) [2022VTA0005]; Hong Kong
   Innovation and Technology Commission (InnoHK Project CIMDA); Hong Kong
   GRF-RGC General Research Fund [11209819 (CityU 9042816), 11203820 (CityU
   9042598)]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172400, 61901459, 61902389 and
   62271276, in part by the Shenzhen Science and Technology Program under
   Grant JCYJ20200109110410133, in part by the Guangdong Basic and Applied
   Basic Research Foundation under Grant 2022A1515011351, in part by the
   Membership of Youth Innovation Promotion Association, Chinese Academy of
   Sciences (CAS) under Grant 2018392, in part by the China Postdoctoral
   Science Foundation under Grant 2021T140696, in part by the CAS Present's
   International Fellowship Initiative (PIFI) under Grant 2022VTA0005, in
   part by the Hong Kong Innovation and Technology Commission (InnoHK
   Project CIMDA), in part by the Hong Kong GRF-RGC General Research Fund
   under Grants 11209819 (CityU 9042816) and 11203820 (CityU 9042598).
CR Abdo Mahmoud A, 2020, 2020 IEEE INT C MULT, P1
   Abdoli M, 2018, IEEE SIGNAL PROC LET, V25, P1690, DOI 10.1109/LSP.2018.2871872
   [Anonymous], 2014, APSIPA TRANS SIGNAL
   Bjotegaard G., 2001, VCEGM33
   Blasi SG, 2015, IEEE T CIRC SYST VID, V25, P798, DOI 10.1109/TCSVT.2014.2359097
   Brand F, 2021, IEEE J-STSP, V15, P354, DOI 10.1109/JSTSP.2020.3034768
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Cai X, 2013, IEEE T IMAGE PROCESS, V22, P5395, DOI 10.1109/TIP.2013.2284073
   Chang YJ, 2019, IEEE DATA COMPR CONF, P559, DOI 10.1109/DCC.2019.00071
   Chen HM, 2016, IEEE T IMAGE PROCESS, V25, P3671, DOI 10.1109/TIP.2016.2573585
   Chen J, 2018, IEEE T IMAGE PROCESS, V27, P314, DOI 10.1109/TIP.2017.2750413
   Dumas T, 2021, IEEE T IMAGE PROCESS, V30, P697, DOI 10.1109/TIP.2020.3038348
   Dumas T, 2020, IEEE T IMAGE PROCESS, V29, P679, DOI 10.1109/TIP.2019.2934565
   François E, 2016, IEEE T CIRC SYST VID, V26, P63, DOI 10.1109/TCSVT.2015.2461911
   Gao H, 2021, IEEE T CIRC SYST VID, V31, P3197, DOI 10.1109/TCSVT.2020.3037024
   Hu YY, 2019, IEEE T MULTIMEDIA, V21, P3024, DOI 10.1109/TMM.2019.2920603
   Huang YW, 2020, IEEE T CIRC SYST VID, V30, P1311, DOI 10.1109/TCSVT.2019.2945048
   Jiang Minqiang, 2018, 2018 IEEE INT S CIRC, P1
   Lainema J, 2012, IEEE T CIRC SYST VID, V22, P1792, DOI 10.1109/TCSVT.2012.2221525
   Li CR, 2019, IEEE DATA COMPR CONF, P587, DOI 10.1109/DCC.2019.00099
   Li JH, 2018, IEEE T CIRC SYST VID, V28, P947, DOI 10.1109/TCSVT.2016.2633377
   Li JR, 2020, IEEE T IMAGE PROCESS, V29, P7245, DOI 10.1109/TIP.2020.3000351
   Li Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3434250
   Ma D, 2022, IEEE T MULTIMEDIA, V24, P3847, DOI 10.1109/TMM.2021.3108943
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Molchanov P., 2017, INT C LEARN REPR ICL, P1, DOI DOI 10.1002/9781118786352.WBIEG1156
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Nasrallah A, 2019, IEEE IMAGE PROC, P3153, DOI [10.1109/icip.2019.8803773, 10.1109/ICIP.2019.8803773]
   Nasrallah A, 2019, IEEE DATA COMPR CONF, P597, DOI 10.1109/DCC.2019.00109
   Pfaff J, 2021, IEEE T CIRC SYST VID, V31, P3834, DOI 10.1109/TCSVT.2021.3072430
   Reuze K, 2019, IEEE DATA COMPR CONF, P601, DOI 10.1109/DCC.2019.00113
   Schäfer M, 2020, IEEE IMAGE PROC, P3364, DOI 10.1109/ICIP40778.2020.9190883
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Seregin V., 2019, JOINT VIDEO EXPERTS
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun HM, 2020, IEEE T MULTIMEDIA, V22, P2764, DOI 10.1109/TMM.2019.2963620
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang Y, 2020, IEEE T CIRC SYST VID, V30, P1803, DOI 10.1109/TCSVT.2019.2934681
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiu Xiaoyu, 2016, 2016 PICTURE CODING, P1
   Xu XZ, 2016, IEEE J EM SEL TOP C, V6, P409, DOI 10.1109/JETCAS.2016.2597645
   Xu XZ, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P457, DOI 10.1109/PCS.2012.6213253
   Ye Y, 2020, IEEE T CIRC SYST VID, V30, P1241, DOI 10.1109/TCSVT.2019.2953827
   Yoon YU, 2019, ELECTRON LETT, V55, P188, DOI 10.1049/el.2018.7452
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P3983, DOI 10.1109/TIP.2018.2830640
   Zhang L, 2019, IEEE DATA COMPR CONF, P43, DOI 10.1109/DCC.2019.00012
   Zhang T, 2018, IEEE T MULTIMEDIA, V20, P1622, DOI 10.1109/TMM.2017.2775223
   Zhang Y, 2020, INFORM SCIENCES, V506, P395, DOI 10.1016/j.ins.2019.07.096
   Zheng AM, 2016, IEEE T CIRC SYST VID, V26, P2152, DOI 10.1109/TCSVT.2015.2501738
   Zhu LW, 2020, IEEE I C VI COM I PR, P164, DOI 10.1109/vcip49819.2020.9301752
   Zhu LW, 2021, IEEE T CIRC SYST VID, V31, P3168, DOI 10.1109/TCSVT.2020.3035356
   Zhu LW, 2020, IEEE T MULTIMEDIA, V22, P45, DOI 10.1109/TMM.2019.2924591
NR 52
TC 0
Z9 0
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2023
VL 19
IS 2
SU S
AR 96
DI 10.1145/3563699
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J8FS5
UT WOS:001011932300021
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Wu, H
   Li, X
   Wang, G
   Cheng, G
   Hu, XY
AF Wu, Hua
   Li, Xin
   Wang, Gang
   Cheng, Guang
   Hu, Xiaoyan
TI Resolution Identification of Encrypted Video Streaming Based on HTTP/2
   Features
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE DASH; resolution identification; encrypted video streaming; HTTP/2
ID REAL-TIME; QOE
AB With the inevitable dominance of video traffic on the Internet, Internet service providers (ISP) are striving to deliver video streaming with high quality. Video resolution, as a direct reflection of video quality, is a key factor of the video quality of experience (QoE). Since the displayed information of video cannot be observed by ISPs, ISPs can only measure the video resolution from traffic. However, with HTTP/2 being gradually adopted in video services, the multiplexing feature of HTTP/2 allows audio and video chunks to be mixed during transmission, making existing monitoring approaches unusable. In this article, we propose a method called H2CI to monitor resolution for adaptive encrypted video traffic under HTTP/2. We consider the size of the mixed data for identification. Specifically, H2CI consists of a length restoration method to extract restored fingerprints and a fingerprint-matching method for fine-grained resolution identification. The experimental results show that H2CI can achieve more than 98% accuracy for fine-grained resolution identification. Our method can be effectively applied to infer the adaptation behavior of encrypted video streaming and monitor the QoE of video services under HTTP/2.
C1 [Wu, Hua] Southeast Univ, Key Lab Comp Network & Informat Integrat, Purple Mt Labs Network & Commun Secur, Sch Cyber Sci & Engn,Minist Educ, 2 Southeast Univ Rd, Nanjing, Jiangsu, Peoples R China.
   [Li, Xin; Cheng, Guang; Hu, Xiaoyan] Southeast Univ, Sch Cyber Sci & Engn, 2 Southeast Univ Rd, Nanjing, Jiangsu, Peoples R China.
   [Wang, Gang] Southeast Univ, Sch Cyber Sci & Engn, 5 Zhuang Yuan Rd, Wuxi, Jiangsu, Peoples R China.
   [Cheng, Guang] Jiangsu Prov Engn Res Ctr Secur Ubiquitous Networ, 2 Southeast Univ Rd, Nanjing, Jiangsu, Peoples R China.
C3 Southeast University - China; Southeast University - China; Southeast
   University - China
RP Wu, H (corresponding author), Southeast Univ, Key Lab Comp Network & Informat Integrat, Purple Mt Labs Network & Commun Secur, Sch Cyber Sci & Engn,Minist Educ, 2 Southeast Univ Rd, Nanjing, Jiangsu, Peoples R China.
EM hwu@seu.edu.cn; xin_li@seu.edu.cn; 220215411@seu.edu.cn;
   gcheng@njnet.edu.cn; xyhu@njnet.edu.cn
FU National Key R&D Program of China [2021YFB3101403, 2020YFB1807503]
FX This work was supported by the National Key R&D Program of China
   (2021YFB3101403, 2020YFB1807503).
CR [Anonymous], 2020, Cisco Annual Internet Report (2018-2023) White Paper
   [Anonymous], 2015, RFC, DOI DOI 10.17487/RFC7540
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   Balachandran A, 2012, PROCEEDINGS OF THE 11TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS-XI), P97
   Ben Yahia M, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3280854
   Ben Yahia M, 2017, IEEE CONF COMPUT, P677, DOI 10.1109/INFCOMW.2017.8116458
   Bronzino F, 2019, P ACM MEAS ANAL COMP, V3, DOI 10.1145/3366704
   Cai X., 2012, P ACM CCS, DOI [10.1145/2382196.2382260, DOI 10.1145/2382196.2382260]
   Cao ZG, 2014, COMM COM INF SC, V490, P73
   Celenk Ozge, 2021, ACM SIGMETRICS Performance Evaluation Review, V48, P33, DOI 10.1145/3466826.3466839
   Dimopoulos G., 2016, P 2016 INT MEAS C SA, P513, DOI DOI 10.1145/2987443.2987459
   Ericsson A., 2020, ERICSSON MOBILITY RE
   Gu JX, 2018, IEEE INFOCOM SER, P1538, DOI 10.1109/INFOCOM.2018.8486211
   Gutterman C, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P48, DOI 10.1145/3304109.3306226
   Hossfeld Tobias, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P111, DOI 10.1109/QoMEX.2014.6982305
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Khokhar MJ, 2019, INT FED INFO PROC, DOI 10.23919/ifipnetworking.2019.8816854
   Mangla T, 2019, IEEE T NETW SERV MAN, V16, P1086, DOI 10.1109/TNSM.2019.2924942
   Mangla T, 2018, 2018 NETWORK TRAFFIC MEASUREMENT AND ANALYSIS CONFERENCE (TMA)
   Mazhar MH, 2018, IEEE INFOCOM SER, P1331, DOI 10.1109/INFOCOM.2018.8486321
   Mok R.K., 2011, PROC ACM SIGCOMM WOR, P31, DOI DOI 10.1145/2018602.2018611
   Nguyen M, 2020, P 25 PACKET VIDEO WO, P1, DOI [10.1145/3386292.3397117, DOI 10.1145/3386292.3397117]
   Orsolic I, 2018, INT WORK QUAL MULTIM, P300
   Orsolic I, 2016, IEEE GLOBE WORK
   Papadogiannaki E, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3457904
   Reed Andrew, 2016, 2016 13th IEEE Annual Consumer Communications & Networking Conference (CCNC), P1107, DOI 10.1109/CCNC.2016.7444944
   Reed A, 2017, PROCEEDINGS OF THE SEVENTH ACM CONFERENCE ON DATA AND APPLICATION SECURITY AND PRIVACY (CODASPY'17), P361, DOI 10.1145/3029806.3029821
   Rescorla E., 2018, RFC 8446, DOI [10.17487/RFC8446, DOI 10.17487/RFC8446]
   Schuster R, 2017, PROCEEDINGS OF THE 26TH USENIX SECURITY SYMPOSIUM (USENIX SECURITY '17), P1357
   Seufert M, 2019, IEEE CONF COMPUT, P688, DOI [10.1109/INFCOMW.2019.8845109, 10.1109/infcomw.2019.8845109]
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Shen M, 2020, 2020 IEEE/ACM 28TH INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS), DOI 10.1109/iwqos49365.2020.9212897
   Siekkinen M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3165279
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3336497
   Tran HTT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3423421
   Ul Mustafa R, 2021, 2021 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P586, DOI 10.1109/SSP49050.2021.9513804
   Wassermann S, 2020, IEEE T NETW SERV MAN, V17, P2007, DOI 10.1109/TNSM.2020.3036497
   Wassermann S, 2019, INTERNET-QOE'19: PROCEEDINGS OF THE 4TH INTERNET-QOE WORKSHOP: QOE-BASED ANALYSIS AND MANAGEMENT OF DATA COMMUNICATION NETWORKS, P1, DOI 10.1145/3349611.3355549
   Wassermann S, 2019, PROCEEDINGS OF THE 3RD NETWORK TRAFFIC MEASUREMENT AND ANALYSIS CONFERENCE (TMA 2019), P199, DOI 10.23919/TMA.2019.8784589
   Wu HJ, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533424
   Wu H, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET TECHNOLOGIES (CFI'19), DOI 10.1145/3341188.3341192
   Xu SC, 2020, PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS'20), DOI 10.1145/3342195.3387558
   Yang LM, 2020, 2020 16TH INTERNATIONAL CONFERENCE ON MOBILITY, SENSING AND NETWORKING (MSN 2020), P283, DOI 10.1109/MSN50589.2020.00055
   Yarnagula HK, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311749
NR 45
TC 9
Z9 11
U1 2
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2023
VL 19
IS 2
AR 73
DI 10.1145/3551891
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7DP3
UT WOS:001011190000023
DA 2024-07-18
ER

PT J
AU Chen, JW
   Pan, YW
   Li, YH
   Yao, T
   Chao, HY
   Mei, T
AF Chen, Jingwen
   Pan, Yingwei
   Li, Yehao
   Yao, Ting
   Chao, Hongyang
   Mei, Tao
TI Retrieval Augmented Convolutional Encoder-decoder Networks for Video
   Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video captioning; deep convolutional neural networks
ID IMAGE
AB Video captioning has been an emerging research topic in computer vision, which aims to generate a natural sentence to correctly reflect the visual content of a video. The well-established way of doing so is to rely on encoder-decoder paradigm by learning to encode the input video and decode the variable-length output sentence in a sequence-to-sequence manner. Nevertheless, these approaches often fail to produce complex and descriptive sentences as natural as those from human being, since the models are incapable of memorizing all visual contents and syntactic structures in the human-annotated video-sentence pairs. In this article, we uniquely introduce a Retrieval Augmentation Mechanism (RAM) that enables the explicit reference to existing video-sentence pairs within any encoder-decoder captioning model. Specifically, for each query video, a video-sentence retrieval model is first utilized to fetch semantically relevant sentences from the training sentence pool, coupled with the corresponding training videos. RAM then writes the relevant video-sentence pairs intomemory and reads the memorized visual contents/syntactic structures in video-sentence pairs from memory to facilitate the word prediction at each timestep. Furthermore, we present Retrieval Augmented Convolutional Encoder-Decoder Network (R-ConvED), which novelly integrates RAM into convolutional encoder-decoder structure to boost video captioning. Extensive experiments on MSVD, MSR-VTT, Activity Net Captions, and VATEX datasets validate the superiority of our proposals and demonstrate quantitatively compelling results.
C1 [Chen, Jingwen; Chao, Hongyang] Sun Yat Sen Univ, Guangzhou Higher Educ Mega Ctr, 132 Waihuan East Rd, Guangzhou 510006, Peoples R China.
   [Pan, Yingwei; Li, Yehao; Yao, Ting; Mei, Tao] JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
C3 Sun Yat Sen University
RP Yao, T (corresponding author), JD AI Res, 8 Beichen West St, Beijing 100105, Peoples R China.
EM chenjingwen.sysu@gmail.com; panyw.ustc@gmail.com;
   yehaoli.sysu@gmail.com; tingyao.ustc@gmail.com;
   isschhy@mail.sysu.edu.cn; tmei@jd.com
RI chen, jw/IQW-1558-2023; Pan, Yingwei/T-7649-2019
OI Pan, Yingwei/0000-0002-4344-8898; Yao, Ting/0000-0001-7587-101X
FU NSF of China [61672548, U1611461]
FX This work is partially supported by NSF of China under Grant 61672548,
   U1611461.
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   [Anonymous], 2016, P 24 ACM INT C MULTI, DOI DOI 10.1145/2964284.2984066
   [Anonymous], 2016, P INT JOINT C ARTIFI
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Awad G., 2016, TRECVID
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Ballas N., 2016, INT C LEARNING REPRE
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen JW, 2019, AAAI CONF ARTIF INTE, P8167
   Chen SZ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1838, DOI 10.1145/3123266.3123420
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chen YY, 2018, LECT NOTES COMPUT SC, V11217, P367, DOI 10.1007/978-3-030-01261-8_22
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Gao LL, 2022, IEEE T IMAGE PROCESS, V31, P202, DOI 10.1109/TIP.2021.3120867
   Gehring J, 2017, PR MACH LEARN RES, V70
   Gkountakos Konstantinos, 2019, ICEITMC, P1
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Gupta S, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3436494
   Hanckmann P, 2012, LECT NOTES COMPUT SC, V7583, P372, DOI 10.1007/978-3-642-33863-2_37
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Jang Y, 2019, INT J COMPUT VISION, V127, P1385, DOI 10.1007/s11263-019-01189-x
   Ji Wanting, 2021, ACM T MULTIM COMPUT, V17, p2s
   Jin T, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P630
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kingma D. P., 2015, INT C LEARNING REPRE
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li XP, 2019, WORLD WIDE WEB, V22, P621, DOI 10.1007/s11280-018-0531-z
   Li XL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2208
   Li YH, 2022, Arxiv, DOI [arXiv:2201.04026, DOI 10.48550/ARXIV.2201.04026]
   Li YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3799, DOI 10.1145/3474085.3478331
   Li YA, 2021, AAAI CONF ARTIF INTE, V35, P8518
   Li YH, 2019, PROC CVPR IEEE, P12489, DOI 10.1109/CVPR.2019.01278
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Li Yehao, 2022, IEEE T PATT ANAL MAC
   Lin KV, 2022, Arxiv, DOI arXiv:2111.13196
   Liu S, 2021, IEEE T PATTERN ANAL, V43, P3259, DOI 10.1109/TPAMI.2019.2940007
   Luo JJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5600, DOI 10.1145/3474085.3475703
   Mazaheri A, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176647
   Meng FD, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P20
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Mordan T, 2019, INT J COMPUT VISION, V127, P1659, DOI 10.1007/s11263-018-1109-z
   Nabati M, 2020, COMPUT VIS IMAGE UND, V190, DOI 10.1016/j.cviu.2019.102840
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Ordonez V, 2016, INT J COMPUT VISION, V119, P46, DOI 10.1007/s11263-015-0840-y
   Pan YW, 2020, Arxiv, DOI arXiv:2007.02375
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pasunuru R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1273, DOI 10.18653/v1/P17-1117
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514
   Sah S, 2020, PATTERN ANAL APPL, V23, P147, DOI 10.1007/s10044-018-00770-3
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Shi XX, 2020, NEUROCOMPUTING, V417, P347, DOI 10.1016/j.neucom.2020.08.035
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Song JK, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2737
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tang PJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303083
   Torabi Atousa, 2016, ABS160908124 CORR
   Tu YB, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107702
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2015, P ANN C N AM CHAPT A, P1494, DOI 10.3115/v1/N15-1173
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468
   Wu AM, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1029
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yao T, 2017, PROC CVPR IEEE, P5263, DOI 10.1109/CVPR.2017.559
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zhang ZQ, 2021, PROC CVPR IEEE, P9832, DOI 10.1109/CVPR46437.2021.00971
   Zhao B, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1177
   Zhao B, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2916757
   Zhao Wentian, 2021, IEEE T CIRC SYST VID, V34
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 98
TC 7
Z9 8
U1 2
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
AR 48
DI 10.1145/3539225
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800022
DA 2024-07-18
ER

PT J
AU Ferrari, C
   Becattini, F
   Galteri, L
   Del Bimbo, A
AF Ferrari, Claudio
   Becattini, Federico
   Galteri, Leonardo
   Del Bimbo, Alberto
TI (Compress and Restore)<SUP>N</SUP> : A Robust Defense Against
   Adversarial Attacks on Image Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adversarial attacks; image restoration
AB Modern image classification approaches often rely on deep neural networks, which have shown pronounced weakness to adversarial examples: images corrupted with specifically designed yet imperceptible noise that causes the network to misclassify. In this article, we propose a conceptually simple yet robust solution to tackle adversarial attacks on image classification. Our defense works by first applying a JPEG compression with a random quality factor; compression artifacts are subsequently removed by means of a generative model Artifact Restoration GAN. The process can be iterated ensuring the image is not degraded and hence the classification not compromised. We train different AR-GANs for different compression factors, so that we can change its parameters dynamically at each iteration depending on the current compression, making the gradient approximation difficult. We experiment with our defense against three white-box and two blackbox attacks, with a particular focus on the state-of-the-art BPDA attack. Our method does not require any adversarial training, and is independent of both the classifier and the attack. Experiments demonstrate that dynamically changing the AR-GAN parameters is of fundamental importance to obtain significant robustness.
C1 [Ferrari, Claudio] Univ Parma, Dept Architecture & Engn, Parco Area Sci 181-A, I-43124 Parma, Italy.
   [Becattini, Federico; Galteri, Leonardo; Del Bimbo, Alberto] Univ Florence, Dept Informat Engn, Via Santa Marta 3, I-50139 Florence, Italy.
C3 University of Parma; University of Florence
RP Ferrari, C (corresponding author), Univ Parma, Dept Architecture & Engn, Parco Area Sci 181-A, I-43124 Parma, Italy.
EM claudio.ferrari2@unipr.it; federico.becattini@unifi.it;
   leonardo.galteri@unifi.it; alberto.delbimbo@unifi.it
RI Galteri, Leonardo/HSI-0092-2023; Ferrari, Claudio/AEQ-4611-2022;
   Becattini, Federico/AAE-8554-2021
OI Galteri, Leonardo/0000-0002-7247-9407; Becattini,
   Federico/0000-0003-2537-2700
FU European Commission under European Horizon 2020 Programme [951911 -
   AI4Media]
FX This work was supported by the European Commission under European
   Horizon 2020 Programme, grant number 951911 - AI4Media.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Andriushchenko Maksym, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P484, DOI 10.1007/978-3-030-58592-1_29
   Athalye A, 2018, PR MACH LEARN RES, V80
   Athalye A, 2018, PR MACH LEARN RES, V80
   Brendel W., 2017, INT C LEARNING REPRE
   Carlini N, 2019, Arxiv, DOI arXiv:1902.06705
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Cavigelli L, 2017, IEEE IJCNN, P752, DOI 10.1109/IJCNN.2017.7965927
   Chen Pin-Yu, 2018, INT C LEARNING REPRE
   Das N, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P196, DOI 10.1145/3219819.3219910
   Das Nilaksh, 2017, arXiv
   Dhillon GS, 2018, ARXIV PREPRINT ARXIV
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong YP, 2020, PROC CVPR IEEE, P318, DOI 10.1109/CVPR42600.2020.00040
   Dong YP, 2019, PROC CVPR IEEE, P7706, DOI 10.1109/CVPR.2019.00790
   Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Galteri L, 2017, IEEE I CONF COMP VIS, P4836, DOI 10.1109/ICCV.2017.517
   Guo C., 2017, COUNTERING ADVERSARI
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Iandola F, 2014, Arxiv, DOI [arXiv:1404.1869, DOI 10.48550/ARXIV.1404.1869]
   Ilyas Andrew, 2018, P INT C MACH LEARN, V80
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Jia XJ, 2019, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2019.00624
   Dziugaite GK, 2016, Arxiv, DOI arXiv:1608.00853
   Kingma D. P., 2014, arXiv
   Kurakin A, 2017, Arxiv, DOI arXiv:1607.02533
   Li YD, 2019, PR MACH LEARN RES, V97
   Li YX, 2020, Arxiv, DOI arXiv:2005.06149
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Liu XQ, 2018, LECT NOTES COMPUT SC, V11211, P381, DOI 10.1007/978-3-030-01234-2_23
   Liu ZH, 2019, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2019.00095
   Madry A., 2018, ARXIV
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Mustafa A, 2020, IEEE T IMAGE PROCESS, V29, P1711, DOI 10.1109/TIP.2019.2940533
   Pang TY, 2019, PR MACH LEARN RES, V97
   Prakash A, 2018, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR.2018.00894
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Samangouei Pouya, 2018, INT C LEARNING REPRE
   Svoboda P, 2016, Arxiv, DOI arXiv:1605.00366
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tramer F., 2018, P INT C LEARN REPR
   Uesato J, 2018, PR MACH LEARN RES, V80
   Xiao C, 2020, PROC CVPR IEEE, P409, DOI 10.1109/CVPR42600.2020.00049
   Xie CY, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2809731
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Xu WL, 2017, Arxiv, DOI [arXiv:1704.01155, 10.48550/arXiv.1704.01155]
NR 50
TC 0
Z9 0
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
DI 10.1145/3524619
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800001
OA Green Published
DA 2024-07-18
ER

PT J
AU Huang, X
AF Huang, Xin
TI On Teaching Mode of MTI Translation Workshop Based on IPT Corpus for
   Tibetan Areas of China
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Translation technology; IPT Corpus; MTI translation teaching; workshop
AB With the technological turn of applied research in translation, increasing attention has been paid to the teaching of translation technology. This article addresses two important questions in this regard: how to independently develop Master of Translation and Interpreting (MTI) translation teaching resources with ethnic minority characteristics and how to use information technology to carry out Tibet-related computer-assisted translation (CAT) teaching. This article discusses the background, structure, and functions of the International Publicity Translation Corpus (IPT Corpus) for Tibetan Areas of China through empirical research, combining theory with practice, and validates the translation teaching mode through case study to better train translators and interpreters working on content related to Tibetan culture. Through teaching practice since 2017, the MTI translation workshop based on the IPT Corpus has proven to be an effective teaching mode that is worthy of further improvement and extension.
C1 [Huang, Xin] Southwest Jiaotong Univ, Sch Foreign Languages, Chengdu 611756, Sichuan, Peoples R China.
C3 Southwest Jiaotong University
RP Huang, X (corresponding author), Southwest Jiaotong Univ, Sch Foreign Languages, Chengdu 611756, Sichuan, Peoples R China.
EM huangxin2009@foxmail.com
RI Huang, Xin/GQQ-4069-2022
OI Huang, Xin/0000-0002-6845-8240
FU National Social Science Fund of China [17XYY018]
FX This research was financially supported by the National Social Science
   Fund of China (grant no. 17XYY018).
CR [Anonymous], 2023, ACM T MULTIM COMPUT, V19
   Chai Mingjiong, 2012, J U SHANGHAI SCI TEC, V34, P91, DOI [10.13256/j.cnki.jusst.sse.2012.02.005, DOI 10.13256/J.CNKI.JUSST.SSE.2012.02.005]
   Cronin M., 2010, Tradumatica, V8, P1, DOI [10.5565/rev/tradumatica.100, DOI 10.5565/REV/TRADUMATICA.100]
   Doherty S, 2016, INT J COMMUN-US, V10, P947
   Fang M., 2013, APPL TRANSLATOLOGY
   Geng Weiming, 2017, NATL IND METROLOGY T
   Gentzler Edwin., 1993, CONT TRANSLATION THE
   Huang Xin, 2016, MINORITY TRANSLATORS, V9, P43, DOI [10.13742/j.cnki.cn11-5684/h.2016.02.007, DOI 10.13742/J.CNKI.CN11-5684/H.2016.02.007]
   Huang Xin, 2018, J SICHUAN MINZU COLL, V27, P88, DOI [10.13934/j.cnki.cn51-1729/g4.2018.05.015, DOI 10.13934/J.CNKI.CN51-1729/G4.2018.05.015]
   Huang Xin, 2012, J SICHUAN MINZU COLL, V21, P88, DOI [10.13934/j.cnki.cn51-1729/g4.2012.05.003, DOI 10.13934/J.CNKI.CN51-1729/G4.2012.05.003]
   Huang Xin, 2018, TIBETAN STUDIES, V38, P152
   Huang Youyi, 2010, CURR CONTENTS, V31, P49
   Huang Youyi, 2019, CURR CONTENTS, V40, P1
   Huang Youyi, 2007, CHINESE TRANSLATORS, V28, P8
   Li Baowei, 2016, J XIZANG MINZU U, V37, P141
   Li Ming, 2010, CHINESE TRANSLATORS, V31, P32
   Liu Jiangwei, 2019, J CHINA 3 GORGES U H, V41, P108, DOI [10.13393/j.cnki.1672-6219.2019.03.022, DOI 10.13393/J.CNKI.1672-6219.2019.03.022]
   Luo Qiong, 2017, TRANSLATION TEACHING
   Mu Lei, 2007, CHINESE TRANSLATORS, V28, P12
   Mu Lei, 2018, CURR CONTENTS, V33, P56
   Pochhacker Franz., 2008, Efforts and Models in Interpreting and Translation Research; a tribute to Daniel Gile, P25
   Shao Lu, 2013, FOREIGN LANGUAGE ED, V1, P39, DOI [10.16739/j.cnki.cn21-9203/g4.2013.01.025, DOI 10.16739/J.CNKI.CN21-9203/G4.2013.01.025]
   Sheng Guorong, 2016, TRANSLATION PRACTICE
   TAC, 2019, REP DEV CHIN LANG SE
   Tao Y., 2015, FLLTP, V4, P87
   Wang Chengshuang, 2016, TECHNOLOGY ENHANCED, V38, P80
   Wang HS., 2018, Technology Enhanced Foreign Language Education, V3, P76
   Wang J., 2017, TECHNOLOGY ENHANCED, P25
   Wang Luming, 2015, CURR CONTENTS, V36, P131
   Wen Jun, 2009, FOREIGN LANGUAGE ED, V30, P92, DOI [10.16362/j.cnki.cn61-1023/h.2009.04.014, DOI 10.16362/J.CNKI.CN61-1023/H.2009.04.014]
   Zhang Chengzhi, 2016, TRANSLATION HORIZONS, V1, P104
   Zhang Xiaojun, 2014, CHINESE TRANSLATORS, V35, P74
   Zhang Xudong, 2016, CURR CONTENTS, V33, P88, DOI [10.13978/j.cnki.wyyj.2016.05.016, DOI 10.13978/J.CNKI.WYYJ.2016.05.016]
   Zhong W., 2007, CHINESE TRANSLATORS, V28, P9
   Zhong Weihe, 2017, CHINESE TRANSLATORS, V38, P7
   Zhu Chaowei, 2015, FOREIGN LANGUAGE WOR, V36, P61
NR 36
TC 0
Z9 0
U1 5
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2023
VL 19
IS 1
SU S
DI 10.1145/3527173
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8Q4HK
UT WOS:000927169800006
DA 2024-07-18
ER

PT J
AU Dai, HB
   Shi, HL
   Liu, W
   Wang, LF
   Liu, YL
   Mei, T
AF Dai, Hanbin
   Shi, Hailin
   Liu, Wu
   Wang, Linfang
   Liu, Yinglu
   Mei, Tao
TI FasterPose: A Faster Simple Baseline for Human Pose Estimation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Pose estimation; keypoint detection
ID NETWORK
AB The performance of human pose estimation depends on the spatial accuracy of keypoint localization. Most existing methods pursue the spatial accuracy through learning the high-resolution (HR) representation from input images. By the experimental analysis, we find that the HR representation leads to a sharp increase of computational cost, while the accuracy improvement remains marginal compared with the low-resolution (LR) representation. In this article, we propose a design paradigm for cost-effective network with LR representation for efficient pose estimation, named FasterPose. Whereas the LR design largely shrinks the model complexity, how to effectively train the network with respect to the spatial accuracy is a concomitant challenge. We study the training behavior of FasterPose and formulate a novel regressive cross-entropy (RCE) loss function for accelerating the convergence and promoting the accuracy. The RCE loss generalizes the ordinary cross-entropy loss from the binary supervision to a continuous range, thus the training of pose estimation network is able to benefit from the sigmoid function. By doing so, the output heatmap can be inferred from the LR features without loss of spatial accuracy, while the computational cost and model size has been significantly reduced. Compared with the previously dominant network of pose estimation, our method reduces 58% of the FLOPs and simultaneously gains 1.3% improvement of accuracy. Extensive experiments show that FasterPose yields promising results on the common benchmarks, i.e., COCO and MPII, consistently validating the effectiveness and efficiency for practical utilization, especially the low-latency and low-energy-budget applications in the non-GPU scenarios.
C1 [Dai, Hanbin; Shi, Hailin; Liu, Wu; Wang, Linfang; Liu, Yinglu; Mei, Tao] JD AI Res, 18 Kechuang 11Th St, Beijing, Peoples R China.
RP Shi, HL (corresponding author), JD AI Res, 18 Kechuang 11Th St, Beijing, Peoples R China.
EM daihanbin3@jd.com; shihailin@jd.com; liuwu1@jd.com; wanglinfang@jd.com;
   liuyinglu1@jd.com; tmei@live.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
FU National Key R&D Program of China [2020AAA0103800]
FX This work was supported by the National Key R&D Program of China under
   Grant No. 2020AAA0103800.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Cai Yuanhao, 2020, COMPUTER VISION ECCV
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chéron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368
   Cho NG, 2013, PATTERN RECOGN, V46, P649, DOI 10.1016/j.patcog.2012.09.006
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu PY, 2016, PROC CVPR IEEE, P5600, DOI 10.1109/CVPR.2016.604
   Huang JJ, 2020, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR42600.2020.00574
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Ke LP, 2018, LECT NOTES COMPUT SC, V11206, P731, DOI 10.1007/978-3-030-01216-8_44
   Lifshitz I, 2016, LECT NOTES COMPUT SC, V9906, P246, DOI 10.1007/978-3-319-46475-6_16
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Moon G, 2019, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2019.00796
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12
   Tompson J, 2014, ADV NEUR IN, V27
   Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123
   Wang Zhicheng, 2018, P JOINT REC CHALL EU, V5
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Yu CQ, 2021, PROC CVPR IEEE, P10435, DOI 10.1109/CVPR46437.2021.01030
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang Z., 2019, SIMPLE LIGHTWEIGHT H
NR 37
TC 9
Z9 9
U1 6
U2 42
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 103
DI 10.1145/3503464
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, YB
   Guo, YY
   Yin, JH
   Song, XM
   Liu, WF
   Nie, LQ
   Zhang, M
AF Liu, Yibing
   Guo, Yangyang
   Yin, Jianhua
   Song, Xuemeng
   Liu, Weifeng
   Nie, Liqiang
   Zhang, Min
TI Answer Questions with Right Image Regions: A Visual Attention
   Regularization Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual question answering; mask-guided learning; visual attention
   regularization
ID NETWORKS
AB Visual attention in Visual Question Answering (VQA) targets at locating the right image regions regarding the answer prediction, offering a powerful technique to promote multi-modal understanding. However, recent studies have pointed out that the highlighted image regions from the visual attention are often irrelevant to the given question and answer, leading to model confusion for correct visual reasoning. To tackle this problem, existing methods mostly resort to aligning the visual attention weights with human attentions. Nevertheless, gathering such human data is laborious and expensive, making it burdensome to adaptwell-developed models across datasets. To address this issue, in this article, we devise a novel visual attention regularization approach, namely, AttReg, for better visual grounding in VQA. Specifically, AttReg first identifies the image regions that are essential for question answering yet unexpectedly ignored (i.e., assigned with low attention weights) by the backbone model. And then a mask-guided learning scheme is leveraged to regularize the visual attention to focus more on these ignored key regions. The proposed method is very flexible and model-agnostic, which can be integrated into most visual attention-based VQA models and require no human attention supervision. Extensive experiments over three benchmark datasets, i.e., VQA-CP v2, VQA-CP v1, and VQA v2, have been conducted to evaluate the effectiveness of AttReg. As a by-product, when incorporating AttReg into the strong baseline LMH, our approach can achieve a new state-of-the-art accuracy of 60.00% with an absolute performance gain of 7.01% on the VQA-CP v2 benchmark dataset. In addition to the effectiveness validation, we recognize that the faithfulness of the visual attention in VQA has not been well explored in literature. In the light of this, we propose to empirically validate such property of visual attention and compare it with the prevalent gradient-based approaches.
C1 [Liu, Yibing; Guo, Yangyang; Yin, Jianhua; Song, Xuemeng; Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, 72 Binhai Rd, Qingdao 266237, Shandong, Peoples R China.
   [Liu, Weifeng] China Univ Petr East China, Coll Control Sci & Engn, 66 West Changjiang Rd, Qingdao 266580, Shandong, Peoples R China.
   [Zhang, Min] Harbin Inst Technol Shenzhen, Sch Comp Sci & Technol, Shenzhen 518055, Guangdong, Peoples R China.
C3 Shandong University; China University of Petroleum; Harbin Institute of
   Technology
RP Nie, LQ (corresponding author), Shandong Univ, Sch Comp Sci & Technol, 72 Binhai Rd, Qingdao 266237, Shandong, Peoples R China.
EM lyibing112@gmail.com; guoyang.eric@gmail.com; jhyin@sdu.edu.cn;
   sxmustc@gmail.com; liuwf@upc.edu.cn; nieliqiang@gmail.com;
   zhangminmt@hotmail.com
RI liu, weifeng/B-7909-2008; Yin, Jianhua/HMD-6684-2023
OI Yin, Jianhua/0000-0002-4611-2986; Liu, Yibing/0000-0002-2862-5542
FU Shandong Provincial Natural Science Foundation [ZR2019JQ23]
FX This work is supported by the Shandong Provincial Natural Science
   Foundation, No. ZR2019JQ23.
CR Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Cadene R., 2019, P INT C NEUR INF PRO, P839
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Chen Long, 2020, P IEEE CVF C COMP VI
   Cho K., 2014, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)
   Clark C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4069
   Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001
   Fan HH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3390891
   Fang ZW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282469
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gat I., 2020, ADV NEURAL INFORM PR, V33, P3197
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guo YY, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P75, DOI 10.1145/3331184.3331186
   Guo Yangyang, 2021, P INT JOINT C ART IN, P708, DOI DOI 10.1109/MWSCAS47672.2021.9531788
   Guo Y, 2020, IEEE SIGNAL PROC LET, V27, P1255, DOI 10.1109/LSP.2020.3003517
   Honnibal Matthew, 2020, **DATA OBJECT**, DOI [10.5281/zenodo.1212303, DOI 10.5281/ZENODO.1212303]
   Jain S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P3543
   Jing CC, 2020, AAAI CONF ARTIF INTE, V34, P11181
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li Q, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3300938
   Li XR, 2019, IEEE T MULTIMEDIA, V21, P2347, DOI 10.1109/TMM.2019.2896494
   Liang JW, 2019, IEEE T PATTERN ANAL, V41, P1893, DOI 10.1109/TPAMI.2018.2890628
   Liang ZJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P3285
   Liu F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1175, DOI 10.1145/3343031.3350993
   Lu JS, 2016, ADV NEUR IN, V29
   Malinowski M, 2018, LECT NOTES COMPUT SC, V11210, P3, DOI 10.1007/978-3-030-01231-1_1
   Patro BN, 2020, AAAI CONF ARTIF INTE, V34, P11848
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Qiao TT, 2018, AAAI CONF ARTIF INTE, P7300
   Ramakrishnan S., 2018, INT C NEURAL INF PRO, V31, P1541
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ross AS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2662
   Selvaraju RR, 2019, IEEE I CONF COMP VIS, P2591, DOI 10.1109/ICCV.2019.00268
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Shrestha Robik, 2020, P 58 ANN M ASS COMPU, P8172
   Teney D., 2020, INT C NEURAL INF PRO, V33, P407
   Wu JL, 2019, ADV NEUR IN, V32
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Yang Hui, 2003, P 11 ACM INT C MULT, P632, DOI DOI 10.1145/957013.957146
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu DF, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3316767
   Zhang P, 2016, PROC CVPR IEEE, P5014, DOI 10.1109/CVPR.2016.542
   Zhang YD, 2019, IEEE WINT CONF APPL, P349, DOI 10.1109/WACV.2019.00043
   Zhao Z, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1050, DOI 10.1145/3123266.3123364
   Zhi X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1083
   Zhou LW, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P305, DOI 10.1145/3126686.3126717
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 53
TC 12
Z9 13
U1 0
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2022
VL 18
IS 4
AR 93
DI 10.1145/3498340
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0E1KA
UT WOS:000776441600004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cornia, M
   Tomei, M
   Baraldi, L
   Cucchiara, R
AF Cornia, Marcella
   Tomei, Matteo
   Baraldi, Lorenzo
   Cucchiara, Rita
TI Matching Faces and Attributes Between the Artistic and the Real Domain:
   the PersonArt Approach
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face similarity; face retrieval; face recognition; cultural heritage
AB In this article, we present an approach for retrieving similar faces between the artistic and the real domain. The application we refer to is an interactive exhibition inside a museum, in which a visitor can take a photo of himself and search for a lookalike in the collection of paintings. The task requires not only to identify faces but also to extract discriminative features from artistic and photo-realistic images, tackling a significant domain shift. Our method integrates feature extraction networks which account for the aesthetic similarity of two faces and their correspondences in terms of semantic attributes. Also, it addresses the domain shift between realistic images and paintings by translating photo-realistic images into the artistic domain. Noticeably, by exploiting the same technique, our model does not need to rely on annotated data in the artistic domain. Experimental results are conducted on different paired datasets to show the effectiveness of the proposed solution in terms of identity and attribute preservation. The approach is also evaluated on unpaired settings and in combination with an interactive relevance feedback strategy. Finally, we show how the proposed algorithm has been implemented in a real showcase at the Gallerie Estensi museum in Italy, with the participation of more than 1,100 visitors in just three days.
C1 [Cornia, Marcella; Tomei, Matteo; Baraldi, Lorenzo; Cucchiara, Rita] Univ Modena & Reggio Emilia, I-41121 Modena, MO, Italy.
C3 Universita di Modena e Reggio Emilia
RP Cornia, M (corresponding author), Univ Modena & Reggio Emilia, I-41121 Modena, MO, Italy.
EM marcella.cornia@unimore.it; matteo.tomei@unimore.it;
   lorenzo.baraldi@unimore.it; rita.cucchiara@unimore.it
RI Cornia, Marcella/Y-9903-2019; Cucchiara, Rita/L-3006-2015
OI Cornia, Marcella/0000-0001-9640-9385; Baraldi,
   Lorenzo/0000-0001-5125-4957; Cucchiara, Rita/0000-0002-2239-283X
FU "Fondazione di Modena" under the project "AI for Digital Humanities";
   national project "IDEHA: Innovation for Data Elaboration in Heritage
   Areas" - Italian Ministry of University and Research [PON ARS01_00421]
FX This work has been supported by "Fondazione di Modena" under the project
   "AI for Digital Humanities" and by the national project "IDEHA:
   Innovation for Data Elaboration in Heritage Areas" (PON ARS01_00421),
   cofunded by the Italian Ministry of University and Research. We also
   gratefully acknowledge "Ago - Modena Fabbriche Culturali", the
   Municipality of Modena, and Gallerie Estensi, in particular Dir. Martina
   Bagnoli and Arch. Silvia Gaiba for the design and creation of the
   PersonArt photo booth and for supporting and promoting the initiative.
CR Abaci B, 2015, SIGNAL IMAGE VIDEO P, V9, P295, DOI 10.1007/s11760-015-0819-8
   Anjomshoae S, 2019, AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P1078
   [Anonymous], 2016, PROC 4 INT C LEARN R
   [Anonymous], 2014, WORKSHOP INT C LEARN
   Anoosheh A, 2018, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2018.00122
   Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591009
   Baraldi L, 2018, INT C PATT RECOG, P1097, DOI 10.1109/ICPR.2018.8545064
   Bhatt HS, 2012, IEEE T INF FOREN SEC, V7, P1522, DOI 10.1109/TIFS.2012.2204252
   Bongini P, 2020, IOP CONF SER-MAT SCI, V949, DOI 10.1088/1757-899X/949/1/012074
   Borghesani D, 2014, MULTIMEDIA SYST, V20, P65, DOI 10.1007/s00530-013-0315-3
   Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413
   Brown Timothy, 2014, P EUR C COMP VIS WOR
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Carraggi A., 2018, P EUR C COMP VIS WOR, P1
   Cascianelli Silvia, P INT C COMP AN IM P
   Castellano G, 2021, NEURAL COMPUT APPL, V33, P12263, DOI 10.1007/s00521-021-05893-z
   Castellano G, 2021, MULTIMED TOOLS APPL, V80, P6599, DOI 10.1007/s11042-020-09995-z
   Chang YJ, 2009, IEEE IMAGE PROC, P1849, DOI 10.1109/ICIP.2009.5413585
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cornia M, 2020, PATTERN RECOGN LETT, V129, P166, DOI 10.1016/j.patrec.2019.11.018
   Crowley E., 2014, BRIT MACH VIS C BMVC
   Crowley Elliot J, 2016, P EUR C COMP VIS
   Crowley Elliot J, 2015, P BRIT MACH VIS C
   Del Chiaro R, 2019, PATTERN RECOGN LETT, V128, P420, DOI 10.1016/j.patrec.2019.09.027
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Ghiasi G., 2017, ARXIV PREPRINT ARXIV
   Gunther Manuel., 2017, P INT JOINT C BIOM
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Hand EM, 2017, AAAI CONF ARTIF INTE, P4068
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huo J, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P340, DOI 10.1145/3126686.3126736
   Huo Jing., 2018, P BRIT MACH VIS C
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jing YC, 2018, LECT NOTES COMPUT SC, V11217, P244, DOI 10.1007/978-3-030-01261-8_15
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karayev S., 2014, P BRIT MACH VIS C, P1, DOI [DOI 10.5244/C.28.122, 10.5244/c.28.122, 10.5244%2Fc.28.122, 10.5244/C.28.122]
   Kingma D. P., 2014, arXiv
   Klare Brendan F., 2012, P INT C BIOM
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li S, 2018, 2018 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATION (ECOC)
   Li YJ, 2017, ADV NEUR IN, V30
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Liu MY, 2017, ADV NEUR IN, V30
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu WY, 2016, PR MACH LEARN RES, V48
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126
   Ma S, 2018, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2018.00593
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Mao H, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1183, DOI 10.1145/3123266.3123405
   Masi I, 2018, SIBGRAPI, P471, DOI 10.1109/SIBGRAPI.2018.00067
   Mishra Ashutosh, 2016, P EUR C COMP VIS WOR
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Picard D, 2015, IEEE SIGNAL PROC MAG, V32, P95, DOI 10.1109/MSP.2015.2409557
   Reed S, 2016, ADV NEUR IN, V29
   Reed S, 2016, PR MACH LEARN RES, V48
   Rudd EM, 2016, LECT NOTES COMPUT SC, V9909, P19, DOI 10.1007/978-3-319-46454-1_2
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Sankaranarayanan S, 2016, 2016 IEEE 8 INT C BI, DOI 10.1109/btas.2016.7791205
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841
   Shen Xi, 2019, P IEEE CVF C COMP VI
   Simonyan K, 2015, IEEE INT C ICLR
   Springenberg Jost Tobias, 2015, Striving for simplicity: The all convolutional net, DOI DOI 10.48550/ARXIV.1412.6806
   Stefanini M, 2019, LECT NOTES COMPUT SC, V11752, P729, DOI 10.1007/978-3-030-30645-8_66
   Strezoski Gjorgji., 2017, ACM T MULTIM COMPUT, V14
   Sun Y., 2015, ADV DIFFER EQU-NY, DOI DOI 10.1186/S13662-015-0433-7
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sundararajan M, 2017, PR MACH LEARN RES, V70
   Taigman Y., 2017, INT C LEARN REPR ICL, P1
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tomei M, 2019, LECT NOTES COMPUT SC, V11752, P741, DOI 10.1007/978-3-030-30645-8_67
   Tomei M, 2019, PROC CVPR IEEE, P5842, DOI 10.1109/CVPR.2019.00600
   Tomei Matteo., 2018, P EUR C COMP VIS WOR
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361
   Wang F, 2018, LECT NOTES COMPUT SC, V11213, P780, DOI 10.1007/978-3-030-01240-3_47
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang M, 2021, NEUROCOMPUTING, V429, P215, DOI 10.1016/j.neucom.2020.10.081
   Westlake Nicholas, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P825, DOI 10.1007/978-3-319-46604-0_57
   Wilber MJ, 2017, IEEE I CONF COMP VIS, P1211, DOI 10.1109/ICCV.2017.136
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Yang XW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P374, DOI 10.1145/3240508.3240716
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30
   Zheng X, 2020, INT J COMPUT VISION, V128, P2002, DOI 10.1007/s11263-020-01308-z
   Zhong Y, 2016, INT C PATT RECOG, P2264, DOI 10.1109/ICPR.2016.7899973
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 103
TC 0
Z9 0
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2022
VL 18
IS 3
AR 77
DI 10.1145/3490033
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5UA
UT WOS:000772650600011
DA 2024-07-18
ER

PT J
AU Li, KP
   Liu, C
   Stopa, MK
   Amano, J
   Fu, Y
AF Li, Kunpeng
   Liu, Chang
   Stopa, Mike
   Amano, Jun
   Fu, Yun
TI Guided Graph Attention Learning for Video-Text Matching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modal retrieval; video-text embedding; graph neural networks;
   multimedia applications
AB As a bridge between videos and natural languages, video-text matching has been a hot multimedia research topic in recent years. Such cross-modal retrieval is usually achieved by learning a common embedding space where videos and text captions are directly comparable. It is still challenging because existing visual representations cannot exploit semantic correlations within videos well, resulting in a mismatch with semantic concepts that are contained in the corresponding text descriptions. In this article, we propose a new Guided Graph Attention Learning (GGAL) model to enhance video embedding learning by capturing important region-level semantic concepts within the spatiotemporal space. Our model builds connections between object regions and performs hierarchical graph reasoning on both frame-level and whole video-level region graphs. During this process, global context is used to guide attention learning on this hierarchical graph topology so that the learned overall video embedding can focus on essential semantic concepts and can be better aligned with text captions. Experiments on commonly used benchmarks validate that GGAL outperforms many recent video-text retrieval methods with a clear margin. As multimedia data in dynamic environments becomes critically important, we also validate GGAL learned video-text representations that can be generalized well to unseen out-of-domain data via cross-dataset evaluations. To further investigate the interpretability of our model, we visualize attention weights learned by GGAL models. We find that GGAL successfully focuses on key semantic concepts in the video and has complementary attention on the context parts based on different ways of building region graphs.
C1 [Li, Kunpeng; Liu, Chang; Fu, Yun] Northeastern Univ, 360 Huntington Ave, Boston, MA 02145 USA.
   [Stopa, Mike; Amano, Jun] Konica Minolta, 2855 Campus Dr,Suite 100, San Mateo, CA 94403 USA.
C3 Northeastern University
RP Li, KP (corresponding author), Northeastern Univ, 360 Huntington Ave, Boston, MA 02145 USA.
EM lcinpeng.h.1994@gmail.com; liu.chang6@husky.neu.edu;
   mstopa@kmbs.konicaminolta.us; jamano@kmbs.konicaminolta.us;
   yunfu@ece.neu.edu
RI Li, Kunpeng/AAH-1164-2019; Li, Kunpeng/KFS-6306-2024
OI Li, Kunpeng/0000-0001-5805-793X; Liu, Chang/0000-0002-0219-4748; Fu,
   Yun/0000-0002-5098-2853
CR Abu-El-Haija Sami, 2018, ADV NEURAL INFORM PR, V2018, P9198
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2018, ICLR 18
   [Anonymous], 2016, arXiv preprint arXiv:1610.02947
   Bertasius G, 2017, PROC CVPR IEEE, P6137, DOI 10.1109/CVPR.2017.650
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chandra S, 2017, IEEE I CONF COMP VIS, P5113, DOI 10.1109/ICCV.2017.546
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen FY, 2021, IEEE T MULTIMEDIA, V23, P3073, DOI 10.1109/TMM.2020.3019710
   Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Chen QC, 2021, AAAI CONF ARTIF INTE, V35, P1072
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Choo S., 2021, 2021 IEEE INT C IM P, P2388
   Chung J., 2014, NIPS 2014 WORKSH DEE, Vabs/1412.3555, P1, DOI DOI 10.48550/ARXIV.1412.3555
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong JF, 2022, IEEE T PATTERN ANAL, V44, P4065, DOI 10.1109/TPAMI.2021.3059295
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Feng ZR, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1005
   Francis D., 2019, P IEEE CVF INT C COM, P1868
   Gabeur Valentin, 2020, P EUR C COMP VIS, V12349, P214, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gao ZJ, 2022, Arxiv, DOI arXiv:2111.05610
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   HARNAD S, 1990, PHYSICA D, V42, P335, DOI 10.1016/0167-2789(90)90087-6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HOBBS JR, 1993, ARTIF INTELL, V63, P69, DOI 10.1016/0004-3702(93)90015-4
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ji RY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3432861
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Katsuki F, 2014, NEUROSCIENTIST, V20, P509, DOI 10.1177/1073858413514136
   Kaufman D, 2017, IEEE I CONF COMP VIS, P94, DOI 10.1109/ICCV.2017.20
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lao N, 2011, P C EMP METH NAT LAN, P529
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Li KW, 2022, INT J AEROSPACE ENG, V2022, DOI 10.1155/2022/5353681
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li KP, 2020, IEEE T PATTERN ANAL, V42, P2996, DOI 10.1109/TPAMI.2019.2921543
   Li Kunpeng, 2020, P IEEECVF C COMPUTER, P12526
   Liu Y., 2019, P BRIT MACH VIS C, P279
   Luo HS, 2021, Arxiv, DOI arXiv:2104.08860
   Luo HS, 2020, Arxiv, DOI arXiv:2002.06353
   Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   NEWELL A, 1980, COGNITIVE SCI, V4, P135, DOI 10.1016/S0364-0213(80)80015-2
   Norcliffe-Brown Will, 2018, ADV NEURAL INFORM PR, V31, P1
   Portillo-Quintero JA, 2021, LECT NOTES COMPUT SC, V12725, P3, DOI 10.1007/978-3-030-77004-4_1
   Qi MS, 2021, IEEE T IMAGE PROCESS, V30, P2989, DOI 10.1109/TIP.2020.3048680
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schwartz I, 2019, PROC CVPR IEEE, P2039, DOI 10.1109/CVPR.2019.00214
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10635, DOI 10.1109/CVPR42600.2020.01065
   Simonyan Karen, 2014, WORKSH P INT C LEARN
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Torabi Atousa, 2016, ARXIV
   Vaswani A, 2017, ADV NEUR IN, V30
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang X., 2020, IEEE transactions on pattern analysis and machine intelligence
   Wang X, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2022, DOI 10.1145/3308558.3313562
   Wang XH, 2021, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR46437.2021.00504
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wei Jiwei, 2020, P IEEE C COMP VIS PA, P13005
   Wray M, 2019, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2019.00054
   Wu Aming, 2019, ADV NEURAL INFORM PR, V32, P1
   Wu P, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3518, DOI 10.1145/3474085.3475515
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang JW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11542, DOI 10.1109/ICCV48922.2021.01136
   Yang W., 2019, 7 INT C LEARN REPR I
   Yang X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1339, DOI 10.1145/3397271.3401151
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yu Weijiang, 2019, ADV NEURAL INFORM PR, V32, P1
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang J., 2016, INT J COMPUT VISION, P1084
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao Rui, 2021, ARXIV
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou W., 2022, ACM T MULTIM COMPUT, V19, P1
   Zhu L., 2020, P IEEE CVF C COMP VI, DOI 10.1109/CVPR42600.2020.00877
NR 94
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2022
VL 18
IS 2
SU S
AR 131
DI 10.1145/3538533
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7R6FY
UT WOS:000910168000019
OA Bronze
DA 2024-07-18
ER

PT J
AU Fincato, M
   Cornia, M
   Landi, F
   Cesari, F
   Cucchiara, R
AF Fincato, Matteo
   Cornia, Marcella
   Landi, Federico
   Cesari, Fabio
   Cucchiara, Rita
TI Transform, Warp, and Dress: A New Transformation-guided Model for
   Virtual Try-on
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Virtual try-on; geometric transformations; generative adversarial
   networks
AB Virtual try-on has recently emerged in computer vision and multimedia communities with the development of architectures that can generate realistic images of a target person wearing a custom garment. This research interest is motivated by the large role played by e-commerce and online shopping in our society. Indeed, the virtual try-on task can offer many opportunities to improve the efficiency of preparing fashion catalogs and to enhance the online user experience. The problem is far to be solved: current architectures do not reach sufficient accuracy with respect to manually generated images and can only be trained on image pairs with a limited variety. Existing virtual try-on datasets have two main limits: they contain only female models, and all the images are available only in low resolution. This not only affects the generalization capabilities of the trained architectures but makes the deployment to real applications impractical. To overcome these issues,we present Dress Code, a new dataset for virtual try-on that contains high-resolution images of a large variety of upper-body clothes and both male and female models. Leveraging this enriched dataset, we propose a new model for virtual try-on capable of generating high-quality and photo-realistic images using a three-stage pipeline. The first two stages perform two different geometric transformations to warp the desired garment and make it fit into the target person's body pose and shape. Then, we generate the new image of that same person wearing the try-on garment using a generative network. We test the proposed solution on the most widely used dataset for this task as well as on our newly collected dataset and demonstrate its effectiveness when compared to current state-of-the-art methods. Through extensive analyses on our Dress Code dataset,we show the adaptability of our model, which can generate try-on images even with a higher resolution.
C1 [Fincato, Matteo; Cornia, Marcella; Landi, Federico; Cucchiara, Rita] Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, Via P Vivarelli 10, I-41125 Modena, Italy.
   [Cesari, Fabio] YOOX NET A PORTER GRP, Via Nerio Nannetti 1, I-40069 Bologna, Italy.
C3 Universita di Modena e Reggio Emilia
RP Fincato, M (corresponding author), Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, Via P Vivarelli 10, I-41125 Modena, Italy.
EM matteo.fincato@unimore.it; marcella.cornia@unimore.it;
   federico.landi@unimore.it; fabio.cesari@ynap.com;
   rita.cucchiara@unimore.it
RI Cornia, Marcella/Y-9903-2019; Cucchiara, Rita/L-3006-2015
OI Cornia, Marcella/0000-0001-9640-9385; Landi,
   Federico/0000-0003-2092-1934; Cucchiara, Rita/0000-0002-2239-283X
FU Emilia Romagna region [POR FESR 2014-2020CUP E81F18000330007]
FX This work has been partially supported by the "SUPER-Supercomputing
   Unified Platform" project (POR FESR 2014-2020CUP E81F18000330007),
   co-funded by Emilia Romagna region.
CR [Anonymous], 2015, 3 INT C LEARN REPR
   Ayush Kumar, 2019, P ICCV WORKSH
   Barratt Shane, 2018, P ICML WORKSH
   Bertiche H., 2020, P ECCV
   Binkowski Mikolaj, 2018, INT C LEARNING REPRE
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Cucurull G, 2019, PROC CVPR IEEE, P12609, DOI 10.1109/CVPR.2019.01290
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Dong HY, 2019, IEEE I CONF COMP VIS, P1161, DOI 10.1109/ICCV.2019.00125
   Dong Haoye, 2018, P NEURIPS
   Dong Xue, 2020, P ACM SIGIR
   Fincato Matteo, 2020, P ICPR
   Ge YY, 2019, PROC CVPR IEEE, P5332, DOI 10.1109/CVPR.2019.00548
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Hahn Fabian, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601160
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Hensel M, 2017, ADV NEUR IN, V30
   Hsiao WL, 2018, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR.2018.00748
   Hsieh CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P275, DOI 10.1145/3343031.3351075
   Hsieh CW, 2019, IEEE IMAGE PROC, P4694, DOI [10.1109/icip.2019.8803681, 10.1109/ICIP.2019.8803681]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Issenhuth Thibaut, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P619, DOI 10.1007/978-3-030-58565-5_37
   Jandial Surgan, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2171, DOI 10.1109/WACV45572.2020.9093458
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kingma D. P., 2014, arXiv
   Kuang ZH, 2019, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2019.00316
   Kuppa G, 2021, IEEE WINT CONF APPL, P191, DOI 10.1109/WACVW52041.2021.00025
   Lee HJ, 2019, IEEE INT CONF COMP V, P3129, DOI 10.1109/ICCVW.2019.00381
   Lewis Kathleen M., VOGUE TRY ON STYLEGA
   Li Peike, 2019, Self correction for human parsing
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lorenz D, 2019, PROC CVPR IEEE, P10947, DOI 10.1109/CVPR.2019.01121
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Manfredi M, 2014, MACH VISION APPL, V25, P955, DOI 10.1007/s00138-013-0580-3
   Minar Matiur Rahman, 2020, P CVPR WORKSH
   Mir A, 2020, PROC CVPR IEEE, P7021, DOI 10.1109/CVPR42600.2020.00705
   Morelli Davide, 2021, IT INF RETR WORKSH
   Neuberger A, 2020, PROC CVPR IEEE, P5183, DOI 10.1109/CVPR42600.2020.00523
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Raffiee Amir Hossein, 2020, GARMENTGAN PHOTOREAL
   Raj A, 2018, LECT NOTES COMPUT SC, V11216, P679, DOI 10.1007/978-3-030-01258-8_41
   Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Song Xuemeng, 2019, SYNTH LECT INFO CONC, V11, P1
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tiwari Garvita, 2020, P ECCV
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wu ZH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P293, DOI 10.1145/3343031.3351083
   Xiao H., 2017, arXiv
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yildirim G, 2019, IEEE INT CONF COMP V, P3161, DOI 10.1109/ICCVW.2019.00389
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu Heming., 2020, P ECCV
NR 70
TC 10
Z9 10
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 62
DI 10.1145/3491226
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400014
DA 2024-07-18
ER

PT J
AU Tang, YS
   Liu, XY
   Yu, XM
   Zhang, DY
   Lu, JW
   Zhou, J
AF Tang, Yansong
   Liu, Xingyu
   Yu, Xumin
   Zhang, Danyang
   Lu, Jiwen
   Zhou, Jie
TI Learning from Temporal Spatial Cubism for Cross-Dataset Skeleton-based
   Action Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cubism; cross-dataset; skeleton-based action recognition; unsupervised
   domain adaptation; self-supervised learning
AB Rapid progress and superior performance have been achieved for skeleton-based action recognition recently. In this article, we investigate this problem under a cross-dataset setting, which is a new, pragmatic, and challenging task in real-world scenarios. Following the unsupervised domain adaptation (UDA) paradigm, the action labels are only available on a source dataset, but unavailable on a target dataset in the training stage. Different from the conventional adversarial learning-based approaches for UDA, we utilize a self-supervision scheme to reduce the domain shift between two skeleton-based action datasets. Our inspiration is drawn from Cubism, an art genre from the early 20th century, which breaks and reassembles the objects to convey a greater context. By segmenting and permuting temporal segments or human body parts, we design two self-supervised learning classification tasks to explore the temporal and spatial dependency of a skeleton-based action and improve the generalization ability of the model. We conduct experiments on six datasets for skeleton-based action recognition, including three large-scale datasets (NTU RGB+D, PKU-MMD, and Kinetics) where new cross-dataset settings and benchmarks are established. Extensive results demonstrate that our method outperforms state-of-the-art approaches. The source codes of our model and all the compared methods are available at https://github.com/shanice-1/st-cubism.
C1 [Tang, Yansong; Liu, Xingyu; Yu, Xumin; Zhang, Danyang; Lu, Jiwen; Zhou, Jie] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Dept Automat, Room 624,Cent Main Bldg, Beijing, Peoples R China.
C3 Tsinghua University
RP Lu, JW (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Dept Automat, Room 624,Cent Main Bldg, Beijing, Peoples R China.
EM tangyansong15@gmail.com; liuxy21@mails.tsinghua.edu.cn;
   yuxm20@mails.tsinghua.edu.cn; zdy004007@163.com;
   lujiwen@tsinghua.edu.cn; jzhou@tsinghua.edu.cn
RI Lu, Jiwen/C-5291-2009
OI Lu, Jiwen/0000-0002-6121-5529; Liu, Xingyu/0000-0003-1156-2263
FU National Natural Science Foundation of China [61822603, U1813218,
   U1713214]; Beijing Academy of Artificial Intelligence (BAAI); Tsinghua
   Zijing Scholar Fellowship; Institute for Guo Qiang, Tsinghua University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61822603, Grant U1813218, Grant
   U1713214, in part by Beijing Academy of Artificial Intelligence (BAAI),
   in part by a grant from the Institute for Guo Qiang, Tsinghua
   University, in part by Tsinghua Zijing Scholar Fellowship.
CR Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XY, 2019, PR MACH LEARN RES, V97
   Choi J., 2020, EUR C COMP VIS GLASG, V12357, P678
   Choutas V, 2018, PROC CVPR IEEE, P7024, DOI 10.1109/CVPR.2018.00734
   Chuang Gan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5589, DOI 10.1109/CVPR.2018.00586
   Ding ZM, 2018, LECT NOTES COMPUT SC, V11206, P36, DOI 10.1007/978-3-030-01216-8_3
   Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dwibedi D, 2019, PROC CVPR IEEE, P1801, DOI 10.1109/CVPR.2019.00190
   Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gidaris S., 2018, P 6 INT C LEARNING R
   Gong B., 2013, P MACHINE LEARNING R, P222
   Han F, 2017, COMPUT VIS IMAGE UND, V158, P85, DOI 10.1016/j.cviu.2017.01.011
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Jamal A, 2018, BMVC, P264
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3
   Lai ZH, 2020, PROC CVPR IEEE, P6478, DOI 10.1109/CVPR42600.2020.00651
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu JY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3365212
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Meagher M, 2007, IEEE INT CONF INF VI, P601
   Min-Hung Chen, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P6320, DOI 10.1109/ICCV.2019.00642
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Simonyan K, 2014, ADV NEUR IN, V27
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Sultani W, 2014, PROC CVPR IEEE, P764, DOI 10.1109/CVPR.2014.103
   Sun Y., 2020, P 37 INT C MACHINE L, P9229
   Sun Yu, 2019, ABS190911825 CORR
   Tang YS, 2020, IEEE T CIRC SYST VID, V30, P2872, DOI 10.1109/TCSVT.2020.2973301
   Tang YS, 2021, IEEE T PATTERN ANAL, V43, P3138, DOI 10.1109/TPAMI.2020.2980824
   Tang YS, 2019, IEEE T IMAGE PROCESS, V28, P4997, DOI 10.1109/TIP.2019.2914577
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Tas Y., 2018, PROC BRIT MACH VIS C, P158
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Verma P, 2020, MULTIMEDIA SYST, V26, P671, DOI 10.1007/s00530-020-00677-2
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang JL, 2019, PROC CVPR IEEE, P4001, DOI 10.1109/CVPR.2019.00413
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang P, 2016, LECT NOTES COMPUT SC, V9911, P370, DOI 10.1007/978-3-319-46478-7_23
   Wang PC, 2018, COMPUT VIS IMAGE UND, V171, P118, DOI 10.1016/j.cviu.2018.04.007
   Wang XL, 2019, PROC CVPR IEEE, P2561, DOI 10.1109/CVPR.2019.00267
   Weng JW, 2017, PROC CVPR IEEE, P445, DOI 10.1109/CVPR.2017.55
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yansong Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9836, DOI 10.1109/CVPR42600.2020.00986
   Ying Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P464, DOI 10.1007/978-3-030-58589-1_28
   Yu G, 2015, LECT NOTES COMPUT SC, V9007, P50, DOI 10.1007/978-3-319-16814-2_4
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang PF, 2018, LECT NOTES COMPUT SC, V11213, P136, DOI 10.1007/978-3-030-01240-3_9
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316
NR 74
TC 8
Z9 8
U1 3
U2 32
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 46
DI 10.1145/3472722
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yeh, CK
   Le, TNH
   Hou, ZY
   Lee, TY
AF Yeh, Chih-Kuo
   Thi-Ngoc-Hanh Le
   Hou, Zhi-Ying
   Lee, Tong-Yee
TI Generating Virtual Wire Sculptural Art from 3D Models
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Wire sculpture; edges; 3D wire art; edge segments; wire composition;
   smoothing
ID MESH SEGMENTATION
AB Wire sculptures are objects sculpted by the use of wires. In this article, we propose practical methods to create 3D virtual wire sculptural art from a given 3D model. In contrast, most of the previous 3D wire art results are reconstructed from input 2D wire art images. Artists usually tend to design their wire art with a single wire if possible. If not possible, they try to create it with the least number of wires. To follow this general design trend, our proposed method generates 3D virtual wire art with the minimum number of continuous wire lines. To achieve this goal, we first adopt a greedy approach to extract important edges of a given 3D model. These extracted important edges become the basis for the subsequent lines to roughly represent the shape of the input model. Then, we connect them with the minimum number of continuous wire lines by the order obtained by optimally solving a traveling salesman problem with some constraints. Finally, we smooth the obtained 3D wires to simulate the real 3D wire results by artists. In addition, we also provide a user interface to control the winding of wires by their design preference. Finally, we experimentally show our 3D virtual wire results and evaluate these created results. As a result, the proposed method is computed effectively and interactively, and results are appealing and comparable to real 3D wire art work.
C1 [Yeh, Chih-Kuo] Zhaoqing Univ, Sch Comp Sci & Software, Zhaoqing, Peoples R China.
   [Thi-Ngoc-Hanh Le; Hou, Zhi-Ying; Lee, Tong-Yee] Natl Cheng Kung Univ, Tainan 701, Taiwan.
C3 Zhaoqing University; National Cheng Kung University
RP Yeh, CK (corresponding author), Zhaoqing Univ, Sch Comp Sci & Software, Zhaoqing, Peoples R China.
EM simpson.ycg@gmail.com; ngochanh.le1987@gmail.com; denny1994a@gmail.com;
   tonylee@mail.ncku.edu.tw
RI Yeh, Chih-Kuo/JBS-2228-2023
FU Ministry of Science and Technology, Taiwan, Republic of China
   [108-2221-E-006-038-MY3, 110-2221-E-006-135-MY3]
FX This work was supported in part by the Ministry of Science and
   Technology (under nos. 108-2221-E-006-038-MY3 and
   110-2221-E-006-135-MY3), Taiwan, Republic of China.
CR [Anonymous], 2009, INT BUS MACH CORP, V46, P157
   Botsch M., 2004, PROC EUROGRAPHICS AC, P185, DOI [10.1145/1057432.1057457, DOI 10.1145/1057432.1057457]
   Cohen-Steiner D, 2004, ACM T GRAPHIC, V23, P905, DOI 10.1145/1015706.1015817
   DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354
   Emil, 2017, FITTING PLANE NOISY
   Gal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531339
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Hsiao KW, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275070
   Iarussi E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2710026
   Kara I, 2006, EUR J OPER RES, V174, P1449, DOI 10.1016/j.ejor.2005.03.008
   Li SW, 2018, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2018.00305
   Lira W, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275049
   Liu LJ, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073682
   Mehra R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618483
   Miguel E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925978
   Nealen A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276429
   Orzan A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360691
   Rodrigues RSV, 2018, COMPUT GRAPH FORUM, V37, P235, DOI 10.1111/cgf.13323
   Shamir A, 2008, COMPUT GRAPH FORUM, V27, P1539, DOI 10.1111/j.1467-8659.2007.01103.x
   Suzuki R., 2017, P 25 PAC C COMP GRAP, P16
   Wong FJ, 2011, COMPUT GRAPH FORUM, V30, P1931, DOI 10.1111/j.1467-8659.2011.02040.x
   Wu RD, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925966
   Zehnder J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925888
   Zhou YN, 2004, 10TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P187
NR 24
TC 0
Z9 0
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2022
VL 18
IS 2
AR 51
DI 10.1145/3475798
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0A0YT
UT WOS:000773689400008
DA 2024-07-18
ER

PT J
AU Siegfried, R
   Odobez, JM
AF Siegfried, Remy
   Odobez, Jean-Marc
TI Robust Unsupervised Gaze Calibration Using Conversation and Manipulation
   Attention Priors
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Gaze estimation; visual focus of attention; remote sensor; RGB-D camera;
   conversation; manipulation; unsupervised calibration; online calibration
ID EYE-GAZE; PERCEPTION
AB Gaze estimation is a difficult task, even for humans. However, as humans, we are good at understanding a situation and exploiting it to guess the expected visual focus of attention of people, and we usually use this information to retrieve people's gaze. In this article, we propose to leverage such situation-based expectation about people's visual focus of attention to collect weakly labeled gaze samples and perform person-specific calibration of gaze estimators in an unsupervised and online way. In this context, our contributions are the following: (i) we show how task contextual attention priors can be used to gather reference gaze samples, which is a cumbersome process otherwise; (ii) we propose a robust estimation framework to exploit these weak labels for the estimation of the calibration model parameters; and (iii) we demonstrate the applicability of this approach on two human-human and human-robot interaction settings, namely conversation and manipulation. Experiments on three datasets validate our approach. providing insights on the priors effectiveness and on the impact of different calibration models, particularly the usefulness of taking head pose into account.
C1 [Siegfried, Remy; Odobez, Jean-Marc] Idiap Res Inst, Martigny, Switzerland.
   [Siegfried, Remy; Odobez, Jean-Marc] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Siegfried, R (corresponding author), Idiap Res Inst, Martigny, Switzerland.; Siegfried, R (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM remy.siegfried@idiap.ch; odobez@idiap.ch
RI Odobez, Jean-Marc/B-1426-2010
OI Odobez, Jean-Marc/0000-0002-9537-9898; Siegfried,
   Remy/0000-0001-6963-6048
FU EU Horizon 2020 research and innovation program [688147]; H2020 -
   Industrial Leadership [688147] Funding Source: H2020 - Industrial
   Leadership
FX This research was supported by the EU Horizon 2020 research and
   innovation program (grant no. 688147, MuMMER project), and uses the
   KTH-Idiap database made available by KTH, Sweden.
CR Admoni H, 2017, J HUM-ROBOT INTERACT, V6, P25, DOI 10.5898/JHRI.6.1.Admoni
   [Anonymous], 2014, P 2014 WORKSH UND MO
   Ba SO, 2011, IEEE T PATTERN ANAL, V33, P101, DOI 10.1109/TPAMI.2010.69
   Bohus Dan., 2010, 2010115 MSRTR
   Chen M. C., 2001, CHI 01 HUM FACT COMP, P281, DOI DOI 10.1145/634067.634234
   Chen Z, 2020, ARXIV PREPRINT ARXIV
   David Klotz, 2011, P SIGDIAL 2011 C
   Funes Kenneth., 2012, P C COMP VIS PATT RE
   Funes Kenneth., 2014, P ACM S EYE TRACK RE
   Funes Kenneth., 2013, P INT C MULT INT ACM
   Funes-Mora KA, 2016, INT J COMPUT VISION, V118, P194, DOI 10.1007/s11263-015-0863-4
   Guestrin ED, 2006, IEEE T BIO-MED ENG, V53, P1124, DOI 10.1109/TBME.2005.863952
   Isaac L, 2014, PSYCHIAT RES, V218, P79, DOI 10.1016/j.psychres.2014.04.002
   Johansson RS, 2001, J NEUROSCI, V21, P6917, DOI 10.1523/JNEUROSCI.21-17-06917.2001
   Katsuki F, 2014, NEUROSCIENTIST, V20, P509, DOI 10.1177/1073858413514136
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kluttz NL, 2009, VISION RES, V49, P1979, DOI 10.1016/j.visres.2009.05.013
   Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239
   Land MF, 2009, VISUAL NEUROSCI, V26, P51, DOI 10.1017/S0952523808080899
   Langton SRH, 2004, PERCEPT PSYCHOPHYS, V66, P752, DOI 10.3758/BF03194970
   Lappi O, 2013, J VISION, V13, DOI 10.1167/13.13.11
   Lee WJ, 2019, EYE, V33, P1145, DOI 10.1038/s41433-019-0376-4
   Linden E, 2019, IEEE INT CONF COMP V, P1140, DOI 10.1109/ICCVW.2019.00145
   Liu G, 2021, IEEE T PATTERN ANAL, V43, P1092, DOI 10.1109/TPAMI.2019.2957373
   MASAME K, 1990, Tohoku Psychologica Folia, V49, P33
   Masko D., 2017, Calibration in Eye Tracking Using Transfer Learning
   Massé B, 2018, IEEE T PATTERN ANAL, V40, P2711, DOI 10.1109/TPAMI.2017.2782819
   Morimoto CH, 2005, COMPUT VIS IMAGE UND, V98, P4, DOI 10.1016/j.cviu.2004.07.010
   Muller Philipp., 2019, P ACM S EYE TRACK AP
   Muralidhar S, 2018, 17TH INTERNATIONAL CONFERENCE ON MOBILE AND UBIQUITOUS MULTIMEDIA (MUM 2018), P121, DOI 10.1145/3282894.3282925
   Muralidhar S, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P84, DOI 10.1145/2993148.2993191
   Nyström M, 2013, BEHAV RES METHODS, V45, P272, DOI 10.3758/s13428-012-0247-4
   Oertel C., 2013, P ANN C INT SPEECH C
   Park S, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204545
   Pfeuffer Ken., 2013, P S US INT S US INT
   Pi J., 2019, P 11 ACM S EYE TRACK, P1
   ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718
   Santini T, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2594, DOI 10.1145/3025453.3025950
   Selva Javier, 2018, P 29 BRIT MACH VIS C
   Sheikhi S, 2015, PATTERN RECOGN LETT, V66, P81, DOI 10.1016/j.patrec.2014.10.002
   Sidenmark L, 2020, ACM T COMPUT-HUM INT, V27, DOI 10.1145/3361218
   Sidenmark L, 2019, ETRA 2019: 2019 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS, DOI 10.1145/3314111.3319815
   Siegfried R., 2020, ACM S EYE TRACKING R
   Siegfried R., 2017, P 19 ACM INT C MULTI
   Smith Brian.., 2013, P ACM S US INT SOFTW
   Sugano Y, 2015, IEEE T HUM-MACH SYST, V45, P750, DOI 10.1109/THMS.2015.2400434
   Sugano Y, 2014, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2014.235
   Wang K, 2019, PROC CVPR IEEE, P11899, DOI [10.1109/CVPR.2019.01218, 10.1109/CVPR.2019.00881]
   Yu Y, 2019, PROC CVPR IEEE, P11929, DOI 10.1109/CVPR.2019.01221
   Yu Y, 2018, IEEE T PATTERN ANAL, V40, P2653, DOI 10.1109/TPAMI.2018.2841403
   Zhang XC, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300646
   Zhang XC, 2019, IEEE T PATTERN ANAL, V41, P162, DOI 10.1109/TPAMI.2017.2778103
   Zhang XC, 2015, PROC CVPR IEEE, P4511, DOI 10.1109/CVPR.2015.7299081
NR 53
TC 1
Z9 1
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2022
VL 18
IS 1
AR 20
DI 10.1145/3472622
PG 27
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZY5OX
UT WOS:000772636900020
DA 2024-07-18
ER

PT J
AU Wang, JW
   Huang, W
   Luo, XY
   Shi, YQ
   Jha, SKR
AF Wang, Jinwei
   Huang, Wei
   Luo, Xiangyang
   Shi, Yun-Qing
   Jha, Sunil K. R.
TI Detecting Non-Aligned Double JPEG Compression Based on Amplitude-Angle
   Feature
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Color image forensics; non-aligned double JPEG compression; spherical
   coordinates; blocking artifacts
ID COLOR; HISTORY; IMAGES
AB Due to the popularity of JPEG format images in recent years, JPEG images will inevitably involve image editing operation. Thus, some tramped images will leave tracks of Non-aligned double JPEG (NA-DJPEG) compression. By detecting the presence of NA-DJPEG compression, one can verify whether a given JPEG image has been tampered with. However, only fewmethods can identify NA-DJPEG compressed images in the case that the primary quality factor is greater than the secondary quality factor. To address this challenging task, this article proposes a novel feature extraction scheme based optimized pixel difference (OPD), which is a new measure for blocking artifacts. Firstly, three color channels (RGB) of a reconstructed image generated by decompressing a given JPEG color image are mapped into spherical coordinates to calculate amplitude and two angles (azimuth and zenith). Then, 16 histograms of OPD along the horizontal and vertical directions are calculated in the amplitude and two angles, respectively. Finally, a set of features formed by arranging the bin values of these histograms is used for binary classification. Experiments demonstrate the effectiveness of the proposed method, and the results show that it significantly outperforms the existing typical methods in the mentioned task.
C1 [Wang, Jinwei; Huang, Wei; Jha, Sunil K. R.] Nanjing Univ Informat Sci & Technol, 219 Ningliu Rd, Nanjing, Jiangsu, Peoples R China.
   [Luo, Xiangyang] State Key Lab Math Engn & Adv Comp, 62 Sci Ave, Zhengzhou, Henan, Peoples R China.
   [Shi, Yun-Qing] New Jersey Inst Technol, 323 Dr Martin Luther King Jr Blvd, Newark, NJ 07102 USA.
C3 Nanjing University of Information Science & Technology; PLA Information
   Engineering University; New Jersey Institute of Technology
RP Luo, XY (corresponding author), State Key Lab Math Engn & Adv Comp, 62 Sci Ave, Zhengzhou, Henan, Peoples R China.
EM xiangyangluo@126.com; shi@njit.edu
RI jinwei, wang/AAG-5700-2019
OI South, Set/0000-0002-3881-3168
FU National Natural Science Foundation of China [62072250, 61772281,
   61702235, U1804263, U20B2065, U1636117, U1636219, 61872203, 61802212];
   National Key R&D Program of China [2016QY01W0105]; plan for Scientific
   Talent of Henan Province [2018JR0018]; Post graduate Research & Practice
   Innvoation Program of Jiang su Province [KYCX200974]; Priority Academic
   Program Development of Jiang su Higher Education Institutions (PAPD)
   fund
FX This work was jointly supported by the National Natural Science
   Foundation of China (Grants No. 62072250, No. 61772281, No. 61702235,
   No. U1804263, No. U20B2065, No. U1636117, No. U1636219, No. 61872203,
   and No. 61802212), in part by the National Key R&D Program of China
   (Grant No. 2016QY01W0105), in part by the plan for Scientific Talent of
   Henan Province (Grant No. 2018JR0018), in part by Post graduate Research
   & Practice Innvoation Program of Jiang su Province (Grant No.
   KYCX200974), and the Priority Academic Program Development of Jiang su
   Higher Education Institutions (PAPD) fund.
CR [Anonymous], 2012, PUMPKIN
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Barni M, 2017, J VIS COMMUN IMAGE R, V49, P153, DOI 10.1016/j.jvcir.2017.09.003
   Bianchi T, 2012, IEEE T INF FOREN SEC, V7, P842, DOI 10.1109/TIFS.2011.2170836
   Chen YL, 2011, IEEE T INF FOREN SEC, V6, P396, DOI 10.1109/TIFS.2011.2106121
   Fan ZG, 2003, IEEE T IMAGE PROCESS, V12, P230, DOI 10.1109/TIP.2002.807361
   Fischler M. A., 1982, RECOVERING INTRINSIC
   Fu DD, 2007, PROC SPIE, V6505, DOI 10.1117/12.704723
   Hernández-Cabronero M, 2018, IEEE T MULTIMEDIA, V20, P257, DOI 10.1109/TMM.2017.2741426
   Huang FJ, 2010, IEEE T INF FOREN SEC, V5, P848, DOI 10.1109/TIFS.2010.2072921
   Lai SY, 2013, INT CONF ACOUST SPEE, P3028, DOI 10.1109/ICASSP.2013.6638214
   Luo WQ, 2007, INT CONF ACOUST SPEE, P217
   Mazaheri G., 2019, CVPR WORKSHOPS, P119
   MCDONNELL JD, 1994, INT CONF ACOUST SPEE, P329
   Mileva Y, 2007, LECT NOTES COMPUT SC, V4713, P152
   Neelamani R, 2006, IEEE T IMAGE PROCESS, V15, P1365, DOI 10.1109/TIP.2005.864171
   Piva A., 2013, ISRN SIGNAL PROCESS, V2013, DOI [10.1155/2013/496701, DOI 10.1155/2013/496701]
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409
   SYEDAMAHMOOD TF, 1992, LECT NOTES COMPUT SC, V588, P115
   van de Weijer J, 2004, IEEE IMAGE PROC, P1835
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang JY, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P433
   Wu LY, 2013, PROC SPIE, V8665, DOI 10.1117/12.2003695
   Yang JQ, 2014, IEEE T INF FOREN SEC, V9, P1933, DOI 10.1109/TIFS.2014.2359368
   Yang Jianquan, 2013, INT WORKSH DIG WAT
   Zhu SY, 2018, IEEE T MULTIMEDIA, V20, P525, DOI 10.1109/TMM.2017.2749162
NR 27
TC 0
Z9 0
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2021
VL 17
IS 4
AR 138
DI 10.1145/3464388
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP8GI
UT WOS:000748857800022
DA 2024-07-18
ER

PT J
AU Wang, Y
   Cao, Y
   Zhang, J
   Wu, F
   Zha, ZJ
AF Wang, Yang
   Cao, Yang
   Zhang, Jing
   Wu, Feng
   Zha, Zheng-Jun
TI Leveraging Deep Statistics for Underwater Image Enhancement
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Underwater image enhancement; pixel disruption strategy; two-branch
   architecture
ID COLOR; WATER
AB Underwater imaging often suffers from color cast and contrast degradation due to range-dependent medium absorption and light scattering. Introducing image statistics as prior has been proved to be an effective solution for underwater image enhancement. However, relative to the modal divergence of light propagation and underwater scenery, the existing methods are limited in representing the inherent statistics of underwater images resulting in color artifacts and haze residuals. To address this problem, this article proposes a convolutional neural network (CNN)-based framework to learn hierarchical statistical features related to color cast and contrast degradation and to leverage them for underwater image enhancement. Specifically, a pixel disruption strategy is first proposed to suppress intrinsic colors' influence and facilitate modeling a unified statistical representation of underwater image. Then, considering the local variation of depth of field, two parallel sub-networks: Color Correction Network (CC-Net) and Contrast Enhancement Network (CE-Net) are presented. The CC-Net and CE-Net can generate pixel-wise color cast and transmission map and achieve spatial-varied color correction and contrast enhancement. Moreover, to address the issue of insufficient training data, an imaging model-based synthesis method that incorporates pixel disruption strategy is presented to generate underwater patches with global degradation consistency. Quantitative and subjective evaluations demonstrate that our proposed method achieves state-of-the-art performance.
C1 [Wang, Yang; Cao, Yang; Wu, Feng; Zha, Zheng-Jun] Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230027, Anhui, Peoples R China.
   [Zhang, Jing] Univ Sydney, Fac Engn, Cleveland St, Sydney, NSW 2008, Australia.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Sydney
RP Cao, Y (corresponding author), Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230027, Anhui, Peoples R China.
EM jing.zhang1@sydney.edu.au; zhazj@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020; Wu, Feng/KCY-3017-2024; ZHANG,
   JING/HKF-4837-2023
OI ZHANG, JING/0000-0001-6595-7661
FU National Key R&D Program of China [2020AAA0105702, 2020AAA0105701];
   National Natural Science Foundation of China (NSFC) [U19B2038,
   61872327]; University Synergy Innovation Program of Anhui Province
   [GXXT-2019-025]; Major Special Science and Technology Project of Anhui
   [012223665049]; key scientific technological innovation research project
   by Ministry of Education
FX This work was supported by the National Key R&D Program of China under
   Grants No. 2020AAA0105702 and No.2020AAA0105701, the National Natural
   Science Foundation of China (NSFC) under Grants No. U19B2038 and No.
   61872327, the University Synergy Innovation Program of Anhui Province
   under Grant No. GXXT-2019-025, the Major Special Science and Technology
   Project of Anhui under Grant No. 012223665049, and the key scientific
   technological innovation research project by Ministry of Education.
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti CO, 2017, IEEE COMPUT SOC CONF, P997, DOI 10.1109/CVPRW.2017.136
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   [Anonymous], 2013, Maxout networks
   Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bianco S, 2015, IEEE COMPUT SOC CONF
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Carlevaris-Bianco N., 2010, OCEANS, P1, DOI DOI 10.1109/OCEANS.2010.5664428
   Chen Z, 2021, INT J COMPUT VISION, V129, P1121, DOI 10.1007/s11263-020-01412-0
   Cheng DL, 2014, J OPT SOC AM A, V31, P1049, DOI 10.1364/JOSAA.31.001049
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Deng YB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P870, DOI 10.1145/3240508.3240531
   Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hung-Yu Yang, 2011, Proceedings of the 2011 2nd International Conference on Innovations in Bio-Inspired Computing and Applications (IBICA 2011), P17, DOI 10.1109/IBICA.2011.9
   Johnson-Roberson M, 2017, J FIELD ROBOT, V34, P625, DOI 10.1002/rob.21658
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lebart K, 2003, IEEE J OCEANIC ENG, V28, P673, DOI 10.1109/JOE.2003.819314
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Li H., 2019, A Fusion Adversarial Underwater Image Enhancement Network with a Public Test Dataset
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Liu Chao, 2010, 2010 2nd International Conference on Computer Engineering and Technology (ICCET), P35, DOI 10.1109/ICCET.2010.5485339
   Liu JW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231741
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Olmos A., 2002, PROC BRIT MACH VIS C, P1, DOI 10.5244/c.16.50
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Shin YS, 2016, OCEANS 2016 MTS/IEEE MONTEREY, DOI 10.1109/OCEANS.2016.7761342
   Uplavikar P. M., 2019, P IEEE C COMP VIS PA, P1
   Vadrevu P, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI 10.14722/ndss.2017.23100
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Wang Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2015, DOI 10.1145/3343031.3350983
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Wen HC, 2013, IEEE INT SYMP CIRC S, P753, DOI 10.1109/ISCAS.2013.6571956
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Yang Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11046, DOI 10.1109/CVPR42600.2020.01106
   Yuh J, 2001, ADV ROBOTICS, V15, P609, DOI 10.1163/156855301317033595
   Zhang J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2355, DOI 10.1145/3394171.3413763
   Zhang J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P984, DOI 10.1145/3240508.3240653
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P72, DOI 10.1109/TIP.2019.2922837
   Zhang J, 2017, PROC CVPR IEEE, P7016, DOI 10.1109/CVPR.2017.742
   Zhang SJ, 2014, IEEE IMAGE PROC, P5422, DOI 10.1109/ICIP.2014.7026097
   Zhao XW, 2015, OCEAN ENG, V94, P163, DOI 10.1016/j.oceaneng.2014.11.036
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 56
TC 5
Z9 5
U1 1
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2021
VL 17
IS 3
SU S
SI SI
AR 116
DI 10.1145/3489520
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YA4BM
UT WOS:000738280600018
DA 2024-07-18
ER

PT J
AU Ding, YL
   Zhu, LH
   Wang, A
   Li, Y
   Wang, YJ
   Yiu, SM
   Gai, KK
AF Ding, Yaoling
   Zhu, Liehuang
   Wang, An
   Li, Yuan
   Wang, Yongjuan
   Yiu, Siu Ming
   Gai, Keke
TI A Multiple Sieve Approach Based on Artificial Intelligent Techniques and
   Correlation Power Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multiple sieve; genetic algorithm; correlation power analysis; parallel
   implementation; AES
ID LEAKAGE
AB Side-channel analysis achieves key recovery by analyzing physical signals generated during the operation of cryptographic devices. Power consumption is one kind of these signals and can be regarded as a multimedia form. In recent years, many artificial intelligence technologies have been combined with classical side-channel analysis methods to improve the efficiency and accuracy. A simple genetic algorithm was employed in Correlation Power Analysis (CPA) when apply to cryptographic algorithms implemented in parallel. However, premature convergence caused failure in recovering the whole key, especially when plenty of large S-boxes were employed in the target primitive, such as in the case of AES.
   In this article, we investigate the reason of premature convergence and propose a Multiple Sieve Method (MS-CPA), which overcomes this problem and reduces the number of traces required in correlation power analysis. Our method can be adjusted to combine with key enumeration algorithms and further improves the efficiency. Simulation experimental results depict that our method reduces the required number of traces by 63.7% and 30.77%, compared to classic CPA and the Simple-Genetic-Algorithm-based CPA (SGA-CPA), respectively, when the success rate is fixed to 90%. Real experiments performed on SAKURA-G confirm that the number of traces required for recovering the correct key in our method is almost equal to the minimum number that makes the correlation coefficients of correct keys stand out from the wrong ones and is much less than the numbers of traces required in CPA and SGA-CPA. When combining with key enumeration algorithms, our method has better performance. For the traces number being 200 (noise standard deviation sigma = 3.0), the attacks success rate of our method is 85%, which is much higher than the classic CPA with key enumeration (10% success rate). Moreover, we adjust our method to work on that DPA contest v1 dataset and achieve a better result (40.04 traces) than the winning proposal (42.42 traces).
C1 [Ding, Yaoling; Zhu, Liehuang; Wang, An; Li, Yuan; Gai, Keke] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
   [Ding, Yaoling; Wang, An] State Key Lab Cryptol, POB 5159, Beijing 100878, Peoples R China.
   [Wang, Yongjuan] Informat Engn Univ, Inst Cyberspace Secur, Zhengzhou 450001, Peoples R China.
   [Yiu, Siu Ming] Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Ding, Yaoling] Henan Key Lab Network Cryptog Technol, Zhengzhou 450001, Peoples R China.
   [Wang, An] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China.
C3 Beijing Institute of Technology; PLA Information Engineering University;
   University of Hong Kong; Chinese Academy of Sciences; Institute of
   Information Engineering, CAS
RP Wang, A (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.; Wang, A (corresponding author), State Key Lab Cryptol, POB 5159, Beijing 100878, Peoples R China.; Wang, A (corresponding author), Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China.
EM dyl19@bit.edu.cn; liehuangz@bit.edu.cn; wanganl@bit.edu.cn;
   ly18@bit.edu.cn; pinkywyj@163.com; smyiu@cs.hku.hk; gaikeke@bit.edu.cn
RI ZHU, LIEHUANG/A-6174-2018; li, liu/JXN-7328-2024
FU Beijing Natural Science Foundation [4202070]; National Natural Science
   Foundation of China [62002021, 61872040]; National Cryptography
   Development Fund [MMJJ20170201]; Henan Key Laboratory of Network
   Cryptography Technology [LNCT2020-A09]
FX This work is supported by Beijing Natural Science Foundation (No.
   4202070), National Natural Science Foundation of China (Nos. 62002021,
   61872040), National Cryptography Development Fund (No. MMJJ20170201),
   Henan Key Laboratory of Network Cryptography Technology (No.
   LNCT2020-A09).
CR [Anonymous], 2015, INT C SMART CARD RES
   [Anonymous], 2015, IACR CRYPTOLOGY EPRI
   [Anonymous], 2011, 2 INT WORKSH CONSTR
   Bartkewitz Timo, 2012, Smart Card Research and Advanced Applications. 11th International Conference (CARDIS 2012). Revised Selected Papers, P263, DOI 10.1007/978-3-642-37288-9_18
   Bogdanov Andrey, 2016, Selected Areas in Cryptography - SAC 2015. 22nd International Conference. Revised Selected Papers: LNCS 9566, P310, DOI 10.1007/978-3-319-31301-6_19
   Brier E, 2004, LECT NOTES COMPUT SC, V3156, P16, DOI 10.1007/978-3-540-28632-5_2
   Cagli E, 2017, LECT NOTES COMPUT SC, V10529, P45, DOI 10.1007/978-3-319-66787-4_3
   Chari S, 2002, LECT NOTES COMPUT SC, V2523, P13
   Clavier Christophe, 2016, The New Codebreakers Essays Dedicated to David Kahn on the Occasion of His 85th Birthday. LNCS 9100, P355, DOI 10.1007/978-3-662-49301-4_22
   Clavier Christophe., 2009, CHES SPECIAL SESSION, V1
   Daemen J, 2020, The design of rijndael: the advanced encryption standard (AES), V2nd, DOI DOI 10.1007/978-3-662-60769-53
   David L, 2017, LECT NOTES COMPUT SC, V10159, P311, DOI 10.1007/978-3-319-52153-4_18
   Ding YL, 2020, FUTURE GENER COMP SY, V106, P34, DOI 10.1016/j.future.2019.12.046
   Duc A, 2015, LECT NOTES COMPUT SC, V9056, P401, DOI 10.1007/978-3-662-46800-5_16
   Gierlichs B, 2008, LECT NOTES COMPUT SC, V5154, P426
   Glowacz C, 2015, LECT NOTES COMPUT SC, V9054, P117, DOI 10.1007/978-3-662-48116-5_6
   Heuser Annelie, 2012, Constructive Side-Channel Analysis and Secure Design. Proceedings Third International Workshop, COSADE 2012, P249, DOI 10.1007/978-3-642-29912-4_18
   Heuser A, 2020, IEEE T COMPUT, V69, P1434, DOI 10.1109/TC.2017.2757921
   Heyszl J, 2014, LECT NOTES COMPUT SC, V8419, P79, DOI 10.1007/978-3-319-08302-5_6
   Holland JH., 1975, Ann Arbor
   Kocher P., 1999, Advances in Cryptology - CRYPTO'99. 19th Annual International Cryptology Conference. Proceedings, P388
   Le TH, 2006, LECT NOTES COMPUT SC, V4249, P174
   Lemke-Rust K, 2007, LECT NOTES COMPUT SC, V4727, P14
   Lerman Liran, 2013, Constructive Side-Channel Analysis and Secure Design. 4th International Workshop, COSADE 2013. Revised Selected Papers. LNCS 7864, P184, DOI 10.1007/978-3-642-40026-1_12
   Lerman L., 2011, Center for Advanced Security Research Darmstadt, V29, DOI DOI 10.1504/IJACT.2014.062722
   Longo Jake, 2016, IACR CRYPTOLOGY EPRI, V2016, P609
   Martin DP, 2015, LECT NOTES COMPUT SC, V9453, P313, DOI 10.1007/978-3-662-48800-3_13
   Martinasek Z, 2013, RADIOENGINEERING, V22, P586
   Messerges Thomas S., 1999, P US WORKSH SMARTC T
   MUHLENBEIN H, 1991, PARALLEL COMPUT, V17, P619, DOI 10.1016/S0167-8191(05)80052-3
   National Bureau of Standards, 1977, DAT SNCRYPT STAND
   Picek S, 2017, IEEE IJCNN, P4095, DOI 10.1109/IJCNN.2017.7966373
   Poussier R, 2016, LECT NOTES COMPUT SC, V9813, P61, DOI 10.1007/978-3-662-53140-2_4
   Schramm K, 2003, LECT NOTES COMPUT SC, V2887, P206
   TELECOM ParisTech SEN Research Group, 2008, DPA CONT
   Timon Benjamin, 2019, IACR Trans. Cryptogr. Hardw. Embed. Syst., P107, DOI DOI 10.13154/TCHES.V2019.I2.107-131
   Veyrat-Charvillon N, 2013, LECT NOTES COMPUT SC, V7881, P126, DOI 10.1007/978-3-642-38348-9_8
   Veyrat-Charvillon Nicolas., 2012, International Conference on Selected Areas in Cryptography, P390, DOI DOI 10.1007/978-3-642-35999-625
   Xin Ye, 2015, Smart Card Research and Advanced Applications. 13th International Conference, CARDIS 2014. Revised Selected Papers: LNCS 8968, P215, DOI 10.1007/978-3-319-16763-3_13
   Zhang ZB, 2015, SECUR COMMUN NETW, V8, P3896, DOI 10.1002/sec.1308
NR 40
TC 1
Z9 1
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2021
VL 17
IS 2
SU S
SI SI
AR 71
DI 10.1145/3433165
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA7ZH
UT WOS:000667464100013
DA 2024-07-18
ER

PT J
AU Huang, CX
   Lan, YS
   Zhang, GK
   Xu, GW
   Jiang, LD
   Zeng, NY
   Tan, J
   Ng, EYK
   Cheng, YQ
   Han, NZ
   Ji, RR
   Peng, YH
AF Huang, Chenxi
   Lan, Yisha
   Zhang, Guokai
   Xu, Gaowei
   Jiang, Landu
   Zeng, Nianyin
   Tan, Jenhong
   Ng, E. Y. K.
   Cheng, Yongqiang
   Han, Ningzhi
   Ji, Rongrong
   Peng, Yonghong
TI A New Transfer Function for Volume Visualization of Aortic Stent and Its
   Application to Virtual Endoscopy
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Aortic stent visualization; transfer function; virtual endoscopy; volume
   rendering
ID OPTICAL COHERENCE TOMOGRAPHY; IMAGES; METHODOLOGY; FRAMEWORK; MODEL
AB Aortic stent has been widely used in restoring vascular stenosis and assisting patients with cardiovascular disease. The effective visualization of aortic stent is considered to be critical to ensure the effectiveness and functions of the aortic stent in clinical practice. Volume rendering with ray casting has been used as an effective approach to enable the effective visualization of aortic stent. The volume rendering relies on the transfer function that converts the medical images into optical attributes including color and transparency. This article proposes a new transfer function, namely, the multi-dimensional transfer function, to provide additional transparency value of a voxel. The proposed approach using the additional transparency value effectively assists the distinguishing of tissues that have the same CT value. The transparency values are simultaneously determined by gray threshold and gray change threshold, which can recognize the unnecessary structures such as bones transparent. A series of experimental results demonstrate that the situation of aorta stent of a patient can be directly observed, and the angle of view can be switched arbitrarily. The proposed method provides a new way for the operation of a virtual endoscopy to reach the place of blood vessels that a traditional endoscopy fails to reach.
C1 [Huang, Chenxi; Ji, Rongrong] Xiamen Univ, Sch Informat, Xiamen, Peoples R China.
   [Lan, Yisha; Xu, Gaowei; Han, Ningzhi] Tongji Univ, Dept Comp Sci & Technol, Shanghai, Peoples R China.
   [Zhang, Guokai] Tongji Univ, Sch Software Engn, Shanghai, Peoples R China.
   [Jiang, Landu] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.
   [Zeng, Nianyin] Xiamen Univ, Sch Aerosp Engn, Xiamen, Peoples R China.
   [Tan, Jenhong] Natl Univ Singapore, Inst Syst Sci, Singapore, Singapore.
   [Ng, E. Y. K.] Nanyang Technol Univ, Sch Mech & Aerosp Engn, Singapore, Singapore.
   [Cheng, Yongqiang] Univ Hull, Dept Comp Sci & Technol, Kingston Upon Hull, N Humberside, England.
   [Cheng, Yongqiang; Peng, Yonghong] Univ Sunderland, Fac Comp Sci, Sunderland, England.
C3 Xiamen University; Tongji University; Tongji University; McGill
   University; Xiamen University; National University of Singapore; Nanyang
   Technological University; University of Hull; University of Sunderland
RP Huang, CX (corresponding author), Xiamen Univ, Sch Informat, Xiamen, Peoples R China.; Peng, YH (corresponding author), Univ Sunderland, Fac Comp Sci, Sunderland, England.
EM 909813723@qq.com; angellanyisha@163.com; zhangguokai_01@163.com;
   1710050@tongji.edu.cn; landu.jiang@mail.mcgill.ca; zny@xmu.edu.cn;
   tanjenhong@nus.edu.sg; MYKNG@ntu.edu.sg; Y.Cheng@hull.ac.uk;
   1035111694@qq.com; rrji@xmu.edu.cn; Yonghong.Peng@Sunderland.ac.uk
RI Ng, Queena Wing-Yin/HHN-1743-2022; Huang, Chenxi/AAC-6316-2019; Ng, E Y
   K/A-1375-2011; Peng, Yonghong/ABD-5633-2021; Zeng, Nianyin/A-5094-2019
OI Ng, Queena Wing-Yin/0009-0009-5616-4382; Ng, E Y K/0000-0002-5701-1080;
   Zeng, Nianyin/0000-0002-6957-2942
FU Fundamental Research Funds for the Central Universities [22120190211];
   Nature Science Foundation of China [U1705262, 61772443, 61572410,
   61802324, 61702136]; National Key RD Program [2017YFC0113000,
   2016YFB1001503]; Nature Science Foundation of Fujian Province, China
   [2017J01125, 2018J01106]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant 22120190211, Nature Science
   Foundation of China (No. U1705262, No. 61772443, No. 61572410, No.
   61802324, and No. 61702136), National Key R&D Program (No.
   2017YFC0113000 and No. 2016YFB1001503), and Nature Science Foundation of
   Fujian Province, China (No. 2017J01125 and No. 2018J01106).
CR Bitter I, 2001, IEEE T VIS COMPUT GR, V7, P195, DOI 10.1109/2945.942688
   Choi K, 2013, MULTIMED TOOLS APPL, V64, P309, DOI 10.1007/s11042-012-1010-7
   Guiver C., 2017, MATH CONTR SIG SYST, V20
   Han P, 2000, EUR ARCH OTO-RHINO-L, V257, P578, DOI 10.1007/s004050000284
   Hashemian R, 2017, CIRC SYST SIGNAL PR, V36, P2473, DOI 10.1007/s00034-016-0418-0
   He TS, 1996, IEEE VISUAL, P227, DOI 10.1109/VISUAL.1996.568113
   Higo Y, 2014, NUCL INSTRUM METH B, V324, P63, DOI 10.1016/j.nimb.2013.11.024
   Holub J, 2017, J DIGIT IMAGING, V30, P738, DOI 10.1007/s10278-017-9986-1
   Huang CX, 2018, J MED IMAG HEALTH IN, V8, P1513, DOI 10.1166/jmihi.2018.2464
   Huang CX, 2018, IEEE ACCESS, V6, P39334, DOI 10.1109/ACCESS.2018.2855060
   Huang CX, 2018, IEEE ACCESS, V6, P36408, DOI 10.1109/ACCESS.2018.2839694
   Huang CX, 2018, IEEE ACCESS, V6, P27726, DOI 10.1109/ACCESS.2018.2840494
   Huang CX, 2018, J MED IMAG HEALTH IN, V8, P98, DOI 10.1166/jmihi.2018.2240
   Huang CX, 2017, COMPUT ASSIST SURG, V22, P127, DOI 10.1080/24699322.2017.1389390
   Huang D.S., 1996, Systematic theory of neural networks for pattern recognition, P201
   Huang DS, 2006, BIOINFORMATICS, V22, P1855, DOI 10.1093/bioinformatics/btl190
   Huang DS, 2012, IEEE T SYST MAN CY B, V42, P1489, DOI 10.1109/TSMCB.2012.2192475
   Huang DS, 2008, IEEE T NEURAL NETWOR, V19, P2099, DOI 10.1109/TNN.2008.2004370
   Huang DS, 1999, INT J PATTERN RECOGN, V13, P1083, DOI 10.1142/S0218001499000604
   Kang Yuzhi, 2009, GPU PROGRAMMING CG L
   Kawashima H, 2017, CARDIOVASC INTERV TH, V32, P165, DOI 10.1007/s12928-016-0385-1
   Kim YG, 2017, J KOREAN PHYS SOC, V71, P1064, DOI 10.3938/jkps.71.1064
   Kniss J, 2002, IEEE T VIS COMPUT GR, V8, P270, DOI 10.1109/TVCG.2002.1021579
   LEVOY M, 1988, IEEE COMPUT GRAPH, V8, P29, DOI 10.1109/38.511
   Liu BG, 2017, INT J ADV MANUF TECH, V89, P2701, DOI 10.1007/s00170-016-9431-5
   Ljung P, 2016, COMPUT GRAPH FORUM, V35, P669, DOI 10.1111/cgf.12934
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Mendonca CT, 2013, EUR J VASC ENDOVASC, V45, P465, DOI 10.1016/j.ejvs.2013.01.040
   Nakasato T, 2001, CLIN IMAG, V25, P171, DOI 10.1016/S0899-7071(01)00260-1
   Peng Jie, 2018, BME CLIN MED, V22, P2
   Sailamul P, 2017, J COMPUT NEUROSCI, V43, P189, DOI 10.1007/s10827-017-0657-5
   Salim MS, 2017, ARAB J SCI ENG, V42, P2717, DOI 10.1007/s13369-016-2345-6
   Sawall M, 2017, ANAL CHIM ACTA, V960, P40, DOI 10.1016/j.aca.2016.11.069
   Song YP, 2015, J MED BIOL ENG, V35, P270, DOI 10.1007/s40846-015-0027-6
   Suzuki N, 2012, PRECIS ENG, V36, P568, DOI 10.1016/j.precisioneng.2012.04.004
   Teistler M, 2007, INT J COMPUT ASS RAD, V2, P55, DOI 10.1007/s11548-007-0079-3
   Vining D. J., 1994, P ANN M AM ROENTG SO
   Volkmer BG, 2002, J UROLOGY, V168, P450, DOI 10.1016/S0022-5347(05)64656-3
   Wang Na, 2017, Marine Science Bulletin (Beijing), V36, DOI 10.11840/j.issn.1001-6392.2017.06.011
   Wang SL, 2012, IEEE ACM T COMPUT BI, V9, P580, DOI 10.1109/TCBB.2011.135
   Wang XF, 2009, IEEE T KNOWL DATA EN, V21, P1515, DOI 10.1109/TKDE.2009.21
   Wang XF, 2010, PATTERN RECOGN, V43, P603, DOI 10.1016/j.patcog.2009.08.002
   Xi T, 2013, INT J ORAL MAX SURG, V42, P1023, DOI 10.1016/j.ijom.2013.01.016
   Yamada K, 2007, VET RADIOL ULTRASOUN, V48, P318, DOI 10.1111/j.1740-8261.2007.00249.x
   Yang Fei, 2014, APPL MATH SERIES B, V29, P468
   Zhang G, 2016, OSTEOARTHR CARTILAGE, V24, P991, DOI 10.1016/j.joca.2016.01.004
   Zhang GM, 2019, IEEE ACCESS, V7, P64315, DOI 10.1109/ACCESS.2019.2916762
   Zhang GM, 2019, IEEE ACCESS, V7, P34645, DOI 10.1109/ACCESS.2019.2900619
   Zhang GM, 2016, MED ENG PHYS, V38, P1369, DOI 10.1016/j.medengphy.2016.09.008
   Zhang GM, 2016, J CRANIOFAC SURG, V27, P636, DOI 10.1097/SCS.0000000000002590
   Zheng CH, 2011, IEEE ACM T COMPUT BI, V8, P1273, DOI 10.1109/TCBB.2011.20
   Zheng CH, 2009, IEEE T INF TECHNOL B, V13, P599, DOI 10.1109/TITB.2009.2018115
NR 52
TC 2
Z9 2
U1 2
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2020
VL 16
IS 2
SU S
SI SI
AR 65
DI 10.1145/3373358
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1FH
UT WOS:000583712600008
DA 2024-07-18
ER

PT J
AU Akpinar, K
   Hua, KA
AF Akpinar, Kutalmis
   Hua, Kien A.
TI PPNet: Privacy Protected CDN-ISP Collaboration for QoS-aware Multi-CDN
   Adaptive Video Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video-on-demand; software-defined networks; Internet; content delivery
   networks; privacy; request routing
AB Software-defined networking introduces opportunities to optimize the Internet Service Provider's network and to improve client experience for the Video-on-Demand applications. Recent studies on SDN frameworks show that traffic engineering methods allow a fair share of bandwidth between adaptive video streaming clients. Additionally, ISPs can make better estimations of bandwidth and contribute to the bitrate selection for the clients. This study focuses on another aspect of network assistance in video delivery: CDN server selection. In a typical framework where the ISP contributes to the OM selection, the video provider and the network provider interfaces are merged together. Clients connect to the ISP to get the best CDN server candidate for a given video. This exposes client requests to the ISP. However, video providers have been investing large resources for encrypted video provisioning to preserve their client's information from third parties, especially network providers. The typical approach is not practical due to privacy concerns. In this study, we present a framework called PPNet to allow CDN-ISP collaboration while preventing the ISP's access to the video request and availability information. Our framework introduces an isolation between the video provider's and the ISP's web interfaces. Clients connect to both of the interfaces and deliver information on a need-to-know basis. As a second contribution, PPNet introduces a practical optimization method for CDN selection. Real-time data collection capabilities of a typical OpenFlow network is used as the input for optimization. Congestion-awareness has been the priority. To adapt for changing network conditions, capability of utilizing multiple servers simultaneously for a single video is introduced. Instead of directing each video client into a CDN node, the proposed system performs request routing per video segment. Finally, we present a system prototype of PPNet and show that our multiple-host adaptive streaming method introduces a significant improvement in quality of experience when compared to the state of the art.
C1 [Akpinar, Kutalmis; Hua, Kien A.] Univ Cent Florida, Dept Comp Sci, Orlando, FL 32826 USA.
C3 State University System of Florida; University of Central Florida
RP Akpinar, K (corresponding author), Univ Cent Florida, Dept Comp Sci, Orlando, FL 32826 USA.
EM kutalmis@knights.ucf.edu; kienhua@cs.ucf.edu
OI AKPINAR, KUTALMIS/0000-0002-8355-5401
CR Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   Adhikari VK, 2012, IEEE CONF COMPUT, P7, DOI 10.1109/INFCOMW.2012.6193524
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], 2011, P 20 INT C WORLD WID
   [Anonymous], 2014, LOGIN
   Arefin A, 2013, INT CON DISTR COMP S, P11, DOI 10.1109/ICDCS.2013.18
   Atary A., 2016, P IEEE INFOCOM, V2016, P1
   Autefage V., 2016, IEEE ICC, P1, DOI DOI 10.1109/ICC.2016.7511135
   Bagci KT, 2016, IEEE IMAGE PROC, P1519, DOI 10.1109/ICIP.2016.7532612
   Bagci KT, 2017, IEEE T MULTIMEDIA, V19, P2152, DOI 10.1109/TMM.2017.2736638
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Berman M, 2014, COMPUT NETW, V61, P5, DOI 10.1016/j.bjp.2013.12.037
   Bhat D, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183516
   Broadbent M., 2015, P 2015 1 IEEE C NETW, P1
   Cao XX, 2009, INTERNATIONAL SYMPOSIUM ON EMERGENCY MANAGEMENT 2009 (ISEM'09), P25
   Chang Dukhyun., 2012, Proceedings of the 7th International Conference on Future Internet Technologies, P29
   Chiang WK, 2016, 2016 INTERNATIONAL COMPUTER SYMPOSIUM (ICS), P159, DOI [10.1109/ICS.2016.39, 10.1109/ICS.2016.0040]
   Cisco, 2018, TECHNICAL REPORT
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   Georgopoulos P, 2015, COMPUT COMMUN, V69, P79, DOI 10.1016/j.comcom.2015.06.015
   Goransson P., 2016, SOFTWARE DEFINED NET
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Jahromi NT, 2017, CONSUM COMM NETWORK, P671, DOI 10.1109/CCNC.2017.7983211
   Jain S, 2013, ACM SIGCOMM COMP COM, V43, P3, DOI 10.1145/2534169.2486019
   Jarschel Michael, 2013, 2013 Second European Workshop on Software Defined Networks (EWSDN), P87, DOI 10.1109/EWSDN.2013.21
   Kleinrouweler JW, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092838
   Knight S, 2011, IEEE J SEL AREA COMM, V29, P1765, DOI 10.1109/JSAC.2011.111002
   Krishnan R, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P190
   Lee D.H., 2014, P NETWORK OPERATING, P31
   Mathis M., 1997, Computer Communication Review, V27, P67, DOI 10.1145/263932.264023
   Nam H., 2016, Measuring and improving the quality of experience of adaptive rate video
   Nam H, 2014, IEEE GLOB COMM CONF, P1317, DOI 10.1109/GLOCOM.2014.7036990
   Naman AT, 2018, COMPUT NETW, V134, P152, DOI 10.1016/j.comnet.2018.01.043
   Naylor D, 2014, PROCEEDINGS OF THE 2014 CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'14), P133, DOI 10.1145/2674005.2674991
   Qin F., 2016, 2016 8th International Conference on Wireless Communications Signal Processing (WCSP), P1, DOI DOI 10.1109/WCSP.2016.7752575
   sandvine, 2015, TECHNICAL REPORT
   Seward Zachary M., 2014, NETFLIX IS MAKING SU
   Shibuya Megumi, 2014, P ICN, P268
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Stewart R., 2015, P AS BSD C
   Hong TP, 2017, PROC INT CONF ADV, P16, DOI 10.1109/ATC.2017.8167608
   Wichtlhuber M, 2015, IEEE T NETW SERV MAN, V12, P48, DOI 10.1109/TNSM.2015.2404792
NR 43
TC 6
Z9 6
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 49
DI 10.1145/3379983
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600010
DA 2024-07-18
ER

PT J
AU Han, XH
   Zheng, YQ
   Sun, JD
   Chen, YW
AF Han, Xian-Hua
   Zheng, Yinqiang
   Sun, Jiande
   Chen, Yen-Wei
TI Hyperspectral Reconstruction with Redundant Camera Spectral Sensitivity
   Functions
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Hyperspectral reconstruction; spectral resolution enhancement; spectral
   redundancy; deep learning; ResNet; spectral sensitivity
ID CLASSIFICATION
AB High-resolution hyperspectral (HS) reconstruction has recently achieved significantly progress, among which the method based on the fusion of the RGB and HS images of the same scene can greatly improve the reconstruction performance compared with those based on the individually spectral or spatial enhancement. It is well known that the HS image is obtained only via the costly hypersoectral sensor, whereas the ROB images can be provided by low-price RGB cameras and the spectral sensitivity (SS) functions of RGB cameras are usually different. Thus, this study proposes a I IS reconstruction, which fuses merely two RGB images with redundant spectral responses. In this work, we design a new RGB camera via shifting the SS of an existed RGB camera, which can provide similar strength of spectral response with different spectral centers of SS, and fuse the new achieved color image with an existed RGB image by a deep ResNet. Experiments validate that fusion of two existed RGB images can provide impressive HS reconstruction performance and further improvement can be achieved by integrating the color image of the simulated SS with the RGB image.
C1 [Han, Xian-Hua] Yamaguchi Univ, Grad Sch Sci & Technol Innovat, 1677-1 Yoshida, Yamaguchi 7538511, Japan.
   [Zheng, Yinqiang] Natl Inst Informat, Chiyoda Ku, 2-1-2 Hitotsubashi, Tokyo, Japan.
   [Sun, Jiande] ShanDong Normal Univ, Sch Informat Sci & Engn, Jinan, Peoples R China.
   [Chen, Yen-Wei] Ritsumeikan Univ, Coll Informat Sci & Engn, 1-1-1 NojiHigashi, Kusatsu, Shiga, Japan.
C3 Yamaguchi University; Research Organization of Information & Systems
   (ROIS); National Institute of Informatics (NII) - Japan; Shandong Normal
   University; Ritsumeikan University
RP Han, XH (corresponding author), Yamaguchi Univ, Grad Sch Sci & Technol Innovat, 1677-1 Yoshida, Yamaguchi 7538511, Japan.
EM hanxhua@yamaguchi-u.ac.jp; yqzheng@nii.ac.jp; jiandesun@hotmail.com;
   chen@is.ritsumei.ac.jp
RI Han, Xian-Hua/A-5563-2017
OI Han, Xian-Hua/0000-0002-5003-3180; Zheng, Yinqiang/0000-0001-7434-5069
FU ROIS NII Open Collaborative Research [20FC02]; Natural Science
   Foundation for Distinguished Young Scholars of Shandong Province
   [JQ201718]; Natural Science Foundation of China [U1736122];
   Grants-in-Aid for Scientific Research [20K11867] Funding Source: KAKEN
FX This work was supported by ROIS NII Open Collaborative Research 2020
   (Grant No.: 20FC02), the Natural Science Foundation for Distinguished
   Young Scholars of Shandong Province (JQ201718), and the Natural Science
   Foundation of China (U1736122).
CR Aeschbacher J., 2017, P IEEE INT C COMP VI
   Akhtar N, 2015, PROC CVPR IEEE, P3631, DOI 10.1109/CVPR.2015.7298986
   Alvarez-Gila Aitor, 2017, P IEEE INT C COMP VI
   [Anonymous], 2016, ARXIV 1606 00915 CS
   [Anonymous], 2013, IEEE GEOSCIENCE REMO
   Arad B, 2016, LECT NOTES COMPUT SC, V9911, P19, DOI 10.1007/978-3-319-46478-7_2
   Chakrabarti A., 2011, PROC CVPR IEEE, P193, DOI [DOI 10.1109/CVPR.2011.5995660, 10.1109/CVPR.2011.5995660]
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Dian RW, 2018, IEEE T NEUR NET LEAR, V29, P5345, DOI 10.1109/TNNLS.2018.2798162
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2016, IEEE T IMAGE PROCESS, V25, P2337, DOI 10.1109/TIP.2016.2542360
   Fauvel M, 2013, P IEEE, V101, P652, DOI 10.1109/JPROC.2012.2197589
   Galliani S., 2017, ARXIV170309470
   Han X., 2019, Evid Based Complement Alternat Med, V2019, P1
   Han XH, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P266, DOI [10.1109/BigMM.2019.00049, 10.1109/BigMM.2019.00-13]
   Han XH, 2018, IEEE IMAGE PROC, P2506, DOI 10.1109/ICIP.2018.8451142
   Han XH, 2018, IEEE T IMAGE PROCESS, V27, P5625, DOI 10.1109/TIP.2018.2855418
   Hardie RC, 2004, IEEE T IMAGE PROCESS, V13, P1174, DOI 10.1109/TIP.2004.829779
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Jiang J, 2013, IEEE WORK APP COMP, P168, DOI 10.1109/WACV.2013.6475015
   Kim J, 2016, IEEE CONF COMPUT
   Kwan P, 2018, EPILEPSY BEHAV CASE, V9, P1, DOI 10.1016/j.ebcr.2017.11.001
   Lanaras C, 2015, IEEE I CONF COMP VIS, P3586, DOI 10.1109/ICCV.2015.409
   Li YS, 2017, NEUROCOMPUTING, V266, P29, DOI 10.1016/j.neucom.2017.05.024
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Loncan L, 2015, IEEE GEOSC REM SEN M, V3, P27, DOI 10.1109/MGRS.2015.2440094
   MEI SH, 2017, NEUROCOMPUTING, V9, DOI DOI 10.3390/RS9111139
   Tarabalka Y, 2010, IEEE T SYST MAN CY B, V40, P1267, DOI 10.1109/TSMCB.2009.2037132
   Wei Q, 2015, IEEE T GEOSCI REMOTE, V53, P3658, DOI 10.1109/TGRS.2014.2381272
   Yang JH, 2008, IET COMPUT DIGIT TEC, V2, P1, DOI 10.1049/iet-cdt:20070066
   Yasuma F, 2010, IEEE T IMAGE PROCESS, V19, P2241, DOI 10.1109/TIP.2010.2046811
   Yi C, 2019, IEEE T GEOSCI REMOTE, V57, P9010, DOI 10.1109/TGRS.2019.2924096
   Yokoya N, 2012, IEEE T GEOSCI REMOTE, V50, P528, DOI 10.1109/TGRS.2011.2161320
   Zhou J, 2016, J APPL REMOTE SENS, V10, DOI 10.1117/1.JRS.10.035024
   Zhou Y, 2014, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2014.394
NR 37
TC 1
Z9 1
U1 1
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2020
VL 16
IS 2
AR 57
DI 10.1145/3386313
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1EO
UT WOS:000583710600018
DA 2024-07-18
ER

PT J
AU Zare, A
   Homayouni, M
   Aminlou, A
   Hannuksela, MM
   Gabbouj, M
AF Zare, Alireza
   Homayouni, Maryam
   Aminlou, Alireza
   Hannuksela, Miska M.
   Gabbouj, Moncef
TI 6K and 8K Effective Resolution with 4K HEVC Decoding Capability for 360
   Video Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Virtual reality; 360 degrees video coding and streaming; adaptive
   streaming; OMAF; HEVC
AB The recent Omnidirectional MediA Format (OMAF) standard, which specifies the delivery of 360 degrees video content, supports only equirectangular projection (ERP) and cubemap projection and their region-wise packing with a limitation on video decoding capability to the maximum resolution of 4K (e.g., 4,096 x 2,048). Streaming of 4K ERP content allows only a limited viewport resolution, which is lower than the resolution of many current head-mounted displays (HMDs). Therefore, to take full advantage of high-resolution HMDs, delivery of 360 degrees video content beyond 4K resolution needs to be enabled. In this regard, we propose two specific mixed-resolution packing schemes of 6K (c.g., 6,144 x 3,072) and 8K (c.g., 8,192 x 4,096) ERP content and their realization in tile-based streaming, while complying with the 4K decoding constraint and the High Efficiency Video Coding standard. The proposed packing schemes offer 6K and 8K effective resolution at the viewport. Using our proposed test methodology, experimental results indicate that the proposed layouts significantly decrease streaming bitrates when compared to mixed-quality viewport-adaptive streaming of 4K ERP. Our results further indicate that 8K-effective packing outperforms 6K-effective packing especially in high-quality videos.
C1 [Zare, Alireza; Homayouni, Maryam; Aminlou, Alireza; Hannuksela, Miska M.] Nokia Technol, Hatanpaan Valtatie 30, Tampere 33100, Finland.
   [Gabbouj, Moncef] Tampere Univ, Korkeakoulunkatu 6, Tampere 33720, Finland.
C3 Nokia Corporation; Nokia Finland; Tampere University
RP Zare, A (corresponding author), Nokia Technol, Hatanpaan Valtatie 30, Tampere 33100, Finland.
EM alireza.zare@nokia.com; maryam.homayouni@nokia.com;
   alireza.aminlou@nokia.com; miska.hannuksela@nokia.com;
   moncef.gabbouj@tuni.fi
RI Gabbouj, Moncef/G-4293-2014; zare, alireza/F-6371-2018
OI Gabbouj, Moncef/0000-0002-9788-2323; 
CR Aminlou A., 2016, P ITU T JOINT VID EX
   [Anonymous], 2017, 2017 IEEE 19 INT WOR, DOI DOI 10.1109/MMSP.2017.8122227
   [Anonymous], 2015, 14496122015 ISOIEC
   Bjontegard G., 2001, Calculation of average psnr differences between rdcurves
   Boyce J., 2016, P ITU T JOINT VID EX
   Choi B., 2017, MPEGM41922
   Hannuksela M. M., 2017, JCTVCV0072
   Hannuksela M. M., 2019, P DAT COMPR C
   Hannuksela MM, 2004, IEEE T MULTIMEDIA, V6, P259, DOI 10.1109/TMM.2003.822784
   Hristova H., 2018, P IEEE 20 INT WORKSH
   ISO/IEC, 2014, Rep. 23009-1. ISO/IEC JTCI/SC29/WG11
   Kammachi-Sreedhar K, 2016, IEEE INT SYM MULTIM, P583, DOI [10.1109/ISM.2016.0126, 10.1109/ISM.2016.143]
   Lederer S., 2017, P MPEG WORKSH GLOB M
   Misra K, 2013, IEEE J-STSP, V7, P969, DOI 10.1109/JSTSP.2013.2271451
   Ozcinar C, 2019, IEEE J EM SEL TOP C, V9, P217, DOI 10.1109/JETCAS.2019.2895096
   Podborski D., 2017, P INT BROADC CONV IB
   Sánchez Y, 2015, IEEE IMAGE PROC, P2244, DOI 10.1109/ICIP.2015.7351200
   Singla A., 2017, P ITU T JOINT VID EX
   Sun Y., 2016, P JOINT VID EXPL TEA
   Van der Auwera G., 2016, P ITU T JOINT VID EX
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Zare A, 2017, P IEEE INT C IM PROC
   Zare A., 2017, P ITU T JOINT VID EX
   Zare A., 2016, P 24 ACM INT C MULT, P601, DOI DOI 10.1145/2964284.2967292
   Zare A, 2018, PROCEEDINGS OF THE 23TH ACM WORKSHOP ON PACKET VIDEO (PV'18), P72, DOI 10.1145/3210424.3210425
NR 25
TC 5
Z9 7
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2019
VL 15
IS 2
SU S
SI SI
AR 68
DI 10.1145/3335053
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IS2RO
UT WOS:000482001900021
DA 2024-07-18
ER

PT J
AU Cecil, J
   Gupta, A
   Pirela-Cruz, M
   Ramanthan, P
AF Cecil, J.
   Gupta, Avinash
   Pirela-Cruz, M.
   Ramanthan, Parmesh
TI A Network-Based Virtual Reality Simulation Training Approach for
   Orthopedic Surgery
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Virtual reality; orthopedic surgery; medical simulation; Next Generation
   Internet technologies; immersive simulator
ID ENVIRONMENT; SKILLS; VALIDATION; SYSTEM; ACQUISITION; FRAMEWORK
AB The focus of this article is on the adoption of immersive and haptic simulators for training of medical residents in a surgical process called Less Invasive Stabilization System (LISS) plating surgery. LISS surgery is an orthopedic surgical procedure to treat fractures of the femur bone. Development of such simulators is a complex task which involves multiple systems, technologies, and human experts. Emerging Next Generation Internet technologies were used to develop the standalone on-line haptic-based simulator accessible to the students 24/7. A standalone immersive surgical simulator was also developed using HTC Vive. Expert surgeons played an important role in developing the simulator system; use cases of the target surgical processes were built using a modeling language called the engineering Enterprise Modeling Language (eEML). A detailed study presenting the comparison between the haptic-based simulator and the immersive simulator has been also presented. The outcomes of this study underscore the potential of using such simulators in surgical training.
C1 [Cecil, J.; Gupta, Avinash] Oklahoma State Univ, Stillwater, OK 74078 USA.
   [Pirela-Cruz, M.] Texas Tech Univ, Hlth Sci Ctr, Lubbock, TX 79409 USA.
   [Ramanthan, Parmesh] Univ Wisconsin, Madison, WI 53706 USA.
C3 Oklahoma State University System; Oklahoma State University -
   Stillwater; Texas Tech University System; Texas Tech University Health
   Science Center; University of Wisconsin System; University of Wisconsin
   Madison
RP Cecil, J (corresponding author), Oklahoma State Univ, Stillwater, OK 74078 USA.
EM j.cecil@okstate.edu; avinash.gupta@okstate.edu; cruzer@zianet.com;
   parmesh.ramanathan@wisc.edu
FU National Science Foundation [CNS 1257803]
FX This material is based upon work supported by the National Science
   Foundation [under grant number CNS 1257803].
CR Acosta E., 2007, 2007 IEEE S 3D US IN
   Aggarwal R, 2008, J SURG RES, V145, P80, DOI 10.1016/j.jss.2007.04.027
   Andersen C, 2011, ACTA ORTHOP, V82, P90, DOI 10.3109/17453674.2011.552776
   [Anonymous], 2011, 5THCONFERENCE BIOINF
   Assassi L, 2009, COMPUT ANIMAT VIRT W, V20, P53, DOI 10.1002/cav.266
   Azzie G, 2011, J PEDIATR SURG, V46, P897, DOI 10.1016/j.jpedsurg.2011.02.026
   Bayona Sofia., 2006, International Conference on Medical iIformation Visualisation-BioMedical Visualisation (MedVis'06), P71
   Berlage T, 2001, INT CONGR SER, V1230, P68, DOI 10.1016/S0531-5131(01)00013-9
   Berman M, 2014, COMPUT NETW, V61, P5, DOI 10.1016/j.bjp.2013.12.037
   Blyth P, 2007, INJURY, V38, P1197, DOI 10.1016/j.injury.2007.03.031
   Braman JP, 2015, ARTHROSCOPY, V31, P104, DOI 10.1016/j.arthro.2014.07.012
   Brandt G, 1999, IEEE Trans Inf Technol Biomed, V3, P252, DOI 10.1109/4233.809169
   Burdea G, 2000, IEEE T REHABIL ENG, V8, P430, DOI 10.1109/86.867886
   Cáceres R, 2012, IEEE PERVAS COMPUT, V11, P14, DOI 10.1109/MPRV.2011.85
   Cecil J, 2018, INT J COMPUT ASS RAD, V13, P305, DOI 10.1007/s11548-017-1688-0
   Cecil J., 2017, 0OGENT MED, V4
   Cecil J., 2016, OTM CONF INT C MOV M, P206
   Chebbi B, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATIONS, VOLS 1-4, CONFERENCE PROCEEDINGS, P315
   Choi KS, 2009, COMPUT BIOL MED, V39, P1020, DOI 10.1016/j.compbiomed.2009.08.003
   Citak M, 2008, J ORTHOP RES, V26, P547, DOI 10.1002/jor.20517
   d'Aulignac D, 1999, LECT NOTES COMPUT SC, V1679, P1191
   de Oliveira JC, 2003, PRESENCE-VIRTUAL AUG, V12, P555, DOI 10.1162/105474603322955888
   DELP SL, 1995, COMPUT BIOL MED, V25, P21, DOI 10.1016/0010-4825(95)98882-E
   Echegaray G, 2014, IEEE COMPUT GRAPH, V34, P12, DOI 10.1109/MCG.2014.43
   Eriksson M, 2005, STUD HEALTH TECHNOL, V111, P133
   Forsslund Jonas, 2009, 2009 World Haptics Conference (WHC 2009), P391, DOI 10.1109/WHC.2009.4810916
   Grantcharov TP, 2004, BRIT J SURG, V91, P146, DOI 10.1002/bjs.4407
   Howells NR, 2008, J BONE JOINT SURG BR, V90B, P494, DOI 10.1302/0301-620X.90B4.20414
   Jalote-Parmar A, 2008, DESIGN STUD, V29, P338, DOI 10.1016/j.destud.2008.03.002
   Jannin P, 2013, 2013 MED MEETS VIRT
   Jay C, 2007, ACM T COMPUT-HUM INT, V14, DOI 10.1145/1275511.1275514
   Jun Y, 2011, INT J PRECIS ENG MAN, V12, P157, DOI 10.1007/s12541-011-0021-z
   Karaliotas C., 2011, Hellenic Journal of Surgery, V83, P303, DOI [10.1007/s13126-011-0055-9, DOI 10.1007/S13126-011-0055-9]
   Kong MX, 2005, P ANN INT IEEE EMBS, P97, DOI 10.1109/IEMBS.2005.1616351
   Kovler I, 2015, INT J COMPUT ASS RAD, V10, P1535, DOI 10.1007/s11548-015-1162-9
   Lin YP, 2014, J BIOMED INFORM, V48, P122, DOI 10.1016/j.jbi.2013.12.010
   Luciano C, 2009, VIRTUAL REAL-LONDON, V13, P69, DOI 10.1007/s10055-009-0112-7
   Makiyama K, 2012, INT J UROL, V19, P829, DOI 10.1111/j.1442-2042.2012.03053.x
   Megard Christine, 2009, P 16 ACM S VIRT REAL, P265, DOI [10.1145/1643928.1643997, DOI 10.1145/1643928.1643997]
   Morris D, 2004, LECT NOTES COMPUT SC, V3217, P319
   Nahvi A, 2016, INT J ROBOT THEORY A, V1, P1
   Nemani Arun, 2013, Stud Health Technol Inform, V184, P293
   Padilla-Castañeda MA, 2013, IEEE INT C INT ROBOT, P1506, DOI 10.1109/IROS.2013.6696548
   Paiva PVF, 2016, SYMP VIRTUAL AUGMENT, P29, DOI 10.1109/SVR.2016.16
   Park JW, 2011, ARTIF ORGANS, V35, P1127, DOI 10.1111/j.1525-1594.2011.01373.x
   Peters TM, 2008, LECT NOTES COMPUT SC, V5128, P1, DOI 10.1007/978-3-540-79982-5_1
   Pettersson J, 2008, IEEE T BIO-MED ENG, V55, P1255, DOI 10.1109/TBME.2007.908099
   Qin J, 2010, J MED SYST, V34, P261, DOI 10.1007/s10916-008-9237-6
   Qin J, 2009, COMPUT METH PROG BIO, V96, P205, DOI 10.1016/j.cmpb.2009.06.008
   Ren J, 2008, IEEE T MED IMAGING, V27, P1061, DOI 10.1109/TMI.2008.917246
   Rongdong Yu, 2010, 2010 Second International Conference on Computer Modeling and Simulation (ICCMS 2010), P68, DOI 10.1109/ICCMS.2010.499
   Rose K, 2015, ARTHROSCOPY, V31, P299, DOI 10.1016/j.arthro.2014.08.016
   Sabri H, 2010, PROCD SOC BEHV, V2, P3483, DOI 10.1016/j.sbspro.2010.03.539
   Sales BRA, 2011, TECHNOLOGY AND MEDICAL SCIENCES - TMSI 2010, P157
   Schlickum MK, 2009, WORLD J SURG, V33, P2360, DOI 10.1007/s00268-009-0151-y
   Seixas-Mikelus SA, 2010, UROLOGY, V76, P357, DOI 10.1016/j.urology.2009.11.069
   Seth T., 2011, 4 INT S APPL SCI BIO, P71
   Shi YF, 2015, 2015 8TH INTERNATIONAL CONFERENCE ON BIOMEDICAL ENGINEERING AND INFORMATICS (BMEI), P355, DOI 10.1109/BMEI.2015.7401529
   Sorensen TS, 2001, ARTIF INTELL MED, V22, P193, DOI 10.1016/S0933-3657(00)00109-3
   Sourin A, 2000, IEEE COMPUT GRAPH, V20, P6, DOI 10.1109/38.844364
   Sourina O., 2000, International Journal of Information Technology, V6, P16
   Stunt J, 2015, KNEE SURG SPORT TR A, V23, P3436, DOI 10.1007/s00167-014-3101-7
   Tang TW, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON INTEGRATION TECHNOLOGY, PROCEEDINGS, P15
   Tolsdorff B, 2010, LARYNGOSCOPE, V120, P420, DOI 10.1002/lary.20676
   Torkington J, 2000, ANN ROY COLL SURG, V82, P88
   Tsai MD, 2001, COMPUT BIOL MED, V31, P333, DOI 10.1016/S0010-4825(01)00014-2
   Tsai MD, 2007, COMPUT BIOL MED, V37, P1709, DOI 10.1016/j.compbiomed.2007.04.006
   Vankipuram M, 2010, J BIOMED INFORM, V43, P661, DOI 10.1016/j.jbi.2010.05.016
   Verdaasdonk EGG, 2008, SURG ENDOSC, V22, P1636, DOI 10.1007/s00464-007-9672-3
   Le VT, 2010, I C CONT AUTOMAT ROB, P448, DOI 10.1109/ICARCV.2010.5707421
   Watterson JD, 2002, J UROLOGY, V168, P1928, DOI 10.1016/S0022-5347(05)64265-6
   Wijewickrema S, 2017, COMP MED SY, P7, DOI 10.1109/CBMS.2017.10
   Willaert WIM, 2012, WORLD J SURG, V36, P1703, DOI 10.1007/s00268-012-1489-0
   Youngblood P, 2008, SIMUL HEALTHC, V3, P146, DOI 10.1097/SIH.0b013e31817bedf7
   Yu L., 2013, 2 INT C COMP SCI EL
NR 75
TC 12
Z9 13
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2018
VL 14
IS 3
AR 77
DI 10.1145/3232678
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GT7CI
UT WOS:000444677600010
OA Bronze
DA 2024-07-18
ER

PT J
AU Guo, D
   Zhou, WG
   Li, HQ
   Wang, M
AF Guo, Dan
   Zhou, Wengang
   Li, Houqiang
   Wang, Meng
TI Online Early-Late Fusion Based on Adaptive HMM for Sign Language
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Sign language recognition; multi-modal feature fusion; query-adaptive;
   HMM; online algorithm
AB In sign language recognition (SLR) with multimodal data, a signword can be represented by multiply features, for which there exist an intrinsic property and a mutually complementary relationship among them. To fully explore those relationships, we propose an online early-late fusion method based on the adaptive Hidden Markov Model (HMM). In terms of the intrinsic property, we discover that inherent latent change states of each sign are related not only to the number of key gestures and body poses but also to their translation relationships. We propose an adaptive HMM method to obtain the hidden state number of each sign by affinity propagation clustering. For the complementary relationship, we propose an online early-late fusion scheme. The early fusion (feature fusion) is dedicated to preserving useful information to achieve a better complementary score, while the late fusion (score fusion) uncovers the significance of those features and aggregates them in a weighting manner. Different from classical fusion methods, the fusion is query adaptive. For different queries, after feature selection (including the combined feature), the fusion weight is inversely proportional to the area under the curve of the normalized query score list for each selected feature. The whole fusion process is effective and efficient. Experiments verify the effectiveness on the signer-independent SLR with large vocabulary. Compared either on different dataset sizes or to different SLR models, our method demonstrates consistent and promising performance.
C1 [Guo, Dan; Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230601, Anhui, Peoples R China.
   [Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, EEIS Dept, Hefei 230027, Anhui, Peoples R China.
C3 Hefei University of Technology; Chinese Academy of Sciences; University
   of Science & Technology of China, CAS
RP Wang, M (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230601, Anhui, Peoples R China.; Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, EEIS Dept, Hefei 230027, Anhui, Peoples R China.
EM guodan@hfut.edu.cn; zhwg@ustc.edu.cn; lihq@ustc.edu.cn;
   eric.mengwang@gmail.com
RI Li, Houqiang Li/B-6259-2013; Wang, Meng/ITR-8699-2023
OI Guo, Dan/0000-0003-2594-254X
FU NSFC [61432019, 61632019]
FX This work was supported in part to Prof. Meng Wang by NSFC under
   contract no. 61432019 and in part to Dr. Wengang Zhou by NSFC under
   contract no. 61632019.
CR Alon J, 2009, IEEE T PATTERN ANAL, V31, P1685, DOI 10.1109/TPAMI.2008.203
   [Anonymous], ECCV CHALEARN LOOK P
   [Anonymous], 2016, PROC IEEE COMPUT VIS
   [Anonymous], 2015, EURASIP J WIRELESS C
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2014, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-16634-6_18
   [Anonymous], 2013, BRIT MACH VIS C
   [Anonymous], 2014, ABS14053531 CORR
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2015, Computer Vision and Pattern Recognition Workshops (CVPRW), 2015 IEEE Conference on
   [Anonymous], ACM T MULTIMEDIA COM
   Belongie S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P675, DOI 10.1109/ICCV.1998.710790
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Camgoz NC, 2016, INT C PATT RECOG, P49, DOI 10.1109/ICPR.2016.7899606
   Celebi Sait, 2013, International Conference on Computer Vision Theory and Applications (VISAPP 2013). Proceedings, P620
   Cheng H, 2016, IEEE T CIRC SYST VID, V26, P1659, DOI 10.1109/TCSVT.2015.2469551
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fothergill S., 2012, P SIGCHI C HUM FACT, P1737, DOI DOI 10.1145/2207676.2208303
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Georgis N, 1998, IEEE T PATTERN ANAL, V20, P366, DOI 10.1109/34.677262
   Guo D, 2016, IEEE IMAGE PROC, P2876, DOI 10.1109/ICIP.2016.7532885
   Jain A, 2005, PATTERN RECOGN, V38, P2270, DOI 10.1016/j.patcog.2005.01.012
   Khan FS, 2012, PROC CVPR IEEE, P3306, DOI 10.1109/CVPR.2012.6248068
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Kurakin A, 2012, EUR SIGNAL PR CONF, P1975
   Lin SY, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089250
   Liu T, 2016, IEEE IMAGE PROC, P2871, DOI 10.1109/ICIP.2016.7532884
   Liu W, 2011, PROC CVPR IEEE, P849, DOI 10.1109/CVPR.2011.5995315
   Liu Z, 2015, IEEE T MULTIMEDIA, V17, P538, DOI 10.1109/TMM.2015.2399851
   Ma WY, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON SMART CITY/SOCIALCOM/SUSTAINCOM (SMARTCITY), P1, DOI [10.1109/NEBEC.2015.7117114, 10.1109/SmartCity.2015.38]
   Mingyuan Gao, 2018, IEEE Transactions on Intelligent Transportation Systems, V19, P900, DOI 10.1109/TITS.2017.2709346
   Molchanov P., 2016, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, P4207, DOI DOI 10.1109/CVPR.2016.456
   Nandakumar K, 2008, IEEE T PATTERN ANAL, V30, P342, DOI 10.1109/TPAMI.2007.70796
   Neverova N, 2016, IEEE T PATTERN ANAL, V38, P1692, DOI 10.1109/TPAMI.2015.2461544
   Neverova N, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P484, DOI 10.1109/ICCVW.2013.69
   Pigou L, 2018, INT J COMPUT VISION, V126, P430, DOI 10.1007/s11263-016-0957-7
   Terrades OR, 2009, IEEE T PATTERN ANAL, V31, P1630, DOI 10.1109/TPAMI.2008.224
   Ran LY, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17061341
   Ren Z., 2011, P 19 ACM INT C MULTI, P1093
   Salvadora S, 2007, INTELL DATA ANAL, V11, P561, DOI 10.3233/IDA-2007-11508
   Khan FS, 2012, INT J COMPUT VISION, V98, P49, DOI 10.1007/s11263-011-0495-2
   Sun C, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2629481
   Sun C, 2013, IEEE T CYBERNETICS, V43, P1418, DOI 10.1109/TCYB.2013.2265337
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Yang XS, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2962719
   Zhang JH, 2016, IEEE INT CON MULTI, DOI 10.1109/ICME.2016.7552950
   Zhang QL, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P561, DOI 10.1145/2733373.2806224
   Zhang QL, 2015, LECT NOTES COMPUT SC, V9003, P65, DOI 10.1007/978-3-319-16865-4_5
   Zhang ST, 2012, LECT NOTES COMPUT SC, V7573, P660, DOI 10.1007/978-3-642-33709-3_47
   Zhao X., 2014, ACM T MULTIMEDIA COM, V11, P1
   Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783
NR 56
TC 40
Z9 42
U1 4
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2018
VL 14
IS 1
AR 8
DI 10.1145/3152121
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FW9HO
UT WOS:000425646500008
DA 2024-07-18
ER

PT J
AU Engelbrecht, HA
   Gilmore, JS
AF Engelbrecht, Herman A.
   Gilmore, John S.
TI Pithos: Distributed Storage for Massive Multi-User Virtual Environments
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Massive multiplayer virtual environments; state consistency; state
   management; state persistency; distributed storage
ID PEER-TO-PEER; NETWORK; MANAGEMENT
AB There has been significant research effort into peer-to-peer (P2P) massively multi-user virtual environments (MMVEs). A number of architectures have been proposed to implement the P2P approach; however, the development of fully distributed MMVEs has met with a number of challenges. In this work, we address one of the key remaining challenges of state consistency and persistency in P2P MMVEs. Having reviewed state management and persistency architectures currently receiving research attention, we have identified deficiencies such as lack of load balancing, responsiveness, and scalability. To address these deficiencies, we present Pithos-a reliable, responsive, secure, load-balanced, and scalable distributed storage system, suited to P2P MMVEs. Pithos is designed specifically for P2P MMVEs, and we show that it improves the reliability and responsiveness of storage architectures as compared to existing P2P state persistency architectures.
   Pithos is implemented as an OverSim simulation running on the OMNeT++ network simulation framework. It is evaluated using up to 10,400 peers, with realistic latency profiles, with up to 15.8 million storage and retrieval requests that are generated to store a total of 2.4 million objects. Each peer in Pithos uses a maximum of 1,950Bps bandwidth to achieve 99.98% storage reliability, while the most reliable overlay storage configuration tested only achieved 93.65% reliability, using 2,182Bps bandwidth. Pithos is also more responsive than overlay storage, with an average responsiveness of 0.192s, compared with the average overlay responsiveness of 1.4s when retrieving objects from storage.
C1 [Engelbrecht, Herman A.; Gilmore, John S.] Stellenbosch Univ, MIH Media Lab, Stellenbosch, South Africa.
   [Engelbrecht, Herman A.] Univ Stellenbosch, MIH Media Lab, Elect & Elect Engn Dept, Stellenbosch, South Africa.
   [Gilmore, John S.] Amazon Web Serv, Cape Town, South Africa.
C3 Stellenbosch University; Stellenbosch University
RP Engelbrecht, HA (corresponding author), Stellenbosch Univ, MIH Media Lab, Stellenbosch, South Africa.; Engelbrecht, HA (corresponding author), Univ Stellenbosch, MIH Media Lab, Elect & Elect Engn Dept, Stellenbosch, South Africa.
EM hebrecht@sun.ac.za; jgilmore@ml.sun.ac.za
CR [Anonymous], THESIS
   [Anonymous], 2001, P 8 WORKSH HOT TOP O
   [Anonymous], 2003, AUSTR TEL NETW APPL
   AURENHAMMER F, 1991, COMPUT SURV, V23, P345, DOI 10.1145/116873.116880
   Bartle R. A., 2004, DESIGNING VIRTUAL WO
   Baughman NE, 2007, IEEE ACM T NETWORK, V15, P1, DOI 10.1109/TNET.2006.886289
   Baumgart I, 2007, 2007 IEEE GLOBAL INTERNET SYMPOSIUM, P79, DOI 10.1109/GI.2007.4301435
   Belapurkar A., 2009, DISTRIBUTED SYSTEMS
   Bharambe A, 2008, ACM SIGCOMM COMP COM, V38, P389, DOI 10.1145/1402946.1403002
   Bharambe Ashwin, 2006, P 3 C NETW SYST DES, V3
   Buyukkaya E, 2015, PEER PEER NETW APPL, V8, P276, DOI 10.1007/s12083-013-0231-5
   Buyukkaya E, 2008, CONSUM COMM NETWORK, P1050, DOI 10.1109/ccnc08.2007.239
   CALVIN J, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P450, DOI 10.1109/VRAIS.1993.380745
   Castro M, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P299, DOI 10.1145/1060289.1060317
   Chang S. A., 2008, THESIS
   Chen F, 2005, ISORC 2005: Eighth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing, Proceedings, P233
   Clarke Ian, 2001, Freenet: A Distributed Anonymous Information Storage and Retrieval System, P46, DOI [DOI 10.1007/3-540-44702-4, DOI 10.1007/3-540-44702-4_4, 10.1007/3-540-44702-4_4]
   Claypool M, 2005, COMPUT NETW, V49, P52, DOI 10.1016/j.comnet.2005.04.008
   Cooperative Association for Internet Data Analysis, 2012, CAIDA UCSD MACR TOP
   Douglas S., 2005, P IEEE INT C INFORM, P7
   Engelbrecht H. A., 2013, P 2013 12 ANN WORKSH, P1
   Fan L., 2007, Framework, P43
   Fan LL, 2009, THESIS
   Ferretti S., 2008, P 2 INT C DISTRIBUTE, P83
   Ferrucci L, 2016, J COMPUT SYST SCI, V82, P1161, DOI 10.1016/j.jcss.2016.04.008
   Frey B., 2007, MULTI DATABASE RETRI, Vvol. 315, ppp, DOI [DOI 10.1126/SCIENCE.1136800, 10.1126/science.1136800]
   Frey D., 2008, INT WORKSHOP MASSIVE, P29
   GauthierDickey C., 2004, P 3 ACM SIGCOMM WORK, P171
   Gautier L, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P233, DOI 10.1109/MMCS.1998.693647
   Gilmore J., 2015, P IEEE INT S ANT PRO, P1
   Gilmore JS, 2012, IEEE T PARALL DISTR, V23, P818, DOI 10.1109/TPDS.2011.210
   Gilmore John Sebastian, 2011, P IEEE INT WORKSH HA, P1
   Hampel T., 2006, PROC 5 ACM SIGCOMM W, P48, DOI [10.1145/1230040.1230058, DOI 10.1145/1230040.1230058]
   Hasan R, 2005, ITCC 2005: International Conference on Information Technology: Coding and Computing, Vol 2, P205, DOI 10.1109/ITCC.2005.42
   Henriques Pedro, 2012, LIGHTWEIGHT DISTRIBU
   Holzapfel S., 2011, Proceedings of the 2011 IEEE 11th International Conference on Computer and Information Technology (CIT 2011), P35, DOI 10.1109/CIT.2011.97
   Hu SY, 2006, IEEE NETWORK, V20, P22, DOI 10.1109/MNET.2006.1668400
   Hu SY, 2008, CONSUM COMM NETWORK, P1134, DOI 10.1109/ccnc08.2007.255
   Hu Shun-Yun, 2011, P 8 ACM INT C AUT CO, P171
   Iimura T., 2004, P 3 ACM SIGCOMM WORK, P116, DOI DOI 10.1145/1016540.1016549
   Jardine J., 2008, Proceedings of the 7th ACM SIGCOMM Workshop on Network and System Support for Games, P60, DOI [DOI 10.1145/1517494.1517507, 10.1145/1517494.1517507]
   Kavalionak H, 2015, PEER PEER NETW APPL, V8, P301, DOI 10.1007/s12083-013-0232-4
   Knutsson B., 2004, INFOCOM 2004. Twenty-third AnnualJoint Conference of the IEEE Computer and Communications Societies, V1, P107
   Kulkarni S, 2009, IEEE INT CONF PEER, P178, DOI 10.1109/P2P.2009.5284508
   Liu MR, 2013, J INTERNET SERV APPL, V4, DOI 10.1186/1869-0238-4-4
   Lu Fan, 2010, International Journal of Advanced Media and Communication, V4, P108, DOI 10.1504/IJAMC.2010.032138
   Lua EK, 2005, IEEE COMMUN SURV TUT, V7, P72, DOI 10.1109/COMST.2005.1610546
   Mauve M, 2004, IEEE T MULTIMEDIA, V6, P47, DOI 10.1109/TMM.2003.819751
   Maymounkov P, 2002, LECT NOTES COMPUT SC, V2429, P53
   Merabti M, 2004, GLOB TELECOMM CONF, P519
   Montresor A, 2004, FOURTH INTERNATIONAL CONFERENCE ON PEER-TO-PEER COMPUTING, PROCEEDINGS, P202, DOI 10.1109/PTP.2004.1334948
   Montresor A, 2009, IEEE INT CONF PEER, P99, DOI 10.1109/P2P.2009.5284506
   Mordacchini M., 2010, IEEE 10 INT C PEER T, P1
   Rausand M, 2004, SYSTEM RELIABILITY T
   Ricci L., 2010, 2010 International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT 2010), P881, DOI 10.1109/ICUMT.2010.5676478
   Rooney S, 2004, IEEE COMMUN MAG, V42, P114, DOI 10.1109/MCOM.2004.1299353
   Rowstron A., 2001, Operating Systems Review, V35, P188, DOI 10.1145/502059.502053
   Rowstron A., 2001, Proceedings of the Middleware 2001, P329, DOI DOI 10.1007/3-540-45518-3_18
   Schuster S, 2011, P 4 INT ICST C SIM T, P288
   Sheldon N., 2003, P 2 WORKSHOP NETWORK, P3, DOI DOI 10.1145/963900.963901
   Steiner M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P117
   Stoica I, 2001, ACM SIGCOMM COMP COM, V31, P149, DOI 10.1145/964723.383071
   Tarng Pin-Yun., 2008, Proceedings of the 7th ACM SIGCOMM Workshop on Network and System Support for Games, P47
   Vahdat A, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P271, DOI 10.1145/1060289.1060315
   Varga A., 2012, OMNET
   Varvello M, 2009, IEEE INFOCOM SER, P1161, DOI 10.1109/INFCOM.2009.5062029
   Varvello Matteo, 2009, P NETG PAR FRANC NOV, P1
   Varvello Matteo., 2008, P 2008 ACM CONEXT C
   Winick Jared, 2002, CSETR45602
   Yahyavi A, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2522968.2522977
   Yu A., 2005, Proceedings of the 15th International Workshop on Network and Operating Systems Support for Digital Audio and Video. NOSSDAV 2005, P99, DOI 10.1145/1065983.1066007
   Yu JY, 2005, IEEE COMMUN SURV TUT, V7, P32, DOI 10.1109/COMST.2005.1423333
   Zhang KW, 2011, SYM REL DIST SYST, P31, DOI 10.1109/SRDS.2011.13
NR 73
TC 7
Z9 8
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD AUG
PY 2017
VL 13
IS 3
AR 31
DI 10.1145/3105577
PG 33
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FD5TE
UT WOS:000407591900009
DA 2024-07-18
ER

PT J
AU Ranasinghe, N
   Do, EYL
AF Ranasinghe, Nimesha
   Do, Ellen Yi-Luen
TI Digital Lollipop: Studying Electrical Stimulation on the Human Tongue to
   Simulate Taste Sensations
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Digital Lollipop; taste simulation; digital taste; gustation; user
   interfaces; virtual reality
ID ELECTROGUSTOMETRY; PERCEPTION; THRESHOLDS
AB Among the five primary senses, the sense of taste is the least explored as a form of digital media applied in Human Computer Interface. This article presents an experimental instrument, the Digital Lollipop, for digitally simulating the sensation of taste (gustation) by utilizing electrical stimulation on the human tongue. The system is capable of manipulating the properties of electric currents (magnitude, frequency, and polarity) to formulate different stimuli. To evaluate the effectiveness of this method, the system was experimentally tested in two studies. The first experiment was conducted using separate regions of the human tongue to record occurrences of basic taste sensations and their respective intensity levels. The results indicate occurrences of sour, salty, bitter, and sweet sensations from different regions of the tongue. One of the major discoveries of this experiment was that the sweet taste emerges via an inverse-current mechanism, which deserves further research in the future. The second study was conducted to compare natural and artificial (virtual) sour taste sensations and examine the possibility of effectively controlling the artificial sour taste at three intensity levels (mild, medium, and strong). The proposed method is attractive since it does not requite any chemical solutions and facilitates further research opportunities in several directions including human computer interaction, virtual reality, food and beverage, as well as medicine.
C1 [Ranasinghe, Nimesha; Do, Ellen Yi-Luen] Natl Univ Singapore, Keio NUS CUTE Ctr, Interact Digital Media Inst, 02-01-01 I Cube Bldg,21 Heng Mui Keng Terrace, Singapore 119613, Singapore.
C3 National University of Singapore
RP Ranasinghe, N; Do, EYL (corresponding author), Natl Univ Singapore, Keio NUS CUTE Ctr, Interact Digital Media Inst, 02-01-01 I Cube Bldg,21 Heng Mui Keng Terrace, Singapore 119613, Singapore.
EM nimesha82@gmail.com; ellendo@acm.org
RI Do, Ellen Yi-Luen/B-3621-2009
OI Do, Ellen Yi-Luen/0000-0002-9948-6375; Ranasinghe,
   Nimesha/0000-0003-3962-8084
FU National Research Foundation, Prime Minister's Office, Singapore, under
   its International Research Centre @ Singapore Funding Initiative
FX This research is supported by the National Research Foundation, Prime
   Minister's Office, Singapore, under its International Research Centre @
   Singapore Funding Initiative and administered by the Interactive and
   Digital Media Programme Office.
CR Bartoshuk LM, 2000, CHEM SENSES, V25, P447, DOI 10.1093/chemse/25.4.447
   Blaycock R., 1996, OBTENIDO, V14, P8
   BROWN R, 1966, J VERB LEARN VERB BE, V5, P325, DOI 10.1016/S0022-5371(66)80040-3
   DALZIEL CF, 1968, IEEE TRANS IND GEN A, VIGA4, P467, DOI 10.1109/TIGA.1968.4180929
   Dan Maynes-Aminzade, 2005, C HUM FACT COMP SYST, P2207
   Derbyshire D, 2009, Revealed: The headset that will mimic all five senses and make the virtual world as convincing as real life
   Dobrin R. J., 1984, US Patent, Patent No. [4,464,293, 4464293]
   Drewnowski A, 1997, ANNU REV NUTR, V17, P237, DOI 10.1146/annurev.nutr.17.1.237
   Firestein S, 2001, NATURE, V413, P211, DOI 10.1038/35093026
   Haag R, 1993, J R Coll Surg Edinb, V38, P354
   Hayward V., 2004, Sensor Review, V24, P16, DOI 10.1108/02602280410515770
   Lackovic I, 2007, IFMBE PROC, V17, P154
   Lawless HT, 2005, CHEM SENSES, V30, P185, DOI 10.1093/chemse/bji014
   Loucks CA, 2004, PHYSIOL BEHAV, V81, P1, DOI 10.1016/j.physbeh.2003.12.014
   Nakamura Hiromi., 2011, Proceedings of the 2Nd Augmented Human International Conference, V34, P1, DOI [DOI 10.1145/1959826.1959860, 10.1145/1959826.1959860]
   Narumi T., 2010, Proceedings of the 1st Augmented Human International Conference, P1, DOI 10.1145/1785455.1785473
   PLATTIG KH, 1976, PFLUG ARCH EUR J PHY, V361, P115, DOI 10.1007/BF00583454
   PLEASONT.AK, 1970, J SPEECH HEAR RES, V13, P635, DOI 10.1044/jshr.1303.635
   Ranasinghe N, 2011, UBICOMP'11: PROCEEDINGS OF THE 2011 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P561
   Ranasinghe N, 2012, IEEE INT SYM WRBL CO, P80, DOI 10.1109/ISWC.2012.16
   Ranasinghe N, 2011, LECT NOTES COMPUT SC, V7040, P345, DOI 10.1007/978-3-642-25167-2_48
   Ratner BD, 2009, Biomedical Engineering e-Mega Reference, P377
   Robertsson L, 2007, INT J HUM-COMPUT ST, V65, P446, DOI 10.1016/j.ijhcs.2006.11.003
   Stillman JA, 2000, CLIN OTOLARYNGOL, V25, P120, DOI 10.1046/j.1365-2273.2000.00328.x
   Stillman JA, 2003, CLIN OTOLARYNGOL, V28, P406, DOI 10.1046/j.1365-2273.2003.00729.x
   Volta A., 1800, ABSTR PHIL T R SOC L, V1, P27
NR 26
TC 30
Z9 33
U1 0
U2 27
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2017
VL 13
IS 1
AR 5
DI 10.1145/2996462
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM7WJ
UT WOS:000395522700005
DA 2024-07-18
ER

PT J
AU Miao, W
   Min, GY
   Wu, YL
   Wang, HZ
   Hu, J
AF Miao, Wang
   Min, Geyong
   Wu, Yulei
   Wang, Haozhe
   Hu, Jia
TI Performance Modelling and Analysis of Software-Defined Networking under
   Bursty Multimedia Traffic
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Software-defined networking; multimedia big data; performance modelling
   and analysis; queueing decomposition; resource allocation
ID QUEUING-SYSTEMS
AB Software-Defined Networking (SDN) is an emerging architecture for the next-generation Internet, providing unprecedented network programmability to handle the explosive growth of big data driven by the popularisation of smart mobile devices and the pervasiveness of content-rich multimedia applications. In order to quantitatively investigate the performance characteristics of SDN networks, several research efforts from both simulation experiments and analytical modelling have been reported in the current literature. Among those studies, analytical modelling has demonstrated its superiority in terms of cost-effectiveness in the evaluation of large-scale networks. However, for analytical tractability and simplification, existing analytical models are derived based on the unrealistic assumptions that the network traffic follows the Poisson process, which is suitable to model nonbursty text data, and the data plane of SDN is modelled by one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies have shown that, due to the features of heavy volume and high velocity, the multimedia big data generated by real-world multimedia applications reveals the bursty and correlated nature in the network transmission. With the aim of capturing such features of realistic traffic patterns and obtaining a comprehensive and deeper understanding of the performance behaviour of SDN networks, this article presents a new analytical model to investigate the performance of SDN in the presence of the bursty and correlated arrivals modelled by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance metrics in terms of the average latency and average network throughput of the SDN networks are derived based on the developed analytical model. To consider a realistic multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted to model the SDN data plane. To address the challenging problem of obtaining the key performance metrics, for example, queue-length distribution of a PQ system with a given service capacity, a versatile methodology extending the Empty Buffer Approximation (EBA) method is proposed to facilitate the decomposition of such a PQ system to two SSSQ systems. The validity of the proposed model is demonstrated through extensive simulation experiments. To illustrate its application, the developed model is then utilised to study the strategy of the network configuration and resource allocation in SDN networks.
C1 [Miao, Wang] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan Provinc, Peoples R China.
   [Miao, Wang] Univ Exeter, Exeter EX4 4QJ, Devon, England.
   [Min, Geyong; Wu, Yulei; Wang, Haozhe; Hu, Jia] Univ Exeter, Coll Engn Math & Phys Sci, Pk Rd, Exeter EX4 4QF, Devon, England.
C3 University of Electronic Science & Technology of China; University of
   Exeter; University of Exeter
RP Miao, W (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan Provinc, Peoples R China.; Miao, W (corresponding author), Univ Exeter, Exeter EX4 4QJ, Devon, England.
EM wm255@hotmail.com; gmin@exeter.ac.uk; ylwu@exeter.ac.uk;
   hw398@exeter.ac.uk; jhu@exeter.ac.uk
RI Hu, Jia/U-8626-2019; Miao, Wang/Z-6061-2019
FU EU FP7 "QUICK" Project [PIRSES-GA-2013-612652]; National Natural Science
   Foundation of China [61303241]
FX This work is supported by the EU FP7 "QUICK" Project (Grant no.
   PIRSES-GA-2013-612652) and the National Natural Science Foundation of
   China (Grant no. 61303241).
CR Abawajy JH, 2009, FUTURE GENER COMP SY, V25, P364, DOI 10.1016/j.future.2006.04.007
   [Anonymous], 2012, TECHNICAL REPORT
   [Anonymous], 2013, P USENIX WORK HOT TO
   [Anonymous], 2010 IEEE INT C COMM, DOI DOI 10.1109/ICC.2010.5502016
   Antichi G., 2011, Global Telecom-munications Conference (GLOBECOM 2011), P1
   Azodolmolky S, 2013, IEEE GLOB COMM CONF, P1397, DOI 10.1109/GLOCOM.2013.6831269
   Beck P., 2013, IBM and Cisco together for a world class data center
   Chowdhury M, 2011, ACM SIGCOMM COMP COM, V41, P98, DOI 10.1145/2043164.2018448
   Cisco, 2016, VIS NETW IND FOR MET
   Congdon PT, 2014, IEEE ACM T NETWORK, V22, P1007, DOI 10.1109/TNET.2013.2270436
   Dong MX, 2015, IEEE NETWORK, V29, P40, DOI 10.1109/MNET.2015.7166189
   Fang S, 2013, IEEE T MAGN, V49, P2723, DOI 10.1109/TMAG.2013.2254703
   Ferng HW, 2000, IEICE T COMMUN, VE83B, P659
   Ferng HW, 2001, QUEUEING SYST, V39, P109, DOI 10.1023/A:1012786932415
   FISCHER W, 1993, PERFORM EVALUATION, V18, P149, DOI 10.1016/0166-5316(93)90035-S
   Gao P, 2010, ELECTRON LETT, V46, P763, DOI 10.1049/el.2010.3207
   HEFFES H, 1980, BELL SYST TECH J, V59, P897, DOI 10.1002/j.1538-7305.1980.tb03039.x
   Helong L., 2014, 2014 23 INT C COMP C, P1, DOI [DOI 10.1145/2670979.2670985, DOI 10.1109/ICCCN.2014.6911807]
   Hu H, 2015, IEEE NETWORK, V29, P43, DOI 10.1109/MNET.2015.7293304
   Huang Benxiong, 2015, IEEE SYST J, V2015
   Jarschel M., 2011, Proceedings of the 2011 23rd International Teletraffic Congress (ITC 2011), P1
   Jin XL, 2009, IEEE T COMMUN, V57, P1444, DOI 10.1109/TCOMM.2009.05.070376
   Kapoor R, 2013, PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT '13), P133, DOI 10.1145/2535372.2535407
   Khan A, 2013, ANN IEEE SYM FIELD P, P145, DOI 10.1109/FCCM.2013.15
   Lee H, 2010, IEEE T VEH TECHNOL, V59, P1646, DOI 10.1109/TVT.2009.2039503
   Li H, 2015, IEEE CLOUD COMPUT, V2, P42, DOI 10.1109/MCC.2015.114
   Liu KH, 2008, IEEE T VEH TECHNOL, V57, P2462, DOI 10.1109/TVT.2007.912139
   Liu L, 2009, 2009 INTERNATIONAL CONFERENCE ON SCALABLE COMPUTING AND COMMUNICATIONS & EIGHTH INTERNATIONAL CONFERENCE ON EMBEDDED COMPUTING, P319, DOI 10.1109/EmbeddedCom-ScalCom.2009.64
   Liu Z, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND COMMUNICATIONS (ICNC), P117, DOI 10.1109/ICCNC.2015.7069326
   Mahmood K, 2015, IET NETW, V4, P278, DOI 10.1049/iet-net.2014.0091
   Mannersalo P, 2002, COMPUT NETW, V40, P399, DOI 10.1016/S1389-1286(02)00302-X
   Mark BL, 2014, IEEE T SIGNAL PROCES, V62, P2709, DOI 10.1109/TSP.2014.2314434
   Miao W, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON SMART CITY/SOCIALCOM/SUSTAINCOM (SMARTCITY), P60, DOI 10.1109/SmartCity.2015.48
   Naous Jad., 2008, P 4 ACMIEEE S ARCHIT, P1, DOI DOI 10.1145/1477942.1477944
   OMNet++, 2011, OMNET NETW SIM
   ONF, 2013, TECHNICAL REPORT
   Raj P, 2015, COMPUT COMMUN NETW S, P1, DOI 10.1007/978-3-319-20744-5
   Sadasivarao A, 2013, SYMP HI PER INT, P87, DOI 10.1109/HOTI.2013.20
   Wang Guohui., 2012, Proceedings of the first workshop on Hot topics in software defined networks, HotSDN '12, P103
   Wu YL, 2013, IEEE T VEH TECHNOL, V62, P449, DOI 10.1109/TVT.2012.2219890
   Xiangxin Kong, 2013, 2013 IEEE Symposium on Computers and Communications (ISCC), P000541, DOI 10.1109/ISCC.2013.6755002
   Yeganeh SH, 2013, IEEE COMMUN MAG, V51, P136, DOI 10.1109/MCOM.2013.6461198
NR 42
TC 56
Z9 56
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2016
VL 12
IS 5
SU S
AR 77
DI 10.1145/2983637
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EJ0VT
UT WOS:000392929700008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Wu, Q
   Boulanger, P
AF Wu, Qiong
   Boulanger, Pierre
TI Enhanced Reweighted MRFs for Efficient Fashion Image Parsing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image parsing; image segmentation; fashion parsing; markov random field;
   conditional random field
ID POSE ESTIMATION; SEGMENTATION
AB Previous image parsing methods usually model the problem in a conditional random field which describes a statistical model learned from a training dataset and then processes a query image using the conditional probability. However, for clothing images, fashion items have a large variety of layering and configuration, and it is hard to learn a certain statistical model of features that apply to general cases. In this article, we take fashion images as an example to show how Markov Random Fields (MRFs) can outperform Conditional Random Fields when the application does not follow a certain statistical model learned from the training data set. We propose a new method for automatically parsing fashion images in high processing efficiency with significantly less training time by applying a modification of MRFs, named reweighted MRF (RW-MRF), which resolves the problem of over smoothing infrequent labels. We further enhance RW-MRF with occlusion prior and background prior to resolve two other common problems in clothing parsing, occlusion, and background spill. Our experimental results indicate that our proposed clothing parsing method significantly improves processing time and training time over state-of-the-art methods, while ensuring comparable parsing accuracy and improving label recall rate.
C1 [Wu, Qiong; Boulanger, Pierre] Univ Alberta, Dept Comp Sci, 116 St & 85 Ave, Edmonton, AB T6G 2R3, Canada.
C3 University of Alberta
RP Wu, Q (corresponding author), Univ Alberta, Dept Comp Sci, 116 St & 85 Ave, Edmonton, AB T6G 2R3, Canada.
RI wu, qiong/GRY-4672-2022; Boulanger, Pierre/AFV-2750-2022
CR [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], ARXIV150203240
   [Anonymous], IEEE TPAMI
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2013, P 3 ACM C INT C MULT
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Deng J., 2009, CVPR09
   Dong J, 2014, PROC CVPR IEEE, P843, DOI 10.1109/CVPR.2014.113
   Dong J, 2013, IEEE I CONF COMP VIS, P3408, DOI 10.1109/ICCV.2013.423
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   He XM, 2004, PROC CVPR IEEE, P695
   Kim E, 2011, IEEE T MULTIMEDIA, V13, P993, DOI 10.1109/TMM.2011.2161275
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Liu S, 2015, PROC CVPR IEEE, P1419, DOI 10.1109/CVPR.2015.7298748
   Liu SY, 2014, ADV MECH ENG, DOI 10.1155/2014/868041
   Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schanda J, 2007, COLORIMETRY: UNDERSTANDING THE CIE SYSTEM, P1, DOI 10.1002/9780470175637
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Winn J, 2005, IEEE I CONF COMP VIS, P756
   Wu Q, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P179, DOI 10.1109/ISM.2014.75
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
NR 30
TC 10
Z9 10
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2016
VL 12
IS 3
AR 42
DI 10.1145/2890104
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YO
UT WOS:000379425400008
DA 2024-07-18
ER

PT J
AU Qian, SS
   Zhang, TZ
   Xu, CS
   Hossain, MS
AF Qian, Shengsheng
   Zhang, Tianzhu
   Xu, Changsheng
   Hossain, M. Shamim
TI Social Event Classification via Boosted Multimodal Supervised Latent
   Dirichlet Allocation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Experimentation; Performance; Social event classification;
   multimodality; supervised LDA; AdaBoost; social media
AB With the rapidly increasing popularity of social media sites (e.g., Flickr, YouTube, and Facebook), it is convenient for users to share their own comments on many social events, which successfully facilitates social event generation, sharing and propagation and results in a large amount of user-contributed media data (e.g., images, videos, and text) for a wide variety of real-world events of different types and scales. As a consequence, it has become more and more difficult to exactly find the interesting events from massive social media data, which is useful to browse, search and monitor social events by users or governments. To deal with these issues, we propose a novel boosted multimodal supervised Latent Dirichlet Allocation (BMM-SLDA) for social event classification by integrating a supervised topic model, denoted as multi-modal supervised Latent Dirichlet Allocation (mm-SLDA), in the boosting framework. Our proposed BMM-SLDA has a number of advantages. (1) Our mm-SLDA can effectively exploit the multimodality and the multiclass property of social events jointly, and make use of the supervised category label information to classify multiclass social event directly. (2) It is suitable for large-scale data analysis by utilizing boosting weighted sampling strategy to iteratively select a small subset of data to efficiently train the corresponding topic models. (3) It effectively exploits social event structure by the document weight distribution with classification error and can iteratively learn new topic model to correct the previously misclassified event documents. We evaluate our BMM-SLDA on a real world dataset and show extensive experimental results, which demonstrate that our model outperforms state-of-the-art methods.
C1 [Qian, Shengsheng; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Qian, Shengsheng; Zhang, Tianzhu; Xu, Changsheng] China Singapore Inst Digital Media, Singapore 119613, Singapore.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, SWE Dept, Riyadh, Saudi Arabia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Saud
   University
RP Xu, CS (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
EM shengsheng.qian@nlpr.ia.ac.cn; tzzhang@nlpr.ia.ac.cn;
   csxu@nlpr.ia.ac.cn; mshossain@ksu.edu.sa
RI Hossain, M. Shamim/K-1362-2014; Zhang, Tianzhu/AGY-9389-2022; xu,
   cj/HJZ-3488-2023; Guizani, Mohsen/AAX-4534-2021
OI Hossain, M. Shamim/0000-0001-5906-9422; Zhang,
   Tianzhu/0000-0003-0764-6106; Guizani, Mohsen/0000-0002-8972-8094
FU National Program on Key Basic Research Project (973 Program)
   [2012CB316304]; National Natural Science Foundation of China [61225009,
   61303173]; Singapore National Research Foundation under International
   Research Centre @ Singapore Funding Initiative; Deanship of Scientific
   Research at King Saud University, Riyadh, Saudi Arabia [IRG-14-18]
FX This work is supported in part by the National Program on Key Basic
   Research Project (973 Program, Project No. 2012CB316304), and National
   Natural Science Foundation of China (61225009, 61303173), also by the
   Singapore National Research Foundation under International Research
   Centre @ Singapore Funding Initiative and administered by the IDM
   Programme Office. The authors extend their appreciation to the Deanship
   of Scientific Research at King Saud University, Riyadh, Saudi Arabia for
   funding this work through the International research group project No.
   IRG-14-18.
CR Achanta R., 2010, PROCEEDINGS OF THE A, P37
   [Anonymous], 2013, P 6 ACM INT C WEB SE
   [Anonymous], 2010, P 3 ACM INT C WEB SE, DOI DOI 10.1145/1718487.1718524
   [Anonymous], 2008, P ADV NEURAL INFORM
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], 2009, Proceedings of the Second ACM international Conference on Web Search and Data Mining (Barcelona, Spain, February 09-12, DOI DOI 10.1145/1498759.1498809
   Bao Y, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P239, DOI 10.1145/2505515.2505556
   Becker Hila, 2009, PROCEEDINGS OF THE I
   Blei David, 2008, NIPS, P77
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen CC, 2009, ACM T INFORM SYST, V27, DOI 10.1145/1462198.1462201
   Diakopoulos N., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P115, DOI 10.1109/VAST.2010.5652922
   Firan C.S., 2010, Proceedings of the 19th ACM international conference on Information and knowledge management, P189
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hai Leong Chieu, 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P425
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Kumar Ravi., 2004, P 10 ACM SIGKDD INT, P216
   Kumaran G., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P297, DOI 10.1145/1008992.1009044
   Li C., 2012, Cikm, P155, DOI DOI 10.1145/2396761.2396785
   Lin CJ, 2008, J MACH LEARN RES, V9, P627
   Lin FR, 2008, DECIS SUPPORT SYST, V45, P473, DOI 10.1016/j.dss.2007.06.009
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Liu XC, 2013, PROCEEDINGS OF THE 1ST INTERNATIONAL JOINT SYMPOSIUM ON JOINING AND WELDING, P151
   Makkonen J, 2004, INFORM RETRIEVAL, V7, P347, DOI 10.1023/B:INRT.0000011210.12953.86
   McMinn AJ, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P409, DOI 10.1145/2505515.2505695
   Niu ZX, 2011, PROC CVPR IEEE, P1769, DOI 10.1109/CVPR.2011.5995426
   Over Paul, 2013, PROCEEDINGS OF TRECV
   Patel D., 2008, P 2008 ACM SIGMOD IN, P393, DOI DOI 10.1145/1376616.1376658
   Phan Xuan-Hieu, 2008, P 17 INT C WORLD WID, P91
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Qian Shengsheng, 2014, PROCEEDINGS OF THE I
   Ramage Daniel., 2009, EMNLP, DOI DOI 10.3115/1699510.1699543
   Reuter T., 2012, PROC ANN ACM INT C M, P22
   Reuter Timo, 2013, PROCEEDINGS OF THE W
   Suhara Y, 2008, PROCEEDINGS OF THE I, P225
   Tianzhu Zhang, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P538, DOI 10.1109/ICCVW.2009.5457654
   Wang C., 2009, PROCEEDINGS OF THE I, P823
   Wang X, 2005, P 3 INT WORKSHOP LIN, P28
   Wilson A., 2010, HUMAN LANGUAGE TECHN, P465
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Zhang TZ, 2013, IEEE I CONF COMP VIS, P281, DOI 10.1109/ICCV.2013.42
   Zhang TZ, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2602633
   Zhang TZ, 2012, IEEE T MULTIMEDIA, V14, P1206, DOI 10.1109/TMM.2012.2191944
   Zhang TZ, 2011, IEEE T CIRC SYST VID, V21, P853, DOI 10.1109/TCSVT.2011.2133090
   Zhao Q., 2006, ACM SIGKDD INT C KNO, P484
   Zhu J, 2009, STAT INTERFACE, V2, P349
   Zhuang Y., 2012, P 20 ACM INT C MULT, P957
NR 47
TC 32
Z9 32
U1 5
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD DEC
PY 2014
VL 11
IS 2
AR 27
DI 10.1145/2659521
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AZ6DM
UT WOS:000348308800004
DA 2024-07-18
ER

PT J
AU Luo, D
   Luo, WQ
   Yang, R
   Huang, JW
AF Luo, Da
   Luo, Weiqi
   Yang, Rui
   Huang, Jiwu
TI Identifying Compression History of Wave Audio and Its Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Security; Audio compression history identification; mel-frequency
   cepstral coefficients; modified discrete cosine transform
ID IDENTIFICATION
AB Audio signal is sometimes stored and/or processed inWAV (waveform) format without any knowledge of its previous compression operations. To perform some subsequent processing, such as digital audio forensics, audio enhancement and blind audio quality assessment, it is necessary to identify its compression history. In this article, we will investigate how to identify a decompressed wave audio that went through one of three popular compression schemes, includingMP3, WMA (windows media audio) and AAC (advanced audio coding). By analyzing the corresponding frequency coefficients, including modified discrete cosine transform (MDCT) and Mel-frequency cepstral coefficients (MFCCs), of those original audio clips and their decompressed versions with different compression schemes and bit rates, we propose several statistics to identify the compression scheme as well as the corresponding bit rate previously used for a given WAV signal. The experimental results evaluated on 8,800 audio clips with various contents have shown the effectiveness of the proposed method. In addition, some potential applications of the proposed method are discussed.
C1 [Luo, Da; Yang, Rui] Sun Yat Sen Univ, Sch Informat Sci Technol, Guangzhou 510006, Guangdong, Peoples R China.
   [Luo, Weiqi] Sun Yat Sen Univ, Sch Software, Guangzhou 510006, Guangdong, Peoples R China.
   [Huang, Jiwu] Shenzhen Univ, Coll Informat Engn, Shenzhen 518060, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Shenzhen University
RP Luo, WQ (corresponding author), Sun Yat Sen Univ, Sch Software, Guangzhou 510006, Guangdong, Peoples R China.
EM is04ld@mail2.sysu.edu.cn; luoweiqi@mail.sysu.edu.cn;
   yrui@mail2.sysu.edu.cn; jwhuang@szu.edu.cn
RI huang, jw/KVY-9917-2024; Luo, Da/ABW-0504-2022
OI Luo, Da/0000-0002-9128-6782
FU National Science & Technology Pillar Program [2012BAK16B06]; NSFC
   [U1135001, 61332012, 61272191, 61202497]; Zhujiang Science and
   Technology [2011J2200091]; Guangdong NSF [S2013010012039]
FX This work is supported in part by National Science & Technology Pillar
   Program (No: 2012BAK16B06), NSFC (U1135001, 61332012, 61272191,
   61202497), the funding of Zhujiang Science and Technology (2011J2200091)
   and the Guangdong NSF (S2013010012039).
CR [Anonymous], 2003, P DIG FOR RES WORKSH
   [Anonymous], 2013, P 21 SIGN PROC COMM
   Bestagini P, 2012, INT CONF ACOUST SPEE, P2257, DOI 10.1109/ICASSP.2012.6288363
   Bianchi T., 2013, P 1 ACM WORKSH INF H, P159
   Chang CS, 2012, IEEE T SMART GRID, V3, P3, DOI 10.1109/TSG.2011.2173358
   Fan ZG, 2003, IEEE T IMAGE PROCESS, V12, P230, DOI 10.1109/TIP.2002.807361
   FU D, 2007, P SPIE ELECT IMAGING, V6505
   Hacker S., 2000, MP3: The Definitive Guide, V1st
   Hicsonmez S., 2011, P INT WORKSH INF FOR
   Jenner F, 2012, INT CONF ACOUST SPEE, P1737, DOI 10.1109/ICASSP.2012.6288234
   Kraetzer C, 2007, MM&SEC'07: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2007, P63
   Liu QZ, 2010, COGN COMPUT, V2, P291, DOI 10.1007/s12559-010-9045-4
   Luo D, 2012, INT CONF ACOUST SPEE, P1733, DOI 10.1109/ICASSP.2012.6288233
   Luo WQ, 2010, IEEE T INF FOREN SEC, V5, P810, DOI 10.1109/TIFS.2010.2074195
   Luo WQ, 2010, IEEE T INF FOREN SEC, V5, P480, DOI 10.1109/TIFS.2010.2051426
   Malik H, 2010, INT CONF ACOUST SPEE, P1710, DOI 10.1109/ICASSP.2010.5495479
   MP3Standard, INF TECHN COD MOV PI
   Painter T, 2000, P IEEE, V88, P451, DOI 10.1109/5.842996
   PAN D, 1995, IEEE MULTIMEDIA, V2, P60, DOI 10.1109/93.388209
   Princen J. P., 1987, Proceedings: ICASSP 87. 1987 International Conference on Acoustics, Speech, and Signal Processing (Cat. No.87CH2396-0), P2161
   Qiao M., 2010, P P 18 ACM INT C MUL, P1011
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Tagliasacchi M., 2010, Image Analysis for Multimedia Interactive Services (WIAMIS), 2010 11th International Workshop on, P1
   Yang R., 2010, P SPIE
   Yang R, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P117
   Yang R, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P21, DOI 10.1145/1411328.1411334
NR 26
TC 18
Z9 25
U1 0
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2014
VL 10
IS 3
AR 30
DI 10.1145/2575978
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF3AM
UT WOS:000334583800008
DA 2024-07-18
ER

PT J
AU Hong, RC
   Wang, M
   Yuan, XT
   Xu, MD
   Jiang, JG
   Yan, SC
   Chua, TS
AF Hong, Richang
   Wang, Meng
   Yuan, Xiao-Tong
   Xu, Mengdi
   Jiang, Jianguo
   Yan, Shuicheng
   Chua, Tat-Seng
TI Video Accessibility Enhancement for Hearing-Impaired Users
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Experimentation; Human Factors; Performance; Accessibility; dynamic
   captioning; hearing impairment
AB There are more than 66 million people suffering from hearing impairment and this disability brings them difficulty in video content understanding due to the loss of audio information. If the scripts are available, captioning technology can help them in a certain degree by synchronously illustrating the scripts during the playing of videos. However, we show that the existing captioning techniques are far from satisfactory in assisting the hearing-impaired audience to enjoy videos. In this article, we introduce a scheme to enhance video accessibility using a Dynamic Captioning approach, which explores a rich set of technologies including face detection and recognition, visual saliency analysis, text-speech alignment, etc. Different from the existing methods that are categorized as static captioning, dynamic captioning puts scripts at suitable positions to help the hearing-impaired audience better recognize the speaking characters. In addition, it progressively highlights the scripts word-by-word via aligning them with the speech signal and illustrates the variation of voice volume. In this way, the special audience can better track the scripts and perceive the moods that are conveyed by the variation of volume. We implemented the technology on 20 video clips and conducted an in-depth study with 60 real hearing-impaired users. The results demonstrated the effectiveness and usefulness of the video accessibility enhancement scheme.
C1 [Hong, Richang; Wang, Meng; Jiang, Jianguo] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
   [Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Yuan, Xiao-Tong; Xu, Mengdi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
C3 Hefei University of Technology; National University of Singapore;
   National University of Singapore
RP Wang, M (corresponding author), Hefei Univ Technol, Sch Comp & Informat, 193 Rd Tunxi, Hefei 230009, Peoples R China.
EM hongrc@hfut.edu.cn; eric.mengwang@gmail.com; eleyuanx@nus.edu.sg;
   jiangjg@hfut.edu.cn; eleyans@nus.edu.sg; chuats@comp.nus.edu.sg
RI Xu, Meng/HJY-7139-2023; Yan, Shuicheng/HCI-1431-2022; Wang,
   Meng/ITR-8699-2023
FU NRF/IDM of Singapore [NRF2007IDM-IDM002-047, NRF2008IDM-IDM004-029]
FX This work was supported by NRF/IDM Program of Singapore, under Research
   Grants NRF2007IDM-IDM002-047 and NRF2008IDM-IDM004-029.
CR AIMERA J, 2003, P IEEE WORKSH AUT SP
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2001, P IEEE INT C COMP VI
   [Anonymous], 2008, SIAM J OPTIM
   Arandjelovic O., 2005, P IEEE INT C COMP VI
   ARRUE M, 2007, P INT C WEB ENG
   Azzopardi L., 2009, P ANN WORKSH HUM COM
   Boyd J, 1972, AM ANN HEAR IMPAIRED, V117, P32
   BRAVERMAN BB, 1980, AM ANN DEAF, V125, P943, DOI 10.1353/aad.2012.1118
   COX S, 2002, P ACM SIGCAPH C ASS
   CU M, 2006, P INT C MULT MOD
   DAELONS W, 1993, P EUR C SPEECH COMM
   EVERINGHAN M, 2006, P BRIT MACH VIS C
   FAJARDO I, 2006, BEHAV INFORM TECHNOL, V26
   Fisher R.A., 1970, STAT METHODS RES WOR
   Garrison W, 1997, J Deaf Stud Deaf Educ, V2, P78
   Gulliver Stephen R., 2003, P IEEE INT C MULT EX
   Gulliver Stephen R, 2003, INFORM SOC J, V2, P374
   HONG R, 2010, P 17 ACM INT C MULT
   HUANG XD, 1993, COMPUTER SPEECH LANG
   Jelinek Lewis M S, 2001, J Deaf Stud Deaf Educ, V6, P43
   JUSLIN PN, 2009, SPEECH EMOTION ANAL
   Ma Y., 2003, P ACM INT C MULT
   MORENO PJ, 1998, P INT C SPOK LANG PR
   Nielsen J., 1995, ADV HUMAN COMPUTER I
   OBOZINSKI G, 2009, J STAT COMPUT
   REYNOLDS DA, 2000, DIGIT SIGNAL PROCESS, P14
   Saenko K., 2005, P IEEE INT C COMP VI
   STADELMANN T, 2009, P ACM INT C MULT
   Tang J., 2009, P ACM INT C MULT
   WAN V, 2000, P IEEE
   WANG M, 2009, P ACM MULT
   Wang M, 2009, SCHOLARPEDIA, V4, P9431
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   XU M, 2008, P ACM INT C MULT
NR 37
TC 35
Z9 38
U1 0
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2011
VL 7
IS 1
SI SI
AR 24
DI 10.1145/2037676.2037681
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 857NR
UT WOS:000297725800005
DA 2024-07-18
ER

PT J
AU Hsu, CH
   Hefeeda, M
AF Hsu, Cheng-Hsin
   Hefeeda, Mohamed
TI Using Simulcast and Scalable Video Coding to Efficiently Control Channel
   Switching Delay in Mobile TV Broadcast Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Design; Mobile TV; DVB-H; channel switching delay; broadcast networks;
   video simulcast; energy saving; scalable video coding
ID DVB-H; SERVICES; SYSTEM
AB Many mobile TV standards dictate using energy saving schemes to increase the viewing time on mobile devices, since mobile receivers are battery powered. The most common scheme for saving energy is to make the base station broadcast the video data of a TV channel in bursts with a bit-rate much higher than the encoding rate of the video stream, which enables mobile devices to turn off their radio frequency circuits when not receiving bursts. Broadcasting TV channels in bursts, however, increases channel switching delay. The switching delay is important, because long and variable switching delays are annoying to users and may turn them away from the mobile TV service. In this article, we first analyze the burst broadcasting scheme currently used in many deployed mobile TV networks, and we show that it is not efficient in terms of controlling the channel switching delay. We then propose new schemes to guarantee that a given maximum switching delay is not exceeded and that the energy consumption of mobile devices is minimized. We prove the correctness of the proposed schemes and analytically analyze the achieved energy saving. We also use scalable video coding to generalize the proposed schemes in order to support mobile devices with heterogeneous resources. We implement the proposed schemes in a mobile TV testbed to show their practicality and to validate our theoretical analysis. The experimental results show that the proposed schemes: (i) significantly increase the energy saving achieved on mobile devices: up to 95% saving is observed, and (ii) support both homogeneous and heterogeneous mobile devices.
C1 [Hsu, Cheng-Hsin; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
C3 Simon Fraser University
RP Hefeeda, M (corresponding author), Simon Fraser Univ, Sch Comp Sci, 250-13450 102nd Ave, Surrey, BC V3T 0A3, Canada.
EM mhefeeda@cs.sfu.ca
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada and in part by the British Columbia
   Innovation Council.
CR [Anonymous], 102377 ETSI EN
   [Anonymous], 2004, 302304 ETSI EN
   [Anonymous], 300401 ETSI EN
   [Anonymous], DEKT DTA 110T PCI MO
   [Anonymous], P IEEE INT PACK VID
   [Anonymous], DIV CATCH RF T H TRA
   Cho S, 2007, IEEE T BROADCAST, V53, P171, DOI 10.1109/TBC.2007.891712
   Creus G., 2007, Mobile Phone Programming, Optimizing Mobile Software with Built-in Power Profiling, P449
   DAOUD K, 2007, P IEEE INT S CONS EL, P1
   *DIBCOM, 2007, EV DVB T FRONT END R
   *DVB H FAQ, 2008, DVB H GLOB MOB TV FA
   Faria G, 2006, P IEEE, V94, P194, DOI 10.1109/JPROC.2005.861011
   *FLO, 2009, FLO TECHN OV
   Hsu C, 2009, P ACM SPIE MULT COMP
   Iizuka K, 2007, IEEE J SOLID-ST CIRC, V42, P862, DOI 10.1109/JSSC.2007.891659
   *JOINT VID TEAM, 2008, JOINT SCAL VID MOD R
   Kornfeld M, 2007, IEEE T BROADCAST, V53, P161, DOI 10.1109/TBC.2006.889210
   May G, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CONSUMER ELECTRONICS, PROCEEDINGS, P509
   Ollikainen V, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P629, DOI 10.1109/ICME.2006.262487
   *PHIL DVB H CHIP, 2006, PHIL SDIO TV1000 TV1
   REZAEI M, 2006, INT CONF ACOUST SPEE, pA897
   REZAEI M, 2007, P IEEE C PORT INF DE, P1
   Rezaei M, 2007, IEEE T BROADCAST, V53, P320, DOI 10.1109/TBC.2006.889682
   Rezaei M, 2008, IEEE T MULTIMEDIA, V10, P1455, DOI 10.1109/TMM.2008.2007315
   Siddarth S, 1998, MARKET SCI, V17, P124, DOI 10.1287/mksc.17.2.124
   Takada M, 2006, P IEEE, V94, P251, DOI 10.1109/JPROC.2005.859692
   Tan E, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, P123, DOI 10.1109/ICNP.2007.4375843
   Wang Y., 2001, VIDEO PROCESSING COM
   Yang XD, 2004, SYMPOTIC '04: JOINT IST WORKSHOP ON MOBILE FUTURE & SYMPOSIUM ON TRENDS IN COMMUNICATIONS, PROCEEDINGS, P183, DOI 10.1109/TIC.2004.1409529
NR 29
TC 1
Z9 1
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2011
VL 7
IS 2
AR 8
DI 10.1145/1925101.1925103
PG 29
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 729AT
UT WOS:000287919200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rahman, AMM
   Hossain, MA
   El Saddik, A
AF Rahman, Abu Saleh Md Mahfujur
   Hossain, M. Anwar
   El Saddik, Abdulmotaleb
TI Spatial-Geometric Approach to Physical Mobile Interaction Based on
   Accelerometer and IR Sensory Data Fusion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Algorithms; Design; Physical mobile interaction; indoor position;
   orientation; multimedia objects tagging; multisensor fusion; physical
   browsing
AB Interaction with the physical environment using mobile phones has become increasingly desirable and feasible. Nowadays mobile phones are being used to control different devices and access information/services related to those devices. To facilitate such interaction, devices are usually marked with RFID tags or visual markers, which are read by a mobile phone equipped with an integrated RFID reader or camera to fetch related information about those objects and initiate further actions. This article contributes in this domain of mobile physical interaction; however, using a spatial-geometric approach for interacting with indoor physical objects and artifacts instead of RFID based solutions. Using this approach, a mobile phone can point from a distance to an annotated object or a spatial subregion of that object for the purpose of interaction. The pointing direction and location is determined based on the fusion of IR camera and accelerometer data, where the IR cameras are used to calculate the 3D position of the mobile phone users and the accelerometer in the phone provides its tilting and orientation information. The annotation of objects and their subregions with which the mobile phone interacts is performed by specifying their geometric coordinates and associating related information or services with them. We perform experiment in a technology-augmented smart space and show the applicability and potential of the proposed approach.
C1 [Rahman, Abu Saleh Md Mahfujur; Hossain, M. Anwar; El Saddik, Abdulmotaleb] Univ Ottawa, Ottawa, ON K1N 6N5, Canada.
   [Hossain, M. Anwar] King Saud Univ, Riyadh 11451, Saudi Arabia.
C3 University of Ottawa; King Saud University
RP Rahman, AMM (corresponding author), Univ Ottawar, Multimedia Commun Res Lab, Sch Informat Technol & Engn, 800 King Edward Ave, Ottawa, ON, Canada.
EM kafi@mcrlab.uottawa.ca
RI Hossain, M. Anwar/J-9601-2013; /D-4159-2009
OI Hossain, M. Anwar/0000-0002-7673-8410; Rahman, A S M
   Mahfujur/0000-0002-8243-1191; /0000-0002-7690-8547
CR Ailisto H, 2006, PERS UBIQUIT COMPUT, V10, P333, DOI 10.1007/s00779-005-0057-0
   [Anonymous], P 5 NORD C HUM COMP
   Ballagas R, 2006, IEEE PERVAS COMPUT, V5, P70, DOI 10.1109/MPRV.2006.18
   BALLAGAS R, 2005, CHI05 HUM FACT COMP
   BROLL G, 2007, P 5 ANN IEEE INT C P
   Broll G, 2009, IEEE INTERNET COMPUT, V13, P74, DOI 10.1109/MIC.2009.120
   EGENHOFER M, 1999, P 1 BRAZ WORKSH GEOI
   Fahiem MA, 2007, GMAI 2007: GEOMETRIC MODELING AND IMAGING, PROCEEDINGS, P60, DOI 10.1109/GMAI.2007.9
   Faugeras O., 1993, Three-dimensional computer vision: a geometric viewpoint
   FROEHLICH P, 2008, INTERACTIONS, V15
   FROELELICH P, 2007, CHI07 HUM FACT COMP
   GELLERSEN H, 2008, PERSO UBIQ COMPUT, V13
   GUJAR UG, 1989, COMPUT GRAPH, V13, P505, DOI 10.1016/0097-8493(89)90012-5
   Hinske S, 2007, LECT NOTES COMPUT SC, V4551, P306
   HOLLEIS P, 2007, P WORKSH MOB SPAT IN
   Hosoya E, 2004, LECT NOTES COMPUT SC, V3058, P72
   Likert R., 1932, Arch. Psychol., V22, P44, DOI DOI 10.4135/9781412961288.N454
   MADHAVAPEDDY A, 2004, P 6 INT C UB COMP UB
   Mantyjarvi Jani., 2006, P 8 C HUMAN COMPUTER, P191, DOI DOI 10.1145/1152215.1152256
   MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029
   NAKAZATO Y, 2008, P ACM S VIRT REAL SO
   Pering T, 2005, COMMUN ACM, V48, P53, DOI 10.1145/1081992.1082020
   Rahman A.S.M.M., 2009, P 17 ACM INT C MULT
   Riekki J, 2006, IEEE PERVAS COMPUT, V5, P40, DOI 10.1109/MPRV.2006.12
   Rohs M, 2005, 25TH IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING SYSTEMS WORKSHOPS, PROCEEDINGS, P506, DOI 10.1109/ICDCSW.2005.140
   ROHS M, 2004, P 2 INT S UB COMP SY
   RUKZIO E, 2006, EXPT COMPARISON PHYS
   RUZKIO E, 2007, P EUR C AMB INT
   SANCHEZ I, 2008, P INT WORKSH RFID TE
   Schoo P, 2009, FIRST INTERNATIONAL WORKSHOP ON NEAR FIELD COMMUNICATION, PROCEEDINGS, P81, DOI 10.1109/NFC.2009.20
   SILTANEN S, 2008, USER INTERACTION MOB, P33
   Simon R, 2008, J LOCAT BASED SERV, V2, P24, DOI 10.1080/17489720802347986
   Strachan S, 2009, PERS UBIQUIT COMPUT, V13, P265, DOI 10.1007/s00779-008-0205-4
   VALKKYNEN P, 2005, PERVASIVE MOBILE INT
   WANT R, 1999, P SIGCHI C HUM FACT, P370, DOI DOI 10.1145/302979.303111
NR 35
TC 8
Z9 23
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD NOV
PY 2010
VL 6
IS 4
AR 28
DI 10.1145/1865106.1865112
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 688VA
UT WOS:000284880900006
DA 2024-07-18
ER

PT J
AU Benevenuto, F
   Rodrigues, T
   Almeida, V
   Almeida, J
   Ross, K
AF Benevenuto, Fabricio
   Rodrigues, Tiago
   Almeida, Virgilio
   Almeida, Jussara
   Ross, Keith
TI Video Interactions in Online Video Social Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human Factors; Measurement; Video interactions; video communication;
   social networks; social media; opportunistic behavior; YouTube; video
   spam; promotion
AB This article characterizes video-based interactions that emerge from YouTube's video response feature, which allows users to discuss themes and to provide reviews for products or places using much richer media than text. Based on crawled data covering a representative subset of videos and users, we present a characterization from two perspectives: the video response view and the interaction network view. In addition to providing valuable statistical models for various characteristics, our study uncovers typical user behavioral patterns in video-based environments and shows evidence of opportunistic behavior.
C1 [Benevenuto, Fabricio; Rodrigues, Tiago; Almeida, Virgilio; Almeida, Jussara] Univ Fed Minas Gerais, Belo Horizonte, MG, Brazil.
   [Ross, Keith] NYU, Polytech Inst, New York, NY 10003 USA.
C3 Universidade Federal de Minas Gerais; New York University; New York
   University Tandon School of Engineering
RP Benevenuto, F (corresponding author), Univ Fed Minas Gerais, Belo Horizonte, MG, Brazil.
EM fabricio@dcc.ufmg.br; tiagorm@dcc.ufmg.br; virgilio@dcc.ufmg.br;
   jussara@dcc.ufmg.br; ross@poly.edu
RI Almeida, Jussara M/AAD-4947-2022; InWeb, Inct/J-9839-2013
OI Almeida, Virgilio/0000-0001-6452-0361
FU Brazilian National Institute of Science and Technology for Web Research
   (MCT/CNPq/INCT) [573871/2008-6]; REBU (MCT/CNPq/CTInfo) [550995/2007-2];
   Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [0916734] Funding Source: National Science Foundation
FX This research is partially funded by the Brazilian National Institute of
   Science and Technology for Web Research (MCT/CNPq/INCT Web Grant Number
   573871/2008-6) and by research project REBU (MCT/CNPq/CTInfo Grant
   Number 550995/2007-2).
CR Albert R, 1999, NATURE, V401, P130, DOI 10.1038/43601
   ALMEIDA V, 1996, P INT C PAR DISTR IN
   [Anonymous], 2008, First Monday, DOI [10.5210/fm.v14i1.2317, DOI 10.5210/FM.V14I1.2317]
   [Anonymous], PHYS REV E
   [Anonymous], P ACM SIGCOMM INT ME
   [Anonymous], 1991, The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling
   [Anonymous], P ACM SIGCOMM INT ME
   [Anonymous], P ACM SIGCOMM INT ME
   Barford P., 1998, Performance Evaluation Review, V26, P151, DOI 10.1145/277858.277897
   Benevenuto F., 2008, P INT WORKSH ADV INF
   BENEVENUTO F, 2009, P INT ACM SIGIR C
   Benevenuto Fabricio, 2008, P ACM INT C MULT MM
   Boll S, 2007, IEEE MULTIMEDIA, V14, P9, DOI 10.1109/MMUL.2007.17
   BRESLAU L, 1999, P IEEE INF
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   Broder A, 2000, COMPUT NETW, V33, P309, DOI 10.1016/S1389-1286(00)00083-9
   BRODER A, 2000, P INT WORLD WID WEB
   CHAU D, 2007, P INT WORLD WID WEB
   CHOUDHURY M, 2009, P INT WORLD WID WEB
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   CHUN H, 2008, P ACM SIGCOMM C INT
   Duarte F., 2008, HIERARCHICAL CHARACT
   EBEL H, 2002, PHYS REV E
   Garlaschelli D, 2004, PHYS REV LETT, V93
   Golder S. A., 2007, P INT C COMM TECHN
   Gomes LH, 2007, PERFORM EVALUATION, V64, P690, DOI 10.1016/j.peva.2006.11.001
   GOMEZ V, 2008, P INT C WORLD WID WE
   Heymann P, 2007, IEEE INTERNET COMPUT, V11, P36, DOI 10.1109/MIC.2007.125
   Java A., 2007, P WORKSH WEB MIN SOC
   Kleinberg JM, 1999, ACM COMPUT SURV, V31
   Krishnamurthy B., 2008, P ACM SIGCOMM WORKSH
   Kumar R, 2003, P INT WORLD WID WEB
   Leskovec J., 2008, WWW
   LIBENNOWELL D, 2005, P NAT AC SCI
   Mattson M., 2002, PHYS REV LETT, V89, P112001, DOI [10.1103/PhysRevLett.89.112001, DOI 10.1103/PHYSREVLETT.89.112001]
   MISLOVE A, 2007, P ACM SIGCOMM INT ME
   Onnela JP, 2007, NEW J PHYS, V9, DOI 10.1088/1367-2630/9/6/179
   PANDURANGAN G, 2002, P INT C COMP COMB CO
   Ross KW, 2003, IEEE MULTIMEDIA, V10, P70, DOI 10.1109/MMUL.2003.1195163
   SESHADRI M, 2008, P ACM SIGKDD INT C K
   SHANNON M, 2007, COMMUN ACM, V50
   Trivedi KishorS., 2002, PROBABILITY STAT REL, V2nd
   Zhang J., 2007, P INT WORLD WID WEB
   Zink M., 2008, P IEEE MULT COMP NET
   ZINMAN A, 2007, P C EM ANT CEAS
   Zlatic V, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.016115
NR 46
TC 43
Z9 49
U1 2
U2 33
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD OCT
PY 2009
VL 5
IS 4
AR 30
DI 10.1145/1596990.1596994
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 529WO
UT WOS:000272549900004
OA Green Submitted
DA 2024-07-18
ER

EF